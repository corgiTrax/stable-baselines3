Current timestep = 0. State = [[-0.25777152  0.00776806]]. Action = [[ 0.0592227  -0.07896101  0.10002702  0.69163465]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0417, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-0.2573559   0.00736899]]. Action = [[-0.23175834  0.16157049 -0.17457716 -0.9083881 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0206, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.25681052  0.00698594]]. Action = [[ 0.10777372 -0.17336068  0.23817003 -0.8266462 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0095, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.25635657  0.00646283]]. Action = [[-0.15476242 -0.21615022 -0.09076014 -0.9427275 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.25620833  0.00632039]]. Action = [[-0.22328484 -0.07975662 -0.01632771 -0.9672626 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-0.25619367  0.00643324]]. Action = [[-0.15625669  0.22396743 -0.17903523 -0.81824446]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.2559739   0.00619498]]. Action = [[-0.22817229 -0.23541759 -0.01779614 -0.9419464 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.25570166  0.00531427]]. Action = [[ 0.24835393 -0.15926161  0.10203969 -0.44685942]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.2556374   0.00455006]]. Action = [[-0.12073897 -0.00840797  0.22876114  0.68638897]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0069, grad_fn=<MseLossBackward0>)
Current timestep = 9. State = [[-0.25578573  0.00452496]]. Action = [[0.11178476 0.22096193 0.17667708 0.394359  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.2560125   0.00329747]]. Action = [[-0.11369312 -0.11519758 -0.12582748 -0.8629738 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.25755218 -0.00057515]]. Action = [[ 0.09159207  0.00998145 -0.18711367 -0.2652762 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 12. State = [[-0.2577637  -0.00297608]]. Action = [[ 0.12575671 -0.0521241   0.24319214 -0.15025967]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.25769016 -0.00344925]]. Action = [[-0.15767829 -0.07685664 -0.11211076 -0.10411209]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.2575374 -0.003898 ]]. Action = [[-0.19216827 -0.08881024  0.19588476 -0.14944673]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-0.25719523 -0.00461133]]. Action = [[-0.22872156 -0.14864178 -0.2280402  -0.09097433]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 16. State = [[-0.2571604  -0.00656088]]. Action = [[ 0.02720761  0.04816377 -0.15911098 -0.6889742 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 17. State = [[-0.25686955 -0.00647957]]. Action = [[ 0.17146724  0.15265644  0.24385068 -0.94935703]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Current timestep = 18. State = [[-0.25654188 -0.00672178]]. Action = [[ 0.09192884 -0.07685201  0.1833018   0.5612252 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 19. State = [[-0.2561632  -0.00710739]]. Action = [[ 0.19213748 -0.04465589 -0.21288802  0.7883644 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.255749   -0.00746753]]. Action = [[ 0.19356224 -0.02162762 -0.20124331 -0.00516373]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Current timestep = 21. State = [[-0.25514567 -0.00792538]]. Action = [[0.16898978 0.06825626 0.23936212 0.69657385]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 22. State = [[-0.25498825 -0.00820936]]. Action = [[-0.10490048 -0.205586    0.0929459   0.8432796 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 23. State = [[-0.25437838 -0.00904888]]. Action = [[ 0.02109122  0.04952088 -0.02995156 -0.9017577 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 23 of -1
Current timestep = 24. State = [[-0.2538477  -0.00899661]]. Action = [[-0.14801098 -0.13947262 -0.01709916  0.3902607 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 25. State = [[-0.2533884  -0.00892651]]. Action = [[-0.11468431 -0.17961937  0.1597091   0.67007184]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 26. State = [[-0.2533084  -0.00901469]]. Action = [[-0.18338421 -0.17478368 -0.03445089 -0.27979952]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.25322795 -0.00908759]]. Action = [[-0.11313266 -0.13504255 -0.17268409  0.6911154 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.25275698 -0.00900352]]. Action = [[-0.19238982  0.08314916  0.0983358   0.9656    ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 29. State = [[-0.2521791 -0.0090225]]. Action = [[-0.01874208 -0.06809507 -0.19939484 -0.17853701]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 29 of 1
Current timestep = 30. State = [[-0.25218177 -0.01021382]]. Action = [[-0.1332438   0.02629793  0.20239508  0.4569943 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 31. State = [[-0.2522942  -0.01039162]]. Action = [[ 0.00454271 -0.1658682  -0.10637772 -0.9665713 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(5.8076e-05, grad_fn=<MseLossBackward0>)
Current timestep = 32. State = [[-0.2522798  -0.01047565]]. Action = [[0.14593688 0.20304176 0.20356047 0.6713116 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 33. State = [[-0.2523108  -0.01054513]]. Action = [[-0.06292322 -0.17158705 -0.07593453 -0.50454545]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 33 of 1
Current timestep = 34. State = [[-0.25234902 -0.01066966]]. Action = [[-0.05228475 -0.18823065  0.01243675 -0.16222483]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 35. State = [[-0.2523632  -0.01078038]]. Action = [[0.07008028 0.20700341 0.18413877 0.64361525]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 36. State = [[-0.25230297 -0.01083822]]. Action = [[ 0.18468726 -0.16684249  0.02484658 -0.78474957]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 37. State = [[-0.2524255  -0.01091939]]. Action = [[ 0.23092055  0.16952154 -0.10305113 -0.4222896 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 38. State = [[-0.2524326  -0.01097474]]. Action = [[-0.04239224 -0.21644545 -0.11797252 -0.6848416 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.2524397  -0.01102971]]. Action = [[ 0.13895297 -0.01006567 -0.09943907  0.31607068]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 40. State = [[-0.25244686 -0.01108505]]. Action = [[ 0.09179342 -0.17108117 -0.12418121  0.4954741 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 41. State = [[-0.25244686 -0.01108505]]. Action = [[-0.04506221 -0.1426232   0.24598712 -0.21620214]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.25246957 -0.0111251 ]]. Action = [[-0.1293534  -0.00811034 -0.09985557 -0.6478488 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 43. State = [[-0.2527312  -0.01125059]]. Action = [[ 0.1021578   0.15607664 -0.15648852 -0.77732706]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
State prediction error at timestep 43 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 44. State = [[-0.2532513  -0.01143041]]. Action = [[-1.2670457e-04  1.8514681e-01  5.1051974e-02 -8.5742801e-01]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(7.3702e-05, grad_fn=<MseLossBackward0>)
Current timestep = 45. State = [[-0.25349632 -0.0114397 ]]. Action = [[-0.14680515  0.05106673 -0.24317671 -0.47378242]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 46. State = [[-0.2541955  -0.01152857]]. Action = [[-0.15364112  0.13866925  0.15125576  0.2631209 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 47. State = [[-0.25517464 -0.01175888]]. Action = [[-0.12986203  0.03230703 -0.07475053  0.7023165 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.25609487 -0.01144405]]. Action = [[ 0.15777433 -0.14189176 -0.20048526  0.6151099 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 49. State = [[-0.25694016 -0.01122377]]. Action = [[-0.19820008 -0.04957984 -0.24084096 -0.7098076 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(3.5384e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 49 of 1
Current timestep = 50. State = [[-0.25895253 -0.01087371]]. Action = [[-0.03737679 -0.0223745   0.17190748 -0.5995739 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 51. State = [[-0.25939938 -0.01098055]]. Action = [[-0.23282352 -0.19724967  0.09029123  0.52555907]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 52. State = [[-0.2596884  -0.01109382]]. Action = [[ 0.16687924 -0.21581692 -0.22476335 -0.2763635 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 53. State = [[-0.25995967 -0.0111809 ]]. Action = [[0.16195855 0.13970578 0.19235939 0.80421495]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
State prediction error at timestep 53 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 54. State = [[-0.2603473  -0.01109098]]. Action = [[ 0.07571796 -0.21809581  0.14536929 -0.00824553]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 55. State = [[-0.2608386  -0.01115017]]. Action = [[-0.05875994  0.23723105  0.0069564   0.04226577]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
State prediction error at timestep 55 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 56. State = [[-0.26110092 -0.01130295]]. Action = [[0.23277712 0.144337   0.22747055 0.5987381 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of -1
Current timestep = 57. State = [[-0.26146698 -0.01134307]]. Action = [[-0.04507665 -0.13384758  0.08346546 -0.90301025]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
State prediction error at timestep 57 is tensor(2.1631e-05, grad_fn=<MseLossBackward0>)
Current timestep = 58. State = [[-0.26191056 -0.0113916 ]]. Action = [[ 0.12624168 -0.19345538 -0.17320903  0.9227669 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 59. State = [[-0.2627327  -0.01157678]]. Action = [[ 0.02901781 -0.04104127 -0.03047998  0.91467714]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
State prediction error at timestep 59 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 60. State = [[-0.26270786 -0.0117793 ]]. Action = [[0.21579474 0.06185246 0.00512597 0.31387782]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
State prediction error at timestep 60 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.26275063 -0.01203911]]. Action = [[-0.16256054 -0.13331786  0.08386377 -0.06111652]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
State prediction error at timestep 61 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 62. State = [[-0.2628498  -0.01247571]]. Action = [[ 0.23175007 -0.15403919 -0.01966406  0.42503154]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
State prediction error at timestep 62 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 63. State = [[-0.26298895 -0.01313052]]. Action = [[-0.11611953 -0.06587416 -0.20476107  0.87919855]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
State prediction error at timestep 63 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 64. State = [[-0.26475862 -0.01595744]]. Action = [[-0.03531164 -0.06886896  0.20474285 -0.51866627]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
State prediction error at timestep 64 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 65. State = [[-0.26539108 -0.01700059]]. Action = [[ 1.5200418e-01  5.0872564e-04 -2.3882160e-01 -6.0166931e-01]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
State prediction error at timestep 65 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 66. State = [[-0.26570672 -0.01760256]]. Action = [[-0.16231118 -0.0891057  -0.23518954 -0.79843146]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(5.3664e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 66 of -1
Current timestep = 67. State = [[-0.26581413 -0.01847612]]. Action = [[-0.20374908 -0.01150632 -0.22282723 -0.3347326 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 68. State = [[-0.26635328 -0.01924104]]. Action = [[-0.12283057  0.23496968  0.10891441  0.857749  ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
State prediction error at timestep 68 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 69. State = [[-0.26708037 -0.02058965]]. Action = [[ 0.19875953 -0.04461728  0.17203295  0.20480514]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
State prediction error at timestep 69 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 70. State = [[-0.2680428  -0.02242324]]. Action = [[ 0.0423055   0.09822208 -0.14065307 -0.16408455]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of -1
Current timestep = 71. State = [[-0.26824147 -0.02160274]]. Action = [[-0.2246993   0.1916818  -0.23948629  0.510025  ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
State prediction error at timestep 71 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 72. State = [[-0.26849777 -0.02111058]]. Action = [[ 0.19663063 -0.03666542 -0.13764651  0.64164233]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
State prediction error at timestep 72 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 73. State = [[-0.26861718 -0.02106388]]. Action = [[-0.16271998 -0.06960629 -0.07580125  0.10896921]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
State prediction error at timestep 73 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-0.26873723 -0.0209674 ]]. Action = [[ 0.15736455 -0.04100072 -0.14124285  0.22896492]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
State prediction error at timestep 74 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 75. State = [[-0.2690458  -0.01982196]]. Action = [[0.11457753 0.00849062 0.02581316 0.9877709 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 76. State = [[-0.26909208 -0.01984438]]. Action = [[-0.12595683 -0.23919265 -0.06192961  0.58599234]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
State prediction error at timestep 76 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 77. State = [[-0.26902977 -0.01972499]]. Action = [[ 0.12726918  0.02514607 -0.10971002  0.00250423]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
State prediction error at timestep 77 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 78. State = [[-0.26722866 -0.01938195]]. Action = [[ 0.11067697 -0.072431   -0.10243985  0.36286962]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
State prediction error at timestep 78 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 79. State = [[-0.26627782 -0.01963796]]. Action = [[-0.19943449  0.10978016  0.22233611 -0.31467527]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 80. State = [[-0.26569834 -0.01981843]]. Action = [[ 0.10765812 -0.21946158  0.23012954 -0.789974  ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
State prediction error at timestep 80 is tensor(7.5647e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 80 of -1
Current timestep = 81. State = [[-0.2635299  -0.02010841]]. Action = [[ 0.06104663 -0.00840832  0.15253043 -0.48061132]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
State prediction error at timestep 81 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 82. State = [[-0.26141283 -0.02016152]]. Action = [[0.0735448  0.09073937 0.23336193 0.06049263]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
State prediction error at timestep 82 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 82 of 1
Current timestep = 83. State = [[-0.25924093 -0.01892991]]. Action = [[-0.11075184  0.02161887  0.24234393 -0.6302764 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
State prediction error at timestep 83 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 83 of 1
Current timestep = 84. State = [[-0.25923926 -0.01851613]]. Action = [[-0.21930888  0.16002226 -0.05937253 -0.6420914 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
State prediction error at timestep 84 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 85. State = [[-0.25941736 -0.01788719]]. Action = [[ 0.05011711 -0.06583133  0.15936634 -0.5074417 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.25941187 -0.0179883 ]]. Action = [[-0.05495408  0.17250872 -0.17666881  0.19780362]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
State prediction error at timestep 86 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 87. State = [[-0.25926742 -0.01810738]]. Action = [[-0.17675535  0.18195528 -0.22459365  0.22639978]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
State prediction error at timestep 87 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 88. State = [[-0.2590732  -0.01825385]]. Action = [[ 0.05171433 -0.01360948  0.20727715 -0.33505005]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.2590354  -0.01828592]]. Action = [[-0.01571646  0.1355387   0.08327758 -0.66688687]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 90. State = [[-0.25893706 -0.0183376 ]]. Action = [[ 0.17337078 -0.08983162 -0.05193281 -0.93309325]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(2.2563e-05, grad_fn=<MseLossBackward0>)
Current timestep = 91. State = [[-0.25889924 -0.01836971]]. Action = [[-0.19855373 -0.12995026  0.08278748 -0.1944164 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
State prediction error at timestep 91 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 91 of 1
Current timestep = 92. State = [[-0.25870225 -0.01836535]]. Action = [[-0.02054094 -0.0116626   0.11954433  0.89220667]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
State prediction error at timestep 92 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 92 of 1
Current timestep = 93. State = [[-0.2586779 -0.0183515]]. Action = [[ 0.08070794 -0.15067993  0.2218706   0.21454954]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
State prediction error at timestep 93 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 94. State = [[-0.25859773 -0.01828517]]. Action = [[0.08936247 0.12830228 0.21467519 0.254758  ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 95. State = [[-0.25823858 -0.01777493]]. Action = [[-0.17765613 -0.07954688 -0.12128666  0.8293729 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
State prediction error at timestep 95 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 96. State = [[-0.25763538 -0.01709712]]. Action = [[-0.21519695 -0.20155801 -0.07710943 -0.71273607]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
State prediction error at timestep 96 is tensor(6.3688e-06, grad_fn=<MseLossBackward0>)
Current timestep = 97. State = [[-0.25582612 -0.01615397]]. Action = [[ 0.07782865  0.08491567 -0.17332174 -0.64080024]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
State prediction error at timestep 97 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 97 of 1
Current timestep = 98. State = [[-0.25531474 -0.01530628]]. Action = [[-0.21121727  0.21663672 -0.21907908 -0.156259  ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
State prediction error at timestep 98 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 99. State = [[-0.25434276 -0.01447497]]. Action = [[-0.09544617  0.14404786  0.17103356 -0.5073035 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
State prediction error at timestep 99 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 100. State = [[-0.25356352 -0.01394501]]. Action = [[-0.17767128 -0.11529484 -0.02419807 -0.45626235]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 100 of 1
Current timestep = 101. State = [[-0.25271872 -0.01363854]]. Action = [[ 0.23595399 -0.17498942 -0.1700629   0.622844  ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
State prediction error at timestep 101 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 102. State = [[-0.24910395 -0.01182203]]. Action = [[-0.11825486  0.0137783   0.14927655 -0.8382864 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
State prediction error at timestep 102 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 103. State = [[-0.24899606 -0.01112344]]. Action = [[-0.08196788 -0.13403194 -0.19329976  0.04177213]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
State prediction error at timestep 103 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 104. State = [[-0.2492849  -0.01075355]]. Action = [[ 0.23764765  0.03176716 -0.2054826  -0.7876281 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
State prediction error at timestep 104 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 105. State = [[-0.24952221 -0.00930653]]. Action = [[ 0.10176265  0.07378903 -0.22044012 -0.80555075]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
State prediction error at timestep 105 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 105 of 1
Current timestep = 106. State = [[-0.24910977 -0.00810522]]. Action = [[-0.16712888  0.21671826 -0.04487    -0.01524734]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 107. State = [[-0.24878015 -0.00769545]]. Action = [[ 0.02269694 -0.20945452 -0.22934674  0.24398887]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 108. State = [[-0.24790463 -0.00562223]]. Action = [[-0.01732881  0.05059433  0.20240027  0.9615035 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(2.3380e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of 1
Current timestep = 109. State = [[-0.2479656 -0.0031549]]. Action = [[ 0.04834995 -0.08322138  0.12541252 -0.82320803]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
State prediction error at timestep 109 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 109 of 1
Current timestep = 110. State = [[-0.24759014 -0.00318469]]. Action = [[-0.1561088  -0.19295336 -0.19326872 -0.06041932]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
State prediction error at timestep 110 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 111. State = [[-0.2473871  -0.00333997]]. Action = [[ 0.18459862 -0.02021891  0.17914134 -0.9226777 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
State prediction error at timestep 111 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 112. State = [[-0.24713182 -0.00355711]]. Action = [[-0.1515417  -0.05644351  0.07382527  0.14427674]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
State prediction error at timestep 112 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 112 of 1
Current timestep = 113. State = [[-0.24668963 -0.00336498]]. Action = [[ 0.2426297  -0.09145305 -0.21388702  0.5367905 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 114. State = [[-0.24602379 -0.00346965]]. Action = [[ 0.11937961 -0.08026537 -0.10960659  0.5398172 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
State prediction error at timestep 114 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 115. State = [[-0.24259605 -0.00448543]]. Action = [[-0.09991226 -0.11299694 -0.2150499   0.926841  ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
State prediction error at timestep 115 is tensor(3.9281e-05, grad_fn=<MseLossBackward0>)
Current timestep = 116. State = [[-0.24180532 -0.00531307]]. Action = [[ 0.16431838  0.07785696 -0.24156457 -0.08261347]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 117. State = [[-0.24159275 -0.00631022]]. Action = [[-0.14439785  0.14231017  0.05749708  0.17133439]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
State prediction error at timestep 117 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 118. State = [[-0.24119611 -0.00827767]]. Action = [[-0.04207885  0.05814937  0.16913256 -0.12588274]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
State prediction error at timestep 118 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 119. State = [[-0.24142492 -0.00801531]]. Action = [[ 0.23112553 -0.15757132 -0.11060619  0.0060786 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 120. State = [[-0.24116199 -0.0078979 ]]. Action = [[0.00809884 0.18266195 0.12871683 0.84330475]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
State prediction error at timestep 120 is tensor(1.7300e-06, grad_fn=<MseLossBackward0>)
Current timestep = 121. State = [[-0.241073   -0.00808424]]. Action = [[ 0.18213499 -0.22147243  0.17671567  0.9560454 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 122. State = [[-0.24114902 -0.00819046]]. Action = [[ 0.06202281 -0.07787545 -0.21602471  0.8649833 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
State prediction error at timestep 122 is tensor(2.4499e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 122 of 1
Current timestep = 123. State = [[-0.2408597  -0.00905184]]. Action = [[-0.14291048 -0.05902681  0.13239735 -0.50249755]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(7.5404e-05, grad_fn=<MseLossBackward0>)
Current timestep = 124. State = [[-0.2407606  -0.00943547]]. Action = [[ 0.11186826  0.2110878  -0.14434566  0.5340612 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.24065496 -0.00982397]]. Action = [[ 0.19000751  0.21453446 -0.12114748 -0.9255161 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
State prediction error at timestep 125 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 126. State = [[-0.24061142 -0.01006884]]. Action = [[-0.24265988 -0.02651055  0.10572168  0.22488546]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(8.5069e-05, grad_fn=<MseLossBackward0>)
Current timestep = 127. State = [[-0.24057294 -0.0104277 ]]. Action = [[ 0.16019279  0.12645614  0.22661972 -0.75371933]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(9.4186e-05, grad_fn=<MseLossBackward0>)
Current timestep = 128. State = [[-0.24050935 -0.01108447]]. Action = [[0.04625228 0.10028899 0.13324505 0.25951838]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
State prediction error at timestep 128 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 128 of 1
Current timestep = 129. State = [[-0.24024755 -0.0106583 ]]. Action = [[-0.06528002  0.17061663  0.02682522  0.6317551 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
State prediction error at timestep 129 is tensor(3.9619e-05, grad_fn=<MseLossBackward0>)
Current timestep = 130. State = [[-0.24004856 -0.01055442]]. Action = [[ 0.18693632 -0.16805568 -0.13558586 -0.07895041]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 131. State = [[-0.23989667 -0.01049209]]. Action = [[-0.17604007 -0.13765728 -0.01848251 -0.8022221 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.23944193 -0.01023504]]. Action = [[-0.05430502  0.01540095  0.23620242 -0.84332407]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 133. State = [[-0.23924798 -0.01021106]]. Action = [[ 0.04297942 -0.15689243  0.20468888 -0.93628776]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 134. State = [[-0.23948765 -0.00993748]]. Action = [[ 0.10988802 -0.19659388  0.06423226 -0.68173456]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 135. State = [[-0.23947325 -0.00970556]]. Action = [[-0.05088952  0.02683505  0.01745114 -0.5931717 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 135 of 1
Current timestep = 136. State = [[-0.23966056 -0.00941797]]. Action = [[0.10237139 0.15606928 0.17285094 0.17276311]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 137. State = [[-0.23977077 -0.0088806 ]]. Action = [[ 0.00567088 -0.05944821 -0.05541758 -0.5556269 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 137 of 1
Current timestep = 138. State = [[-0.23964684 -0.00918307]]. Action = [[-0.02311236 -0.10790688 -0.10823506  0.76591194]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(8.5835e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 138 of 1
Current timestep = 139. State = [[-0.23948434 -0.01030451]]. Action = [[-0.16262701  0.18779501 -0.1879952  -0.7503608 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 140. State = [[-0.23942941 -0.01099048]]. Action = [[-0.13684107  0.20577776 -0.11555922 -0.59315753]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 141. State = [[-0.23949085 -0.01238394]]. Action = [[ 0.08871961 -0.05729014  0.13635251 -0.51662487]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(7.9482e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.23932211 -0.01491882]]. Action = [[-0.07282111 -0.10820191 -0.13352542 -0.46911263]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(4.2526e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 142 of 1
Current timestep = 143. State = [[-0.23922881 -0.01643413]]. Action = [[-0.23447695 -0.12601383  0.0398283   0.25171828]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
State prediction error at timestep 143 is tensor(4.2311e-06, grad_fn=<MseLossBackward0>)
Current timestep = 144. State = [[-0.2389584 -0.0176519]]. Action = [[-0.04801449 -0.13626365 -0.00891985  0.36565065]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(3.6185e-05, grad_fn=<MseLossBackward0>)
Current timestep = 145. State = [[-0.2389283  -0.01860098]]. Action = [[0.15744936 0.18116617 0.20850652 0.54815567]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 146. State = [[-0.23905139 -0.01902666]]. Action = [[-0.18861166 -0.22007121  0.18697736  0.57423306]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(9.3998e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 146 of 1
Current timestep = 147. State = [[-0.23934342 -0.02233213]]. Action = [[-0.08079329 -0.09783754 -0.1682417   0.15947771]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 148. State = [[-0.23940447 -0.023652  ]]. Action = [[-0.15502517 -0.14477193  0.11983541  0.7791295 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of 1
Current timestep = 149. State = [[-0.23936728 -0.02490627]]. Action = [[-0.03598535 -0.14547062 -0.07821907  0.6257515 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(7.4609e-05, grad_fn=<MseLossBackward0>)
Current timestep = 150. State = [[-0.23949245 -0.02613808]]. Action = [[0.06829241 0.225716   0.19303107 0.4776919 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 151. State = [[-0.2397536  -0.02705971]]. Action = [[-0.22463866  0.2326158   0.16054767  0.759802  ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, True, False]
State prediction error at timestep 151 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 152. State = [[-0.23989186 -0.02834319]]. Action = [[-0.14579846  0.1829418   0.00201452 -0.7884516 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 152 of 1
Current timestep = 153. State = [[-0.24021861 -0.03021271]]. Action = [[0.10950324 0.13129655 0.01372573 0.85467577]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 154. State = [[-0.24020477 -0.02977994]]. Action = [[-0.17623958 -0.14941312 -0.20753668  0.77132297]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(9.1656e-05, grad_fn=<MseLossBackward0>)
Current timestep = 155. State = [[-0.24024053 -0.02926346]]. Action = [[ 0.02906373 -0.15042673  0.05268306  0.7863357 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 156. State = [[-0.24029484 -0.02912911]]. Action = [[ 0.24245143  0.20031193  0.2333042  -0.99635684]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 157. State = [[-0.24011745 -0.02941637]]. Action = [[-0.1751768  -0.07073659  0.10406426  0.6212616 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 158. State = [[-0.2401164  -0.02904441]]. Action = [[ 0.09604096  0.14184225  0.05701581 -0.84436476]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of 1
Current timestep = 159. State = [[-0.24022017 -0.0288265 ]]. Action = [[ 0.21611738  0.05178416  0.00205615 -0.4366697 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(6.1210e-05, grad_fn=<MseLossBackward0>)
Current timestep = 160. State = [[-0.24013573 -0.02848765]]. Action = [[-0.03956728  0.05196536 -0.17637889 -0.7690759 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 161. State = [[-0.24009383 -0.02818182]]. Action = [[-0.04558367  0.20071983  0.13138515 -0.75162345]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 162. State = [[-0.2402321  -0.02783234]]. Action = [[-0.20476045 -0.07543531 -0.24394642  0.63232493]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(1.8113e-05, grad_fn=<MseLossBackward0>)
Current timestep = 163. State = [[-0.24026334 -0.02761504]]. Action = [[ 0.091609    0.1449827  -0.17278089  0.1015029 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 164. State = [[-0.24030729 -0.02693149]]. Action = [[-0.09742844  0.08446881  0.05812299 -0.65005475]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 164 of 1
Current timestep = 165. State = [[-0.24067366 -0.02605469]]. Action = [[ 0.05925363  0.21395653 -0.21235776 -0.48051327]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(2.4461e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of 1
Current timestep = 166. State = [[-0.24087623 -0.02525091]]. Action = [[-0.18454215 -0.09273484 -0.19647057 -0.40207165]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(6.3425e-06, grad_fn=<MseLossBackward0>)
Current timestep = 167. State = [[-0.2413521  -0.02294507]]. Action = [[ 0.12616682  0.03434646  0.05373666 -0.04073173]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(8.6359e-06, grad_fn=<MseLossBackward0>)
Current timestep = 168. State = [[-0.24157439 -0.02150162]]. Action = [[-0.12025839 -0.00854777 -0.0061065   0.6359198 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(3.3227e-05, grad_fn=<MseLossBackward0>)
Current timestep = 169. State = [[-0.24167341 -0.02133248]]. Action = [[ 0.01956242  0.21521217 -0.15477435 -0.82368386]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 170. State = [[-0.24169835 -0.02111528]]. Action = [[ 0.2142643  -0.10980508 -0.04350321  0.9067836 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(8.6548e-05, grad_fn=<MseLossBackward0>)
Current timestep = 171. State = [[-0.24183539 -0.02076736]]. Action = [[-0.06478073  0.14555392  0.06792024  0.8929224 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(6.6269e-05, grad_fn=<MseLossBackward0>)
Current timestep = 172. State = [[-0.2420835 -0.0195213]]. Action = [[ 0.0135113   0.11786246  0.13388896 -0.9444268 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 172 of 1
Current timestep = 173. State = [[-0.24237248 -0.01840026]]. Action = [[ 0.09800205 -0.20195256 -0.11271137  0.6698122 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(4.8215e-06, grad_fn=<MseLossBackward0>)
Current timestep = 174. State = [[-0.24257375 -0.01764353]]. Action = [[ 0.0275394  -0.14608139  0.16629177 -0.95294   ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 175. State = [[-0.24271785 -0.01682348]]. Action = [[-0.2254536  -0.10713556 -0.11812523 -0.9061707 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 176. State = [[-0.24291259 -0.01631965]]. Action = [[ 0.10753301  0.13948306  0.24537238 -0.6642753 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(6.5908e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of 1
Current timestep = 177. State = [[-0.24323201 -0.01500637]]. Action = [[ 0.16308111 -0.05155638  0.19390652 -0.14175284]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(4.8641e-06, grad_fn=<MseLossBackward0>)
Current timestep = 178. State = [[-0.2434284  -0.01428027]]. Action = [[-0.09772614 -0.16748533  0.01377445 -0.6326216 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 179. State = [[-0.2435694  -0.01396523]]. Action = [[-0.13690184 -0.03434303 -0.21734636  0.608598  ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(1.0621e-05, grad_fn=<MseLossBackward0>)
Current timestep = 180. State = [[-0.24367851 -0.01347752]]. Action = [[-0.20745732  0.12187093  0.01428294  0.05571628]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(7.2058e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of 1
Current timestep = 181. State = [[-0.24385647 -0.0129054 ]]. Action = [[-0.04776633 -0.168192    0.04222524  0.11732411]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(3.5265e-05, grad_fn=<MseLossBackward0>)
Current timestep = 182. State = [[-0.24396656 -0.01241837]]. Action = [[-0.06767042  0.14554727 -0.08425927  0.11421788]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
State prediction error at timestep 182 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 183. State = [[-0.24402657 -0.0122395 ]]. Action = [[-0.17457929 -0.08796147 -0.00375819 -0.6543242 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.24412067 -0.0121085 ]]. Action = [[-0.22683245 -0.01692022  0.20630583  0.76400185]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(3.3242e-05, grad_fn=<MseLossBackward0>)
Current timestep = 185. State = [[-0.24407586 -0.01199113]]. Action = [[ 0.06544828  0.22787657 -0.09680519  0.8338592 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(1.2723e-05, grad_fn=<MseLossBackward0>)
Current timestep = 186. State = [[-0.24413925 -0.01195105]]. Action = [[ 0.00531846 -0.14328207  0.0481385   0.40819013]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(1.1530e-06, grad_fn=<MseLossBackward0>)
Current timestep = 187. State = [[-0.24418946 -0.01160959]]. Action = [[ 0.11978874 -0.14521974  0.16051441  0.99029803]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(9.0754e-05, grad_fn=<MseLossBackward0>)
Current timestep = 188. State = [[-0.24423859 -0.01161752]]. Action = [[ 0.12729704 -0.14775966 -0.11078712 -0.3318081 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(7.5538e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of -1
Current timestep = 189. State = [[-0.2442529  -0.01156955]]. Action = [[ 0.13514847  0.19358265 -0.23177038  0.13175666]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 190. State = [[-0.24431397 -0.01145817]]. Action = [[-0.0074389  -0.13326395  0.0345293   0.89444137]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(3.9225e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of -1
Current timestep = 191. State = [[-0.24419534 -0.01208384]]. Action = [[ 0.24074465 -0.07647446  0.05805689  0.59211254]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(1.3983e-05, grad_fn=<MseLossBackward0>)
Current timestep = 192. State = [[-0.24407913 -0.01311829]]. Action = [[ 0.08867526  0.04982758  0.01120567 -0.95324314]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 193. State = [[-0.24393833 -0.01311594]]. Action = [[ 0.05175892 -0.06392822  0.0518887   0.66170526]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
State prediction error at timestep 193 is tensor(3.9168e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 193 of -1
Current timestep = 194. State = [[-0.2435588  -0.01350207]]. Action = [[ 0.1604262   0.16796851  0.19971573 -0.74272084]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
State prediction error at timestep 194 is tensor(7.5129e-05, grad_fn=<MseLossBackward0>)
Current timestep = 195. State = [[-0.24281761 -0.01408777]]. Action = [[-0.10573415 -0.10993016 -0.19767676 -0.9245976 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 195 of -1
Current timestep = 196. State = [[-0.2427069  -0.01539144]]. Action = [[-0.1432923  -0.10520233  0.1912243   0.6742997 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(9.0757e-05, grad_fn=<MseLossBackward0>)
Current timestep = 197. State = [[-0.24280067 -0.01622875]]. Action = [[ 0.21587509 -0.15284973 -0.14928709 -0.09116149]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(3.6225e-05, grad_fn=<MseLossBackward0>)
Current timestep = 198. State = [[-0.24288015 -0.01658364]]. Action = [[-0.04305226  0.21425134 -0.09441184 -0.6868247 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 199. State = [[-0.2429388  -0.01714719]]. Action = [[-0.24033114  0.06724507 -0.22133161 -0.37287784]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(6.0176e-06, grad_fn=<MseLossBackward0>)
Current timestep = 200. State = [[-0.2428785  -0.01769449]]. Action = [[ 0.09693334 -0.17533699  0.07859778  0.3945886 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(5.3962e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 200 of -1
Current timestep = 201. State = [[-0.242981  -0.0185127]]. Action = [[-0.00579128 -0.19538102  0.19863236 -0.5110748 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
State prediction error at timestep 201 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 202. State = [[-0.24301401 -0.01874162]]. Action = [[-0.10314485 -0.1867862  -0.18816994  0.31915462]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(2.8416e-06, grad_fn=<MseLossBackward0>)
Current timestep = 203. State = [[-0.24299772 -0.01909157]]. Action = [[-0.05202444  0.19464093  0.04833186  0.28743446]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(1.9513e-05, grad_fn=<MseLossBackward0>)
Current timestep = 204. State = [[-0.24304833 -0.01943896]]. Action = [[ 0.17625439  0.21007803 -0.20397525  0.861004  ]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(4.2101e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 204 of 1
Current timestep = 205. State = [[-0.24337782 -0.02120988]]. Action = [[-0.05953029 -0.02478799  0.01093549  0.07462859]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(6.6261e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of -1
Current timestep = 206. State = [[-0.2435682  -0.02264574]]. Action = [[-0.05068947  0.00400472 -0.00103284 -0.7874888 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 207. State = [[-0.24414042 -0.02357597]]. Action = [[ 0.10266703  0.07781014 -0.219182    0.73290837]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(6.0799e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 207 of -1
Current timestep = 208. State = [[-0.24413295 -0.02341006]]. Action = [[ 0.0257754   0.15568799 -0.1209062  -0.94658357]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 209. State = [[-0.24406327 -0.0231433 ]]. Action = [[-0.20292889 -0.03117119 -0.19598213 -0.969793  ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 210. State = [[-0.2440546  -0.02288465]]. Action = [[-0.12070312 -0.07115859  0.00070962 -0.37221998]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(3.3834e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[-0.24465056 -0.02411083]]. Action = [[ 0.06572768 -0.11014128 -0.06695646  0.49420965]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(9.8439e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 211 of -1
Current timestep = 212. State = [[-0.24464491 -0.02546742]]. Action = [[ 0.10836837 -0.22709075 -0.1755041  -0.40463817]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(3.7822e-05, grad_fn=<MseLossBackward0>)
Current timestep = 213. State = [[-0.24442875 -0.02609282]]. Action = [[ 0.17017925 -0.03941254  0.18553662 -0.02796447]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(8.3975e-05, grad_fn=<MseLossBackward0>)
Current timestep = 214. State = [[-0.24433301 -0.02661576]]. Action = [[ 0.19354033  0.03357458 -0.02178627 -0.3818394 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(9.9853e-05, grad_fn=<MseLossBackward0>)
Current timestep = 215. State = [[-0.24461152 -0.02692554]]. Action = [[-0.16066898  0.2230767   0.20221478 -0.8128633 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 215 of -1
Current timestep = 216. State = [[-0.24470866 -0.02801932]]. Action = [[-0.12304217  0.14232618  0.12141937  0.06497967]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(3.4057e-05, grad_fn=<MseLossBackward0>)
Current timestep = 217. State = [[-0.24471693 -0.02895867]]. Action = [[-0.18381895 -0.19065297 -0.11216599  0.08870566]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(4.3023e-05, grad_fn=<MseLossBackward0>)
Current timestep = 218. State = [[-0.24493292 -0.02925209]]. Action = [[-0.08431602  0.18033421  0.13530242 -0.21431881]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(5.5177e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of -1
Current timestep = 219. State = [[-0.24497332 -0.03030921]]. Action = [[-0.06415395  0.04315755 -0.10615581 -0.8282051 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 220. State = [[-0.24503982 -0.03030489]]. Action = [[ 0.03618827 -0.22478446  0.1000253   0.17987835]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 220 of -1
Current timestep = 221. State = [[-0.24515279 -0.03026643]]. Action = [[-0.18418762 -0.07326129  0.09705821 -0.7432909 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, True, False]
State prediction error at timestep 221 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 222. State = [[-0.24515279 -0.03026643]]. Action = [[-0.10750721 -0.22903453 -0.09325525  0.8035321 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 223. State = [[-0.24533963 -0.03030091]]. Action = [[-0.1433786   0.21207654 -0.00216256 -0.54232407]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
State prediction error at timestep 223 is tensor(9.1730e-05, grad_fn=<MseLossBackward0>)
Current timestep = 224. State = [[-0.24566196 -0.03005869]]. Action = [[-0.04979987  0.11629316 -0.15243946 -0.88142073]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 225. State = [[-0.24657951 -0.02742887]]. Action = [[-0.0929613   0.10925004  0.22194248  0.10050976]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(5.4271e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 225 of -1
Current timestep = 226. State = [[-0.24886005 -0.02332534]]. Action = [[-0.12833935 -0.11855501 -0.02788791  0.01924145]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(4.2285e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 226 of -1
Current timestep = 227. State = [[-0.25004324 -0.023441  ]]. Action = [[ 0.08340093 -0.21024111  0.18894672 -0.26677692]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
State prediction error at timestep 227 is tensor(4.7097e-05, grad_fn=<MseLossBackward0>)
Current timestep = 228. State = [[-0.25273815 -0.02356748]]. Action = [[-0.02095085 -0.06655845 -0.07828669 -0.8770306 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(5.6239e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 228 of -1
Current timestep = 229. State = [[-0.25378314 -0.02420434]]. Action = [[-0.19063728  0.07575923 -0.2377342   0.20399904]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 230. State = [[-0.2543197  -0.02463778]]. Action = [[ 0.16608894 -0.19394632 -0.0833405  -0.6459004 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(2.4629e-05, grad_fn=<MseLossBackward0>)
Current timestep = 231. State = [[-0.25665468 -0.02561339]]. Action = [[-0.00327718  0.01557323 -0.00178196 -0.08510154]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(3.5152e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of -1
Current timestep = 232. State = [[-0.25742802 -0.02586439]]. Action = [[-0.22155339  0.13810736  0.13421464 -0.16611171]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
State prediction error at timestep 232 is tensor(3.7796e-05, grad_fn=<MseLossBackward0>)
Current timestep = 233. State = [[-0.25794685 -0.02584601]]. Action = [[-0.07413948  0.20377606  0.22389734  0.6005411 ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, True, False]
State prediction error at timestep 233 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 234. State = [[-0.25918847 -0.02684738]]. Action = [[ 0.03245127 -0.12854104  0.10563844 -0.2121265 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(3.1122e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 234 of -1
Current timestep = 235. State = [[-0.25927106 -0.02781471]]. Action = [[ 0.02466705 -0.1520685  -0.10372847 -0.77924514]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(3.3240e-05, grad_fn=<MseLossBackward0>)
Current timestep = 236. State = [[-0.25933313 -0.02887631]]. Action = [[-0.2259964  -0.21232496 -0.00767325 -0.21791875]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, True, False]
State prediction error at timestep 236 is tensor(4.5732e-05, grad_fn=<MseLossBackward0>)
Current timestep = 237. State = [[-0.25962132 -0.02958734]]. Action = [[-0.17402422  0.15923804  0.05213287 -0.32998127]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(9.1047e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 237 of -1
Current timestep = 238. State = [[-0.2601965 -0.0302804]]. Action = [[ 0.15389022  0.22104716 -0.20197779 -0.32428467]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, True, False]
State prediction error at timestep 238 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 239. State = [[-0.2607813  -0.03315381]]. Action = [[ 0.04363742  0.05474454 -0.02673729 -0.4230855 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
State prediction error at timestep 239 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 240. State = [[-0.26093155 -0.0330154 ]]. Action = [[ 0.06260437  0.12921318 -0.15551338  0.9729253 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 240 of -1
Current timestep = 241. State = [[-0.2612728  -0.03047507]]. Action = [[ 0.01556316 -0.00060914 -0.03265482  0.23201632]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
State prediction error at timestep 241 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 241 of -1
Current timestep = 242. State = [[-0.26131427 -0.0299447 ]]. Action = [[ 0.23242924  0.12125513 -0.15238707 -0.07959688]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
State prediction error at timestep 242 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 243. State = [[-0.26157776 -0.02868447]]. Action = [[-0.03020133  0.12221345 -0.09793448 -0.50923383]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, False, True, False]
State prediction error at timestep 243 is tensor(9.7384e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 243 of -1
Current timestep = 244. State = [[-0.26246285 -0.02438879]]. Action = [[ 0.13171208  0.07880127  0.13741636 -0.6286291 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(4.7762e-05, grad_fn=<MseLossBackward0>)
Current timestep = 245. State = [[-0.26239726 -0.02310588]]. Action = [[-0.16365215  0.22627491 -0.20164926 -0.92094475]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, False, True, False]
State prediction error at timestep 245 is tensor(1.4788e-05, grad_fn=<MseLossBackward0>)
Current timestep = 246. State = [[-0.261969   -0.02180034]]. Action = [[-0.17994477 -0.00540677 -0.04587397 -0.27298486]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, False, True, False]
State prediction error at timestep 246 is tensor(7.1690e-05, grad_fn=<MseLossBackward0>)
Current timestep = 247. State = [[-0.26157585 -0.02052479]]. Action = [[-0.12732746  0.13418883 -0.05799517  0.29562366]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
State prediction error at timestep 247 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 248. State = [[-0.26160997 -0.01990493]]. Action = [[ 0.01400581 -0.21373144  0.21007863 -0.9668727 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(2.0505e-05, grad_fn=<MseLossBackward0>)
Current timestep = 249. State = [[-0.2616268  -0.01883543]]. Action = [[-0.15284997  0.2104029  -0.14510135 -0.41335082]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
State prediction error at timestep 249 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 250. State = [[-0.26155472 -0.01801843]]. Action = [[ 0.09930778 -0.18020815 -0.08025098 -0.41809767]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, True, False]
State prediction error at timestep 250 is tensor(2.0565e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 250 of -1
Current timestep = 251. State = [[-0.26156312 -0.01717734]]. Action = [[-0.23727815 -0.10756879  0.21904057  0.89907694]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, True, False]
State prediction error at timestep 251 is tensor(6.0126e-05, grad_fn=<MseLossBackward0>)
Current timestep = 252. State = [[-0.26156402 -0.01664256]]. Action = [[-0.18210211 -0.14188647  0.15421551 -0.48514986]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(1.2181e-05, grad_fn=<MseLossBackward0>)
Current timestep = 253. State = [[-0.261543   -0.01615396]]. Action = [[-0.19012251 -0.23969911  0.11458358 -0.21891618]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, True, False]
State prediction error at timestep 253 is tensor(5.7661e-06, grad_fn=<MseLossBackward0>)
Current timestep = 254. State = [[-0.26153803 -0.01566071]]. Action = [[-0.2072068  -0.06708655  0.21786481  0.6945746 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
State prediction error at timestep 254 is tensor(4.0248e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 254 of -1
Current timestep = 255. State = [[-0.26159847 -0.01531734]]. Action = [[ 0.23174995  0.10945004 -0.14817117  0.6357484 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, True, False]
State prediction error at timestep 255 is tensor(2.2954e-06, grad_fn=<MseLossBackward0>)
Current timestep = 256. State = [[-0.2616425  -0.01517869]]. Action = [[-0.22406349 -0.19702353  0.07464808 -0.5200771 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(8.7100e-06, grad_fn=<MseLossBackward0>)
Current timestep = 257. State = [[-0.2616355  -0.01485972]]. Action = [[-0.14892538  0.19574356  0.19381708 -0.6565091 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(3.5383e-05, grad_fn=<MseLossBackward0>)
Current timestep = 258. State = [[-0.26164177 -0.01466877]]. Action = [[ 0.1685242  -0.14582995 -0.08215302 -0.74977136]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
State prediction error at timestep 258 is tensor(8.3518e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 258 of -1
Current timestep = 259. State = [[-0.26159582 -0.01415159]]. Action = [[ 0.02938271  0.02942821 -0.17298244  0.4369005 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
State prediction error at timestep 259 is tensor(2.7508e-05, grad_fn=<MseLossBackward0>)
Current timestep = 260. State = [[-0.2616757  -0.01394793]]. Action = [[-0.00811291  0.21652299  0.07440442  0.3221928 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 261. State = [[-0.26099348 -0.01301482]]. Action = [[-0.0043001   0.05875826  0.09637743  0.0954808 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, True, False]
State prediction error at timestep 261 is tensor(7.5935e-05, grad_fn=<MseLossBackward0>)
Current timestep = 262. State = [[-0.26089945 -0.01220775]]. Action = [[ 0.1830666  -0.17194429  0.06079268  0.7016262 ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, True, False]
State prediction error at timestep 262 is tensor(1.4800e-06, grad_fn=<MseLossBackward0>)
Current timestep = 263. State = [[-0.26100916 -0.01167831]]. Action = [[-0.19588257  0.07995415 -0.1646079   0.01630104]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 264. State = [[-0.26103398 -0.01126845]]. Action = [[-0.24305177 -0.08438686  0.18257242 -0.82947624]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, True, False]
State prediction error at timestep 264 is tensor(7.9403e-06, grad_fn=<MseLossBackward0>)
Current timestep = 265. State = [[-0.26053542 -0.00980684]]. Action = [[-0.0335965  -0.11307819  0.17280921  0.9510479 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(3.2341e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 265 of -1
Current timestep = 266. State = [[-0.26049986 -0.01001592]]. Action = [[-0.16667908  0.15719461  0.13131523  0.7427962 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(3.4354e-05, grad_fn=<MseLossBackward0>)
Current timestep = 267. State = [[-0.26040038 -0.01022903]]. Action = [[ 0.1437498   0.20355976 -0.20753905 -0.8381848 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
State prediction error at timestep 267 is tensor(6.2062e-05, grad_fn=<MseLossBackward0>)
Current timestep = 268. State = [[-0.25991818 -0.01032505]]. Action = [[0.13049969 0.10319284 0.10168278 0.19420803]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
State prediction error at timestep 268 is tensor(4.9639e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 268 of -1
Current timestep = 269. State = [[-0.25946218 -0.00998806]]. Action = [[-0.17299412 -0.01919463 -0.17583111 -0.5285316 ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(4.2195e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[-0.25873587 -0.0094529 ]]. Action = [[ 0.19039118 -0.13366243  0.17101869 -0.24823695]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
State prediction error at timestep 270 is tensor(8.6799e-06, grad_fn=<MseLossBackward0>)
Current timestep = 271. State = [[-0.25619736 -0.00897605]]. Action = [[-0.07422408  0.00674599  0.11383218 -0.02458113]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
State prediction error at timestep 271 is tensor(6.3084e-05, grad_fn=<MseLossBackward0>)
Current timestep = 272. State = [[-0.25574154 -0.0084524 ]]. Action = [[-0.07206774 -0.12756939 -0.02217487 -0.96689945]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
State prediction error at timestep 272 is tensor(3.3688e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 272 of -1
Current timestep = 273. State = [[-0.2557727  -0.00996632]]. Action = [[-0.02014486 -0.09818502  0.188849    0.8446598 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
State prediction error at timestep 273 is tensor(1.3394e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 273 of -1
Current timestep = 274. State = [[-0.25571495 -0.01149822]]. Action = [[-0.05858594 -0.20857342  0.2370269  -0.53701615]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(2.0167e-05, grad_fn=<MseLossBackward0>)
Current timestep = 275. State = [[-0.2559188  -0.01346876]]. Action = [[-0.00926125  0.1329717   0.15478462 -0.66549736]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(4.4499e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 275 of 1
Current timestep = 276. State = [[-0.25600103 -0.01271386]]. Action = [[-0.1990669   0.19261873  0.06542856 -0.7400085 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(1.8116e-05, grad_fn=<MseLossBackward0>)
Current timestep = 277. State = [[-0.2561737  -0.01196032]]. Action = [[ 0.12507325  0.09486684  0.13263977 -0.81169033]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, True, False]
State prediction error at timestep 277 is tensor(1.5921e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 277 of 1
Current timestep = 278. State = [[-0.2561391  -0.01110469]]. Action = [[ 0.20951432  0.13603961 -0.22412531  0.3634659 ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(1.4442e-05, grad_fn=<MseLossBackward0>)
Current timestep = 279. State = [[-0.25599226 -0.009399  ]]. Action = [[-0.09502372 -0.10259703 -0.14616878 -0.9709169 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, True, False]
State prediction error at timestep 279 is tensor(3.2964e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 279 of 1
Current timestep = 280. State = [[-0.2559627  -0.00986842]]. Action = [[ 0.11304611 -0.05152169 -0.24794194 -0.3338737 ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, True, False]
State prediction error at timestep 280 is tensor(5.1894e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 280 of 1
Current timestep = 281. State = [[-0.25497332 -0.01090664]]. Action = [[-0.10528742  0.12888172 -0.21054226  0.36317015]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(5.1968e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 281 of 1
Current timestep = 282. State = [[-0.2548841  -0.01024902]]. Action = [[ 0.13350606 -0.16209356  0.00361249  0.06531   ]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, True, False]
State prediction error at timestep 282 is tensor(5.0757e-05, grad_fn=<MseLossBackward0>)
Current timestep = 283. State = [[-0.25528452 -0.00894959]]. Action = [[ 0.01910806  0.0227406  -0.10624251 -0.31829834]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(5.0463e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 283 of 1
Current timestep = 284. State = [[-0.25552565 -0.00860788]]. Action = [[-0.18185034 -0.23005734  0.13267362  0.6806989 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(1.9095e-06, grad_fn=<MseLossBackward0>)
Current timestep = 285. State = [[-0.25548944 -0.00842577]]. Action = [[-0.15784314  0.19359457 -0.14579278  0.74665236]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, True, False]
State prediction error at timestep 285 is tensor(3.9075e-05, grad_fn=<MseLossBackward0>)
Current timestep = 286. State = [[-0.25559264 -0.00777837]]. Action = [[ 0.13060099  0.09153396 -0.09358391 -0.04325336]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, True, False]
State prediction error at timestep 286 is tensor(7.9741e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 286 of 1
Current timestep = 287. State = [[-0.2554428  -0.00734221]]. Action = [[-0.03379983  0.22332332 -0.23543869  0.8691735 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(5.1969e-05, grad_fn=<MseLossBackward0>)
Current timestep = 288. State = [[-0.253722   -0.00456196]]. Action = [[-0.03205198  0.09876999  0.2411955  -0.81303245]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
State prediction error at timestep 288 is tensor(6.6644e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 288 of 1
Current timestep = 289. State = [[-0.2536246  -0.00327342]]. Action = [[ 0.05472845 -0.16660686  0.01382929  0.92266273]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
State prediction error at timestep 289 is tensor(3.6967e-05, grad_fn=<MseLossBackward0>)
Current timestep = 290. State = [[-0.25380298 -0.00226435]]. Action = [[-0.07106692 -0.17951593 -0.06488732  0.32077396]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(8.2332e-05, grad_fn=<MseLossBackward0>)
Current timestep = 291. State = [[-0.2538559  -0.00144835]]. Action = [[ 0.03199151  0.24668711  0.09897181 -0.2935629 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(8.4468e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 291 of 1
Current timestep = 292. State = [[-0.25369957 -0.00053861]]. Action = [[ 0.01076454  0.22037897 -0.19108726  0.6785159 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 293. State = [[-0.25403437  0.000681  ]]. Action = [[ 0.20783362  0.09412238 -0.23712528  0.01901662]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 294. State = [[-0.25433898  0.00287663]]. Action = [[ 0.05323136  0.05833447  0.02140105 -0.12729758]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, True, False]
State prediction error at timestep 294 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 294 of 1
Current timestep = 295. State = [[-0.25374022  0.0042291 ]]. Action = [[-0.22719385  0.20448709 -0.21152054 -0.5033418 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, True, False]
State prediction error at timestep 295 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 296. State = [[-0.2536627   0.00461681]]. Action = [[-0.1996912  -0.04255933 -0.16007847 -0.01387906]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, True, False]
State prediction error at timestep 296 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 297. State = [[-0.25366226  0.00499204]]. Action = [[ 0.14918795  0.02106643 -0.20740207  0.3431039 ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 298. State = [[-0.25345775  0.0054627 ]]. Action = [[-0.07466973  0.13417181 -0.15598947 -0.7765383 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 298 of 1
Current timestep = 299. State = [[-0.25328767  0.0062365 ]]. Action = [[-0.00053808 -0.22841917 -0.19659077 -0.2136882 ]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 300. State = [[-0.25322866  0.00687328]]. Action = [[-0.13208047  0.14584878  0.01668993 -0.79754996]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, True, False]
State prediction error at timestep 300 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 301. State = [[-0.2531209   0.00731904]]. Action = [[ 0.21512204 -0.21643302  0.22810248 -0.882808  ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, True, False]
State prediction error at timestep 301 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 301 of 1
Current timestep = 302. State = [[-0.25316235  0.00758839]]. Action = [[ 0.13885921 -0.10088944 -0.23237328  0.31793618]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 303. State = [[-0.2529745   0.00801926]]. Action = [[-0.13401613  0.02640563  0.21803117 -0.8517758 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, True, False]
State prediction error at timestep 303 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 304. State = [[-0.25290397  0.00821193]]. Action = [[-0.21843755  0.11962965 -0.03535201  0.56238794]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, True, False]
State prediction error at timestep 304 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 305. State = [[-0.2530706   0.00826221]]. Action = [[ 0.04119572 -0.16366887 -0.18745264 -0.5742913 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of -1
Current timestep = 306. State = [[-0.25280926  0.0085947 ]]. Action = [[ 0.24190396  0.23425657 -0.21828751  0.53845024]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, True, False]
State prediction error at timestep 306 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 307. State = [[-0.25288367  0.00863057]]. Action = [[ 0.1485663  -0.0277748   0.03112811  0.33375227]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, True, False]
State prediction error at timestep 307 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 308. State = [[-0.2525976   0.00936743]]. Action = [[-0.07117996  0.09374091 -0.10433865  0.25524032]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, True, False]
State prediction error at timestep 308 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 308 of -1
Current timestep = 309. State = [[-0.25336355  0.01203517]]. Action = [[ 0.1302608  -0.03259619  0.20269579 -0.723987  ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, True, False]
State prediction error at timestep 309 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 309 of -1
Current timestep = 310. State = [[-0.25288594  0.01257244]]. Action = [[-0.1649692   0.18966547  0.23544401  0.8628293 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, True, False]
State prediction error at timestep 310 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 311. State = [[-0.25123262  0.01320854]]. Action = [[-0.08494124 -0.09630966  0.22812566 -0.37064463]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 311 of -1
Current timestep = 312. State = [[-0.251249    0.01295593]]. Action = [[ 0.20701426  0.09160137 -0.06370464 -0.8741612 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 313. State = [[-0.251636    0.01271258]]. Action = [[-0.20422277 -0.00242187 -0.24826139 -0.61238253]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 314. State = [[-0.25166342  0.01273608]]. Action = [[-0.18837455  0.15382981 -0.10616517  0.67757773]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
State prediction error at timestep 314 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 315. State = [[-0.25131848  0.01301414]]. Action = [[ 0.20167094  0.1359343  -0.12818605 -0.02378052]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 316. State = [[-0.25092486  0.01316718]]. Action = [[ 0.11825019  0.03750351 -0.2445665   0.29618526]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
State prediction error at timestep 316 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 317. State = [[-0.24916455  0.01368645]]. Action = [[-0.03858919  0.03803688 -0.07329276  0.5409658 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 317 of 1
Current timestep = 318. State = [[-0.24883102  0.01384006]]. Action = [[-0.1497147   0.05883414 -0.1941702   0.36673093]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 319. State = [[-0.24864636  0.01416073]]. Action = [[-0.0517154   0.07183811 -0.0595701  -0.5502315 ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, True, False]
State prediction error at timestep 319 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 319 of 1
Current timestep = 320. State = [[-0.24909146  0.01474   ]]. Action = [[-0.15960293  0.01177734 -0.06003308  0.26475692]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, True, False]
State prediction error at timestep 320 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 321. State = [[-0.24904525  0.01552102]]. Action = [[ 0.1735326   0.21890914 -0.13921663 -0.9697515 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, True, False]
State prediction error at timestep 321 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 321 of 1
Current timestep = 322. State = [[-0.24909113  0.01591483]]. Action = [[ 0.19859326 -0.11087725  0.11638004  0.80205846]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, True, False]
State prediction error at timestep 322 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 323. State = [[-0.24946709  0.01635754]]. Action = [[-0.17013645 -0.09834532  0.04993534 -0.80445296]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, True, False]
State prediction error at timestep 323 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 324. State = [[-0.24954335  0.01694838]]. Action = [[ 0.18305174 -0.07399589 -0.09460458  0.49999607]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 325. State = [[-0.24955592  0.01724774]]. Action = [[ 2.0042062e-04  1.8693745e-01 -1.7393818e-01  3.9501691e-01]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, True, False]
State prediction error at timestep 325 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 325 of 1
Current timestep = 326. State = [[-0.24978489  0.0175641 ]]. Action = [[-0.23476663 -0.21822138 -0.18287522 -0.5507086 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 327. State = [[-0.25001836  0.01829418]]. Action = [[-0.1282572   0.04963115  0.1771838   0.95516634]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 328. State = [[-0.25029594  0.0189282 ]]. Action = [[ 0.0972538  -0.1655302  -0.22798747  0.59119415]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, True, False]
State prediction error at timestep 328 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 329. State = [[-0.25119853  0.0213595 ]]. Action = [[ 0.05328274  0.06363761 -0.02561101 -0.9377849 ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, True, False]
State prediction error at timestep 329 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 330. State = [[-0.2515734   0.02241794]]. Action = [[-0.21107122 -0.2403958   0.07825425 -0.858125  ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, True, False]
State prediction error at timestep 330 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 331. State = [[-0.2517239   0.02296017]]. Action = [[ 0.15903214 -0.14210546 -0.23267668 -0.7178079 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 332. State = [[-0.2518737  0.0235044]]. Action = [[-0.08267617 -0.19400927 -0.14290585 -0.39528143]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, True, False]
State prediction error at timestep 332 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 333. State = [[-0.25241494  0.02548169]]. Action = [[ 0.07380921 -0.0378833   0.03072935 -0.23805273]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 334. State = [[-0.25200516  0.02538751]]. Action = [[-0.07486363 -0.11947197 -0.19284575  0.83074045]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, True, False]
State prediction error at timestep 334 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 335. State = [[-0.2519138   0.02497923]]. Action = [[ 0.07178026 -0.18696952  0.19322109  0.7467971 ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, True, False]
State prediction error at timestep 335 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 336. State = [[-0.251694    0.02451408]]. Action = [[ 0.20136702 -0.06572825 -0.13353835  0.4837649 ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 337. State = [[-0.25155568  0.02427348]]. Action = [[-0.10937372  0.19593844  0.20589924 -0.24192989]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, True, False]
State prediction error at timestep 337 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 338. State = [[-0.2516123   0.02408548]]. Action = [[-0.23305704 -0.19416185  0.10052842  0.36644244]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, True, False]
State prediction error at timestep 338 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 339. State = [[-0.2516621   0.02298249]]. Action = [[-0.08395258  0.05233297 -0.12072572 -0.9596108 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, True, False]
State prediction error at timestep 339 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 339 of 1
Current timestep = 340. State = [[-0.25168777  0.02316152]]. Action = [[-0.15660477  0.18418175 -0.0435576  -0.4779526 ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, True, False]
State prediction error at timestep 340 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 341. State = [[-0.2517622  0.023347 ]]. Action = [[ 0.09755474 -0.03644918 -0.11170733  0.8470764 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 341 of 1
Current timestep = 342. State = [[-0.25154284  0.02296079]]. Action = [[ 0.09950939 -0.03599036  0.01999661 -0.5145771 ]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 343. State = [[-0.2507459   0.02184567]]. Action = [[-0.10508072  0.00678229  0.19401407 -0.8428534 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 344. State = [[-0.2507469   0.02161999]]. Action = [[-0.10949337  0.2202071  -0.15118521 -0.24917376]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 344 of 1
Current timestep = 345. State = [[-0.2506173   0.02147665]]. Action = [[ 0.20560491  0.0725854  -0.16747026  0.1990403 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, True, False]
State prediction error at timestep 345 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 346. State = [[-0.25062358  0.02135847]]. Action = [[-0.06846806  0.17461598 -0.04154624 -0.59039485]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, True, False]
State prediction error at timestep 346 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 347. State = [[-0.25070044  0.02097142]]. Action = [[-0.07524049  0.00437477 -0.10663798  0.16486454]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, True, False]
State prediction error at timestep 347 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 347 of 1
Current timestep = 348. State = [[-0.250859    0.02080044]]. Action = [[ 0.09288469  0.07755357 -0.04243335 -0.9067954 ]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, True, False]
State prediction error at timestep 348 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 349. State = [[-0.25105503  0.02113997]]. Action = [[ 0.22688323  0.07541847 -0.23182821  0.38115013]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, True, False]
State prediction error at timestep 349 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 350. State = [[-0.2511752   0.02152802]]. Action = [[ 1.6252500e-01  1.7545524e-01 -3.2608211e-04  8.7427831e-01]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, True, False]
State prediction error at timestep 350 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 351. State = [[-0.251201    0.02174145]]. Action = [[ 0.11352706  0.24588299  0.02485883 -0.44431913]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, True, False]
State prediction error at timestep 351 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 352. State = [[-0.25132862  0.0221869 ]]. Action = [[-0.00388698  0.10457486 -0.2402258   0.09675872]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, True, False]
State prediction error at timestep 352 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 352 of 1
Current timestep = 353. State = [[-0.25208995  0.0247272 ]]. Action = [[-0.0936815  -0.00209723  0.08358675  0.92282844]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 354. State = [[-0.2522619   0.02537286]]. Action = [[-0.19461416  0.07042944  0.14268905 -0.22013271]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, True, False]
State prediction error at timestep 354 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 355. State = [[-0.25248533  0.0259087 ]]. Action = [[-0.22335806 -0.12765642  0.02777949  0.9467447 ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 356. State = [[-0.25266984  0.02622403]]. Action = [[0.2106052  0.21899402 0.03080925 0.0008136 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, True, False]
State prediction error at timestep 356 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 357. State = [[-0.25286224  0.02668137]]. Action = [[0.17764556 0.056833   0.1799838  0.8344462 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, True, False]
State prediction error at timestep 357 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 358. State = [[-0.2534038   0.02814553]]. Action = [[-0.02881081 -0.12328842 -0.02144834 -0.33975732]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 358 of 1
Current timestep = 359. State = [[-0.25352243  0.02707352]]. Action = [[-0.02229144  0.13120526  0.15134066 -0.19023561]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, True, False]
State prediction error at timestep 359 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 359 of 1
Current timestep = 360. State = [[-0.25372747  0.02765994]]. Action = [[-0.16490386  0.07350588 -0.13501292  0.27248514]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, True, False]
State prediction error at timestep 360 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 361. State = [[-0.2539081   0.02818611]]. Action = [[-0.00197637 -0.14700046  0.04362196 -0.9151432 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 362. State = [[-0.25402185  0.02861947]]. Action = [[-0.00705339 -0.23793793 -0.08014089 -0.4632945 ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, True, False]
State prediction error at timestep 362 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 362 of -1
Current timestep = 363. State = [[-0.2546573   0.02979443]]. Action = [[-0.05285031 -0.07936509 -0.0647532   0.50132596]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, True, False]
State prediction error at timestep 363 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 364. State = [[-0.2554911   0.02937283]]. Action = [[ 0.0039334  -0.03502586 -0.10494161  0.7747098 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, True, False]
State prediction error at timestep 364 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 365. State = [[-0.25595808  0.02836329]]. Action = [[ 0.00690058  0.05857855  0.11626995 -0.49883008]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, True, False]
State prediction error at timestep 365 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 365 of -1
Current timestep = 366. State = [[-0.25622442  0.02877394]]. Action = [[-0.04099616  0.06509739 -0.03610189  0.74033403]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, True, False]
State prediction error at timestep 366 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 366 of -1
Current timestep = 367. State = [[-0.25720814  0.03034944]]. Action = [[-0.22585459  0.23148966 -0.18554778  0.30463052]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, True, False]
State prediction error at timestep 367 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 368. State = [[-0.2573081   0.03066621]]. Action = [[ 0.20533052 -0.15699074  0.15230614 -0.2700174 ]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of -1
Current timestep = 369. State = [[-0.2577715   0.03108559]]. Action = [[-0.15887156  0.09428215  0.05184981  0.8517952 ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, True, False]
State prediction error at timestep 369 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 370. State = [[-0.25810122  0.03198748]]. Action = [[ 0.0556457   0.11031285 -0.1903702   0.04219556]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, True, False]
State prediction error at timestep 370 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 371. State = [[-0.2583929   0.03290626]]. Action = [[ 0.24039328  0.15157187  0.0875068  -0.78183204]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, True, False]
State prediction error at timestep 371 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 372. State = [[-0.25871253  0.03388359]]. Action = [[ 0.14777434  0.20032915 -0.12129907  0.17913425]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, True, False]
State prediction error at timestep 372 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 373. State = [[-0.25900283  0.03445728]]. Action = [[-0.09661806 -0.19469874  0.2471177  -0.25890177]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, False, True, False]
State prediction error at timestep 373 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 374. State = [[-0.2593198   0.03516882]]. Action = [[-0.20126365 -0.04596628 -0.20818591 -0.80748516]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, False, True, False]
State prediction error at timestep 374 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 375. State = [[-0.2595225  0.035855 ]]. Action = [[ 0.04091763  0.21325368 -0.06643119  0.04072773]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, False, True, False]
State prediction error at timestep 375 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 376. State = [[-0.259721    0.03632911]]. Action = [[-0.01356237 -0.14680317  0.05438673  0.48731196]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, False, True, False]
State prediction error at timestep 376 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 376 of -1
Current timestep = 377. State = [[-0.2599292   0.03676227]]. Action = [[-0.08561975  0.14071718 -0.09243873 -0.6775459 ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, False, True, False]
State prediction error at timestep 377 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 378. State = [[-0.26001978  0.03711182]]. Action = [[-0.13465172 -0.23054244 -0.18483952 -0.7399166 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, False, True, False]
State prediction error at timestep 378 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 379. State = [[-0.260181    0.03749361]]. Action = [[-0.051688   -0.20216295  0.0776242   0.71224153]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, False, True, False]
State prediction error at timestep 379 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 380. State = [[-0.26032144  0.03769643]]. Action = [[0.16930026 0.15363598 0.2070263  0.49888206]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, False, True, False]
State prediction error at timestep 380 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 381. State = [[-0.2603966   0.03799815]]. Action = [[0.20528686 0.20427504 0.24284837 0.61170053]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, False, True, False]
State prediction error at timestep 381 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 382. State = [[-0.26040864  0.03803064]]. Action = [[-0.18003573 -0.06778143  0.10150701  0.7400656 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, False, True, False]
State prediction error at timestep 382 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 382 of -1
Current timestep = 383. State = [[-0.2605748  0.0385887]]. Action = [[ 0.06477475  0.06606826  0.20514917 -0.39250493]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, False, True, False]
State prediction error at timestep 383 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 383 of -1
Current timestep = 384. State = [[-0.26104456  0.04023413]]. Action = [[-0.13085742  0.03281605 -0.15862733 -0.06617743]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, False, True, False]
State prediction error at timestep 384 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 385. State = [[-0.26130593  0.04090367]]. Action = [[-0.21026438 -0.0707895  -0.03267881  0.87779534]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, False, True, False]
State prediction error at timestep 385 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 386. State = [[-0.26166326  0.04172834]]. Action = [[ 0.19627726  0.17177206 -0.17678545  0.08083427]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, False, True, False]
State prediction error at timestep 386 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.2628034   0.04449304]]. Action = [[-0.10728186 -0.01864892  0.23211867 -0.859575  ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, False, True, False]
State prediction error at timestep 387 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 388. State = [[-0.26319924  0.04489546]]. Action = [[-0.15092073  0.02704835  0.21531522  0.9409683 ]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, False, True, False]
State prediction error at timestep 388 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 389. State = [[-0.26344103  0.04516356]]. Action = [[ 0.19988027  0.10371751 -0.01968962  0.03019214]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, False, True, False]
State prediction error at timestep 389 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 390. State = [[-0.26445848  0.04634287]]. Action = [[-0.04019745 -0.02164932 -0.18159187 -0.7273726 ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, False, True, False]
State prediction error at timestep 390 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 390 of -1
Current timestep = 391. State = [[-0.26576924  0.04677276]]. Action = [[ 0.06302029  0.09467942 -0.24450947 -0.83967376]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, False, True, False]
State prediction error at timestep 391 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 391 of -1
Current timestep = 392. State = [[-0.26678717  0.04877645]]. Action = [[-0.13199684 -0.12259239  0.11065626  0.36243033]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, False, True, False]
State prediction error at timestep 392 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 393. State = [[-0.26843402  0.04735312]]. Action = [[ 0.04325664 -0.08424123  0.02664787 -0.9200986 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, False, True, False]
State prediction error at timestep 393 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 393 of -1
Current timestep = 394. State = [[-0.2684303   0.04639113]]. Action = [[-0.18365596 -0.06277226  0.19070882 -0.86368597]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, False, True, False]
State prediction error at timestep 394 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 395. State = [[-0.26880562  0.04463894]]. Action = [[-0.11016351  0.012642   -0.13982439 -0.9184529 ]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, False, True, False]
State prediction error at timestep 395 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 395 of -1
Current timestep = 396. State = [[-0.2702475  0.0433111]]. Action = [[-0.08166286 -0.05067044 -0.07120156 -0.85669225]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, False, True, False]
State prediction error at timestep 396 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 396 of -1
Current timestep = 397. State = [[-0.2710005   0.04258698]]. Action = [[-0.19624454 -0.21499631 -0.22222279 -0.01082891]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, False, True, False]
State prediction error at timestep 397 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 398. State = [[-0.27188385  0.04205018]]. Action = [[ 0.09228131  0.19456682 -0.17523961  0.512352  ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, False, True, False]
State prediction error at timestep 398 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 399. State = [[-0.272349   0.0415973]]. Action = [[ 0.00753856 -0.15392052 -0.1454468   0.09348226]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, False, True, False]
State prediction error at timestep 399 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 399 of -1
Current timestep = 400. State = [[-0.2728109   0.04110148]]. Action = [[-0.21450625  0.06575617  0.08095908 -0.41739023]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, False, True, False]
State prediction error at timestep 400 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 401. State = [[-0.2732139   0.04078482]]. Action = [[ 0.15984654 -0.16794963  0.18138736 -0.7358724 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, False, True, False]
State prediction error at timestep 401 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 402. State = [[-0.27379268  0.04007567]]. Action = [[ 0.06294888  0.2445488   0.18238372 -0.3289138 ]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, False, True, False]
State prediction error at timestep 402 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 403. State = [[-0.27411637  0.03976929]]. Action = [[ 0.05162844  0.22951958  0.1387738  -0.51165634]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, False, True, False]
State prediction error at timestep 403 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 404. State = [[-0.27460244  0.03936599]]. Action = [[-0.16857135  0.00422621 -0.209978    0.9747181 ]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, False, True, False]
State prediction error at timestep 404 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 404 of -1
Current timestep = 405. State = [[-0.27565807  0.03846787]]. Action = [[ 0.10796189  0.1199196   0.24080843 -0.7185182 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, False, True, False]
State prediction error at timestep 405 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 406. State = [[-0.27654874  0.04068506]]. Action = [[-0.1007445  -0.00532448  0.20794022  0.37826395]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, False, True, False]
State prediction error at timestep 406 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 407. State = [[-0.27688226  0.04149701]]. Action = [[-0.16306585 -0.02175084 -0.18297635 -0.32420456]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, False, True, False]
State prediction error at timestep 407 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 408. State = [[-0.2770207   0.04174948]]. Action = [[-0.22295243  0.15915415 -0.07855925  0.86455894]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, False, True, False]
State prediction error at timestep 408 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 409. State = [[-0.27741596  0.04217461]]. Action = [[-0.22130772  0.09289104 -0.14209148  0.287099  ]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, False, True, False]
State prediction error at timestep 409 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 410. State = [[-0.27773646  0.04287969]]. Action = [[ 0.0496285   0.16472757 -0.10384041  0.06300747]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, False, True, False]
State prediction error at timestep 410 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 411. State = [[-0.2780735   0.04311305]]. Action = [[ 0.17310017  0.06647301 -0.12163663  0.3068254 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, False, True, False]
State prediction error at timestep 411 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 411 of -1
Current timestep = 412. State = [[-0.27817473  0.04350528]]. Action = [[ 0.24292636 -0.0486266   0.14083982 -0.54163074]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, False, True, False]
State prediction error at timestep 412 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 413. State = [[-0.27841774  0.04380257]]. Action = [[ 0.20772272 -0.19282193  0.06937593 -0.81955636]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, False, True, False]
State prediction error at timestep 413 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 414. State = [[-0.27862418  0.04398894]]. Action = [[-0.2247675  -0.0251087  -0.1869755  -0.07871556]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, False, True, False]
State prediction error at timestep 414 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 415. State = [[-0.27867407  0.04420519]]. Action = [[-0.1692165   0.19681907 -0.24304797 -0.6589024 ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 416. State = [[-0.27875394  0.04432362]]. Action = [[ 0.13373822 -0.22587289 -0.08180678 -0.83407015]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, False, True, False]
State prediction error at timestep 416 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 416 of -1
Current timestep = 417. State = [[-0.2788776   0.04435525]]. Action = [[ 0.1562221   0.15461981  0.0130322  -0.9275966 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, False, True, False]
State prediction error at timestep 417 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 418. State = [[-0.278974    0.04463521]]. Action = [[-0.1142763  -0.22915992 -0.15335165 -0.7971869 ]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, False, True, False]
State prediction error at timestep 418 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 419. State = [[-0.27910063  0.04464485]]. Action = [[ 0.1858635   0.1489746  -0.02874751 -0.46388614]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 420. State = [[-0.27927735  0.04481187]]. Action = [[-0.11249535 -0.07580438 -0.05625625 -0.5204787 ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, False, True, False]
State prediction error at timestep 420 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 420 of -1
Current timestep = 421. State = [[-0.27968118  0.044238  ]]. Action = [[-0.05165097 -0.2318984   0.13924477 -0.73712814]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, False, True, False]
State prediction error at timestep 421 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 422. State = [[-0.2800666   0.04383194]]. Action = [[ 0.19559532 -0.0480777   0.05635679  0.66324544]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 422 of -1
Current timestep = 423. State = [[-0.28032765  0.04337391]]. Action = [[ 0.00777125 -0.20687203 -0.20007241 -0.26915443]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 423 is [True, False, False, False, True, False]
State prediction error at timestep 423 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 424. State = [[-0.2807346  0.0429708]]. Action = [[ 0.22113663  0.06904444  0.01856726 -0.5089893 ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 424 is [True, False, False, False, True, False]
State prediction error at timestep 424 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 425. State = [[-0.28103662  0.04262763]]. Action = [[-0.17498718 -0.00995322 -0.06194013 -0.8462659 ]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 425 is [True, False, False, False, True, False]
State prediction error at timestep 425 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 426. State = [[-0.28145877  0.04244056]]. Action = [[-0.17908756  0.18466857 -0.20239992  0.8282349 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 426 is [True, False, False, False, True, False]
State prediction error at timestep 426 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 426 of -1
Current timestep = 427. State = [[-0.28178     0.04225738]]. Action = [[-0.23557764 -0.02231523  0.16257262  0.05281389]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 427 is [True, False, False, False, True, False]
State prediction error at timestep 427 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 428. State = [[-0.28341606  0.04141343]]. Action = [[0.06760356 0.01791283 0.06264132 0.390316  ]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 428 is [True, False, False, False, True, False]
State prediction error at timestep 428 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 429. State = [[-0.28333548  0.0413872 ]]. Action = [[ 0.23920518  0.10567006 -0.1401101   0.0781759 ]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 429 is [True, False, False, False, True, False]
State prediction error at timestep 429 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 430. State = [[-0.28339267  0.0413828 ]]. Action = [[ 0.08717558 -0.05862141  0.13487947  0.21598911]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 430 of -1
Current timestep = 431. State = [[-0.28338185  0.04099398]]. Action = [[-0.14242192 -0.12559596  0.00422004 -0.70878565]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 431 is [True, False, False, False, True, False]
State prediction error at timestep 431 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 432. State = [[-0.28331682  0.0405178 ]]. Action = [[0.03448626 0.21332943 0.2177903  0.0026623 ]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 433. State = [[-0.28329954  0.04024414]]. Action = [[ 0.20271534  0.23391342 -0.15937631 -0.2296021 ]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 433 is [True, False, False, False, True, False]
State prediction error at timestep 433 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 434. State = [[-0.28334445  0.03981545]]. Action = [[ 0.09966952  0.16857234 -0.04207112 -0.16051096]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 434 is [True, False, False, False, True, False]
State prediction error at timestep 434 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 434 of -1
Current timestep = 435. State = [[-0.28335008  0.03935642]]. Action = [[-0.2103556  -0.0394315  -0.19510749  0.75909126]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 435 is [True, False, False, False, True, False]
State prediction error at timestep 435 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 436. State = [[-0.28333524  0.03896877]]. Action = [[0.16420004 0.08735681 0.18102089 0.31264305]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 436 is [True, False, False, False, True, False]
State prediction error at timestep 436 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 437. State = [[-0.28333926  0.03868205]]. Action = [[ 0.21413553  0.15015343 -0.2324182   0.18304956]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 437 is [True, False, False, False, True, False]
State prediction error at timestep 437 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 438. State = [[-0.28336278  0.03843885]]. Action = [[-0.15209895  0.23640335 -0.0354079  -0.9172669 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 438 is [True, False, False, False, True, False]
State prediction error at timestep 438 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 438 of -1
Current timestep = 439. State = [[-0.28334886  0.03804737]]. Action = [[-0.21244965  0.19836664 -0.04738915  0.00814307]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 439 is [True, False, False, False, True, False]
State prediction error at timestep 439 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 440. State = [[-0.28335258  0.03781462]]. Action = [[-0.17804855  0.08301443 -0.0817813   0.32812095]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 440 is [True, False, False, False, True, False]
State prediction error at timestep 440 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 441. State = [[-0.2833545  0.0376998]]. Action = [[ 0.06517607  0.22518891  0.19276488 -0.32858157]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 441 is [True, False, False, False, True, False]
State prediction error at timestep 441 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 442. State = [[-0.2833766   0.03757108]]. Action = [[-0.16928273  0.11373121 -0.11492115 -0.03311586]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 442 is [True, False, False, False, True, False]
State prediction error at timestep 442 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 442 of -1
Current timestep = 443. State = [[-0.28338668  0.0369978 ]]. Action = [[-0.05843413 -0.0245036  -0.02237746  0.40603518]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 443 is [True, False, False, False, True, False]
State prediction error at timestep 443 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 444. State = [[-0.28348923  0.03602418]]. Action = [[ 0.056934    0.04874283 -0.10706317 -0.3370484 ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 444 is [True, False, False, False, True, False]
State prediction error at timestep 444 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 445. State = [[-0.28348923  0.03602418]]. Action = [[-0.18897434  0.21026582  0.11007017  0.9057772 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 445 is [True, False, False, False, True, False]
State prediction error at timestep 445 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 446. State = [[-0.28340918  0.03586816]]. Action = [[ 0.03995028 -0.11503589 -0.04365408 -0.487958  ]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 446 is [True, False, False, False, True, False]
State prediction error at timestep 446 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 446 of -1
Current timestep = 447. State = [[-0.28319204  0.03403803]]. Action = [[0.04466942 0.12137079 0.149225   0.6864276 ]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 447 is [True, False, False, False, True, False]
State prediction error at timestep 447 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 447 of -1
Current timestep = 448. State = [[-0.28315514  0.03430439]]. Action = [[ 0.13272452  0.20256957  0.07890597 -0.17619258]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 448 is [True, False, False, False, True, False]
State prediction error at timestep 448 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 449. State = [[-0.2830508   0.03448331]]. Action = [[0.18146381 0.084005   0.0661757  0.67912114]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 449 is [True, False, False, False, True, False]
State prediction error at timestep 449 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 450. State = [[-0.28281513  0.03480625]]. Action = [[-0.08690551 -0.06745569 -0.21202105 -0.9320977 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 450 is [True, False, False, False, True, False]
State prediction error at timestep 450 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 450 of -1
Current timestep = 451. State = [[-0.28287742  0.03438711]]. Action = [[ 0.06618863 -0.05945486 -0.08153063 -0.30860114]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 451 is [True, False, False, False, True, False]
State prediction error at timestep 451 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 451 of -1
Current timestep = 452. State = [[-0.28262898  0.03381628]]. Action = [[ 0.1418004   0.07732415 -0.20067208 -0.295915  ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 452 is [True, False, False, False, True, False]
State prediction error at timestep 452 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 453. State = [[-0.2821854   0.03328883]]. Action = [[-0.22617982  0.19705749 -0.0471094   0.35760462]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 453 is [True, False, False, False, True, False]
State prediction error at timestep 453 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 454. State = [[-0.2821488   0.03308665]]. Action = [[-0.05506748 -0.2235981  -0.16486849  0.52507305]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 454 is [True, False, False, False, True, False]
State prediction error at timestep 454 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 455. State = [[-0.28239748  0.03302064]]. Action = [[ 0.13434964  0.05643424 -0.09649271  0.5293095 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 455 is [True, False, False, False, True, False]
State prediction error at timestep 455 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 456. State = [[-0.2823588   0.03230463]]. Action = [[-0.043704    0.06779036  0.05800432 -0.28163397]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 456 is [True, False, False, False, True, False]
State prediction error at timestep 456 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 456 of -1
Current timestep = 457. State = [[-0.28248593  0.03284316]]. Action = [[ 0.0831562   0.1315195  -0.19101164  0.82913244]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 457 is [True, False, False, False, True, False]
State prediction error at timestep 457 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 457 of -1
Current timestep = 458. State = [[-0.28267363  0.03573027]]. Action = [[-0.01135282  0.03941002 -0.1424464   0.7199681 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 458 is [True, False, False, False, True, False]
State prediction error at timestep 458 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 458 of -1
Current timestep = 459. State = [[-0.28280437  0.03690847]]. Action = [[ 0.1915789   0.21022263 -0.16445024 -0.16419673]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 459 is [True, False, False, False, True, False]
State prediction error at timestep 459 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 460. State = [[-0.28279468  0.03736417]]. Action = [[-0.1424759  -0.15964434  0.02534592  0.41929626]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 460 is [True, False, False, False, True, False]
State prediction error at timestep 460 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 461. State = [[-0.28285792  0.03771505]]. Action = [[-0.22159512 -0.0771445   0.00788444 -0.6545527 ]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 461 is [True, False, False, False, True, False]
State prediction error at timestep 461 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 462. State = [[-0.28304255  0.0381331 ]]. Action = [[-0.14697866 -0.23060182 -0.10959017  0.5638406 ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 462 is [True, False, False, False, True, False]
State prediction error at timestep 462 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 462 of -1
Current timestep = 463. State = [[-0.2834113   0.03992643]]. Action = [[-0.10373268  0.10326117  0.19194251  0.2779143 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 463 is [True, False, False, False, True, False]
State prediction error at timestep 463 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 464. State = [[-0.2837105   0.04116243]]. Action = [[ 0.18401143  0.11070511 -0.21735057 -0.4087988 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 464 is [True, False, False, False, True, False]
State prediction error at timestep 464 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 465. State = [[-0.28416267  0.04233777]]. Action = [[ 0.23468688 -0.02079017  0.19218111 -0.315037  ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 465 is [True, False, False, False, True, False]
State prediction error at timestep 465 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 466. State = [[-0.28458843  0.04319223]]. Action = [[ 0.24424809 -0.05863802 -0.09252557  0.8393464 ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 466 is [True, False, False, False, True, False]
State prediction error at timestep 466 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 467. State = [[-0.28622195  0.04676209]]. Action = [[-0.08365119  0.10745862 -0.04904689 -0.10515612]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 467 is [True, False, False, False, True, False]
State prediction error at timestep 467 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 468. State = [[-0.28679115  0.04802825]]. Action = [[-0.02169141 -0.14545959  0.03410321 -0.8769977 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 468 is [True, False, False, False, True, False]
State prediction error at timestep 468 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 469. State = [[-0.28916767  0.05379746]]. Action = [[ 0.1303916   0.11953667  0.21487975 -0.01843405]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 469 is [True, False, False, False, True, False]
State prediction error at timestep 469 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 469 of -1
Current timestep = 470. State = [[-0.28924295  0.0551439 ]]. Action = [[ 0.17745781  0.12249413 -0.02427426 -0.40151596]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 470 is [True, False, False, False, True, False]
State prediction error at timestep 470 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 471. State = [[-0.28913033  0.0559778 ]]. Action = [[ 0.00506517 -0.13594587 -0.13063408 -0.3517214 ]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 471 is [True, False, False, False, True, False]
State prediction error at timestep 471 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 472. State = [[-0.28901502  0.05711573]]. Action = [[ 0.16092068  0.03508273 -0.18385197  0.9381641 ]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 472 is [True, False, False, False, True, False]
State prediction error at timestep 472 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 473. State = [[-0.28910577  0.05806324]]. Action = [[-0.19326767 -0.00682275 -0.07020941 -0.73766726]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 473 is [True, False, False, False, True, False]
State prediction error at timestep 473 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 474. State = [[-0.28915256  0.05932444]]. Action = [[ 0.18740273 -0.0116391  -0.06639278  0.5101671 ]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 474 is [True, False, False, False, True, False]
State prediction error at timestep 474 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 474 of -1
Current timestep = 475. State = [[-0.28896987  0.06115042]]. Action = [[ 0.21875635  0.22295707 -0.01366304  0.92286825]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 475 is [True, False, False, False, True, False]
State prediction error at timestep 475 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 476. State = [[-0.28899822  0.06166575]]. Action = [[-0.05352522 -0.19045721 -0.23039535 -0.37873673]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 476 is [True, False, False, False, True, False]
State prediction error at timestep 476 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 477. State = [[-0.2886798   0.06361997]]. Action = [[-0.02412897 -0.03134793 -0.08473547 -0.60843784]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 477 is [True, False, False, False, True, False]
State prediction error at timestep 477 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 477 of -1
Current timestep = 478. State = [[-0.28857824  0.06409216]]. Action = [[ 0.03487152 -0.05830349 -0.14932238 -0.27818435]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 478 is [True, False, False, False, True, False]
State prediction error at timestep 478 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 478 of -1
Current timestep = 479. State = [[-0.28794423  0.06397156]]. Action = [[-0.0125445   0.01896432  0.10696661  0.1813091 ]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 479 is [True, False, False, False, True, False]
State prediction error at timestep 479 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 480. State = [[-0.28777683  0.06416803]]. Action = [[-0.01057696  0.12150064 -0.11923799  0.49557364]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 480 is [True, False, False, False, True, False]
State prediction error at timestep 480 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 481. State = [[-0.28801075  0.06489065]]. Action = [[-0.16786735  0.09143841 -0.08404174  0.94057715]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 481 is [True, False, False, False, True, False]
State prediction error at timestep 481 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 482. State = [[-0.2884009  0.0657552]]. Action = [[ 0.19553205  0.11052352 -0.07108188 -0.6151726 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 482 is [True, False, False, False, True, False]
State prediction error at timestep 482 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 482 of -1
Current timestep = 483. State = [[-0.28856614  0.06608585]]. Action = [[ 0.23014063  0.2280016  -0.23146732 -0.89922947]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 483 is [True, False, False, False, True, False]
State prediction error at timestep 483 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 484. State = [[-0.28876933  0.06653221]]. Action = [[-0.17859098 -0.07343183 -0.20403655  0.09620428]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 484 is [True, False, False, False, True, False]
State prediction error at timestep 484 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 485. State = [[-0.28936195  0.06812903]]. Action = [[-0.05765003  0.00721434 -0.15585938  0.95590377]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 485 is [True, False, False, False, True, False]
State prediction error at timestep 485 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 485 of -1
Current timestep = 486. State = [[-0.28997645  0.06942619]]. Action = [[ 0.08172381 -0.04162124  0.11321828  0.43386042]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 486 is [True, False, False, False, True, False]
State prediction error at timestep 486 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 486 of -1
Current timestep = 487. State = [[-0.28985748  0.06938434]]. Action = [[-0.21864042 -0.15021038  0.00431821  0.95881915]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 487 is [True, False, False, False, True, False]
State prediction error at timestep 487 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 488. State = [[-0.28962284  0.06931379]]. Action = [[ 0.21029952 -0.10448667  0.02188483  0.6717951 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 488 is [True, False, False, False, True, False]
State prediction error at timestep 488 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 489. State = [[-0.28935298  0.06927861]]. Action = [[ 0.00827354 -0.10732302  0.11836988  0.56720793]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 489 is [True, False, False, False, True, False]
State prediction error at timestep 489 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 489 of -1
Current timestep = 490. State = [[-0.28890327  0.06845736]]. Action = [[ 0.04881904  0.17320225 -0.14875877 -0.6976891 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 490 is [True, False, False, False, True, False]
State prediction error at timestep 490 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 491. State = [[-0.28879416  0.06798484]]. Action = [[ 0.18274117  0.17451417 -0.08205193  0.86752176]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 491 is [True, False, False, False, True, False]
State prediction error at timestep 491 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 492. State = [[-0.2889724   0.06750156]]. Action = [[-0.1767769  -0.19789062 -0.23172753  0.48736453]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 492 is [True, False, False, False, True, False]
State prediction error at timestep 492 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 493. State = [[-0.28860807  0.06700287]]. Action = [[-0.20645474 -0.11018348  0.06999087  0.5330937 ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 493 is [True, False, False, False, True, False]
State prediction error at timestep 493 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 493 of -1
Current timestep = 494. State = [[-0.28836012  0.06605187]]. Action = [[ 0.03388166 -0.0788292   0.05470413  0.19521952]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 494 is [True, False, False, False, True, False]
State prediction error at timestep 494 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 495. State = [[-0.28805524  0.06540425]]. Action = [[ 0.12121719  0.1796689   0.12177256 -0.96667546]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 495 is [True, False, False, False, True, False]
State prediction error at timestep 495 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 496. State = [[-0.28775805  0.06497472]]. Action = [[-0.15417916 -0.01781851 -0.1635988   0.01532125]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 496 is [True, False, False, False, True, False]
State prediction error at timestep 496 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 497. State = [[-0.28729835  0.06434487]]. Action = [[-0.18685572 -0.14615548  0.20894131 -0.65483236]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 498. State = [[-0.28758863  0.06392538]]. Action = [[0.10611516 0.18884933 0.00934932 0.25827348]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 498 is [True, False, False, False, True, False]
State prediction error at timestep 498 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 499. State = [[-0.2872247   0.06356996]]. Action = [[-0.05049059  0.21964484 -0.1757098   0.64303625]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 500. State = [[-0.28666902  0.06322628]]. Action = [[-0.23143914 -0.14097476 -0.03046821 -0.06529313]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 500 is [True, False, False, False, True, False]
State prediction error at timestep 500 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 500 of -1
Current timestep = 501. State = [[-0.28694716  0.06289837]]. Action = [[-0.22811739  0.16690859  0.24524125  0.25055707]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 501 is [True, False, False, False, True, False]
State prediction error at timestep 501 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 502. State = [[-0.28612146  0.06230393]]. Action = [[-0.09892589 -0.11350471 -0.0236439   0.9640744 ]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 502 is [True, False, False, False, True, False]
State prediction error at timestep 502 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 503. State = [[-0.28563538  0.05926253]]. Action = [[ 0.00413337  0.11720291 -0.0950039  -0.10724932]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 503 is [True, False, False, False, True, False]
State prediction error at timestep 503 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 504. State = [[-0.28561255  0.05940828]]. Action = [[ 0.18469405  0.02015388 -0.06945665  0.8968861 ]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 504 is [True, False, False, False, True, False]
State prediction error at timestep 504 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 505. State = [[-0.28584182  0.05948215]]. Action = [[-0.05893838  0.16894612  0.19485459  0.15635395]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 505 is [True, False, False, False, True, False]
State prediction error at timestep 505 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 506. State = [[-0.28596133  0.0594984 ]]. Action = [[-0.19815522  0.19600022  0.22193444  0.82186913]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 506 is [True, False, False, False, True, False]
State prediction error at timestep 506 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 507. State = [[-0.28577384  0.05933306]]. Action = [[ 0.20537508  0.2084735  -0.18816482 -0.04198205]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 507 is [True, False, False, False, True, False]
State prediction error at timestep 507 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 508. State = [[-0.28581905  0.0593616 ]]. Action = [[ 0.23403388  0.09594962 -0.10380976  0.7287574 ]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 508 is [True, False, False, False, True, False]
State prediction error at timestep 508 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 509. State = [[-0.28593546  0.05945483]]. Action = [[ 0.15647167  0.03716233  0.11306831 -0.01568705]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 509 is [True, False, False, False, True, False]
State prediction error at timestep 509 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 509 of -1
Current timestep = 510. State = [[-0.2857953   0.05937631]]. Action = [[0.17076856 0.20081177 0.19411016 0.14714527]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 510 is [True, False, False, False, True, False]
State prediction error at timestep 510 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 511. State = [[-0.2857953   0.05937631]]. Action = [[ 0.24585938 -0.21615894 -0.14756578  0.1323067 ]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 511 is [True, False, False, False, True, False]
State prediction error at timestep 511 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 512. State = [[-0.28586832  0.05940615]]. Action = [[-0.2230423  -0.18876266 -0.20516588 -0.40480328]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 512 is [True, False, False, False, True, False]
State prediction error at timestep 512 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 513. State = [[-0.2857953   0.05937631]]. Action = [[ 0.16347939 -0.08459842  0.08828032 -0.16898996]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 513 is [True, False, False, False, True, False]
State prediction error at timestep 513 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 513 of -1
Current timestep = 514. State = [[-0.28599638  0.05969013]]. Action = [[ 0.0041109   0.129197    0.10231754 -0.10933715]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 514 is [True, False, False, False, True, False]
State prediction error at timestep 514 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 515. State = [[-0.28638887  0.06072251]]. Action = [[-0.23718672 -0.15704249 -0.1744708  -0.37655252]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 515 is [True, False, False, False, True, False]
State prediction error at timestep 515 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 516. State = [[-0.28667316  0.06134422]]. Action = [[-0.1736301   0.05789864  0.24067616  0.02705109]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 516 is [True, False, False, False, True, False]
State prediction error at timestep 516 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 516 of -1
Current timestep = 517. State = [[-0.28692767  0.06180568]]. Action = [[-0.21393788 -0.24232471 -0.12804271  0.6472714 ]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 517 is [True, False, False, False, True, False]
State prediction error at timestep 517 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 518. State = [[-0.28712854  0.06231137]]. Action = [[-0.17846137 -0.20780925  0.01552719 -0.02651507]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 518 is [True, False, False, False, True, False]
State prediction error at timestep 518 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 519. State = [[-0.28726217  0.06262962]]. Action = [[ 0.21674788  0.15317854 -0.20610388 -0.22951055]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 519 is [True, False, False, False, True, False]
State prediction error at timestep 519 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 520. State = [[-0.287417   0.0628741]]. Action = [[ 0.20323586  0.10973522  0.01670438 -0.37963033]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 520 is [True, False, False, False, True, False]
State prediction error at timestep 520 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 520 of -1
Current timestep = 521. State = [[-0.28748104  0.06306215]]. Action = [[ 0.134487    0.0560773  -0.00714545  0.18216419]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 521 is [True, False, False, False, True, False]
State prediction error at timestep 521 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 522. State = [[-0.2876013   0.06335221]]. Action = [[-0.16815601 -0.1360554   0.10568371  0.53578734]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 522 is [True, False, False, False, True, False]
State prediction error at timestep 522 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 523. State = [[-0.287686    0.06362607]]. Action = [[ 0.03302631 -0.1688951  -0.05109254 -0.5346466 ]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 523 is [True, False, False, False, True, False]
State prediction error at timestep 523 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 524. State = [[-0.28775707  0.06387196]]. Action = [[0.16292617 0.1430332  0.04186267 0.89176345]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 524 is [True, False, False, False, True, False]
State prediction error at timestep 524 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 525. State = [[-0.28775707  0.06387196]]. Action = [[-0.16291024 -0.23148605  0.20661354  0.25000167]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 525 is [True, False, False, False, True, False]
State prediction error at timestep 525 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 525 of -1
Current timestep = 526. State = [[-0.28788108  0.06422772]]. Action = [[ 0.20550388 -0.1304752   0.21214592 -0.573671  ]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 526 is [True, False, False, False, True, False]
State prediction error at timestep 526 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 527. State = [[-0.28785345  0.06422713]]. Action = [[ 0.22830349  0.17568341  0.15151435 -0.19421214]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 527 is [True, False, False, False, True, False]
State prediction error at timestep 527 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 528. State = [[-0.28788075  0.06438689]]. Action = [[ 0.18992472 -0.06725624 -0.24690548 -0.94954073]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 528 is [True, False, False, False, True, False]
State prediction error at timestep 528 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 529. State = [[-0.2879054   0.06448828]]. Action = [[-0.03085208 -0.16963418  0.17728221  0.9912758 ]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 529 is [True, False, False, False, True, False]
State prediction error at timestep 529 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 529 of -1
Current timestep = 530. State = [[-0.2879276   0.06453173]]. Action = [[-0.19481996  0.03418747  0.09251019  0.5329423 ]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 530 is [True, False, False, False, True, False]
State prediction error at timestep 530 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 531. State = [[-0.2879303   0.06459009]]. Action = [[-0.15794785 -0.02621269  0.0286302   0.8437234 ]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 531 is [True, False, False, False, True, False]
State prediction error at timestep 531 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 532. State = [[-0.287933    0.06464805]]. Action = [[-0.10713305 -0.20109479  0.04779983  0.7089704 ]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 532 is [True, False, False, False, True, False]
State prediction error at timestep 532 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 532 of -1
Current timestep = 533. State = [[-0.2880517   0.06504381]]. Action = [[-0.08479333  0.12242115 -0.07360911 -0.7919592 ]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 533 is [True, False, False, False, True, False]
State prediction error at timestep 533 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 534. State = [[-0.2884719   0.06616554]]. Action = [[ 0.18659437 -0.22638096 -0.23724058  0.7314533 ]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 534 is [True, False, False, False, True, False]
State prediction error at timestep 534 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 534 of -1
Current timestep = 535. State = [[-0.2901431   0.06973194]]. Action = [[ 0.0851104  -0.07294989  0.24722448 -0.8195781 ]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 535 is [True, False, False, False, True, False]
State prediction error at timestep 535 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 536. State = [[-0.28993377  0.06990793]]. Action = [[-0.00152059 -0.10226262 -0.19419943 -0.34218413]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 536 is [True, False, False, False, True, False]
State prediction error at timestep 536 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 537. State = [[-0.2896612   0.06948233]]. Action = [[ 0.19748238 -0.22379097 -0.13049437 -0.0977481 ]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 537 is [True, False, False, False, True, False]
State prediction error at timestep 537 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 538. State = [[-0.28898153  0.06884003]]. Action = [[ 0.24068746 -0.16825528  0.13783246 -0.7811795 ]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 538 is [True, False, False, False, True, False]
State prediction error at timestep 538 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 539. State = [[-0.28877372  0.06853418]]. Action = [[ 0.19224408  0.21280882  0.10666841 -0.66215014]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 539 of -1
Current timestep = 540. State = [[-0.28923967  0.06838424]]. Action = [[ 0.21354532  0.24349806  0.0084945  -0.63755244]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 540 is [True, False, False, False, True, False]
State prediction error at timestep 540 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 541. State = [[-0.28903943  0.06795422]]. Action = [[-0.23253785  0.0555284   0.00447738 -0.49996692]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 541 is [True, False, False, False, True, False]
State prediction error at timestep 541 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 542. State = [[-0.2887436   0.06726055]]. Action = [[-0.07061556  0.04335549  0.08743083  0.8872585 ]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 542 is [True, False, False, False, True, False]
State prediction error at timestep 542 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 542 of -1
Current timestep = 543. State = [[-0.28880838  0.06755876]]. Action = [[-0.24489352  0.09885198  0.182666   -0.03107512]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 543 is [True, False, False, False, True, False]
State prediction error at timestep 543 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 544. State = [[-0.2889701  0.0676005]]. Action = [[ 0.06570604 -0.00949934 -0.02857587  0.6886411 ]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 544 is [True, False, False, False, True, False]
State prediction error at timestep 544 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 544 of -1
Current timestep = 545. State = [[-0.28913146  0.06766135]]. Action = [[-0.06357357 -0.2139283  -0.09282658  0.85579467]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 545 is [True, False, False, False, True, False]
State prediction error at timestep 545 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 546. State = [[-0.28911665  0.06767324]]. Action = [[-0.23944281 -0.07897165  0.09566149  0.40972555]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 546 is [True, False, False, False, True, False]
State prediction error at timestep 546 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 547. State = [[-0.28900316  0.06775711]]. Action = [[ 0.15367168 -0.19549322 -0.23181868 -0.5397326 ]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 547 is [True, False, False, False, True, False]
State prediction error at timestep 547 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 548. State = [[-0.28900316  0.06775711]]. Action = [[ 0.05058581 -0.171247    0.15660441 -0.8341867 ]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 548 is [True, False, False, False, True, False]
State prediction error at timestep 548 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 549. State = [[-0.28906533  0.06768771]]. Action = [[-0.08984256 -0.00449304  0.03932828  0.50575054]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 549 is [True, False, False, False, True, False]
State prediction error at timestep 549 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 549 of -1
Current timestep = 550. State = [[-0.28906533  0.06768771]]. Action = [[-0.18117712 -0.00703517 -0.15088008 -0.6524596 ]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 550 is [True, False, False, False, True, False]
State prediction error at timestep 550 is tensor(9.6457e-05, grad_fn=<MseLossBackward0>)
Current timestep = 551. State = [[-0.28906533  0.06768771]]. Action = [[-0.06643382 -0.23928024  0.1438599   0.43546152]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 551 is [True, False, False, False, True, False]
State prediction error at timestep 551 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 552. State = [[-0.28900316  0.06775711]]. Action = [[-0.14666328 -0.05486725  0.18576336  0.7203567 ]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 552 is [True, False, False, False, True, False]
State prediction error at timestep 552 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 553. State = [[-0.28906533  0.06768771]]. Action = [[-0.22757329  0.10117728 -0.23657131  0.41749334]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 553 is [True, False, False, False, True, False]
State prediction error at timestep 553 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 553 of -1
Current timestep = 554. State = [[-0.28900316  0.06775711]]. Action = [[ 0.09233901 -0.22299814  0.05223277  0.27461052]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 554 is [True, False, False, False, True, False]
State prediction error at timestep 554 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 555. State = [[-0.28906533  0.06768771]]. Action = [[ 0.0160397   0.22870874 -0.04875781 -0.26445198]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 555 is [True, False, False, False, True, False]
State prediction error at timestep 555 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 556. State = [[-0.28906533  0.06768771]]. Action = [[-0.2068275   0.10080704  0.2221277  -0.8632951 ]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 556 is [True, False, False, False, True, False]
State prediction error at timestep 556 is tensor(8.0050e-05, grad_fn=<MseLossBackward0>)
Current timestep = 557. State = [[-0.28906533  0.06768771]]. Action = [[ 0.01189643 -0.13528143 -0.04550695  0.5321586 ]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 557 is [True, False, False, False, True, False]
State prediction error at timestep 557 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 557 of -1
Current timestep = 558. State = [[-0.28906533  0.06768771]]. Action = [[-0.08712156 -0.02582179 -0.15926811  0.50156355]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 558 is [True, False, False, False, True, False]
State prediction error at timestep 558 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 559. State = [[-0.28982753  0.06717557]]. Action = [[ 0.10191295 -0.06950247  0.17200047 -0.87463695]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 559 is [True, False, False, False, True, False]
State prediction error at timestep 559 is tensor(8.5892e-05, grad_fn=<MseLossBackward0>)
Current timestep = 560. State = [[-0.2896357   0.06653168]]. Action = [[ 0.18251443 -0.10318938 -0.14548054  0.8087312 ]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 560 is [True, False, False, False, True, False]
State prediction error at timestep 560 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 561. State = [[-0.28945598  0.06595846]]. Action = [[-0.14261769  0.07999116 -0.16323955  0.10800564]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 561 is [True, False, False, False, True, False]
State prediction error at timestep 561 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 562. State = [[-0.2895161   0.06552768]]. Action = [[ 0.15143383 -0.09070519 -0.00093256  0.15183806]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 562 is [True, False, False, False, True, False]
State prediction error at timestep 562 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 563. State = [[-0.28942832  0.06366239]]. Action = [[ 0.04365268  0.03425184  0.13391879 -0.0210529 ]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 563 is [True, False, False, False, True, False]
State prediction error at timestep 563 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 563 of -1
Current timestep = 564. State = [[-0.28947234  0.06374855]]. Action = [[ 0.13982755  0.04126513 -0.15992382 -0.85240734]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 564 is [True, False, False, False, True, False]
State prediction error at timestep 564 is tensor(9.2632e-05, grad_fn=<MseLossBackward0>)
Current timestep = 565. State = [[-0.28951088  0.06371798]]. Action = [[ 0.08911318 -0.23129053 -0.05168174 -0.3846225 ]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 565 is [True, False, False, False, True, False]
State prediction error at timestep 565 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 566. State = [[-0.28946447  0.06357389]]. Action = [[ 0.23888835  0.2034617   0.03072092 -0.11036026]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 566 is [True, False, False, False, True, False]
State prediction error at timestep 566 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 567. State = [[-0.28941777  0.06342583]]. Action = [[0.19678089 0.24864382 0.07918108 0.8936248 ]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 567 is [True, False, False, False, True, False]
State prediction error at timestep 567 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 567 of -1
Current timestep = 568. State = [[-0.28947046  0.06350893]]. Action = [[ 0.20841748  0.06988877 -0.23121285 -0.60931903]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 568 is [True, False, False, False, True, False]
State prediction error at timestep 568 is tensor(4.2280e-05, grad_fn=<MseLossBackward0>)
Current timestep = 569. State = [[-0.28951997  0.0635526 ]]. Action = [[-0.18965855  0.2293447   0.167561    0.2984426 ]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 569 is [True, False, False, False, True, False]
State prediction error at timestep 569 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 570. State = [[-0.2894319   0.06329367]]. Action = [[-0.24531706 -0.16661958  0.01788935  0.956182  ]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 570 is [True, False, False, False, True, False]
State prediction error at timestep 570 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 571. State = [[-0.2893835   0.06320374]]. Action = [[ 0.02565497 -0.13334367 -0.08407375 -0.77822185]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 571 is [True, False, False, False, True, False]
State prediction error at timestep 571 is tensor(1.7155e-06, grad_fn=<MseLossBackward0>)
Current timestep = 572. State = [[-0.289452    0.06339468]]. Action = [[ 0.17733258  0.14578164 -0.21869825 -0.93968534]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 572 is [True, False, False, False, True, False]
State prediction error at timestep 572 is tensor(7.2913e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 572 of -1
Current timestep = 573. State = [[-0.2894758   0.06337982]]. Action = [[-0.24818775 -0.10829635 -0.15639961 -0.5101299 ]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 573 is [True, False, False, False, True, False]
State prediction error at timestep 573 is tensor(1.7258e-05, grad_fn=<MseLossBackward0>)
Current timestep = 574. State = [[-0.28940532  0.06324662]]. Action = [[-0.1173099   0.13487643  0.01870829 -0.81076556]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 574 is [True, False, False, False, True, False]
State prediction error at timestep 574 is tensor(4.2479e-06, grad_fn=<MseLossBackward0>)
Current timestep = 575. State = [[-0.28945503  0.06329074]]. Action = [[ 0.00735751  0.21478286  0.03965569 -0.12128597]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 575 is [True, False, False, False, True, False]
State prediction error at timestep 575 is tensor(7.4020e-05, grad_fn=<MseLossBackward0>)
Current timestep = 576. State = [[-0.289452    0.06339468]]. Action = [[-0.00796239  0.1380952   0.1972323   0.85102284]]. Reward = [0.]
Curr episode timestep = 576
Scene graph at timestep 576 is [True, False, False, False, True, False]
State prediction error at timestep 576 is tensor(7.8993e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 576 of -1
Current timestep = 577. State = [[-0.2895206  0.0632847]]. Action = [[-0.09422736  0.1066561  -0.08045444  0.2962265 ]]. Reward = [0.]
Curr episode timestep = 577
Scene graph at timestep 577 is [True, False, False, False, True, False]
State prediction error at timestep 577 is tensor(9.8439e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 577 of -1
Current timestep = 578. State = [[-0.28984833  0.06399117]]. Action = [[-0.2303494  -0.07368992 -0.20322427  0.58644485]]. Reward = [0.]
Curr episode timestep = 578
Scene graph at timestep 578 is [True, False, False, False, True, False]
State prediction error at timestep 578 is tensor(5.0559e-05, grad_fn=<MseLossBackward0>)
Current timestep = 579. State = [[-0.29016903  0.0647869 ]]. Action = [[-0.11430204  0.15792999  0.12816244 -0.6356796 ]]. Reward = [0.]
Curr episode timestep = 579
Scene graph at timestep 579 is [True, False, False, False, True, False]
State prediction error at timestep 579 is tensor(1.5168e-05, grad_fn=<MseLossBackward0>)
Current timestep = 580. State = [[-0.29048464  0.06532692]]. Action = [[ 0.1380139  -0.16846567  0.20763502  0.06597412]]. Reward = [0.]
Curr episode timestep = 580
Scene graph at timestep 580 is [True, False, False, False, True, False]
State prediction error at timestep 580 is tensor(5.2509e-05, grad_fn=<MseLossBackward0>)
Current timestep = 581. State = [[-0.29066393  0.06563838]]. Action = [[-0.05155382  0.2078585  -0.13764702 -0.9782427 ]]. Reward = [0.]
Curr episode timestep = 581
Scene graph at timestep 581 is [True, False, False, False, True, False]
State prediction error at timestep 581 is tensor(4.8195e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 581 of -1
Current timestep = 582. State = [[-0.29091415  0.06623034]]. Action = [[-0.21432255  0.17124242  0.03653648 -0.5856639 ]]. Reward = [0.]
Curr episode timestep = 582
Scene graph at timestep 582 is [True, False, False, False, True, False]
State prediction error at timestep 582 is tensor(7.7505e-06, grad_fn=<MseLossBackward0>)
Current timestep = 583. State = [[-0.29110086  0.06657016]]. Action = [[0.06663194 0.15120006 0.21883053 0.0557754 ]]. Reward = [0.]
Curr episode timestep = 583
Scene graph at timestep 583 is [True, False, False, False, True, False]
State prediction error at timestep 583 is tensor(7.3206e-05, grad_fn=<MseLossBackward0>)
Current timestep = 584. State = [[-0.29146808  0.06724311]]. Action = [[0.04127648 0.01167011 0.11848652 0.7617798 ]]. Reward = [0.]
Curr episode timestep = 584
Scene graph at timestep 584 is [True, False, False, False, True, False]
State prediction error at timestep 584 is tensor(2.4705e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 584 of -1
Current timestep = 585. State = [[-0.29149055  0.06728619]]. Action = [[-0.23088503  0.12510729 -0.19221564  0.8095856 ]]. Reward = [0.]
Curr episode timestep = 585
Scene graph at timestep 585 is [True, False, False, False, True, False]
State prediction error at timestep 585 is tensor(7.8007e-05, grad_fn=<MseLossBackward0>)
Current timestep = 586. State = [[-0.2915694   0.06746975]]. Action = [[ 0.16900596 -0.20075494  0.18696457 -0.85291994]]. Reward = [0.]
Curr episode timestep = 586
Scene graph at timestep 586 is [True, False, False, False, True, False]
State prediction error at timestep 586 is tensor(4.1385e-05, grad_fn=<MseLossBackward0>)
Current timestep = 587. State = [[-0.29161933  0.06751295]]. Action = [[ 0.15235013 -0.15274362 -0.2311688  -0.31962806]]. Reward = [0.]
Curr episode timestep = 587
Scene graph at timestep 587 is [True, False, False, False, True, False]
State prediction error at timestep 587 is tensor(4.8575e-05, grad_fn=<MseLossBackward0>)
Current timestep = 588. State = [[-0.2916365   0.06759813]]. Action = [[-0.01797961 -0.21987975  0.14210129  0.8415003 ]]. Reward = [0.]
Curr episode timestep = 588
Scene graph at timestep 588 is [True, False, False, False, True, False]
State prediction error at timestep 588 is tensor(2.8788e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 588 of -1
Current timestep = 589. State = [[-0.2916501   0.06764393]]. Action = [[-0.05851413  0.22554111  0.04897657 -0.36395997]]. Reward = [0.]
Curr episode timestep = 589
Scene graph at timestep 589 is [True, False, False, False, True, False]
State prediction error at timestep 589 is tensor(1.4700e-05, grad_fn=<MseLossBackward0>)
Current timestep = 590. State = [[-0.2916501   0.06764393]]. Action = [[-0.21068181  0.18438107 -0.2200434  -0.7576333 ]]. Reward = [0.]
Curr episode timestep = 590
Scene graph at timestep 590 is [True, False, False, False, True, False]
State prediction error at timestep 590 is tensor(3.0012e-05, grad_fn=<MseLossBackward0>)
Current timestep = 591. State = [[-0.29165325  0.06770225]]. Action = [[ 0.08874115 -0.00488418  0.1614851  -0.8049775 ]]. Reward = [0.]
Curr episode timestep = 591
Scene graph at timestep 591 is [True, False, False, False, True, False]
State prediction error at timestep 591 is tensor(2.5378e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 591 of -1
Current timestep = 592. State = [[-0.29158795  0.06771345]]. Action = [[ 0.09933272 -0.21903518  0.1380434  -0.8667709 ]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 592 is [True, False, False, False, True, False]
State prediction error at timestep 592 is tensor(5.1726e-05, grad_fn=<MseLossBackward0>)
Current timestep = 593. State = [[-0.2915643   0.06772855]]. Action = [[-0.07705605 -0.03796661 -0.13553692  0.07515717]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 593 is [True, False, False, False, True, False]
State prediction error at timestep 593 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 593 of -1
Current timestep = 594. State = [[-0.2915643   0.06772855]]. Action = [[0.11865541 0.1738922  0.01021695 0.9126018 ]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 594 is [True, False, False, False, True, False]
State prediction error at timestep 594 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 595. State = [[-0.2915611   0.06767022]]. Action = [[-0.1050863  -0.22028013 -0.15171577 -0.76537776]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 595 is [True, False, False, False, True, False]
State prediction error at timestep 595 is tensor(4.2408e-05, grad_fn=<MseLossBackward0>)
Current timestep = 596. State = [[-0.2915611   0.06767022]]. Action = [[-0.18276833 -0.00398475 -0.18800884  0.82686734]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 596 is [True, False, False, False, True, False]
State prediction error at timestep 596 is tensor(3.2387e-05, grad_fn=<MseLossBackward0>)
Current timestep = 597. State = [[-0.2915611   0.06767022]]. Action = [[-0.18051988 -0.2199557  -0.24349967 -0.19592983]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 597 is [True, False, False, False, True, False]
State prediction error at timestep 597 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 597 of -1
Current timestep = 598. State = [[-0.2915611   0.06767022]]. Action = [[-0.21657112  0.22321337 -0.14079072 -0.3075115 ]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 598 is [True, False, False, False, True, False]
State prediction error at timestep 598 is tensor(1.0098e-06, grad_fn=<MseLossBackward0>)
Current timestep = 599. State = [[-0.2915611   0.06767022]]. Action = [[-0.00901502  0.03266177  0.06543922 -0.5489639 ]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 599 is [True, False, False, False, True, False]
State prediction error at timestep 599 is tensor(2.7076e-05, grad_fn=<MseLossBackward0>)
Current timestep = 600. State = [[-0.29151425  0.06746033]]. Action = [[ 0.07209098 -0.12295359 -0.14671491 -0.9064076 ]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 600 is [True, False, False, False, True, False]
State prediction error at timestep 600 is tensor(1.8313e-05, grad_fn=<MseLossBackward0>)
Current timestep = 601. State = [[-0.29109195  0.06680375]]. Action = [[-0.22084947  0.08129561 -0.03863962 -0.7632923 ]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 601 is [True, False, False, False, True, False]
State prediction error at timestep 601 is tensor(9.6139e-05, grad_fn=<MseLossBackward0>)
Current timestep = 602. State = [[-0.2907052   0.06612683]]. Action = [[-2.4676873e-01 -2.4132510e-01  5.5152178e-04 -7.4093360e-01]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 602 is [True, False, False, False, True, False]
State prediction error at timestep 602 is tensor(6.2404e-05, grad_fn=<MseLossBackward0>)
Current timestep = 603. State = [[-0.29061177  0.06597419]]. Action = [[ 0.12073869  0.19983575 -0.05427508  0.1818583 ]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 603 is [True, False, False, False, True, False]
State prediction error at timestep 603 is tensor(7.2159e-05, grad_fn=<MseLossBackward0>)
Current timestep = 604. State = [[-0.29063663  0.06551345]]. Action = [[ 0.16502231  0.16934389  0.18839878 -0.28986633]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 604 is [True, False, False, False, True, False]
State prediction error at timestep 604 is tensor(1.6753e-06, grad_fn=<MseLossBackward0>)
Current timestep = 605. State = [[-0.28994805  0.06430122]]. Action = [[ 0.07439891 -0.06799549 -0.08804938  0.7906587 ]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 605 is [True, False, False, False, True, False]
State prediction error at timestep 605 is tensor(1.9835e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 605 of -1
Current timestep = 606. State = [[-0.28933153  0.06364668]]. Action = [[-0.03849743  0.18832946 -0.02736256  0.42991138]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 606 is [True, False, False, False, True, False]
State prediction error at timestep 606 is tensor(2.1883e-06, grad_fn=<MseLossBackward0>)
Current timestep = 607. State = [[-0.28906038  0.06303409]]. Action = [[ 0.12294498 -0.23332551  0.18379241 -0.04925436]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 607 is [True, False, False, False, True, False]
State prediction error at timestep 607 is tensor(4.3084e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 607 of -1
Current timestep = 608. State = [[-0.28906897  0.06258886]]. Action = [[ 0.08718559 -0.16635893  0.08924329 -0.28500587]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 608 is [True, False, False, False, True, False]
State prediction error at timestep 608 is tensor(7.1262e-07, grad_fn=<MseLossBackward0>)
Current timestep = 609. State = [[-0.2887643   0.06225547]]. Action = [[-0.07132098  0.1988453   0.14430448  0.9369247 ]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 609 is [True, False, False, False, True, False]
State prediction error at timestep 609 is tensor(2.0663e-05, grad_fn=<MseLossBackward0>)
Current timestep = 610. State = [[-0.2879233  0.061148 ]]. Action = [[-0.08745921  0.03410083 -0.06358114 -0.900281  ]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 610 is [True, False, False, False, True, False]
State prediction error at timestep 610 is tensor(6.0390e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 610 of -1
Current timestep = 611. State = [[-0.28799668  0.06146261]]. Action = [[-0.09654552  0.1263178   0.13632035 -0.58959126]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 611 is [True, False, False, False, True, False]
State prediction error at timestep 611 is tensor(4.1810e-05, grad_fn=<MseLossBackward0>)
Current timestep = 612. State = [[-0.28906873  0.0636449 ]]. Action = [[ 0.09084991 -0.10249132  0.125207    0.87660193]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 612 is [True, False, False, False, True, False]
State prediction error at timestep 612 is tensor(2.2116e-06, grad_fn=<MseLossBackward0>)
Current timestep = 613. State = [[-0.2890007   0.06348611]]. Action = [[0.1652993  0.21299568 0.1411184  0.06832778]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 613 is [True, False, False, False, True, False]
State prediction error at timestep 613 is tensor(4.4759e-06, grad_fn=<MseLossBackward0>)
Current timestep = 614. State = [[-0.28893232  0.06329843]]. Action = [[ 0.24718994 -0.11938602  0.07819125 -0.90781444]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 614 is [True, False, False, False, True, False]
State prediction error at timestep 614 is tensor(3.9364e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 614 of 1
Current timestep = 615. State = [[-0.28891847  0.06313619]]. Action = [[-0.08136362 -0.21584822  0.19012383 -0.84305006]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 615 is [True, False, False, False, True, False]
State prediction error at timestep 615 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 616. State = [[-0.28892103  0.06319468]]. Action = [[ 0.1817714  -0.22634536  0.11758888  0.8070605 ]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 616 is [True, False, False, False, True, False]
State prediction error at timestep 616 is tensor(7.5846e-08, grad_fn=<MseLossBackward0>)
Current timestep = 617. State = [[-0.28885323  0.06312276]]. Action = [[0.13583344 0.21631548 0.16599882 0.3899157 ]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 617 is [True, False, False, False, True, False]
State prediction error at timestep 617 is tensor(2.5233e-06, grad_fn=<MseLossBackward0>)
Current timestep = 618. State = [[-0.28880894  0.0629207 ]]. Action = [[ 0.0030162  -0.21205983  0.01950878  0.2183125 ]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 618 is [True, False, False, False, True, False]
State prediction error at timestep 618 is tensor(5.1817e-06, grad_fn=<MseLossBackward0>)
Current timestep = 619. State = [[-0.28880894  0.0629207 ]]. Action = [[-0.17650138 -0.09586585 -0.12310618  0.9257579 ]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 619 is [True, False, False, False, True, False]
State prediction error at timestep 619 is tensor(1.1264e-05, grad_fn=<MseLossBackward0>)
Current timestep = 620. State = [[-0.28880894  0.0629207 ]]. Action = [[-0.04441489  0.0189974  -0.23994198  0.35660768]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 620 is [True, False, False, False, True, False]
State prediction error at timestep 620 is tensor(4.1383e-06, grad_fn=<MseLossBackward0>)
Current timestep = 621. State = [[-0.2888768   0.06299256]]. Action = [[-0.06303048 -0.1364179  -0.13341205 -0.6005832 ]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 621 is [True, False, False, False, True, False]
State prediction error at timestep 621 is tensor(5.8748e-05, grad_fn=<MseLossBackward0>)
Current timestep = 622. State = [[-0.28887936  0.06305104]]. Action = [[-0.05845672 -0.18642765  0.22584724  0.12408376]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 622 is [True, False, False, False, True, False]
State prediction error at timestep 622 is tensor(5.0321e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of -1
Current timestep = 623. State = [[-0.28893983  0.0630394 ]]. Action = [[-0.12011395  0.00744349  0.19819206  0.06057084]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 623 is [True, False, False, False, True, False]
State prediction error at timestep 623 is tensor(1.0625e-05, grad_fn=<MseLossBackward0>)
Current timestep = 624. State = [[-0.28926614  0.06312652]]. Action = [[ 0.23625469 -0.22218715 -0.10096434  0.28964472]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 624 is [True, False, False, False, True, False]
State prediction error at timestep 624 is tensor(1.3301e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 624 of -1
Current timestep = 625. State = [[-0.28984335  0.06336691]]. Action = [[0.08397809 0.22129542 0.05145848 0.75880957]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 625 is [True, False, False, False, True, False]
State prediction error at timestep 625 is tensor(1.9876e-05, grad_fn=<MseLossBackward0>)
Current timestep = 626. State = [[-0.2907935  0.0635666]]. Action = [[ 0.04202655  0.07864824  0.15534604 -0.20958126]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 626 is [True, False, False, False, True, False]
State prediction error at timestep 626 is tensor(2.0772e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 626 of -1
Current timestep = 627. State = [[-0.29084125  0.06396563]]. Action = [[-0.2188862   0.01408929  0.01146978  0.83286643]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 627 is [True, False, False, False, True, False]
State prediction error at timestep 627 is tensor(5.6715e-07, grad_fn=<MseLossBackward0>)
Current timestep = 628. State = [[-0.29109395  0.06439635]]. Action = [[-0.20959342 -0.12095976 -0.11110559  0.26473713]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 628 is [True, False, False, False, True, False]
State prediction error at timestep 628 is tensor(4.1177e-05, grad_fn=<MseLossBackward0>)
Current timestep = 629. State = [[-0.29137695  0.0647558 ]]. Action = [[ 0.0358139   0.16286808 -0.01448929 -0.29576254]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 629 is [True, False, False, False, True, False]
State prediction error at timestep 629 is tensor(3.1191e-06, grad_fn=<MseLossBackward0>)
Current timestep = 630. State = [[-0.29150507  0.06519386]]. Action = [[-0.09067664 -0.216992   -0.21133004  0.97146344]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 630 is [True, False, False, False, True, False]
State prediction error at timestep 630 is tensor(7.5836e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 630 of -1
Current timestep = 631. State = [[-0.2919396   0.06602905]]. Action = [[-0.09859005 -0.05238342  0.1563139   0.8658234 ]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 631 is [True, False, False, False, True, False]
State prediction error at timestep 631 is tensor(4.3290e-07, grad_fn=<MseLossBackward0>)
Current timestep = 632. State = [[-0.2921325   0.06593816]]. Action = [[-0.19920027  0.03104833 -0.12033719  0.407457  ]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 632 is [True, False, False, False, True, False]
State prediction error at timestep 632 is tensor(1.4956e-05, grad_fn=<MseLossBackward0>)
Current timestep = 633. State = [[-0.29347596  0.06588725]]. Action = [[-0.02864888 -0.02874327 -0.10621074  0.40271664]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 633 is [True, False, False, False, True, False]
State prediction error at timestep 633 is tensor(8.0426e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 633 of -1
Current timestep = 634. State = [[-0.29399297  0.06568675]]. Action = [[-0.15018043  0.05290908  0.10555533 -0.17606127]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 634 is [True, False, False, False, True, False]
State prediction error at timestep 634 is tensor(9.5769e-06, grad_fn=<MseLossBackward0>)
Current timestep = 635. State = [[-0.2942734   0.06562719]]. Action = [[ 0.22036815  0.21221703  0.11024398 -0.82885015]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 635 is [True, False, False, False, True, False]
State prediction error at timestep 635 is tensor(5.8841e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 635 of -1
Current timestep = 636. State = [[-0.29472333  0.06549126]]. Action = [[ 0.10594192 -0.16582593 -0.23133965 -0.84211165]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 636 is [True, False, False, False, True, False]
State prediction error at timestep 636 is tensor(5.7169e-05, grad_fn=<MseLossBackward0>)
Current timestep = 637. State = [[-0.29513213  0.06535592]]. Action = [[-0.15757981  0.07069764 -0.00379255  0.8039534 ]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 637 is [True, False, False, False, True, False]
State prediction error at timestep 637 is tensor(4.0502e-06, grad_fn=<MseLossBackward0>)
Current timestep = 638. State = [[-0.2954568   0.06524317]]. Action = [[0.15304804 0.16690645 0.03041208 0.72464716]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 638 is [True, False, False, False, True, False]
State prediction error at timestep 638 is tensor(2.2068e-05, grad_fn=<MseLossBackward0>)
Current timestep = 639. State = [[-0.29584676  0.06511965]]. Action = [[ 0.09357765  0.21652696  0.08860707 -0.9334459 ]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 639 is [True, False, False, False, True, False]
State prediction error at timestep 639 is tensor(8.3317e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 639 of -1
Current timestep = 640. State = [[-0.29623494  0.0650567 ]]. Action = [[-0.19594157 -0.20596938  0.18591517  0.5039284 ]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 640 is [True, False, False, False, True, False]
State prediction error at timestep 640 is tensor(5.4661e-05, grad_fn=<MseLossBackward0>)
Current timestep = 641. State = [[-0.29645497  0.06503417]]. Action = [[-0.20209683  0.15416706 -0.19030735 -0.1581918 ]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 641 is [True, False, False, False, True, False]
State prediction error at timestep 641 is tensor(4.5808e-05, grad_fn=<MseLossBackward0>)
Current timestep = 642. State = [[-0.29698297  0.06488101]]. Action = [[ 0.22282648  0.23452413  0.23791784 -0.90967524]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 642 is [True, False, False, False, True, False]
State prediction error at timestep 642 is tensor(7.6711e-05, grad_fn=<MseLossBackward0>)
Current timestep = 643. State = [[-0.29720268  0.06485198]]. Action = [[ 0.18193904 -0.13731429 -0.11328797  0.88179994]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 643 is [True, False, False, False, True, False]
State prediction error at timestep 643 is tensor(6.7450e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 643 of -1
Current timestep = 644. State = [[-0.2977652   0.06475289]]. Action = [[ 0.12016881  0.00114867 -0.10290146  0.9573047 ]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 644 is [True, False, False, False, True, False]
State prediction error at timestep 644 is tensor(6.0956e-05, grad_fn=<MseLossBackward0>)
Current timestep = 645. State = [[-0.29774103  0.06476746]]. Action = [[-0.15677749 -0.1044829  -0.07909179  0.8982922 ]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 645 is [True, False, False, False, True, False]
State prediction error at timestep 645 is tensor(2.8426e-05, grad_fn=<MseLossBackward0>)
Current timestep = 646. State = [[-0.29774103  0.06476746]]. Action = [[-0.23420918  0.05220175  0.18771201  0.89343977]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 646 is [True, False, False, False, True, False]
State prediction error at timestep 646 is tensor(3.4009e-05, grad_fn=<MseLossBackward0>)
Current timestep = 647. State = [[-0.29776636  0.06484753]]. Action = [[-0.12278979  0.05400506 -0.12190023 -0.29915988]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 647 is [True, False, False, False, True, False]
State prediction error at timestep 647 is tensor(4.2670e-05, grad_fn=<MseLossBackward0>)
Current timestep = 648. State = [[-0.298117    0.06501536]]. Action = [[-0.24547096  0.1299091   0.18622085  0.13173592]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 648 is [True, False, False, False, True, False]
State prediction error at timestep 648 is tensor(3.2016e-05, grad_fn=<MseLossBackward0>)
Current timestep = 649. State = [[-0.29819223  0.06513844]]. Action = [[-0.0523835   0.23504102  0.17881677 -0.92127305]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 649 is [True, False, False, False, True, False]
State prediction error at timestep 649 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 650. State = [[-0.29820412  0.065378  ]]. Action = [[ 0.08316839 -0.18127087  0.07765794 -0.25109947]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 650 is [True, False, False, False, True, False]
State prediction error at timestep 650 is tensor(4.9351e-05, grad_fn=<MseLossBackward0>)
Current timestep = 651. State = [[-0.29854515  0.06604857]]. Action = [[ 0.10061452  0.1272862   0.22998762 -0.8633372 ]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 651 is [True, False, False, False, True, False]
State prediction error at timestep 651 is tensor(9.5694e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 651 of -1
Current timestep = 652. State = [[-0.2987828   0.06677741]]. Action = [[-0.23552448  0.07012996  0.05697599  0.8565886 ]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 652 is [True, False, False, False, True, False]
State prediction error at timestep 652 is tensor(2.2218e-05, grad_fn=<MseLossBackward0>)
Current timestep = 653. State = [[-0.2989899   0.06727122]]. Action = [[ 0.18477497 -0.243508    0.07086533  0.9802172 ]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 653 is [True, False, False, False, True, False]
State prediction error at timestep 653 is tensor(4.4190e-05, grad_fn=<MseLossBackward0>)
Current timestep = 654. State = [[-0.2991883   0.06771519]]. Action = [[ 0.03001183  0.1633941  -0.18297355  0.66329324]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 654 is [True, False, False, False, True, False]
State prediction error at timestep 654 is tensor(2.8383e-05, grad_fn=<MseLossBackward0>)
Current timestep = 655. State = [[-0.29947552  0.06826546]]. Action = [[0.13979459 0.16886318 0.10629323 0.11953461]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 655 is [True, False, False, False, True, False]
State prediction error at timestep 655 is tensor(9.1197e-05, grad_fn=<MseLossBackward0>)
Current timestep = 656. State = [[-0.29978558  0.06903805]]. Action = [[-0.19602309 -0.17448486 -0.15654959  0.0675596 ]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 656 is [True, False, False, False, True, False]
State prediction error at timestep 656 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 656 of -1
Current timestep = 657. State = [[-0.3001829   0.06974868]]. Action = [[ 0.03868243 -0.15289547 -0.15493995 -0.04143769]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 657 is [True, False, False, False, True, False]
State prediction error at timestep 657 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 658. State = [[-0.30070066  0.07111624]]. Action = [[ 0.08754912  0.12296325  0.16632241 -0.41251558]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 658 is [True, False, False, False, True, False]
State prediction error at timestep 658 is tensor(4.6066e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 658 of -1
Current timestep = 659. State = [[-0.30041346  0.07282452]]. Action = [[-0.21774538 -0.17545663 -0.04524201 -0.28563786]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 659 is [True, False, False, False, True, False]
State prediction error at timestep 659 is tensor(8.4293e-05, grad_fn=<MseLossBackward0>)
Current timestep = 660. State = [[-0.3002215   0.07350306]]. Action = [[ 0.12397185 -0.1850183   0.21667725  0.4065385 ]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 660 is [True, False, False, False, True, False]
State prediction error at timestep 660 is tensor(2.0773e-05, grad_fn=<MseLossBackward0>)
Current timestep = 661. State = [[-0.30012536  0.0738228 ]]. Action = [[-0.19205163  0.16822281 -0.15435058  0.5021827 ]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 661 is [True, False, False, False, True, False]
State prediction error at timestep 661 is tensor(5.6353e-05, grad_fn=<MseLossBackward0>)
Current timestep = 662. State = [[-0.3000046   0.07453173]]. Action = [[ 0.23705155 -0.16119066  0.04315263  0.6200398 ]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 662 is [True, False, False, False, True, False]
State prediction error at timestep 662 is tensor(1.1664e-06, grad_fn=<MseLossBackward0>)
Current timestep = 663. State = [[-0.2997404   0.07689975]]. Action = [[-0.0278433   0.10906029 -0.05834362  0.7747996 ]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 663 is [True, False, False, False, True, False]
State prediction error at timestep 663 is tensor(1.7553e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 663 of -1
Current timestep = 664. State = [[-0.30068436  0.08097416]]. Action = [[-0.00738215 -0.04134569  0.16000599 -0.86902523]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 664 is [True, False, False, False, True, False]
State prediction error at timestep 664 is tensor(2.8662e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 664 of -1
Current timestep = 665. State = [[-0.30089158  0.08162104]]. Action = [[ 0.22765607  0.05740744 -0.16516322  0.922343  ]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 665 is [True, False, False, False, True, False]
State prediction error at timestep 665 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 666. State = [[-0.3009695   0.08188538]]. Action = [[-0.18164481 -0.20138635 -0.11036563  0.17288923]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 666 is [True, False, False, False, True, False]
State prediction error at timestep 666 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 666 of -1
Current timestep = 667. State = [[-0.3011532   0.08295731]]. Action = [[0.13182819 0.10695481 0.07825857 0.30747437]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 667 is [True, False, False, False, True, False]
State prediction error at timestep 667 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 668. State = [[-0.30066565  0.08378407]]. Action = [[ 0.16575265 -0.18344876  0.08410275 -0.35190976]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 668 is [True, False, False, False, True, False]
State prediction error at timestep 668 is tensor(5.3975e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 668 of -1
Current timestep = 669. State = [[-0.29973856  0.08463873]]. Action = [[ 0.2124236   0.00135329 -0.02132408 -0.918388  ]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 669 is [True, False, False, False, True, False]
State prediction error at timestep 669 is tensor(5.9947e-05, grad_fn=<MseLossBackward0>)
Current timestep = 670. State = [[-0.29689535  0.08720034]]. Action = [[0.10528541 0.04905286 0.02103785 0.10559714]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 670 is [True, False, False, False, True, False]
State prediction error at timestep 670 is tensor(9.5448e-05, grad_fn=<MseLossBackward0>)
Current timestep = 671. State = [[-0.2957421   0.08803718]]. Action = [[ 0.03970751  0.23092481 -0.22865006  0.32198203]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 671 is [True, False, False, False, True, False]
State prediction error at timestep 671 is tensor(2.8667e-05, grad_fn=<MseLossBackward0>)
Current timestep = 672. State = [[-0.2951446   0.08853938]]. Action = [[ 0.13288409 -0.1628832   0.16374338 -0.11886847]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 672 is [True, False, False, False, True, False]
State prediction error at timestep 672 is tensor(2.6240e-05, grad_fn=<MseLossBackward0>)
Current timestep = 673. State = [[-0.29415864  0.08913355]]. Action = [[-0.1201636  -0.17873853  0.13148281 -0.9338147 ]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 673 is [True, False, False, False, True, False]
State prediction error at timestep 673 is tensor(3.5894e-05, grad_fn=<MseLossBackward0>)
Current timestep = 674. State = [[-0.2912998   0.09134626]]. Action = [[ 0.09256333 -0.10122681  0.16356859 -0.59297246]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 674 is [True, False, False, False, True, False]
State prediction error at timestep 674 is tensor(6.8374e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 674 of -1
Current timestep = 675. State = [[-0.29020622  0.09146059]]. Action = [[ 0.0780884   0.17785993 -0.05511424  0.35959172]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 675 is [True, False, False, False, True, False]
State prediction error at timestep 675 is tensor(4.4665e-05, grad_fn=<MseLossBackward0>)
Current timestep = 676. State = [[-0.2895636  0.0914575]]. Action = [[ 0.14214665 -0.1545031  -0.2263903   0.4084283 ]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 676 is [True, False, False, False, True, False]
State prediction error at timestep 676 is tensor(8.0769e-05, grad_fn=<MseLossBackward0>)
Current timestep = 677. State = [[-0.28878802  0.09144389]]. Action = [[ 0.22994894 -0.01655549 -0.1412719  -0.25194883]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 677 is [True, False, False, False, True, False]
State prediction error at timestep 677 is tensor(2.2695e-05, grad_fn=<MseLossBackward0>)
Current timestep = 678. State = [[-0.28812587  0.09176912]]. Action = [[ 0.1501599  -0.11928564 -0.17026304  0.1942786 ]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 678 is [True, False, False, False, True, False]
State prediction error at timestep 678 is tensor(4.6332e-05, grad_fn=<MseLossBackward0>)
Current timestep = 679. State = [[-0.2853839   0.09250338]]. Action = [[-0.01758954 -0.03382292 -0.09932303 -0.91591215]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 679 is [True, False, False, False, True, False]
State prediction error at timestep 679 is tensor(9.0415e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 679 of 1
Current timestep = 680. State = [[-0.28494513  0.09250395]]. Action = [[-0.01247124  0.20704937 -0.22351228  0.08365595]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 680 is [True, False, False, False, True, False]
State prediction error at timestep 680 is tensor(5.7639e-05, grad_fn=<MseLossBackward0>)
Current timestep = 681. State = [[-0.28484035  0.09231562]]. Action = [[-0.05052775 -0.21603702 -0.16975968 -0.914758  ]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 681 is [True, False, False, False, True, False]
State prediction error at timestep 681 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 681 of 1
Current timestep = 682. State = [[-0.28375486  0.09219866]]. Action = [[-0.08402604 -0.12438899  0.14012891 -0.97069615]]. Reward = [0.]
Curr episode timestep = 682
Scene graph at timestep 682 is [True, False, False, False, True, False]
State prediction error at timestep 682 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 683. State = [[-0.2833486   0.09104894]]. Action = [[ 0.04896608 -0.17144501 -0.04980329  0.5096066 ]]. Reward = [0.]
Curr episode timestep = 683
Scene graph at timestep 683 is [True, False, False, False, True, False]
State prediction error at timestep 683 is tensor(6.7056e-05, grad_fn=<MseLossBackward0>)
Current timestep = 684. State = [[-0.28315365  0.09020884]]. Action = [[ 0.20059747  0.19816521 -0.09289539  0.48076212]]. Reward = [0.]
Curr episode timestep = 684
Scene graph at timestep 684 is [True, False, False, False, True, False]
State prediction error at timestep 684 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 684 of 1
Current timestep = 685. State = [[-0.2830079   0.08771449]]. Action = [[-0.07166407  0.08698076 -0.22665936 -0.4449227 ]]. Reward = [0.]
Curr episode timestep = 685
Scene graph at timestep 685 is [True, False, False, False, True, False]
State prediction error at timestep 685 is tensor(4.7681e-05, grad_fn=<MseLossBackward0>)
Current timestep = 686. State = [[-0.28321302  0.08804361]]. Action = [[-0.19198239  0.11918864 -0.1631468  -0.52333486]]. Reward = [0.]
Curr episode timestep = 686
Scene graph at timestep 686 is [True, False, False, False, True, False]
State prediction error at timestep 686 is tensor(4.0480e-05, grad_fn=<MseLossBackward0>)
Current timestep = 687. State = [[-0.28349975  0.08849648]]. Action = [[-0.13508946  0.16293752 -0.22182825 -0.7607474 ]]. Reward = [0.]
Curr episode timestep = 687
Scene graph at timestep 687 is [True, False, False, False, True, False]
State prediction error at timestep 687 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 688. State = [[-0.28354236  0.08852343]]. Action = [[-0.04997283  0.22923797  0.20362341  0.09537327]]. Reward = [0.]
Curr episode timestep = 688
Scene graph at timestep 688 is [True, False, False, False, True, False]
State prediction error at timestep 688 is tensor(6.1211e-05, grad_fn=<MseLossBackward0>)
Current timestep = 689. State = [[-0.28343195  0.08835734]]. Action = [[-0.15102206 -0.08197445  0.20168889 -0.9576107 ]]. Reward = [0.]
Curr episode timestep = 689
Scene graph at timestep 689 is [True, False, False, False, True, False]
State prediction error at timestep 689 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 690. State = [[-0.28363103  0.08852246]]. Action = [[ 0.07450169 -0.22431132  0.04579571  0.46562552]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 690 is [True, False, False, False, True, False]
State prediction error at timestep 690 is tensor(4.8137e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 690 of 1
Current timestep = 691. State = [[-0.28378057  0.0888287 ]]. Action = [[ 0.09151193  0.07313582 -0.07507494  0.17263448]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 691 is [True, False, False, False, True, False]
State prediction error at timestep 691 is tensor(3.2136e-05, grad_fn=<MseLossBackward0>)
Current timestep = 692. State = [[-0.28384396  0.08942842]]. Action = [[ 0.04490361  0.01758879  0.09526539 -0.67382306]]. Reward = [0.]
Curr episode timestep = 692
Scene graph at timestep 692 is [True, False, False, False, True, False]
State prediction error at timestep 692 is tensor(3.5983e-05, grad_fn=<MseLossBackward0>)
Current timestep = 693. State = [[-0.28383937  0.08962278]]. Action = [[-0.21864091 -0.20804307  0.19058132 -0.84557986]]. Reward = [0.]
Curr episode timestep = 693
Scene graph at timestep 693 is [True, False, False, False, True, False]
State prediction error at timestep 693 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 694. State = [[-0.28385863  0.08980282]]. Action = [[-0.18687265  0.01900241 -0.16766563  0.43475103]]. Reward = [0.]
Curr episode timestep = 694
Scene graph at timestep 694 is [True, False, False, False, True, False]
State prediction error at timestep 694 is tensor(3.6866e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 694 of 1
Current timestep = 695. State = [[-0.2838779   0.08998245]]. Action = [[ 0.18165743  0.19276881 -0.22137357  0.18929613]]. Reward = [0.]
Curr episode timestep = 695
Scene graph at timestep 695 is [True, False, False, False, True, False]
State prediction error at timestep 695 is tensor(3.7748e-05, grad_fn=<MseLossBackward0>)
Current timestep = 696. State = [[-0.28389084  0.09010248]]. Action = [[-0.23581706  0.19571522  0.01259932 -0.39297223]]. Reward = [0.]
Curr episode timestep = 696
Scene graph at timestep 696 is [True, False, False, False, True, False]
State prediction error at timestep 696 is tensor(1.7764e-05, grad_fn=<MseLossBackward0>)
Current timestep = 697. State = [[-0.28391024  0.0902821 ]]. Action = [[0.10840949 0.14926112 0.15922791 0.2830664 ]]. Reward = [0.]
Curr episode timestep = 697
Scene graph at timestep 697 is [True, False, False, False, True, False]
State prediction error at timestep 697 is tensor(2.1094e-05, grad_fn=<MseLossBackward0>)
Current timestep = 698. State = [[-0.28392324  0.09040213]]. Action = [[-0.2316393  -0.23559773  0.21343386 -0.07256287]]. Reward = [0.]
Curr episode timestep = 698
Scene graph at timestep 698 is [True, False, False, False, True, False]
State prediction error at timestep 698 is tensor(3.8546e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 698 of 1
Current timestep = 699. State = [[-0.2839363   0.09052215]]. Action = [[ 0.18891734  0.04274654 -0.15648718  0.44155812]]. Reward = [0.]
Curr episode timestep = 699
Scene graph at timestep 699 is [True, False, False, False, True, False]
State prediction error at timestep 699 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 700. State = [[-0.2839328   0.09086742]]. Action = [[ 0.05161017 -0.07856825  0.07891786  0.10843229]]. Reward = [0.]
Curr episode timestep = 700
Scene graph at timestep 700 is [True, False, False, False, True, False]
State prediction error at timestep 700 is tensor(2.0519e-05, grad_fn=<MseLossBackward0>)
Current timestep = 701. State = [[-0.28344268  0.0906378 ]]. Action = [[ 0.00442955 -0.23767151 -0.17700374  0.30620635]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 701 is [True, False, False, False, True, False]
State prediction error at timestep 701 is tensor(4.8507e-05, grad_fn=<MseLossBackward0>)
Current timestep = 702. State = [[-0.28296202  0.09048995]]. Action = [[ 0.23647761  0.2287705  -0.04315367 -0.20476836]]. Reward = [0.]
Curr episode timestep = 702
Scene graph at timestep 702 is [True, False, False, False, True, False]
State prediction error at timestep 702 is tensor(2.7657e-05, grad_fn=<MseLossBackward0>)
Current timestep = 703. State = [[-0.28282493  0.09037828]]. Action = [[ 0.22333533 -0.00333524  0.15510654 -0.77923703]]. Reward = [0.]
Curr episode timestep = 703
Scene graph at timestep 703 is [True, False, False, False, True, False]
State prediction error at timestep 703 is tensor(3.3582e-05, grad_fn=<MseLossBackward0>)
Current timestep = 704. State = [[-0.28261855  0.09035763]]. Action = [[-0.23203254  0.18719983  0.15770137 -0.15888882]]. Reward = [0.]
Curr episode timestep = 704
Scene graph at timestep 704 is [True, False, False, False, True, False]
State prediction error at timestep 704 is tensor(6.9999e-05, grad_fn=<MseLossBackward0>)
Current timestep = 705. State = [[-0.28137547  0.09046923]]. Action = [[-0.01499969  0.01120925  0.22745574 -0.42645937]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 705 is [True, False, False, False, True, False]
State prediction error at timestep 705 is tensor(3.4678e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 705 of 1
Current timestep = 706. State = [[-0.28117946  0.09053784]]. Action = [[0.1288696  0.10749009 0.08108222 0.5212922 ]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 706 is [True, False, False, False, True, False]
State prediction error at timestep 706 is tensor(7.6865e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 706 of 1
Current timestep = 707. State = [[-0.27786836  0.09185212]]. Action = [[ 0.01020652  0.03497452  0.15412515 -0.38526845]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 707 is [True, False, False, False, True, False]
State prediction error at timestep 707 is tensor(6.4159e-05, grad_fn=<MseLossBackward0>)
Current timestep = 708. State = [[-0.2746872   0.09367082]]. Action = [[ 0.19305551 -0.11440031 -0.07277545  0.9151206 ]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 708 is [True, False, False, False, True, False]
State prediction error at timestep 708 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 709. State = [[-0.27390075  0.09410431]]. Action = [[-0.21580644  0.15378052  0.05469871 -0.29690087]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 709 is [True, False, False, False, True, False]
State prediction error at timestep 709 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 710. State = [[-0.273436    0.09449667]]. Action = [[ 0.18303022  0.03550142 -0.11146215 -0.6309261 ]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 710 is [True, False, False, False, True, False]
State prediction error at timestep 710 is tensor(8.0272e-05, grad_fn=<MseLossBackward0>)
Current timestep = 711. State = [[-0.27321023  0.09453936]]. Action = [[ 0.12886006  0.21026957 -0.1921541   0.97026145]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 711 is [True, False, False, False, True, False]
State prediction error at timestep 711 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 712. State = [[-0.27277845  0.094707  ]]. Action = [[ 0.22612557  0.09444442  0.08478567 -0.6691187 ]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 712 is [True, False, False, False, True, False]
State prediction error at timestep 712 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 712 of 1
Current timestep = 713. State = [[-0.27132058  0.09543723]]. Action = [[ 0.13128161  0.03150639 -0.13183594 -0.291121  ]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 713 is [True, False, False, False, True, False]
State prediction error at timestep 713 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 714. State = [[-0.27064738  0.09572213]]. Action = [[ 0.06605306 -0.15158859 -0.19693933  0.33872294]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 714 is [True, False, False, False, True, False]
State prediction error at timestep 714 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 715. State = [[-0.2698251  0.0960478]]. Action = [[ 0.12058896 -0.20187195 -0.13355759  0.04294527]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 715 is [True, False, False, False, True, False]
State prediction error at timestep 715 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 716. State = [[-0.2690484  0.0962286]]. Action = [[ 0.2100004   0.01693311  0.02325889 -0.11784309]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 716 is [True, False, False, False, True, False]
State prediction error at timestep 716 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 717. State = [[-0.26658887  0.0971996 ]]. Action = [[-0.09165812  0.05448562  0.00506395 -0.09464967]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 717 is [True, False, False, False, True, False]
State prediction error at timestep 717 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 717 of 1
Current timestep = 718. State = [[-0.26663247  0.09760906]]. Action = [[ 0.12409446 -0.22618607 -0.03383246 -0.34332025]]. Reward = [0.]
Curr episode timestep = 718
Scene graph at timestep 718 is [True, False, False, False, True, False]
State prediction error at timestep 718 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 719. State = [[-0.26675794  0.09832737]]. Action = [[-0.09083512 -0.20049557 -0.19240607 -0.5326834 ]]. Reward = [0.]
Curr episode timestep = 719
Scene graph at timestep 719 is [True, False, False, False, True, False]
State prediction error at timestep 719 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 720. State = [[-0.266765    0.09884828]]. Action = [[-0.04635803 -0.23451929 -0.03122041 -0.8906468 ]]. Reward = [0.]
Curr episode timestep = 720
Scene graph at timestep 720 is [True, False, False, False, True, False]
State prediction error at timestep 720 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 720 of 1
Current timestep = 721. State = [[-0.2669399   0.09923224]]. Action = [[ 0.16598731  0.16833997 -0.13548894  0.6652584 ]]. Reward = [0.]
Curr episode timestep = 721
Scene graph at timestep 721 is [True, False, False, False, True, False]
State prediction error at timestep 721 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 722. State = [[-0.26725405  0.09971964]]. Action = [[-0.06397581 -0.17701292 -0.04466064 -0.18559468]]. Reward = [0.]
Curr episode timestep = 722
Scene graph at timestep 722 is [True, False, False, False, True, False]
State prediction error at timestep 722 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 723. State = [[-0.2673304   0.10018797]]. Action = [[ 0.13173175  0.10371158  0.06125498 -0.14296913]]. Reward = [0.]
Curr episode timestep = 723
Scene graph at timestep 723 is [True, False, False, False, True, False]
State prediction error at timestep 723 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 723 of 1
Current timestep = 724. State = [[-0.2666251   0.10113334]]. Action = [[-0.17956342 -0.14994147 -0.09869608  0.4936781 ]]. Reward = [0.]
Curr episode timestep = 724
Scene graph at timestep 724 is [True, False, False, False, True, False]
State prediction error at timestep 724 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 725. State = [[-0.26602876  0.10168184]]. Action = [[-0.09837247 -0.18180113  0.07253659 -0.88765174]]. Reward = [0.]
Curr episode timestep = 725
Scene graph at timestep 725 is [True, False, False, False, True, False]
State prediction error at timestep 725 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 726. State = [[-0.26548555  0.10220538]]. Action = [[ 0.17904109 -0.04124495 -0.21907279  0.66819704]]. Reward = [0.]
Curr episode timestep = 726
Scene graph at timestep 726 is [True, False, False, False, True, False]
State prediction error at timestep 726 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 727. State = [[-0.26512754  0.10242   ]]. Action = [[ 0.16915703 -0.06058797 -0.10899343 -0.62991726]]. Reward = [0.]
Curr episode timestep = 727
Scene graph at timestep 727 is [True, False, False, False, True, False]
State prediction error at timestep 727 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 727 of 1
Current timestep = 728. State = [[-0.26260757  0.1049628 ]]. Action = [[-0.0251381   0.07664317  0.23258081  0.71089494]]. Reward = [0.]
Curr episode timestep = 728
Scene graph at timestep 728 is [True, False, False, False, True, False]
State prediction error at timestep 728 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 729. State = [[-0.26232094  0.10611394]]. Action = [[-0.13453303  0.2313641   0.12034816  0.8453455 ]]. Reward = [0.]
Curr episode timestep = 729
Scene graph at timestep 729 is [True, False, False, False, True, False]
State prediction error at timestep 729 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 729 of 1
Current timestep = 730. State = [[-0.26183173  0.10954119]]. Action = [[-0.02096425 -0.09730728  0.1510644   0.68477297]]. Reward = [0.]
Curr episode timestep = 730
Scene graph at timestep 730 is [True, False, False, False, True, False]
State prediction error at timestep 730 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 731. State = [[-0.26166448  0.1097104 ]]. Action = [[-0.02001248  0.19636518  0.15885946 -0.4490713 ]]. Reward = [0.]
Curr episode timestep = 731
Scene graph at timestep 731 is [True, False, False, False, True, False]
State prediction error at timestep 731 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 731 of 1
Current timestep = 732. State = [[-0.26116452  0.10950992]]. Action = [[ 0.19814578  0.18450159 -0.02737279 -0.881098  ]]. Reward = [0.]
Curr episode timestep = 732
Scene graph at timestep 732 is [True, False, False, False, True, False]
State prediction error at timestep 732 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 733. State = [[-0.26112905  0.10952412]]. Action = [[-0.09206277 -0.24311371 -0.21935885  0.80424607]]. Reward = [0.]
Curr episode timestep = 733
Scene graph at timestep 733 is [True, False, False, False, True, False]
State prediction error at timestep 733 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 734. State = [[-0.2613813   0.10972023]]. Action = [[-0.00976422  0.11147767 -0.15849297 -0.9293273 ]]. Reward = [0.]
Curr episode timestep = 734
Scene graph at timestep 734 is [True, False, False, False, True, False]
State prediction error at timestep 734 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 734 of 1
Current timestep = 735. State = [[-0.26137406  0.11046795]]. Action = [[ 0.15977931 -0.0544745  -0.20228037 -0.17728877]]. Reward = [0.]
Curr episode timestep = 735
Scene graph at timestep 735 is [True, False, False, False, True, False]
State prediction error at timestep 735 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 736. State = [[-0.26132098  0.11156761]]. Action = [[ 0.09816283 -0.15455185 -0.10899127 -0.68219554]]. Reward = [0.]
Curr episode timestep = 736
Scene graph at timestep 736 is [True, False, False, False, True, False]
State prediction error at timestep 736 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 737. State = [[-0.2615368   0.11173434]]. Action = [[ 0.20691743 -0.09271222 -0.1726293   0.8114147 ]]. Reward = [0.]
Curr episode timestep = 737
Scene graph at timestep 737 is [True, False, False, False, True, False]
State prediction error at timestep 737 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 738. State = [[-0.2618962   0.11315286]]. Action = [[-0.11164461  0.07402641 -0.07396719 -0.08458656]]. Reward = [0.]
Curr episode timestep = 738
Scene graph at timestep 738 is [True, False, False, False, True, False]
State prediction error at timestep 738 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 738 of 1
Current timestep = 739. State = [[-0.26248947  0.11453724]]. Action = [[ 0.10196242  0.20354551  0.16122955 -0.88231695]]. Reward = [0.]
Curr episode timestep = 739
Scene graph at timestep 739 is [True, False, False, False, True, False]
State prediction error at timestep 739 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 740. State = [[-0.26301214  0.11571658]]. Action = [[-0.04371195  0.2388635   0.18201256 -0.90638375]]. Reward = [0.]
Curr episode timestep = 740
Scene graph at timestep 740 is [True, False, False, False, True, False]
State prediction error at timestep 740 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 740 of 1
Current timestep = 741. State = [[-0.26490363  0.11890432]]. Action = [[0.04221991 0.12878776 0.01327556 0.3202486 ]]. Reward = [0.]
Curr episode timestep = 741
Scene graph at timestep 741 is [True, False, False, False, True, False]
State prediction error at timestep 741 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 742. State = [[-0.26550013  0.12023599]]. Action = [[-0.13794212 -0.17056918  0.02900085  0.07478321]]. Reward = [0.]
Curr episode timestep = 742
Scene graph at timestep 742 is [True, False, False, False, True, False]
State prediction error at timestep 742 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 743. State = [[-0.2659013   0.12150162]]. Action = [[-0.0715639   0.24608207  0.18956035  0.09609175]]. Reward = [0.]
Curr episode timestep = 743
Scene graph at timestep 743 is [True, False, False, False, True, False]
State prediction error at timestep 743 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 744. State = [[-0.2663021   0.12298691]]. Action = [[-0.07773705 -0.21516596  0.11423922  0.97733307]]. Reward = [0.]
Curr episode timestep = 744
Scene graph at timestep 744 is [True, False, False, False, True, False]
State prediction error at timestep 744 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 745. State = [[-0.26656157  0.12392965]]. Action = [[ 0.09255207  0.19236308  0.11641753 -0.7084121 ]]. Reward = [0.]
Curr episode timestep = 745
Scene graph at timestep 745 is [True, False, False, False, True, False]
State prediction error at timestep 745 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 745 of 1
Current timestep = 746. State = [[-0.266723    0.12475931]]. Action = [[ 0.16152394  0.07539734  0.2031922  -0.8714346 ]]. Reward = [0.]
Curr episode timestep = 746
Scene graph at timestep 746 is [True, False, False, False, True, False]
State prediction error at timestep 746 is tensor(8.1160e-05, grad_fn=<MseLossBackward0>)
Current timestep = 747. State = [[-0.2668908   0.12582482]]. Action = [[ 0.21688193 -0.08467476  0.1208235  -0.25345325]]. Reward = [0.]
Curr episode timestep = 747
Scene graph at timestep 747 is [True, False, False, False, False, True]
State prediction error at timestep 747 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 748. State = [[-0.2668174   0.12626016]]. Action = [[-0.15180646 -0.11742961  0.05899981 -0.13482583]]. Reward = [0.]
Curr episode timestep = 748
Scene graph at timestep 748 is [True, False, False, False, False, True]
State prediction error at timestep 748 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 749. State = [[-0.26681283  0.12718867]]. Action = [[-0.24573138  0.19272432 -0.10235842  0.00573897]]. Reward = [0.]
Curr episode timestep = 749
Scene graph at timestep 749 is [True, False, False, False, False, True]
State prediction error at timestep 749 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 749 of 1
Current timestep = 750. State = [[-0.2668595   0.12766863]]. Action = [[ 0.19866937  0.22128874  0.03964886 -0.95715445]]. Reward = [0.]
Curr episode timestep = 750
Scene graph at timestep 750 is [True, False, False, False, False, True]
State prediction error at timestep 750 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 751. State = [[-0.26689798  0.12787288]]. Action = [[ 0.13569736 -0.22343363 -0.12783414  0.4153812 ]]. Reward = [0.]
Curr episode timestep = 751
Scene graph at timestep 751 is [True, False, False, False, False, True]
State prediction error at timestep 751 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 752. State = [[-0.26711103  0.12817532]]. Action = [[-0.21752776  0.07833442 -0.0138811  -0.8142662 ]]. Reward = [0.]
Curr episode timestep = 752
Scene graph at timestep 752 is [True, False, False, False, False, True]
State prediction error at timestep 752 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 752 of 1
Current timestep = 753. State = [[-0.2671025   0.12857293]]. Action = [[ 0.14315033 -0.14425534 -0.07023904 -0.5464256 ]]. Reward = [0.]
Curr episode timestep = 753
Scene graph at timestep 753 is [True, False, False, False, False, True]
State prediction error at timestep 753 is tensor(8.7736e-05, grad_fn=<MseLossBackward0>)
Current timestep = 754. State = [[-0.2669579   0.12890026]]. Action = [[ 0.1734114   0.2180959   0.11651045 -0.29171288]]. Reward = [0.]
Curr episode timestep = 754
Scene graph at timestep 754 is [True, False, False, False, False, True]
State prediction error at timestep 754 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 754 of 1
Current timestep = 755. State = [[-0.26710105  0.12905817]]. Action = [[-0.19457202  0.16233867 -0.05666274 -0.72177756]]. Reward = [0.]
Curr episode timestep = 755
Scene graph at timestep 755 is [True, False, False, False, False, True]
State prediction error at timestep 755 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 756. State = [[-0.2669573   0.12916055]]. Action = [[-0.10417727  0.19007975  0.02440977  0.16034675]]. Reward = [0.]
Curr episode timestep = 756
Scene graph at timestep 756 is [True, False, False, False, False, True]
State prediction error at timestep 756 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 757. State = [[-0.26692986  0.12935497]]. Action = [[ 0.04210809  0.14413762 -0.02098319  0.41866422]]. Reward = [0.]
Curr episode timestep = 757
Scene graph at timestep 757 is [True, False, False, False, False, True]
State prediction error at timestep 757 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 758. State = [[-0.26699996  0.1296767 ]]. Action = [[ 0.09295636  0.09104609 -0.09077667  0.47860956]]. Reward = [0.]
Curr episode timestep = 758
Scene graph at timestep 758 is [True, False, False, False, False, True]
State prediction error at timestep 758 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 758 of 1
Current timestep = 759. State = [[-0.26649696  0.13023347]]. Action = [[ 0.00644886  0.16073668 -0.1804522   0.43099916]]. Reward = [0.]
Curr episode timestep = 759
Scene graph at timestep 759 is [True, False, False, False, False, True]
State prediction error at timestep 759 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 760. State = [[-0.26375622  0.13318537]]. Action = [[ 0.06590256  0.05923352 -0.2121819  -0.4736433 ]]. Reward = [0.]
Curr episode timestep = 760
Scene graph at timestep 760 is [True, False, False, False, False, True]
State prediction error at timestep 760 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 761. State = [[-0.26278058  0.13438977]]. Action = [[0.07017809 0.20473278 0.140106   0.7083869 ]]. Reward = [0.]
Curr episode timestep = 761
Scene graph at timestep 761 is [True, False, False, False, False, True]
State prediction error at timestep 761 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 762. State = [[-0.26200533  0.1349984 ]]. Action = [[ 0.21229497  0.01857769  0.09010226 -0.9641019 ]]. Reward = [0.]
Curr episode timestep = 762
Scene graph at timestep 762 is [True, False, False, False, False, True]
State prediction error at timestep 762 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 763. State = [[-0.25952053  0.13717519]]. Action = [[ 0.10164145 -0.1257178   0.20171574 -0.94864005]]. Reward = [0.]
Curr episode timestep = 763
Scene graph at timestep 763 is [True, False, False, False, False, True]
State prediction error at timestep 763 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 763 of 1
Current timestep = 764. State = [[-0.25835162  0.1370224 ]]. Action = [[ 0.24583715 -0.22260404  0.23855448 -0.79377633]]. Reward = [0.]
Curr episode timestep = 764
Scene graph at timestep 764 is [True, False, False, False, False, True]
State prediction error at timestep 764 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 765. State = [[-0.2568323   0.13660978]]. Action = [[ 0.18665296  0.24490571 -0.1883906  -0.4178328 ]]. Reward = [0.]
Curr episode timestep = 765
Scene graph at timestep 765 is [True, False, False, False, False, True]
State prediction error at timestep 765 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 765 of 1
Current timestep = 766. State = [[-0.25430712  0.1370128 ]]. Action = [[ 0.03573123  0.08576357 -0.05564258 -0.18286061]]. Reward = [0.]
Curr episode timestep = 766
Scene graph at timestep 766 is [True, False, False, False, False, True]
State prediction error at timestep 766 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 767. State = [[-0.2540188   0.13748997]]. Action = [[-0.0890355  -0.22257952 -0.1699431   0.20922768]]. Reward = [0.]
Curr episode timestep = 767
Scene graph at timestep 767 is [True, False, False, False, False, True]
State prediction error at timestep 767 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 768. State = [[-0.25166103  0.1388189 ]]. Action = [[ 0.04742959 -0.12647025 -0.03610535 -0.8168203 ]]. Reward = [0.]
Curr episode timestep = 768
Scene graph at timestep 768 is [True, False, False, False, False, True]
State prediction error at timestep 768 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 769. State = [[-0.25079507  0.1383166 ]]. Action = [[-0.21024515  0.09834236 -0.00267126  0.7772132 ]]. Reward = [0.]
Curr episode timestep = 769
Scene graph at timestep 769 is [True, False, False, False, False, True]
State prediction error at timestep 769 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 770. State = [[-0.25028798  0.13798499]]. Action = [[ 0.14761299  0.20594716 -0.08416891  0.5484102 ]]. Reward = [0.]
Curr episode timestep = 770
Scene graph at timestep 770 is [True, False, False, False, False, True]
State prediction error at timestep 770 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 771. State = [[-0.24968947  0.1374248 ]]. Action = [[ 0.22547597  0.00562119  0.2202735  -0.43221056]]. Reward = [0.]
Curr episode timestep = 771
Scene graph at timestep 771 is [True, False, False, False, False, True]
State prediction error at timestep 771 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 772. State = [[-0.2486832   0.13636526]]. Action = [[-0.03935283 -0.04295126  0.03921983 -0.29556906]]. Reward = [0.]
Curr episode timestep = 772
Scene graph at timestep 772 is [True, False, False, False, False, True]
State prediction error at timestep 772 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 772 of 1
Current timestep = 773. State = [[-0.24857728  0.13578576]]. Action = [[ 0.03055543  0.21369922 -0.21834643 -0.7673634 ]]. Reward = [0.]
Curr episode timestep = 773
Scene graph at timestep 773 is [True, False, False, False, False, True]
State prediction error at timestep 773 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 774. State = [[-0.24841158  0.13532777]]. Action = [[-0.22491771  0.24657899  0.01000619 -0.69478244]]. Reward = [0.]
Curr episode timestep = 774
Scene graph at timestep 774 is [True, False, False, False, False, True]
State prediction error at timestep 774 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 775. State = [[-0.248361    0.13507167]]. Action = [[-0.11877099  0.24174619 -0.14711596 -0.36672044]]. Reward = [0.]
Curr episode timestep = 775
Scene graph at timestep 775 is [True, False, False, False, False, True]
State prediction error at timestep 775 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 776. State = [[-0.24830207  0.134671  ]]. Action = [[-0.11791886 -0.14005883  0.12013742 -0.309173  ]]. Reward = [0.]
Curr episode timestep = 776
Scene graph at timestep 776 is [True, False, False, False, False, True]
State prediction error at timestep 776 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 777. State = [[-0.24820709  0.13385063]]. Action = [[-0.07597727  0.06307277  0.06122097 -0.04610926]]. Reward = [0.]
Curr episode timestep = 777
Scene graph at timestep 777 is [True, False, False, False, False, True]
State prediction error at timestep 777 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 777 of 1
Current timestep = 778. State = [[-0.24842685  0.13420019]]. Action = [[ 0.16851282 -0.09139729  0.22384214 -0.05880338]]. Reward = [0.]
Curr episode timestep = 778
Scene graph at timestep 778 is [True, False, False, False, False, True]
State prediction error at timestep 778 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 778 of 1
Current timestep = 779. State = [[-0.24854782  0.13453151]]. Action = [[ 0.15927786  0.13577455 -0.08218542  0.89365697]]. Reward = [0.]
Curr episode timestep = 779
Scene graph at timestep 779 is [True, False, False, False, False, True]
State prediction error at timestep 779 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 780. State = [[-0.24857807  0.13440272]]. Action = [[ 0.12074146 -0.1384416  -0.07001823 -0.4735366 ]]. Reward = [0.]
Curr episode timestep = 780
Scene graph at timestep 780 is [True, False, False, False, False, True]
State prediction error at timestep 780 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 781. State = [[-0.24877083  0.13435283]]. Action = [[-0.14107758 -0.01827039 -0.14190143 -0.64683634]]. Reward = [0.]
Curr episode timestep = 781
Scene graph at timestep 781 is [True, False, False, False, False, True]
State prediction error at timestep 781 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 782. State = [[-0.24889627  0.13468637]]. Action = [[-0.19472903 -0.02106266  0.21729964 -0.05057871]]. Reward = [0.]
Curr episode timestep = 782
Scene graph at timestep 782 is [True, False, False, False, False, True]
State prediction error at timestep 782 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 782 of 1
Current timestep = 783. State = [[-0.24892803  0.13460504]]. Action = [[ 0.07007763 -0.22845212 -0.24152951  0.9362776 ]]. Reward = [0.]
Curr episode timestep = 783
Scene graph at timestep 783 is [True, False, False, False, False, True]
State prediction error at timestep 783 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 784. State = [[-0.24892803  0.13460504]]. Action = [[-0.14414094 -0.24672258 -0.0859078   0.08593464]]. Reward = [0.]
Curr episode timestep = 784
Scene graph at timestep 784 is [True, False, False, False, False, True]
State prediction error at timestep 784 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 785. State = [[-0.24902372  0.13471572]]. Action = [[-0.17181605 -0.22606596 -0.12695761 -0.6156241 ]]. Reward = [0.]
Curr episode timestep = 785
Scene graph at timestep 785 is [True, False, False, False, False, True]
State prediction error at timestep 785 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 786. State = [[-0.24893007  0.13452442]]. Action = [[ 0.07021916 -0.08989418  0.06745508 -0.260769  ]]. Reward = [0.]
Curr episode timestep = 786
Scene graph at timestep 786 is [True, False, False, False, False, True]
State prediction error at timestep 786 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 786 of 1
Current timestep = 787. State = [[-0.24830793  0.1330003 ]]. Action = [[0.05166441 0.07822287 0.07921606 0.7742251 ]]. Reward = [0.]
Curr episode timestep = 787
Scene graph at timestep 787 is [True, False, False, False, False, True]
State prediction error at timestep 787 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 787 of 1
Current timestep = 788. State = [[-0.2478475   0.13323294]]. Action = [[ 0.04051632  0.05563188 -0.13053499  0.58746195]]. Reward = [0.]
Curr episode timestep = 788
Scene graph at timestep 788 is [True, False, False, False, False, True]
State prediction error at timestep 788 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 788 of 1
Current timestep = 789. State = [[-0.24756977  0.13365299]]. Action = [[-0.14320806 -0.01472048  0.14731646  0.8963287 ]]. Reward = [0.]
Curr episode timestep = 789
Scene graph at timestep 789 is [True, False, False, False, False, True]
State prediction error at timestep 789 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 789 of 1
Current timestep = 790. State = [[-0.24558185  0.13457705]]. Action = [[0.05703813 0.03988126 0.24310786 0.8483498 ]]. Reward = [0.]
Curr episode timestep = 790
Scene graph at timestep 790 is [True, False, False, False, False, True]
State prediction error at timestep 790 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 791. State = [[-0.24486125  0.13493754]]. Action = [[-0.22830038  0.23132423  0.18758237  0.749632  ]]. Reward = [0.]
Curr episode timestep = 791
Scene graph at timestep 791 is [True, False, False, False, False, True]
State prediction error at timestep 791 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 792. State = [[-0.2416779   0.13650012]]. Action = [[0.00239614 0.05712029 0.17788818 0.02901828]]. Reward = [0.]
Curr episode timestep = 792
Scene graph at timestep 792 is [True, False, False, False, False, True]
State prediction error at timestep 792 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 793. State = [[-0.23821287  0.13947582]]. Action = [[-0.03326648  0.12632495 -0.07354088  0.47755814]]. Reward = [0.]
Curr episode timestep = 793
Scene graph at timestep 793 is [True, False, False, False, False, True]
State prediction error at timestep 793 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 794. State = [[-0.23796472  0.1413566 ]]. Action = [[ 0.05181974  0.23060614 -0.15910123 -0.7114509 ]]. Reward = [0.]
Curr episode timestep = 794
Scene graph at timestep 794 is [True, False, False, False, False, True]
State prediction error at timestep 794 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 795. State = [[-0.23791933  0.14278077]]. Action = [[ 0.18301213 -0.19323318 -0.18703341 -0.94913876]]. Reward = [0.]
Curr episode timestep = 795
Scene graph at timestep 795 is [True, False, False, False, False, True]
State prediction error at timestep 795 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 796. State = [[-0.2378318   0.14331453]]. Action = [[ 0.10875428  0.16890845 -0.1586633  -0.65780556]]. Reward = [0.]
Curr episode timestep = 796
Scene graph at timestep 796 is [True, False, False, False, False, True]
State prediction error at timestep 796 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 797. State = [[-0.23787074  0.14617705]]. Action = [[-0.09891443 -0.06500341 -0.08887702 -0.67351085]]. Reward = [0.]
Curr episode timestep = 797
Scene graph at timestep 797 is [True, False, False, False, False, True]
State prediction error at timestep 797 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 797 of 1
Current timestep = 798. State = [[-0.23881963  0.14677957]]. Action = [[-0.10197395 -0.1304409   0.22332457  0.3940735 ]]. Reward = [0.]
Curr episode timestep = 798
Scene graph at timestep 798 is [True, False, False, False, False, True]
State prediction error at timestep 798 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 798 of 1
Current timestep = 799. State = [[-0.23902129  0.14625707]]. Action = [[ 0.20726699 -0.22257447 -0.23144278 -0.910798  ]]. Reward = [0.]
Curr episode timestep = 799
Scene graph at timestep 799 is [True, False, False, False, False, True]
State prediction error at timestep 799 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 800. State = [[-0.23936138  0.14530979]]. Action = [[-0.11957535  0.13254273 -0.0575929   0.11753404]]. Reward = [0.]
Curr episode timestep = 800
Scene graph at timestep 800 is [True, False, False, False, False, True]
State prediction error at timestep 800 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 800 of 1
Current timestep = 801. State = [[-0.2423375   0.14868426]]. Action = [[ 0.08398893  0.11753583 -0.05147645  0.2878052 ]]. Reward = [0.]
Curr episode timestep = 801
Scene graph at timestep 801 is [True, False, False, False, False, True]
State prediction error at timestep 801 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 801 of 1
Current timestep = 802. State = [[-0.24497516  0.15265086]]. Action = [[ 0.01397794 -0.05688727 -0.21548826 -0.09645915]]. Reward = [0.]
Curr episode timestep = 802
Scene graph at timestep 802 is [True, False, False, False, False, True]
State prediction error at timestep 802 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 803. State = [[-0.24529636  0.15319109]]. Action = [[-0.20383123  0.07286042 -0.10573812  0.06681311]]. Reward = [0.]
Curr episode timestep = 803
Scene graph at timestep 803 is [True, False, False, False, False, True]
State prediction error at timestep 803 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 804. State = [[-0.24541467  0.15331264]]. Action = [[-0.14510897  0.07685232 -0.22494212  0.79067516]]. Reward = [0.]
Curr episode timestep = 804
Scene graph at timestep 804 is [True, False, False, False, False, True]
State prediction error at timestep 804 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 805. State = [[-0.24546717  0.15334322]]. Action = [[-0.21812476 -0.07172954 -0.0913045   0.73279643]]. Reward = [0.]
Curr episode timestep = 805
Scene graph at timestep 805 is [True, False, False, False, False, True]
State prediction error at timestep 805 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 806. State = [[-0.24549855  0.15339105]]. Action = [[ 0.00261018 -0.24169216  0.14235264 -0.03217286]]. Reward = [0.]
Curr episode timestep = 806
Scene graph at timestep 806 is [True, False, False, False, False, True]
State prediction error at timestep 806 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 806 of 1
Current timestep = 807. State = [[-0.24562325  0.15358101]]. Action = [[ 0.22222549  0.15337795 -0.22387432 -0.61344826]]. Reward = [0.]
Curr episode timestep = 807
Scene graph at timestep 807 is [True, False, False, False, False, True]
State prediction error at timestep 807 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 808. State = [[-0.24578296  0.15386692]]. Action = [[ 0.09774178  0.12075636  0.08077163 -0.13249016]]. Reward = [0.]
Curr episode timestep = 808
Scene graph at timestep 808 is [True, False, False, False, False, True]
State prediction error at timestep 808 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 809. State = [[-0.24567166  0.15482436]]. Action = [[-0.24625538  0.05038238 -0.14091498 -0.00211978]]. Reward = [0.]
Curr episode timestep = 809
Scene graph at timestep 809 is [True, False, False, False, False, True]
State prediction error at timestep 809 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 810. State = [[-0.24563937  0.15537342]]. Action = [[0.21565187 0.01193637 0.18977419 0.9429859 ]]. Reward = [0.]
Curr episode timestep = 810
Scene graph at timestep 810 is [True, False, False, False, False, True]
State prediction error at timestep 810 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 811. State = [[-0.24587189  0.15557495]]. Action = [[-0.18655384 -0.23541167 -0.17004497 -0.579896  ]]. Reward = [0.]
Curr episode timestep = 811
Scene graph at timestep 811 is [True, False, False, False, False, True]
State prediction error at timestep 811 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 812. State = [[-0.2461751   0.15712999]]. Action = [[-0.10965313  0.06566668 -0.07374473  0.40413165]]. Reward = [0.]
Curr episode timestep = 812
Scene graph at timestep 812 is [True, False, False, False, False, True]
State prediction error at timestep 812 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 812 of 1
Current timestep = 813. State = [[-0.24827905  0.16054405]]. Action = [[ 0.08756912  0.08314845  0.08228105 -0.6542819 ]]. Reward = [0.]
Curr episode timestep = 813
Scene graph at timestep 813 is [True, False, False, False, False, True]
State prediction error at timestep 813 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 813 of 1
Current timestep = 814. State = [[-0.24868582  0.1618821 ]]. Action = [[-0.14805192  0.24449867 -0.23758893 -0.5136599 ]]. Reward = [0.]
Curr episode timestep = 814
Scene graph at timestep 814 is [True, False, False, False, False, True]
State prediction error at timestep 814 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 815. State = [[-0.24894656  0.16291037]]. Action = [[ 0.22569126 -0.21580094 -0.00247492  0.14161086]]. Reward = [0.]
Curr episode timestep = 815
Scene graph at timestep 815 is [True, False, False, False, False, True]
State prediction error at timestep 815 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 815 of 1
Current timestep = 816. State = [[-0.24906842  0.16346218]]. Action = [[ 0.11955521  0.24781203  0.13530347 -0.20427239]]. Reward = [0.]
Curr episode timestep = 816
Scene graph at timestep 816 is [True, False, False, False, False, True]
State prediction error at timestep 816 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 817. State = [[-0.24922255  0.16400418]]. Action = [[-0.07342356  0.15785384 -0.07295623 -0.9690324 ]]. Reward = [0.]
Curr episode timestep = 817
Scene graph at timestep 817 is [True, False, False, False, False, True]
State prediction error at timestep 817 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 818. State = [[-0.24959253  0.16481794]]. Action = [[ 0.11948287  0.2263734  -0.10843594 -0.40138984]]. Reward = [0.]
Curr episode timestep = 818
Scene graph at timestep 818 is [True, False, False, False, False, True]
State prediction error at timestep 818 is tensor(4.3962e-05, grad_fn=<MseLossBackward0>)
Current timestep = 819. State = [[-0.24978563  0.16514933]]. Action = [[-0.10284895 -0.24728553  0.04062629  0.5648055 ]]. Reward = [0.]
Curr episode timestep = 819
Scene graph at timestep 819 is [True, False, False, False, False, True]
State prediction error at timestep 819 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 819 of 1
Current timestep = 820. State = [[-0.24991277  0.16582243]]. Action = [[0.17357507 0.02305099 0.0907647  0.85078466]]. Reward = [0.]
Curr episode timestep = 820
Scene graph at timestep 820 is [True, False, False, False, False, True]
State prediction error at timestep 820 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 821. State = [[-0.25005934  0.16607478]]. Action = [[-0.15373     0.21283907 -0.18523733 -0.8622329 ]]. Reward = [0.]
Curr episode timestep = 821
Scene graph at timestep 821 is [True, False, False, False, False, True]
State prediction error at timestep 821 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 822. State = [[-0.25016627  0.16629232]]. Action = [[-0.11167325 -0.16243172  0.0226084  -0.87775785]]. Reward = [0.]
Curr episode timestep = 822
Scene graph at timestep 822 is [True, False, False, False, False, True]
State prediction error at timestep 822 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 823. State = [[-0.25026503  0.16651496]]. Action = [[-0.16192545 -0.07075556 -0.12979434 -0.29888785]]. Reward = [0.]
Curr episode timestep = 823
Scene graph at timestep 823 is [True, False, False, False, False, True]
State prediction error at timestep 823 is tensor(8.4254e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 823 of 1
Current timestep = 824. State = [[-0.2502033   0.16667336]]. Action = [[-0.16957054 -0.1449014   0.22541404 -0.08300048]]. Reward = [0.]
Curr episode timestep = 824
Scene graph at timestep 824 is [True, False, False, False, False, True]
State prediction error at timestep 824 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 825. State = [[-0.25012866  0.16691425]]. Action = [[-0.18542604  0.23355407  0.08081293 -0.13844359]]. Reward = [0.]
Curr episode timestep = 825
Scene graph at timestep 825 is [True, False, False, False, False, True]
State prediction error at timestep 825 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 826. State = [[-0.2503139   0.16721384]]. Action = [[ 0.21634594 -0.01550259 -0.1749696   0.5841491 ]]. Reward = [0.]
Curr episode timestep = 826
Scene graph at timestep 826 is [True, False, False, False, False, True]
State prediction error at timestep 826 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 827. State = [[-0.25007254  0.16739003]]. Action = [[ 0.21342754 -0.2202279  -0.1534005   0.08319497]]. Reward = [0.]
Curr episode timestep = 827
Scene graph at timestep 827 is [True, False, False, False, False, True]
State prediction error at timestep 827 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 827 of 1
Current timestep = 828. State = [[-0.25007263  0.16766042]]. Action = [[ 0.07685137 -0.11708125 -0.01221351  0.3675797 ]]. Reward = [0.]
Curr episode timestep = 828
Scene graph at timestep 828 is [True, False, False, False, False, True]
State prediction error at timestep 828 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 829. State = [[-0.2488539   0.16640118]]. Action = [[ 0.10729036  0.11611715 -0.12338224 -0.99465936]]. Reward = [0.]
Curr episode timestep = 829
Scene graph at timestep 829 is [True, False, False, False, False, True]
State prediction error at timestep 829 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 830. State = [[-0.24535763  0.16834319]]. Action = [[-0.06883681  0.01016018  0.20576668 -0.15543526]]. Reward = [0.]
Curr episode timestep = 830
Scene graph at timestep 830 is [True, False, False, False, False, True]
State prediction error at timestep 830 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 831. State = [[-0.24334355  0.16997509]]. Action = [[-0.02490258  0.0269658   0.13031965  0.10350132]]. Reward = [0.]
Curr episode timestep = 831
Scene graph at timestep 831 is [True, False, False, False, False, True]
State prediction error at timestep 831 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 831 of 1
Current timestep = 832. State = [[-0.24311449  0.1704613 ]]. Action = [[-0.21333718 -0.2222495   0.21893525 -0.6558857 ]]. Reward = [0.]
Curr episode timestep = 832
Scene graph at timestep 832 is [True, False, False, False, False, True]
State prediction error at timestep 832 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 832 of 1
Current timestep = 833. State = [[-0.2421886   0.17148288]]. Action = [[ 0.06049785 -0.10679957 -0.07841024  0.85968757]]. Reward = [0.]
Curr episode timestep = 833
Scene graph at timestep 833 is [True, False, False, False, False, True]
State prediction error at timestep 833 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 834. State = [[-0.24146058  0.17117324]]. Action = [[-0.22382228 -0.02915888 -0.12051797  0.35793912]]. Reward = [0.]
Curr episode timestep = 834
Scene graph at timestep 834 is [True, False, False, False, False, True]
State prediction error at timestep 834 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 835. State = [[-0.24079137  0.17029959]]. Action = [[-0.02501523 -0.01733643  0.20056638  0.00444353]]. Reward = [0.]
Curr episode timestep = 835
Scene graph at timestep 835 is [True, False, False, False, False, True]
State prediction error at timestep 835 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 836. State = [[-0.24017094  0.16966808]]. Action = [[0.08971328 0.02718604 0.20535815 0.8158591 ]]. Reward = [0.]
Curr episode timestep = 836
Scene graph at timestep 836 is [True, False, False, False, False, True]
State prediction error at timestep 836 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 837. State = [[-0.23949982  0.16985796]]. Action = [[-0.17675312 -0.04342395 -0.22241898 -0.95478994]]. Reward = [0.]
Curr episode timestep = 837
Scene graph at timestep 837 is [True, False, False, False, False, True]
State prediction error at timestep 837 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 838. State = [[-0.23917611  0.16997828]]. Action = [[ 0.07970876  0.13516486  0.24075553 -0.47030902]]. Reward = [0.]
Curr episode timestep = 838
Scene graph at timestep 838 is [True, False, False, False, False, True]
State prediction error at timestep 838 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 839. State = [[-0.23919629  0.16997685]]. Action = [[ 0.22930163  0.19637808  0.20362097 -0.6921102 ]]. Reward = [0.]
Curr episode timestep = 839
Scene graph at timestep 839 is [True, False, False, False, False, True]
State prediction error at timestep 839 is tensor(3.3859e-05, grad_fn=<MseLossBackward0>)
Current timestep = 840. State = [[-0.23899454  0.17012142]]. Action = [[ 0.14780802  0.16206485  0.07468134 -0.20131469]]. Reward = [0.]
Curr episode timestep = 840
Scene graph at timestep 840 is [True, False, False, False, False, True]
State prediction error at timestep 840 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 841. State = [[-0.23871218  0.17022507]]. Action = [[-0.19611067  0.1674164   0.01957408 -0.4929167 ]]. Reward = [0.]
Curr episode timestep = 841
Scene graph at timestep 841 is [True, False, False, False, False, True]
State prediction error at timestep 841 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 842. State = [[-0.23826668  0.1701522 ]]. Action = [[-0.15016013  0.1463019  -0.03529096  0.01568961]]. Reward = [0.]
Curr episode timestep = 842
Scene graph at timestep 842 is [True, False, False, False, False, True]
State prediction error at timestep 842 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 842 of 1
Current timestep = 843. State = [[-0.23757428  0.17018273]]. Action = [[-0.02453503  0.04687768  0.12410197  0.40641606]]. Reward = [0.]
Curr episode timestep = 843
Scene graph at timestep 843 is [True, False, False, False, False, True]
State prediction error at timestep 843 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 843 of 1
Current timestep = 844. State = [[-0.23763393  0.17042515]]. Action = [[ 0.12304026 -0.15463284 -0.21955983  0.11022985]]. Reward = [0.]
Curr episode timestep = 844
Scene graph at timestep 844 is [True, False, False, False, False, True]
State prediction error at timestep 844 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 845. State = [[-0.2373689   0.17093726]]. Action = [[-0.13637388  0.00783196 -0.11442071 -0.6561422 ]]. Reward = [0.]
Curr episode timestep = 845
Scene graph at timestep 845 is [True, False, False, False, False, True]
State prediction error at timestep 845 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 846. State = [[-0.23741025  0.17098188]]. Action = [[-0.15381402 -0.09327407  0.04567313  0.74565554]]. Reward = [0.]
Curr episode timestep = 846
Scene graph at timestep 846 is [True, False, False, False, False, True]
State prediction error at timestep 846 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 847. State = [[-0.23744115  0.1707606 ]]. Action = [[-0.1023352   0.22429359  0.13278484  0.5272093 ]]. Reward = [0.]
Curr episode timestep = 847
Scene graph at timestep 847 is [True, False, False, False, False, True]
State prediction error at timestep 847 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 847 of 1
Current timestep = 848. State = [[-0.23706883  0.17101133]]. Action = [[-0.00066006 -0.22398797 -0.07248431 -0.560648  ]]. Reward = [0.]
Curr episode timestep = 848
Scene graph at timestep 848 is [True, False, False, False, False, True]
State prediction error at timestep 848 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 849. State = [[-0.23706158  0.1711784 ]]. Action = [[ 0.09282878 -0.15097539 -0.0163257  -0.90769506]]. Reward = [0.]
Curr episode timestep = 849
Scene graph at timestep 849 is [True, False, False, False, False, True]
State prediction error at timestep 849 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 850. State = [[-0.23663072  0.1712257 ]]. Action = [[-0.1582054  -0.20703967 -0.15750894 -0.5518736 ]]. Reward = [0.]
Curr episode timestep = 850
Scene graph at timestep 850 is [True, False, False, False, False, True]
State prediction error at timestep 850 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 851. State = [[-0.23663211  0.1711413 ]]. Action = [[0.16000864 0.20624569 0.24061102 0.8157313 ]]. Reward = [0.]
Curr episode timestep = 851
Scene graph at timestep 851 is [True, False, False, False, False, True]
State prediction error at timestep 851 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 851 of 1
Current timestep = 852. State = [[-0.23627356  0.1715024 ]]. Action = [[-0.1659573   0.19126052  0.1757747  -0.67453057]]. Reward = [0.]
Curr episode timestep = 852
Scene graph at timestep 852 is [True, False, False, False, False, True]
State prediction error at timestep 852 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 853. State = [[-0.23614573  0.17141405]]. Action = [[-0.16074844 -0.14529699 -0.16219242 -0.047948  ]]. Reward = [0.]
Curr episode timestep = 853
Scene graph at timestep 853 is [True, False, False, False, False, True]
State prediction error at timestep 853 is tensor(7.9419e-05, grad_fn=<MseLossBackward0>)
Current timestep = 854. State = [[-0.2354633   0.17154919]]. Action = [[ 0.06031111 -0.03936259 -0.19345167  0.56611645]]. Reward = [0.]
Curr episode timestep = 854
Scene graph at timestep 854 is [True, False, False, False, False, True]
State prediction error at timestep 854 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 854 of 1
Current timestep = 855. State = [[-0.23483244  0.17160997]]. Action = [[ 0.02250749  0.15640375 -0.11281452  0.7673155 ]]. Reward = [0.]
Curr episode timestep = 855
Scene graph at timestep 855 is [True, False, False, False, False, True]
State prediction error at timestep 855 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 856. State = [[-0.2333449   0.17164367]]. Action = [[-0.05573267 -0.01244326 -0.12790349 -0.89262193]]. Reward = [0.]
Curr episode timestep = 856
Scene graph at timestep 856 is [True, False, False, False, False, True]
State prediction error at timestep 856 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 856 of 1
Current timestep = 857. State = [[-0.23308212  0.17158027]]. Action = [[ 0.14320523  0.08843949 -0.22169642 -0.4169491 ]]. Reward = [0.]
Curr episode timestep = 857
Scene graph at timestep 857 is [True, False, False, False, False, True]
State prediction error at timestep 857 is tensor(7.3940e-05, grad_fn=<MseLossBackward0>)
Current timestep = 858. State = [[-0.23270051  0.17134348]]. Action = [[-0.09728608 -0.12122199 -0.23639663 -0.43378985]]. Reward = [0.]
Curr episode timestep = 858
Scene graph at timestep 858 is [True, False, False, False, False, True]
State prediction error at timestep 858 is tensor(5.8515e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 858 of 1
Current timestep = 859. State = [[-0.23245345  0.16795099]]. Action = [[ 0.12600607 -0.12456106  0.03318411  0.76060057]]. Reward = [0.]
Curr episode timestep = 859
Scene graph at timestep 859 is [True, False, False, False, False, True]
State prediction error at timestep 859 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 859 of 1
Current timestep = 860. State = [[-0.23165087  0.16616397]]. Action = [[ 0.1738863  -0.22386217 -0.04637124 -0.27034354]]. Reward = [0.]
Curr episode timestep = 860
Scene graph at timestep 860 is [True, False, False, False, False, True]
State prediction error at timestep 860 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 861. State = [[-0.23042737  0.16109389]]. Action = [[ 0.08472615  0.0785017  -0.0568859  -0.01450437]]. Reward = [0.]
Curr episode timestep = 861
Scene graph at timestep 861 is [True, False, False, False, False, True]
State prediction error at timestep 861 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 861 of 1
Current timestep = 862. State = [[-0.22975448  0.1603875 ]]. Action = [[-0.20414922  0.23082423 -0.1283532  -0.7215297 ]]. Reward = [0.]
Curr episode timestep = 862
Scene graph at timestep 862 is [True, False, False, False, False, True]
State prediction error at timestep 862 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 863. State = [[-0.229542    0.16025192]]. Action = [[ 0.17549598 -0.23962964 -0.16829176 -0.22841668]]. Reward = [0.]
Curr episode timestep = 863
Scene graph at timestep 863 is [True, False, False, False, False, True]
State prediction error at timestep 863 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 864. State = [[-0.22830904  0.15946925]]. Action = [[-0.08421856  0.09027874  0.22530115 -0.3171736 ]]. Reward = [0.]
Curr episode timestep = 864
Scene graph at timestep 864 is [True, False, False, False, False, True]
State prediction error at timestep 864 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 864 of 1
Current timestep = 865. State = [[-0.22923155  0.16086529]]. Action = [[-0.12783208 -0.00772275  0.04345044 -0.11451668]]. Reward = [0.]
Curr episode timestep = 865
Scene graph at timestep 865 is [True, False, False, False, False, True]
State prediction error at timestep 865 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 865 of 1
Current timestep = 866. State = [[-0.23073035  0.16250792]]. Action = [[-0.10279182 -0.01312968  0.08691418  0.10194671]]. Reward = [0.]
Curr episode timestep = 866
Scene graph at timestep 866 is [True, False, False, False, False, True]
State prediction error at timestep 866 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 866 of 1
Current timestep = 867. State = [[-0.23122337  0.1631606 ]]. Action = [[-0.1915919  -0.19272806  0.03243721  0.49568677]]. Reward = [0.]
Curr episode timestep = 867
Scene graph at timestep 867 is [True, False, False, False, False, True]
State prediction error at timestep 867 is tensor(8.7202e-05, grad_fn=<MseLossBackward0>)
Current timestep = 868. State = [[-0.23260228  0.16437851]]. Action = [[-0.04988149 -0.0217194  -0.21794638 -0.20444775]]. Reward = [0.]
Curr episode timestep = 868
Scene graph at timestep 868 is [True, False, False, False, False, True]
State prediction error at timestep 868 is tensor(1.9488e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 868 of 1
Current timestep = 869. State = [[-0.23287117  0.16462155]]. Action = [[ 0.18720686  0.22919339 -0.14829117 -0.7135712 ]]. Reward = [0.]
Curr episode timestep = 869
Scene graph at timestep 869 is [True, False, False, False, False, True]
State prediction error at timestep 869 is tensor(8.6013e-05, grad_fn=<MseLossBackward0>)
Current timestep = 870. State = [[-0.23305246  0.16466181]]. Action = [[0.213462   0.16790867 0.22033966 0.9827405 ]]. Reward = [0.]
Curr episode timestep = 870
Scene graph at timestep 870 is [True, False, False, False, False, True]
State prediction error at timestep 870 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 871. State = [[-0.23317847  0.16469164]]. Action = [[-0.06037845  0.19335353 -0.07581656  0.23120332]]. Reward = [0.]
Curr episode timestep = 871
Scene graph at timestep 871 is [True, False, False, False, False, True]
State prediction error at timestep 871 is tensor(8.6360e-05, grad_fn=<MseLossBackward0>)
Current timestep = 872. State = [[-0.2341854   0.16512983]]. Action = [[ 0.0650768   0.09662515 -0.17928408  0.6167494 ]]. Reward = [0.]
Curr episode timestep = 872
Scene graph at timestep 872 is [True, False, False, False, False, True]
State prediction error at timestep 872 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 872 of 1
Current timestep = 873. State = [[-0.23456205  0.16582336]]. Action = [[-0.0175975  -0.17038216  0.18307045 -0.83699673]]. Reward = [0.]
Curr episode timestep = 873
Scene graph at timestep 873 is [True, False, False, False, False, True]
State prediction error at timestep 873 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 874. State = [[-0.23492709  0.16625091]]. Action = [[-0.03726035  0.24448764 -0.17930001  0.589417  ]]. Reward = [0.]
Curr episode timestep = 874
Scene graph at timestep 874 is [True, False, False, False, False, True]
State prediction error at timestep 874 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 874 of 1
Current timestep = 875. State = [[-0.23552297  0.16720591]]. Action = [[ 0.12735808 -0.13224036  0.11377522 -0.88576114]]. Reward = [0.]
Curr episode timestep = 875
Scene graph at timestep 875 is [True, False, False, False, False, True]
State prediction error at timestep 875 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 876. State = [[-0.23516107  0.16660449]]. Action = [[-0.1402683  -0.17355327 -0.2304652  -0.8783721 ]]. Reward = [0.]
Curr episode timestep = 876
Scene graph at timestep 876 is [True, False, False, False, False, True]
State prediction error at timestep 876 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 877. State = [[-0.2345192   0.16565877]]. Action = [[ 0.21708882 -0.16991886  0.21964735  0.53027725]]. Reward = [0.]
Curr episode timestep = 877
Scene graph at timestep 877 is [True, False, False, False, False, True]
State prediction error at timestep 877 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 878. State = [[-0.23430423  0.16522415]]. Action = [[-0.09109893 -0.23552296  0.11873737  0.5352911 ]]. Reward = [0.]
Curr episode timestep = 878
Scene graph at timestep 878 is [True, False, False, False, False, True]
State prediction error at timestep 878 is tensor(5.9258e-05, grad_fn=<MseLossBackward0>)
Current timestep = 879. State = [[-0.23443289  0.16492563]]. Action = [[-2.3165321e-01  6.4218044e-04  2.2729951e-01 -7.7763546e-01]]. Reward = [0.]
Curr episode timestep = 879
Scene graph at timestep 879 is [True, False, False, False, False, True]
State prediction error at timestep 879 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 879 of -1
Current timestep = 880. State = [[-0.23389517  0.1634977 ]]. Action = [[-0.05526099 -0.03464635 -0.15266387  0.965606  ]]. Reward = [0.]
Curr episode timestep = 880
Scene graph at timestep 880 is [True, False, False, False, False, True]
State prediction error at timestep 880 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 881. State = [[-0.23355153  0.16213162]]. Action = [[ 0.09312934  0.0103766   0.05459133 -0.66599566]]. Reward = [0.]
Curr episode timestep = 881
Scene graph at timestep 881 is [True, False, False, False, False, True]
State prediction error at timestep 881 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 882. State = [[-0.23331858  0.1617518 ]]. Action = [[-0.05611014  0.14478946 -0.05879976  0.17260885]]. Reward = [0.]
Curr episode timestep = 882
Scene graph at timestep 882 is [True, False, False, False, False, True]
State prediction error at timestep 882 is tensor(5.6982e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 882 of 1
Current timestep = 883. State = [[-0.23314579  0.1611939 ]]. Action = [[ 0.15418798  0.23233551 -0.21537133  0.5375831 ]]. Reward = [0.]
Curr episode timestep = 883
Scene graph at timestep 883 is [True, False, False, False, False, True]
State prediction error at timestep 883 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 884. State = [[-0.23314564  0.16111274]]. Action = [[-0.0573518   0.14227629  0.24217254 -0.7994924 ]]. Reward = [0.]
Curr episode timestep = 884
Scene graph at timestep 884 is [True, False, False, False, False, True]
State prediction error at timestep 884 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 885. State = [[-0.23288845  0.16007164]]. Action = [[ 0.04551524  0.14264533 -0.03555852  0.4597962 ]]. Reward = [0.]
Curr episode timestep = 885
Scene graph at timestep 885 is [True, False, False, False, False, True]
State prediction error at timestep 885 is tensor(9.6113e-05, grad_fn=<MseLossBackward0>)
Current timestep = 886. State = [[-0.23284039  0.15987615]]. Action = [[ 0.09980866 -0.18327837 -0.17613344  0.9430504 ]]. Reward = [0.]
Curr episode timestep = 886
Scene graph at timestep 886 is [True, False, False, False, False, True]
State prediction error at timestep 886 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 887. State = [[-0.23262323  0.15924503]]. Action = [[ 0.11066443  0.0661425  -0.16113625  0.4255792 ]]. Reward = [0.]
Curr episode timestep = 887
Scene graph at timestep 887 is [True, False, False, False, False, True]
State prediction error at timestep 887 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 887 of 1
Current timestep = 888. State = [[-0.23220159  0.1594866 ]]. Action = [[-5.5508316e-04 -1.6087671e-01 -2.0328981e-01  8.0295038e-01]]. Reward = [0.]
Curr episode timestep = 888
Scene graph at timestep 888 is [True, False, False, False, False, True]
State prediction error at timestep 888 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 889. State = [[-0.23180543  0.15966001]]. Action = [[ 0.24032563 -0.22105584  0.2024365   0.369174  ]]. Reward = [0.]
Curr episode timestep = 889
Scene graph at timestep 889 is [True, False, False, False, False, True]
State prediction error at timestep 889 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 890. State = [[-0.23081766  0.1602219 ]]. Action = [[-0.04107881  0.09495151  0.09749761 -0.4106375 ]]. Reward = [0.]
Curr episode timestep = 890
Scene graph at timestep 890 is [True, False, False, False, False, True]
State prediction error at timestep 890 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 890 of 1
Current timestep = 891. State = [[-0.23084918  0.16072957]]. Action = [[-0.19318376  0.18199447  0.21927336 -0.83213633]]. Reward = [0.]
Curr episode timestep = 891
Scene graph at timestep 891 is [True, False, False, False, False, True]
State prediction error at timestep 891 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 892. State = [[-0.23097774  0.16152681]]. Action = [[-0.18092743 -0.08934131 -0.17928924  0.34255838]]. Reward = [0.]
Curr episode timestep = 892
Scene graph at timestep 892 is [True, False, False, False, False, True]
State prediction error at timestep 892 is tensor(6.5542e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 892 of 1
Current timestep = 893. State = [[-0.23117007  0.16182217]]. Action = [[-0.00175063 -0.15750174  0.16863763 -0.22315544]]. Reward = [0.]
Curr episode timestep = 893
Scene graph at timestep 893 is [True, False, False, False, False, True]
State prediction error at timestep 893 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 894. State = [[-0.23108622  0.16183001]]. Action = [[ 0.12972456  0.14064807 -0.06783122 -0.8388992 ]]. Reward = [0.]
Curr episode timestep = 894
Scene graph at timestep 894 is [True, False, False, False, False, True]
State prediction error at timestep 894 is tensor(7.6473e-05, grad_fn=<MseLossBackward0>)
Current timestep = 895. State = [[-0.23120722  0.16224445]]. Action = [[ 0.22775751 -0.16117905 -0.22566599  0.53026915]]. Reward = [0.]
Curr episode timestep = 895
Scene graph at timestep 895 is [True, False, False, False, False, True]
State prediction error at timestep 895 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 896. State = [[-0.23121978  0.16253842]]. Action = [[-0.19991766 -0.06537846  0.06185356  0.28155375]]. Reward = [0.]
Curr episode timestep = 896
Scene graph at timestep 896 is [True, False, False, False, False, True]
State prediction error at timestep 896 is tensor(3.8491e-05, grad_fn=<MseLossBackward0>)
Current timestep = 897. State = [[-0.23098424  0.16256331]]. Action = [[-0.13588314  0.04876155  0.13054794  0.32395828]]. Reward = [0.]
Curr episode timestep = 897
Scene graph at timestep 897 is [True, False, False, False, False, True]
State prediction error at timestep 897 is tensor(4.0361e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 897 of 1
Current timestep = 898. State = [[-0.23114304  0.16281362]]. Action = [[-0.19322698  0.15493828  0.18017262  0.4257865 ]]. Reward = [0.]
Curr episode timestep = 898
Scene graph at timestep 898 is [True, False, False, False, False, True]
State prediction error at timestep 898 is tensor(5.5551e-05, grad_fn=<MseLossBackward0>)
Current timestep = 899. State = [[-0.23104332  0.16313253]]. Action = [[-0.23697317  0.22612709  0.02079147 -0.35041344]]. Reward = [0.]
Curr episode timestep = 899
Scene graph at timestep 899 is [True, False, False, False, False, True]
State prediction error at timestep 899 is tensor(9.5661e-05, grad_fn=<MseLossBackward0>)
Current timestep = 900. State = [[-0.23110482  0.16302603]]. Action = [[ 0.00643378  0.19943902 -0.19562198 -0.91085154]]. Reward = [0.]
Curr episode timestep = 900
Scene graph at timestep 900 is [True, False, False, False, False, True]
State prediction error at timestep 900 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 900 of 1
Current timestep = 901. State = [[-0.2581595   0.00806189]]. Action = [[-0.17863609 -0.09202556  0.17316559  0.18878734]]. Reward = [0.]
Curr episode timestep = 901
Scene graph at timestep 901 is [True, False, False, False, True, False]
State prediction error at timestep 901 is tensor(0.0113, grad_fn=<MseLossBackward0>)
Current timestep = 902. State = [[-0.2581595   0.00806189]]. Action = [[-0.15144746 -0.14142919 -0.16680548  0.8572247 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 902 is [True, False, False, False, True, False]
State prediction error at timestep 902 is tensor(6.5459e-06, grad_fn=<MseLossBackward0>)
Current timestep = 903. State = [[-0.2581595   0.00806189]]. Action = [[ 0.21977478  0.21427691 -0.07398468  0.10566783]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 903 is [True, False, False, False, True, False]
State prediction error at timestep 903 is tensor(1.7801e-05, grad_fn=<MseLossBackward0>)
Current timestep = 904. State = [[-0.2581595   0.00806189]]. Action = [[ 0.1740132   0.00371039  0.12917167 -0.8024096 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 904 is [True, False, False, False, True, False]
State prediction error at timestep 904 is tensor(9.3480e-06, grad_fn=<MseLossBackward0>)
Current timestep = 905. State = [[-0.2581595   0.00806189]]. Action = [[-0.08973858 -0.19159055  0.12746018 -0.51949   ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 905 is [True, False, False, False, True, False]
State prediction error at timestep 905 is tensor(5.9072e-05, grad_fn=<MseLossBackward0>)
Current timestep = 906. State = [[-0.2581595   0.00806189]]. Action = [[-0.03464434 -0.2198264  -0.11444354 -0.7759191 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 906 is [True, False, False, False, True, False]
State prediction error at timestep 906 is tensor(7.1266e-05, grad_fn=<MseLossBackward0>)
Current timestep = 907. State = [[-0.2581595   0.00806189]]. Action = [[-0.18823844  0.17480135 -0.21527363 -0.36141413]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 907 is [True, False, False, False, True, False]
State prediction error at timestep 907 is tensor(6.5208e-05, grad_fn=<MseLossBackward0>)
Current timestep = 908. State = [[-0.2581595   0.00806189]]. Action = [[-0.04753694  0.23104447  0.12886927 -0.05058342]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 908 is [True, False, False, False, True, False]
State prediction error at timestep 908 is tensor(3.4164e-05, grad_fn=<MseLossBackward0>)
Current timestep = 909. State = [[-0.2581595   0.00806189]]. Action = [[-0.11563498 -0.21675017  0.16983575 -0.8500352 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 909 is [True, False, False, False, True, False]
State prediction error at timestep 909 is tensor(2.8655e-05, grad_fn=<MseLossBackward0>)
Current timestep = 910. State = [[-0.2581595   0.00806189]]. Action = [[ 0.16459727 -0.16715719  0.11560085  0.7157128 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 910 is [True, False, False, False, True, False]
State prediction error at timestep 910 is tensor(9.0725e-05, grad_fn=<MseLossBackward0>)
Current timestep = 911. State = [[-0.2581595   0.00806189]]. Action = [[-0.13561612 -0.07145447 -0.23117706 -0.19942039]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 911 is [True, False, False, False, True, False]
State prediction error at timestep 911 is tensor(5.7320e-05, grad_fn=<MseLossBackward0>)
Current timestep = 912. State = [[-0.2581595   0.00806189]]. Action = [[-0.22962624  0.22457618  0.08848119 -0.81566256]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 912 is [True, False, False, False, True, False]
State prediction error at timestep 912 is tensor(1.8075e-05, grad_fn=<MseLossBackward0>)
Current timestep = 913. State = [[-0.2581595   0.00806189]]. Action = [[ 0.14191675 -0.14390312 -0.21737759 -0.39079666]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 913 is [True, False, False, False, True, False]
State prediction error at timestep 913 is tensor(2.9756e-05, grad_fn=<MseLossBackward0>)
Current timestep = 914. State = [[-0.2581595   0.00806189]]. Action = [[0.1320411  0.18125355 0.21202213 0.65721035]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 914 is [True, False, False, False, True, False]
State prediction error at timestep 914 is tensor(2.7864e-05, grad_fn=<MseLossBackward0>)
Current timestep = 915. State = [[-0.2581595   0.00806189]]. Action = [[-0.2090235   0.09456438 -0.23431753 -0.9614977 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 915 is [True, False, False, False, True, False]
State prediction error at timestep 915 is tensor(1.7575e-06, grad_fn=<MseLossBackward0>)
Current timestep = 916. State = [[-0.2581595   0.00806189]]. Action = [[-0.14699398  0.1893678  -0.12303421  0.00375783]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 916 is [True, False, False, False, True, False]
State prediction error at timestep 916 is tensor(4.3046e-05, grad_fn=<MseLossBackward0>)
Current timestep = 917. State = [[-0.2581595   0.00806189]]. Action = [[ 0.22634125  0.20242378 -0.17468536  0.69500184]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 917 is [True, False, False, False, True, False]
State prediction error at timestep 917 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 918. State = [[-0.2581595   0.00806189]]. Action = [[ 0.2366094   0.11295125 -0.06688993  0.88121104]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 918 is [True, False, False, False, True, False]
State prediction error at timestep 918 is tensor(6.0753e-05, grad_fn=<MseLossBackward0>)
Current timestep = 919. State = [[-0.25698096  0.01073004]]. Action = [[-0.09857656 -0.08275262 -0.06127219  0.8221626 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 919 is [True, False, False, False, True, False]
State prediction error at timestep 919 is tensor(6.6956e-05, grad_fn=<MseLossBackward0>)
Current timestep = 920. State = [[-0.25647214  0.01299538]]. Action = [[-0.04834522 -0.20969224  0.0593144   0.5598705 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 920 is [True, False, False, False, True, False]
State prediction error at timestep 920 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 921. State = [[-0.2560617  0.017449 ]]. Action = [[-0.17381129  0.09085777 -0.1579285  -0.8922817 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 921 is [True, False, False, False, True, False]
State prediction error at timestep 921 is tensor(2.3998e-05, grad_fn=<MseLossBackward0>)
Current timestep = 922. State = [[-0.25534388  0.02561695]]. Action = [[ 0.04748183 -0.1687277  -0.24008386 -0.96641904]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 922 is [True, False, False, False, True, False]
State prediction error at timestep 922 is tensor(4.7347e-05, grad_fn=<MseLossBackward0>)
Current timestep = 923. State = [[-0.25435528  0.03399429]]. Action = [[-0.22925659  0.07009253  0.15366697 -0.30862534]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 923 is [True, False, False, False, True, False]
State prediction error at timestep 923 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 924. State = [[-0.25333413  0.05132385]]. Action = [[-0.19964348  0.06578249  0.0309667  -0.6496408 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 924 is [True, False, False, False, True, False]
State prediction error at timestep 924 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 925. State = [[-0.25272608  0.06370559]]. Action = [[-0.18333045  0.16330251 -0.2370203   0.8160279 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 925 is [True, False, False, False, True, False]
State prediction error at timestep 925 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 926. State = [[-0.25087705  0.08562516]]. Action = [[ 0.1804226  -0.20041631 -0.06574164 -0.8154578 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 926 is [True, False, False, False, True, False]
State prediction error at timestep 926 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 927. State = [[-0.2499417   0.09406605]]. Action = [[-0.21600705  0.08828837  0.19929278  0.36772037]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 927 is [True, False, False, False, True, False]
State prediction error at timestep 927 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 928. State = [[-0.24794586  0.10442028]]. Action = [[-0.10206187 -0.16643192 -0.14084636 -0.10839224]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 928 is [True, False, False, False, True, False]
State prediction error at timestep 928 is tensor(7.3790e-05, grad_fn=<MseLossBackward0>)
Current timestep = 929. State = [[-0.24323887  0.12843853]]. Action = [[-0.04496166 -0.02274781 -0.08098298  0.82266116]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 929 is [True, False, False, False, False, True]
State prediction error at timestep 929 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 930. State = [[-0.24264719  0.13169987]]. Action = [[-0.01407477 -0.13366604  0.17764616 -0.8026933 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 930 is [True, False, False, False, False, True]
State prediction error at timestep 930 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 931. State = [[-0.24164218  0.14182234]]. Action = [[ 0.08066624 -0.03545594 -0.10822999  0.10063052]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 931 is [True, False, False, False, False, True]
State prediction error at timestep 931 is tensor(9.8408e-05, grad_fn=<MseLossBackward0>)
Current timestep = 932. State = [[-0.23885256  0.1476165 ]]. Action = [[ 0.08675683  0.02066272 -0.07922751 -0.62256664]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 932 is [True, False, False, False, False, True]
State prediction error at timestep 932 is tensor(7.4697e-05, grad_fn=<MseLossBackward0>)
Current timestep = 933. State = [[-0.23823723  0.14827354]]. Action = [[-0.10982049  0.23184183  0.14544952  0.07697237]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 933 is [True, False, False, False, False, True]
State prediction error at timestep 933 is tensor(7.4684e-05, grad_fn=<MseLossBackward0>)
Current timestep = 934. State = [[-0.2371246   0.14924435]]. Action = [[-0.11798304  0.16894245 -0.21682999 -0.2643882 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 934 is [True, False, False, False, False, True]
State prediction error at timestep 934 is tensor(1.5735e-05, grad_fn=<MseLossBackward0>)
Current timestep = 935. State = [[-0.23628873  0.15014103]]. Action = [[-0.05322507 -0.18667272 -0.12866941  0.40743804]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 935 is [True, False, False, False, False, True]
State prediction error at timestep 935 is tensor(1.6806e-05, grad_fn=<MseLossBackward0>)
Current timestep = 936. State = [[-0.23582767  0.15060885]]. Action = [[ 0.20847315 -0.08618306  0.19352776 -0.35021186]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 936 is [True, False, False, False, False, True]
State prediction error at timestep 936 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 937. State = [[-0.23478524  0.15146318]]. Action = [[ 0.17323488 -0.05721842  0.1792252   0.0128895 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 937 is [True, False, False, False, False, True]
State prediction error at timestep 937 is tensor(6.2323e-05, grad_fn=<MseLossBackward0>)
Current timestep = 938. State = [[-0.234052    0.15206295]]. Action = [[ 0.22879267 -0.06945643  0.07147947 -0.12470281]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 938 is [True, False, False, False, False, True]
State prediction error at timestep 938 is tensor(9.1528e-05, grad_fn=<MseLossBackward0>)
Current timestep = 939. State = [[-0.2316233   0.15388823]]. Action = [[ 0.10395122  0.12460685 -0.18229458  0.11983669]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 939 is [True, False, False, False, False, True]
State prediction error at timestep 939 is tensor(2.5470e-06, grad_fn=<MseLossBackward0>)
Current timestep = 940. State = [[-0.2309504   0.15478273]]. Action = [[ 0.12370875 -0.1741386   0.08481407 -0.34782135]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 940 is [True, False, False, False, False, True]
State prediction error at timestep 940 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 941. State = [[-0.23007083  0.15602657]]. Action = [[ 0.09243149 -0.18636534 -0.04256104  0.69146264]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 941 is [True, False, False, False, False, True]
State prediction error at timestep 941 is tensor(5.7889e-05, grad_fn=<MseLossBackward0>)
Current timestep = 942. State = [[-0.2269936  0.1596828]]. Action = [[-0.09298909 -0.09268454 -0.22090775 -0.7300259 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 942 is [True, False, False, False, False, True]
State prediction error at timestep 942 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 943. State = [[-0.22689342  0.15973663]]. Action = [[-0.23626179 -0.02268161 -0.17032737 -0.45215964]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 943 is [True, False, False, False, False, True]
State prediction error at timestep 943 is tensor(5.9362e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 943 of 1
Current timestep = 944. State = [[-0.22655787  0.15945448]]. Action = [[-0.07612772  0.15302718  0.09837118  0.35903132]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 944 is [True, False, False, False, False, True]
State prediction error at timestep 944 is tensor(2.5151e-05, grad_fn=<MseLossBackward0>)
Current timestep = 945. State = [[-0.22534782  0.16058375]]. Action = [[-0.04191619  0.06806239 -0.09831268 -0.04056543]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 945 is [True, False, False, False, False, True]
State prediction error at timestep 945 is tensor(3.0126e-05, grad_fn=<MseLossBackward0>)
Current timestep = 946. State = [[-0.22528945  0.16101553]]. Action = [[-0.14533034 -0.18094714  0.09324899  0.83468616]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 946 is [True, False, False, False, False, True]
State prediction error at timestep 946 is tensor(6.6392e-05, grad_fn=<MseLossBackward0>)
Current timestep = 947. State = [[-0.22580719  0.16173288]]. Action = [[-0.24278176 -0.08721179  0.21494687 -0.43004417]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 947 is [True, False, False, False, False, True]
State prediction error at timestep 947 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 948. State = [[-0.22592843  0.1622966 ]]. Action = [[-0.19681667 -0.15401365  0.09298715 -0.08204901]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 948 is [True, False, False, False, False, True]
State prediction error at timestep 948 is tensor(2.6448e-05, grad_fn=<MseLossBackward0>)
Current timestep = 949. State = [[-0.22591552  0.16259402]]. Action = [[ 0.13928404 -0.15508597  0.23559481 -0.81277275]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 949 is [True, False, False, False, False, True]
State prediction error at timestep 949 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 950. State = [[-0.2262382  0.1633466]]. Action = [[-0.0893634  -0.18672277 -0.1787828  -0.1006974 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 950 is [True, False, False, False, False, True]
State prediction error at timestep 950 is tensor(7.0645e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 950 of 1
Current timestep = 951. State = [[-0.22625527  0.1636947 ]]. Action = [[-0.21048804  0.07780105 -0.06330182 -0.06634605]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 951 is [True, False, False, False, False, True]
State prediction error at timestep 951 is tensor(1.7733e-05, grad_fn=<MseLossBackward0>)
Current timestep = 952. State = [[-0.22639342  0.16374354]]. Action = [[-0.18169396 -0.02679233  0.23075026  0.61398196]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 952 is [True, False, False, False, False, True]
State prediction error at timestep 952 is tensor(2.1700e-05, grad_fn=<MseLossBackward0>)
Current timestep = 953. State = [[-0.22642653  0.16409776]]. Action = [[-0.19243515 -0.08921492 -0.1431015   0.3707863 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 953 is [True, False, False, False, False, True]
State prediction error at timestep 953 is tensor(2.8650e-05, grad_fn=<MseLossBackward0>)
Current timestep = 954. State = [[-0.22640324  0.16437025]]. Action = [[ 0.01431537 -0.15694717 -0.12961614  0.55771446]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 954 is [True, False, False, False, False, True]
State prediction error at timestep 954 is tensor(1.4471e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 954 of 1
Current timestep = 955. State = [[-0.2267041   0.16474034]]. Action = [[ 0.09548345 -0.04698461 -0.0817401   0.34777904]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 955 is [True, False, False, False, False, True]
State prediction error at timestep 955 is tensor(3.1479e-05, grad_fn=<MseLossBackward0>)
Current timestep = 956. State = [[-0.22611272  0.16440488]]. Action = [[-0.2246327   0.09688714  0.11397603  0.83573437]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 956 is [True, False, False, False, False, True]
State prediction error at timestep 956 is tensor(3.5652e-05, grad_fn=<MseLossBackward0>)
Current timestep = 957. State = [[-0.22570246  0.16432297]]. Action = [[-0.18999879  0.22112012  0.02786356  0.93867636]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 957 is [True, False, False, False, False, True]
State prediction error at timestep 957 is tensor(3.2973e-05, grad_fn=<MseLossBackward0>)
Current timestep = 958. State = [[-0.22537878  0.16425428]]. Action = [[ 0.1485686  -0.08479479 -0.01311037 -0.2847693 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 958 is [True, False, False, False, False, True]
State prediction error at timestep 958 is tensor(9.2095e-05, grad_fn=<MseLossBackward0>)
Current timestep = 959. State = [[-0.22522572  0.16420674]]. Action = [[ 0.15910569 -0.11257568 -0.02274609  0.5803747 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 959 is [True, False, False, False, False, True]
State prediction error at timestep 959 is tensor(3.2774e-05, grad_fn=<MseLossBackward0>)
Current timestep = 960. State = [[-0.22459397  0.16413234]]. Action = [[ 0.03631675 -0.21142791  0.05608639 -0.3655271 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 960 is [True, False, False, False, False, True]
State prediction error at timestep 960 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 960 of 1
Current timestep = 961. State = [[-0.22471139  0.1638082 ]]. Action = [[-0.12102103  0.21550453 -0.1276242   0.04180658]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 961 is [True, False, False, False, False, True]
State prediction error at timestep 961 is tensor(1.0710e-05, grad_fn=<MseLossBackward0>)
Current timestep = 962. State = [[-0.22422543  0.16417043]]. Action = [[ 0.09444451  0.17781082 -0.14485967 -0.6070353 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 962 is [True, False, False, False, False, True]
State prediction error at timestep 962 is tensor(9.4607e-05, grad_fn=<MseLossBackward0>)
Current timestep = 963. State = [[-0.22409774  0.16407146]]. Action = [[ 0.20689672 -0.03193101  0.20883805 -0.91148967]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 963 is [True, False, False, False, False, True]
State prediction error at timestep 963 is tensor(5.2392e-05, grad_fn=<MseLossBackward0>)
Current timestep = 964. State = [[-0.22438884  0.16380435]]. Action = [[-0.20926285 -0.2098766  -0.07765219  0.45248318]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 964 is [True, False, False, False, False, True]
State prediction error at timestep 964 is tensor(1.3893e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 964 of 1
Current timestep = 965. State = [[-0.22397391  0.16380447]]. Action = [[ 0.09675917  0.09840447 -0.19567862  0.59438133]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 965 is [True, False, False, False, False, True]
State prediction error at timestep 965 is tensor(4.1599e-05, grad_fn=<MseLossBackward0>)
Current timestep = 966. State = [[-0.22313233  0.16422337]]. Action = [[ 0.01953465  0.24266016  0.01257661 -0.3956381 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 966 is [True, False, False, False, False, True]
State prediction error at timestep 966 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 967. State = [[-0.22251788  0.16462868]]. Action = [[ 0.06025943 -0.1679538  -0.19489057  0.04801905]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 967 is [True, False, False, False, False, True]
State prediction error at timestep 967 is tensor(1.9914e-06, grad_fn=<MseLossBackward0>)
Current timestep = 968. State = [[-0.22081429  0.16534126]]. Action = [[ 0.13032427  0.01852202 -0.2109953   0.46600437]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 968 is [True, False, False, False, False, True]
State prediction error at timestep 968 is tensor(2.9981e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 968 of 1
Current timestep = 969. State = [[-0.2201314   0.16561723]]. Action = [[-0.04735562 -0.19148758 -0.02579151 -0.76508826]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 969 is [True, False, False, False, False, True]
State prediction error at timestep 969 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 970. State = [[-0.2170706   0.16646099]]. Action = [[-0.0022997  -0.07255915  0.09348616 -0.1946382 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 970 is [True, False, False, False, False, True]
State prediction error at timestep 970 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 970 of 1
Current timestep = 971. State = [[-0.21622817  0.16631924]]. Action = [[ 0.11264235  0.14390099  0.16523838 -0.37087035]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 971 is [True, False, False, False, False, True]
State prediction error at timestep 971 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 972. State = [[-0.21372989  0.16628516]]. Action = [[ 0.02961951  0.03493962 -0.22838876  0.10296381]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 972 is [True, False, False, False, False, True]
State prediction error at timestep 972 is tensor(1.1658e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 972 of 1
Current timestep = 973. State = [[-0.21274799  0.16653612]]. Action = [[ 0.03161469 -0.19713846  0.05767089 -0.45235085]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 973 is [True, False, False, False, False, True]
State prediction error at timestep 973 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 974. State = [[-0.21233913  0.16665317]]. Action = [[-0.10079604  0.20115358 -0.1753982   0.9238961 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 974 is [True, False, False, False, False, True]
State prediction error at timestep 974 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 975. State = [[-0.21163072  0.16689636]]. Action = [[-0.2417141   0.19365734  0.08553421  0.6813514 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 975 is [True, False, False, False, False, True]
State prediction error at timestep 975 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 975 of 1
Current timestep = 976. State = [[-0.21076216  0.16724028]]. Action = [[-0.2200008  -0.08259857 -0.18427528  0.2031697 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 976 is [True, False, False, False, False, True]
State prediction error at timestep 976 is tensor(3.6510e-06, grad_fn=<MseLossBackward0>)
Current timestep = 977. State = [[-0.2099073   0.16745493]]. Action = [[ 0.01428217  0.23385596 -0.10096489 -0.8277619 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 977 is [True, False, False, False, False, True]
State prediction error at timestep 977 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 978. State = [[-0.20903003  0.16761543]]. Action = [[ 0.18261456  0.08308762 -0.11022501 -0.5509649 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 978 is [True, False, False, False, False, True]
State prediction error at timestep 978 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 979. State = [[-0.20846282  0.1678038 ]]. Action = [[-0.11164238 -0.21363299 -0.10997751 -0.23704678]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 979 is [True, False, False, False, False, True]
State prediction error at timestep 979 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 980. State = [[-0.20810544  0.16796066]]. Action = [[ 0.06831887 -0.21746454  0.23752761  0.89715207]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 980 is [True, False, False, False, False, True]
State prediction error at timestep 980 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 980 of 1
Current timestep = 981. State = [[-0.20728748  0.1681674 ]]. Action = [[-0.19168146  0.1375947  -0.24066913 -0.73980683]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 981 is [True, False, False, False, False, True]
State prediction error at timestep 981 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 982. State = [[-0.20696065  0.16815639]]. Action = [[-0.19623888  0.15411153 -0.2248328  -0.70635194]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 982 is [True, False, False, False, False, True]
State prediction error at timestep 982 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 983. State = [[-0.20677802  0.16820642]]. Action = [[-0.03538626 -0.18659079  0.04353836 -0.80099034]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 983 is [True, False, False, False, False, True]
State prediction error at timestep 983 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 984. State = [[-0.20663804  0.16831958]]. Action = [[-0.0439457  -0.23117068 -0.18426538  0.5820844 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 984 is [True, False, False, False, False, True]
State prediction error at timestep 984 is tensor(9.0618e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 984 of 1
Current timestep = 985. State = [[-0.20667745  0.16820812]]. Action = [[ 0.18760207  0.0672389  -0.02630499 -0.7175695 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 985 is [True, False, False, False, False, True]
State prediction error at timestep 985 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 986. State = [[-0.20636562  0.16831462]]. Action = [[-0.00686464  0.22180504  0.2444216  -0.4287647 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 986 is [True, False, False, False, False, True]
State prediction error at timestep 986 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 987. State = [[-0.2062889  0.1684159]]. Action = [[ 0.03684774  0.22186285  0.1827659  -0.11049825]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 987 is [True, False, False, False, False, True]
State prediction error at timestep 987 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 987 of 1
Current timestep = 988. State = [[-0.20612267  0.16833723]]. Action = [[ 0.20169497 -0.21340834 -0.02009813  0.09485364]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 988 is [True, False, False, False, False, True]
State prediction error at timestep 988 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 989. State = [[-0.20595016  0.16842805]]. Action = [[-0.13362463 -0.04315567 -0.16493803  0.20558882]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 989 is [True, False, False, False, False, True]
State prediction error at timestep 989 is tensor(7.3347e-06, grad_fn=<MseLossBackward0>)
Current timestep = 990. State = [[-0.20597762  0.16838092]]. Action = [[-0.15691173  0.05955076  0.11468574 -0.5328261 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 990 is [True, False, False, False, False, True]
State prediction error at timestep 990 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 991. State = [[-0.20577511  0.16847767]]. Action = [[-0.17983288 -0.05538197 -0.0943298   0.56236935]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 991 is [True, False, False, False, False, True]
State prediction error at timestep 991 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 991 of 1
Current timestep = 992. State = [[-0.20588091  0.16833897]]. Action = [[-0.23436232  0.21738142  0.02218553  0.51458013]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 992 is [True, False, False, False, False, True]
State prediction error at timestep 992 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 993. State = [[-0.20586465  0.16834767]]. Action = [[ 0.1851277  -0.07119752  0.11502814  0.69294906]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 993 is [True, False, False, False, False, True]
State prediction error at timestep 993 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 994. State = [[-0.20579109  0.16838792]]. Action = [[0.043562   0.16836119 0.10381088 0.40054083]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 994 is [True, False, False, False, False, True]
State prediction error at timestep 994 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 995. State = [[-0.20560877  0.16843776]]. Action = [[-0.08921579  0.24600989  0.23175809 -0.8370937 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 995 is [True, False, False, False, False, True]
State prediction error at timestep 995 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 995 of 1
Current timestep = 996. State = [[-0.20570754  0.16838738]]. Action = [[-0.20742516  0.18404794  0.01561379 -0.24015832]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 996 is [True, False, False, False, False, True]
State prediction error at timestep 996 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 997. State = [[-0.20568153  0.16848838]]. Action = [[ 0.05151075  0.11725822  0.0564993  -0.6620952 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 997 is [True, False, False, False, False, True]
State prediction error at timestep 997 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 997 of 1
Current timestep = 998. State = [[-0.20534119  0.16910873]]. Action = [[ 0.23449105 -0.23159367  0.00260347  0.4694729 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 998 is [True, False, False, False, False, True]
State prediction error at timestep 998 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 999. State = [[-0.20452833  0.17070419]]. Action = [[ 0.12513769 -0.06701478 -0.15790272 -0.45271313]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 999 is [True, False, False, False, False, True]
State prediction error at timestep 999 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 999 of 1
Current timestep = 1000. State = [[-0.20181309  0.17040738]]. Action = [[ 0.13845903  0.14578927 -0.04010296 -0.51334554]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1000 is [True, False, False, False, False, True]
State prediction error at timestep 1000 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1000 of 1
Current timestep = 1001. State = [[-0.20112437  0.17079556]]. Action = [[ 0.20756954 -0.18281613  0.02699175  0.21149325]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1001 is [True, False, False, False, False, True]
State prediction error at timestep 1001 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1002. State = [[-0.2003129   0.17117907]]. Action = [[-0.01418006  0.19397914  0.20335227  0.29021478]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1002 is [True, False, False, False, False, True]
State prediction error at timestep 1002 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1003. State = [[-0.19936274  0.17129569]]. Action = [[ 0.02751222  0.15655518 -0.13792416  0.12261784]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1003 is [True, False, False, False, False, True]
State prediction error at timestep 1003 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1004. State = [[-0.19859816  0.17116155]]. Action = [[ 0.15494427  0.10569936 -0.02871126  0.77759373]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1004 is [True, False, False, False, False, True]
State prediction error at timestep 1004 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1005. State = [[-0.19800854  0.17126615]]. Action = [[ 0.10141873 -0.2228741   0.16719973 -0.40285355]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1005 is [True, False, False, False, False, True]
State prediction error at timestep 1005 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1005 of 1
Current timestep = 1006. State = [[-0.19746992  0.17162518]]. Action = [[ 0.23784915 -0.20613678  0.03996709  0.6528218 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1006 is [True, False, False, False, False, True]
State prediction error at timestep 1006 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1007. State = [[-0.19704708  0.17168738]]. Action = [[ 0.1957674  -0.19837561 -0.16555324 -0.06298286]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1007 is [True, False, False, False, False, True]
State prediction error at timestep 1007 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1008. State = [[-0.19534022  0.17206173]]. Action = [[0.03407156 0.08083701 0.2076363  0.54986024]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1008 is [True, False, False, False, False, True]
State prediction error at timestep 1008 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1008 of 1
Current timestep = 1009. State = [[-0.19469382  0.1725213 ]]. Action = [[ 0.01855934 -0.23424692 -0.08999899  0.49920905]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1009 is [True, False, False, False, False, True]
State prediction error at timestep 1009 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1010. State = [[-0.19270967  0.1734827 ]]. Action = [[ 0.1191552   0.00939497  0.21264619 -0.07960683]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1010 is [True, False, False, False, False, True]
State prediction error at timestep 1010 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1010 of 1
Current timestep = 1011. State = [[-0.18922491  0.17447598]]. Action = [[-0.11921489  0.0412122  -0.23900352  0.4163977 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1011 is [True, False, False, False, False, True]
State prediction error at timestep 1011 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1011 of 1
Current timestep = 1012. State = [[-0.18908319  0.17504755]]. Action = [[ 0.14968938  0.05147931  0.10428664 -0.43258   ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1012 is [True, False, False, False, False, True]
State prediction error at timestep 1012 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 1013. State = [[-0.1890165   0.17572407]]. Action = [[0.19564101 0.01720098 0.14721993 0.3008846 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1013 is [True, False, False, False, False, True]
State prediction error at timestep 1013 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1014. State = [[-0.18805042  0.17793588]]. Action = [[-7.0680559e-02 -5.7408214e-04  5.0886780e-02  7.7463031e-01]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1014 is [True, False, False, False, False, True]
State prediction error at timestep 1014 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1014 of 1
Current timestep = 1015. State = [[-0.18827295  0.1783877 ]]. Action = [[-0.16320771 -0.1397957   0.20255303 -0.6803307 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1015 is [True, False, False, False, False, True]
State prediction error at timestep 1015 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 1016. State = [[-0.18927452  0.17951834]]. Action = [[-0.00518237 -0.05644694  0.06219935 -0.24039668]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1016 is [True, False, False, False, False, True]
State prediction error at timestep 1016 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1016 of 1
Current timestep = 1017. State = [[-0.18932427  0.1793832 ]]. Action = [[ 0.07883227  0.24236381 -0.04383418 -0.5910767 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1017 is [True, False, False, False, False, True]
State prediction error at timestep 1017 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1018. State = [[-0.18921421  0.17928214]]. Action = [[0.04129741 0.1957668  0.22171766 0.93426704]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1018 is [True, False, False, False, False, True]
State prediction error at timestep 1018 is tensor(8.0186e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1018 of 1
Current timestep = 1019. State = [[-0.18925974  0.17931119]]. Action = [[ 0.01111248 -0.02262725 -0.23642088 -0.35256398]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1019 is [True, False, False, False, False, True]
State prediction error at timestep 1019 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1020. State = [[-0.18922657  0.17918147]]. Action = [[-0.13154227  0.22422937 -0.21631362 -0.812325  ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1020 is [True, False, False, False, False, True]
State prediction error at timestep 1020 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 1021. State = [[-0.18912256  0.17862381]]. Action = [[-0.08538449  0.01017734 -0.01995258  0.11560714]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1021 is [True, False, False, False, False, True]
State prediction error at timestep 1021 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1022. State = [[-0.18925326  0.17882848]]. Action = [[-0.07792994  0.13988146 -0.18046826 -0.28294975]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1022 is [True, False, False, False, False, True]
State prediction error at timestep 1022 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1023. State = [[-0.1895191   0.17896049]]. Action = [[-0.21532582  0.15561086  0.0173541   0.7986889 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1023 is [True, False, False, False, False, True]
State prediction error at timestep 1023 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1024. State = [[-0.18968418  0.17899863]]. Action = [[-0.22915465  0.16281027 -0.21321459  0.411232  ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1024 is [True, False, False, False, False, True]
State prediction error at timestep 1024 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1025. State = [[-0.1901895   0.17887893]]. Action = [[ 0.04016298  0.06753391  0.13101274 -0.6681125 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1025 is [True, False, False, False, False, True]
State prediction error at timestep 1025 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1025 of 1
Current timestep = 1026. State = [[-0.19037905  0.17906253]]. Action = [[-0.20525002  0.14506632  0.08618835 -0.2591061 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1026 is [True, False, False, False, False, True]
State prediction error at timestep 1026 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1027. State = [[-0.19102569  0.1801328 ]]. Action = [[-0.10857219  0.02857512  0.2007182  -0.14762819]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1027 is [True, False, False, False, False, True]
State prediction error at timestep 1027 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1027 of 1
Current timestep = 1028. State = [[-0.19270478  0.182358  ]]. Action = [[-0.06303513 -0.11921161 -0.1774256   0.91153574]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1028 is [True, False, False, False, False, True]
State prediction error at timestep 1028 is tensor(9.1057e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1028 of 1
Current timestep = 1029. State = [[-0.19354801  0.18155669]]. Action = [[-0.08006577  0.11663294 -0.13537094  0.7657567 ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 1029 is [True, False, False, False, False, True]
State prediction error at timestep 1029 is tensor(5.3250e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1029 of 1
Current timestep = 1030. State = [[-0.19664226  0.18437506]]. Action = [[ 0.13136289  0.01265091 -0.0151996  -0.3812772 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 1030 is [True, False, False, False, False, True]
State prediction error at timestep 1030 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1030 of 1
Current timestep = 1031. State = [[-0.19705814  0.18470697]]. Action = [[ 0.02027181 -0.18141046 -0.14253831 -0.90161276]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 1031 is [True, False, False, False, False, True]
State prediction error at timestep 1031 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 1032. State = [[-0.19723582  0.18482941]]. Action = [[ 0.20254222 -0.16036297 -0.06465787 -0.6732582 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 1032 is [True, False, False, False, False, True]
State prediction error at timestep 1032 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1033. State = [[-0.19751453  0.18502921]]. Action = [[0.18079415 0.11580163 0.04197437 0.52561307]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 1033 is [True, False, False, False, False, True]
State prediction error at timestep 1033 is tensor(8.6812e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1033 of 1
Current timestep = 1034. State = [[-0.1982134   0.18544191]]. Action = [[-0.11263198 -0.04083745  0.09059879 -0.07405639]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 1034 is [True, False, False, False, False, True]
State prediction error at timestep 1034 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1035. State = [[-0.19897294  0.1857572 ]]. Action = [[-0.12715834  0.09683347  0.12056413  0.8349607 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 1035 is [True, False, False, False, False, True]
State prediction error at timestep 1035 is tensor(8.7910e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1036. State = [[-0.19995746  0.18708786]]. Action = [[-0.00590676 -0.17111085 -0.13523056  0.51443875]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 1036 is [True, False, False, False, False, True]
State prediction error at timestep 1036 is tensor(2.0055e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1036 of 1
Current timestep = 1037. State = [[-0.20087591  0.18831119]]. Action = [[ 0.20668703  0.07805735 -0.08278257  0.71884656]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 1037 is [True, False, False, False, False, True]
State prediction error at timestep 1037 is tensor(2.6757e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1038. State = [[-0.20138057  0.18899664]]. Action = [[-0.14282705 -0.22740534 -0.19705331 -0.28812718]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 1038 is [True, False, False, False, False, True]
State prediction error at timestep 1038 is tensor(2.5268e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1039. State = [[-0.2041621   0.19165432]]. Action = [[ 0.12417084 -0.00967929  0.19681239 -0.69026715]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 1039 is [True, False, False, False, False, True]
State prediction error at timestep 1039 is tensor(8.2435e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1039 of -1
Current timestep = 1040. State = [[-0.20431621  0.19121945]]. Action = [[-0.06398226 -0.0657829   0.1496703   0.9868884 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 1040 is [True, False, False, False, False, True]
State prediction error at timestep 1040 is tensor(6.4262e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1040 of -1
Current timestep = 1041. State = [[-0.20423175  0.19091092]]. Action = [[ 0.09916806 -0.20463589 -0.18767597  0.23204815]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 1041 is [True, False, False, False, False, True]
State prediction error at timestep 1041 is tensor(3.1945e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1042. State = [[-0.20411004  0.18987349]]. Action = [[-0.13199496  0.04238683 -0.14133173 -0.49495924]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 1042 is [True, False, False, False, False, True]
State prediction error at timestep 1042 is tensor(3.5227e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1042 of -1
Current timestep = 1043. State = [[-0.20465627  0.19028251]]. Action = [[-0.24714687  0.07213107  0.02714241  0.82810223]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 1043 is [True, False, False, False, False, True]
State prediction error at timestep 1043 is tensor(2.1501e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1044. State = [[-0.2051465   0.19080918]]. Action = [[-0.15494715 -0.15711306 -0.20376313 -0.00356025]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 1044 is [True, False, False, False, False, True]
State prediction error at timestep 1044 is tensor(7.8423e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1045. State = [[-0.20545603  0.19096299]]. Action = [[-0.13091542 -0.21697557  0.15791911  0.5594814 ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 1045 is [True, False, False, False, False, True]
State prediction error at timestep 1045 is tensor(2.2928e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1045 of -1
Current timestep = 1046. State = [[-0.20611025  0.1913003 ]]. Action = [[ 0.20285317  0.16692725 -0.14491273  0.5452106 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 1046 is [True, False, False, False, False, True]
State prediction error at timestep 1046 is tensor(5.3220e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1047. State = [[-0.20686983  0.19181292]]. Action = [[-0.08758512  0.17265883  0.08881521 -0.2786659 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 1047 is [True, False, False, False, False, True]
State prediction error at timestep 1047 is tensor(3.5722e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1048. State = [[-0.20734552  0.19190471]]. Action = [[-0.21690826 -0.17954715 -0.18722433 -0.9205701 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 1048 is [True, False, False, False, False, True]
State prediction error at timestep 1048 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1049. State = [[-0.20787029  0.19207978]]. Action = [[-0.1416263  -0.21135497  0.08830518 -0.10022014]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 1049 is [True, False, False, False, False, True]
State prediction error at timestep 1049 is tensor(1.3777e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1049 of -1
Current timestep = 1050. State = [[-0.20823222  0.1923434 ]]. Action = [[-0.18343106 -0.02630183 -0.18239424 -0.01682109]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 1050 is [True, False, False, False, False, True]
State prediction error at timestep 1050 is tensor(9.4142e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1051. State = [[-0.20853887  0.19233924]]. Action = [[-0.00568272  0.21683711  0.04455388  0.6921197 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 1051 is [True, False, False, False, False, True]
State prediction error at timestep 1051 is tensor(2.2374e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1052. State = [[-0.2089927  0.1924087]]. Action = [[0.21427688 0.19216797 0.05214426 0.76660943]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 1052 is [True, False, False, False, False, True]
State prediction error at timestep 1052 is tensor(1.4182e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1052 of -1
Current timestep = 1053. State = [[-0.20919266  0.19254376]]. Action = [[ 0.14999479 -0.18275805  0.19254261  0.7233298 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 1053 is [True, False, False, False, False, True]
State prediction error at timestep 1053 is tensor(5.4749e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1054. State = [[-0.21023014  0.19236585]]. Action = [[0.07626069 0.05205742 0.09960824 0.58593774]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 1054 is [True, False, False, False, False, True]
State prediction error at timestep 1054 is tensor(1.7652e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1054 of -1
Current timestep = 1055. State = [[-0.21044254  0.1924529 ]]. Action = [[ 0.15083736 -0.19201663  0.21982795  0.62652206]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 1055 is [True, False, False, False, False, True]
State prediction error at timestep 1055 is tensor(5.0311e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1056. State = [[-0.21073069  0.19265075]]. Action = [[ 0.16523337 -0.00382356 -0.16098003  0.31596982]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 1056 is [True, False, False, False, False, True]
State prediction error at timestep 1056 is tensor(3.9453e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1057. State = [[-0.21097057  0.19268188]]. Action = [[ 0.06143939 -0.1224318   0.01780096 -0.4239465 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 1057 is [True, False, False, False, False, True]
State prediction error at timestep 1057 is tensor(1.2630e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1057 of -1
Current timestep = 1058. State = [[-0.21039572  0.19153547]]. Action = [[-0.14257307 -0.13375972 -0.03341     0.03052115]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 1058 is [True, False, False, False, False, True]
State prediction error at timestep 1058 is tensor(6.4620e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1059. State = [[-0.20954537  0.18873239]]. Action = [[ 0.05275491 -0.09563372 -0.04884125 -0.71913964]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 1059 is [True, False, False, False, False, True]
State prediction error at timestep 1059 is tensor(6.3033e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1059 of -1
Current timestep = 1060. State = [[-0.2079955   0.18447955]]. Action = [[ 0.01431924  0.04352975 -0.23293859  0.23606753]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 1060 is [True, False, False, False, False, True]
State prediction error at timestep 1060 is tensor(7.4716e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1060 of -1
Current timestep = 1061. State = [[-0.20742704  0.18275815]]. Action = [[-0.07454303  0.10761982 -0.2479575   0.7984414 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 1061 is [True, False, False, False, False, True]
State prediction error at timestep 1061 is tensor(3.1996e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1061 of -1
Current timestep = 1062. State = [[-0.2079149   0.18349895]]. Action = [[-0.24492814  0.05195788 -0.00174266  0.0204097 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 1062 is [True, False, False, False, False, True]
State prediction error at timestep 1062 is tensor(6.6074e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1063. State = [[-0.20826851  0.18418965]]. Action = [[-0.03892574  0.23811549 -0.24095763 -0.06311214]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 1063 is [True, False, False, False, False, True]
State prediction error at timestep 1063 is tensor(2.8204e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1063 of -1
Current timestep = 1064. State = [[-0.20900764  0.18522555]]. Action = [[ 0.03269958 -0.06171089  0.10173896 -0.03229833]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 1064 is [True, False, False, False, False, True]
State prediction error at timestep 1064 is tensor(1.8483e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1065. State = [[-0.20868188  0.18410839]]. Action = [[-0.0352414  -0.06434524  0.094946   -0.11300367]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 1065 is [True, False, False, False, False, True]
State prediction error at timestep 1065 is tensor(3.5956e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1066. State = [[-0.20857155  0.18368079]]. Action = [[-0.19256534  0.20366323  0.05100381  0.38549018]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 1066 is [True, False, False, False, False, True]
State prediction error at timestep 1066 is tensor(5.8775e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1067. State = [[-0.20815516  0.18212274]]. Action = [[-0.0079674   0.0396308  -0.19432595  0.15878868]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 1067 is [True, False, False, False, False, True]
State prediction error at timestep 1067 is tensor(5.3461e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1067 of -1
Current timestep = 1068. State = [[-0.20817201  0.18210928]]. Action = [[-0.21988983  0.05211702 -0.14854412 -0.57172227]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 1068 is [True, False, False, False, False, True]
State prediction error at timestep 1068 is tensor(6.4659e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1069. State = [[-0.2082404   0.18221217]]. Action = [[ 0.19999748 -0.24122241  0.1762138  -0.63166976]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 1069 is [True, False, False, False, False, True]
State prediction error at timestep 1069 is tensor(6.8992e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1070. State = [[-0.20835981  0.18227558]]. Action = [[ 0.05108631  0.05448294 -0.16454656 -0.48069865]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 1070 is [True, False, False, False, False, True]
State prediction error at timestep 1070 is tensor(2.9458e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1071. State = [[-0.20837702  0.18234025]]. Action = [[-0.19398598  0.21023178 -0.01174903 -0.6355985 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 1071 is [True, False, False, False, False, True]
State prediction error at timestep 1071 is tensor(8.0776e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1072. State = [[-0.20853657  0.18262231]]. Action = [[ 0.00788534 -0.02999996 -0.22100209  0.72947574]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 1072 is [True, False, False, False, False, True]
State prediction error at timestep 1072 is tensor(8.7163e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1072 of -1
Current timestep = 1073. State = [[-0.20848486  0.18242835]]. Action = [[-0.196926   -0.15141791 -0.11541775  0.20801318]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 1073 is [True, False, False, False, False, True]
State prediction error at timestep 1073 is tensor(3.1846e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1074. State = [[-0.20846763  0.18236367]]. Action = [[-0.15010123 -0.05916153  0.15678883  0.49035478]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 1074 is [True, False, False, False, False, True]
State prediction error at timestep 1074 is tensor(5.1240e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1074 of -1
Current timestep = 1075. State = [[-0.2085017   0.18241486]]. Action = [[ 0.07388261  0.14993766 -0.1198616  -0.7881932 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 1075 is [True, False, False, False, False, True]
State prediction error at timestep 1075 is tensor(3.2112e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1076. State = [[-0.20853576  0.18246603]]. Action = [[ 0.20539623 -0.00593066 -0.14732337 -0.22834456]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 1076 is [True, False, False, False, False, True]
State prediction error at timestep 1076 is tensor(3.3606e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1077. State = [[-0.20857017  0.18251769]]. Action = [[-0.22714718 -0.08443081  0.2282615  -0.573099  ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 1077 is [True, False, False, False, False, True]
State prediction error at timestep 1077 is tensor(9.8327e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1078. State = [[-0.20855293  0.18245304]]. Action = [[ 0.12356845 -0.23751253 -0.15344635 -0.27898395]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 1078 is [True, False, False, False, False, True]
State prediction error at timestep 1078 is tensor(9.4136e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1079. State = [[-0.20855293  0.18245304]]. Action = [[ 0.19200647 -0.17877178 -0.15159102  0.86500454]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 1079 is [True, False, False, False, False, True]
State prediction error at timestep 1079 is tensor(4.6650e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1079 of -1
Current timestep = 1080. State = [[-0.20853582  0.18238881]]. Action = [[ 0.11829212  0.1450935  -0.05771217  0.2533977 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 1080 is [True, False, False, False, False, True]
State prediction error at timestep 1080 is tensor(4.9253e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1081. State = [[-0.20851861  0.18232416]]. Action = [[ 0.2135728  -0.04365802  0.228585   -0.6221167 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 1081 is [True, False, False, False, False, True]
State prediction error at timestep 1081 is tensor(3.0010e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1082. State = [[-0.20851861  0.18232416]]. Action = [[-0.17319372  0.20168692  0.07898709  0.22826457]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 1082 is [True, False, False, False, False, True]
State prediction error at timestep 1082 is tensor(7.3718e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1082 of -1
Current timestep = 1083. State = [[-0.20851861  0.18232416]]. Action = [[ 0.19766459 -0.22462231 -0.1746593   0.87093556]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 1083 is [True, False, False, False, False, True]
State prediction error at timestep 1083 is tensor(1.5566e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1084. State = [[-0.20851861  0.18232416]]. Action = [[ 0.0011051   0.0266827  -0.13715194  0.5303407 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 1084 is [True, False, False, False, False, True]
State prediction error at timestep 1084 is tensor(9.9466e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1085. State = [[-0.20851861  0.18232416]]. Action = [[ 0.03080922  0.13721132 -0.23993853 -0.4323665 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 1085 is [True, False, False, False, False, True]
State prediction error at timestep 1085 is tensor(1.7208e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1086. State = [[-0.20854478  0.18243252]]. Action = [[0.06759986 0.1122629  0.12117136 0.7226608 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 1086 is [True, False, False, False, False, True]
State prediction error at timestep 1086 is tensor(1.0627e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1087. State = [[-0.20856707  0.18306534]]. Action = [[ 0.12743089 -0.21970771  0.13611194  0.8901094 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 1087 is [True, False, False, False, False, True]
State prediction error at timestep 1087 is tensor(6.2481e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1088. State = [[-0.2084045  0.1852704]]. Action = [[ 0.1283508  -0.07485996 -0.16254567  0.8184414 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 1088 is [True, False, False, False, False, True]
State prediction error at timestep 1088 is tensor(1.4303e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1089. State = [[-0.2074125   0.18537357]]. Action = [[-0.21711104  0.16332427 -0.2138213  -0.8820666 ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 1089 is [True, False, False, False, False, True]
State prediction error at timestep 1089 is tensor(8.5004e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1090. State = [[-0.20633504  0.185345  ]]. Action = [[0.17126966 0.00878406 0.19348559 0.0502944 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 1090 is [True, False, False, False, False, True]
State prediction error at timestep 1090 is tensor(5.9680e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1091. State = [[-0.20560929  0.18524072]]. Action = [[-0.09070031 -0.17374358  0.22634459  0.0049094 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 1091 is [True, False, False, False, False, True]
State prediction error at timestep 1091 is tensor(3.6675e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1092. State = [[-0.20479262  0.18590848]]. Action = [[ 0.20651111 -0.07089874  0.01715773  0.48040915]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 1092 is [True, False, False, False, False, True]
State prediction error at timestep 1092 is tensor(7.0646e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1092 of -1
Current timestep = 1093. State = [[-0.20305493  0.18611875]]. Action = [[-0.06780916 -0.11620089  0.06808287 -0.82201225]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 1093 is [True, False, False, False, False, True]
State prediction error at timestep 1093 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1094. State = [[-0.20230427  0.18400359]]. Action = [[-0.01022515  0.1259417   0.1464293  -0.9703971 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 1094 is [True, False, False, False, False, True]
State prediction error at timestep 1094 is tensor(4.2190e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1095. State = [[-0.20270208  0.18437627]]. Action = [[-0.23704287  0.00405014 -0.05305846 -0.43415165]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 1095 is [True, False, False, False, False, True]
State prediction error at timestep 1095 is tensor(3.4902e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1096. State = [[-0.20279002  0.18514849]]. Action = [[0.09860101 0.06393248 0.05897212 0.15666604]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 1096 is [True, False, False, False, False, True]
State prediction error at timestep 1096 is tensor(2.3425e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1096 of 1
Current timestep = 1097. State = [[-0.20275423  0.18564463]]. Action = [[-0.19010925  0.21891981  0.24243069 -0.71881807]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 1097 is [True, False, False, False, False, True]
State prediction error at timestep 1097 is tensor(6.1606e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1098. State = [[-0.20273827  0.1858774 ]]. Action = [[0.15952563 0.11735231 0.21968919 0.6226014 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 1098 is [True, False, False, False, False, True]
State prediction error at timestep 1098 is tensor(4.7476e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1099. State = [[-0.20259969  0.18599764]]. Action = [[-0.08878917  0.17602116 -0.18102458  0.49710464]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 1099 is [True, False, False, False, False, True]
State prediction error at timestep 1099 is tensor(1.1878e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1100. State = [[-0.20240276  0.1861533 ]]. Action = [[-0.01156516 -0.17912394  0.10400149  0.1550951 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 1100 is [True, False, False, False, False, True]
State prediction error at timestep 1100 is tensor(7.5168e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1100 of 1
Current timestep = 1101. State = [[-0.20235519  0.18628775]]. Action = [[-0.24010584  0.00518855  0.07553184  0.44726574]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 1101 is [True, False, False, False, False, True]
State prediction error at timestep 1101 is tensor(7.7596e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1102. State = [[-0.20168357  0.18691568]]. Action = [[-0.01844709  0.00271246 -0.12295249 -0.5929789 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 1102 is [True, False, False, False, False, True]
State prediction error at timestep 1102 is tensor(7.6548e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1102 of 1
Current timestep = 1103. State = [[-0.2015543   0.18753292]]. Action = [[-0.09124465  0.01798403 -0.18950689 -0.7912604 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 1103 is [True, False, False, False, False, True]
State prediction error at timestep 1103 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1103 of 1
Current timestep = 1104. State = [[-0.20193046  0.18774873]]. Action = [[ 0.24017796 -0.12264216 -0.24333127  0.6635722 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 1104 is [True, False, False, False, False, True]
State prediction error at timestep 1104 is tensor(8.8797e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1105. State = [[-0.2022798   0.18827716]]. Action = [[-0.19012254 -0.20987193  0.23723143 -0.991065  ]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 1105 is [True, False, False, False, False, True]
State prediction error at timestep 1105 is tensor(7.6872e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1106. State = [[-0.20315315  0.1899549 ]]. Action = [[ 0.09240085  0.07227191 -0.15240954 -0.8712385 ]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 1106 is [True, False, False, False, False, True]
State prediction error at timestep 1106 is tensor(4.3850e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1106 of 1
Current timestep = 1107. State = [[-0.20317596  0.19020109]]. Action = [[-0.16701838  0.03580877  0.11668268  0.86199737]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 1107 is [True, False, False, False, False, True]
State prediction error at timestep 1107 is tensor(5.1220e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1108. State = [[-0.20328063  0.1905041 ]]. Action = [[-2.0990953e-01  2.0909780e-01 -4.9306452e-04 -8.1784731e-01]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 1108 is [True, False, False, False, False, True]
State prediction error at timestep 1108 is tensor(6.8108e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1109. State = [[-0.20365775  0.19140774]]. Action = [[-0.1212754  -0.06282392 -0.04121637  0.6368443 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 1109 is [True, False, False, False, False, True]
State prediction error at timestep 1109 is tensor(2.3596e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1109 of 1
Current timestep = 1110. State = [[-0.20373693  0.19172089]]. Action = [[-0.13925795  0.20605865 -0.20327334  0.6991106 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 1110 is [True, False, False, False, False, True]
State prediction error at timestep 1110 is tensor(5.7751e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1111. State = [[-0.20401827  0.19213557]]. Action = [[ 0.1035676   0.06575459 -0.15154505 -0.38143194]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 1111 is [True, False, False, False, False, True]
State prediction error at timestep 1111 is tensor(3.0895e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1111 of 1
Current timestep = 1112. State = [[-0.20395504  0.19229086]]. Action = [[ 0.2240709  -0.17387387  0.07471603 -0.3374226 ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 1112 is [True, False, False, False, False, True]
State prediction error at timestep 1112 is tensor(3.1449e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1113. State = [[-0.20391484  0.19252285]]. Action = [[-0.07435825  0.2415537  -0.00674872 -0.39290988]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 1113 is [True, False, False, False, False, True]
State prediction error at timestep 1113 is tensor(1.4858e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1114. State = [[-0.20410308  0.19287272]]. Action = [[ 0.06876886 -0.07588184  0.0355143  -0.21099442]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 1114 is [True, False, False, False, False, True]
State prediction error at timestep 1114 is tensor(2.8722e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1114 of 1
Current timestep = 1115. State = [[-0.20344688  0.19254176]]. Action = [[-0.09789321  0.23712009  0.11570957  0.02900279]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 1115 is [True, False, False, False, False, True]
State prediction error at timestep 1115 is tensor(3.1154e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1116. State = [[-0.20319398  0.19230515]]. Action = [[-0.14061387  0.1274428  -0.11742064 -0.5867261 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 1116 is [True, False, False, False, False, True]
State prediction error at timestep 1116 is tensor(4.5097e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1117. State = [[-0.20217544  0.19226325]]. Action = [[ 0.00751215 -0.02645773  0.23112592  0.7449548 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 1117 is [True, False, False, False, False, True]
State prediction error at timestep 1117 is tensor(4.7504e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1117 of 1
Current timestep = 1118. State = [[-0.20063706  0.19193362]]. Action = [[-0.17881273 -0.08078721  0.16372603  0.06442034]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 1118 is [True, False, False, False, False, True]
State prediction error at timestep 1118 is tensor(8.3375e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1119. State = [[-0.2001491   0.19178918]]. Action = [[ 0.05307421 -0.20701443  0.22926247 -0.12193239]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 1119 is [True, False, False, False, False, True]
State prediction error at timestep 1119 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1120. State = [[-0.19966196  0.19159685]]. Action = [[-0.22438428  0.16498059  0.10246801 -0.5815106 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 1120 is [True, False, False, False, False, True]
State prediction error at timestep 1120 is tensor(5.4096e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1121. State = [[-0.19869445  0.1918742 ]]. Action = [[-0.10417269  0.08563423 -0.17037697 -0.6302623 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 1121 is [True, False, False, False, False, True]
State prediction error at timestep 1121 is tensor(8.4698e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1121 of 1
Current timestep = 1122. State = [[-0.20002244  0.19388638]]. Action = [[-0.06433402  0.04760104  0.22254592 -0.80409086]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 1122 is [True, False, False, False, False, True]
State prediction error at timestep 1122 is tensor(5.5592e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1122 of 1
Current timestep = 1123. State = [[-0.20057651  0.19436154]]. Action = [[-0.1771555  -0.18239823 -0.03669353  0.14476824]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 1123 is [True, False, False, False, False, True]
State prediction error at timestep 1123 is tensor(2.4470e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1124. State = [[-0.20104916  0.19549888]]. Action = [[-0.20294775 -0.05421588 -0.11963926 -0.48048854]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 1124 is [True, False, False, False, False, True]
State prediction error at timestep 1124 is tensor(4.8564e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1125. State = [[-0.2021845   0.19735657]]. Action = [[ 0.06796607 -0.08657551 -0.10265736 -0.283584  ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 1125 is [True, False, False, False, False, True]
State prediction error at timestep 1125 is tensor(9.0189e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1125 of 1
Current timestep = 1126. State = [[-0.20166159  0.19626297]]. Action = [[ 0.03300476 -0.12600912 -0.14898735 -0.01552719]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 1126 is [True, False, False, False, False, True]
State prediction error at timestep 1126 is tensor(2.3510e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1126 of 1
Current timestep = 1127. State = [[-0.20118687  0.19521524]]. Action = [[ 0.24737537 -0.03031261 -0.01955962 -0.9429169 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 1127 is [True, False, False, False, False, True]
State prediction error at timestep 1127 is tensor(9.5104e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1128. State = [[-0.20047483  0.19398494]]. Action = [[ 0.139763   -0.18966866  0.1395083   0.06858933]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 1128 is [True, False, False, False, False, True]
State prediction error at timestep 1128 is tensor(2.2001e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1129. State = [[-0.19977172  0.19189343]]. Action = [[ 0.05016097  0.04379129 -0.16487525  0.28364348]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 1129 is [True, False, False, False, False, True]
State prediction error at timestep 1129 is tensor(2.2622e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1129 of 1
Current timestep = 1130. State = [[-0.19922289  0.19125456]]. Action = [[-0.05892867 -0.18895192  0.13736695 -0.41625738]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 1130 is [True, False, False, False, False, True]
State prediction error at timestep 1130 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1131. State = [[-0.1992019   0.19125448]]. Action = [[-0.04952087  0.14755759 -0.22014776 -0.5491346 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 1131 is [True, False, False, False, False, True]
State prediction error at timestep 1131 is tensor(3.0307e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1131 of 1
Current timestep = 1132. State = [[-0.19853216  0.19054276]]. Action = [[-0.03368784  0.07017314  0.1264872   0.3340373 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 1132 is [True, False, False, False, False, True]
State prediction error at timestep 1132 is tensor(4.4231e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1133. State = [[-0.19881354  0.19130819]]. Action = [[-0.06253913 -0.01933865  0.09157276  0.59911513]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 1133 is [True, False, False, False, False, True]
State prediction error at timestep 1133 is tensor(9.4916e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1134. State = [[-0.19890976  0.19146787]]. Action = [[ 0.20507714 -0.02942412  0.1771735  -0.92121387]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 1134 is [True, False, False, False, False, True]
State prediction error at timestep 1134 is tensor(5.6601e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1135. State = [[-0.1990869   0.19195691]]. Action = [[ 0.07597929 -0.1860749   0.11347866 -0.04985732]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 1135 is [True, False, False, False, False, True]
State prediction error at timestep 1135 is tensor(4.9183e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1135 of 1
Current timestep = 1136. State = [[-0.19931623  0.19233622]]. Action = [[ 0.20865041 -0.08213639  0.06847647  0.2895738 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 1136 is [True, False, False, False, False, True]
State prediction error at timestep 1136 is tensor(1.8110e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1137. State = [[-0.19938508  0.19221364]]. Action = [[-0.0466285   0.16860643  0.09309733  0.0258677 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 1137 is [True, False, False, False, False, True]
State prediction error at timestep 1137 is tensor(5.8589e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1138. State = [[-0.1994908   0.19251883]]. Action = [[-0.10234612 -0.07741284  0.0533672  -0.67070335]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 1138 is [True, False, False, False, False, True]
State prediction error at timestep 1138 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1139. State = [[-0.19954449  0.19155237]]. Action = [[-0.09872022 -0.11561483 -0.10693878  0.49252057]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 1139 is [True, False, False, False, False, True]
State prediction error at timestep 1139 is tensor(3.5607e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1140. State = [[-0.19960229  0.1906107 ]]. Action = [[-0.21013565 -0.16377953 -0.0970715  -0.1720202 ]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 1140 is [True, False, False, False, False, True]
State prediction error at timestep 1140 is tensor(1.4960e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1141. State = [[-0.19952409  0.18801844]]. Action = [[-0.00863214 -0.04861242  0.1789428  -0.36262596]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 1141 is [True, False, False, False, False, True]
State prediction error at timestep 1141 is tensor(9.9538e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1142. State = [[-0.19942564  0.18458252]]. Action = [[ 0.05856061 -0.11329117  0.11173648  0.30736113]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 1142 is [True, False, False, False, False, True]
State prediction error at timestep 1142 is tensor(1.1981e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1143. State = [[-0.19895932  0.18314531]]. Action = [[ 0.15257472 -0.21692453  0.08240482  0.00970042]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 1143 is [True, False, False, False, False, True]
State prediction error at timestep 1143 is tensor(7.0484e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1144. State = [[-0.19856773  0.18184637]]. Action = [[ 0.02826819  0.1967771  -0.10188033  0.96450067]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 1144 is [True, False, False, False, False, True]
State prediction error at timestep 1144 is tensor(2.9917e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1144 of 1
Current timestep = 1145. State = [[-0.19795749  0.17771754]]. Action = [[-0.10014492 -0.04948618 -0.14659378  0.7416259 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 1145 is [True, False, False, False, False, True]
State prediction error at timestep 1145 is tensor(2.7180e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1146. State = [[-0.19825168  0.17679043]]. Action = [[-0.16357967  0.11830553 -0.02939084 -0.3540588 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 1146 is [True, False, False, False, False, True]
State prediction error at timestep 1146 is tensor(8.0680e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1146 of 1
Current timestep = 1147. State = [[-0.1987984   0.17567968]]. Action = [[ 0.15056604 -0.21759276 -0.2185109  -0.3758644 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 1147 is [True, False, False, False, False, True]
State prediction error at timestep 1147 is tensor(5.8188e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1148. State = [[-0.1990094   0.17476468]]. Action = [[ 0.19422257 -0.21478072 -0.2168714   0.23572981]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 1148 is [True, False, False, False, False, True]
State prediction error at timestep 1148 is tensor(1.7718e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1149. State = [[-0.19948827  0.17385878]]. Action = [[ 0.07044327 -0.24870825  0.05604163  0.6244689 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 1149 is [True, False, False, False, False, True]
State prediction error at timestep 1149 is tensor(4.3326e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1150. State = [[-0.19974959  0.17328492]]. Action = [[0.02326003 0.15774351 0.120451   0.8409859 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 1150 is [True, False, False, False, False, True]
State prediction error at timestep 1150 is tensor(1.0389e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1151. State = [[-0.19995072  0.17284903]]. Action = [[ 0.18278396 -0.22716197 -0.09240246 -0.5393755 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 1151 is [True, False, False, False, False, True]
State prediction error at timestep 1151 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1151 of 1
Current timestep = 1152. State = [[-0.2002353   0.17249888]]. Action = [[0.17726743 0.21302709 0.24038339 0.7536459 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 1152 is [True, False, False, False, False, True]
State prediction error at timestep 1152 is tensor(2.9151e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1153. State = [[-0.20095849  0.17134422]]. Action = [[-0.13122717  0.0892399   0.1035201   0.8502748 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 1153 is [True, False, False, False, False, True]
State prediction error at timestep 1153 is tensor(2.1263e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1153 of 1
Current timestep = 1154. State = [[-0.20213754  0.17215385]]. Action = [[ 0.24532437  0.13017195 -0.07985711 -0.75080997]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 1154 is [True, False, False, False, False, True]
State prediction error at timestep 1154 is tensor(5.2315e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1155. State = [[-0.20286709  0.17264248]]. Action = [[ 0.17642936  0.03668001 -0.20257233 -0.48214227]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 1155 is [True, False, False, False, False, True]
State prediction error at timestep 1155 is tensor(6.3541e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1156. State = [[-0.2036831   0.17314164]]. Action = [[ 0.22418737 -0.19224988  0.01812145 -0.51853377]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 1156 is [True, False, False, False, False, True]
State prediction error at timestep 1156 is tensor(8.1039e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1157. State = [[-0.20466371  0.1734103 ]]. Action = [[ 0.1855492   0.11709192  0.04701325 -0.37335443]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 1157 is [True, False, False, False, False, True]
State prediction error at timestep 1157 is tensor(7.2645e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1158. State = [[-0.20646305  0.17419891]]. Action = [[ 0.04774472 -0.22591817  0.17567664  0.17592132]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 1158 is [True, False, False, False, False, True]
State prediction error at timestep 1158 is tensor(3.9366e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1158 of 1
Current timestep = 1159. State = [[-0.20749807  0.17442   ]]. Action = [[-0.0168893   0.21676564 -0.02990432 -0.78049517]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 1159 is [True, False, False, False, False, True]
State prediction error at timestep 1159 is tensor(4.4371e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1160. State = [[-0.20800796  0.17461108]]. Action = [[-0.16491114  0.00714397  0.18174747  0.62885904]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 1160 is [True, False, False, False, False, True]
State prediction error at timestep 1160 is tensor(3.2038e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1161. State = [[-0.2098609  0.1751375]]. Action = [[ 0.10558018  0.01742819  0.05170032 -0.0528692 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 1161 is [True, False, False, False, False, True]
State prediction error at timestep 1161 is tensor(4.7542e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1161 of 1
Current timestep = 1162. State = [[-0.20988742  0.17512128]]. Action = [[ 0.15605801  0.09441113 -0.22664048 -0.03630799]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 1162 is [True, False, False, False, False, True]
State prediction error at timestep 1162 is tensor(4.9025e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1163. State = [[-0.20973992  0.17505784]]. Action = [[ 0.14306912  0.01639864 -0.18973435 -0.5534921 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 1163 is [True, False, False, False, False, True]
State prediction error at timestep 1163 is tensor(1.9216e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1164. State = [[-0.20980717  0.17508893]]. Action = [[-0.07062343 -0.2253744   0.01112285 -0.31393123]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 1164 is [True, False, False, False, False, True]
State prediction error at timestep 1164 is tensor(3.1778e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1165. State = [[-0.21006699  0.17515795]]. Action = [[-0.1134734   0.01290148 -0.19728012 -0.44824147]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 1165 is [True, False, False, False, False, True]
State prediction error at timestep 1165 is tensor(1.2026e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1165 of 1
Current timestep = 1166. State = [[-0.21060517  0.17544441]]. Action = [[ 0.14039797 -0.18926106  0.02179262 -0.65931225]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 1166 is [True, False, False, False, False, True]
State prediction error at timestep 1166 is tensor(3.3633e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1167. State = [[-0.21084672  0.17567953]]. Action = [[ 0.20775163  0.16517967 -0.0373468  -0.4742121 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 1167 is [True, False, False, False, False, True]
State prediction error at timestep 1167 is tensor(2.7562e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1168. State = [[-0.21107529  0.1759975 ]]. Action = [[ 0.05756098  0.22950745  0.18019807 -0.16300333]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 1168 is [True, False, False, False, False, True]
State prediction error at timestep 1168 is tensor(4.8204e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1169. State = [[-0.21139787  0.17616425]]. Action = [[ 0.23963553 -0.08172382  0.24657965  0.6321933 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 1169 is [True, False, False, False, False, True]
State prediction error at timestep 1169 is tensor(8.7602e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1170. State = [[-0.21169196  0.17642285]]. Action = [[-0.15322499 -0.11135073 -0.18459575  0.18216896]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 1170 is [True, False, False, False, False, True]
State prediction error at timestep 1170 is tensor(3.8675e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1171. State = [[-0.21177961  0.17648926]]. Action = [[ 0.04721245 -0.1771142   0.1937463  -0.966227  ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 1171 is [True, False, False, False, False, True]
State prediction error at timestep 1171 is tensor(1.8705e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1172. State = [[-0.21189424  0.17684518]]. Action = [[-0.20244971  0.17988506  0.18347621 -0.6780754 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 1172 is [True, False, False, False, False, True]
State prediction error at timestep 1172 is tensor(3.6994e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1172 of 1
Current timestep = 1173. State = [[-0.21226119  0.17712028]]. Action = [[-0.24286781  0.02209535  0.11806723 -0.65890276]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 1173 is [True, False, False, False, False, True]
State prediction error at timestep 1173 is tensor(5.3831e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1174. State = [[-0.21239592  0.17724502]]. Action = [[ 0.20741808  0.02463692 -0.06862192  0.10677814]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 1174 is [True, False, False, False, False, True]
State prediction error at timestep 1174 is tensor(6.2624e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1175. State = [[-0.21255168  0.17728803]]. Action = [[ 0.01633883 -0.2172239  -0.15850084  0.01750457]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 1175 is [True, False, False, False, False, True]
State prediction error at timestep 1175 is tensor(1.1782e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1175 of 1
Current timestep = 1176. State = [[-0.21268296  0.17729685]]. Action = [[ 0.06851473 -0.11086801  0.1166164  -0.8801188 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 1176 is [True, False, False, False, False, True]
State prediction error at timestep 1176 is tensor(1.4592e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1177. State = [[-0.21184918  0.17426713]]. Action = [[-0.11237767 -0.0296286   0.07374576 -0.29452753]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 1177 is [True, False, False, False, False, True]
State prediction error at timestep 1177 is tensor(2.6829e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1178. State = [[-0.21262291  0.1715495 ]]. Action = [[-0.00076939 -0.12126374 -0.15707213 -0.16252428]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 1178 is [True, False, False, False, False, True]
State prediction error at timestep 1178 is tensor(2.2360e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1178 of -1
Current timestep = 1179. State = [[-0.21276954  0.16960727]]. Action = [[-0.23221153 -0.21524355  0.05359435 -0.31721246]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 1179 is [True, False, False, False, False, True]
State prediction error at timestep 1179 is tensor(3.9011e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1180. State = [[-0.21319318  0.16629265]]. Action = [[ 0.08896464 -0.02023824  0.12431026 -0.7908759 ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 1180 is [True, False, False, False, False, True]
State prediction error at timestep 1180 is tensor(3.8000e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1180 of -1
Current timestep = 1181. State = [[-0.21266884  0.16502963]]. Action = [[-0.12393253  0.16547617 -0.0989055  -0.70767385]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 1181 is [True, False, False, False, False, True]
State prediction error at timestep 1181 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1182. State = [[-0.21229789  0.16413723]]. Action = [[ 0.14179003 -0.23250219  0.00246388 -0.09163725]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 1182 is [True, False, False, False, False, True]
State prediction error at timestep 1182 is tensor(6.9628e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1183. State = [[-0.21229455  0.1634703 ]]. Action = [[-0.17149678  0.01617157 -0.13416024 -0.9808988 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 1183 is [True, False, False, False, False, True]
State prediction error at timestep 1183 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1184. State = [[-0.21226557  0.1626748 ]]. Action = [[-0.04341853 -0.15654476  0.09924328  0.22486198]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 1184 is [True, False, False, False, False, True]
State prediction error at timestep 1184 is tensor(3.4459e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1185. State = [[-0.21203198  0.16209549]]. Action = [[-0.19482869  0.16861206 -0.23148707 -0.439034  ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 1185 is [True, False, False, False, False, True]
State prediction error at timestep 1185 is tensor(4.9751e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1186. State = [[-0.21176721  0.1610696 ]]. Action = [[-0.14294145  0.0045456  -0.11440817  0.47941697]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 1186 is [True, False, False, False, False, True]
State prediction error at timestep 1186 is tensor(3.0729e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1186 of -1
Current timestep = 1187. State = [[-0.21166657  0.16025472]]. Action = [[0.22482315 0.15529975 0.18860054 0.9580091 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 1187 is [True, False, False, False, False, True]
State prediction error at timestep 1187 is tensor(1.6376e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1188. State = [[-0.21137491  0.15895663]]. Action = [[-0.03983058  0.08087227 -0.17955467 -0.05805486]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 1188 is [True, False, False, False, False, True]
State prediction error at timestep 1188 is tensor(4.5694e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1188 of -1
Current timestep = 1189. State = [[-0.21161757  0.15934616]]. Action = [[ 0.21055084  0.2013722  -0.12351872  0.5558808 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 1189 is [True, False, False, False, False, True]
State prediction error at timestep 1189 is tensor(4.6189e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1190. State = [[-0.21182889  0.15992813]]. Action = [[ 0.23659009 -0.08343771 -0.0260317  -0.7019401 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 1190 is [True, False, False, False, False, True]
State prediction error at timestep 1190 is tensor(6.3074e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1191. State = [[-0.21196209  0.16019185]]. Action = [[ 0.13660085  0.06057921 -0.04138646 -0.25740027]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 1191 is [True, False, False, False, False, True]
State prediction error at timestep 1191 is tensor(7.1930e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1192. State = [[-0.21219343  0.16018476]]. Action = [[ 0.17464435 -0.08948857  0.22326708  0.61136603]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 1192 is [True, False, False, False, False, True]
State prediction error at timestep 1192 is tensor(5.3901e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1193. State = [[-0.21238627  0.1605806 ]]. Action = [[-0.19693014 -0.0513739  -0.08173683 -0.34080768]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 1193 is [True, False, False, False, False, True]
State prediction error at timestep 1193 is tensor(4.1055e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1193 of -1
Current timestep = 1194. State = [[-0.21248539  0.16076417]]. Action = [[ 0.15731192 -0.1245034  -0.03824003 -0.1370337 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 1194 is [True, False, False, False, False, True]
State prediction error at timestep 1194 is tensor(8.6147e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1195. State = [[-0.21265747  0.16088355]]. Action = [[-0.09829512  0.23240393  0.14636865  0.92946804]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 1195 is [True, False, False, False, False, True]
State prediction error at timestep 1195 is tensor(1.0463e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1196. State = [[-0.21300454  0.16155516]]. Action = [[-0.027384    0.08786505  0.02454475  0.05923331]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 1196 is [True, False, False, False, False, True]
State prediction error at timestep 1196 is tensor(6.1083e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1197. State = [[-0.21327259  0.16206165]]. Action = [[ 0.24235648 -0.13139167 -0.11445138 -0.8732551 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 1197 is [True, False, False, False, False, True]
State prediction error at timestep 1197 is tensor(5.6153e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1198. State = [[-0.2137855   0.16290179]]. Action = [[ 0.06991377 -0.24826112  0.17209852  0.79056525]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 1198 is [True, False, False, False, False, True]
State prediction error at timestep 1198 is tensor(1.0382e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1199. State = [[-0.21407208  0.16337973]]. Action = [[ 0.02748773 -0.13889349  0.04639697 -0.28213298]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 1199 is [True, False, False, False, False, True]
State prediction error at timestep 1199 is tensor(6.8423e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1200. State = [[-0.214387   0.1638438]]. Action = [[ 0.17031491  0.09627035  0.03621346 -0.63186103]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 1200 is [True, False, False, False, False, True]
State prediction error at timestep 1200 is tensor(2.6643e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1200 of 0
Current timestep = 1201. State = [[-0.21490484  0.16487722]]. Action = [[-0.22316371  0.06923813  0.23489901 -0.01626194]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 1201 is [True, False, False, False, False, True]
State prediction error at timestep 1201 is tensor(4.1074e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1202. State = [[-0.21572223  0.166326  ]]. Action = [[-0.09801713 -0.00910772  0.16872784  0.07317746]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 1202 is [True, False, False, False, False, True]
State prediction error at timestep 1202 is tensor(1.9191e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1203. State = [[-0.2163435   0.16671613]]. Action = [[ 0.00948662 -0.17902917 -0.05104454  0.06679368]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 1203 is [True, False, False, False, False, True]
State prediction error at timestep 1203 is tensor(2.2596e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1204. State = [[-0.21796629  0.16821182]]. Action = [[-0.10641387  0.05695936  0.23039734 -0.9018023 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 1204 is [True, False, False, False, False, True]
State prediction error at timestep 1204 is tensor(1.7820e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1204 of 0
Current timestep = 1205. State = [[-0.21926323  0.16933587]]. Action = [[-0.03694318 -0.14650339  0.17836243 -0.939227  ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 1205 is [True, False, False, False, False, True]
State prediction error at timestep 1205 is tensor(1.2242e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1206. State = [[-0.21997091  0.16976196]]. Action = [[-0.22325742 -0.02411956 -0.22262621 -0.61055326]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 1206 is [True, False, False, False, False, True]
State prediction error at timestep 1206 is tensor(1.3030e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1207. State = [[-0.22063553  0.17032838]]. Action = [[ 0.02400643 -0.20453556 -0.05065639  0.40518725]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 1207 is [True, False, False, False, False, True]
State prediction error at timestep 1207 is tensor(2.5921e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1208. State = [[-0.22300176  0.17192942]]. Action = [[-0.00814648 -0.06249666  0.02393451  0.56156826]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 1208 is [True, False, False, False, False, True]
State prediction error at timestep 1208 is tensor(9.9145e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1208 of 0
Current timestep = 1209. State = [[-0.22330165  0.17174633]]. Action = [[ 0.11585915 -0.14016284  0.1998451   0.9323088 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 1209 is [True, False, False, False, False, True]
State prediction error at timestep 1209 is tensor(9.3080e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1210. State = [[-0.223812    0.17138366]]. Action = [[ 0.10261843 -0.15094832 -0.21002263 -0.03314894]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 1210 is [True, False, False, False, False, True]
State prediction error at timestep 1210 is tensor(2.2580e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1210 of 0
Current timestep = 1211. State = [[-0.22545315  0.1712168 ]]. Action = [[-0.10388801  0.0924564  -0.1273959  -0.11222148]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 1211 is [True, False, False, False, False, True]
State prediction error at timestep 1211 is tensor(1.5609e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1211 of -1
Current timestep = 1212. State = [[-0.22950597  0.17422198]]. Action = [[-0.09917127  0.12201291 -0.22153437 -0.8362755 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 1212 is [True, False, False, False, False, True]
State prediction error at timestep 1212 is tensor(1.6512e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1213. State = [[-0.23568422  0.17967053]]. Action = [[-0.10788307  0.03722513  0.19792187  0.9323585 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 1213 is [True, False, False, False, False, True]
State prediction error at timestep 1213 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1214. State = [[-0.2423312   0.18433267]]. Action = [[-0.08702375 -0.06555021  0.2156868  -0.5999542 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 1214 is [True, False, False, False, False, True]
State prediction error at timestep 1214 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1215. State = [[-0.24380146  0.18461692]]. Action = [[ 0.04378071  0.16268748 -0.05288993 -0.58221316]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 1215 is [True, False, False, False, False, True]
State prediction error at timestep 1215 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1216. State = [[-0.2451279   0.18467857]]. Action = [[-0.18289119 -0.241046    0.1284157  -0.6644812 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 1216 is [True, False, False, False, False, True]
State prediction error at timestep 1216 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1217. State = [[-0.24647918  0.18498619]]. Action = [[ 0.12902218  0.13433495 -0.01261984 -0.19524509]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 1217 is [True, False, False, False, False, True]
State prediction error at timestep 1217 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1218. State = [[-0.24758884  0.18512702]]. Action = [[ 0.05248275 -0.13926044 -0.05202609  0.38901532]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 1218 is [True, False, False, False, False, True]
State prediction error at timestep 1218 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1219. State = [[-0.2489393   0.18500148]]. Action = [[ 0.19992006 -0.00428689 -0.12121493  0.23106241]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 1219 is [True, False, False, False, False, True]
State prediction error at timestep 1219 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1219 of -1
Current timestep = 1220. State = [[-0.24969313  0.1849956 ]]. Action = [[ 0.16563785 -0.19923607 -0.02435981 -0.52776176]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 1220 is [True, False, False, False, False, True]
State prediction error at timestep 1220 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1221. State = [[-0.25135028  0.18493961]]. Action = [[ 0.23812431 -0.10206042 -0.02039903  0.3397584 ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 1221 is [True, False, False, False, False, True]
State prediction error at timestep 1221 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1222. State = [[-0.25227574  0.18474253]]. Action = [[-0.23214948  0.02989092 -0.04427201 -0.94175947]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 1222 is [True, False, False, False, False, True]
State prediction error at timestep 1222 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1222 of -1
Current timestep = 1223. State = [[-0.25332734  0.18451525]]. Action = [[-0.13968164  0.13340414 -0.20319112  0.3513006 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 1223 is [True, False, False, False, False, True]
State prediction error at timestep 1223 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 1224. State = [[-0.25423396  0.18426214]]. Action = [[ 0.19240755 -0.11846578  0.13562733  0.18305647]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 1224 is [True, False, False, False, False, True]
State prediction error at timestep 1224 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1225. State = [[-0.25478727  0.18407017]]. Action = [[-0.03287022  0.22979963 -0.04479308  0.61042213]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 1225 is [True, False, False, False, False, True]
State prediction error at timestep 1225 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 1226. State = [[-0.25524512  0.18388495]]. Action = [[-0.19169424 -0.21559016  0.07501954 -0.27542907]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 1226 is [True, False, False, False, False, True]
State prediction error at timestep 1226 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1226 of -1
Current timestep = 1227. State = [[-0.25686276  0.183854  ]]. Action = [[-0.04882844  0.11340147 -0.13103634 -0.85171497]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 1227 is [True, False, False, False, False, True]
State prediction error at timestep 1227 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1227 of -1
Current timestep = 1228. State = [[-0.25739932  0.18468125]]. Action = [[-2.1791816e-01 -3.2673776e-04  1.8342620e-01 -9.7056818e-01]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 1228 is [True, False, False, False, False, True]
State prediction error at timestep 1228 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1229. State = [[-0.25824058  0.18603843]]. Action = [[-0.20308445  0.09818143 -0.16590555 -0.00098729]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 1229 is [True, False, False, False, False, True]
State prediction error at timestep 1229 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 1230. State = [[-0.2586725   0.18654865]]. Action = [[-0.18175736  0.23269215  0.03034145  0.08872235]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 1230 is [True, False, False, False, False, True]
State prediction error at timestep 1230 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 1231. State = [[-0.2604531   0.18915884]]. Action = [[0.06733701 0.01578644 0.05781567 0.9175494 ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 1231 is [True, False, False, False, False, True]
State prediction error at timestep 1231 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1231 of -1
Current timestep = 1232. State = [[-0.26059508  0.19023275]]. Action = [[-0.13195471  0.03547373 -0.15991561 -0.9279141 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 1232 is [True, False, False, False, False, True]
State prediction error at timestep 1232 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1232 of -1
Current timestep = 1233. State = [[-0.26306233  0.19286111]]. Action = [[ 0.00129828  0.03294659 -0.19998029 -0.96063894]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 1233 is [True, False, False, False, False, True]
State prediction error at timestep 1233 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1233 of -1
Current timestep = 1234. State = [[-0.26389074  0.19352296]]. Action = [[ 0.19464925 -0.12734118  0.16948646  0.36880517]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 1234 is [True, False, False, False, False, True]
State prediction error at timestep 1234 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 1235. State = [[-0.26427254  0.19416912]]. Action = [[-0.22797473  0.17005867  0.07144186 -0.33740693]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 1235 is [True, False, False, False, False, True]
State prediction error at timestep 1235 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 1236. State = [[-0.26544097  0.19599266]]. Action = [[-0.03661849  0.01000369 -0.2185905  -0.9709675 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 1236 is [True, False, False, False, False, True]
State prediction error at timestep 1236 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1236 of -1
Current timestep = 1237. State = [[-0.26589248  0.19638638]]. Action = [[ 0.1347203  -0.03083679 -0.16675554 -0.481736  ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 1237 is [True, False, False, False, False, True]
State prediction error at timestep 1237 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1237 of -1
Current timestep = 1238. State = [[-0.2663699   0.19704275]]. Action = [[ 0.04186219 -0.13903934 -0.19460715  0.03554368]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 1238 is [True, False, False, False, False, True]
State prediction error at timestep 1238 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 1239. State = [[-0.2664705   0.19723271]]. Action = [[ 0.21908104 -0.02236219  0.04192087 -0.9837931 ]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 1239 is [True, False, False, False, False, True]
State prediction error at timestep 1239 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 1240. State = [[-0.26686275  0.19743977]]. Action = [[-0.11812869 -0.19475679  0.16425574 -0.15501505]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 1240 is [True, False, False, False, False, True]
State prediction error at timestep 1240 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 1241. State = [[-0.2677791   0.19853443]]. Action = [[ 0.08911872  0.11872497 -0.19864132  0.8375757 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 1241 is [True, False, False, False, False, True]
State prediction error at timestep 1241 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1241 of -1
Current timestep = 1242. State = [[-0.26812884  0.19912253]]. Action = [[-0.2322233   0.06715044 -0.04646444  0.4934318 ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 1242 is [True, False, False, False, False, True]
State prediction error at timestep 1242 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1242 of -1
Current timestep = 1243. State = [[-0.26846132  0.20013145]]. Action = [[-0.21868728  0.12283984  0.24718937  0.81925905]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 1243 is [True, False, False, False, False, True]
State prediction error at timestep 1243 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 1244. State = [[-0.26847985  0.20032966]]. Action = [[-0.23889975  0.19859767  0.05893847 -0.8240214 ]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 1244 is [True, False, False, False, False, True]
State prediction error at timestep 1244 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 1245. State = [[-0.2685931   0.20263678]]. Action = [[ 0.08408117  0.05914083 -0.20642596  0.89020467]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 1245 is [True, False, False, False, False, True]
State prediction error at timestep 1245 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1245 of -1
Current timestep = 1246. State = [[-0.26848283  0.20308249]]. Action = [[-0.16181408  0.06575194  0.0028652  -0.82657343]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 1246 is [True, False, False, False, False, True]
State prediction error at timestep 1246 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 1247. State = [[-0.26819792  0.20412257]]. Action = [[-0.0595032  -0.04159325  0.16765836  0.30042577]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 1247 is [True, False, False, False, False, True]
State prediction error at timestep 1247 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1247 of -1
Current timestep = 1248. State = [[-0.26837406  0.20419684]]. Action = [[-0.13214669  0.1557121   0.01369241  0.8422446 ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 1248 is [True, False, False, False, False, True]
State prediction error at timestep 1248 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 1249. State = [[-0.26873243  0.20452064]]. Action = [[-0.10961491  0.05084035  0.12587333 -0.34038174]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 1249 is [True, False, False, False, False, True]
State prediction error at timestep 1249 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1249 of -1
Current timestep = 1250. State = [[-0.26923814  0.20513389]]. Action = [[-0.24345897  0.14123172  0.12389544 -0.9250743 ]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 1250 is [True, False, False, False, False, True]
State prediction error at timestep 1250 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 1251. State = [[-0.2697801   0.20591372]]. Action = [[ 0.01036739 -0.22292499  0.2171244  -0.54426175]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 1251 is [True, False, False, False, False, True]
State prediction error at timestep 1251 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1251 of -1
Current timestep = 1252. State = [[-0.27148616  0.2082261 ]]. Action = [[ 0.09916025  0.08792135  0.19912928 -0.8693291 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 1252 is [True, False, False, False, False, True]
State prediction error at timestep 1252 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 1253. State = [[-0.2719006   0.20879702]]. Action = [[-0.18665916  0.02044964 -0.2282264  -0.14770079]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 1253 is [True, False, False, False, False, True]
State prediction error at timestep 1253 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 1254. State = [[-0.27204928  0.20909856]]. Action = [[ 0.02482843 -0.20665152 -0.00811245 -0.10587424]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 1254 is [True, False, False, False, False, True]
State prediction error at timestep 1254 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 1255. State = [[-0.27292186  0.21048641]]. Action = [[ 0.01257196 -0.08726649 -0.0310145   0.68088746]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 1255 is [True, False, False, False, False, True]
State prediction error at timestep 1255 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1255 of -1
Current timestep = 1256. State = [[-0.2728165   0.21048273]]. Action = [[-0.12422751 -0.20549878 -0.09973036 -0.5236392 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 1256 is [True, False, False, False, False, True]
State prediction error at timestep 1256 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 1257. State = [[-0.27261972  0.21026234]]. Action = [[0.09027159 0.08891147 0.1971319  0.48854244]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 1257 is [True, False, False, False, False, True]
State prediction error at timestep 1257 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1257 of -1
Current timestep = 1258. State = [[-0.272418    0.21055402]]. Action = [[-0.1670858  -0.09922607 -0.14446166 -0.17936331]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 1258 is [True, False, False, False, False, True]
State prediction error at timestep 1258 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 1259. State = [[-0.2721031   0.21066387]]. Action = [[ 0.12131301  0.19002914 -0.13957231  0.5415163 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 1259 is [True, False, False, False, False, True]
State prediction error at timestep 1259 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 1260. State = [[-0.27212512  0.21083485]]. Action = [[-0.24375184 -0.04726747 -0.08093438 -0.13783431]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 1260 is [True, False, False, False, False, True]
State prediction error at timestep 1260 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1260 of -1
Current timestep = 1261. State = [[-0.27201664  0.21118319]]. Action = [[ 0.07403433  0.22787225  0.14594698 -0.7264418 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 1261 is [True, False, False, False, False, True]
State prediction error at timestep 1261 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 1262. State = [[-0.27165398  0.2123234 ]]. Action = [[-0.11691472 -0.00253683  0.01139349  0.09632027]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 1262 is [True, False, False, False, False, True]
State prediction error at timestep 1262 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1262 of -1
Current timestep = 1263. State = [[-0.27174094  0.2125087 ]]. Action = [[-0.17477077 -0.10186929 -0.09892637  0.6487609 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 1263 is [True, False, False, False, False, True]
State prediction error at timestep 1263 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 1264. State = [[-0.27202725  0.21286033]]. Action = [[-0.23883438  0.17129326 -0.06297147 -0.7164201 ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 1264 is [True, False, False, False, False, True]
State prediction error at timestep 1264 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 1265. State = [[-0.27210972  0.21294567]]. Action = [[-0.04118952  0.21016648 -0.11905122 -0.35937798]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 1265 is [True, False, False, False, False, True]
State prediction error at timestep 1265 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 1266. State = [[-0.27214316  0.21304497]]. Action = [[-0.16396272  0.23078972  0.198371   -0.29466283]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 1266 is [True, False, False, False, False, True]
State prediction error at timestep 1266 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1266 of -1
Current timestep = 1267. State = [[-0.27226916  0.2132447 ]]. Action = [[ 0.02388078 -0.2283934  -0.16628404  0.48919415]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 1267 is [True, False, False, False, False, True]
State prediction error at timestep 1267 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 1268. State = [[-0.2725165   0.21349257]]. Action = [[ 0.07586291  0.01859328 -0.16163084 -0.31817138]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 1268 is [True, False, False, False, False, True]
State prediction error at timestep 1268 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1268 of -1
Current timestep = 1269. State = [[-0.27254856  0.21351743]]. Action = [[ 0.0872893   0.19069552 -0.04398237  0.58499885]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 1269 is [True, False, False, False, False, True]
State prediction error at timestep 1269 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 1270. State = [[-0.27225742  0.21402203]]. Action = [[ 0.05422777  0.03689724 -0.0251921   0.22229803]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 1270 is [True, False, False, False, False, True]
State prediction error at timestep 1270 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1270 of -1
Current timestep = 1271. State = [[-0.27190498  0.21433857]]. Action = [[0.23840895 0.18468115 0.20373222 0.49980485]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 1271 is [True, False, False, False, False, True]
State prediction error at timestep 1271 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 1272. State = [[-0.27142504  0.21509893]]. Action = [[-0.08448666  0.00244293 -0.03636637 -0.27493584]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 1272 is [True, False, False, False, False, True]
State prediction error at timestep 1272 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 1273. State = [[-0.27165264  0.2151833 ]]. Action = [[ 0.20875227  0.02366602  0.05509692 -0.7763059 ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 1273 is [True, False, False, False, False, True]
State prediction error at timestep 1273 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 1274. State = [[-0.2718965   0.21512885]]. Action = [[ 0.19553381 -0.0671525  -0.17078958 -0.485331  ]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 1274 is [True, False, False, False, False, True]
State prediction error at timestep 1274 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1274 of -1
Current timestep = 1275. State = [[-0.27208933  0.21552218]]. Action = [[ 0.05486885 -0.05337971  0.03283224  0.00658417]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 1275 is [True, False, False, False, False, True]
State prediction error at timestep 1275 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 1276. State = [[-0.27188718  0.21548586]]. Action = [[-0.22073713 -0.11939138 -0.18832295 -0.70721585]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 1276 is [True, False, False, False, False, True]
State prediction error at timestep 1276 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 1277. State = [[-0.27185026  0.21534729]]. Action = [[ 0.07846928  0.20110434  0.22169256 -0.9284381 ]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 1277 is [True, False, False, False, False, True]
State prediction error at timestep 1277 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1277 of -1
Current timestep = 1278. State = [[-0.2717095   0.21541804]]. Action = [[ 0.22448343  0.23111084  0.22338432 -0.20229459]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 1278 is [True, False, False, False, False, True]
State prediction error at timestep 1278 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 1279. State = [[-0.27154642  0.21529213]]. Action = [[-0.09054925 -0.1037415  -0.24461614  0.64053357]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 1279 is [True, False, False, False, False, True]
State prediction error at timestep 1279 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1279 of -1
Current timestep = 1280. State = [[-0.27142942  0.21469365]]. Action = [[ 0.203098    0.0499137   0.19812152 -0.3957044 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 1280 is [True, False, False, False, False, True]
State prediction error at timestep 1280 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 1281. State = [[-0.27117348  0.21392363]]. Action = [[ 0.06572035 -0.15125184 -0.12655611 -0.7991349 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 1281 is [True, False, False, False, False, True]
State prediction error at timestep 1281 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 1282. State = [[-0.27101254  0.2136381 ]]. Action = [[ 0.14489746  0.19148621 -0.08811527  0.3400501 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 1282 is [True, False, False, False, False, True]
State prediction error at timestep 1282 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1282 of -1
Current timestep = 1283. State = [[-0.27101824  0.21307994]]. Action = [[ 0.03514147 -0.24127004  0.05015361 -0.48724902]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 1283 is [True, False, False, False, False, True]
State prediction error at timestep 1283 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 1284. State = [[-0.27081743  0.21251813]]. Action = [[-0.06407818 -0.13764504  0.20059642  0.8058864 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 1284 is [True, False, False, False, False, True]
State prediction error at timestep 1284 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 1285. State = [[-0.27064583  0.21218084]]. Action = [[ 0.24730778  0.0409542  -0.19582896  0.18791008]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 1285 is [True, False, False, False, False, True]
State prediction error at timestep 1285 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 1286. State = [[-0.27068946  0.21198216]]. Action = [[ 0.08621332  0.23069423 -0.08672301  0.8898263 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 1286 is [True, False, False, False, False, True]
State prediction error at timestep 1286 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 1287. State = [[-0.2703475   0.21089995]]. Action = [[ 0.00204799 -0.05156989  0.10041481 -0.55734503]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 1287 is [True, False, False, False, False, True]
State prediction error at timestep 1287 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1287 of -1
Current timestep = 1288. State = [[-0.2701122   0.21029061]]. Action = [[ 0.12519884 -0.21631095 -0.21373063 -0.9517405 ]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 1288 is [True, False, False, False, False, True]
State prediction error at timestep 1288 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1288 of -1
Current timestep = 1289. State = [[-0.26994038  0.20970991]]. Action = [[-0.20720063 -0.17524742  0.02496004 -0.5356244 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 1289 is [True, False, False, False, False, True]
State prediction error at timestep 1289 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 1290. State = [[-0.26977208  0.20937559]]. Action = [[-0.14201902  0.20276088 -0.01587898  0.5637374 ]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 1290 is [True, False, False, False, False, True]
State prediction error at timestep 1290 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 1291. State = [[-0.26929083  0.20807011]]. Action = [[ 0.03676143 -0.11705621 -0.09368178  0.10773838]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 1291 is [True, False, False, False, False, True]
State prediction error at timestep 1291 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1291 of -1
Current timestep = 1292. State = [[-0.26866063  0.20661102]]. Action = [[-0.19973369  0.08435938  0.04031795  0.3527106 ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 1292 is [True, False, False, False, False, True]
State prediction error at timestep 1292 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 1293. State = [[-0.26811478  0.20541465]]. Action = [[-0.1452082   0.2363315   0.24183166  0.6820015 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 1293 is [True, False, False, False, False, True]
State prediction error at timestep 1293 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 1294. State = [[-0.26705465  0.20246653]]. Action = [[ 0.10500011 -0.06920856  0.035864   -0.57519466]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 1294 is [True, False, False, False, False, True]
State prediction error at timestep 1294 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 1295. State = [[-0.26689687  0.20211749]]. Action = [[-0.19355917  0.03552994 -0.10463786 -0.21214175]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 1295 is [True, False, False, False, False, True]
State prediction error at timestep 1295 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 1296. State = [[-0.26627383  0.20059566]]. Action = [[ 0.02733943 -0.13425457 -0.19098265  0.32132435]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 1296 is [True, False, False, False, False, True]
State prediction error at timestep 1296 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1296 of -1
Current timestep = 1297. State = [[-0.26581162  0.19944109]]. Action = [[ 0.16063035 -0.05101421  0.12866259  0.47743988]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 1297 is [True, False, False, False, False, True]
State prediction error at timestep 1297 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1298. State = [[-0.26565737  0.19862251]]. Action = [[-0.14495161 -0.15607087  0.0428817  -0.21405602]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 1298 is [True, False, False, False, False, True]
State prediction error at timestep 1298 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 1299. State = [[-0.26461247  0.19552794]]. Action = [[ 0.10192442  0.05425647 -0.04576573 -0.26387733]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 1299 is [True, False, False, False, False, True]
State prediction error at timestep 1299 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1299 of -1
Current timestep = 1300. State = [[-0.26445055  0.1954847 ]]. Action = [[-0.12733954 -0.19037329 -0.05750088 -0.09464949]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 1300 is [True, False, False, False, False, True]
State prediction error at timestep 1300 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 1301. State = [[-0.26408795  0.19534175]]. Action = [[ 0.07121778 -0.16173074  0.18515664  0.7984319 ]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 1301 is [True, False, False, False, False, True]
State prediction error at timestep 1301 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 1302. State = [[-0.26340112  0.1951824 ]]. Action = [[-0.04497388 -0.04056647  0.02817425 -0.6657235 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 1302 is [True, False, False, False, False, True]
State prediction error at timestep 1302 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1302 of -1
Current timestep = 1303. State = [[-0.26321277  0.19491628]]. Action = [[-0.2030952  -0.03754014  0.09113798  0.8285303 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 1303 is [True, False, False, False, False, True]
State prediction error at timestep 1303 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 1304. State = [[-0.2632683   0.19460434]]. Action = [[-0.0321978   0.14553529 -0.11755779 -0.75207704]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 1304 is [True, False, False, False, False, True]
State prediction error at timestep 1304 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1305. State = [[-0.26316625  0.19444416]]. Action = [[ 0.20080635  0.05493224  0.0418044  -0.05957258]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 1305 is [True, False, False, False, False, True]
State prediction error at timestep 1305 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1305 of -1
Current timestep = 1306. State = [[-0.26305506  0.19415669]]. Action = [[-0.11384463 -0.2103921  -0.20137733 -0.19108939]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 1306 is [True, False, False, False, False, True]
State prediction error at timestep 1306 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1306 of -1
Current timestep = 1307. State = [[-0.26307386  0.19399984]]. Action = [[-0.23618217  0.0539307   0.00325903  0.957227  ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 1307 is [True, False, False, False, False, True]
State prediction error at timestep 1307 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 1308. State = [[-0.2630002   0.19404887]]. Action = [[-0.22715095  0.0532769  -0.09559445  0.04260921]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 1308 is [True, False, False, False, False, True]
State prediction error at timestep 1308 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 1309. State = [[-0.26280805  0.19350523]]. Action = [[-0.03937429 -0.07906708  0.17130366 -0.81135255]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 1309 is [True, False, False, False, False, True]
State prediction error at timestep 1309 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1309 of -1
Current timestep = 1310. State = [[-0.26236805  0.19201274]]. Action = [[ 0.02662537  0.08649194 -0.10678774  0.51695585]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 1310 is [True, False, False, False, False, True]
State prediction error at timestep 1310 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1310 of -1
Current timestep = 1311. State = [[-0.26240665  0.19213693]]. Action = [[-0.20293659  0.06059361 -0.14766437 -0.52286404]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 1311 is [True, False, False, False, False, True]
State prediction error at timestep 1311 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1312. State = [[-0.26233283  0.19218577]]. Action = [[-0.1962555  -0.04100458 -0.15249756 -0.00232673]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 1312 is [True, False, False, False, False, True]
State prediction error at timestep 1312 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 1313. State = [[-0.2624286   0.19220583]]. Action = [[ 0.22274026  0.014851    0.12871963 -0.92053974]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 1313 is [True, False, False, False, False, True]
State prediction error at timestep 1313 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 1314. State = [[-0.26238742  0.19207504]]. Action = [[-0.21966292 -0.14158408 -0.07617113  0.92441726]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 1314 is [True, False, False, False, False, True]
State prediction error at timestep 1314 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1314 of -1
Current timestep = 1315. State = [[-0.26233283  0.19218577]]. Action = [[-0.07251143 -0.2144079   0.2441501  -0.08719951]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 1315 is [True, False, False, False, False, True]
State prediction error at timestep 1315 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1316. State = [[-0.2626099   0.19242625]]. Action = [[-0.05672443  0.1326932   0.18229493  0.49842834]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 1316 is [True, False, False, False, False, True]
State prediction error at timestep 1316 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1317. State = [[-0.2629983   0.19293123]]. Action = [[ 0.22837302  0.0603863  -0.14682798  0.10943663]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 1317 is [True, False, False, False, False, True]
State prediction error at timestep 1317 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1318. State = [[-0.26411825  0.19500001]]. Action = [[ 0.0930478  -0.13076118  0.10788709  0.72195387]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 1318 is [True, False, False, False, False, True]
State prediction error at timestep 1318 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1318 of -1
Current timestep = 1319. State = [[-0.26364854  0.19467318]]. Action = [[ 0.23731554 -0.24026287 -0.15597545 -0.6285075 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 1319 is [True, False, False, False, False, True]
State prediction error at timestep 1319 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1320. State = [[-0.26313454  0.19407293]]. Action = [[ 0.02922487 -0.22154473  0.05492091  0.6264758 ]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 1320 is [True, False, False, False, False, True]
State prediction error at timestep 1320 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1321. State = [[-0.26292062  0.1937284 ]]. Action = [[ 0.1271649   0.21332097 -0.15866011 -0.4196626 ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 1321 is [True, False, False, False, False, True]
State prediction error at timestep 1321 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1321 of -1
Current timestep = 1322. State = [[-0.26302978  0.19404146]]. Action = [[-0.12099133 -0.14133069 -0.10735427  0.9452529 ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 1322 is [True, False, False, False, False, True]
State prediction error at timestep 1322 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 1323. State = [[-0.26262215  0.19375026]]. Action = [[ 0.22289571 -0.11211109 -0.21566272  0.32631922]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 1323 is [True, False, False, False, False, True]
State prediction error at timestep 1323 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1324. State = [[-0.26251084  0.193517  ]]. Action = [[ 0.15903819  0.11266792 -0.12615441  0.20994484]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 1324 is [True, False, False, False, False, True]
State prediction error at timestep 1324 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1324 of -1
Current timestep = 1325. State = [[-0.26223826  0.19346647]]. Action = [[ 0.06905088  0.00990582  0.21361142 -0.6407693 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 1325 is [True, False, False, False, False, True]
State prediction error at timestep 1325 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1326. State = [[-0.26161042  0.1934328 ]]. Action = [[-0.1193517  -0.05950493 -0.21989907 -0.9559224 ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 1326 is [True, False, False, False, False, True]
State prediction error at timestep 1326 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1326 of -1
Current timestep = 1327. State = [[-0.26164547  0.19310243]]. Action = [[-0.19808006  0.01707128  0.05949149 -0.00403219]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 1327 is [True, False, False, False, False, True]
State prediction error at timestep 1327 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1328. State = [[-0.26169783  0.19294147]]. Action = [[-0.13270284  0.19734913 -0.17084937  0.70531774]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 1328 is [True, False, False, False, False, True]
State prediction error at timestep 1328 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1329. State = [[-0.2614601   0.19244014]]. Action = [[-0.07831869  0.03423059 -0.17147218 -0.40676403]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 1329 is [True, False, False, False, False, True]
State prediction error at timestep 1329 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1329 of -1
Current timestep = 1330. State = [[-0.26187477  0.19288176]]. Action = [[ 0.08109891 -0.00605017 -0.01766397  0.6472738 ]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 1330 is [True, False, False, False, False, True]
State prediction error at timestep 1330 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1330 of -1
Current timestep = 1331. State = [[-0.26198483  0.19291446]]. Action = [[-0.07796752  0.088002    0.20767576 -0.64833546]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 1331 is [True, False, False, False, False, True]
State prediction error at timestep 1331 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1331 of -1
Current timestep = 1332. State = [[-0.2625199   0.19362935]]. Action = [[0.20378625 0.16453731 0.22447404 0.03599477]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 1332 is [True, False, False, False, False, True]
State prediction error at timestep 1332 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1333. State = [[-0.26272064  0.19394061]]. Action = [[-0.1647022  -0.07109384  0.11773318 -0.9270903 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 1333 is [True, False, False, False, False, True]
State prediction error at timestep 1333 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1334. State = [[-0.26283416  0.1941831 ]]. Action = [[-0.15631351  0.13800174  0.22633892 -0.61364007]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 1334 is [True, False, False, False, False, True]
State prediction error at timestep 1334 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1334 of -1
Current timestep = 1335. State = [[-0.2634863   0.19498195]]. Action = [[0.08119491 0.04979709 0.22624388 0.00488043]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 1335 is [True, False, False, False, False, True]
State prediction error at timestep 1335 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1336. State = [[-0.26356655  0.19501832]]. Action = [[ 0.04504016 -0.20629267  0.03105152  0.8635876 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 1336 is [True, False, False, False, False, True]
State prediction error at timestep 1336 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1337. State = [[-0.26356688  0.1952387 ]]. Action = [[0.12219936 0.17557248 0.11250046 0.67223716]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 1337 is [True, False, False, False, False, True]
State prediction error at timestep 1337 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1338. State = [[-0.26375058  0.19575243]]. Action = [[ 0.06211671  0.1196503  -0.07426253 -0.21013111]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 1338 is [True, False, False, False, False, True]
State prediction error at timestep 1338 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1339. State = [[-0.2637961  0.1967412]]. Action = [[-0.24673767  0.19399378  0.17837599  0.88601565]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 1339 is [True, False, False, False, False, True]
State prediction error at timestep 1339 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 1340. State = [[-0.26363456  0.19766144]]. Action = [[ 0.09106359  0.15720415 -0.13636528  0.23607123]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 1340 is [True, False, False, False, False, True]
State prediction error at timestep 1340 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1341. State = [[-0.26298034  0.19996396]]. Action = [[ 0.10812521  0.04102087 -0.15316674  0.27568483]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 1341 is [True, False, False, False, False, True]
State prediction error at timestep 1341 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1341 of -1
Current timestep = 1342. State = [[-0.26227584  0.20061609]]. Action = [[-0.09209172 -0.23347132 -0.14908437 -0.4070387 ]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 1342 is [True, False, False, False, False, True]
State prediction error at timestep 1342 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1343. State = [[-0.26152098  0.20126803]]. Action = [[-0.1143634   0.17690024 -0.10007733 -0.4951731 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 1343 is [True, False, False, False, False, True]
State prediction error at timestep 1343 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1344. State = [[-0.25941068  0.20315894]]. Action = [[ 0.03801405 -0.03852019 -0.1267868  -0.19906086]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 1344 is [True, False, False, False, False, True]
State prediction error at timestep 1344 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1344 of -1
Current timestep = 1345. State = [[-0.25867832  0.20348012]]. Action = [[-0.20218045 -0.05198584  0.05596831  0.32489824]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 1345 is [True, False, False, False, False, True]
State prediction error at timestep 1345 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1346. State = [[-0.2580619   0.20378615]]. Action = [[-0.15578568 -0.15003444  0.01546341 -0.2825188 ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 1346 is [True, False, False, False, False, True]
State prediction error at timestep 1346 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1347. State = [[-0.2575346   0.20410426]]. Action = [[ 0.02857152 -0.13608901  0.07428139 -0.4835888 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 1347 is [True, False, False, False, False, True]
State prediction error at timestep 1347 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1347 of -1
Current timestep = 1348. State = [[-0.2571933   0.20438914]]. Action = [[ 0.23705536  0.00287619 -0.07262459 -0.4256997 ]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 1348 is [True, False, False, False, False, True]
State prediction error at timestep 1348 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1349. State = [[-0.25673926  0.20467769]]. Action = [[-0.13622482 -0.21968496  0.17961276  0.36090434]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 1349 is [True, False, False, False, False, True]
State prediction error at timestep 1349 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1350. State = [[-0.2563067   0.20488644]]. Action = [[ 0.1195583   0.22812545  0.16336292 -0.28911853]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 1350 is [True, False, False, False, False, True]
State prediction error at timestep 1350 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1351. State = [[-0.25612485  0.20489438]]. Action = [[-0.11954069  0.24450567  0.15677425 -0.6954088 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 1351 is [True, False, False, False, False, True]
State prediction error at timestep 1351 is tensor(8.6804e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1351 of 1
Current timestep = 1352. State = [[-0.25536388  0.20505863]]. Action = [[ 0.10012749 -0.12838288  0.24793398 -0.01750171]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 1352 is [True, False, False, False, False, True]
State prediction error at timestep 1352 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1353. State = [[-0.2540234  0.2041934]]. Action = [[-0.06022809  0.18470353 -0.20524347 -0.0992465 ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 1353 is [True, False, False, False, False, True]
State prediction error at timestep 1353 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1354. State = [[-0.25188777  0.20206885]]. Action = [[-0.1269167  -0.01839574 -0.09020874  0.9522681 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 1354 is [True, False, False, False, False, True]
State prediction error at timestep 1354 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1354 of 1
Current timestep = 1355. State = [[-0.25174305  0.20158096]]. Action = [[-0.16074961 -0.16843088 -0.19433348  0.3517909 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 1355 is [True, False, False, False, False, True]
State prediction error at timestep 1355 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1356. State = [[-0.25188497  0.20147206]]. Action = [[ 0.02077696 -0.23960605  0.23707843  0.8877785 ]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 1356 is [True, False, False, False, False, True]
State prediction error at timestep 1356 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1356 of 1
Current timestep = 1357. State = [[-0.25154072  0.20082042]]. Action = [[ 0.1321002  -0.00552402 -0.13416785  0.03171861]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 1357 is [True, False, False, False, False, True]
State prediction error at timestep 1357 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1358. State = [[-0.25108945  0.20035173]]. Action = [[-0.1729527   0.05415452 -0.14579889 -0.02692187]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 1358 is [True, False, False, False, False, True]
State prediction error at timestep 1358 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1359. State = [[-0.25066832  0.19993351]]. Action = [[-0.11527655  0.16992462 -0.05716223 -0.57384044]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 1359 is [True, False, False, False, False, True]
State prediction error at timestep 1359 is tensor(4.3243e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1360. State = [[-0.24948782  0.19922538]]. Action = [[-0.07554875  0.00421873  0.22495544 -0.7368449 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 1360 is [True, False, False, False, False, True]
State prediction error at timestep 1360 is tensor(5.4034e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1361. State = [[-0.24974532  0.19907352]]. Action = [[ 0.17579326  0.14273545  0.01476389 -0.07145041]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 1361 is [True, False, False, False, False, True]
State prediction error at timestep 1361 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1362. State = [[-0.24976888  0.1990533 ]]. Action = [[ 0.15423387  0.21739435 -0.16136374  0.28927565]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 1362 is [True, False, False, False, False, True]
State prediction error at timestep 1362 is tensor(7.7500e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1363. State = [[-0.24966487  0.19882476]]. Action = [[-0.00498985 -0.04467863 -0.15368387  0.50072575]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 1363 is [True, False, False, False, False, True]
State prediction error at timestep 1363 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1363 of 1
Current timestep = 1364. State = [[-0.24947344  0.19854477]]. Action = [[ 0.23523474  0.05677131 -0.03858295 -0.77708447]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 1364 is [True, False, False, False, False, True]
State prediction error at timestep 1364 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1365. State = [[-0.24948873  0.19832732]]. Action = [[0.21375361 0.1858359  0.1555863  0.58885396]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 1365 is [True, False, False, False, False, True]
State prediction error at timestep 1365 is tensor(9.3759e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1366. State = [[-0.24944434  0.19797915]]. Action = [[-0.10589367  0.24835756 -0.22657149 -0.4950201 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 1366 is [True, False, False, False, False, True]
State prediction error at timestep 1366 is tensor(2.9181e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1367. State = [[-0.24932107  0.19783385]]. Action = [[-2.2907032e-01  6.3824654e-04  2.4215353e-01  7.2010469e-01]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 1367 is [True, False, False, False, False, True]
State prediction error at timestep 1367 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1367 of 1
Current timestep = 1368. State = [[-0.2493085   0.19745345]]. Action = [[ 0.18728578 -0.03384624 -0.22394559 -0.13497412]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 1368 is [True, False, False, False, False, True]
State prediction error at timestep 1368 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1369. State = [[-0.24912186  0.19720125]]. Action = [[-0.21299161 -0.00889517 -0.21711631 -0.33723783]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 1369 is [True, False, False, False, False, True]
State prediction error at timestep 1369 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1370. State = [[-0.24906112  0.19700788]]. Action = [[-0.0540559  -0.18689667 -0.07822546  0.8219278 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 1370 is [True, False, False, False, False, True]
State prediction error at timestep 1370 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1371. State = [[-0.24903202  0.19677764]]. Action = [[-0.09699269 -0.14291057  0.16054922  0.8193642 ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 1371 is [True, False, False, False, False, True]
State prediction error at timestep 1371 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1371 of 1
Current timestep = 1372. State = [[-0.24879575  0.19613825]]. Action = [[ 0.02170953 -0.10697582 -0.1370509  -0.7889212 ]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 1372 is [True, False, False, False, False, True]
State prediction error at timestep 1372 is tensor(2.1775e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1373. State = [[-0.24827456  0.19500412]]. Action = [[0.18176836 0.12613714 0.18906558 0.61636424]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 1373 is [True, False, False, False, False, True]
State prediction error at timestep 1373 is tensor(8.5497e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1373 of 1
Current timestep = 1374. State = [[-0.24755844  0.19241843]]. Action = [[-0.00127046  0.1056875  -0.0372549   0.4225912 ]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 1374 is [True, False, False, False, False, True]
State prediction error at timestep 1374 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1375. State = [[-0.24761169  0.192663  ]]. Action = [[-0.07345712 -0.21562664 -0.12877415 -0.35051322]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 1375 is [True, False, False, False, False, True]
State prediction error at timestep 1375 is tensor(7.8560e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1375 of 1
Current timestep = 1376. State = [[-0.24797443  0.19306387]]. Action = [[-0.0254139   0.12748611 -0.20538528 -0.31604373]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 1376 is [True, False, False, False, False, True]
State prediction error at timestep 1376 is tensor(4.3625e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1377. State = [[-0.24972238  0.19625825]]. Action = [[-0.09440446  0.12823957  0.23209316 -0.47641146]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 1377 is [True, False, False, False, False, True]
State prediction error at timestep 1377 is tensor(4.0841e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1378. State = [[-0.2504851   0.19726576]]. Action = [[ 0.16046688 -0.08236992  0.18574053  0.13085902]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 1378 is [True, False, False, False, False, True]
State prediction error at timestep 1378 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1379. State = [[-0.25143242  0.1991379 ]]. Action = [[ 0.16858709 -0.22223106 -0.08343858  0.9802203 ]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 1379 is [True, False, False, False, False, True]
State prediction error at timestep 1379 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1380. State = [[-0.253536    0.20274806]]. Action = [[ 0.07003146 -0.11735988  0.16621095  0.5020131 ]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 1380 is [True, False, False, False, False, True]
State prediction error at timestep 1380 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1380 of 1
Current timestep = 1381. State = [[-0.25312778  0.2029077 ]]. Action = [[0.02975756 0.080405   0.14928752 0.7822156 ]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 1381 is [True, False, False, False, False, True]
State prediction error at timestep 1381 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1381 of -1
Current timestep = 1382. State = [[-0.25271934  0.20365316]]. Action = [[-0.2304531  -0.11797076  0.16728899 -0.63845766]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 1382 is [True, False, False, False, False, True]
State prediction error at timestep 1382 is tensor(2.7613e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1383. State = [[-0.25245273  0.20377246]]. Action = [[ 0.24677458  0.20204812  0.16672993 -0.02774924]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 1383 is [True, False, False, False, False, True]
State prediction error at timestep 1383 is tensor(7.3302e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1384. State = [[-0.25174308  0.20482007]]. Action = [[-0.05755313  0.11074549  0.19404715  0.02133381]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 1384 is [True, False, False, False, False, True]
State prediction error at timestep 1384 is tensor(8.9638e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1384 of -1
Current timestep = 1385. State = [[-0.25235888  0.20617868]]. Action = [[ 0.14727923 -0.23627687  0.05533874  0.44466305]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 1385 is [True, False, False, False, False, True]
State prediction error at timestep 1385 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1386. State = [[-0.25249916  0.20710203]]. Action = [[-0.19193926 -0.07373597  0.09149888  0.23462772]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 1386 is [True, False, False, False, False, True]
State prediction error at timestep 1386 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1387. State = [[-0.252576   0.2080941]]. Action = [[0.21279034 0.2125268  0.23757023 0.2542305 ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 1387 is [True, False, False, False, False, True]
State prediction error at timestep 1387 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1388. State = [[-0.25289243  0.20829774]]. Action = [[ 0.08501041  0.20540005 -0.03688726 -0.05308431]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 1388 is [True, False, False, False, False, True]
State prediction error at timestep 1388 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1388 of -1
Current timestep = 1389. State = [[-0.25320306  0.2100373 ]]. Action = [[-0.23389749  0.21148604 -0.1943747   0.76770175]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 1389 is [True, False, False, False, False, True]
State prediction error at timestep 1389 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1390. State = [[-0.25324556  0.2102117 ]]. Action = [[ 0.04426059  0.23452324  0.20070371 -0.52972853]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 1390 is [True, False, False, False, False, True]
State prediction error at timestep 1390 is tensor(6.9463e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1391. State = [[-0.2534415   0.21020728]]. Action = [[ 0.22971082 -0.12680914  0.09604627  0.4941187 ]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 1391 is [True, False, False, False, False, True]
State prediction error at timestep 1391 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1392. State = [[-0.25351295  0.21111438]]. Action = [[-0.10077655 -0.16475779  0.05379057  0.16117895]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 1392 is [True, False, False, False, False, True]
State prediction error at timestep 1392 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1392 of -1
Current timestep = 1393. State = [[-0.25342697  0.21172655]]. Action = [[-0.15017492  0.18280151 -0.08776948 -0.42140806]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 1393 is [True, False, False, False, False, True]
State prediction error at timestep 1393 is tensor(9.1157e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1394. State = [[-0.25338513  0.21255225]]. Action = [[ 0.03231552  0.01258734  0.13010463 -0.44118762]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 1394 is [True, False, False, False, False, True]
State prediction error at timestep 1394 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1394 of -1
Current timestep = 1395. State = [[-0.2531027   0.21283221]]. Action = [[-0.14003803 -0.12949869  0.19187757 -0.7019051 ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 1395 is [True, False, False, False, False, True]
State prediction error at timestep 1395 is tensor(3.5507e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1396. State = [[-0.2529888   0.21313049]]. Action = [[ 0.04862562  0.18356836  0.09979659 -0.88287497]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 1396 is [True, False, False, False, False, True]
State prediction error at timestep 1396 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1397. State = [[-0.2528548   0.21330857]]. Action = [[-0.19231458  0.19972771  0.06072971  0.8998121 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 1397 is [True, False, False, False, False, True]
State prediction error at timestep 1397 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1398. State = [[-0.25274545  0.21337579]]. Action = [[-0.13197882 -0.22855102  0.05283436  0.91195726]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 1398 is [True, False, False, False, False, True]
State prediction error at timestep 1398 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1398 of -1
Current timestep = 1399. State = [[-0.2526286   0.21345545]]. Action = [[-0.21919581 -0.01221341  0.1764412  -0.33535182]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 1399 is [True, False, False, False, False, True]
State prediction error at timestep 1399 is tensor(9.9736e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1400. State = [[-0.25192484  0.21394207]]. Action = [[-0.04854017 -0.09518631 -0.12804025 -0.40071374]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 1400 is [True, False, False, False, False, True]
State prediction error at timestep 1400 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1401. State = [[-0.251856    0.21353287]]. Action = [[ 0.05449712 -0.02121989 -0.13801008 -0.7072197 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 1401 is [True, False, False, False, False, True]
State prediction error at timestep 1401 is tensor(7.4571e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1402. State = [[-0.25172108  0.21314164]]. Action = [[ 0.18278232 -0.135802    0.24709803  0.21621239]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 1402 is [True, False, False, False, False, True]
State prediction error at timestep 1402 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1403. State = [[-0.2515652   0.21287432]]. Action = [[0.14008656 0.17454416 0.1959087  0.9536246 ]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 1403 is [True, False, False, False, False, True]
State prediction error at timestep 1403 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1403 of -1
Current timestep = 1404. State = [[-0.2514536   0.21272202]]. Action = [[0.18571043 0.04116797 0.2201783  0.6524768 ]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 1404 is [True, False, False, False, False, True]
State prediction error at timestep 1404 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1405. State = [[-0.25148082  0.21246192]]. Action = [[-0.04792857  0.21709377  0.02922872 -0.12777257]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 1405 is [True, False, False, False, False, True]
State prediction error at timestep 1405 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1406. State = [[-0.25130206  0.21212733]]. Action = [[-0.24308759  0.12236455  0.05671597  0.20078087]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 1406 is [True, False, False, False, False, True]
State prediction error at timestep 1406 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1407. State = [[-0.25108334  0.21198903]]. Action = [[-0.14702208 -0.02861515  0.11852878  0.6106322 ]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 1407 is [True, False, False, False, False, True]
State prediction error at timestep 1407 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1408. State = [[-0.2511918   0.21190284]]. Action = [[-0.24793714  0.16010815 -0.20494793  0.05184758]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 1408 is [True, False, False, False, False, True]
State prediction error at timestep 1408 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1408 of -1
Current timestep = 1409. State = [[-0.25102252  0.21165948]]. Action = [[ 0.08304197  0.14685625  0.06062478 -0.9496241 ]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 1409 is [True, False, False, False, False, True]
State prediction error at timestep 1409 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1410. State = [[-0.25095558  0.21146134]]. Action = [[ 0.17639029 -0.2173692   0.11081719 -0.0315243 ]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 1410 is [True, False, False, False, False, True]
State prediction error at timestep 1410 is tensor(8.9220e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1411. State = [[-0.25099295  0.2115086 ]]. Action = [[-0.17748557  0.11381564 -0.0424207   0.14533663]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 1411 is [True, False, False, False, False, True]
State prediction error at timestep 1411 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1411 of -1
Current timestep = 1412. State = [[-0.2510454   0.21153931]]. Action = [[-0.17388773 -0.11073038 -0.18823478 -0.8628436 ]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 1412 is [True, False, False, False, False, True]
State prediction error at timestep 1412 is tensor(3.3399e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1413. State = [[-0.25098658  0.21142913]]. Action = [[-0.18541014  0.23641324 -0.18481597  0.46478474]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 1413 is [True, False, False, False, False, True]
State prediction error at timestep 1413 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1414. State = [[-0.25094953  0.21138231]]. Action = [[ 0.23127794 -0.21925756 -0.05355412 -0.24694091]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 1414 is [True, False, False, False, False, True]
State prediction error at timestep 1414 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1415. State = [[-0.25099418  0.21132496]]. Action = [[-0.12324464 -0.02337052 -0.20009491 -0.8664892 ]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 1415 is [True, False, False, False, False, True]
State prediction error at timestep 1415 is tensor(6.1995e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1415 of -1
Current timestep = 1416. State = [[-0.25123152  0.2116068 ]]. Action = [[-0.08327736  0.05595386  0.04925743  0.66889715]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 1416 is [True, False, False, False, False, True]
State prediction error at timestep 1416 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1416 of -1
Current timestep = 1417. State = [[-0.25190642  0.21230766]]. Action = [[-0.06628494 -0.20715374 -0.16546132  0.51963234]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 1417 is [True, False, False, False, False, True]
State prediction error at timestep 1417 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1418. State = [[-0.2522889   0.21277654]]. Action = [[-0.18713132  0.0446505  -0.1192303  -0.79948235]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 1418 is [True, False, False, False, False, True]
State prediction error at timestep 1418 is tensor(7.9722e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1419. State = [[-0.25256932  0.21324724]]. Action = [[ 0.09527731  0.23023665  0.06867048 -0.5022145 ]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 1419 is [True, False, False, False, False, True]
State prediction error at timestep 1419 is tensor(5.2789e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1420. State = [[-0.25275674  0.21329781]]. Action = [[1.1612496e-01 1.4766300e-01 1.2001395e-04 2.7960491e-01]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 1420 is [True, False, False, False, False, True]
State prediction error at timestep 1420 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1420 of -1
Current timestep = 1421. State = [[-0.25315893  0.21380886]]. Action = [[0.13681895 0.04914612 0.15226609 0.85818887]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 1421 is [True, False, False, False, False, True]
State prediction error at timestep 1421 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1422. State = [[-0.25350493  0.21431193]]. Action = [[-0.18709491 -0.21330267 -0.02502283 -0.7358393 ]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 1422 is [True, False, False, False, False, True]
State prediction error at timestep 1422 is tensor(6.8118e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1423. State = [[-0.2541999  0.2150254]]. Action = [[ 0.08385506  0.05181617 -0.16085207  0.8164582 ]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 1423 is [True, False, False, False, False, True]
State prediction error at timestep 1423 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1423 of -1
Current timestep = 1424. State = [[-0.25418544  0.21495795]]. Action = [[ 0.15756178 -0.2406243   0.19331902 -0.76626194]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 1424 is [True, False, False, False, False, True]
State prediction error at timestep 1424 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1425. State = [[-0.25442162  0.21533193]]. Action = [[ 0.03744745 -0.09608856  0.01045254  0.59094644]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 1425 is [True, False, False, False, False, True]
State prediction error at timestep 1425 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1425 of -1
Current timestep = 1426. State = [[-0.25403345  0.21471424]]. Action = [[-0.16390644  0.20942909 -0.24235073  0.45891476]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 1426 is [True, False, False, False, False, True]
State prediction error at timestep 1426 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1427. State = [[-0.25378942  0.21402001]]. Action = [[ 0.03412217  0.03309321 -0.09745374  0.05653048]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 1427 is [True, False, False, False, False, True]
State prediction error at timestep 1427 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1427 of -1
Current timestep = 1428. State = [[-0.25360385  0.2137945 ]]. Action = [[ 0.19926077 -0.03205812 -0.01757623  0.30504692]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 1428 is [True, False, False, False, False, True]
State prediction error at timestep 1428 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1429. State = [[-0.2534514   0.21366367]]. Action = [[ 0.01945844 -0.1646128  -0.03921992 -0.15113825]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 1429 is [True, False, False, False, False, True]
State prediction error at timestep 1429 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1430. State = [[-0.2535068   0.21363772]]. Action = [[ 0.2105695  -0.2191881  -0.0740566   0.30591142]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 1430 is [True, False, False, False, False, True]
State prediction error at timestep 1430 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1431. State = [[-0.25357348  0.21364376]]. Action = [[-0.18688421  0.24782127 -0.18837543 -0.34190118]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 1431 is [True, False, False, False, False, True]
State prediction error at timestep 1431 is tensor(5.0203e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1431 of -1
Current timestep = 1432. State = [[-0.25365663  0.21382482]]. Action = [[-0.22116964 -0.21886449 -0.20448038  0.56258917]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 1432 is [True, False, False, False, False, True]
State prediction error at timestep 1432 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1433. State = [[-0.25361237  0.2136984 ]]. Action = [[-0.11841249  0.01253429  0.01127106 -0.66401047]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 1433 is [True, False, False, False, False, True]
State prediction error at timestep 1433 is tensor(2.1862e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1433 of -1
Current timestep = 1434. State = [[-0.25372455  0.21383849]]. Action = [[-0.18487847  0.08843678  0.13269836 -0.6150636 ]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 1434 is [True, False, False, False, False, True]
State prediction error at timestep 1434 is tensor(3.0077e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1435. State = [[-0.25397125  0.21418172]]. Action = [[-0.19387035 -0.20704801 -0.2351159  -0.5238342 ]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 1435 is [True, False, False, False, False, True]
State prediction error at timestep 1435 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1436. State = [[-0.25425595  0.21457173]]. Action = [[ 0.1268127   0.09842125 -0.21637896 -0.5388436 ]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 1436 is [True, False, False, False, False, True]
State prediction error at timestep 1436 is tensor(8.3276e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1436 of -1
Current timestep = 1437. State = [[-0.2542451   0.21472414]]. Action = [[-0.01099586  0.22526932  0.21502632 -0.734936  ]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 1437 is [True, False, False, False, False, True]
State prediction error at timestep 1437 is tensor(7.8031e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1438. State = [[-0.25426736  0.2147873 ]]. Action = [[0.00153774 0.14603001 0.05736992 0.30847907]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 1438 is [True, False, False, False, False, True]
State prediction error at timestep 1438 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1439. State = [[-0.2543671   0.21488714]]. Action = [[ 0.06126049  0.14449564  0.13331166 -0.13237226]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 1439 is [True, False, False, False, False, True]
State prediction error at timestep 1439 is tensor(7.3627e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1440. State = [[-0.2543853   0.21499853]]. Action = [[-0.14210548 -0.06234983 -0.10780177 -0.5568643 ]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 1440 is [True, False, False, False, False, True]
State prediction error at timestep 1440 is tensor(8.4525e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1441. State = [[-0.25421268  0.21518096]]. Action = [[-0.1576525   0.08645749  0.24056536 -0.15264785]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 1441 is [True, False, False, False, False, True]
State prediction error at timestep 1441 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1441 of -1
Current timestep = 1442. State = [[-0.25427952  0.21537004]]. Action = [[ 0.03288931 -0.14654233 -0.06925073 -0.8558654 ]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 1442 is [True, False, False, False, False, True]
State prediction error at timestep 1442 is tensor(7.0007e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1443. State = [[-0.2543321   0.21549204]]. Action = [[ 0.18126935  0.12660402 -0.06377128  0.51134884]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 1443 is [True, False, False, False, False, True]
State prediction error at timestep 1443 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1444. State = [[-0.25425762  0.21558307]]. Action = [[ 0.2258021  -0.19071688 -0.13723181 -0.5265498 ]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 1444 is [True, False, False, False, False, True]
State prediction error at timestep 1444 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1445. State = [[-0.25405395  0.21586244]]. Action = [[ 0.06556824 -0.06805575 -0.22295715 -0.37307507]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 1445 is [True, False, False, False, False, True]
State prediction error at timestep 1445 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1445 of -1
Current timestep = 1446. State = [[-0.25360075  0.21572298]]. Action = [[ 0.2287001  -0.23348531  0.00256297  0.5241847 ]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 1446 is [True, False, False, False, False, True]
State prediction error at timestep 1446 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1446 of -1
Current timestep = 1447. State = [[-0.25346115  0.21538384]]. Action = [[-0.22567077  0.00594056 -0.16416867  0.44419312]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 1447 is [True, False, False, False, False, True]
State prediction error at timestep 1447 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1448. State = [[-0.25339535  0.21529415]]. Action = [[ 0.14101014  0.01958951 -0.0591733  -0.3954748 ]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 1448 is [True, False, False, False, False, True]
State prediction error at timestep 1448 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1449. State = [[-0.2532968   0.21538502]]. Action = [[0.08482778 0.21899807 0.02049631 0.7155179 ]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 1449 is [True, False, False, False, False, True]
State prediction error at timestep 1449 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1450. State = [[-0.25312808  0.2151345 ]]. Action = [[-0.05148494 -0.2029608  -0.16032457 -0.57095116]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 1450 is [True, False, False, False, False, True]
State prediction error at timestep 1450 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1451. State = [[-0.25293952  0.21498613]]. Action = [[ 0.06737643 -0.14259493 -0.22370532 -0.27210426]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 1451 is [True, False, False, False, False, True]
State prediction error at timestep 1451 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1452. State = [[-0.25288475  0.21510383]]. Action = [[ 0.08205423  0.16651648  0.1046325  -0.8022387 ]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 1452 is [True, False, False, False, False, True]
State prediction error at timestep 1452 is tensor(7.5885e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1452 of -1
Current timestep = 1453. State = [[-0.25297168  0.21514499]]. Action = [[ 0.08752131 -0.20632967 -0.00406273 -0.8458879 ]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 1453 is [True, False, False, False, False, True]
State prediction error at timestep 1453 is tensor(4.1363e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1454. State = [[-0.2527676  0.2149916]]. Action = [[-0.1742725   0.19534558  0.21994972 -0.3665917 ]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 1454 is [True, False, False, False, False, True]
State prediction error at timestep 1454 is tensor(4.3567e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1455. State = [[-0.2526843   0.21481083]]. Action = [[ 0.16950488  0.13346052 -0.05402318  0.70627904]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 1455 is [True, False, False, False, False, True]
State prediction error at timestep 1455 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1455 of -1
Current timestep = 1456. State = [[-0.25279906  0.21489437]]. Action = [[ 0.01454079 -0.16304936 -0.10330725  0.6131048 ]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 1456 is [True, False, False, False, False, True]
State prediction error at timestep 1456 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1457. State = [[-0.25261965  0.2149962 ]]. Action = [[-0.04823135  0.14500004  0.15585637  0.9263058 ]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 1457 is [True, False, False, False, False, True]
State prediction error at timestep 1457 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1458. State = [[-0.2523841   0.21468471]]. Action = [[ 0.01159364  0.16112521  0.17671612 -0.14354938]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 1458 is [True, False, False, False, False, True]
State prediction error at timestep 1458 is tensor(5.6985e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1459. State = [[-0.2523841   0.21468471]]. Action = [[ 0.16006124  0.06747615 -0.2238938  -0.6341584 ]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 1459 is [True, False, False, False, False, True]
State prediction error at timestep 1459 is tensor(4.2944e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1459 of -1
Current timestep = 1460. State = [[-0.25241223  0.2148919 ]]. Action = [[ 0.23438278 -0.19428708  0.09345785  0.51988435]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 1460 is [True, False, False, False, False, True]
State prediction error at timestep 1460 is tensor(8.6155e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1461. State = [[-0.2524966   0.21482517]]. Action = [[-0.20349047  0.04576904 -0.03925866  0.3307022 ]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 1461 is [True, False, False, False, False, True]
State prediction error at timestep 1461 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1462. State = [[-0.2523574   0.21474081]]. Action = [[-0.16936184  0.22285461  0.11440027  0.749779  ]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 1462 is [True, False, False, False, False, True]
State prediction error at timestep 1462 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1463. State = [[-0.25225073  0.21474433]]. Action = [[ 0.2238459  -0.17298035  0.2155301  -0.19332218]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 1463 is [True, False, False, False, False, True]
State prediction error at timestep 1463 is tensor(4.7781e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1463 of -1
Current timestep = 1464. State = [[-0.25239512  0.21478793]]. Action = [[-0.17370282  0.11771974  0.13119671 -0.44301677]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 1464 is [True, False, False, False, False, True]
State prediction error at timestep 1464 is tensor(2.5793e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1465. State = [[-0.25230363  0.21477486]]. Action = [[-0.10836567 -0.00565411 -0.1398717  -0.39237064]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 1465 is [True, False, False, False, False, True]
State prediction error at timestep 1465 is tensor(5.9260e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1465 of -1
Current timestep = 1466. State = [[-0.25240064  0.2149315 ]]. Action = [[-0.16961089 -0.18100677  0.17028403 -0.04334426]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 1466 is [True, False, False, False, False, True]
State prediction error at timestep 1466 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1467. State = [[-0.25253105  0.21499936]]. Action = [[-0.08359697 -0.24250667 -0.06003985  0.6983123 ]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 1467 is [True, False, False, False, False, True]
State prediction error at timestep 1467 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1468. State = [[-0.25255328  0.21506263]]. Action = [[-0.03662924 -0.22950365 -0.13042478 -0.54484475]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 1468 is [True, False, False, False, False, True]
State prediction error at timestep 1468 is tensor(7.8457e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1469. State = [[-0.25249097  0.21500869]]. Action = [[ 0.04590786  0.18452704 -0.01931709  0.18820691]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 1469 is [True, False, False, False, False, True]
State prediction error at timestep 1469 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1469 of -1
Current timestep = 1470. State = [[-0.25271887  0.2152335 ]]. Action = [[-0.17955649  0.23148316 -0.05587862 -0.70202756]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 1470 is [True, False, False, False, False, True]
State prediction error at timestep 1470 is tensor(1.2978e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1471. State = [[-0.25273883  0.21535319]]. Action = [[ 0.17972615  0.11243474 -0.00285472  0.09830427]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 1471 is [True, False, False, False, False, True]
State prediction error at timestep 1471 is tensor(5.9683e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1472. State = [[-0.2527941   0.21532723]]. Action = [[-0.13200027 -0.14029534  0.06551898  0.08807313]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 1472 is [True, False, False, False, False, True]
State prediction error at timestep 1472 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1473. State = [[-0.25269657  0.21517025]]. Action = [[-0.20841904 -0.1825355  -0.15986183  0.21769416]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 1473 is [True, False, False, False, False, True]
State prediction error at timestep 1473 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1473 of -1
Current timestep = 1474. State = [[-0.2527563   0.21528015]]. Action = [[ 0.12264174 -0.14265117 -0.19006933 -0.51762867]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 1474 is [True, False, False, False, False, True]
State prediction error at timestep 1474 is tensor(5.9214e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1475. State = [[-0.25270668  0.21519433]]. Action = [[-0.09826165 -0.05901717 -0.20257095 -0.37146926]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 1475 is [True, False, False, False, False, True]
State prediction error at timestep 1475 is tensor(8.7976e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1475 of -1
Current timestep = 1476. State = [[-0.2527914   0.21512786]]. Action = [[-0.04299942 -0.18235335 -0.06610495  0.8546841 ]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 1476 is [True, False, False, False, False, True]
State prediction error at timestep 1476 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1477. State = [[-0.25299874  0.21513782]]. Action = [[-0.08461767  0.00269419 -0.22067565  0.2837007 ]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 1477 is [True, False, False, False, False, True]
State prediction error at timestep 1477 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1477 of -1
Current timestep = 1478. State = [[-0.25390136  0.21562406]]. Action = [[ 0.068082   -0.09909718 -0.15642937  0.29493594]]. Reward = [0.]
Curr episode timestep = 576
Scene graph at timestep 1478 is [True, False, False, False, False, True]
State prediction error at timestep 1478 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1478 of -1
Current timestep = 1479. State = [[-0.2537387  0.215026 ]]. Action = [[ 0.06904629  0.14975476 -0.04074818 -0.9151893 ]]. Reward = [0.]
Curr episode timestep = 577
Scene graph at timestep 1479 is [True, False, False, False, False, True]
State prediction error at timestep 1479 is tensor(7.4749e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1480. State = [[-0.25317132  0.21280453]]. Action = [[ 0.07317376 -0.06778842 -0.02674744  0.42147017]]. Reward = [0.]
Curr episode timestep = 578
Scene graph at timestep 1480 is [True, False, False, False, False, True]
State prediction error at timestep 1480 is tensor(8.4486e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1480 of -1
Current timestep = 1481. State = [[-0.25167537  0.20946953]]. Action = [[ 0.06584951  0.0385102  -0.01265174  0.60579467]]. Reward = [0.]
Curr episode timestep = 579
Scene graph at timestep 1481 is [True, False, False, False, False, True]
State prediction error at timestep 1481 is tensor(6.9280e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1482. State = [[-0.25137246  0.2089807 ]]. Action = [[-0.23817831  0.15708289  0.02109304 -0.9788074 ]]. Reward = [0.]
Curr episode timestep = 580
Scene graph at timestep 1482 is [True, False, False, False, False, True]
State prediction error at timestep 1482 is tensor(2.2808e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1482 of -1
Current timestep = 1483. State = [[-0.250986    0.20817201]]. Action = [[ 0.14667195  0.24701655 -0.14953531 -0.8027834 ]]. Reward = [0.]
Curr episode timestep = 581
Scene graph at timestep 1483 is [True, False, False, False, False, True]
State prediction error at timestep 1483 is tensor(4.8940e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1484. State = [[-0.25078967  0.2077768 ]]. Action = [[-0.22630227 -0.14061303  0.20231497 -0.38629198]]. Reward = [0.]
Curr episode timestep = 582
Scene graph at timestep 1484 is [True, False, False, False, False, True]
State prediction error at timestep 1484 is tensor(1.2255e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1485. State = [[-0.25077167  0.20763746]]. Action = [[ 0.22204041 -0.07770266  0.13454008  0.35356474]]. Reward = [0.]
Curr episode timestep = 583
Scene graph at timestep 1485 is [True, False, False, False, False, True]
State prediction error at timestep 1485 is tensor(3.3132e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1486. State = [[-0.25027278  0.20617503]]. Action = [[ 0.07605755 -0.00943948  0.05562916  0.55018115]]. Reward = [0.]
Curr episode timestep = 584
Scene graph at timestep 1486 is [True, False, False, False, False, True]
State prediction error at timestep 1486 is tensor(3.0443e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1486 of -1
Current timestep = 1487. State = [[-0.24956779  0.20462504]]. Action = [[ 0.0253883  -0.01739748 -0.04441756  0.531682  ]]. Reward = [0.]
Curr episode timestep = 585
Scene graph at timestep 1487 is [True, False, False, False, False, True]
State prediction error at timestep 1487 is tensor(3.7730e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1487 of -1
Current timestep = 1488. State = [[-0.24941866  0.20437177]]. Action = [[ 0.15733108 -0.07894999 -0.15469345  0.83842826]]. Reward = [0.]
Curr episode timestep = 586
Scene graph at timestep 1488 is [True, False, False, False, False, True]
State prediction error at timestep 1488 is tensor(3.7224e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1489. State = [[-0.24874143  0.20310971]]. Action = [[0.01346707 0.07648805 0.08270055 0.8425001 ]]. Reward = [0.]
Curr episode timestep = 587
Scene graph at timestep 1489 is [True, False, False, False, False, True]
State prediction error at timestep 1489 is tensor(4.0769e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1490. State = [[-0.24876852  0.20326117]]. Action = [[-0.16635138  0.0693298  -0.05674675  0.1324377 ]]. Reward = [0.]
Curr episode timestep = 588
Scene graph at timestep 1490 is [True, False, False, False, False, True]
State prediction error at timestep 1490 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1491. State = [[-0.2485352   0.20356375]]. Action = [[-0.03023595  0.01850438  0.20276129  0.22902656]]. Reward = [0.]
Curr episode timestep = 589
Scene graph at timestep 1491 is [True, False, False, False, False, True]
State prediction error at timestep 1491 is tensor(5.7661e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1492. State = [[-0.24875215  0.203582  ]]. Action = [[-0.22010998  0.23939264 -0.05287695  0.30981922]]. Reward = [0.]
Curr episode timestep = 590
Scene graph at timestep 1492 is [True, False, False, False, False, True]
State prediction error at timestep 1492 is tensor(6.4339e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1493. State = [[-0.2488515   0.20366874]]. Action = [[ 0.17367882  0.01285887 -0.05177915  0.27765322]]. Reward = [0.]
Curr episode timestep = 591
Scene graph at timestep 1493 is [True, False, False, False, False, True]
State prediction error at timestep 1493 is tensor(1.0902e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1493 of -1
Current timestep = 1494. State = [[-0.24882133  0.20390171]]. Action = [[ 0.22395438  0.14951867  0.05580765 -0.05483007]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 1494 is [True, False, False, False, False, True]
State prediction error at timestep 1494 is tensor(1.7444e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1495. State = [[-0.24879237  0.20394991]]. Action = [[ 0.17336878 -0.17042725 -0.15194318 -0.853587  ]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 1495 is [True, False, False, False, False, True]
State prediction error at timestep 1495 is tensor(1.0807e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1496. State = [[-0.24876338  0.2039981 ]]. Action = [[-0.20617297  0.03560656  0.08762878 -0.43168354]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 1496 is [True, False, False, False, False, True]
State prediction error at timestep 1496 is tensor(4.7238e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1497. State = [[-0.24862732  0.20379777]]. Action = [[ 0.1264342  -0.09533848  0.09987223  0.22273684]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 1497 is [True, False, False, False, False, True]
State prediction error at timestep 1497 is tensor(1.6522e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1498. State = [[-0.2478802   0.20333336]]. Action = [[ 0.17270416 -0.18807559  0.03502131 -0.20807916]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 1498 is [True, False, False, False, False, True]
State prediction error at timestep 1498 is tensor(5.0237e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1499. State = [[-0.24711041  0.20292579]]. Action = [[ 0.19517827 -0.14783116  0.02368584  0.6145985 ]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 1499 is [True, False, False, False, False, True]
State prediction error at timestep 1499 is tensor(1.2165e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1499 of 1
Current timestep = 1500. State = [[-0.24678187  0.20258342]]. Action = [[-0.18135495 -0.21102707  0.07759941  0.89135134]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 1500 is [True, False, False, False, False, True]
State prediction error at timestep 1500 is tensor(9.1226e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1501. State = [[-0.24665299  0.20225017]]. Action = [[ 0.20330763 -0.0521777  -0.19220571  0.04010451]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 1501 is [True, False, False, False, False, True]
State prediction error at timestep 1501 is tensor(5.5434e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1502. State = [[-0.24578066  0.2020335 ]]. Action = [[ 0.21987724  0.22063926 -0.02942622 -0.63724947]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 1502 is [True, False, False, False, False, True]
State prediction error at timestep 1502 is tensor(1.2768e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1503. State = [[-0.24551728  0.20188786]]. Action = [[-0.16566776  0.12412217  0.05849651  0.23182452]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 1503 is [True, False, False, False, False, True]
State prediction error at timestep 1503 is tensor(5.1125e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1503 of 1
Current timestep = 1504. State = [[-0.24509136  0.20187448]]. Action = [[ 0.10915115 -0.24371657 -0.18862836 -0.39885426]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 1504 is [True, False, False, False, False, True]
State prediction error at timestep 1504 is tensor(1.5637e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1505. State = [[-0.24449432  0.20139156]]. Action = [[ 0.03435066  0.16668409 -0.19828056 -0.7990741 ]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 1505 is [True, False, False, False, False, True]
State prediction error at timestep 1505 is tensor(1.6696e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1506. State = [[-0.24410002  0.20145059]]. Action = [[ 0.24452507  0.18384153 -0.14641352 -0.9172377 ]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 1506 is [True, False, False, False, False, True]
State prediction error at timestep 1506 is tensor(4.9853e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1507. State = [[-0.2439664   0.20145242]]. Action = [[-0.13902295  0.21513116 -0.21127987  0.58443296]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 1507 is [True, False, False, False, False, True]
State prediction error at timestep 1507 is tensor(4.6203e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1507 of 1
Current timestep = 1508. State = [[-0.2437968  0.2013849]]. Action = [[0.16174966 0.0295426  0.01822388 0.30491185]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 1508 is [True, False, False, False, False, True]
State prediction error at timestep 1508 is tensor(5.3647e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1509. State = [[-0.24353336  0.2014461 ]]. Action = [[-0.19253193 -0.09352431 -0.2226235  -0.20995438]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 1509 is [True, False, False, False, False, True]
State prediction error at timestep 1509 is tensor(4.4160e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1510. State = [[-0.24363166  0.20120461]]. Action = [[-0.03286463  0.1619575   0.20421523  0.16072035]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 1510 is [True, False, False, False, False, True]
State prediction error at timestep 1510 is tensor(2.8428e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1511. State = [[-0.24342676  0.20118581]]. Action = [[-0.1212709  -0.19455902 -0.15848723  0.4166131 ]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 1511 is [True, False, False, False, False, True]
State prediction error at timestep 1511 is tensor(6.0719e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1511 of 1
Current timestep = 1512. State = [[-0.24330837  0.20087475]]. Action = [[ 0.01975399 -0.06193055  0.14118308 -0.771145  ]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 1512 is [True, False, False, False, False, True]
State prediction error at timestep 1512 is tensor(5.0498e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1513. State = [[-0.24286479  0.20027044]]. Action = [[ 0.14574865  0.23630953  0.05601606 -0.24961436]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 1513 is [True, False, False, False, False, True]
State prediction error at timestep 1513 is tensor(2.0105e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1514. State = [[-0.24251348  0.19993159]]. Action = [[-0.11875692  0.20162445  0.14318532  0.19508731]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 1514 is [True, False, False, False, False, True]
State prediction error at timestep 1514 is tensor(2.7202e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1515. State = [[-0.24236453  0.19952357]]. Action = [[-0.10941693 -0.13735394 -0.19617276  0.424487  ]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 1515 is [True, False, False, False, False, True]
State prediction error at timestep 1515 is tensor(4.1577e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1516. State = [[-0.24233189  0.19939129]]. Action = [[ 0.11593187  0.13353884 -0.18331152 -0.35396993]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 1516 is [True, False, False, False, False, True]
State prediction error at timestep 1516 is tensor(4.3776e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1517. State = [[-0.24216361  0.1990633 ]]. Action = [[-0.14984111  0.15981638 -0.04854284  0.32054448]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 1517 is [True, False, False, False, False, True]
State prediction error at timestep 1517 is tensor(3.5970e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1517 of 1
Current timestep = 1518. State = [[-0.24195512  0.19859274]]. Action = [[-0.19150044  0.11182934 -0.14496692  0.24285007]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 1518 is [True, False, False, False, False, True]
State prediction error at timestep 1518 is tensor(5.9792e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1519. State = [[-0.24181834  0.19822434]]. Action = [[ 0.00383088  0.07269475 -0.23364107 -0.7068784 ]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 1519 is [True, False, False, False, False, True]
State prediction error at timestep 1519 is tensor(7.3373e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1519 of 1
Current timestep = 1520. State = [[-0.24170735  0.19841067]]. Action = [[-0.06348839  0.13977829  0.06696674  0.19152975]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 1520 is [True, False, False, False, False, True]
State prediction error at timestep 1520 is tensor(3.0984e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1521. State = [[-0.24171285  0.19877884]]. Action = [[-0.2244729   0.00970158  0.0656487  -0.6973513 ]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 1521 is [True, False, False, False, False, True]
State prediction error at timestep 1521 is tensor(2.0316e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1522. State = [[-0.24155267  0.19868444]]. Action = [[ 0.22355843  0.03447169 -0.07544303 -0.06090456]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 1522 is [True, False, False, False, False, True]
State prediction error at timestep 1522 is tensor(7.6423e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1523. State = [[-0.2414078   0.19867933]]. Action = [[ 0.15006962  0.21110737 -0.05019449 -0.1985851 ]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 1523 is [True, False, False, False, False, True]
State prediction error at timestep 1523 is tensor(9.2496e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1524. State = [[-0.24124192  0.19895905]]. Action = [[ 0.06301469  0.04563421 -0.14532301 -0.18432975]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 1524 is [True, False, False, False, False, True]
State prediction error at timestep 1524 is tensor(3.4082e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1524 of 1
Current timestep = 1525. State = [[-0.24058813  0.19925427]]. Action = [[-0.16170725 -0.10689282 -0.10838149  0.9761623 ]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 1525 is [True, False, False, False, False, True]
State prediction error at timestep 1525 is tensor(7.9402e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1526. State = [[-0.24016568  0.19953017]]. Action = [[ 0.08589813  0.15746778  0.14277261 -0.8031716 ]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 1526 is [True, False, False, False, False, True]
State prediction error at timestep 1526 is tensor(2.0006e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1526 of 1
Current timestep = 1527. State = [[-0.23988698  0.19975038]]. Action = [[ 0.18200776 -0.03738618  0.040755    0.95884585]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 1527 is [True, False, False, False, False, True]
State prediction error at timestep 1527 is tensor(1.3325e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1528. State = [[-0.23956294  0.19997874]]. Action = [[ 0.13006979 -0.18607754 -0.02204153  0.21446812]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 1528 is [True, False, False, False, False, True]
State prediction error at timestep 1528 is tensor(5.3314e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1529. State = [[-0.2382978   0.20058393]]. Action = [[-0.06250921  0.05296752 -0.16980559  0.532773  ]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 1529 is [True, False, False, False, False, True]
State prediction error at timestep 1529 is tensor(3.2833e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1529 of 1
Current timestep = 1530. State = [[-0.23883714  0.20120928]]. Action = [[0.24558604 0.19073921 0.13050413 0.06608713]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 1530 is [True, False, False, False, False, True]
State prediction error at timestep 1530 is tensor(7.5735e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1531. State = [[-0.23921528  0.20181233]]. Action = [[ 0.01228118 -0.14107995 -0.08260038  0.71486473]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 1531 is [True, False, False, False, False, True]
State prediction error at timestep 1531 is tensor(2.4259e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1532. State = [[-0.2392419   0.20223881]]. Action = [[-0.17973636 -0.0078495  -0.1917645   0.8054383 ]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 1532 is [True, False, False, False, False, True]
State prediction error at timestep 1532 is tensor(5.4278e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1532 of 1
Current timestep = 1533. State = [[-0.23931056  0.20229003]]. Action = [[-0.16240078 -0.12022901  0.1977449  -0.96266794]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 1533 is [True, False, False, False, False, True]
State prediction error at timestep 1533 is tensor(3.4831e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1534. State = [[-0.23954715  0.20294975]]. Action = [[ 0.12723088  0.03682217 -0.14677437  0.6531147 ]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 1534 is [True, False, False, False, False, True]
State prediction error at timestep 1534 is tensor(2.5655e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1535. State = [[-0.23911735  0.2034628 ]]. Action = [[ 0.14888477 -0.04939315  0.07531586 -0.03796864]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 1535 is [True, False, False, False, False, True]
State prediction error at timestep 1535 is tensor(3.6867e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1536. State = [[-0.23868339  0.20380141]]. Action = [[ 0.22242326  0.06378198 -0.1241376  -0.29430854]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 1536 is [True, False, False, False, False, True]
State prediction error at timestep 1536 is tensor(2.2541e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1537. State = [[-0.23771702  0.20460214]]. Action = [[ 0.04969835  0.13175768 -0.01564471 -0.4507184 ]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 1537 is [True, False, False, False, False, True]
State prediction error at timestep 1537 is tensor(4.0253e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1538. State = [[-0.23729818  0.2058077 ]]. Action = [[-0.17933646  0.10101768  0.05691412 -0.88707113]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 1538 is [True, False, False, False, False, True]
State prediction error at timestep 1538 is tensor(5.4521e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1538 of 1
Current timestep = 1539. State = [[-0.23666403  0.20713955]]. Action = [[-0.18043214 -0.10331802 -0.00292286  0.7773968 ]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 1539 is [True, False, False, False, False, True]
State prediction error at timestep 1539 is tensor(1.7969e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1540. State = [[-0.23600353  0.20766596]]. Action = [[ 0.22258422  0.0763092  -0.02856056 -0.23330933]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 1540 is [True, False, False, False, False, True]
State prediction error at timestep 1540 is tensor(6.4441e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1541. State = [[-0.23502494  0.2084019 ]]. Action = [[-0.08904788 -0.2224552   0.05059728  0.9587076 ]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 1541 is [True, False, False, False, False, True]
State prediction error at timestep 1541 is tensor(7.1922e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1542. State = [[-0.2340263   0.20933647]]. Action = [[-0.1883979   0.08227271 -0.02020484  0.20656669]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 1542 is [True, False, False, False, False, True]
State prediction error at timestep 1542 is tensor(3.7856e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1543. State = [[-0.233305    0.20991021]]. Action = [[-0.10311955  0.21512204  0.11287066  0.20715904]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 1543 is [True, False, False, False, False, True]
State prediction error at timestep 1543 is tensor(9.5388e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1543 of 1
Current timestep = 1544. State = [[-0.23165308  0.21115607]]. Action = [[ 0.10751855 -0.18484941  0.03609565 -0.40329325]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 1544 is [True, False, False, False, False, True]
State prediction error at timestep 1544 is tensor(1.9756e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1545. State = [[-0.2288621   0.21346201]]. Action = [[-0.02507991  0.04322219 -0.2122115   0.5883174 ]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 1545 is [True, False, False, False, False, True]
State prediction error at timestep 1545 is tensor(2.7014e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1545 of 1
Current timestep = 1546. State = [[-0.22874154  0.2143506 ]]. Action = [[-0.12632623  0.13962948  0.16507211  0.06924069]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 1546 is [True, False, False, False, False, True]
State prediction error at timestep 1546 is tensor(8.2973e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1547. State = [[-0.22875105  0.21482678]]. Action = [[-0.1970698   0.20999244  0.24629474  0.36005282]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 1547 is [True, False, False, False, False, True]
State prediction error at timestep 1547 is tensor(1.0065e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1548. State = [[-0.22857219  0.21516556]]. Action = [[ 0.23377126 -0.19646715  0.11536404  0.47717774]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 1548 is [True, False, False, False, False, True]
State prediction error at timestep 1548 is tensor(4.4202e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1549. State = [[-0.22844048  0.21543735]]. Action = [[-0.24712436  0.03661042 -0.13946502  0.31353962]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 1549 is [True, False, False, False, False, True]
State prediction error at timestep 1549 is tensor(4.0136e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1549 of 1
Current timestep = 1550. State = [[-0.22874498  0.21661656]]. Action = [[-0.06215876  0.01902384  0.18014252 -0.26472068]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 1550 is [True, False, False, False, False, True]
State prediction error at timestep 1550 is tensor(1.0711e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1551. State = [[-0.22910982  0.21698989]]. Action = [[ 0.10807914 -0.14758466 -0.24595691  0.6004623 ]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 1551 is [True, False, False, False, False, True]
State prediction error at timestep 1551 is tensor(1.4854e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1552. State = [[-0.2300157   0.21841428]]. Action = [[ 0.09317741 -0.00187533 -0.11923543 -0.895272  ]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 1552 is [True, False, False, False, False, True]
State prediction error at timestep 1552 is tensor(9.5306e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1553. State = [[-0.22971855  0.21855131]]. Action = [[0.20346606 0.10467708 0.173388   0.22984481]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 1553 is [True, False, False, False, False, True]
State prediction error at timestep 1553 is tensor(4.1172e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1554. State = [[-0.2293622   0.21850377]]. Action = [[ 0.11613777 -0.14942674 -0.04872404 -0.83206046]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 1554 is [True, False, False, False, False, True]
State prediction error at timestep 1554 is tensor(9.1000e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1555. State = [[-0.22927867  0.21859951]]. Action = [[ 0.12954485  0.22381467 -0.07850359 -0.680371  ]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 1555 is [True, False, False, False, False, True]
State prediction error at timestep 1555 is tensor(8.2181e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1556. State = [[-0.22905281  0.21891828]]. Action = [[ 0.188793   -0.18910567  0.12039581 -0.4416315 ]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 1556 is [True, False, False, False, False, True]
State prediction error at timestep 1556 is tensor(4.5001e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1557. State = [[-0.22889355  0.21913747]]. Action = [[-0.09634283  0.24518895  0.08434013  0.7316892 ]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 1557 is [True, False, False, False, False, True]
State prediction error at timestep 1557 is tensor(5.6844e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1558. State = [[-0.22862867  0.2191837 ]]. Action = [[ 0.1564498  -0.02264532 -0.05192336 -0.44668698]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 1558 is [True, False, False, False, False, True]
State prediction error at timestep 1558 is tensor(2.4637e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1558 of 1
Current timestep = 1559. State = [[-0.22852388  0.21915522]]. Action = [[-0.19267876  0.11196268 -0.22779939  0.64626634]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 1559 is [True, False, False, False, False, True]
State prediction error at timestep 1559 is tensor(8.3817e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1560. State = [[-0.22842102  0.21937563]]. Action = [[ 0.07166666 -0.13414063  0.01827413  0.7636142 ]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 1560 is [True, False, False, False, False, True]
State prediction error at timestep 1560 is tensor(2.4121e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1561. State = [[-0.22850806  0.21923009]]. Action = [[-0.04669857 -0.23664509 -0.19661553  0.1413523 ]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 1561 is [True, False, False, False, False, True]
State prediction error at timestep 1561 is tensor(6.1029e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1562. State = [[-0.22829053  0.2193626 ]]. Action = [[-0.21058118  0.06382462 -0.15129079  0.5680233 ]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 1562 is [True, False, False, False, False, True]
State prediction error at timestep 1562 is tensor(5.9510e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1562 of 1
Current timestep = 1563. State = [[-0.22835049  0.21953642]]. Action = [[-0.07756332  0.04962528  0.06815824 -0.12955427]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 1563 is [True, False, False, False, False, True]
State prediction error at timestep 1563 is tensor(1.7039e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1564. State = [[-0.22881635  0.22005978]]. Action = [[-0.12252922 -0.19053     0.22448695  0.26064026]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 1564 is [True, False, False, False, False, True]
State prediction error at timestep 1564 is tensor(1.0053e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1565. State = [[-0.22956842  0.22113627]]. Action = [[-0.03338575 -0.08433798  0.19444579 -0.75629514]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 1565 is [True, False, False, False, False, True]
State prediction error at timestep 1565 is tensor(2.9074e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1566. State = [[-0.2296982   0.22126801]]. Action = [[-0.20186962  0.1288011  -0.0588697  -0.89272255]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 1566 is [True, False, False, False, False, True]
State prediction error at timestep 1566 is tensor(4.9643e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1567. State = [[-0.22966768  0.22120813]]. Action = [[-0.05964565 -0.03300852  0.19606155 -0.8815225 ]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 1567 is [True, False, False, False, False, True]
State prediction error at timestep 1567 is tensor(2.6375e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1568. State = [[-0.22966768  0.22120813]]. Action = [[ 0.2011317  -0.02137455  0.0378854  -0.43578553]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 1568 is [True, False, False, False, False, True]
State prediction error at timestep 1568 is tensor(4.6238e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1569. State = [[-0.22964494  0.22114336]]. Action = [[ 0.18737686  0.1913831  -0.18931767  0.801209  ]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 1569 is [True, False, False, False, False, True]
State prediction error at timestep 1569 is tensor(4.2022e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1570. State = [[-0.22963661  0.22105487]]. Action = [[-0.15033318  0.02418515  0.10266328  0.25353718]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 1570 is [True, False, False, False, False, True]
State prediction error at timestep 1570 is tensor(2.9084e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1571. State = [[-0.22969864  0.22117707]]. Action = [[ 0.22247657  0.06323156 -0.16958953  0.44134212]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 1571 is [True, False, False, False, False, True]
State prediction error at timestep 1571 is tensor(1.7173e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1572. State = [[-0.2298278   0.22127564]]. Action = [[-0.08460478  0.09160107 -0.17222519 -0.3542999 ]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 1572 is [True, False, False, False, False, True]
State prediction error at timestep 1572 is tensor(1.1672e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1572 of 1
Current timestep = 1573. State = [[-0.23036627  0.22191618]]. Action = [[ 0.2421912 -0.2370949 -0.241625  -0.4956665]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 1573 is [True, False, False, False, False, True]
State prediction error at timestep 1573 is tensor(7.7225e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1574. State = [[-0.23223554  0.22431001]]. Action = [[-0.02034786 -0.0019317  -0.11003436 -0.09736067]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 1574 is [True, False, False, False, False, True]
State prediction error at timestep 1574 is tensor(4.7117e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1574 of 1
Current timestep = 1575. State = [[-0.23265314  0.22495212]]. Action = [[-0.12045681  0.22050697 -0.04984817  0.90365124]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 1575 is [True, False, False, False, False, True]
State prediction error at timestep 1575 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1576. State = [[-0.23300347  0.22531922]]. Action = [[-0.22079739  0.12278795 -0.21240771 -0.38275886]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 1576 is [True, False, False, False, False, True]
State prediction error at timestep 1576 is tensor(1.0219e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1577. State = [[-0.23399326  0.22639309]]. Action = [[-0.07252018 -0.03516912  0.10583559 -0.5245606 ]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 1577 is [True, False, False, False, False, True]
State prediction error at timestep 1577 is tensor(2.4767e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1577 of 1
Current timestep = 1578. State = [[-0.23444085  0.22669782]]. Action = [[-0.23923473 -0.13949332  0.02910274  0.81488156]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 1578 is [True, False, False, False, False, True]
State prediction error at timestep 1578 is tensor(9.0027e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1579. State = [[-0.23461078  0.22687635]]. Action = [[ 0.15166193  0.13551623 -0.06859827 -0.13205767]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 1579 is [True, False, False, False, False, True]
State prediction error at timestep 1579 is tensor(2.4666e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1579 of 1
Current timestep = 1580. State = [[-0.23481238  0.22708401]]. Action = [[-0.13489382 -0.05020434  0.06157249  0.7282522 ]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 1580 is [True, False, False, False, False, True]
State prediction error at timestep 1580 is tensor(3.7585e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1581. State = [[-0.23501277  0.2271715 ]]. Action = [[-0.20556296  0.20563823  0.10269174  0.80124044]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 1581 is [True, False, False, False, False, True]
State prediction error at timestep 1581 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1582. State = [[-0.23531628  0.22740409]]. Action = [[ 0.04634684 -0.19036041 -0.03885247 -0.24008226]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 1582 is [True, False, False, False, False, True]
State prediction error at timestep 1582 is tensor(8.4210e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1583. State = [[-0.23543409  0.22755823]]. Action = [[ 0.07062235 -0.22845212  0.22001669  0.17181182]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 1583 is [True, False, False, False, False, True]
State prediction error at timestep 1583 is tensor(9.4757e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1584. State = [[-0.23553237  0.2275351 ]]. Action = [[-0.06285672 -0.2226036  -0.11826426 -0.5474941 ]]. Reward = [0.]
Curr episode timestep = 682
Scene graph at timestep 1584 is [True, False, False, False, False, True]
State prediction error at timestep 1584 is tensor(5.1678e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1584 of 1
Current timestep = 1585. State = [[-0.23570715  0.22769044]]. Action = [[ 0.18078315 -0.19852468  0.192673   -0.3903185 ]]. Reward = [0.]
Curr episode timestep = 683
Scene graph at timestep 1585 is [True, False, False, False, False, True]
State prediction error at timestep 1585 is tensor(4.1407e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1586. State = [[-0.23624797  0.2280744 ]]. Action = [[ 0.0657317   0.12778115 -0.22965193 -0.8994895 ]]. Reward = [0.]
Curr episode timestep = 684
Scene graph at timestep 1586 is [True, False, False, False, False, True]
State prediction error at timestep 1586 is tensor(6.1440e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1587. State = [[-0.23759675  0.22983693]]. Action = [[-0.01445964  0.07434481  0.11740154  0.34578264]]. Reward = [0.]
Curr episode timestep = 685
Scene graph at timestep 1587 is [True, False, False, False, False, True]
State prediction error at timestep 1587 is tensor(4.6019e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1587 of 1
Current timestep = 1588. State = [[-0.23822252  0.23076648]]. Action = [[ 0.05312926 -0.14414583 -0.14590311 -0.02887201]]. Reward = [0.]
Curr episode timestep = 686
Scene graph at timestep 1588 is [True, False, False, False, False, True]
State prediction error at timestep 1588 is tensor(7.6053e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1589. State = [[-0.23890594  0.23166253]]. Action = [[-0.00458093 -0.216687   -0.1238651   0.6126387 ]]. Reward = [0.]
Curr episode timestep = 687
Scene graph at timestep 1589 is [True, False, False, False, False, True]
State prediction error at timestep 1589 is tensor(8.4417e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1590. State = [[-0.23924662  0.23200524]]. Action = [[-0.17604406  0.22601819  0.20736569  0.8860545 ]]. Reward = [0.]
Curr episode timestep = 688
Scene graph at timestep 1590 is [True, False, False, False, False, True]
State prediction error at timestep 1590 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1591. State = [[-0.23964694  0.23247558]]. Action = [[ 0.00945041 -0.13502432 -0.15631081 -0.13957375]]. Reward = [0.]
Curr episode timestep = 689
Scene graph at timestep 1591 is [True, False, False, False, False, True]
State prediction error at timestep 1591 is tensor(1.6493e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1591 of 1
Current timestep = 1592. State = [[-0.24017838  0.23340145]]. Action = [[ 0.19303548  0.22338635 -0.24006617 -0.4364065 ]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 1592 is [True, False, False, False, False, True]
State prediction error at timestep 1592 is tensor(2.6501e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1593. State = [[-0.24051975  0.2336021 ]]. Action = [[0.12333983 0.1863909  0.12192732 0.28211117]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 1593 is [True, False, False, False, False, True]
State prediction error at timestep 1593 is tensor(7.7814e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1594. State = [[-0.24078803  0.23394284]]. Action = [[ 0.1694017  -0.01218474 -0.1437253   0.8239925 ]]. Reward = [0.]
Curr episode timestep = 692
Scene graph at timestep 1594 is [True, False, False, False, False, True]
State prediction error at timestep 1594 is tensor(5.3592e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1595. State = [[-0.24100937  0.23433825]]. Action = [[-0.02521114 -0.20656879 -0.20744886  0.9266708 ]]. Reward = [0.]
Curr episode timestep = 693
Scene graph at timestep 1595 is [True, False, False, False, False, True]
State prediction error at timestep 1595 is tensor(9.5380e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1595 of 1
Current timestep = 1596. State = [[-0.24175675  0.23522972]]. Action = [[-0.0378454   0.12497088  0.18023121  0.15618551]]. Reward = [0.]
Curr episode timestep = 694
Scene graph at timestep 1596 is [True, False, False, False, False, True]
State prediction error at timestep 1596 is tensor(7.8421e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1597. State = [[-0.24284673  0.23686658]]. Action = [[-0.2273592  -0.14547458  0.22697082  0.47918534]]. Reward = [0.]
Curr episode timestep = 695
Scene graph at timestep 1597 is [True, False, False, False, False, True]
State prediction error at timestep 1597 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1598. State = [[-0.24348493  0.23783487]]. Action = [[0.22847113 0.12271678 0.05721486 0.04138017]]. Reward = [0.]
Curr episode timestep = 696
Scene graph at timestep 1598 is [True, False, False, False, False, True]
State prediction error at timestep 1598 is tensor(3.8393e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1599. State = [[-0.24427442  0.23854579]]. Action = [[-0.1788569  -0.21424168  0.12285948 -0.43105626]]. Reward = [0.]
Curr episode timestep = 697
Scene graph at timestep 1599 is [True, False, False, False, False, True]
State prediction error at timestep 1599 is tensor(1.0269e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1600. State = [[-0.24484691  0.23935206]]. Action = [[ 0.20427358 -0.22950268 -0.18385097 -0.21245104]]. Reward = [0.]
Curr episode timestep = 698
Scene graph at timestep 1600 is [True, False, False, False, False, True]
State prediction error at timestep 1600 is tensor(7.0769e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1600 of -1
Current timestep = 1601. State = [[-0.24554956  0.24025376]]. Action = [[-0.203243    0.18535197 -0.1597717  -0.27225065]]. Reward = [0.]
Curr episode timestep = 699
Scene graph at timestep 1601 is [True, False, False, False, False, True]
State prediction error at timestep 1601 is tensor(2.1873e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1602. State = [[-0.24583335  0.2404823 ]]. Action = [[-0.24154568 -0.15947938  0.21079022  0.89299226]]. Reward = [0.]
Curr episode timestep = 700
Scene graph at timestep 1602 is [True, False, False, False, False, True]
State prediction error at timestep 1602 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1603. State = [[-0.24687867  0.24178894]]. Action = [[ 0.0226869   0.06816292 -0.07245684  0.6907015 ]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 1603 is [True, False, False, False, False, True]
State prediction error at timestep 1603 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1603 of -1
Current timestep = 1604. State = [[-0.2473083   0.24251498]]. Action = [[-0.17252684  0.16278899  0.10305712  0.24535704]]. Reward = [0.]
Curr episode timestep = 702
Scene graph at timestep 1604 is [True, False, False, False, False, True]
State prediction error at timestep 1604 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1605. State = [[-0.24798092  0.24416032]]. Action = [[0.07887143 0.09112555 0.22290275 0.26352012]]. Reward = [0.]
Curr episode timestep = 703
Scene graph at timestep 1605 is [True, False, False, False, False, True]
State prediction error at timestep 1605 is tensor(7.9825e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1605 of -1
Current timestep = 1606. State = [[-0.24791196  0.2450686 ]]. Action = [[ 0.18508536  0.06539375 -0.05096409 -0.5734395 ]]. Reward = [0.]
Curr episode timestep = 704
Scene graph at timestep 1606 is [True, False, False, False, False, True]
State prediction error at timestep 1606 is tensor(1.7495e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1607. State = [[-0.24762222  0.24577816]]. Action = [[-0.17298236  0.02821469 -0.06772402  0.36579025]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 1607 is [True, False, False, False, False, True]
State prediction error at timestep 1607 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1608. State = [[-0.24734694  0.24626549]]. Action = [[ 0.13676769  0.21291733 -0.15832056  0.331138  ]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 1608 is [True, False, False, False, False, True]
State prediction error at timestep 1608 is tensor(1.6401e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1608 of -1
Current timestep = 1609. State = [[-0.2462119   0.24891604]]. Action = [[-0.11334413  0.01972091 -0.06999691  0.2349832 ]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 1609 is [True, False, False, False, False, True]
State prediction error at timestep 1609 is tensor(8.4693e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1610. State = [[-0.24670357  0.24958697]]. Action = [[ 0.21924531  0.07742214  0.23459667 -0.6751764 ]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 1610 is [True, False, False, False, False, True]
State prediction error at timestep 1610 is tensor(2.8546e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1611. State = [[-0.24721166  0.25035062]]. Action = [[-0.07406563 -0.22583163 -0.11461964  0.6485735 ]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 1611 is [True, False, False, False, False, True]
State prediction error at timestep 1611 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1612. State = [[-0.24794205  0.25122738]]. Action = [[-0.14298558  0.1430529   0.10251319  0.96402645]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 1612 is [True, False, False, False, False, True]
State prediction error at timestep 1612 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1613. State = [[-0.2493083   0.25290406]]. Action = [[ 0.06682608  0.09207231 -0.03913483  0.34042096]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 1613 is [True, False, False, False, False, True]
State prediction error at timestep 1613 is tensor(3.8266e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1613 of -1
Current timestep = 1614. State = [[-0.24985188  0.25546163]]. Action = [[ 0.00571325  0.10828531 -0.15141773  0.8915403 ]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 1614 is [True, False, False, False, False, True]
State prediction error at timestep 1614 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1614 of -1
Current timestep = 1615. State = [[-0.25002748  0.25718018]]. Action = [[ 0.23419705  0.24259412  0.0269185  -0.4017185 ]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 1615 is [True, False, False, False, False, True]
State prediction error at timestep 1615 is tensor(5.9131e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1616. State = [[-0.2503584   0.26138481]]. Action = [[ 0.10547119  0.01429918  0.02688831 -0.46477473]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 1616 is [True, False, False, False, False, True]
State prediction error at timestep 1616 is tensor(4.8790e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1616 of -1
Current timestep = 1617. State = [[-0.24961229  0.26240164]]. Action = [[-0.13374256 -0.11262369 -0.03964716 -0.05750614]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 1617 is [True, False, False, False, False, True]
State prediction error at timestep 1617 is tensor(9.0408e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1618. State = [[-0.24859874  0.2631213 ]]. Action = [[ 0.17763239  0.17515355 -0.18548514 -0.01421934]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 1618 is [True, False, False, False, False, True]
State prediction error at timestep 1618 is tensor(2.2091e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1619. State = [[-0.24782655  0.26367658]]. Action = [[-0.13844368 -0.17515416 -0.23253128 -0.23632479]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 1619 is [True, False, False, False, False, True]
State prediction error at timestep 1619 is tensor(9.6088e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1620. State = [[-0.2445005   0.26640055]]. Action = [[ 0.08149847 -0.01718497  0.01073107 -0.55857223]]. Reward = [0.]
Curr episode timestep = 718
Scene graph at timestep 1620 is [True, False, False, False, False, True]
State prediction error at timestep 1620 is tensor(4.3340e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1620 of -1
Current timestep = 1621. State = [[-0.24324822  0.26711535]]. Action = [[ 0.1687814  -0.04410815 -0.18623053  0.48776686]]. Reward = [0.]
Curr episode timestep = 719
Scene graph at timestep 1621 is [True, False, False, False, False, True]
State prediction error at timestep 1621 is tensor(5.7142e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1622. State = [[-0.24191085  0.26788548]]. Action = [[-0.03846607  0.23009664 -0.09904251 -0.24790484]]. Reward = [0.]
Curr episode timestep = 720
Scene graph at timestep 1622 is [True, False, False, False, False, True]
State prediction error at timestep 1622 is tensor(6.9994e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1622 of -1
Current timestep = 1623. State = [[-0.24081947  0.26831076]]. Action = [[-0.20863143  0.2246877  -0.06091221 -0.6757943 ]]. Reward = [0.]
Curr episode timestep = 721
Scene graph at timestep 1623 is [True, False, False, False, False, True]
State prediction error at timestep 1623 is tensor(2.9851e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1624. State = [[-0.24004005  0.2688095 ]]. Action = [[-0.06330994 -0.24138966  0.24889147 -0.2550742 ]]. Reward = [0.]
Curr episode timestep = 722
Scene graph at timestep 1624 is [True, False, False, False, False, True]
State prediction error at timestep 1624 is tensor(2.5952e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1625. State = [[-0.23907459  0.26939282]]. Action = [[-0.24035983 -0.0708546  -0.2231476  -0.8126912 ]]. Reward = [0.]
Curr episode timestep = 723
Scene graph at timestep 1625 is [True, False, False, False, False, True]
State prediction error at timestep 1625 is tensor(2.2882e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1626. State = [[-0.23789378  0.2698633 ]]. Action = [[-0.02043958 -0.2036154   0.08808231  0.452124  ]]. Reward = [0.]
Curr episode timestep = 724
Scene graph at timestep 1626 is [True, False, False, False, False, True]
State prediction error at timestep 1626 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1626 of -1
Current timestep = 1627. State = [[-0.23566382  0.27095017]]. Action = [[-0.00835821 -0.10778162  0.01897994  0.73235273]]. Reward = [0.]
Curr episode timestep = 725
Scene graph at timestep 1627 is [True, False, False, False, False, True]
State prediction error at timestep 1627 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1628. State = [[-0.23551174  0.2703736 ]]. Action = [[ 0.05560166  0.16771245 -0.16984706 -0.4625641 ]]. Reward = [0.]
Curr episode timestep = 726
Scene graph at timestep 1628 is [True, False, False, False, False, True]
State prediction error at timestep 1628 is tensor(1.1530e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1628 of -1
Current timestep = 1629. State = [[-0.23527019  0.26961038]]. Action = [[ 0.22853518 -0.18497127  0.04426867  0.01551509]]. Reward = [0.]
Curr episode timestep = 727
Scene graph at timestep 1629 is [True, False, False, False, False, True]
State prediction error at timestep 1629 is tensor(5.5854e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1630. State = [[-0.23481742  0.2691985 ]]. Action = [[-0.14295295 -0.01789252 -0.08874416 -0.74583894]]. Reward = [0.]
Curr episode timestep = 728
Scene graph at timestep 1630 is [True, False, False, False, False, True]
State prediction error at timestep 1630 is tensor(4.7161e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1631. State = [[-0.23460582  0.2690152 ]]. Action = [[-0.19351934 -0.23036455  0.09653309 -0.68496835]]. Reward = [0.]
Curr episode timestep = 729
Scene graph at timestep 1631 is [True, False, False, False, False, True]
State prediction error at timestep 1631 is tensor(6.3523e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1632. State = [[-0.2346538  0.268578 ]]. Action = [[ 0.08657789  0.2215639  -0.09708291  0.32637668]]. Reward = [0.]
Curr episode timestep = 730
Scene graph at timestep 1632 is [True, False, False, False, False, True]
State prediction error at timestep 1632 is tensor(1.7293e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1633. State = [[-0.23429567  0.26832825]]. Action = [[ 0.11865515 -0.17022443 -0.21936482  0.03673577]]. Reward = [0.]
Curr episode timestep = 731
Scene graph at timestep 1633 is [True, False, False, False, False, True]
State prediction error at timestep 1633 is tensor(2.5582e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1633 of -1
Current timestep = 1634. State = [[-0.23405933  0.26798594]]. Action = [[-0.2412765  -0.23651786 -0.11045495 -0.36337578]]. Reward = [0.]
Curr episode timestep = 732
Scene graph at timestep 1634 is [True, False, False, False, False, True]
State prediction error at timestep 1634 is tensor(4.2096e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1635. State = [[-0.23419355  0.2679209 ]]. Action = [[ 0.20150739 -0.13990687  0.14979726  0.7331183 ]]. Reward = [0.]
Curr episode timestep = 733
Scene graph at timestep 1635 is [True, False, False, False, False, True]
State prediction error at timestep 1635 is tensor(5.4472e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1636. State = [[-0.23408018  0.26793724]]. Action = [[ 0.18985313  0.06871358 -0.17794026 -0.11553842]]. Reward = [0.]
Curr episode timestep = 734
Scene graph at timestep 1636 is [True, False, False, False, False, True]
State prediction error at timestep 1636 is tensor(2.6829e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1637. State = [[-0.23382622  0.2678398 ]]. Action = [[ 0.1899147  -0.03206137  0.13001436 -0.8912576 ]]. Reward = [0.]
Curr episode timestep = 735
Scene graph at timestep 1637 is [True, False, False, False, False, True]
State prediction error at timestep 1637 is tensor(1.9698e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1637 of -1
Current timestep = 1638. State = [[-0.23382373  0.26759532]]. Action = [[ 0.16428155 -0.15710492 -0.24536705  0.45715213]]. Reward = [0.]
Curr episode timestep = 736
Scene graph at timestep 1638 is [True, False, False, False, False, True]
State prediction error at timestep 1638 is tensor(3.8670e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1639. State = [[-0.23380339  0.26752663]]. Action = [[-0.20428464 -0.23637585  0.02369392 -0.8269358 ]]. Reward = [0.]
Curr episode timestep = 737
Scene graph at timestep 1639 is [True, False, False, False, False, True]
State prediction error at timestep 1639 is tensor(6.8645e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1640. State = [[-0.23371078  0.26733962]]. Action = [[-0.00895433  0.00382802 -0.17061009 -0.34190953]]. Reward = [0.]
Curr episode timestep = 738
Scene graph at timestep 1640 is [True, False, False, False, False, True]
State prediction error at timestep 1640 is tensor(1.8132e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1640 of -1
Current timestep = 1641. State = [[-0.23363245  0.2673053 ]]. Action = [[-0.2013901  -0.11956434  0.2236759  -0.4358878 ]]. Reward = [0.]
Curr episode timestep = 739
Scene graph at timestep 1641 is [True, False, False, False, False, True]
State prediction error at timestep 1641 is tensor(6.9898e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1642. State = [[-0.23375355  0.26738822]]. Action = [[0.21961933 0.11231005 0.14213061 0.66360986]]. Reward = [0.]
Curr episode timestep = 740
Scene graph at timestep 1642 is [True, False, False, False, False, True]
State prediction error at timestep 1642 is tensor(1.9106e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1642 of -1
Current timestep = 1643. State = [[-0.23375355  0.26738822]]. Action = [[-0.17919281 -0.12269045 -0.2003418  -0.07022375]]. Reward = [0.]
Curr episode timestep = 741
Scene graph at timestep 1643 is [True, False, False, False, False, True]
State prediction error at timestep 1643 is tensor(3.9882e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1644. State = [[-0.23363245  0.2673053 ]]. Action = [[-0.2457395   0.19861206 -0.2186537   0.31438196]]. Reward = [0.]
Curr episode timestep = 742
Scene graph at timestep 1644 is [True, False, False, False, False, True]
State prediction error at timestep 1644 is tensor(1.9308e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1645. State = [[-0.23363245  0.2673053 ]]. Action = [[-0.03942445  0.24850875  0.10805982  0.40174317]]. Reward = [0.]
Curr episode timestep = 743
Scene graph at timestep 1645 is [True, False, False, False, False, True]
State prediction error at timestep 1645 is tensor(1.4969e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1646. State = [[-0.23367521  0.2673539 ]]. Action = [[ 0.03644156 -0.20893635 -0.0960848   0.8461468 ]]. Reward = [0.]
Curr episode timestep = 744
Scene graph at timestep 1646 is [True, False, False, False, False, True]
State prediction error at timestep 1646 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1646 of -1
Current timestep = 1647. State = [[-0.23367521  0.2673539 ]]. Action = [[ 0.24500927  0.23603797 -0.0097812   0.45908177]]. Reward = [0.]
Curr episode timestep = 745
Scene graph at timestep 1647 is [True, False, False, False, False, True]
State prediction error at timestep 1647 is tensor(4.0045e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1648. State = [[-0.23367521  0.2673539 ]]. Action = [[-0.22149417 -0.14380658 -0.16194978  0.04393506]]. Reward = [0.]
Curr episode timestep = 746
Scene graph at timestep 1648 is [True, False, False, False, False, True]
State prediction error at timestep 1648 is tensor(6.9330e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1649. State = [[-0.23367521  0.2673539 ]]. Action = [[ 0.21663141 -0.12424357  0.20561215 -0.656598  ]]. Reward = [0.]
Curr episode timestep = 747
Scene graph at timestep 1649 is [True, False, False, False, False, True]
State prediction error at timestep 1649 is tensor(3.2675e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1650. State = [[-0.23367521  0.2673539 ]]. Action = [[ 2.1234545e-01 -1.4083087e-04  6.7352295e-02  7.9584169e-01]]. Reward = [0.]
Curr episode timestep = 748
Scene graph at timestep 1650 is [True, False, False, False, False, True]
State prediction error at timestep 1650 is tensor(2.0369e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1650 of -1
Current timestep = 1651. State = [[-0.23367521  0.2673539 ]]. Action = [[-0.20404845  0.08782169 -0.0188906   0.32525492]]. Reward = [0.]
Curr episode timestep = 749
Scene graph at timestep 1651 is [True, False, False, False, False, True]
State prediction error at timestep 1651 is tensor(5.1225e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1652. State = [[-0.23364694  0.26728985]]. Action = [[-0.11782745 -0.07447711 -0.02436003 -0.9494308 ]]. Reward = [0.]
Curr episode timestep = 750
Scene graph at timestep 1652 is [True, False, False, False, False, True]
State prediction error at timestep 1652 is tensor(3.4202e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1653. State = [[-0.23358421  0.2670044 ]]. Action = [[-0.14422753 -0.03494716 -0.21189    -0.55532956]]. Reward = [0.]
Curr episode timestep = 751
Scene graph at timestep 1653 is [True, False, False, False, False, True]
State prediction error at timestep 1653 is tensor(9.2534e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1654. State = [[-0.23359513  0.26690748]]. Action = [[-0.1101062  -0.21485175 -0.16596383  0.2802484 ]]. Reward = [0.]
Curr episode timestep = 752
Scene graph at timestep 1654 is [True, False, False, False, False, True]
State prediction error at timestep 1654 is tensor(5.4733e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1655. State = [[-0.23353596  0.26653624]]. Action = [[-0.09724082 -0.11247629 -0.15959443 -0.1352976 ]]. Reward = [0.]
Curr episode timestep = 753
Scene graph at timestep 1655 is [True, False, False, False, False, True]
State prediction error at timestep 1655 is tensor(1.1349e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1655 of -1
Current timestep = 1656. State = [[-0.23355588  0.2656629 ]]. Action = [[0.19222185 0.07670736 0.12665665 0.08287609]]. Reward = [0.]
Curr episode timestep = 754
Scene graph at timestep 1656 is [True, False, False, False, False, True]
State prediction error at timestep 1656 is tensor(2.6001e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1657. State = [[-0.23388404  0.2640598 ]]. Action = [[-0.0893133   0.0198037  -0.07599756  0.23495626]]. Reward = [0.]
Curr episode timestep = 755
Scene graph at timestep 1657 is [True, False, False, False, False, True]
State prediction error at timestep 1657 is tensor(3.5113e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1657 of -1
Current timestep = 1658. State = [[-0.23446189  0.26401675]]. Action = [[-0.21676862 -0.01388061  0.1718908  -0.48298222]]. Reward = [0.]
Curr episode timestep = 756
Scene graph at timestep 1658 is [True, False, False, False, False, True]
State prediction error at timestep 1658 is tensor(4.0314e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1659. State = [[-0.23499453  0.26396808]]. Action = [[-0.18750216 -0.24161114 -0.15702595  0.5928353 ]]. Reward = [0.]
Curr episode timestep = 757
Scene graph at timestep 1659 is [True, False, False, False, False, True]
State prediction error at timestep 1659 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1660. State = [[-0.23533623  0.2639713 ]]. Action = [[ 0.06043795  0.15703931  0.17940468 -0.52715904]]. Reward = [0.]
Curr episode timestep = 758
Scene graph at timestep 1660 is [True, False, False, False, False, True]
State prediction error at timestep 1660 is tensor(8.5930e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1660 of -1
Current timestep = 1661. State = [[-0.23592679  0.2635927 ]]. Action = [[-0.17673314 -0.01621893  0.14901102 -0.63710314]]. Reward = [0.]
Curr episode timestep = 759
Scene graph at timestep 1661 is [True, False, False, False, False, True]
State prediction error at timestep 1661 is tensor(4.6210e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1662. State = [[-0.23679219  0.26309022]]. Action = [[ 0.03912467 -0.12995625 -0.09224862  0.72857475]]. Reward = [0.]
Curr episode timestep = 760
Scene graph at timestep 1662 is [True, False, False, False, False, True]
State prediction error at timestep 1662 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1662 of -1
Current timestep = 1663. State = [[-0.23586594  0.2593185 ]]. Action = [[ 0.0323247 -0.1182714  0.1239031  0.5209174]]. Reward = [0.]
Curr episode timestep = 761
Scene graph at timestep 1663 is [True, False, False, False, False, True]
State prediction error at timestep 1663 is tensor(3.5783e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1663 of -1
Current timestep = 1664. State = [[-0.23510776  0.25755385]]. Action = [[-0.06740168 -0.21892187  0.19282839 -0.6366888 ]]. Reward = [0.]
Curr episode timestep = 762
Scene graph at timestep 1664 is [True, False, False, False, False, True]
State prediction error at timestep 1664 is tensor(4.7116e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1665. State = [[-0.23451646  0.25539616]]. Action = [[-0.22756714 -0.04149285  0.04776973 -0.6167557 ]]. Reward = [0.]
Curr episode timestep = 763
Scene graph at timestep 1665 is [True, False, False, False, False, True]
State prediction error at timestep 1665 is tensor(4.4351e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1666. State = [[-0.23438263  0.25439286]]. Action = [[0.22609583 0.09953678 0.00383875 0.77332664]]. Reward = [0.]
Curr episode timestep = 764
Scene graph at timestep 1666 is [True, False, False, False, False, True]
State prediction error at timestep 1666 is tensor(5.2705e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1667. State = [[-0.23431839  0.2533826 ]]. Action = [[-0.1795107   0.06843439  0.14105618 -0.5887469 ]]. Reward = [0.]
Curr episode timestep = 765
Scene graph at timestep 1667 is [True, False, False, False, False, True]
State prediction error at timestep 1667 is tensor(2.6827e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1667 of -1
Current timestep = 1668. State = [[-0.23389629  0.25195116]]. Action = [[-0.14367798 -0.20899582  0.20753014 -0.81540287]]. Reward = [0.]
Curr episode timestep = 766
Scene graph at timestep 1668 is [True, False, False, False, False, True]
State prediction error at timestep 1668 is tensor(5.6052e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1669. State = [[-0.23361056  0.25086012]]. Action = [[-0.18800434  0.1266523   0.03628179  0.39172697]]. Reward = [0.]
Curr episode timestep = 767
Scene graph at timestep 1669 is [True, False, False, False, False, True]
State prediction error at timestep 1669 is tensor(2.7083e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1670. State = [[-0.2336486   0.25023195]]. Action = [[ 0.11949694 -0.19908206  0.09468439  0.70678186]]. Reward = [0.]
Curr episode timestep = 768
Scene graph at timestep 1670 is [True, False, False, False, False, True]
State prediction error at timestep 1670 is tensor(4.3078e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1671. State = [[-0.23349537  0.2494218 ]]. Action = [[ 0.07099429  0.14264232 -0.11641054 -0.6246364 ]]. Reward = [0.]
Curr episode timestep = 769
Scene graph at timestep 1671 is [True, False, False, False, False, True]
State prediction error at timestep 1671 is tensor(1.5687e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1671 of -1
Current timestep = 1672. State = [[-0.23300894  0.24862519]]. Action = [[-0.04343951 -0.18735681 -0.11325985  0.3793528 ]]. Reward = [0.]
Curr episode timestep = 770
Scene graph at timestep 1672 is [True, False, False, False, False, True]
State prediction error at timestep 1672 is tensor(2.0483e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1673. State = [[-0.23307903  0.24818116]]. Action = [[-0.22511083  0.19683433 -0.19996837  0.7908908 ]]. Reward = [0.]
Curr episode timestep = 771
Scene graph at timestep 1673 is [True, False, False, False, False, True]
State prediction error at timestep 1673 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1674. State = [[-0.23310435  0.24771704]]. Action = [[-0.17970216 -0.20785517  0.08444282  0.21488154]]. Reward = [0.]
Curr episode timestep = 772
Scene graph at timestep 1674 is [True, False, False, False, False, True]
State prediction error at timestep 1674 is tensor(1.3515e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1675. State = [[-0.2326597   0.24629271]]. Action = [[0.02344313 0.11124018 0.18133342 0.04813659]]. Reward = [0.]
Curr episode timestep = 773
Scene graph at timestep 1675 is [True, False, False, False, False, True]
State prediction error at timestep 1675 is tensor(2.2588e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1675 of -1
Current timestep = 1676. State = [[-0.23310705  0.24676481]]. Action = [[ 0.08990479  0.14458013 -0.12463662 -0.3067732 ]]. Reward = [0.]
Curr episode timestep = 774
Scene graph at timestep 1676 is [True, False, False, False, False, True]
State prediction error at timestep 1676 is tensor(1.0063e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1677. State = [[-0.23350012  0.24743757]]. Action = [[-0.13443819  0.13175523 -0.17892216 -0.58635294]]. Reward = [0.]
Curr episode timestep = 775
Scene graph at timestep 1677 is [True, False, False, False, False, True]
State prediction error at timestep 1677 is tensor(8.0468e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1677 of -1
Current timestep = 1678. State = [[-0.23359656  0.24751958]]. Action = [[-0.22428815 -0.10608821  0.19925356 -0.7706707 ]]. Reward = [0.]
Curr episode timestep = 776
Scene graph at timestep 1678 is [True, False, False, False, False, True]
State prediction error at timestep 1678 is tensor(8.2863e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1679. State = [[-0.23411453  0.24807072]]. Action = [[-0.20191608 -0.04520978  0.13337225  0.4480331 ]]. Reward = [0.]
Curr episode timestep = 777
Scene graph at timestep 1679 is [True, False, False, False, False, True]
State prediction error at timestep 1679 is tensor(6.6852e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1680. State = [[-0.23421107  0.24815258]]. Action = [[ 0.19277799  0.08300802  0.22349757 -0.49059546]]. Reward = [0.]
Curr episode timestep = 778
Scene graph at timestep 1680 is [True, False, False, False, False, True]
State prediction error at timestep 1680 is tensor(5.6343e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1681. State = [[-0.23440894  0.24840672]]. Action = [[-0.21285085  0.0108555   0.2162984   0.3654971 ]]. Reward = [0.]
Curr episode timestep = 779
Scene graph at timestep 1681 is [True, False, False, False, False, True]
State prediction error at timestep 1681 is tensor(4.9925e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1682. State = [[-0.23463736  0.24863805]]. Action = [[-0.03230405 -0.10407707 -0.15887073  0.12301528]]. Reward = [0.]
Curr episode timestep = 780
Scene graph at timestep 1682 is [True, False, False, False, False, True]
State prediction error at timestep 1682 is tensor(2.2117e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1682 of -1
Current timestep = 1683. State = [[-0.23455068  0.24810228]]. Action = [[-0.20558396  0.0541485   0.2210744   0.93407965]]. Reward = [0.]
Curr episode timestep = 781
Scene graph at timestep 1683 is [True, False, False, False, False, True]
State prediction error at timestep 1683 is tensor(3.5508e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1684. State = [[-0.23435293  0.24759643]]. Action = [[ 0.1343824   0.06324434 -0.06414109  0.28793824]]. Reward = [0.]
Curr episode timestep = 782
Scene graph at timestep 1684 is [True, False, False, False, False, True]
State prediction error at timestep 1684 is tensor(3.6807e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1685. State = [[-0.23414679  0.24730153]]. Action = [[ 0.24385864 -0.17458884  0.09347892 -0.8815197 ]]. Reward = [0.]
Curr episode timestep = 783
Scene graph at timestep 1685 is [True, False, False, False, False, True]
State prediction error at timestep 1685 is tensor(5.7982e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1685 of -1
Current timestep = 1686. State = [[-0.23428969  0.24710438]]. Action = [[ 0.15536034 -0.22461629  0.0469057  -0.73914057]]. Reward = [0.]
Curr episode timestep = 784
Scene graph at timestep 1686 is [True, False, False, False, False, True]
State prediction error at timestep 1686 is tensor(3.2449e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1687. State = [[-0.2342353  0.2468903]]. Action = [[-0.20011134  0.04320478  0.22851235  0.22027636]]. Reward = [0.]
Curr episode timestep = 785
Scene graph at timestep 1687 is [True, False, False, False, False, True]
State prediction error at timestep 1687 is tensor(4.7898e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1688. State = [[-0.23420687  0.24652442]]. Action = [[-0.01004732  0.06705421  0.00423947 -0.4018674 ]]. Reward = [0.]
Curr episode timestep = 786
Scene graph at timestep 1688 is [True, False, False, False, False, True]
State prediction error at timestep 1688 is tensor(1.9978e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1688 of -1
Current timestep = 1689. State = [[-0.23437408  0.24673164]]. Action = [[0.05810839 0.24348927 0.19592625 0.61332345]]. Reward = [0.]
Curr episode timestep = 787
Scene graph at timestep 1689 is [True, False, False, False, False, True]
State prediction error at timestep 1689 is tensor(3.7114e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1690. State = [[-0.2347984  0.247171 ]]. Action = [[-0.07351425  0.02410573 -0.18711048  0.49456418]]. Reward = [0.]
Curr episode timestep = 788
Scene graph at timestep 1690 is [True, False, False, False, False, True]
State prediction error at timestep 1690 is tensor(1.9914e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1690 of -1
Current timestep = 1691. State = [[-0.23535311  0.24789003]]. Action = [[ 0.05112582 -0.19629696 -0.0792523   0.49182582]]. Reward = [0.]
Curr episode timestep = 789
Scene graph at timestep 1691 is [True, False, False, False, False, True]
State prediction error at timestep 1691 is tensor(3.1073e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1692. State = [[-0.2369127   0.24932046]]. Action = [[-0.10077414  0.02063987 -0.11151829  0.6130245 ]]. Reward = [0.]
Curr episode timestep = 790
Scene graph at timestep 1692 is [True, False, False, False, False, True]
State prediction error at timestep 1692 is tensor(7.5757e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1692 of -1
Current timestep = 1693. State = [[-0.24069555  0.2513573 ]]. Action = [[-0.01892854  0.03894094 -0.17838496  0.25480378]]. Reward = [0.]
Curr episode timestep = 791
Scene graph at timestep 1693 is [True, False, False, False, False, True]
State prediction error at timestep 1693 is tensor(2.5108e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1693 of -1
Current timestep = 1694. State = [[-0.24427418  0.25362143]]. Action = [[-0.12037636  0.03386319 -0.03848422  0.1916138 ]]. Reward = [0.]
Curr episode timestep = 792
Scene graph at timestep 1694 is [True, False, False, False, False, True]
State prediction error at timestep 1694 is tensor(5.8306e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1694 of -1
Current timestep = 1695. State = [[-0.24541259  0.25421503]]. Action = [[ 0.20802537  0.22326082 -0.18230526  0.5977516 ]]. Reward = [0.]
Curr episode timestep = 793
Scene graph at timestep 1695 is [True, False, False, False, False, True]
State prediction error at timestep 1695 is tensor(1.9763e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1696. State = [[-0.24725382  0.2554485 ]]. Action = [[-0.16031799  0.22167209  0.24861372 -0.82415646]]. Reward = [0.]
Curr episode timestep = 794
Scene graph at timestep 1696 is [True, False, False, False, False, True]
State prediction error at timestep 1696 is tensor(5.0183e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1697. State = [[-0.24845216  0.25604898]]. Action = [[-0.06142962  0.15846902 -0.00789075  0.8583994 ]]. Reward = [0.]
Curr episode timestep = 795
Scene graph at timestep 1697 is [True, False, False, False, False, True]
State prediction error at timestep 1697 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1698. State = [[-0.24937247  0.25628018]]. Action = [[-0.07494739  0.15853918 -0.06915066  0.29719353]]. Reward = [0.]
Curr episode timestep = 796
Scene graph at timestep 1698 is [True, False, False, False, False, True]
State prediction error at timestep 1698 is tensor(6.2358e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1699. State = [[-0.2503341   0.25670952]]. Action = [[-0.06040032  0.24371004 -0.00633727 -0.637495  ]]. Reward = [0.]
Curr episode timestep = 797
Scene graph at timestep 1699 is [True, False, False, False, False, True]
State prediction error at timestep 1699 is tensor(1.4248e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1699 of -1
Current timestep = 1700. State = [[-0.25201693  0.25739816]]. Action = [[-0.23536946  0.19536692 -0.06550224  0.05798042]]. Reward = [0.]
Curr episode timestep = 798
Scene graph at timestep 1700 is [True, False, False, False, False, True]
State prediction error at timestep 1700 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1701. State = [[-0.25281525  0.2576078 ]]. Action = [[ 0.15093502 -0.14656696  0.18947142  0.5282552 ]]. Reward = [0.]
Curr episode timestep = 799
Scene graph at timestep 1701 is [True, False, False, False, False, True]
State prediction error at timestep 1701 is tensor(4.3873e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1702. State = [[-0.25366482  0.25796196]]. Action = [[-0.02889293 -0.18557097  0.21378785 -0.06267989]]. Reward = [0.]
Curr episode timestep = 800
Scene graph at timestep 1702 is [True, False, False, False, False, True]
State prediction error at timestep 1702 is tensor(2.4682e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1703. State = [[-0.2541106  0.2580747]]. Action = [[-0.12116903 -0.21930917 -0.06472325 -0.6140233 ]]. Reward = [0.]
Curr episode timestep = 801
Scene graph at timestep 1703 is [True, False, False, False, False, True]
State prediction error at timestep 1703 is tensor(1.1791e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1703 of -1
Current timestep = 1704. State = [[-0.2547747  0.2582484]]. Action = [[ 0.19802183  0.23829171 -0.19095525 -0.1163559 ]]. Reward = [0.]
Curr episode timestep = 802
Scene graph at timestep 1704 is [True, False, False, False, False, True]
State prediction error at timestep 1704 is tensor(1.2507e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1705. State = [[-0.25527415  0.2583733 ]]. Action = [[-0.04053867  0.24809146 -0.1896198  -0.07481992]]. Reward = [0.]
Curr episode timestep = 803
Scene graph at timestep 1705 is [True, False, False, False, False, True]
State prediction error at timestep 1705 is tensor(7.8681e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1706. State = [[-0.25634995  0.25871572]]. Action = [[-0.03540535 -0.00629465 -0.07848379  0.9593148 ]]. Reward = [0.]
Curr episode timestep = 804
Scene graph at timestep 1706 is [True, False, False, False, False, True]
State prediction error at timestep 1706 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1706 of -1
Current timestep = 1707. State = [[-0.2567153   0.25868863]]. Action = [[0.16579396 0.09123066 0.07801899 0.82570505]]. Reward = [0.]
Curr episode timestep = 805
Scene graph at timestep 1707 is [True, False, False, False, False, True]
State prediction error at timestep 1707 is tensor(1.8663e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1708. State = [[-0.25705516  0.2587646 ]]. Action = [[0.04624116 0.24791801 0.1789963  0.3409562 ]]. Reward = [0.]
Curr episode timestep = 806
Scene graph at timestep 1708 is [True, False, False, False, False, True]
State prediction error at timestep 1708 is tensor(1.2364e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1709. State = [[-0.25715712  0.25880826]]. Action = [[ 0.16591394 -0.00170809 -0.1141021   0.73677886]]. Reward = [0.]
Curr episode timestep = 807
Scene graph at timestep 1709 is [True, False, False, False, False, True]
State prediction error at timestep 1709 is tensor(3.7555e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1709 of -1
Current timestep = 1710. State = [[-0.2583758   0.25900608]]. Action = [[-0.02773556  0.09428626  0.13604021  0.41248918]]. Reward = [0.]
Curr episode timestep = 808
Scene graph at timestep 1710 is [True, False, False, False, False, True]
State prediction error at timestep 1710 is tensor(5.3768e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1711. State = [[-0.25917566  0.25959098]]. Action = [[0.06084189 0.15460652 0.2486057  0.7016771 ]]. Reward = [0.]
Curr episode timestep = 809
Scene graph at timestep 1711 is [True, False, False, False, False, True]
State prediction error at timestep 1711 is tensor(4.6293e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1712. State = [[-0.25976175  0.26052344]]. Action = [[ 0.20688483 -0.23984635 -0.12537417  0.2510078 ]]. Reward = [0.]
Curr episode timestep = 810
Scene graph at timestep 1712 is [True, False, False, False, False, True]
State prediction error at timestep 1712 is tensor(4.7331e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1713. State = [[-0.26013532  0.26109105]]. Action = [[0.13520718 0.21312445 0.05353057 0.179438  ]]. Reward = [0.]
Curr episode timestep = 811
Scene graph at timestep 1713 is [True, False, False, False, False, True]
State prediction error at timestep 1713 is tensor(1.7457e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1714. State = [[-0.26169872  0.26255316]]. Action = [[ 0.11702242  0.00386816 -0.07890143  0.1377511 ]]. Reward = [0.]
Curr episode timestep = 812
Scene graph at timestep 1714 is [True, False, False, False, False, True]
State prediction error at timestep 1714 is tensor(1.3562e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1714 of -1
Current timestep = 1715. State = [[-0.2613475  0.2628944]]. Action = [[-0.11701536 -0.00541596  0.14539397  0.3317926 ]]. Reward = [0.]
Curr episode timestep = 813
Scene graph at timestep 1715 is [True, False, False, False, False, True]
State prediction error at timestep 1715 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1715 of -1
Current timestep = 1716. State = [[-0.26161927  0.26325282]]. Action = [[-0.15749611 -0.00098047 -0.03981191 -0.11565691]]. Reward = [0.]
Curr episode timestep = 814
Scene graph at timestep 1716 is [True, False, False, False, False, True]
State prediction error at timestep 1716 is tensor(5.3624e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1717. State = [[-0.26176882  0.2634668 ]]. Action = [[-0.13972145 -0.2084696   0.16106218 -0.52698505]]. Reward = [0.]
Curr episode timestep = 815
Scene graph at timestep 1717 is [True, False, False, False, False, True]
State prediction error at timestep 1717 is tensor(1.6752e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1718. State = [[-0.26226565  0.26387048]]. Action = [[-0.01543061 -0.08577803 -0.02710654 -0.09274071]]. Reward = [0.]
Curr episode timestep = 816
Scene graph at timestep 1718 is [True, False, False, False, False, True]
State prediction error at timestep 1718 is tensor(1.8739e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1718 of -1
Current timestep = 1719. State = [[-0.26240864  0.26316658]]. Action = [[ 0.15022737 -0.15437077  0.12898737 -0.5895163 ]]. Reward = [0.]
Curr episode timestep = 817
Scene graph at timestep 1719 is [True, False, False, False, False, True]
State prediction error at timestep 1719 is tensor(1.7662e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1720. State = [[-0.2624255  0.2629123]]. Action = [[ 0.120314    0.19451815  0.1919077  -0.26981544]]. Reward = [0.]
Curr episode timestep = 818
Scene graph at timestep 1720 is [True, False, False, False, False, True]
State prediction error at timestep 1720 is tensor(1.1704e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1721. State = [[-0.26236293  0.26273486]]. Action = [[ 0.11346009 -0.20572817 -0.04043701 -0.4017166 ]]. Reward = [0.]
Curr episode timestep = 819
Scene graph at timestep 1721 is [True, False, False, False, False, True]
State prediction error at timestep 1721 is tensor(5.1592e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1721 of -1
Current timestep = 1722. State = [[-0.26281595  0.26191938]]. Action = [[ 0.02947956  0.07599166 -0.01684192  0.5678642 ]]. Reward = [0.]
Curr episode timestep = 820
Scene graph at timestep 1722 is [True, False, False, False, False, True]
State prediction error at timestep 1722 is tensor(2.5679e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1723. State = [[-0.2628793  0.2621059]]. Action = [[ 0.16507077  0.04236668  0.12731016 -0.36331636]]. Reward = [0.]
Curr episode timestep = 821
Scene graph at timestep 1723 is [True, False, False, False, False, True]
State prediction error at timestep 1723 is tensor(1.0717e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1723 of -1
Current timestep = 1724. State = [[-0.26303     0.26232478]]. Action = [[-0.12836426  0.1717366   0.20649654  0.27756608]]. Reward = [0.]
Curr episode timestep = 822
Scene graph at timestep 1724 is [True, False, False, False, False, True]
State prediction error at timestep 1724 is tensor(7.1517e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1725. State = [[-0.26339942  0.262729  ]]. Action = [[-0.05310647  0.07506573 -0.15257032 -0.6682683 ]]. Reward = [0.]
Curr episode timestep = 823
Scene graph at timestep 1725 is [True, False, False, False, False, True]
State prediction error at timestep 1725 is tensor(8.7023e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1726. State = [[-0.26406962  0.26331845]]. Action = [[ 0.22457904  0.17204261  0.17358848 -0.6563736 ]]. Reward = [0.]
Curr episode timestep = 824
Scene graph at timestep 1726 is [True, False, False, False, False, True]
State prediction error at timestep 1726 is tensor(5.2542e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1727. State = [[-0.264625    0.26401484]]. Action = [[-0.23854768 -0.22308993  0.15180886  0.38321352]]. Reward = [0.]
Curr episode timestep = 825
Scene graph at timestep 1727 is [True, False, False, False, False, True]
State prediction error at timestep 1727 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1728. State = [[-0.26501477  0.2644868 ]]. Action = [[ 0.12062776  0.14364547 -0.10690914 -0.07959598]]. Reward = [0.]
Curr episode timestep = 826
Scene graph at timestep 1728 is [True, False, False, False, False, True]
State prediction error at timestep 1728 is tensor(4.2599e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1729. State = [[-0.26633346  0.26584375]]. Action = [[ 0.06299958 -0.01541306 -0.04912601 -0.68194836]]. Reward = [0.]
Curr episode timestep = 827
Scene graph at timestep 1729 is [True, False, False, False, False, True]
State prediction error at timestep 1729 is tensor(1.7113e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1729 of -1
Current timestep = 1730. State = [[-0.26625386  0.26588193]]. Action = [[-0.22805491  0.07099122  0.20975646  0.02335501]]. Reward = [0.]
Curr episode timestep = 828
Scene graph at timestep 1730 is [True, False, False, False, False, True]
State prediction error at timestep 1730 is tensor(8.8985e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1731. State = [[-0.2661147  0.2659093]]. Action = [[-0.20152329  0.14516523  0.11070013 -0.4942329 ]]. Reward = [0.]
Curr episode timestep = 829
Scene graph at timestep 1731 is [True, False, False, False, False, True]
State prediction error at timestep 1731 is tensor(7.3321e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1731 of -1
Current timestep = 1732. State = [[-0.26619646  0.26592058]]. Action = [[ 0.00828418 -0.15264748 -0.05503295  0.38193846]]. Reward = [0.]
Curr episode timestep = 830
Scene graph at timestep 1732 is [True, False, False, False, False, True]
State prediction error at timestep 1732 is tensor(8.1581e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1733. State = [[-0.26619163  0.26600572]]. Action = [[ 0.11384514  0.03923389 -0.08504808  0.17094386]]. Reward = [0.]
Curr episode timestep = 831
Scene graph at timestep 1733 is [True, False, False, False, False, True]
State prediction error at timestep 1733 is tensor(3.5651e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1734. State = [[-0.266028    0.26626408]]. Action = [[-0.21094163 -0.11782722  0.01905197 -0.7683042 ]]. Reward = [0.]
Curr episode timestep = 832
Scene graph at timestep 1734 is [True, False, False, False, False, True]
State prediction error at timestep 1734 is tensor(1.8090e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1735. State = [[-0.26561874  0.2668191 ]]. Action = [[ 0.06589267 -0.02913567 -0.07565981  0.62224984]]. Reward = [0.]
Curr episode timestep = 833
Scene graph at timestep 1735 is [True, False, False, False, False, True]
State prediction error at timestep 1735 is tensor(6.3671e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1736. State = [[-0.2642867  0.2666693]]. Action = [[-0.11057115  0.11940941  0.24445838 -0.25673681]]. Reward = [0.]
Curr episode timestep = 834
Scene graph at timestep 1736 is [True, False, False, False, False, True]
State prediction error at timestep 1736 is tensor(2.3496e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1737. State = [[-0.26591763  0.26863888]]. Action = [[-0.0154655   0.13285145  0.22867894  0.0222826 ]]. Reward = [0.]
Curr episode timestep = 835
Scene graph at timestep 1737 is [True, False, False, False, False, True]
State prediction error at timestep 1737 is tensor(5.2263e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1738. State = [[-0.26829553  0.2724834 ]]. Action = [[ 0.08207166  0.07743132 -0.07101396  0.6089845 ]]. Reward = [0.]
Curr episode timestep = 836
Scene graph at timestep 1738 is [True, False, False, False, False, True]
State prediction error at timestep 1738 is tensor(5.8680e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1738 of -1
Current timestep = 1739. State = [[-0.26842508  0.2736099 ]]. Action = [[-0.08884314  0.19429308  0.18576723  0.76114774]]. Reward = [0.]
Curr episode timestep = 837
Scene graph at timestep 1739 is [True, False, False, False, False, True]
State prediction error at timestep 1739 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1740. State = [[-0.26834324  0.27448347]]. Action = [[ 0.02923387  0.22167307 -0.00658074  0.13646662]]. Reward = [0.]
Curr episode timestep = 838
Scene graph at timestep 1740 is [True, False, False, False, False, True]
State prediction error at timestep 1740 is tensor(2.5771e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1741. State = [[-0.2683979   0.27755237]]. Action = [[-0.12761249  0.08729291 -0.16718026  0.773355  ]]. Reward = [0.]
Curr episode timestep = 839
Scene graph at timestep 1741 is [True, False, False, False, False, True]
State prediction error at timestep 1741 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1741 of -1
Current timestep = 1742. State = [[-0.26958516  0.2788281 ]]. Action = [[ 0.0097748  -0.21900187  0.17901778  0.38880122]]. Reward = [0.]
Curr episode timestep = 840
Scene graph at timestep 1742 is [True, False, False, False, False, True]
State prediction error at timestep 1742 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1743. State = [[-0.2705076  0.2799179]]. Action = [[0.01244789 0.20199415 0.00768203 0.03060865]]. Reward = [0.]
Curr episode timestep = 841
Scene graph at timestep 1743 is [True, False, False, False, False, True]
State prediction error at timestep 1743 is tensor(4.3630e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1744. State = [[-0.27130777  0.28100273]]. Action = [[-0.1867272  -0.1420314   0.24259347  0.2156533 ]]. Reward = [0.]
Curr episode timestep = 842
Scene graph at timestep 1744 is [True, False, False, False, False, True]
State prediction error at timestep 1744 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1744 of -1
Current timestep = 1745. State = [[-0.2719874   0.28194433]]. Action = [[-0.1412563   0.16098088  0.0334115  -0.33501637]]. Reward = [0.]
Curr episode timestep = 843
Scene graph at timestep 1745 is [True, False, False, False, False, True]
State prediction error at timestep 1745 is tensor(1.7193e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1746. State = [[-0.2726208  0.28298  ]]. Action = [[-0.12466587  0.22176126  0.18066785  0.14073527]]. Reward = [0.]
Curr episode timestep = 844
Scene graph at timestep 1746 is [True, False, False, False, False, True]
State prediction error at timestep 1746 is tensor(9.1825e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1747. State = [[-0.2733547   0.28391472]]. Action = [[-0.0738402   0.23867643  0.23166853 -0.24695677]]. Reward = [0.]
Curr episode timestep = 845
Scene graph at timestep 1747 is [True, False, False, False, False, True]
State prediction error at timestep 1747 is tensor(3.0749e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1748. State = [[-0.27466044  0.28544495]]. Action = [[ 0.0495804  -0.08877429  0.17952931  0.73581886]]. Reward = [0.]
Curr episode timestep = 846
Scene graph at timestep 1748 is [True, False, False, False, False, True]
State prediction error at timestep 1748 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1748 of -1
Current timestep = 1749. State = [[-0.2744544  0.2851593]]. Action = [[-0.11748634  0.23064297  0.15174764 -0.38500893]]. Reward = [0.]
Curr episode timestep = 847
Scene graph at timestep 1749 is [True, False, False, False, False, True]
State prediction error at timestep 1749 is tensor(1.9733e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1750. State = [[-0.2743046  0.2851444]]. Action = [[ 0.0644114   0.09998193 -0.15942816  0.3552611 ]]. Reward = [0.]
Curr episode timestep = 848
Scene graph at timestep 1750 is [True, False, False, False, False, True]
State prediction error at timestep 1750 is tensor(7.1641e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1750 of -1
Current timestep = 1751. State = [[-0.27387965  0.28550506]]. Action = [[ 0.09692651 -0.1858501  -0.24403861 -0.28220123]]. Reward = [0.]
Curr episode timestep = 849
Scene graph at timestep 1751 is [True, False, False, False, False, True]
State prediction error at timestep 1751 is tensor(1.1936e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1752. State = [[-0.27339312  0.28581637]]. Action = [[0.08331186 0.15592867 0.18213487 0.09648132]]. Reward = [0.]
Curr episode timestep = 850
Scene graph at timestep 1752 is [True, False, False, False, False, True]
State prediction error at timestep 1752 is tensor(3.9023e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1753. State = [[-0.2733054   0.28594673]]. Action = [[ 0.1673696   0.18418229  0.12605333 -0.8714662 ]]. Reward = [0.]
Curr episode timestep = 851
Scene graph at timestep 1753 is [True, False, False, False, False, True]
State prediction error at timestep 1753 is tensor(6.6712e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1753 of -1
Current timestep = 1754. State = [[-0.27291146  0.28646114]]. Action = [[ 0.14031541  0.0107041  -0.21748656 -0.21182132]]. Reward = [0.]
Curr episode timestep = 852
Scene graph at timestep 1754 is [True, False, False, False, False, True]
State prediction error at timestep 1754 is tensor(3.7234e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1755. State = [[-0.27262884  0.28656006]]. Action = [[ 0.22219038  0.00171363 -0.00935495 -0.89325327]]. Reward = [0.]
Curr episode timestep = 853
Scene graph at timestep 1755 is [True, False, False, False, False, True]
State prediction error at timestep 1755 is tensor(7.4156e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1756. State = [[-0.27254465  0.28661105]]. Action = [[ 0.19006023  0.17202592  0.03594708 -0.36495638]]. Reward = [0.]
Curr episode timestep = 854
Scene graph at timestep 1756 is [True, False, False, False, False, True]
State prediction error at timestep 1756 is tensor(1.2371e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1756 of -1
Current timestep = 1757. State = [[-0.27224806  0.28694648]]. Action = [[ 0.21195233 -0.21280156 -0.06447579 -0.18706399]]. Reward = [0.]
Curr episode timestep = 855
Scene graph at timestep 1757 is [True, False, False, False, False, True]
State prediction error at timestep 1757 is tensor(1.0373e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1758. State = [[-0.27214274  0.28707132]]. Action = [[ 0.03048509  0.03011703  0.09521788 -0.8559623 ]]. Reward = [0.]
Curr episode timestep = 856
Scene graph at timestep 1758 is [True, False, False, False, False, True]
State prediction error at timestep 1758 is tensor(1.0651e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1759. State = [[-0.2720471   0.28732166]]. Action = [[-0.04082452  0.13832796 -0.05362618 -0.6790236 ]]. Reward = [0.]
Curr episode timestep = 857
Scene graph at timestep 1759 is [True, False, False, False, False, True]
State prediction error at timestep 1759 is tensor(9.1861e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1760. State = [[-0.27118698  0.28795347]]. Action = [[ 0.03681073  0.01489636  0.24592102 -0.64458245]]. Reward = [0.]
Curr episode timestep = 858
Scene graph at timestep 1760 is [True, False, False, False, False, True]
State prediction error at timestep 1760 is tensor(1.9372e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1761. State = [[-0.27086562  0.28815687]]. Action = [[-0.23705429 -0.08454588  0.01722988  0.8642677 ]]. Reward = [0.]
Curr episode timestep = 859
Scene graph at timestep 1761 is [True, False, False, False, False, True]
State prediction error at timestep 1761 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1762. State = [[-0.27077925  0.28821433]]. Action = [[-0.20544307  0.20775485  0.2454266   0.3289566 ]]. Reward = [0.]
Curr episode timestep = 860
Scene graph at timestep 1762 is [True, False, False, False, False, True]
State prediction error at timestep 1762 is tensor(5.3540e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1763. State = [[-0.27034572  0.28850648]]. Action = [[ 0.16913664  0.05302092 -0.10325415  0.27299035]]. Reward = [0.]
Curr episode timestep = 861
Scene graph at timestep 1763 is [True, False, False, False, False, True]
State prediction error at timestep 1763 is tensor(1.4014e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1764. State = [[-0.26974073  0.28895918]]. Action = [[ 0.22631884  0.01653078 -0.17278033 -0.8050364 ]]. Reward = [0.]
Curr episode timestep = 862
Scene graph at timestep 1764 is [True, False, False, False, False, True]
State prediction error at timestep 1764 is tensor(2.1586e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1765. State = [[-0.2694446   0.28913286]]. Action = [[-0.01742236  0.15563583 -0.10544276 -0.15079284]]. Reward = [0.]
Curr episode timestep = 863
Scene graph at timestep 1765 is [True, False, False, False, False, True]
State prediction error at timestep 1765 is tensor(2.7699e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1765 of -1
Current timestep = 1766. State = [[-0.26875246  0.28962895]]. Action = [[ 0.11537874  0.16732535 -0.12857966  0.5875876 ]]. Reward = [0.]
Curr episode timestep = 864
Scene graph at timestep 1766 is [True, False, False, False, False, True]
State prediction error at timestep 1766 is tensor(3.3806e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1767. State = [[-0.2671325  0.2907784]]. Action = [[ 0.03130853 -0.04108414  0.21205163  0.39421153]]. Reward = [0.]
Curr episode timestep = 865
Scene graph at timestep 1767 is [True, False, False, False, False, True]
State prediction error at timestep 1767 is tensor(5.0402e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1767 of -1
Current timestep = 1768. State = [[-0.26673907  0.29095542]]. Action = [[ 0.22680548 -0.20710647 -0.00532562  0.05075479]]. Reward = [0.]
Curr episode timestep = 866
Scene graph at timestep 1768 is [True, False, False, False, False, True]
State prediction error at timestep 1768 is tensor(4.0819e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1769. State = [[-0.26657033  0.29107442]]. Action = [[ 0.02275088  0.21302551 -0.08100301 -0.03732604]]. Reward = [0.]
Curr episode timestep = 867
Scene graph at timestep 1769 is [True, False, False, False, False, True]
State prediction error at timestep 1769 is tensor(6.2395e-08, grad_fn=<MseLossBackward0>)
Current timestep = 1770. State = [[-0.26623112  0.29123053]]. Action = [[ 0.14024323  0.05577913  0.23537046 -0.7374093 ]]. Reward = [0.]
Curr episode timestep = 868
Scene graph at timestep 1770 is [True, False, False, False, False, True]
State prediction error at timestep 1770 is tensor(7.7497e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1771. State = [[-0.26529467  0.29176503]]. Action = [[ 0.03034744 -0.03402758  0.15190813 -0.26466972]]. Reward = [0.]
Curr episode timestep = 869
Scene graph at timestep 1771 is [True, False, False, False, False, True]
State prediction error at timestep 1771 is tensor(3.1103e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1771 of -1
Current timestep = 1772. State = [[-0.26495022  0.29122066]]. Action = [[-0.22133197 -0.08065286 -0.14665268  0.2658919 ]]. Reward = [0.]
Curr episode timestep = 870
Scene graph at timestep 1772 is [True, False, False, False, False, True]
State prediction error at timestep 1772 is tensor(4.1910e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1772 of -1
Current timestep = 1773. State = [[-0.26441976  0.29106575]]. Action = [[-0.14853601  0.08510587  0.05549616 -0.94055086]]. Reward = [0.]
Curr episode timestep = 871
Scene graph at timestep 1773 is [True, False, False, False, False, True]
State prediction error at timestep 1773 is tensor(1.6635e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1774. State = [[-0.2643479   0.29097086]]. Action = [[-0.2440866   0.16683954 -0.04963292  0.07849014]]. Reward = [0.]
Curr episode timestep = 872
Scene graph at timestep 1774 is [True, False, False, False, False, True]
State prediction error at timestep 1774 is tensor(5.9870e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1775. State = [[-0.26424915  0.29072067]]. Action = [[ 0.04843369  0.18661967 -0.01210178  0.37885702]]. Reward = [0.]
Curr episode timestep = 873
Scene graph at timestep 1775 is [True, False, False, False, False, True]
State prediction error at timestep 1775 is tensor(1.9593e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1776. State = [[-0.26420006  0.290582  ]]. Action = [[0.2158097  0.22692692 0.00721931 0.06439793]]. Reward = [0.]
Curr episode timestep = 874
Scene graph at timestep 1776 is [True, False, False, False, False, True]
State prediction error at timestep 1776 is tensor(1.1848e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1777. State = [[-0.26378804  0.29064056]]. Action = [[-0.0845226  -0.0650273   0.04509819 -0.66978675]]. Reward = [0.]
Curr episode timestep = 875
Scene graph at timestep 1777 is [True, False, False, False, False, True]
State prediction error at timestep 1777 is tensor(3.1325e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1777 of -1
Current timestep = 1778. State = [[-0.26371378  0.2904337 ]]. Action = [[-0.12946942  0.21641022 -0.0605935   0.2612698 ]]. Reward = [0.]
Curr episode timestep = 876
Scene graph at timestep 1778 is [True, False, False, False, False, True]
State prediction error at timestep 1778 is tensor(6.3388e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1779. State = [[-0.26359725  0.29026538]]. Action = [[-0.15564811 -0.20201096 -0.22032623  0.88054407]]. Reward = [0.]
Curr episode timestep = 877
Scene graph at timestep 1779 is [True, False, False, False, False, True]
State prediction error at timestep 1779 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1779 of -1
Current timestep = 1780. State = [[-0.26330405  0.29008114]]. Action = [[ 0.0892536  -0.23571439  0.07299867  0.28939605]]. Reward = [0.]
Curr episode timestep = 878
Scene graph at timestep 1780 is [True, False, False, False, False, True]
State prediction error at timestep 1780 is tensor(5.2626e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1781. State = [[-0.26347628  0.29000407]]. Action = [[ 0.04389971  0.22152412 -0.17983864 -0.738948  ]]. Reward = [0.]
Curr episode timestep = 879
Scene graph at timestep 1781 is [True, False, False, False, False, True]
State prediction error at timestep 1781 is tensor(8.9630e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1782. State = [[-0.26346377  0.29002848]]. Action = [[ 0.16751248 -0.17971419  0.14320022  0.9292941 ]]. Reward = [0.]
Curr episode timestep = 880
Scene graph at timestep 1782 is [True, False, False, False, False, True]
State prediction error at timestep 1782 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1783. State = [[-0.26341614  0.29006255]]. Action = [[-0.02851543  0.02760005  0.17353809 -0.09421408]]. Reward = [0.]
Curr episode timestep = 881
Scene graph at timestep 1783 is [True, False, False, False, False, True]
State prediction error at timestep 1783 is tensor(1.9442e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1783 of -1
Current timestep = 1784. State = [[-0.26341614  0.29006255]]. Action = [[ 0.06473964  0.21341607 -0.03531829  0.38543642]]. Reward = [0.]
Curr episode timestep = 882
Scene graph at timestep 1784 is [True, False, False, False, False, True]
State prediction error at timestep 1784 is tensor(3.1663e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1785. State = [[-0.26347202  0.29016352]]. Action = [[ 0.06933481  0.10662052 -0.20310971 -0.44456482]]. Reward = [0.]
Curr episode timestep = 883
Scene graph at timestep 1785 is [True, False, False, False, False, True]
State prediction error at timestep 1785 is tensor(1.0492e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1785 of -1
Current timestep = 1786. State = [[-0.2635913   0.29066858]]. Action = [[ 0.062693    0.2450509  -0.18269311  0.00571311]]. Reward = [0.]
Curr episode timestep = 884
Scene graph at timestep 1786 is [True, False, False, False, False, True]
State prediction error at timestep 1786 is tensor(2.5733e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1786 of -1
Current timestep = 1787. State = [[-0.2638163   0.29098162]]. Action = [[-0.22649488 -0.23741265 -0.07216129 -0.9830519 ]]. Reward = [0.]
Curr episode timestep = 885
Scene graph at timestep 1787 is [True, False, False, False, False, True]
State prediction error at timestep 1787 is tensor(1.6816e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1788. State = [[-0.26385456  0.29092363]]. Action = [[-0.19752708 -0.07316905 -0.17460951 -0.28272617]]. Reward = [0.]
Curr episode timestep = 886
Scene graph at timestep 1788 is [True, False, False, False, False, True]
State prediction error at timestep 1788 is tensor(1.1128e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1789. State = [[-0.26379097  0.29161793]]. Action = [[-0.12181003 -0.10282162  0.21937108  0.30725694]]. Reward = [0.]
Curr episode timestep = 887
Scene graph at timestep 1789 is [True, False, False, False, False, True]
State prediction error at timestep 1789 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1789 of -1
Current timestep = 1790. State = [[-0.2637147  0.2913099]]. Action = [[ 0.20166463  0.05221862 -0.10969694 -0.6877767 ]]. Reward = [0.]
Curr episode timestep = 888
Scene graph at timestep 1790 is [True, False, False, False, False, True]
State prediction error at timestep 1790 is tensor(1.7298e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1791. State = [[-0.26370034  0.2913216 ]]. Action = [[-5.6827068e-04 -1.9701827e-01  4.8882425e-02  7.7975833e-01]]. Reward = [0.]
Curr episode timestep = 889
Scene graph at timestep 1791 is [True, False, False, False, False, True]
State prediction error at timestep 1791 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1792. State = [[-0.26376355  0.29153326]]. Action = [[ 0.1219767   0.21058005  0.22019863 -0.21342641]]. Reward = [0.]
Curr episode timestep = 890
Scene graph at timestep 1792 is [True, False, False, False, False, True]
State prediction error at timestep 1792 is tensor(1.4335e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1793. State = [[-0.26392123  0.29156047]]. Action = [[-0.08356342 -0.13829756 -0.02705729  0.7855594 ]]. Reward = [0.]
Curr episode timestep = 891
Scene graph at timestep 1793 is [True, False, False, False, False, True]
State prediction error at timestep 1793 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1793 of -1
Current timestep = 1794. State = [[-0.26389652  0.2914928 ]]. Action = [[-0.22106525 -0.0759905  -0.06502163  0.23454666]]. Reward = [0.]
Curr episode timestep = 892
Scene graph at timestep 1794 is [True, False, False, False, False, True]
State prediction error at timestep 1794 is tensor(3.8498e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1795. State = [[-0.26391932  0.29163992]]. Action = [[-0.22261238 -0.10234518  0.19659144  0.14461815]]. Reward = [0.]
Curr episode timestep = 893
Scene graph at timestep 1795 is [True, False, False, False, False, True]
State prediction error at timestep 1795 is tensor(8.2117e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1796. State = [[-0.2639665   0.29160595]]. Action = [[-0.077042    0.22525668 -0.09011428 -0.9709423 ]]. Reward = [0.]
Curr episode timestep = 894
Scene graph at timestep 1796 is [True, False, False, False, False, True]
State prediction error at timestep 1796 is tensor(2.4343e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1796 of -1
Current timestep = 1797. State = [[-0.2639665   0.29160595]]. Action = [[ 0.15144926  0.12196225 -0.19743073  0.30796075]]. Reward = [0.]
Curr episode timestep = 895
Scene graph at timestep 1797 is [True, False, False, False, False, True]
State prediction error at timestep 1797 is tensor(1.4626e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1798. State = [[-0.26391932  0.29163992]]. Action = [[-0.08450308 -0.15835847 -0.118994    0.8312764 ]]. Reward = [0.]
Curr episode timestep = 896
Scene graph at timestep 1798 is [True, False, False, False, False, True]
State prediction error at timestep 1798 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1799. State = [[-0.26391932  0.29163992]]. Action = [[ 0.235351    0.0298667   0.09581596 -0.40123302]]. Reward = [0.]
Curr episode timestep = 897
Scene graph at timestep 1799 is [True, False, False, False, False, True]
State prediction error at timestep 1799 is tensor(8.4932e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1800. State = [[-0.26391932  0.29163992]]. Action = [[-0.2432595  -0.05909802  0.15138853 -0.3217231 ]]. Reward = [0.]
Curr episode timestep = 898
Scene graph at timestep 1800 is [True, False, False, False, False, True]
State prediction error at timestep 1800 is tensor(2.1901e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1800 of -1
Current timestep = 1801. State = [[-0.26388738  0.29157814]]. Action = [[-0.01119038 -0.07625265 -0.09205733  0.05327642]]. Reward = [0.]
Curr episode timestep = 899
Scene graph at timestep 1801 is [True, False, False, False, False, True]
State prediction error at timestep 1801 is tensor(5.1087e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1802. State = [[-0.26376164  0.29125145]]. Action = [[0.13049418 0.16983145 0.22029209 0.52989244]]. Reward = [0.]
Curr episode timestep = 900
Scene graph at timestep 1802 is [True, False, False, False, False, True]
State prediction error at timestep 1802 is tensor(1.3265e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1803. State = [[-0.2576539   0.00824015]]. Action = [[-0.04579349  0.14324754  0.17971927 -0.03746182]]. Reward = [0.]
Curr episode timestep = 901
Scene graph at timestep 1803 is [True, False, False, False, True, False]
State prediction error at timestep 1803 is tensor(0.0388, grad_fn=<MseLossBackward0>)
Current timestep = 1804. State = [[-0.260585    0.01000365]]. Action = [[-0.03082952  0.13051924  0.11008325 -0.07662582]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1804 is [True, False, False, False, True, False]
State prediction error at timestep 1804 is tensor(5.6301e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1805. State = [[-0.2624446   0.01139778]]. Action = [[ 0.15147948  0.23778766  0.06861883 -0.932962  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1805 is [True, False, False, False, True, False]
State prediction error at timestep 1805 is tensor(6.3467e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1806. State = [[-0.2644325   0.01312377]]. Action = [[-0.19187339  0.10836285 -0.09459333  0.8576524 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1806 is [True, False, False, False, True, False]
State prediction error at timestep 1806 is tensor(3.5334e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1807. State = [[-0.27596685  0.03896751]]. Action = [[0.07924441 0.12069917 0.16734368 0.9495852 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1807 is [True, False, False, False, True, False]
State prediction error at timestep 1807 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1808. State = [[-0.2768221   0.04879857]]. Action = [[-0.16661772  0.22396201 -0.10543352 -0.7005004 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1808 is [True, False, False, False, True, False]
State prediction error at timestep 1808 is tensor(1.3378e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1809. State = [[-0.27704698  0.06072808]]. Action = [[-0.1575527   0.0975818   0.17988437 -0.250072  ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1809 is [True, False, False, False, True, False]
State prediction error at timestep 1809 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1810. State = [[-0.2774174   0.06951495]]. Action = [[ 0.17640138  0.10752529 -0.15522328  0.00989103]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1810 is [True, False, False, False, True, False]
State prediction error at timestep 1810 is tensor(6.1865e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1811. State = [[-0.27817228  0.0840861 ]]. Action = [[ 0.04056394  0.2107327  -0.1411633  -0.90851074]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1811 is [True, False, False, False, True, False]
State prediction error at timestep 1811 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1812. State = [[-0.27920204  0.10629303]]. Action = [[-0.17651433 -0.10781682  0.01058316 -0.5912907 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1812 is [True, False, False, False, True, False]
State prediction error at timestep 1812 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1813. State = [[-0.28208822  0.14414869]]. Action = [[-0.22237086 -0.21090284 -0.15884495  0.8208592 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1813 is [True, False, False, False, False, True]
State prediction error at timestep 1813 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 1814. State = [[-0.2831382   0.21219943]]. Action = [[-0.05052544  0.04108652  0.1631543   0.8750317 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1814 is [True, False, False, False, False, True]
State prediction error at timestep 1814 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 1815. State = [[-0.28307694  0.2220273 ]]. Action = [[-0.22963649 -0.02831909  0.20380211  0.6162138 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1815 is [True, False, False, False, False, True]
State prediction error at timestep 1815 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1816. State = [[-0.28275314  0.23080325]]. Action = [[0.03145736 0.14831746 0.100409   0.22460115]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1816 is [True, False, False, False, False, True]
State prediction error at timestep 1816 is tensor(3.1261e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1817. State = [[-0.28177622  0.23873347]]. Action = [[-0.08550701  0.21740633 -0.13730568  0.63664365]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1817 is [True, False, False, False, False, True]
State prediction error at timestep 1817 is tensor(4.6656e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1818. State = [[-0.28098232  0.2472342 ]]. Action = [[ 0.12118435 -0.21839385 -0.06952558 -0.5630459 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1818 is [True, False, False, False, False, True]
State prediction error at timestep 1818 is tensor(3.8350e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1819. State = [[-0.28032196  0.25448692]]. Action = [[-0.04512417 -0.19708614 -0.01408999 -0.38961136]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1819 is [True, False, False, False, False, True]
State prediction error at timestep 1819 is tensor(3.6057e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1820. State = [[-0.27971604  0.26194888]]. Action = [[-0.23932244 -0.02780993 -0.20569289  0.00546992]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1820 is [True, False, False, False, False, True]
State prediction error at timestep 1820 is tensor(6.5517e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1821. State = [[-0.27934214  0.26584455]]. Action = [[-0.16566156  0.18128228 -0.12499146  0.3623923 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1821 is [True, False, False, False, False, True]
State prediction error at timestep 1821 is tensor(1.8095e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1822. State = [[-0.27885142  0.27220744]]. Action = [[-0.09766889  0.18844235  0.18673524  0.56983304]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1822 is [True, False, False, False, False, True]
State prediction error at timestep 1822 is tensor(6.1299e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1823. State = [[-0.2785652   0.27518535]]. Action = [[ 0.01427314 -0.19261667  0.06117779 -0.1859451 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1823 is [True, False, False, False, False, True]
State prediction error at timestep 1823 is tensor(1.0452e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1824. State = [[-0.278247   0.2787911]]. Action = [[-0.00586146 -0.15015176 -0.07798733 -0.11153555]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1824 is [True, False, False, False, False, True]
State prediction error at timestep 1824 is tensor(1.5712e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1825. State = [[-0.27823913  0.28144807]]. Action = [[-0.13813663  0.19163072  0.14532986 -0.68300563]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1825 is [True, False, False, False, False, True]
State prediction error at timestep 1825 is tensor(1.1413e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1826. State = [[-0.27828667  0.28391019]]. Action = [[-0.13221565  0.24011457 -0.13262793 -0.85132223]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1826 is [True, False, False, False, False, True]
State prediction error at timestep 1826 is tensor(1.4492e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1827. State = [[-0.27810335  0.286471  ]]. Action = [[ 0.21392256 -0.00455727  0.20508653 -0.4943607 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1827 is [True, False, False, False, False, True]
State prediction error at timestep 1827 is tensor(2.5079e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1828. State = [[-0.2776539  0.2882988]]. Action = [[-0.06376055  0.1453287  -0.14005785  0.48477685]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1828 is [True, False, False, False, False, True]
State prediction error at timestep 1828 is tensor(2.8337e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1829. State = [[-0.27631024  0.2933153 ]]. Action = [[ 0.11849225  0.01566249  0.18651727 -0.24233943]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1829 is [True, False, False, False, False, True]
State prediction error at timestep 1829 is tensor(3.9203e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1830. State = [[-0.27550936  0.293861  ]]. Action = [[-0.05849329  0.24633598  0.20023221  0.72324467]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1830 is [True, False, False, False, False, True]
State prediction error at timestep 1830 is tensor(4.1982e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1831. State = [[-0.2717448   0.29633063]]. Action = [[ 0.10934669 -0.04648763  0.10763299  0.2370416 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1831 is [True, False, False, False, False, True]
State prediction error at timestep 1831 is tensor(2.7287e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1832. State = [[-0.2704978   0.29631838]]. Action = [[ 0.23802754 -0.23592302 -0.20796384  0.4450369 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1832 is [True, False, False, False, False, True]
State prediction error at timestep 1832 is tensor(1.0907e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1833. State = [[-0.26671815  0.29685253]]. Action = [[ 0.10819423  0.03760561 -0.2070523   0.8034539 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1833 is [True, False, False, False, False, True]
State prediction error at timestep 1833 is tensor(1.3311e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1834. State = [[-0.26540112  0.29687953]]. Action = [[-6.8223476e-04  1.7438889e-01 -1.6292457e-01 -8.0295563e-01]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1834 is [True, False, False, False, False, True]
State prediction error at timestep 1834 is tensor(1.8383e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1835. State = [[-0.2637248   0.29709783]]. Action = [[ 0.12236077 -0.23615319 -0.14746195 -0.58319384]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1835 is [True, False, False, False, False, True]
State prediction error at timestep 1835 is tensor(3.1235e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1836. State = [[-0.26243758  0.29745266]]. Action = [[-0.22310504  0.21767345  0.23552275  0.47616625]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1836 is [True, False, False, False, False, True]
State prediction error at timestep 1836 is tensor(1.1232e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1837. State = [[-0.26155427  0.2979111 ]]. Action = [[-0.02251583  0.14656138  0.07544005  0.6375339 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1837 is [True, False, False, False, False, True]
State prediction error at timestep 1837 is tensor(5.8290e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1838. State = [[-0.26067358  0.2985825 ]]. Action = [[-0.17881992  0.18000609 -0.10105065 -0.9598471 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1838 is [True, False, False, False, False, True]
State prediction error at timestep 1838 is tensor(2.8504e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1839. State = [[-0.25888512  0.29901797]]. Action = [[ 9.3132257e-05 -1.8506843e-01  1.3472733e-01  8.5842168e-01]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1839 is [True, False, False, False, False, True]
State prediction error at timestep 1839 is tensor(5.5036e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1840. State = [[-0.25767386  0.29921973]]. Action = [[ 0.22537905  0.00313196 -0.14964713 -0.81621957]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1840 is [True, False, False, False, False, True]
State prediction error at timestep 1840 is tensor(6.2646e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1841. State = [[-0.25703594  0.2994712 ]]. Action = [[-0.17429827  0.06334135 -0.10865094 -0.16520047]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1841 is [True, False, False, False, False, True]
State prediction error at timestep 1841 is tensor(1.1564e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1842. State = [[-0.25304517  0.30123898]]. Action = [[-0.00819655 -0.02829376  0.19529372 -0.78391254]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1842 is [True, False, False, False, False, True]
State prediction error at timestep 1842 is tensor(4.7687e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1843. State = [[-0.25139067  0.3013355 ]]. Action = [[-0.08659558  0.23007637  0.01278517  0.38315845]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1843 is [True, False, False, False, False, True]
State prediction error at timestep 1843 is tensor(1.5448e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1843 of 1
Current timestep = 1844. State = [[-0.25128034  0.30132797]]. Action = [[ 0.19553363 -0.07985525 -0.0507898  -0.12260973]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1844 is [True, False, False, False, False, True]
State prediction error at timestep 1844 is tensor(3.8954e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1845. State = [[-0.2511731   0.30153367]]. Action = [[-0.19713052  0.186827    0.13786316 -0.07868612]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1845 is [True, False, False, False, False, True]
State prediction error at timestep 1845 is tensor(1.3748e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1846. State = [[-0.25117722  0.30134535]]. Action = [[-0.19945005 -0.21280016  0.24621254 -0.3110634 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1846 is [True, False, False, False, False, True]
State prediction error at timestep 1846 is tensor(6.7475e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1847. State = [[-0.25096738  0.30123222]]. Action = [[0.19983786 0.17965418 0.0546262  0.2929592 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1847 is [True, False, False, False, False, True]
State prediction error at timestep 1847 is tensor(2.0367e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1848. State = [[-0.25063393  0.30077106]]. Action = [[ 0.07565704 -0.07361403 -0.21113826 -0.841666  ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1848 is [True, False, False, False, False, True]
State prediction error at timestep 1848 is tensor(1.5562e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1849. State = [[-0.24893339  0.2983844 ]]. Action = [[-0.00378107 -0.03916234  0.15505141  0.31834936]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1849 is [True, False, False, False, False, True]
State prediction error at timestep 1849 is tensor(1.8477e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1850. State = [[-0.24857545  0.29804122]]. Action = [[ 0.17110527  0.2217195  -0.17898789 -0.7634559 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1850 is [True, False, False, False, False, True]
State prediction error at timestep 1850 is tensor(3.6552e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1851. State = [[-0.24710888  0.29578656]]. Action = [[0.10846305 0.01754332 0.01534712 0.30657852]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1851 is [True, False, False, False, False, True]
State prediction error at timestep 1851 is tensor(4.5452e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1852. State = [[-0.2465447  0.2953246]]. Action = [[ 0.21711993 -0.23164642 -0.16617797  0.82756305]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1852 is [True, False, False, False, False, True]
State prediction error at timestep 1852 is tensor(7.7673e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1853. State = [[-0.24584045  0.29456782]]. Action = [[-0.17997812  0.21089125 -0.2144212  -0.11089528]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1853 is [True, False, False, False, False, True]
State prediction error at timestep 1853 is tensor(3.2746e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1854. State = [[-0.24543008  0.29428503]]. Action = [[-0.14364897  0.1485936  -0.08058295 -0.47710496]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1854 is [True, False, False, False, False, True]
State prediction error at timestep 1854 is tensor(5.5423e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1855. State = [[-0.24499372  0.2943142 ]]. Action = [[ 0.23106131 -0.05007756  0.20293456  0.3134048 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1855 is [True, False, False, False, False, True]
State prediction error at timestep 1855 is tensor(2.9955e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1856. State = [[-0.24454902  0.29419643]]. Action = [[-0.24160181  0.24935976 -0.19023523 -0.11330426]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1856 is [True, False, False, False, False, True]
State prediction error at timestep 1856 is tensor(1.0765e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1857. State = [[-0.24377409  0.29345697]]. Action = [[ 0.02069104 -0.20940223  0.11665535 -0.5635324 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1857 is [True, False, False, False, False, True]
State prediction error at timestep 1857 is tensor(6.5870e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1858. State = [[-0.24327268  0.29332387]]. Action = [[-0.01428537 -0.2241153   0.16026282  0.6683049 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1858 is [True, False, False, False, False, True]
State prediction error at timestep 1858 is tensor(1.1446e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1859. State = [[-0.24327947  0.2933497 ]]. Action = [[ 0.19024485  0.21553028  0.17963073 -0.03083116]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1859 is [True, False, False, False, False, True]
State prediction error at timestep 1859 is tensor(1.2214e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1859 of 1
Current timestep = 1860. State = [[-0.2426234   0.29262602]]. Action = [[-0.1276512   0.08302656 -0.18522452  0.34590185]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1860 is [True, False, False, False, False, True]
State prediction error at timestep 1860 is tensor(1.4093e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1860 of 1
Current timestep = 1861. State = [[-0.24472171  0.2946965 ]]. Action = [[-0.02959317  0.04773623  0.10438535 -0.4619649 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1861 is [True, False, False, False, False, True]
State prediction error at timestep 1861 is tensor(4.1437e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1862. State = [[-0.24532577  0.29532516]]. Action = [[ 0.22414386  0.1615834  -0.00979771 -0.2742237 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1862 is [True, False, False, False, False, True]
State prediction error at timestep 1862 is tensor(1.0173e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1863. State = [[-0.24601343  0.29632896]]. Action = [[-0.15399227  0.00708678  0.11531079 -0.26174235]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1863 is [True, False, False, False, False, True]
State prediction error at timestep 1863 is tensor(4.3998e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1864. State = [[-0.24639577  0.2966123 ]]. Action = [[-0.10358635 -0.1970051   0.0928435   0.6750555 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1864 is [True, False, False, False, False, True]
State prediction error at timestep 1864 is tensor(3.5709e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1865. State = [[-0.2467329   0.29672584]]. Action = [[ 0.17265224  0.01417434  0.13220769 -0.34369385]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1865 is [True, False, False, False, False, True]
State prediction error at timestep 1865 is tensor(2.0947e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1866. State = [[-0.24709985  0.2972434 ]]. Action = [[-0.20186803 -0.00580998  0.07355958 -0.16893578]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1866 is [True, False, False, False, False, True]
State prediction error at timestep 1866 is tensor(2.3507e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1867. State = [[-0.24745353  0.2978121 ]]. Action = [[-0.14852504  0.20202696  0.06105575 -0.94000214]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1867 is [True, False, False, False, False, True]
State prediction error at timestep 1867 is tensor(2.1567e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1868. State = [[-0.2476745   0.29783463]]. Action = [[-0.22267304 -0.09214427 -0.0118847   0.76629996]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1868 is [True, False, False, False, False, True]
State prediction error at timestep 1868 is tensor(5.5140e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1869. State = [[-0.24816893  0.29843518]]. Action = [[-0.22208576  0.02218759  0.23316422 -0.34448874]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1869 is [True, False, False, False, False, True]
State prediction error at timestep 1869 is tensor(4.0149e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1870. State = [[-0.24825302  0.29857874]]. Action = [[0.21181613 0.11902839 0.02413288 0.8292904 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1870 is [True, False, False, False, False, True]
State prediction error at timestep 1870 is tensor(1.6412e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1871. State = [[-0.24842049  0.29851654]]. Action = [[-0.21624213 -0.0310697  -0.21597706  0.35891128]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1871 is [True, False, False, False, False, True]
State prediction error at timestep 1871 is tensor(1.7946e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1872. State = [[-0.24862042  0.29880443]]. Action = [[-0.16727285  0.01435816 -0.20538306 -0.85701764]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1872 is [True, False, False, False, False, True]
State prediction error at timestep 1872 is tensor(5.1905e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1873. State = [[-0.24871434  0.29905087]]. Action = [[ 0.22538495 -0.22681254  0.23632273  0.9326196 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1873 is [True, False, False, False, False, True]
State prediction error at timestep 1873 is tensor(1.8142e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1874. State = [[-0.2488125   0.29902878]]. Action = [[-0.19681603 -0.07561213 -0.05890241 -0.4416765 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1874 is [True, False, False, False, False, True]
State prediction error at timestep 1874 is tensor(3.3093e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1875. State = [[-0.24892987  0.29921558]]. Action = [[ 0.13897169 -0.0634166  -0.1852924   0.19172454]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1875 is [True, False, False, False, False, True]
State prediction error at timestep 1875 is tensor(7.5407e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1875 of -1
Current timestep = 1876. State = [[-0.24919264  0.2994843 ]]. Action = [[0.08405215 0.20440537 0.13237733 0.65967894]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1876 is [True, False, False, False, False, True]
State prediction error at timestep 1876 is tensor(6.1795e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1877. State = [[-0.24923822  0.2995582 ]]. Action = [[ 0.12872288  0.18891996  0.22186768 -0.5861737 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1877 is [True, False, False, False, False, True]
State prediction error at timestep 1877 is tensor(1.1046e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1878. State = [[-0.24934576  0.29964   ]]. Action = [[-0.08461508  0.23457044 -0.03038198 -0.3042416 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1878 is [True, False, False, False, False, True]
State prediction error at timestep 1878 is tensor(1.1681e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1878 of -1
Current timestep = 1879. State = [[-0.24946146  0.29967284]]. Action = [[ 0.04867926 -0.08066197 -0.15700735  0.2665279 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1879 is [True, False, False, False, False, True]
State prediction error at timestep 1879 is tensor(8.0091e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1880. State = [[-0.248994   0.2989909]]. Action = [[-0.19881935 -0.2053985  -0.06386189 -0.910403  ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1880 is [True, False, False, False, False, True]
State prediction error at timestep 1880 is tensor(2.0136e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1881. State = [[-0.24839367  0.29772973]]. Action = [[ 0.11953014 -0.1328424   0.07368097  0.5549369 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1881 is [True, False, False, False, False, True]
State prediction error at timestep 1881 is tensor(4.8652e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1882. State = [[-0.24686459  0.29614884]]. Action = [[-0.19889407 -0.20001733  0.09634787  0.26702464]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1882 is [True, False, False, False, False, True]
State prediction error at timestep 1882 is tensor(2.7439e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1883. State = [[-0.24566582  0.294732  ]]. Action = [[ 0.06978467  0.22244197  0.20422766 -0.1284514 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1883 is [True, False, False, False, False, True]
State prediction error at timestep 1883 is tensor(7.4440e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1884. State = [[-0.24503608  0.29347315]]. Action = [[-0.15017459  0.16205862  0.13484436 -0.4004144 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1884 is [True, False, False, False, False, True]
State prediction error at timestep 1884 is tensor(2.2686e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1885. State = [[-0.2448863   0.29320747]]. Action = [[-0.20059864 -0.21376172  0.15134895 -0.6514288 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1885 is [True, False, False, False, False, True]
State prediction error at timestep 1885 is tensor(8.9825e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1886. State = [[-0.24441797  0.29231822]]. Action = [[ 0.17654067 -0.15376067  0.15830672  0.14866519]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1886 is [True, False, False, False, False, True]
State prediction error at timestep 1886 is tensor(4.1770e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1887. State = [[-0.24380283  0.29117343]]. Action = [[0.0930855  0.15014547 0.09554112 0.10256064]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1887 is [True, False, False, False, False, True]
State prediction error at timestep 1887 is tensor(3.6349e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1888. State = [[-0.24263142  0.28894582]]. Action = [[-0.07358827 -0.09442824  0.10090056 -0.7330812 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 1888 is [True, False, False, False, False, True]
State prediction error at timestep 1888 is tensor(4.4808e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1888 of 1
Current timestep = 1889. State = [[-0.24149932  0.28667423]]. Action = [[0.12989011 0.0411137  0.13782245 0.63674927]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1889 is [True, False, False, False, False, True]
State prediction error at timestep 1889 is tensor(1.9279e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1889 of 1
Current timestep = 1890. State = [[-0.23949121  0.28470212]]. Action = [[ 0.06135496 -0.10194306  0.08799627  0.4409927 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1890 is [True, False, False, False, False, True]
State prediction error at timestep 1890 is tensor(9.9257e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1890 of 1
Current timestep = 1891. State = [[-0.23834117  0.28326187]]. Action = [[ 0.18492407 -0.1551314   0.21886459 -0.25493526]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1891 is [True, False, False, False, False, True]
State prediction error at timestep 1891 is tensor(5.5841e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1892. State = [[-0.23594888  0.2799817 ]]. Action = [[ 0.07110333 -0.11583194  0.05865714 -0.736071  ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1892 is [True, False, False, False, False, True]
State prediction error at timestep 1892 is tensor(4.3707e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1892 of 1
Current timestep = 1893. State = [[-0.23380464  0.2775072 ]]. Action = [[-0.15616299 -0.21274044  0.10141572  0.1623404 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 1893 is [True, False, False, False, False, True]
State prediction error at timestep 1893 is tensor(1.0857e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1894. State = [[-0.23302807  0.27638948]]. Action = [[-0.11070526 -0.14582945 -0.1323723  -0.4628423 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1894 is [True, False, False, False, False, True]
State prediction error at timestep 1894 is tensor(2.9349e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1895. State = [[-0.22987083  0.2731358 ]]. Action = [[ 0.05062547  0.11987147 -0.21046436 -0.25572717]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1895 is [True, False, False, False, False, True]
State prediction error at timestep 1895 is tensor(9.5302e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1895 of 1
Current timestep = 1896. State = [[-0.22720642  0.27409655]]. Action = [[-0.10393517 -0.07408482 -0.18725684 -0.7645627 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 1896 is [True, False, False, False, False, True]
State prediction error at timestep 1896 is tensor(9.2879e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1896 of 1
Current timestep = 1897. State = [[-0.22686933  0.2734238 ]]. Action = [[-0.06894976 -0.02874687  0.15275824 -0.23117036]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1897 is [True, False, False, False, False, True]
State prediction error at timestep 1897 is tensor(5.6323e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1897 of 1
Current timestep = 1898. State = [[-0.22684222  0.27315342]]. Action = [[-0.14068484 -0.02380346  0.18649924  0.77942336]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1898 is [True, False, False, False, False, True]
State prediction error at timestep 1898 is tensor(6.2607e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1899. State = [[-0.22683157  0.27316803]]. Action = [[-0.00676855 -0.20462447 -0.10013574 -0.6931541 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 1899 is [True, False, False, False, False, True]
State prediction error at timestep 1899 is tensor(1.6768e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1900. State = [[-0.22687076  0.27274013]]. Action = [[-0.10970123  0.02961043  0.19029373 -0.60919416]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 1900 is [True, False, False, False, False, True]
State prediction error at timestep 1900 is tensor(6.2382e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1900 of 1
Current timestep = 1901. State = [[-0.22821607  0.2739788 ]]. Action = [[ 0.08689436 -0.0579468   0.20479134  0.33977842]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 1901 is [True, False, False, False, False, True]
State prediction error at timestep 1901 is tensor(2.2380e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1901 of 1
Current timestep = 1902. State = [[-0.22798316  0.27353144]]. Action = [[-0.07919185 -0.21808617 -0.09709859 -0.5945819 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1902 is [True, False, False, False, False, True]
State prediction error at timestep 1902 is tensor(2.0199e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1903. State = [[-0.22780177  0.2731445 ]]. Action = [[-0.20706496  0.02031887 -0.16575047  0.23136914]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1903 is [True, False, False, False, False, True]
State prediction error at timestep 1903 is tensor(2.4040e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1904. State = [[-0.22736438  0.2720707 ]]. Action = [[ 0.04531199 -0.09692889  0.03682199 -0.4173363 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1904 is [True, False, False, False, False, True]
State prediction error at timestep 1904 is tensor(4.5295e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1904 of 1
Current timestep = 1905. State = [[-0.2266792  0.2708908]]. Action = [[-0.24357885 -0.09977007  0.2162149   0.7079325 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1905 is [True, False, False, False, False, True]
State prediction error at timestep 1905 is tensor(3.3852e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1906. State = [[-0.22610708  0.2698087 ]]. Action = [[-0.14096922  0.20148554  0.2337828   0.5738003 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1906 is [True, False, False, False, False, True]
State prediction error at timestep 1906 is tensor(1.2709e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1907. State = [[-0.22592814  0.26938832]]. Action = [[-0.12826009 -0.13525915  0.10778388  0.89672625]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1907 is [True, False, False, False, False, True]
State prediction error at timestep 1907 is tensor(2.7315e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1907 of 1
Current timestep = 1908. State = [[-0.22509721  0.26744786]]. Action = [[-0.03181577  0.13646826 -0.16762407  0.4292761 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1908 is [True, False, False, False, False, True]
State prediction error at timestep 1908 is tensor(3.7013e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1909. State = [[-0.22505876  0.26724696]]. Action = [[ 0.22737297  0.05173647  0.08159962 -0.647428  ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1909 is [True, False, False, False, False, True]
State prediction error at timestep 1909 is tensor(1.4413e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1909 of 1
Current timestep = 1910. State = [[-0.22499646  0.26687068]]. Action = [[ 0.00479087 -0.24166341 -0.20060413  0.4761436 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1910 is [True, False, False, False, False, True]
State prediction error at timestep 1910 is tensor(3.1401e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1911. State = [[-0.22472855  0.26630947]]. Action = [[-0.0446316   0.05010831  0.218995    0.31787145]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1911 is [True, False, False, False, False, True]
State prediction error at timestep 1911 is tensor(2.5241e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1912. State = [[-0.22493948  0.26649585]]. Action = [[ 0.11165985 -0.22390611 -0.16505824 -0.42866433]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1912 is [True, False, False, False, False, True]
State prediction error at timestep 1912 is tensor(3.3297e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1913. State = [[-0.22499686  0.26668227]]. Action = [[-0.19326515  0.10774609  0.07081786  0.47460926]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1913 is [True, False, False, False, False, True]
State prediction error at timestep 1913 is tensor(1.0387e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1914. State = [[-0.22499686  0.26668227]]. Action = [[-0.15016219  0.05865857  0.19064158  0.8726115 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1914 is [True, False, False, False, False, True]
State prediction error at timestep 1914 is tensor(2.9961e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1915. State = [[-0.2249878  0.26658  ]]. Action = [[-0.15394704  0.15574425  0.13598645  0.9847145 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1915 is [True, False, False, False, False, True]
State prediction error at timestep 1915 is tensor(8.6269e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1916. State = [[-0.22515379  0.2667442 ]]. Action = [[ 0.14461982 -0.09634317  0.17535925  0.23702407]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1916 is [True, False, False, False, False, True]
State prediction error at timestep 1916 is tensor(5.5678e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1917. State = [[-0.22538629  0.2669525 ]]. Action = [[-0.11997966 -0.07267734  0.19497514 -0.21875554]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1917 is [True, False, False, False, False, True]
State prediction error at timestep 1917 is tensor(2.8188e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1917 of 1
Current timestep = 1918. State = [[-0.22552441  0.26710194]]. Action = [[ 0.11953413 -0.04045708 -0.19116855  0.43823838]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1918 is [True, False, False, False, False, True]
State prediction error at timestep 1918 is tensor(2.1511e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1918 of 1
Current timestep = 1919. State = [[-0.22515407  0.26636526]]. Action = [[-0.2141422  -0.14138263  0.04822126  0.21599162]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1919 is [True, False, False, False, False, True]
State prediction error at timestep 1919 is tensor(1.0805e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1920. State = [[-0.22475965  0.26522663]]. Action = [[-0.04616603  0.07428163 -0.16588055 -0.1792624 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1920 is [True, False, False, False, False, True]
State prediction error at timestep 1920 is tensor(3.5498e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1920 of 1
Current timestep = 1921. State = [[-0.22506759  0.26567703]]. Action = [[-0.19833155 -0.19095348 -0.13997355  0.90720105]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1921 is [True, False, False, False, False, True]
State prediction error at timestep 1921 is tensor(3.2156e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1922. State = [[-0.22519107  0.265881  ]]. Action = [[0.19095477 0.22148123 0.18911707 0.40277147]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1922 is [True, False, False, False, False, True]
State prediction error at timestep 1922 is tensor(1.7068e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1923. State = [[-0.22532214  0.2660027 ]]. Action = [[-0.19091     0.1280685  -0.1828469  -0.24985051]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1923 is [True, False, False, False, False, True]
State prediction error at timestep 1923 is tensor(3.7109e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1924. State = [[-0.22565506  0.26619098]]. Action = [[-0.09257922  0.08174556 -0.04913828  0.898288  ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1924 is [True, False, False, False, False, True]
State prediction error at timestep 1924 is tensor(2.6493e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1924 of 1
Current timestep = 1925. State = [[-0.22614117  0.2667093 ]]. Action = [[ 0.14014679  0.23335367 -0.06793013  0.91296077]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1925 is [True, False, False, False, False, True]
State prediction error at timestep 1925 is tensor(5.5206e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1926. State = [[-0.22816114  0.26903087]]. Action = [[ 0.0436649  -0.0845378  -0.18255213  0.00791514]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1926 is [True, False, False, False, False, True]
State prediction error at timestep 1926 is tensor(7.0215e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1926 of 1
Current timestep = 1927. State = [[-0.22801286  0.2685299 ]]. Action = [[-0.07005325 -0.07138345  0.10612121 -0.92205495]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1927 is [True, False, False, False, False, True]
State prediction error at timestep 1927 is tensor(4.4412e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1927 of 1
Current timestep = 1928. State = [[-0.22792879  0.2680817 ]]. Action = [[0.20839602 0.0037488  0.13193783 0.43316984]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1928 is [True, False, False, False, False, True]
State prediction error at timestep 1928 is tensor(9.2396e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1929. State = [[-0.22781932  0.26783252]]. Action = [[ 0.10595882 -0.2219378  -0.19326715 -0.9367819 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1929 is [True, False, False, False, False, True]
State prediction error at timestep 1929 is tensor(2.0096e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1930. State = [[-0.22776374  0.26679763]]. Action = [[-0.10179123 -0.01966469  0.16215584  0.38334727]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1930 is [True, False, False, False, False, True]
State prediction error at timestep 1930 is tensor(2.5618e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1930 of 1
Current timestep = 1931. State = [[-0.22822075  0.266603  ]]. Action = [[ 0.18518913 -0.06448382  0.00558192  0.059605  ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 1931 is [True, False, False, False, False, True]
State prediction error at timestep 1931 is tensor(1.0944e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1932. State = [[-0.22885114  0.26655295]]. Action = [[ 0.2303192  -0.14444804  0.10730281  0.5034387 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 1932 is [True, False, False, False, False, True]
State prediction error at timestep 1932 is tensor(6.3017e-09, grad_fn=<MseLossBackward0>)
Current timestep = 1933. State = [[-0.22931889  0.26657784]]. Action = [[ 0.12857682 -0.18484187  0.13753444  0.19692254]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 1933 is [True, False, False, False, False, True]
State prediction error at timestep 1933 is tensor(7.7614e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1933 of 1
Current timestep = 1934. State = [[-0.22967659  0.26630035]]. Action = [[ 0.21082553  0.15201849  0.13916701 -0.00315475]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 1934 is [True, False, False, False, False, True]
State prediction error at timestep 1934 is tensor(3.9089e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1935. State = [[-0.23038551  0.2660155 ]]. Action = [[-0.200566    0.1532672   0.13134363  0.2820716 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 1935 is [True, False, False, False, False, True]
State prediction error at timestep 1935 is tensor(2.2685e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1936. State = [[-0.23090446  0.26591673]]. Action = [[ 0.08023357 -0.18797165 -0.20081455 -0.72732985]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 1936 is [True, False, False, False, False, True]
State prediction error at timestep 1936 is tensor(2.3777e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1937. State = [[-0.23134731  0.2655068 ]]. Action = [[ 0.1697284  -0.06517237 -0.16145542 -0.8248131 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 1937 is [True, False, False, False, False, True]
State prediction error at timestep 1937 is tensor(4.3457e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1938. State = [[-0.23179835  0.26521468]]. Action = [[ 0.03618774 -0.12216973 -0.17884688  0.14362216]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 1938 is [True, False, False, False, False, True]
State prediction error at timestep 1938 is tensor(1.2422e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1938 of 1
Current timestep = 1939. State = [[-0.2311773   0.26193464]]. Action = [[-0.01561818  0.03213286  0.16938722  0.76075137]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 1939 is [True, False, False, False, False, True]
State prediction error at timestep 1939 is tensor(1.1386e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1939 of 1
Current timestep = 1940. State = [[-0.23115498  0.26151997]]. Action = [[ 0.11542386  0.1616103   0.17467153 -0.5791331 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 1940 is [True, False, False, False, False, True]
State prediction error at timestep 1940 is tensor(9.0484e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1941. State = [[-0.23105927  0.26117182]]. Action = [[ 0.14798427  0.05042106  0.05507779 -0.6334405 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 1941 is [True, False, False, False, False, True]
State prediction error at timestep 1941 is tensor(1.0324e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1942. State = [[-0.23103522  0.26095304]]. Action = [[-0.03487256  0.21861315 -0.07376054  0.6057867 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 1942 is [True, False, False, False, False, True]
State prediction error at timestep 1942 is tensor(3.7672e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1943. State = [[-0.2311374  0.2604321]]. Action = [[-0.12556091  0.09373599  0.14053777  0.63730717]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 1943 is [True, False, False, False, False, True]
State prediction error at timestep 1943 is tensor(1.0705e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1943 of 1
Current timestep = 1944. State = [[-0.23236173  0.2611756 ]]. Action = [[ 0.1476242   0.17094487 -0.12295485  0.39246404]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 1944 is [True, False, False, False, False, True]
State prediction error at timestep 1944 is tensor(6.8037e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1945. State = [[-0.235554    0.26305297]]. Action = [[ 0.07046264  0.02438617 -0.18735367 -0.23165369]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 1945 is [True, False, False, False, False, True]
State prediction error at timestep 1945 is tensor(1.0453e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1945 of 1
Current timestep = 1946. State = [[-0.23600668  0.2637355 ]]. Action = [[-0.05034001  0.19137207  0.02839163  0.7597532 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 1946 is [True, False, False, False, False, True]
State prediction error at timestep 1946 is tensor(1.3995e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1947. State = [[-0.23607296  0.2641054 ]]. Action = [[-0.2124335  -0.24796443  0.06641716 -0.69977564]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 1947 is [True, False, False, False, False, True]
State prediction error at timestep 1947 is tensor(3.6918e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1947 of 1
Current timestep = 1948. State = [[-0.23678721  0.26472238]]. Action = [[-0.07733324 -0.07700013 -0.0361782  -0.10125494]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 1948 is [True, False, False, False, False, True]
State prediction error at timestep 1948 is tensor(6.3916e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1949. State = [[-0.23716114  0.2643506 ]]. Action = [[-0.0037241   0.13506883  0.01673299  0.8710654 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 1949 is [True, False, False, False, False, True]
State prediction error at timestep 1949 is tensor(2.0415e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1950. State = [[-0.2375704  0.264022 ]]. Action = [[ 0.09414619 -0.23825105  0.14633429 -0.39889783]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 1950 is [True, False, False, False, False, True]
State prediction error at timestep 1950 is tensor(1.4197e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1950 of 1
Current timestep = 1951. State = [[-0.23800907  0.26400447]]. Action = [[-0.07377103  0.20804405  0.157282   -0.6183437 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 1951 is [True, False, False, False, False, True]
State prediction error at timestep 1951 is tensor(4.7539e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1952. State = [[-0.23941998  0.2632125 ]]. Action = [[ 0.03241506  0.08718666  0.13200474 -0.29431945]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 1952 is [True, False, False, False, False, True]
State prediction error at timestep 1952 is tensor(3.0138e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1953. State = [[-0.23980759  0.26351264]]. Action = [[ 0.2408759  -0.11379288  0.01188511  0.73474526]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 1953 is [True, False, False, False, False, True]
State prediction error at timestep 1953 is tensor(1.2141e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1954. State = [[-0.24008787  0.26380286]]. Action = [[-0.2444066   0.01755109 -0.09929502 -0.9689219 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 1954 is [True, False, False, False, False, True]
State prediction error at timestep 1954 is tensor(6.8065e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1955. State = [[-0.24103434  0.26466766]]. Action = [[-0.11458057 -0.01161554 -0.00550328  0.22593749]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 1955 is [True, False, False, False, False, True]
State prediction error at timestep 1955 is tensor(1.4988e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1955 of 1
Current timestep = 1956. State = [[-0.24201405  0.26482132]]. Action = [[ 0.05209985  0.21415609 -0.00822018  0.11607313]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 1956 is [True, False, False, False, False, True]
State prediction error at timestep 1956 is tensor(1.9191e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1957. State = [[-0.24413203  0.2658069 ]]. Action = [[ 0.12513596 -0.0083826   0.08310646 -0.6742187 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 1957 is [True, False, False, False, False, True]
State prediction error at timestep 1957 is tensor(8.8310e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1957 of 1
Current timestep = 1958. State = [[-0.24395767  0.2657625 ]]. Action = [[ 0.1119459   0.14181882  0.24469993 -0.47352982]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 1958 is [True, False, False, False, False, True]
State prediction error at timestep 1958 is tensor(1.3376e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1959. State = [[-0.24366084  0.26556942]]. Action = [[0.15732986 0.08077991 0.13024908 0.7702997 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 1959 is [True, False, False, False, False, True]
State prediction error at timestep 1959 is tensor(1.2469e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1960. State = [[-0.24367437  0.2654066 ]]. Action = [[-0.15828778  0.03921914 -0.15627123 -0.09199643]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 1960 is [True, False, False, False, False, True]
State prediction error at timestep 1960 is tensor(9.6184e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1961. State = [[-0.24376267  0.26532462]]. Action = [[-0.23064555  0.13482213 -0.22302799  0.16341424]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 1961 is [True, False, False, False, False, True]
State prediction error at timestep 1961 is tensor(9.2169e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1962. State = [[-0.2437364   0.26520208]]. Action = [[ 0.01126966 -0.24877092 -0.07741991 -0.11510229]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 1962 is [True, False, False, False, False, True]
State prediction error at timestep 1962 is tensor(9.4987e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1962 of -1
Current timestep = 1963. State = [[-0.24359336  0.26497614]]. Action = [[ 0.16325474 -0.1512505   0.14355901 -0.20932859]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 1963 is [True, False, False, False, False, True]
State prediction error at timestep 1963 is tensor(2.8364e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1964. State = [[-0.24356869  0.26491475]]. Action = [[ 0.21222833  0.20286494 -0.18017773 -0.9181026 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 1964 is [True, False, False, False, False, True]
State prediction error at timestep 1964 is tensor(6.5389e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1965. State = [[-0.24359523  0.2647553 ]]. Action = [[-0.09761569  0.06760418  0.18524233  0.6830733 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 1965 is [True, False, False, False, False, True]
State prediction error at timestep 1965 is tensor(6.9676e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1965 of -1
Current timestep = 1966. State = [[-0.24430071  0.26551968]]. Action = [[ 0.09886491 -0.13804452  0.11127502  0.19966424]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 1966 is [True, False, False, False, False, True]
State prediction error at timestep 1966 is tensor(3.5734e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1967. State = [[-0.2447669   0.26610008]]. Action = [[-0.12094525  0.17979914  0.05781066 -0.3552721 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 1967 is [True, False, False, False, False, True]
State prediction error at timestep 1967 is tensor(4.7520e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1967 of -1
Current timestep = 1968. State = [[-0.24522583  0.26654857]]. Action = [[-0.1795309   0.20859563  0.09960234  0.5913534 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 1968 is [True, False, False, False, False, True]
State prediction error at timestep 1968 is tensor(1.0280e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1969. State = [[-0.24643303  0.26771995]]. Action = [[ 0.10919935 -0.04334733 -0.02363952 -0.31487215]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 1969 is [True, False, False, False, False, True]
State prediction error at timestep 1969 is tensor(8.4481e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1970. State = [[-0.24599718  0.26721534]]. Action = [[-0.21704085 -0.12878467  0.00719255 -0.75875044]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 1970 is [True, False, False, False, False, True]
State prediction error at timestep 1970 is tensor(5.1304e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1971. State = [[-0.24569598  0.26685706]]. Action = [[ 0.13667065 -0.21085273  0.01790532  0.38072968]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 1971 is [True, False, False, False, False, True]
State prediction error at timestep 1971 is tensor(1.0886e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1972. State = [[-0.24548008  0.2661098 ]]. Action = [[-0.00165087  0.05674106 -0.2083498  -0.64073503]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 1972 is [True, False, False, False, False, True]
State prediction error at timestep 1972 is tensor(3.2826e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1972 of -1
Current timestep = 1973. State = [[-0.24571934  0.2663491 ]]. Action = [[-0.20940892 -0.0767583  -0.05876397 -0.01227731]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 1973 is [True, False, False, False, False, True]
State prediction error at timestep 1973 is tensor(1.6942e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1974. State = [[-0.24591595  0.26662868]]. Action = [[-0.05964127  0.18534562 -0.24492577  0.9727453 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 1974 is [True, False, False, False, False, True]
State prediction error at timestep 1974 is tensor(4.6412e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1975. State = [[-0.2460819   0.26676816]]. Action = [[-0.09966907 -0.19657044  0.1127232   0.5603131 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 1975 is [True, False, False, False, False, True]
State prediction error at timestep 1975 is tensor(3.9342e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1976. State = [[-0.24614154  0.2668681 ]]. Action = [[ 0.07138467 -0.22599813  0.0898827  -0.11298037]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 1976 is [True, False, False, False, False, True]
State prediction error at timestep 1976 is tensor(2.3256e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1976 of -1
Current timestep = 1977. State = [[-0.24588704  0.26696715]]. Action = [[ 0.1822961   0.17416215 -0.1129217  -0.40314412]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 1977 is [True, False, False, False, False, True]
State prediction error at timestep 1977 is tensor(5.4199e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1978. State = [[-0.24613601  0.26711378]]. Action = [[-0.14040811  0.12859046 -0.23646137 -0.7146894 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 1978 is [True, False, False, False, False, True]
State prediction error at timestep 1978 is tensor(4.8748e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1979. State = [[-0.24600787  0.26686978]]. Action = [[ 0.07487348 -0.11811754  0.09654331  0.11493158]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 1979 is [True, False, False, False, False, True]
State prediction error at timestep 1979 is tensor(3.9236e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1979 of -1
Current timestep = 1980. State = [[-0.24511124  0.26581708]]. Action = [[ 0.04163831  0.24061239 -0.18605448  0.8408402 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 1980 is [True, False, False, False, False, True]
State prediction error at timestep 1980 is tensor(4.9739e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1981. State = [[-0.24465984  0.26520872]]. Action = [[-0.21664086 -0.18168175  0.1506787   0.05106592]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 1981 is [True, False, False, False, False, True]
State prediction error at timestep 1981 is tensor(1.3869e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1982. State = [[-0.24402703  0.26433468]]. Action = [[-0.13281089 -0.21982095  0.21859378  0.58533895]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 1982 is [True, False, False, False, False, True]
State prediction error at timestep 1982 is tensor(2.3190e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1982 of -1
Current timestep = 1983. State = [[-0.24303545  0.26240018]]. Action = [[-0.05789232 -0.05234386  0.21547389  0.85712624]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 1983 is [True, False, False, False, False, True]
State prediction error at timestep 1983 is tensor(6.3688e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1984. State = [[-0.24303731  0.2621706 ]]. Action = [[-0.02256341 -0.15888797  0.15107313  0.2149769 ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 1984 is [True, False, False, False, False, True]
State prediction error at timestep 1984 is tensor(5.3058e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1985. State = [[-0.24294704  0.26169923]]. Action = [[0.09273809 0.23193502 0.24020293 0.09830606]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 1985 is [True, False, False, False, False, True]
State prediction error at timestep 1985 is tensor(4.3754e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1986. State = [[-0.24259855  0.26131383]]. Action = [[-0.03286089 -0.2253573  -0.19396275 -0.34113127]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 1986 is [True, False, False, False, False, True]
State prediction error at timestep 1986 is tensor(8.8004e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1987. State = [[-0.24228433  0.26093227]]. Action = [[-0.19920038  0.18285269  0.16171521  0.06373751]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 1987 is [True, False, False, False, False, True]
State prediction error at timestep 1987 is tensor(2.0525e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1988. State = [[-0.24233924  0.2607387 ]]. Action = [[ 0.08193386  0.23111868  0.24602968 -0.06914532]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 1988 is [True, False, False, False, False, True]
State prediction error at timestep 1988 is tensor(9.7527e-09, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1988 of -1
Current timestep = 1989. State = [[-0.24230196  0.2605398 ]]. Action = [[-0.1366569  -0.03875208  0.1910485   0.56682634]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 1989 is [True, False, False, False, False, True]
State prediction error at timestep 1989 is tensor(7.7102e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1990. State = [[-0.24211004  0.2601965 ]]. Action = [[-0.04256316  0.1710679  -0.01170985  0.7566652 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 1990 is [True, False, False, False, False, True]
State prediction error at timestep 1990 is tensor(8.4777e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1991. State = [[-0.2420022   0.26001152]]. Action = [[ 0.21710607  0.19393736 -0.02528383  0.74014044]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 1991 is [True, False, False, False, False, True]
State prediction error at timestep 1991 is tensor(4.4841e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1992. State = [[-0.24203871  0.26000813]]. Action = [[-0.08240339  0.1456852   0.10554227 -0.12590015]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 1992 is [True, False, False, False, False, True]
State prediction error at timestep 1992 is tensor(2.2140e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1992 of -1
Current timestep = 1993. State = [[-0.2417738   0.25946897]]. Action = [[0.10586452 0.05149657 0.12734368 0.4850402 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 1993 is [True, False, False, False, False, True]
State prediction error at timestep 1993 is tensor(7.1268e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1994. State = [[-0.24173833  0.25949156]]. Action = [[0.13873872 0.19108713 0.05411509 0.7143148 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 1994 is [True, False, False, False, False, True]
State prediction error at timestep 1994 is tensor(1.4481e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1994 of -1
Current timestep = 1995. State = [[-0.24118501  0.2596669 ]]. Action = [[ 0.01785424 -0.1019664  -0.19682065  0.51606584]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 1995 is [True, False, False, False, False, True]
State prediction error at timestep 1995 is tensor(1.8319e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1996. State = [[-0.24073523  0.2591512 ]]. Action = [[-0.17410825  0.06277332  0.20100904  0.87048125]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 1996 is [True, False, False, False, False, True]
State prediction error at timestep 1996 is tensor(1.6221e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1997. State = [[-0.24013866  0.25833225]]. Action = [[ 0.21872282  0.14650226 -0.09892678  0.8229195 ]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 1997 is [True, False, False, False, False, True]
State prediction error at timestep 1997 is tensor(5.7270e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1998. State = [[-0.23961578  0.25760126]]. Action = [[-0.13862187  0.06541485  0.14494509  0.60750556]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 1998 is [True, False, False, False, False, True]
State prediction error at timestep 1998 is tensor(1.0508e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1999. State = [[-0.23954137  0.2570318 ]]. Action = [[-0.02681476  0.17407876  0.19882202 -0.61439764]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 1999 is [True, False, False, False, False, True]
State prediction error at timestep 1999 is tensor(7.2287e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2000. State = [[-0.23918992  0.2562481 ]]. Action = [[-0.06375113  0.19143716 -0.18702854  0.9634776 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 2000 is [True, False, False, False, False, True]
State prediction error at timestep 2000 is tensor(3.7172e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2000 of -1
Current timestep = 2001. State = [[-0.23866284  0.25540358]]. Action = [[-0.22991246  0.06449947  0.20257574 -0.638378  ]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 2001 is [True, False, False, False, False, True]
State prediction error at timestep 2001 is tensor(2.0156e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2002. State = [[-0.23867933  0.25510025]]. Action = [[-0.21650738  0.1368146   0.20288652 -0.2596178 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 2002 is [True, False, False, False, False, True]
State prediction error at timestep 2002 is tensor(3.1110e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2003. State = [[-0.23869434  0.25494528]]. Action = [[ 0.11427519 -0.1380136  -0.07268535  0.03547955]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 2003 is [True, False, False, False, False, True]
State prediction error at timestep 2003 is tensor(2.1806e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2004. State = [[-0.23851322  0.25452438]]. Action = [[-0.2460033   0.03827593 -0.03319548 -0.942576  ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 2004 is [True, False, False, False, False, True]
State prediction error at timestep 2004 is tensor(5.0095e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2004 of -1
Current timestep = 2005. State = [[-0.23810044  0.25391805]]. Action = [[-0.21498555  0.12063462 -0.22461802  0.39541948]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 2005 is [True, False, False, False, False, True]
State prediction error at timestep 2005 is tensor(1.5389e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2006. State = [[-0.23802254  0.2537318 ]]. Action = [[-0.1466785  -0.13078597 -0.17356552 -0.93492913]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 2006 is [True, False, False, False, False, True]
State prediction error at timestep 2006 is tensor(1.4642e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2007. State = [[-0.23810886  0.253659  ]]. Action = [[ 0.08552194  0.20627713 -0.2009919   0.26417875]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 2007 is [True, False, False, False, False, True]
State prediction error at timestep 2007 is tensor(3.6378e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2008. State = [[-0.23810144  0.25360027]]. Action = [[-0.2386393   0.14832032  0.03849477  0.6560986 ]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 2008 is [True, False, False, False, False, True]
State prediction error at timestep 2008 is tensor(2.0896e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2009. State = [[-0.23782504  0.2530542 ]]. Action = [[ 0.18666822  0.15792203  0.09853131 -0.6430198 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 2009 is [True, False, False, False, False, True]
State prediction error at timestep 2009 is tensor(2.9487e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2009 of -1
Current timestep = 2010. State = [[-0.23772666  0.25261304]]. Action = [[ 0.0978204  -0.0614863   0.20425504  0.83365536]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 2010 is [True, False, False, False, False, True]
State prediction error at timestep 2010 is tensor(6.1160e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2010 of -1
Current timestep = 2011. State = [[-0.23695983  0.25183603]]. Action = [[-0.23800682  0.23352581 -0.22572955  0.69182646]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 2011 is [True, False, False, False, False, True]
State prediction error at timestep 2011 is tensor(9.3168e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2012. State = [[-0.23624262  0.2509206 ]]. Action = [[-0.19735731 -0.19861698  0.10334575  0.14706433]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 2012 is [True, False, False, False, False, True]
State prediction error at timestep 2012 is tensor(4.7287e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2013. State = [[-0.23611303  0.25060517]]. Action = [[ 0.22672898 -0.07045165 -0.08316983  0.21543586]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 2013 is [True, False, False, False, False, True]
State prediction error at timestep 2013 is tensor(2.1933e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2014. State = [[-0.23612952  0.25028425]]. Action = [[-0.12576282  0.24496531  0.13870454  0.6306734 ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 2014 is [True, False, False, False, False, True]
State prediction error at timestep 2014 is tensor(6.7886e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2015. State = [[-0.23587203  0.24982066]]. Action = [[0.13873792 0.14057806 0.13874206 0.31398392]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 2015 is [True, False, False, False, False, True]
State prediction error at timestep 2015 is tensor(4.4402e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2016. State = [[-0.23529544  0.24926674]]. Action = [[-0.22029334  0.13237876 -0.18286592  0.06386864]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 2016 is [True, False, False, False, False, True]
State prediction error at timestep 2016 is tensor(2.2733e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2017. State = [[-0.23523906  0.24897023]]. Action = [[ 0.21706676 -0.24116634 -0.13159092 -0.81658864]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 2017 is [True, False, False, False, False, True]
State prediction error at timestep 2017 is tensor(1.3858e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2018. State = [[-0.23497573  0.24862008]]. Action = [[ 0.16738701  0.05141935 -0.13349704  0.19631219]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 2018 is [True, False, False, False, False, True]
State prediction error at timestep 2018 is tensor(1.9457e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2018 of 1
Current timestep = 2019. State = [[-0.23459697  0.24809709]]. Action = [[-0.19357355 -0.20317595 -0.07968086  0.65343344]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 2019 is [True, False, False, False, False, True]
State prediction error at timestep 2019 is tensor(7.7751e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2020. State = [[-0.2346634   0.24796298]]. Action = [[ 0.24070498 -0.07631525  0.15451169 -0.6355583 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 2020 is [True, False, False, False, False, True]
State prediction error at timestep 2020 is tensor(2.4697e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2021. State = [[-0.23467755  0.2480359 ]]. Action = [[0.2139219  0.14374095 0.22294456 0.7736678 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 2021 is [True, False, False, False, False, True]
State prediction error at timestep 2021 is tensor(1.2226e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2022. State = [[-0.2344998   0.24767908]]. Action = [[-0.17074244  0.06793219  0.10547385  0.76731765]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 2022 is [True, False, False, False, False, True]
State prediction error at timestep 2022 is tensor(9.8201e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2022 of 1
Current timestep = 2023. State = [[-0.23431276  0.24742053]]. Action = [[ 0.10509834 -0.21666224  0.07775551  0.33245873]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 2023 is [True, False, False, False, False, True]
State prediction error at timestep 2023 is tensor(1.0460e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2024. State = [[-0.23441721  0.24750222]]. Action = [[-0.17829747  0.16769218 -0.04416673  0.4225229 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 2024 is [True, False, False, False, False, True]
State prediction error at timestep 2024 is tensor(2.1612e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2025. State = [[-0.23453608  0.24767682]]. Action = [[-0.10265501 -0.15512009  0.03999037  0.6680827 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 2025 is [True, False, False, False, False, True]
State prediction error at timestep 2025 is tensor(1.3082e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2025 of 1
Current timestep = 2026. State = [[-0.23440675  0.24752422]]. Action = [[ 0.01981857 -0.22162068 -0.14422204 -0.03643024]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 2026 is [True, False, False, False, False, True]
State prediction error at timestep 2026 is tensor(7.4466e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2027. State = [[-0.23412892  0.24717078]]. Action = [[-0.24023251  0.07494226 -0.17553882  0.07347429]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 2027 is [True, False, False, False, False, True]
State prediction error at timestep 2027 is tensor(2.2495e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2028. State = [[-0.23419303  0.24720445]]. Action = [[-0.11224109  0.16132611  0.07787299 -0.40831745]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 2028 is [True, False, False, False, False, True]
State prediction error at timestep 2028 is tensor(1.5117e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2029. State = [[-0.23426531  0.24722576]]. Action = [[ 0.16975796 -0.19711377 -0.21740976 -0.23959732]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 2029 is [True, False, False, False, False, True]
State prediction error at timestep 2029 is tensor(2.2083e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2029 of 1
Current timestep = 2030. State = [[-0.23415834  0.24706562]]. Action = [[ 0.1515579   0.00729039  0.06101236 -0.5744166 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 2030 is [True, False, False, False, False, True]
State prediction error at timestep 2030 is tensor(1.2067e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2031. State = [[-0.23407532  0.24687904]]. Action = [[-0.13287371 -0.07206467  0.16298276  0.03725183]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 2031 is [True, False, False, False, False, True]
State prediction error at timestep 2031 is tensor(2.7794e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2031 of 1
Current timestep = 2032. State = [[-0.23406094  0.24655871]]. Action = [[ 0.07344979  0.00961637  0.03094247 -0.8246572 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 2032 is [True, False, False, False, False, True]
State prediction error at timestep 2032 is tensor(1.4394e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2032 of 1
Current timestep = 2033. State = [[-0.23374632  0.24569978]]. Action = [[ 0.00376642 -0.09637275 -0.1768677  -0.02279544]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 2033 is [True, False, False, False, False, True]
State prediction error at timestep 2033 is tensor(1.1525e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2033 of 1
Current timestep = 2034. State = [[-0.23250486  0.24278456]]. Action = [[ 0.04575026  0.0454827   0.17197084 -0.6188937 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 2034 is [True, False, False, False, False, True]
State prediction error at timestep 2034 is tensor(1.8352e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2035. State = [[-0.23239806  0.2421147 ]]. Action = [[-0.06374642  0.17240334 -0.08284877 -0.8749867 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 2035 is [True, False, False, False, False, True]
State prediction error at timestep 2035 is tensor(1.6952e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2036. State = [[-0.2322436  0.2418282]]. Action = [[0.22625506 0.22276533 0.05134028 0.8027524 ]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 2036 is [True, False, False, False, False, True]
State prediction error at timestep 2036 is tensor(2.6224e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2037. State = [[-0.2319947   0.24151573]]. Action = [[ 0.19353315 -0.08764589  0.11738789  0.41371286]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 2037 is [True, False, False, False, False, True]
State prediction error at timestep 2037 is tensor(4.4225e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2038. State = [[-0.23206507  0.24145876]]. Action = [[0.15102357 0.10246846 0.19862452 0.5176096 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 2038 is [True, False, False, False, False, True]
State prediction error at timestep 2038 is tensor(1.3045e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2039. State = [[-0.23203893  0.24122861]]. Action = [[ 0.05865788  0.22655466  0.2236231  -0.20751965]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 2039 is [True, False, False, False, False, True]
State prediction error at timestep 2039 is tensor(3.7175e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2039 of 1
Current timestep = 2040. State = [[-0.23170261  0.24053195]]. Action = [[-0.06375551 -0.08334446 -0.01749262  0.7982403 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 2040 is [True, False, False, False, False, True]
State prediction error at timestep 2040 is tensor(8.3172e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2041. State = [[-0.23160277  0.24003525]]. Action = [[-0.21136996  0.12585431 -0.03059286  0.83464956]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 2041 is [True, False, False, False, False, True]
State prediction error at timestep 2041 is tensor(5.8841e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2042. State = [[-0.23105323  0.23852465]]. Action = [[ 0.0801087  -0.06339455 -0.2120224   0.27842844]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 2042 is [True, False, False, False, False, True]
State prediction error at timestep 2042 is tensor(9.8395e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2043. State = [[-0.2295929   0.23569931]]. Action = [[-0.01993288 -0.00087333  0.23246652  0.42940724]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 2043 is [True, False, False, False, False, True]
State prediction error at timestep 2043 is tensor(1.8606e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2044. State = [[-0.22929011  0.23508911]]. Action = [[-0.21273325  0.00222155 -0.11958666 -0.6258448 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 2044 is [True, False, False, False, False, True]
State prediction error at timestep 2044 is tensor(3.4785e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2045. State = [[-0.22903202  0.23480363]]. Action = [[0.04265469 0.21162456 0.03520015 0.6874285 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 2045 is [True, False, False, False, False, True]
State prediction error at timestep 2045 is tensor(2.8071e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2046. State = [[-0.2288495   0.23437826]]. Action = [[0.21589884 0.19713074 0.09722912 0.8588635 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 2046 is [True, False, False, False, False, True]
State prediction error at timestep 2046 is tensor(2.2830e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2047. State = [[-0.22852963  0.23333584]]. Action = [[ 0.12453136  0.1298824   0.2445904  -0.78334546]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 2047 is [True, False, False, False, False, True]
State prediction error at timestep 2047 is tensor(1.9219e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2047 of 1
Current timestep = 2048. State = [[-0.22861049  0.23367722]]. Action = [[ 0.14632303 -0.13354935  0.10435387 -0.3756265 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 2048 is [True, False, False, False, False, True]
State prediction error at timestep 2048 is tensor(7.4678e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2049. State = [[-0.22839168  0.23456232]]. Action = [[0.00921488 0.06336477 0.24628949 0.4040873 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 2049 is [True, False, False, False, False, True]
State prediction error at timestep 2049 is tensor(5.3987e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2050. State = [[-0.22841938  0.23486951]]. Action = [[ 0.17418003  0.094515   -0.12546799  0.75920177]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 2050 is [True, False, False, False, False, True]
State prediction error at timestep 2050 is tensor(3.4880e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2050 of 1
Current timestep = 2051. State = [[-0.22868071  0.23566043]]. Action = [[ 0.01143315 -0.16711022 -0.01919182  0.3618331 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 2051 is [True, False, False, False, False, True]
State prediction error at timestep 2051 is tensor(1.8506e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2052. State = [[-0.22880949  0.2360346 ]]. Action = [[-0.00802547 -0.15944542 -0.19505599 -0.36520344]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 2052 is [True, False, False, False, False, True]
State prediction error at timestep 2052 is tensor(1.1546e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2053. State = [[-0.22830497  0.23706228]]. Action = [[ 0.03489605 -0.07718813  0.12413827 -0.5077529 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 2053 is [True, False, False, False, False, True]
State prediction error at timestep 2053 is tensor(1.4915e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2053 of 1
Current timestep = 2054. State = [[-0.22789711  0.23683819]]. Action = [[-0.12777935 -0.18378794 -0.05386272 -0.8558195 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 2054 is [True, False, False, False, False, True]
State prediction error at timestep 2054 is tensor(2.4086e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2055. State = [[-0.22743039  0.23646407]]. Action = [[-0.14863102  0.10652885  0.10874549  0.04855359]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 2055 is [True, False, False, False, False, True]
State prediction error at timestep 2055 is tensor(1.3792e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2056. State = [[-0.22719677  0.23618989]]. Action = [[-0.19712642  0.23835713 -0.17520764 -0.13529348]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 2056 is [True, False, False, False, False, True]
State prediction error at timestep 2056 is tensor(2.9699e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2057. State = [[-0.227376    0.23620997]]. Action = [[-0.15702087  0.23452824 -0.2070269   0.5050633 ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 2057 is [True, False, False, False, False, True]
State prediction error at timestep 2057 is tensor(4.3348e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2058. State = [[-0.22732309  0.23618902]]. Action = [[-0.2251176  -0.12603258  0.03675127 -0.5040371 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 2058 is [True, False, False, False, False, True]
State prediction error at timestep 2058 is tensor(2.0781e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2058 of 1
Current timestep = 2059. State = [[-0.22669426  0.23600899]]. Action = [[ 0.03452748 -0.03066719 -0.21638142 -0.9129064 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 2059 is [True, False, False, False, False, True]
State prediction error at timestep 2059 is tensor(8.9087e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2060. State = [[-0.22626612  0.23566411]]. Action = [[ 0.09135228  0.20269528 -0.15296686  0.10814941]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 2060 is [True, False, False, False, False, True]
State prediction error at timestep 2060 is tensor(1.2207e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2061. State = [[-0.22604692  0.23536053]]. Action = [[ 0.21821514 -0.0198717  -0.03434762  0.5928652 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 2061 is [True, False, False, False, False, True]
State prediction error at timestep 2061 is tensor(8.9517e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2061 of 1
Current timestep = 2062. State = [[-0.22583228  0.23492883]]. Action = [[-0.21134385  0.0729976   0.12992853 -0.5912241 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 2062 is [True, False, False, False, False, True]
State prediction error at timestep 2062 is tensor(7.1852e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2063. State = [[-0.22537473  0.23387529]]. Action = [[-0.011052   -0.07803257 -0.11066568 -0.5354484 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 2063 is [True, False, False, False, False, True]
State prediction error at timestep 2063 is tensor(1.2355e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2064. State = [[-0.22493923  0.23326644]]. Action = [[ 0.14755565 -0.12128218  0.17387134  0.17128706]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 2064 is [True, False, False, False, False, True]
State prediction error at timestep 2064 is tensor(2.5822e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2065. State = [[-0.22460903  0.23246302]]. Action = [[-0.04972273 -0.177048    0.10362881  0.7859814 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 2065 is [True, False, False, False, False, True]
State prediction error at timestep 2065 is tensor(1.9420e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2066. State = [[-0.22430687  0.2316431 ]]. Action = [[ 0.16968566 -0.05945259  0.06722993 -0.84041905]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 2066 is [True, False, False, False, False, True]
State prediction error at timestep 2066 is tensor(1.0583e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2067. State = [[-0.2242231  0.2312106]]. Action = [[ 0.20249045 -0.1166966   0.13958031 -0.10680413]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 2067 is [True, False, False, False, False, True]
State prediction error at timestep 2067 is tensor(1.7123e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2067 of 1
Current timestep = 2068. State = [[-0.22406994  0.23067933]]. Action = [[-0.03871545 -0.20296572 -0.02781148 -0.71527064]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 2068 is [True, False, False, False, False, True]
State prediction error at timestep 2068 is tensor(8.4307e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2069. State = [[-0.22366601  0.22993645]]. Action = [[-0.12758188  0.1687015   0.20542145 -0.47718132]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 2069 is [True, False, False, False, False, True]
State prediction error at timestep 2069 is tensor(1.0763e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2070. State = [[-0.2236457   0.22971559]]. Action = [[-0.14626335  0.1430107  -0.12730335 -0.17398214]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 2070 is [True, False, False, False, False, True]
State prediction error at timestep 2070 is tensor(4.3430e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2071. State = [[-0.22361465  0.22965185]]. Action = [[-0.24488227 -0.12258157  0.02632898  0.21585345]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 2071 is [True, False, False, False, False, True]
State prediction error at timestep 2071 is tensor(6.0502e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2072. State = [[-0.22350308  0.22927126]]. Action = [[-0.15100646 -0.17179991  0.01335976 -0.81320244]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 2072 is [True, False, False, False, False, True]
State prediction error at timestep 2072 is tensor(2.9775e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2072 of 1
Current timestep = 2073. State = [[-0.2233009   0.22884029]]. Action = [[ 0.09281322  0.22697026 -0.19610178 -0.81378835]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 2073 is [True, False, False, False, False, True]
State prediction error at timestep 2073 is tensor(8.7737e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2074. State = [[-0.22323228  0.22864953]]. Action = [[-0.19738477 -0.00785452  0.1317662  -0.503991  ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 2074 is [True, False, False, False, False, True]
State prediction error at timestep 2074 is tensor(7.0224e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2075. State = [[-0.22329843  0.22862265]]. Action = [[ 0.0719651   0.16089353 -0.16879588 -0.6711596 ]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 2075 is [True, False, False, False, False, True]
State prediction error at timestep 2075 is tensor(2.7158e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2076. State = [[-0.22325125  0.22846799]]. Action = [[-0.01462328  0.10165375 -0.04297996 -0.28823906]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 2076 is [True, False, False, False, False, True]
State prediction error at timestep 2076 is tensor(2.6407e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2076 of 1
Current timestep = 2077. State = [[-0.22361012  0.22902122]]. Action = [[-0.20245424 -0.24780206  0.17725703  0.14396167]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 2077 is [True, False, False, False, False, True]
State prediction error at timestep 2077 is tensor(2.1731e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2078. State = [[-0.22385089  0.22939692]]. Action = [[-0.18803848  0.08906338 -0.1014325   0.97732747]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 2078 is [True, False, False, False, False, True]
State prediction error at timestep 2078 is tensor(1.0589e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2079. State = [[-0.22390504  0.22951032]]. Action = [[-0.05667756 -0.19729425 -0.01037577 -0.10785061]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 2079 is [True, False, False, False, False, True]
State prediction error at timestep 2079 is tensor(3.2277e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2080. State = [[-0.22395553  0.22960173]]. Action = [[ 0.16355014  0.0971415   0.01099125 -0.40985918]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 2080 is [True, False, False, False, False, True]
State prediction error at timestep 2080 is tensor(1.9155e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2080 of 1
Current timestep = 2081. State = [[-0.22411095  0.22987828]]. Action = [[ 0.14237398  0.12221915  0.21544206 -0.24486995]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 2081 is [True, False, False, False, False, True]
State prediction error at timestep 2081 is tensor(1.6190e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2082. State = [[-0.22422521  0.23007293]]. Action = [[ 0.07184747  0.03757203 -0.04737404 -0.03785521]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 2082 is [True, False, False, False, False, True]
State prediction error at timestep 2082 is tensor(1.0025e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2083. State = [[-0.22398183  0.23034279]]. Action = [[ 0.00776893 -0.2053371   0.08793026 -0.4626881 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 2083 is [True, False, False, False, False, True]
State prediction error at timestep 2083 is tensor(2.0592e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2083 of 1
Current timestep = 2084. State = [[-0.22359367  0.23070298]]. Action = [[ 0.02493215 -0.24226405  0.11850327 -0.7474455 ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 2084 is [True, False, False, False, False, True]
State prediction error at timestep 2084 is tensor(1.4748e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2085. State = [[-0.22286884  0.23106892]]. Action = [[-0.09533027 -0.10247073 -0.00078572  0.00194502]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 2085 is [True, False, False, False, False, True]
State prediction error at timestep 2085 is tensor(7.0939e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2086. State = [[-0.22310422  0.23076974]]. Action = [[-0.07795049  0.0527873   0.16685241  0.36290598]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 2086 is [True, False, False, False, False, True]
State prediction error at timestep 2086 is tensor(3.8364e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2086 of 1
Current timestep = 2087. State = [[-0.22347896  0.2311178 ]]. Action = [[ 0.188303   -0.10877012 -0.15741265  0.2438283 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 2087 is [True, False, False, False, False, True]
State prediction error at timestep 2087 is tensor(5.7253e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2088. State = [[-0.22375262  0.23161413]]. Action = [[0.18547994 0.05609715 0.12261117 0.6582897 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 2088 is [True, False, False, False, False, True]
State prediction error at timestep 2088 is tensor(1.3522e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2089. State = [[-0.22411664  0.23207285]]. Action = [[-0.02900741 -0.11378895 -0.07239103 -0.6980757 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 2089 is [True, False, False, False, False, True]
State prediction error at timestep 2089 is tensor(1.1136e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2089 of 1
Current timestep = 2090. State = [[-0.2240151   0.23182213]]. Action = [[-0.24744514 -0.16345294 -0.10873546 -0.01820719]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 2090 is [True, False, False, False, False, True]
State prediction error at timestep 2090 is tensor(2.2454e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2091. State = [[-0.22382519  0.23145753]]. Action = [[ 0.21423274  0.1442413   0.03509    -0.04641783]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 2091 is [True, False, False, False, False, True]
State prediction error at timestep 2091 is tensor(3.6201e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2092. State = [[-0.22364058  0.23104805]]. Action = [[-0.14309692  0.11335301 -0.07074422  0.11979222]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 2092 is [True, False, False, False, False, True]
State prediction error at timestep 2092 is tensor(1.4165e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2092 of 1
Current timestep = 2093. State = [[-0.22367348  0.23112552]]. Action = [[-0.20525424  0.20544639 -0.07139251  0.8813431 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 2093 is [True, False, False, False, False, True]
State prediction error at timestep 2093 is tensor(1.0652e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2094. State = [[-0.22386986  0.2310448 ]]. Action = [[ 0.15460601  0.02871677  0.16035897 -0.08127123]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 2094 is [True, False, False, False, False, True]
State prediction error at timestep 2094 is tensor(3.0805e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2095. State = [[-0.22365431  0.2305609 ]]. Action = [[ 0.00449368 -0.04162246  0.20206219 -0.01026326]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 2095 is [True, False, False, False, False, True]
State prediction error at timestep 2095 is tensor(8.3502e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2095 of 1
Current timestep = 2096. State = [[-0.22345544  0.2301747 ]]. Action = [[ 0.22586894 -0.2204116  -0.03699562 -0.48781914]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 2096 is [True, False, False, False, False, True]
State prediction error at timestep 2096 is tensor(5.3276e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2097. State = [[-0.22326854  0.22959022]]. Action = [[-0.08011951  0.18729296  0.2433244   0.21795249]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 2097 is [True, False, False, False, False, True]
State prediction error at timestep 2097 is tensor(2.8026e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2098. State = [[-0.22312187  0.2292999 ]]. Action = [[-0.23297928  0.18936685 -0.21448345 -0.47435284]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 2098 is [True, False, False, False, False, True]
State prediction error at timestep 2098 is tensor(1.2816e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2099. State = [[-0.22296792  0.22855085]]. Action = [[-0.07670712 -0.01513624  0.04374409  0.7508178 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 2099 is [True, False, False, False, False, True]
State prediction error at timestep 2099 is tensor(8.7841e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2099 of 1
Current timestep = 2100. State = [[-0.2233924   0.22831875]]. Action = [[-0.11105129 -0.15340498 -0.17392902  0.54930544]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 2100 is [True, False, False, False, False, True]
State prediction error at timestep 2100 is tensor(1.9139e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2101. State = [[-0.22386627  0.22834899]]. Action = [[ 0.16628659 -0.17308773  0.17330807  0.6962867 ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 2101 is [True, False, False, False, False, True]
State prediction error at timestep 2101 is tensor(6.0445e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2102. State = [[-0.22436085  0.22785594]]. Action = [[-0.08857876  0.00128517 -0.07685474 -0.95350415]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 2102 is [True, False, False, False, False, True]
State prediction error at timestep 2102 is tensor(2.7808e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2102 of 1
Current timestep = 2103. State = [[-0.22514024  0.22804107]]. Action = [[-0.17836054  0.20815206  0.08597243 -0.47932434]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 2103 is [True, False, False, False, False, True]
State prediction error at timestep 2103 is tensor(2.5466e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2104. State = [[-0.2255552   0.22805266]]. Action = [[-0.20674941 -0.20234188  0.22387606  0.89854527]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 2104 is [True, False, False, False, False, True]
State prediction error at timestep 2104 is tensor(7.7210e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2105. State = [[-0.2258637   0.22805773]]. Action = [[ 0.02506462 -0.16150765 -0.09805967 -0.8614822 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 2105 is [True, False, False, False, False, True]
State prediction error at timestep 2105 is tensor(9.2261e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2105 of 1
Current timestep = 2106. State = [[-0.2262045   0.22787401]]. Action = [[-0.19631824 -0.11272341  0.09644243  0.5643084 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 2106 is [True, False, False, False, False, True]
State prediction error at timestep 2106 is tensor(1.2095e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2107. State = [[-0.22669086  0.22803013]]. Action = [[-0.18091744  0.2030876   0.04436395  0.21857297]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 2107 is [True, False, False, False, False, True]
State prediction error at timestep 2107 is tensor(1.6766e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2108. State = [[-0.22742344  0.2282283 ]]. Action = [[-0.07662754  0.07198849  0.18265206 -0.36178076]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 2108 is [True, False, False, False, False, True]
State prediction error at timestep 2108 is tensor(1.3275e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2108 of 1
Current timestep = 2109. State = [[-0.2286537   0.22926116]]. Action = [[-0.13845415  0.13454157 -0.20889623  0.6679343 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 2109 is [True, False, False, False, False, True]
State prediction error at timestep 2109 is tensor(9.2960e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2110. State = [[-0.22921251  0.22984646]]. Action = [[ 0.13448647 -0.20217356  0.1272285  -0.400239  ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 2110 is [True, False, False, False, False, True]
State prediction error at timestep 2110 is tensor(1.6462e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2111. State = [[-0.22944652  0.23015168]]. Action = [[-0.10881393  0.15895802  0.02367428  0.3657992 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 2111 is [True, False, False, False, False, True]
State prediction error at timestep 2111 is tensor(7.5463e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2111 of 1
Current timestep = 2112. State = [[-0.23109058  0.23185682]]. Action = [[-0.04000305 -0.01049002 -0.1947238   0.5620085 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 2112 is [True, False, False, False, False, True]
State prediction error at timestep 2112 is tensor(6.6277e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2113. State = [[-0.23274612  0.23257454]]. Action = [[0.02378613 0.03049701 0.20840806 0.5518793 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 2113 is [True, False, False, False, False, True]
State prediction error at timestep 2113 is tensor(2.2436e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2114. State = [[-0.23366183  0.23371091]]. Action = [[ 0.06285647  0.11690262 -0.10632834 -0.24882436]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 2114 is [True, False, False, False, False, True]
State prediction error at timestep 2114 is tensor(3.0407e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2114 of 1
Current timestep = 2115. State = [[-0.23405342  0.23427247]]. Action = [[-0.19082732 -0.00646015  0.07102084  0.43634713]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 2115 is [True, False, False, False, False, True]
State prediction error at timestep 2115 is tensor(1.0847e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2116. State = [[-0.23441903  0.23495206]]. Action = [[ 0.10777542  0.21150374  0.20241508 -0.854641  ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 2116 is [True, False, False, False, False, True]
State prediction error at timestep 2116 is tensor(1.0921e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2117. State = [[-0.23465432  0.23513912]]. Action = [[-0.21880119 -0.18860725 -0.14160883  0.8984399 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 2117 is [True, False, False, False, False, True]
State prediction error at timestep 2117 is tensor(2.8506e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2118. State = [[-0.23581438  0.23648998]]. Action = [[-0.06690618  0.05063778 -0.1756296   0.38624072]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 2118 is [True, False, False, False, False, True]
State prediction error at timestep 2118 is tensor(1.2033e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2118 of 1
Current timestep = 2119. State = [[-0.2366054   0.23752007]]. Action = [[-0.0521304  -0.17744617 -0.04789716  0.78287804]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 2119 is [True, False, False, False, False, True]
State prediction error at timestep 2119 is tensor(2.9266e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2120. State = [[-0.2371725   0.23841162]]. Action = [[ 0.15787125 -0.19562668  0.1438402  -0.21969122]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 2120 is [True, False, False, False, False, True]
State prediction error at timestep 2120 is tensor(5.2680e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2120 of 1
Current timestep = 2121. State = [[-0.23755386  0.23894057]]. Action = [[ 0.07350224 -0.167298   -0.11661556  0.29760766]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 2121 is [True, False, False, False, False, True]
State prediction error at timestep 2121 is tensor(8.7153e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2122. State = [[-0.23800682  0.23929413]]. Action = [[-0.1785373  -0.19449987 -0.19412476 -0.7575573 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 2122 is [True, False, False, False, False, True]
State prediction error at timestep 2122 is tensor(2.4402e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2123. State = [[-0.23853987  0.24016294]]. Action = [[-0.23034589  0.18949527 -0.04914694  0.6060116 ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 2123 is [True, False, False, False, False, True]
State prediction error at timestep 2123 is tensor(2.5567e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2124. State = [[-0.2388456   0.24056356]]. Action = [[-0.21979177  0.14396155 -0.18469626  0.98262525]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 2124 is [True, False, False, False, False, True]
State prediction error at timestep 2124 is tensor(2.7981e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2125. State = [[-0.23904271  0.24064025]]. Action = [[ 0.1195842   0.19278327 -0.19667222  0.96955013]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 2125 is [True, False, False, False, False, True]
State prediction error at timestep 2125 is tensor(2.2785e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2125 of 1
Current timestep = 2126. State = [[-0.23985015  0.24163233]]. Action = [[ 0.01108694 -0.19841838 -0.00280346  0.02328324]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 2126 is [True, False, False, False, False, True]
State prediction error at timestep 2126 is tensor(9.6742e-09, grad_fn=<MseLossBackward0>)
Current timestep = 2127. State = [[-0.24040593  0.24234743]]. Action = [[-0.03475432  0.08100608 -0.0369177  -0.6553375 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 2127 is [True, False, False, False, False, True]
State prediction error at timestep 2127 is tensor(1.0349e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2127 of -1
Current timestep = 2128. State = [[-0.24239181  0.2447481 ]]. Action = [[ 0.00729072  0.00453806  0.22893563 -0.7907793 ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 2128 is [True, False, False, False, False, True]
State prediction error at timestep 2128 is tensor(5.3821e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2128 of -1
Current timestep = 2129. State = [[-0.24348097  0.24610181]]. Action = [[0.019503   0.0071713  0.20831794 0.4950863 ]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 2129 is [True, False, False, False, False, True]
State prediction error at timestep 2129 is tensor(3.6931e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2129 of -1
Current timestep = 2130. State = [[-0.24358271  0.24628799]]. Action = [[ 0.16843575  0.18022192 -0.20552683  0.06628132]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 2130 is [True, False, False, False, False, True]
State prediction error at timestep 2130 is tensor(1.6376e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2131. State = [[-0.24384329  0.2468157 ]]. Action = [[ 0.04880184 -0.03467005 -0.19369547  0.724581  ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 2131 is [True, False, False, False, False, True]
State prediction error at timestep 2131 is tensor(8.4266e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2131 of -1
Current timestep = 2132. State = [[-0.24371596  0.24656737]]. Action = [[ 0.02378798  0.03898197  0.1236544  -0.906001  ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 2132 is [True, False, False, False, False, True]
State prediction error at timestep 2132 is tensor(7.7416e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2133. State = [[-0.24371083  0.24659544]]. Action = [[-0.10685565  0.14854938 -0.11691338 -0.60934067]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 2133 is [True, False, False, False, False, True]
State prediction error at timestep 2133 is tensor(1.7323e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2133 of -1
Current timestep = 2134. State = [[-0.2437362   0.24665779]]. Action = [[ 0.20038351 -0.1805125  -0.22936803 -0.86188424]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 2134 is [True, False, False, False, False, True]
State prediction error at timestep 2134 is tensor(3.1160e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2135. State = [[-0.24371083  0.24659544]]. Action = [[-0.20294562  0.11337417 -0.20676814 -0.22466487]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 2135 is [True, False, False, False, False, True]
State prediction error at timestep 2135 is tensor(4.4793e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2136. State = [[-0.2437362   0.24665779]]. Action = [[-0.04509386 -0.21632415 -0.09385018 -0.22623432]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 2136 is [True, False, False, False, False, True]
State prediction error at timestep 2136 is tensor(8.9263e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2137. State = [[-0.2437362   0.24665779]]. Action = [[ 0.10290366  0.14505816 -0.1314233  -0.09640229]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 2137 is [True, False, False, False, False, True]
State prediction error at timestep 2137 is tensor(1.6240e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2138. State = [[-0.2437362   0.24665779]]. Action = [[-0.17180462  0.1784383  -0.01279025  0.8287667 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 2138 is [True, False, False, False, False, True]
State prediction error at timestep 2138 is tensor(1.5048e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2138 of -1
Current timestep = 2139. State = [[-0.2437362   0.24665779]]. Action = [[-0.09896189  0.15670943  0.22362947 -0.8341067 ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 2139 is [True, False, False, False, False, True]
State prediction error at timestep 2139 is tensor(5.3388e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2140. State = [[-0.2437362   0.24665779]]. Action = [[ 0.18889424  0.11098778 -0.09698126 -0.3398764 ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 2140 is [True, False, False, False, False, True]
State prediction error at timestep 2140 is tensor(3.7106e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2141. State = [[-0.24387383  0.24682823]]. Action = [[-0.09928089  0.07631534 -0.1722735  -0.29906857]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 2141 is [True, False, False, False, False, True]
State prediction error at timestep 2141 is tensor(8.7461e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2141 of -1
Current timestep = 2142. State = [[-0.24451366  0.2475959 ]]. Action = [[0.18158674 0.06343624 0.11915168 0.1133002 ]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 2142 is [True, False, False, False, False, True]
State prediction error at timestep 2142 is tensor(3.4682e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2143. State = [[-0.24481341  0.2481423 ]]. Action = [[0.19289222 0.03005776 0.17290789 0.5775397 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 2143 is [True, False, False, False, False, True]
State prediction error at timestep 2143 is tensor(6.3401e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2144. State = [[-0.24525301  0.24870513]]. Action = [[-0.20912449  0.2141496  -0.05133075  0.93414855]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 2144 is [True, False, False, False, False, True]
State prediction error at timestep 2144 is tensor(2.0057e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2145. State = [[-0.2462848   0.24996868]]. Action = [[ 0.12773222 -0.09271163  0.15908474  0.86460733]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 2145 is [True, False, False, False, False, True]
State prediction error at timestep 2145 is tensor(4.1555e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2145 of -1
Current timestep = 2146. State = [[-0.2456169   0.24936143]]. Action = [[-0.14789602 -0.22826524  0.09713441 -0.76104647]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 2146 is [True, False, False, False, False, True]
State prediction error at timestep 2146 is tensor(2.2804e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2147. State = [[-0.24485037  0.24849072]]. Action = [[ 0.12322575 -0.00105079  0.17374891  0.97883725]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 2147 is [True, False, False, False, False, True]
State prediction error at timestep 2147 is tensor(1.2423e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2147 of -1
Current timestep = 2148. State = [[-0.241805    0.24760133]]. Action = [[ 0.05783013 -0.08667597  0.11896729 -0.29100728]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 2148 is [True, False, False, False, False, True]
State prediction error at timestep 2148 is tensor(1.7146e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2148 of -1
Current timestep = 2149. State = [[-0.240844    0.24691066]]. Action = [[-0.2367109  -0.24389559  0.14255756  0.25679922]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 2149 is [True, False, False, False, False, True]
State prediction error at timestep 2149 is tensor(3.4456e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2150. State = [[-0.24016273  0.24589711]]. Action = [[-0.081285  -0.1903868  0.2377069 -0.6385764]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 2150 is [True, False, False, False, False, True]
State prediction error at timestep 2150 is tensor(1.0724e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2151. State = [[-0.23960853  0.24508049]]. Action = [[-0.16860397  0.03823286 -0.15275696 -0.8289592 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 2151 is [True, False, False, False, False, True]
State prediction error at timestep 2151 is tensor(3.6264e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2152. State = [[-0.23912887  0.24477838]]. Action = [[ 0.05730665 -0.1539772   0.09886292 -0.31957185]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 2152 is [True, False, False, False, False, True]
State prediction error at timestep 2152 is tensor(1.0440e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2153. State = [[-0.23846795  0.24407102]]. Action = [[-0.19656602  0.24193048 -0.1975911   0.54186034]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 2153 is [True, False, False, False, False, True]
State prediction error at timestep 2153 is tensor(4.2190e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2153 of -1
Current timestep = 2154. State = [[-0.23801647  0.24328336]]. Action = [[ 0.1522708  -0.24062634 -0.1424067  -0.2210958 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 2154 is [True, False, False, False, False, True]
State prediction error at timestep 2154 is tensor(2.2306e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2155. State = [[-0.23718558  0.24216136]]. Action = [[ 0.07333609  0.09341267  0.15048885 -0.9476691 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 2155 is [True, False, False, False, False, True]
State prediction error at timestep 2155 is tensor(1.0256e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2155 of 1
Current timestep = 2156. State = [[-0.23687549  0.24230456]]. Action = [[0.02185348 0.14333633 0.15541637 0.76129067]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 2156 is [True, False, False, False, False, True]
State prediction error at timestep 2156 is tensor(4.5285e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2157. State = [[-0.23644957  0.24272071]]. Action = [[-0.2162666  -0.18348758  0.00987756  0.10797954]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 2157 is [True, False, False, False, False, True]
State prediction error at timestep 2157 is tensor(9.3087e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2158. State = [[-0.23601806  0.24296205]]. Action = [[ 0.05899432 -0.20382062 -0.22154301 -0.50247633]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 2158 is [True, False, False, False, False, True]
State prediction error at timestep 2158 is tensor(2.6727e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2159. State = [[-0.23556381  0.24313927]]. Action = [[-0.14165163 -0.06745045  0.14410484 -0.68880534]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 2159 is [True, False, False, False, False, True]
State prediction error at timestep 2159 is tensor(1.4929e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2160. State = [[-0.2351306   0.24340606]]. Action = [[-0.04309951 -0.17006466  0.08387429 -0.6157351 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 2160 is [True, False, False, False, False, True]
State prediction error at timestep 2160 is tensor(1.7808e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2160 of 1
Current timestep = 2161. State = [[-0.23422363  0.24408458]]. Action = [[ 0.05829564 -0.07875901  0.1352449  -0.16739225]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 2161 is [True, False, False, False, False, True]
State prediction error at timestep 2161 is tensor(3.5970e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2162. State = [[-0.23335896  0.24333547]]. Action = [[ 0.24355757 -0.24012172 -0.10752681 -0.8747468 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 2162 is [True, False, False, False, False, True]
State prediction error at timestep 2162 is tensor(5.7709e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2163. State = [[-0.23256785  0.24269329]]. Action = [[ 0.17070511 -0.02650072  0.12432104 -0.32256973]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 2163 is [True, False, False, False, False, True]
State prediction error at timestep 2163 is tensor(4.7983e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2164. State = [[-0.2320885   0.24204372]]. Action = [[-0.22415781 -0.02441265 -0.11989403  0.53942895]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 2164 is [True, False, False, False, False, True]
State prediction error at timestep 2164 is tensor(1.6562e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2165. State = [[-0.23195627  0.24183597]]. Action = [[ 0.11563945 -0.14169694 -0.20240505  0.958792  ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 2165 is [True, False, False, False, False, True]
State prediction error at timestep 2165 is tensor(8.2959e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2166. State = [[-0.2313864  0.2414864]]. Action = [[-0.19598538  0.04900488 -0.13379556 -0.76306385]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 2166 is [True, False, False, False, False, True]
State prediction error at timestep 2166 is tensor(2.0358e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2166 of 1
Current timestep = 2167. State = [[-0.23108765  0.24095814]]. Action = [[-0.00655161 -0.17883807 -0.2295282   0.7716689 ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 2167 is [True, False, False, False, False, True]
State prediction error at timestep 2167 is tensor(4.7171e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2168. State = [[-0.230879    0.24052425]]. Action = [[ 0.23814455  0.23277444  0.11013773 -0.60119617]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 2168 is [True, False, False, False, False, True]
State prediction error at timestep 2168 is tensor(6.6296e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2169. State = [[-0.23074958  0.2402191 ]]. Action = [[-0.00505158 -0.2018441  -0.07371725 -0.7425886 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 2169 is [True, False, False, False, False, True]
State prediction error at timestep 2169 is tensor(2.7613e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2170. State = [[-0.23057231  0.23987205]]. Action = [[-0.18296263  0.06804788 -0.21349472 -0.5415532 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 2170 is [True, False, False, False, False, True]
State prediction error at timestep 2170 is tensor(2.1345e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2171. State = [[-0.23043035  0.23982008]]. Action = [[ 0.1448364   0.20908806  0.16527656 -0.5036484 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 2171 is [True, False, False, False, False, True]
State prediction error at timestep 2171 is tensor(2.4842e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2171 of 1
Current timestep = 2172. State = [[-0.2300308   0.23926474]]. Action = [[ 0.11774769  0.00981712  0.03108516 -0.13547093]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 2172 is [True, False, False, False, False, True]
State prediction error at timestep 2172 is tensor(5.6877e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2173. State = [[-0.22699963  0.23903221]]. Action = [[ 0.02320921  0.05911803 -0.20726354 -0.15978831]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 2173 is [True, False, False, False, False, True]
State prediction error at timestep 2173 is tensor(3.9422e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2173 of 1
Current timestep = 2174. State = [[-0.22380221  0.24043058]]. Action = [[-0.08143537  0.07103044  0.02315453 -0.49457145]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 2174 is [True, False, False, False, False, True]
State prediction error at timestep 2174 is tensor(2.3141e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2174 of 1
Current timestep = 2175. State = [[-0.2236256   0.24164353]]. Action = [[-0.12471443  0.2129274  -0.1448642   0.7379805 ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 2175 is [True, False, False, False, False, True]
State prediction error at timestep 2175 is tensor(1.4138e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2176. State = [[-0.22220539  0.24357224]]. Action = [[ 0.00531471 -0.0111389   0.11842328 -0.6867235 ]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 2176 is [True, False, False, False, False, True]
State prediction error at timestep 2176 is tensor(2.6648e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2176 of 1
Current timestep = 2177. State = [[-0.2219358   0.24427694]]. Action = [[ 0.19194162 -0.12615635 -0.19042163  0.8495195 ]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 2177 is [True, False, False, False, False, True]
State prediction error at timestep 2177 is tensor(1.5495e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2178. State = [[-0.2209514   0.24505211]]. Action = [[-0.00955255 -0.02799785 -0.22555476  0.63012826]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 2178 is [True, False, False, False, False, True]
State prediction error at timestep 2178 is tensor(5.5785e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2178 of 1
Current timestep = 2179. State = [[-0.22096165  0.24518603]]. Action = [[ 0.1560398  -0.06611107 -0.0097376   0.02791965]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 2179 is [True, False, False, False, False, True]
State prediction error at timestep 2179 is tensor(1.0399e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2180. State = [[-0.22103226  0.2450613 ]]. Action = [[-0.0869135  -0.19183482  0.15072045  0.6708572 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 2180 is [True, False, False, False, False, True]
State prediction error at timestep 2180 is tensor(6.3715e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2181. State = [[-0.22096956  0.2450148 ]]. Action = [[ 0.06784412 -0.19808078 -0.04300982  0.72875917]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 2181 is [True, False, False, False, False, True]
State prediction error at timestep 2181 is tensor(3.2133e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2182. State = [[-0.22079293  0.24488874]]. Action = [[ 0.11143202 -0.04397152 -0.09567745  0.4139545 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 2182 is [True, False, False, False, False, True]
State prediction error at timestep 2182 is tensor(7.1063e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2182 of 1
Current timestep = 2183. State = [[-0.22004117  0.24437657]]. Action = [[ 0.10609895  0.16026449  0.21360886 -0.30610824]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 2183 is [True, False, False, False, False, True]
State prediction error at timestep 2183 is tensor(5.3297e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2184. State = [[-0.21942686  0.24411356]]. Action = [[ 0.24645549  0.04360825  0.21469751 -0.76895493]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 2184 is [True, False, False, False, False, True]
State prediction error at timestep 2184 is tensor(2.1009e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2185. State = [[-0.21906932  0.2439741 ]]. Action = [[ 0.19846994  0.13902378  0.18779492 -0.8365251 ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 2185 is [True, False, False, False, False, True]
State prediction error at timestep 2185 is tensor(1.3013e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2186. State = [[-0.21874776  0.24397992]]. Action = [[-0.01808096  0.18234468  0.04119375 -0.33383882]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 2186 is [True, False, False, False, False, True]
State prediction error at timestep 2186 is tensor(2.6487e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2186 of 1
Current timestep = 2187. State = [[-0.2180623   0.24370554]]. Action = [[-0.15951449 -0.00132608 -0.09733927 -0.66493714]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 2187 is [True, False, False, False, False, True]
State prediction error at timestep 2187 is tensor(2.9367e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2188. State = [[-0.21697298  0.24332055]]. Action = [[-0.08269283 -0.08211215 -0.06536762 -0.69755787]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 2188 is [True, False, False, False, False, True]
State prediction error at timestep 2188 is tensor(2.7279e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2188 of 1
Current timestep = 2189. State = [[-0.21681517  0.24251918]]. Action = [[ 0.2159248   0.20329908 -0.11592355  0.8345207 ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 2189 is [True, False, False, False, False, True]
State prediction error at timestep 2189 is tensor(4.1260e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2190. State = [[-0.21676642  0.24239072]]. Action = [[-0.19804528  0.24112144  0.1726368   0.32922816]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 2190 is [True, False, False, False, False, True]
State prediction error at timestep 2190 is tensor(1.0918e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2191. State = [[-0.21629775  0.24144322]]. Action = [[-0.0452453  -0.09217274 -0.12623964 -0.03126448]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 2191 is [True, False, False, False, False, True]
State prediction error at timestep 2191 is tensor(1.1959e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2191 of 1
Current timestep = 2192. State = [[-0.21600494  0.24073242]]. Action = [[ 0.08903378  0.24764559 -0.20382759 -0.74305075]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 2192 is [True, False, False, False, False, True]
State prediction error at timestep 2192 is tensor(9.5318e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2193. State = [[-0.2158354   0.24026963]]. Action = [[-0.00678422  0.15283906  0.17616549  0.8019712 ]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 2193 is [True, False, False, False, False, True]
State prediction error at timestep 2193 is tensor(1.9968e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2194. State = [[-0.21526226  0.23870128]]. Action = [[ 0.11888549 -0.09412217 -0.10750186 -0.677623  ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 2194 is [True, False, False, False, False, True]
State prediction error at timestep 2194 is tensor(6.0711e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2194 of 1
Current timestep = 2195. State = [[-0.21420845  0.2369817 ]]. Action = [[-0.03027269  0.14124542  0.14173067 -0.2509898 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 2195 is [True, False, False, False, False, True]
State prediction error at timestep 2195 is tensor(3.3494e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2196. State = [[-0.21357378  0.23576824]]. Action = [[ 0.20039865  0.17570072  0.234541   -0.53650486]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 2196 is [True, False, False, False, False, True]
State prediction error at timestep 2196 is tensor(1.7134e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2197. State = [[-0.2131243  0.2348703]]. Action = [[-0.08798532  0.22898608 -0.11438072 -0.04573369]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 2197 is [True, False, False, False, False, True]
State prediction error at timestep 2197 is tensor(6.1544e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2198. State = [[-0.21274133  0.23414065]]. Action = [[ 0.1979258   0.18666852 -0.08686279  0.35341048]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 2198 is [True, False, False, False, False, True]
State prediction error at timestep 2198 is tensor(2.9305e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2198 of 1
Current timestep = 2199. State = [[-0.2121298   0.23276936]]. Action = [[-0.16468585  0.19072193 -0.14980051 -0.7058399 ]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 2199 is [True, False, False, False, False, True]
State prediction error at timestep 2199 is tensor(1.4888e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2200. State = [[-0.21173307  0.23183712]]. Action = [[ 0.22093943  0.01540843  0.17815518 -0.43374074]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 2200 is [True, False, False, False, False, True]
State prediction error at timestep 2200 is tensor(7.4500e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2201. State = [[-0.21157506  0.23122582]]. Action = [[ 0.14947861  0.08883357  0.00873235 -0.9397713 ]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 2201 is [True, False, False, False, False, True]
State prediction error at timestep 2201 is tensor(1.0793e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2202. State = [[-0.21137048  0.23056266]]. Action = [[-0.11797632 -0.17088437 -0.0933944  -0.5154367 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 2202 is [True, False, False, False, False, True]
State prediction error at timestep 2202 is tensor(2.0209e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2202 of 1
Current timestep = 2203. State = [[-0.21107948  0.2297993 ]]. Action = [[ 0.24190944  0.07835025  0.19581592 -0.84696203]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 2203 is [True, False, False, False, False, True]
State prediction error at timestep 2203 is tensor(3.4836e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2204. State = [[-0.21096548  0.2294758 ]]. Action = [[-0.1882521   0.09850508 -0.01829731  0.6668751 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 2204 is [True, False, False, False, False, True]
State prediction error at timestep 2204 is tensor(8.7436e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2205. State = [[-0.21080616  0.22902262]]. Action = [[ 0.1230503  -0.17133701  0.14907104 -0.8585819 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 2205 is [True, False, False, False, False, True]
State prediction error at timestep 2205 is tensor(1.1237e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2206. State = [[-0.21040563  0.22795983]]. Action = [[ 0.10967577  0.02085492 -0.22020523 -0.7450798 ]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 2206 is [True, False, False, False, False, True]
State prediction error at timestep 2206 is tensor(1.1046e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2206 of 1
Current timestep = 2207. State = [[-0.20995528  0.227734  ]]. Action = [[-0.22920017 -0.1258865   0.22946036  0.04195106]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 2207 is [True, False, False, False, False, True]
State prediction error at timestep 2207 is tensor(4.0074e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2208. State = [[-0.2094511  0.2271261]]. Action = [[-0.07441331 -0.05806628 -0.17723319 -0.96022874]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 2208 is [True, False, False, False, False, True]
State prediction error at timestep 2208 is tensor(1.5269e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2208 of 1
Current timestep = 2209. State = [[-0.20928779  0.22658129]]. Action = [[-0.07315828  0.17760777  0.12108842  0.09449196]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 2209 is [True, False, False, False, False, True]
State prediction error at timestep 2209 is tensor(9.1506e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2210. State = [[-0.2091982   0.22626105]]. Action = [[-0.1608007   0.23825592  0.09627986 -0.4088527 ]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 2210 is [True, False, False, False, False, True]
State prediction error at timestep 2210 is tensor(5.1324e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2211. State = [[-0.20890844  0.22544166]]. Action = [[ 0.11598605  0.02805847 -0.04760046  0.8805902 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 2211 is [True, False, False, False, False, True]
State prediction error at timestep 2211 is tensor(1.2102e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2211 of 1
Current timestep = 2212. State = [[-0.20832515  0.22526972]]. Action = [[-0.13572295  0.05378246  0.01089564  0.5823691 ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 2212 is [True, False, False, False, False, True]
State prediction error at timestep 2212 is tensor(9.2897e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2213. State = [[-0.20823546  0.22497582]]. Action = [[ 0.05242682  0.23898396 -0.09095478  0.67052054]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 2213 is [True, False, False, False, False, True]
State prediction error at timestep 2213 is tensor(1.9897e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2214. State = [[-0.20821641  0.22482423]]. Action = [[-0.20267801 -0.07349595 -0.16087064  0.83323956]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 2214 is [True, False, False, False, False, True]
State prediction error at timestep 2214 is tensor(2.4042e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2214 of 1
Current timestep = 2215. State = [[-0.20766021  0.22504449]]. Action = [[ 0.04552323 -0.19075987  0.02766365  0.5180385 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 2215 is [True, False, False, False, False, True]
State prediction error at timestep 2215 is tensor(1.0929e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2216. State = [[-0.2073712   0.22504993]]. Action = [[-0.16794771  0.01083246  0.10111558 -0.36025727]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 2216 is [True, False, False, False, False, True]
State prediction error at timestep 2216 is tensor(1.3329e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2217. State = [[-0.2060672   0.22445448]]. Action = [[-0.0089312  -0.09216702  0.12291646  0.39901257]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 2217 is [True, False, False, False, False, True]
State prediction error at timestep 2217 is tensor(1.7329e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2218. State = [[-0.20577455  0.22369832]]. Action = [[ 0.1392656  -0.1423711   0.14288038  0.1219449 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 2218 is [True, False, False, False, False, True]
State prediction error at timestep 2218 is tensor(1.0680e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2219. State = [[-0.20543686  0.22296855]]. Action = [[ 0.09477636  0.14427695  0.20719475 -0.54223007]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 2219 is [True, False, False, False, False, True]
State prediction error at timestep 2219 is tensor(2.5127e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2220. State = [[-0.20517507  0.22280915]]. Action = [[ 0.02964258 -0.22777541 -0.034036    0.98113084]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 2220 is [True, False, False, False, False, True]
State prediction error at timestep 2220 is tensor(3.3059e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2221. State = [[-0.20450251  0.22166389]]. Action = [[-0.08092171  0.10712957  0.23476392 -0.8156675 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 2221 is [True, False, False, False, False, True]
State prediction error at timestep 2221 is tensor(6.0639e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2221 of 1
Current timestep = 2222. State = [[-0.20499979  0.22243735]]. Action = [[ 0.12676826 -0.10489416 -0.04578742 -0.55859995]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 2222 is [True, False, False, False, False, True]
State prediction error at timestep 2222 is tensor(5.3459e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2222 of 1
Current timestep = 2223. State = [[-0.20441343  0.22198424]]. Action = [[-0.14129898 -0.1699944  -0.22239314 -0.864727  ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 2223 is [True, False, False, False, False, True]
State prediction error at timestep 2223 is tensor(2.0747e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2224. State = [[-0.20384893  0.2215581 ]]. Action = [[-0.23022757 -0.01222439 -0.16548797 -0.2906083 ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 2224 is [True, False, False, False, False, True]
State prediction error at timestep 2224 is tensor(4.8600e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2225. State = [[-0.20369282  0.22101018]]. Action = [[-0.07080254  0.22832331 -0.14528696 -0.05625087]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 2225 is [True, False, False, False, False, True]
State prediction error at timestep 2225 is tensor(2.5018e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2226. State = [[-0.20338647  0.22068933]]. Action = [[0.06623766 0.22509116 0.15313908 0.77956796]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 2226 is [True, False, False, False, False, True]
State prediction error at timestep 2226 is tensor(7.4996e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2226 of 1
Current timestep = 2227. State = [[-0.20279846  0.22032672]]. Action = [[ 0.20252025  0.03257996 -0.16518134  0.06015098]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 2227 is [True, False, False, False, False, True]
State prediction error at timestep 2227 is tensor(1.4800e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2228. State = [[-0.20252249  0.21983413]]. Action = [[ 0.23443764  0.16994762 -0.12842155 -0.38392448]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 2228 is [True, False, False, False, False, True]
State prediction error at timestep 2228 is tensor(1.6124e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2229. State = [[-0.20213504  0.21980022]]. Action = [[-0.18643014  0.23505214  0.13249871  0.11148119]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 2229 is [True, False, False, False, False, True]
State prediction error at timestep 2229 is tensor(1.6089e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2230. State = [[-0.20208286  0.21941525]]. Action = [[ 0.22018927  0.21542054 -0.08935903  0.790256  ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 2230 is [True, False, False, False, False, True]
State prediction error at timestep 2230 is tensor(2.4662e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2231. State = [[-0.20174067  0.21934956]]. Action = [[ 0.21478963 -0.21144977 -0.21820544  0.06287777]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 2231 is [True, False, False, False, False, True]
State prediction error at timestep 2231 is tensor(1.2615e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2231 of 1
Current timestep = 2232. State = [[-0.2016093   0.21906798]]. Action = [[-0.2155078  -0.05476387 -0.19078168 -0.51380086]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 2232 is [True, False, False, False, False, True]
State prediction error at timestep 2232 is tensor(1.0744e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2233. State = [[-0.20119232  0.21855973]]. Action = [[ 0.05825171 -0.12620284  0.0391137   0.22308135]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 2233 is [True, False, False, False, False, True]
State prediction error at timestep 2233 is tensor(5.2895e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2233 of 1
Current timestep = 2234. State = [[-0.20025855  0.2172848 ]]. Action = [[ 0.03936556  0.1981273   0.11686534 -0.75457907]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 2234 is [True, False, False, False, False, True]
State prediction error at timestep 2234 is tensor(1.1960e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2235. State = [[-0.19804487  0.21367015]]. Action = [[ 0.00466213 -0.09911755  0.08265394  0.04360676]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 2235 is [True, False, False, False, False, True]
State prediction error at timestep 2235 is tensor(4.0919e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2235 of 1
Current timestep = 2236. State = [[-0.1970583   0.21186681]]. Action = [[-0.06065328 -0.22446693  0.11570799 -0.5930075 ]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 2236 is [True, False, False, False, False, True]
State prediction error at timestep 2236 is tensor(1.2438e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2237. State = [[-0.1964698   0.21075252]]. Action = [[ 0.03751579 -0.17240551 -0.05191925  0.6636164 ]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 2237 is [True, False, False, False, False, True]
State prediction error at timestep 2237 is tensor(1.7997e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2238. State = [[-0.19593178  0.20980638]]. Action = [[-0.17373633  0.16985118  0.10329032 -0.24730688]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 2238 is [True, False, False, False, False, True]
State prediction error at timestep 2238 is tensor(7.0107e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2239. State = [[-0.19547457  0.20885885]]. Action = [[ 0.16591555 -0.19220228  0.11386269 -0.6106591 ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 2239 is [True, False, False, False, False, True]
State prediction error at timestep 2239 is tensor(1.2485e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2240. State = [[-0.19491085  0.2075676 ]]. Action = [[ 0.07333872  0.23854187  0.16510314 -0.2835452 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 2240 is [True, False, False, False, False, True]
State prediction error at timestep 2240 is tensor(7.1141e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2240 of 1
Current timestep = 2241. State = [[-0.19463117  0.20690557]]. Action = [[-0.22188836 -0.21709266  0.15545762 -0.58849895]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 2241 is [True, False, False, False, False, True]
State prediction error at timestep 2241 is tensor(2.3588e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2242. State = [[-0.19449915  0.20643765]]. Action = [[ 0.1083456  -0.22697552 -0.14297454 -0.48733002]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 2242 is [True, False, False, False, False, True]
State prediction error at timestep 2242 is tensor(1.0406e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2243. State = [[-0.19429588  0.205861  ]]. Action = [[-0.04531917 -0.22609115 -0.01739711  0.7340106 ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 2243 is [True, False, False, False, False, True]
State prediction error at timestep 2243 is tensor(2.5115e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2244. State = [[-0.19404234  0.20531844]]. Action = [[-0.09299473  0.24694794 -0.06153738  0.7039521 ]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 2244 is [True, False, False, False, False, True]
State prediction error at timestep 2244 is tensor(1.6063e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2245. State = [[-0.19393268  0.20492476]]. Action = [[-0.24732488 -0.00818148  0.22175574 -0.1489566 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 2245 is [True, False, False, False, False, True]
State prediction error at timestep 2245 is tensor(8.6108e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2245 of 1
Current timestep = 2246. State = [[-0.1938712   0.20464851]]. Action = [[ 0.01005775 -0.19983505  0.00795865 -0.36596584]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 2246 is [True, False, False, False, False, True]
State prediction error at timestep 2246 is tensor(2.6325e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2247. State = [[-0.1935268   0.20369838]]. Action = [[ 0.03920782  0.11509979 -0.18710652  0.5246053 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 2247 is [True, False, False, False, False, True]
State prediction error at timestep 2247 is tensor(4.9455e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2248. State = [[-0.19362377  0.20404   ]]. Action = [[-0.15404409  0.10779485 -0.1667095  -0.32895672]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 2248 is [True, False, False, False, False, True]
State prediction error at timestep 2248 is tensor(6.1488e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2249. State = [[-0.19376913  0.2042806 ]]. Action = [[ 0.24656847  0.05516371 -0.16263743  0.55182624]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 2249 is [True, False, False, False, False, True]
State prediction error at timestep 2249 is tensor(1.1983e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2250. State = [[-0.19376913  0.2042806 ]]. Action = [[-0.05812681 -0.18517937  0.02035627  0.6044897 ]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 2250 is [True, False, False, False, False, True]
State prediction error at timestep 2250 is tensor(2.3239e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2251. State = [[-0.19400136  0.20476943]]. Action = [[-0.08931388 -0.00703746  0.19078064  0.5975939 ]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 2251 is [True, False, False, False, False, True]
State prediction error at timestep 2251 is tensor(7.1960e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2251 of 1
Current timestep = 2252. State = [[-0.19439729  0.20542756]]. Action = [[-0.01314132 -0.00514537  0.22939393 -0.92564195]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 2252 is [True, False, False, False, False, True]
State prediction error at timestep 2252 is tensor(3.7486e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2253. State = [[-0.19446835  0.2054426 ]]. Action = [[ 0.08615327 -0.19131742  0.0976944   0.3948524 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 2253 is [True, False, False, False, False, True]
State prediction error at timestep 2253 is tensor(8.3796e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2253 of 1
Current timestep = 2254. State = [[-0.1945157   0.20558813]]. Action = [[-0.15822196 -0.11517742  0.02679896 -0.826547  ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 2254 is [True, False, False, False, False, True]
State prediction error at timestep 2254 is tensor(2.1871e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2255. State = [[-0.19458328  0.20577091]]. Action = [[-0.20210977  0.12075424 -0.00312738  0.8329077 ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 2255 is [True, False, False, False, False, True]
State prediction error at timestep 2255 is tensor(2.3185e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2256. State = [[-0.194682    0.20586547]]. Action = [[ 0.23175049 -0.0159723  -0.23716387 -0.9415558 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 2256 is [True, False, False, False, False, True]
State prediction error at timestep 2256 is tensor(7.5711e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2257. State = [[-0.19465655  0.2058766 ]]. Action = [[-0.0959582   0.2269634   0.06581399  0.5397315 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 2257 is [True, False, False, False, False, True]
State prediction error at timestep 2257 is tensor(1.7838e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2258. State = [[-0.19477478  0.20603658]]. Action = [[ 0.16036123 -0.19158834 -0.01413569  0.04592967]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 2258 is [True, False, False, False, False, True]
State prediction error at timestep 2258 is tensor(5.8660e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2258 of 1
Current timestep = 2259. State = [[-0.19481161  0.20608966]]. Action = [[ 0.23877832 -0.19677186  0.0035232  -0.65444607]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 2259 is [True, False, False, False, False, True]
State prediction error at timestep 2259 is tensor(5.1224e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2260. State = [[-0.19479178  0.20602374]]. Action = [[0.15592033 0.10132796 0.20231825 0.58674574]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 2260 is [True, False, False, False, False, True]
State prediction error at timestep 2260 is tensor(1.9882e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2261. State = [[-0.1948481   0.20614223]]. Action = [[ 0.01203322  0.23728085 -0.08143985 -0.39044064]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 2261 is [True, False, False, False, False, True]
State prediction error at timestep 2261 is tensor(3.1640e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2262. State = [[-0.19488496  0.20619531]]. Action = [[-0.19219649  0.20229682  0.11027315 -0.5619452 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 2262 is [True, False, False, False, False, True]
State prediction error at timestep 2262 is tensor(8.8844e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2262 of 1
Current timestep = 2263. State = [[-0.19487616  0.20619312]]. Action = [[-0.09032494 -0.11591019  0.16846788 -0.6673109 ]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 2263 is [True, False, False, False, False, True]
State prediction error at timestep 2263 is tensor(1.1998e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2264. State = [[-0.19476616  0.20580019]]. Action = [[-0.03209201  0.20493978  0.03343186 -0.28073537]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 2264 is [True, False, False, False, False, True]
State prediction error at timestep 2264 is tensor(4.0407e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2264 of 1
Current timestep = 2265. State = [[-0.194617    0.20524614]]. Action = [[ 0.08344284  0.08280897  0.17908484 -0.86274046]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 2265 is [True, False, False, False, False, True]
State prediction error at timestep 2265 is tensor(2.1802e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2266. State = [[-0.19455473  0.20523335]]. Action = [[ 0.2166574   0.1731728  -0.03396025  0.28978384]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 2266 is [True, False, False, False, False, True]
State prediction error at timestep 2266 is tensor(6.6454e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2267. State = [[-0.19452928  0.20524445]]. Action = [[-0.13663462  0.1620894   0.14775047  0.2732365 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 2267 is [True, False, False, False, False, True]
State prediction error at timestep 2267 is tensor(1.8454e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2268. State = [[-0.19455473  0.20523335]]. Action = [[-0.19366604  0.17782164 -0.0649527   0.37817335]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 2268 is [True, False, False, False, False, True]
State prediction error at timestep 2268 is tensor(7.8745e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2269. State = [[-0.1945942   0.20536526]]. Action = [[ 0.05429491  0.04823944  0.10954511 -0.4680885 ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 2269 is [True, False, False, False, False, True]
State prediction error at timestep 2269 is tensor(2.9299e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2269 of 1
Current timestep = 2270. State = [[-0.19465335  0.20556264]]. Action = [[ 0.04963803 -0.17341845 -0.02181265 -0.6385154 ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 2270 is [True, False, False, False, False, True]
State prediction error at timestep 2270 is tensor(9.3035e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2271. State = [[-0.19464767  0.2056397 ]]. Action = [[-0.17424119 -0.17209806 -0.05930424  0.74420106]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 2271 is [True, False, False, False, False, True]
State prediction error at timestep 2271 is tensor(1.8377e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2272. State = [[-0.19470957  0.20568117]]. Action = [[ 0.16828907 -0.23059699  0.04443607  0.7701266 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 2272 is [True, False, False, False, False, True]
State prediction error at timestep 2272 is tensor(2.1591e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2273. State = [[-0.19467312  0.20562857]]. Action = [[ 0.18970662 -0.0221858   0.06793505  0.2348206 ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 2273 is [True, False, False, False, False, True]
State prediction error at timestep 2273 is tensor(1.4018e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2274. State = [[-0.19492233  0.20607735]]. Action = [[-0.12475947  0.06954622 -0.13351887  0.2613337 ]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 2274 is [True, False, False, False, False, True]
State prediction error at timestep 2274 is tensor(5.0285e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2274 of 1
Current timestep = 2275. State = [[-0.19571169  0.20724028]]. Action = [[ 0.05512238  0.22228628  0.15907335 -0.44342923]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 2275 is [True, False, False, False, False, True]
State prediction error at timestep 2275 is tensor(3.3517e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2276. State = [[-0.19618036  0.20827635]]. Action = [[ 0.18238449 -0.00040281  0.13584208  0.0253967 ]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 2276 is [True, False, False, False, False, True]
State prediction error at timestep 2276 is tensor(5.6658e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2277. State = [[-0.19639574  0.20869312]]. Action = [[ 0.20011055  0.1043492  -0.03983387 -0.89268404]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 2277 is [True, False, False, False, False, True]
State prediction error at timestep 2277 is tensor(8.4010e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2277 of 1
Current timestep = 2278. State = [[-0.19677565  0.20901163]]. Action = [[-0.23456578 -0.15902074 -0.03137845 -0.12743461]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 2278 is [True, False, False, False, False, True]
State prediction error at timestep 2278 is tensor(2.5451e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2279. State = [[-0.1971804  0.2097741]]. Action = [[ 0.07719943 -0.15487748 -0.1260209   0.31099415]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 2279 is [True, False, False, False, False, True]
State prediction error at timestep 2279 is tensor(1.0617e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2280. State = [[-0.19789286  0.21074006]]. Action = [[-0.0963904   0.01896536 -0.19095998 -0.3753922 ]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 2280 is [True, False, False, False, False, True]
State prediction error at timestep 2280 is tensor(1.8574e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2280 of 1
Current timestep = 2281. State = [[-0.19869573  0.21185052]]. Action = [[ 0.209297    0.06362224  0.02861142 -0.08662814]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 2281 is [True, False, False, False, False, True]
State prediction error at timestep 2281 is tensor(2.0973e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2282. State = [[-0.19890459  0.21231274]]. Action = [[-0.07958786 -0.16219881 -0.03996055 -0.95470226]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 2282 is [True, False, False, False, False, True]
State prediction error at timestep 2282 is tensor(2.4880e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2283. State = [[-0.19915329  0.21260314]]. Action = [[-0.20748794  0.19189975 -0.19098045 -0.29262078]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 2283 is [True, False, False, False, False, True]
State prediction error at timestep 2283 is tensor(3.9449e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2283 of 1
Current timestep = 2284. State = [[-0.19970246  0.21326299]]. Action = [[-2.3475666e-01 -7.3051453e-04  2.5162786e-02  7.3775887e-01]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 2284 is [True, False, False, False, False, True]
State prediction error at timestep 2284 is tensor(3.9657e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2285. State = [[-0.20005833  0.21385927]]. Action = [[ 0.10373446  0.24085176  0.21111435 -0.37970787]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 2285 is [True, False, False, False, False, True]
State prediction error at timestep 2285 is tensor(1.1967e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2286. State = [[-0.20025396  0.21409321]]. Action = [[-0.02524112 -0.20324564  0.0591076  -0.5034417 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 2286 is [True, False, False, False, False, True]
State prediction error at timestep 2286 is tensor(9.2108e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2287. State = [[-0.20086086  0.21487805]]. Action = [[-0.09699211 -0.07882722  0.00925013  0.94461954]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 2287 is [True, False, False, False, False, True]
State prediction error at timestep 2287 is tensor(2.6973e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2287 of 1
Current timestep = 2288. State = [[-0.20086078  0.2148493 ]]. Action = [[-0.2167768  -0.24356532 -0.01336151 -0.66099906]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 2288 is [True, False, False, False, False, True]
State prediction error at timestep 2288 is tensor(1.0351e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2289. State = [[-0.20127785  0.21509509]]. Action = [[-0.05755937  0.10269722  0.20976835 -0.906713  ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 2289 is [True, False, False, False, False, True]
State prediction error at timestep 2289 is tensor(1.0440e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2289 of 1
Current timestep = 2290. State = [[-0.20190604  0.21602906]]. Action = [[ 0.01919052  0.15065438 -0.23232397 -0.7057188 ]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 2290 is [True, False, False, False, False, True]
State prediction error at timestep 2290 is tensor(1.8293e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2291. State = [[-0.20245512  0.2168514 ]]. Action = [[-0.1651888   0.10111669 -0.12021227 -0.5516907 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 2291 is [True, False, False, False, False, True]
State prediction error at timestep 2291 is tensor(6.5389e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2292. State = [[-0.20271203  0.2172602 ]]. Action = [[ 0.24416167  0.1923025  -0.0701741   0.8027291 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 2292 is [True, False, False, False, False, True]
State prediction error at timestep 2292 is tensor(1.7380e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2292 of 1
Current timestep = 2293. State = [[-0.20370844  0.21843311]]. Action = [[-0.24002397 -0.12813443 -0.13781123  0.60693514]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 2293 is [True, False, False, False, False, True]
State prediction error at timestep 2293 is tensor(4.1598e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2294. State = [[-0.20491101  0.22006783]]. Action = [[ 0.04899663 -0.01813589  0.03243557 -0.3837666 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 2294 is [True, False, False, False, False, True]
State prediction error at timestep 2294 is tensor(4.9422e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2294 of -1
Current timestep = 2295. State = [[-0.20496833  0.21999958]]. Action = [[-0.18223396  0.2015276   0.16748911  0.3221023 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 2295 is [True, False, False, False, False, True]
State prediction error at timestep 2295 is tensor(1.7858e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2296. State = [[-0.20517926  0.2203553 ]]. Action = [[-0.05533253  0.08256817 -0.017055    0.499493  ]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 2296 is [True, False, False, False, False, True]
State prediction error at timestep 2296 is tensor(1.1661e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2296 of -1
Current timestep = 2297. State = [[-0.20568405  0.22096802]]. Action = [[0.19046336 0.01373157 0.23168874 0.05196416]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 2297 is [True, False, False, False, False, True]
State prediction error at timestep 2297 is tensor(1.1848e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2298. State = [[-0.2061146  0.2217057]]. Action = [[ 0.06170717  0.1389966  -0.05638471  0.8316705 ]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 2298 is [True, False, False, False, False, True]
State prediction error at timestep 2298 is tensor(1.0042e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2299. State = [[-0.20720643  0.22307071]]. Action = [[-0.00157844 -0.09201469  0.14009821  0.36484623]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 2299 is [True, False, False, False, False, True]
State prediction error at timestep 2299 is tensor(8.3677e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2299 of -1
Current timestep = 2300. State = [[-0.20739402  0.22263065]]. Action = [[ 0.132983    0.11370343 -0.09865005 -0.63163036]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 2300 is [True, False, False, False, False, True]
State prediction error at timestep 2300 is tensor(1.2447e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2300 of -1
Current timestep = 2301. State = [[-0.2075148   0.22286414]]. Action = [[-0.11292766 -0.15845011 -0.23149623  0.3921231 ]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 2301 is [True, False, False, False, False, True]
State prediction error at timestep 2301 is tensor(5.1074e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2302. State = [[-0.20751786  0.2231095 ]]. Action = [[ 0.10044339  0.06607848 -0.20796183 -0.8884099 ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 2302 is [True, False, False, False, False, True]
State prediction error at timestep 2302 is tensor(4.8531e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2303. State = [[-0.20746405  0.22349586]]. Action = [[ 0.148469    0.1478898  -0.08947164  0.38845587]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 2303 is [True, False, False, False, False, True]
State prediction error at timestep 2303 is tensor(1.4685e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2304. State = [[-0.2074891   0.22348385]]. Action = [[ 0.15167916  0.18228811  0.05081245 -0.23168391]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 2304 is [True, False, False, False, False, True]
State prediction error at timestep 2304 is tensor(2.6628e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2305. State = [[-0.20753576  0.2234476 ]]. Action = [[-0.15006481  0.06802648 -0.05357981  0.44392443]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 2305 is [True, False, False, False, False, True]
State prediction error at timestep 2305 is tensor(5.4914e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2306. State = [[-0.20749982  0.22361235]]. Action = [[ 0.2426303  -0.1888447   0.00921321 -0.2793306 ]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 2306 is [True, False, False, False, False, True]
State prediction error at timestep 2306 is tensor(5.8279e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2307. State = [[-0.20759657  0.22388661]]. Action = [[-0.1183811  -0.0866996   0.11905026 -0.8039778 ]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 2307 is [True, False, False, False, False, True]
State prediction error at timestep 2307 is tensor(8.3916e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2307 of -1
Current timestep = 2308. State = [[-0.20760739  0.223782  ]]. Action = [[ 0.15661725 -0.07716143  0.21469241  0.01391971]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 2308 is [True, False, False, False, False, True]
State prediction error at timestep 2308 is tensor(3.7851e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2309. State = [[-0.20758528  0.22371681]]. Action = [[-0.0732668  -0.13639633 -0.08535901 -0.3704034 ]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 2309 is [True, False, False, False, False, True]
State prediction error at timestep 2309 is tensor(1.0305e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2309 of -1
Current timestep = 2310. State = [[-0.20754698  0.22366554]]. Action = [[-0.17308721  0.23285729 -0.01070921 -0.346362  ]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 2310 is [True, False, False, False, False, True]
State prediction error at timestep 2310 is tensor(4.2829e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2311. State = [[-0.20748706  0.22352916]]. Action = [[ 0.12372741 -0.04771453 -0.22363319  0.08428693]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 2311 is [True, False, False, False, False, True]
State prediction error at timestep 2311 is tensor(1.6594e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2312. State = [[-0.20699444  0.22279496]]. Action = [[-0.01031628  0.21904677 -0.20026946 -0.4551016 ]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 2312 is [True, False, False, False, False, True]
State prediction error at timestep 2312 is tensor(8.1160e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2313. State = [[-0.20656456  0.2223017 ]]. Action = [[-0.09548804  0.13848466  0.06380999 -0.7137744 ]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 2313 is [True, False, False, False, False, True]
State prediction error at timestep 2313 is tensor(7.4359e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2314. State = [[-0.20631012  0.22209376]]. Action = [[ 0.19861013 -0.22264582  0.14887899 -0.4625848 ]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 2314 is [True, False, False, False, False, True]
State prediction error at timestep 2314 is tensor(3.2280e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2315. State = [[-0.2063443   0.22197443]]. Action = [[ 0.23484823 -0.0562609   0.1559267   0.76890934]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 2315 is [True, False, False, False, False, True]
State prediction error at timestep 2315 is tensor(9.1167e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2316. State = [[-0.20572877  0.22127478]]. Action = [[ 0.04694206 -0.04213229 -0.07265687 -0.8632284 ]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 2316 is [True, False, False, False, False, True]
State prediction error at timestep 2316 is tensor(2.2024e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2316 of -1
Current timestep = 2317. State = [[-0.20518844  0.22065045]]. Action = [[-0.00873622 -0.16120505  0.11603963  0.43313217]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 2317 is [True, False, False, False, False, True]
State prediction error at timestep 2317 is tensor(8.9991e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2318. State = [[-0.20465595  0.22002332]]. Action = [[0.19957823 0.01631811 0.12723118 0.32508445]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 2318 is [True, False, False, False, False, True]
State prediction error at timestep 2318 is tensor(1.2261e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2319. State = [[-0.20426153  0.21951416]]. Action = [[-0.10928367 -0.21910886  0.23984021 -0.23217666]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 2319 is [True, False, False, False, False, True]
State prediction error at timestep 2319 is tensor(4.8988e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2319 of -1
Current timestep = 2320. State = [[-0.20390262  0.21896581]]. Action = [[ 0.0568496   0.23177594  0.04992965 -0.16771775]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 2320 is [True, False, False, False, False, True]
State prediction error at timestep 2320 is tensor(8.0112e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2321. State = [[-0.20363519  0.21868025]]. Action = [[-0.1627527   0.23796383 -0.0201873  -0.214917  ]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 2321 is [True, False, False, False, False, True]
State prediction error at timestep 2321 is tensor(1.6890e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2322. State = [[-0.20324598  0.21808541]]. Action = [[-0.15321115  0.10993865  0.06142816  0.05843925]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 2322 is [True, False, False, False, False, True]
State prediction error at timestep 2322 is tensor(1.3459e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2322 of -1
Current timestep = 2323. State = [[-0.20324598  0.21808541]]. Action = [[ 0.15578705  0.1895155  -0.15277694 -0.44611597]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 2323 is [True, False, False, False, False, True]
State prediction error at timestep 2323 is tensor(7.8339e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2324. State = [[-0.20321105  0.21793908]]. Action = [[-0.11936772  0.02440161  0.14630425 -0.2605148 ]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 2324 is [True, False, False, False, False, True]
State prediction error at timestep 2324 is tensor(1.4404e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2324 of 1
Current timestep = 2325. State = [[-0.20334983  0.21808317]]. Action = [[-0.14983298  0.06075132 -0.07047541 -0.9813503 ]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 2325 is [True, False, False, False, False, True]
State prediction error at timestep 2325 is tensor(1.0816e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2326. State = [[-0.20358667  0.2185621 ]]. Action = [[-0.19506241 -0.07351814  0.14324683 -0.41687107]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 2326 is [True, False, False, False, False, True]
State prediction error at timestep 2326 is tensor(2.5106e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2327. State = [[-0.20374708  0.21877146]]. Action = [[-0.21748318  0.23651552 -0.11803445  0.19363904]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 2327 is [True, False, False, False, False, True]
State prediction error at timestep 2327 is tensor(8.6181e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2328. State = [[-0.20372562  0.21870604]]. Action = [[-0.08760136 -0.23550288 -0.13094504  0.11625195]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 2328 is [True, False, False, False, False, True]
State prediction error at timestep 2328 is tensor(2.2790e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2328 of 1
Current timestep = 2329. State = [[-0.20374231  0.21869278]]. Action = [[-0.16363402  0.17488566 -0.0801799   0.19511425]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 2329 is [True, False, False, False, False, True]
State prediction error at timestep 2329 is tensor(9.0678e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2330. State = [[-0.20380943  0.21878448]]. Action = [[0.05941719 0.04726836 0.20726734 0.1914382 ]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 2330 is [True, False, False, False, False, True]
State prediction error at timestep 2330 is tensor(3.8527e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2330 of 1
Current timestep = 2331. State = [[-0.20389017  0.21896695]]. Action = [[ 0.10290998  0.13460094 -0.07767206 -0.08289903]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 2331 is [True, False, False, False, False, True]
State prediction error at timestep 2331 is tensor(1.4687e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2332. State = [[-0.20404693  0.2192631 ]]. Action = [[-0.07209876 -0.1277373   0.14690852 -0.59460527]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 2332 is [True, False, False, False, False, True]
State prediction error at timestep 2332 is tensor(6.5262e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2333. State = [[-0.20385382  0.21866496]]. Action = [[-0.19946483 -0.10846591 -0.01989387 -0.81056285]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 2333 is [True, False, False, False, False, True]
State prediction error at timestep 2333 is tensor(6.8498e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2334. State = [[-0.20382687  0.21845625]]. Action = [[ 0.21310002 -0.01275869 -0.06302388  0.95506644]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 2334 is [True, False, False, False, False, True]
State prediction error at timestep 2334 is tensor(6.4513e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2335. State = [[-0.20367447  0.21831915]]. Action = [[ 0.18048263 -0.05346468 -0.1352928   0.53098416]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 2335 is [True, False, False, False, False, True]
State prediction error at timestep 2335 is tensor(3.5425e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2336. State = [[-0.20371151  0.21821558]]. Action = [[-0.05717358 -0.15443061  0.06193191 -0.20610821]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 2336 is [True, False, False, False, False, True]
State prediction error at timestep 2336 is tensor(2.1186e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2337. State = [[-0.20366873  0.21808472]]. Action = [[-0.13628243 -0.14196719 -0.08093324 -0.04192197]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 2337 is [True, False, False, False, False, True]
State prediction error at timestep 2337 is tensor(5.2487e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2338. State = [[-0.20360477  0.21788885]]. Action = [[-0.20986116  0.11994514  0.17333049  0.00299537]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 2338 is [True, False, False, False, False, True]
State prediction error at timestep 2338 is tensor(9.2848e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2339. State = [[-0.20356672  0.21783668]]. Action = [[ 0.23821276 -0.19695924 -0.14100823 -0.5868795 ]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 2339 is [True, False, False, False, False, True]
State prediction error at timestep 2339 is tensor(6.0564e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2339 of -1
Current timestep = 2340. State = [[-0.20347741  0.21765184]]. Action = [[-0.17201486 -0.08618692 -0.12743744 -0.9121629 ]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 2340 is [True, False, False, False, False, True]
State prediction error at timestep 2340 is tensor(1.1428e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2341. State = [[-0.20355771  0.21761814]]. Action = [[ 0.24756032 -0.22566    -0.15203463 -0.4111455 ]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 2341 is [True, False, False, False, False, True]
State prediction error at timestep 2341 is tensor(5.9002e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2342. State = [[-0.20343974  0.21760015]]. Action = [[ 0.14704698 -0.13979115 -0.04268464 -0.312073  ]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 2342 is [True, False, False, False, False, True]
State prediction error at timestep 2342 is tensor(1.0450e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2343. State = [[-0.20334673  0.21735384]]. Action = [[ 0.16990644  0.1862109   0.1375575  -0.9194073 ]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 2343 is [True, False, False, False, False, True]
State prediction error at timestep 2343 is tensor(1.2538e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2343 of -1
Current timestep = 2344. State = [[-0.20334673  0.21735384]]. Action = [[-0.1625025  -0.15673348  0.12928045  0.96196795]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 2344 is [True, False, False, False, False, True]
State prediction error at timestep 2344 is tensor(1.7636e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2345. State = [[-0.2033636  0.217341 ]]. Action = [[0.11200026 0.20944327 0.16218418 0.19611669]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 2345 is [True, False, False, False, False, True]
State prediction error at timestep 2345 is tensor(6.4232e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2346. State = [[-0.2033636  0.217341 ]]. Action = [[ 0.00973859 -0.20654412 -0.08130629 -0.06370765]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 2346 is [True, False, False, False, False, True]
State prediction error at timestep 2346 is tensor(3.6688e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2347. State = [[-0.2033043   0.21722335]]. Action = [[ 0.18949324  0.03508261 -0.07184741 -0.09734768]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 2347 is [True, False, False, False, False, True]
State prediction error at timestep 2347 is tensor(3.7153e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2347 of -1
Current timestep = 2348. State = [[-0.2033043   0.21722335]]. Action = [[ 0.1207425  -0.21007176 -0.21744812  0.17979491]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 2348 is [True, False, False, False, False, True]
State prediction error at timestep 2348 is tensor(1.3344e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2349. State = [[-0.2033043   0.21722335]]. Action = [[-0.22686145 -0.00634362 -0.0229252   0.8388392 ]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 2349 is [True, False, False, False, False, True]
State prediction error at timestep 2349 is tensor(9.2654e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2350. State = [[-0.2033043   0.21722335]]. Action = [[ 0.20216167  0.16261107  0.07032332 -0.59156054]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 2350 is [True, False, False, False, False, True]
State prediction error at timestep 2350 is tensor(3.6346e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2350 of -1
Current timestep = 2351. State = [[-0.2033043   0.21722335]]. Action = [[-0.06489202  0.17366922 -0.02294022 -0.74684846]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 2351 is [True, False, False, False, False, True]
State prediction error at timestep 2351 is tensor(6.9452e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2352. State = [[-0.2033043   0.21722335]]. Action = [[-0.221577   -0.21317981  0.05362636 -0.15392488]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 2352 is [True, False, False, False, False, True]
State prediction error at timestep 2352 is tensor(2.2305e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2353. State = [[-0.2033043   0.21722335]]. Action = [[-0.08949953  0.19243705  0.1961636  -0.74683166]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 2353 is [True, False, False, False, False, True]
State prediction error at timestep 2353 is tensor(6.4788e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2354. State = [[-0.2033043   0.21722335]]. Action = [[ 0.14118275 -0.00299086  0.23204815 -0.77446663]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 2354 is [True, False, False, False, False, True]
State prediction error at timestep 2354 is tensor(3.7723e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2354 of -1
Current timestep = 2355. State = [[-0.2033043   0.21722335]]. Action = [[ 0.11288494  0.22226655 -0.13465463  0.672675  ]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 2355 is [True, False, False, False, False, True]
State prediction error at timestep 2355 is tensor(1.9681e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2356. State = [[-0.2033043   0.21722335]]. Action = [[ 0.21746725  0.06465128 -0.17190196 -0.12506211]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 2356 is [True, False, False, False, False, True]
State prediction error at timestep 2356 is tensor(3.0230e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2357. State = [[-0.2033043   0.21722335]]. Action = [[-0.12317072  0.22129112  0.19839984 -0.29766786]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 2357 is [True, False, False, False, False, True]
State prediction error at timestep 2357 is tensor(4.3409e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2358. State = [[-0.2033043   0.21722335]]. Action = [[0.20865673 0.23596007 0.12045196 0.69072986]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 2358 is [True, False, False, False, False, True]
State prediction error at timestep 2358 is tensor(2.2756e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2359. State = [[-0.2033043   0.21722335]]. Action = [[-0.0144612   0.17602089  0.07823679 -0.6571161 ]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 2359 is [True, False, False, False, False, True]
State prediction error at timestep 2359 is tensor(6.3414e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2359 of 1
Current timestep = 2360. State = [[-0.2033043   0.21722335]]. Action = [[ 0.22901008 -0.17269044 -0.23838998  0.27261746]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 2360 is [True, False, False, False, False, True]
State prediction error at timestep 2360 is tensor(1.8470e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2361. State = [[-0.2033043   0.21722335]]. Action = [[-0.22905327  0.06613582  0.14591211  0.77347195]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 2361 is [True, False, False, False, False, True]
State prediction error at timestep 2361 is tensor(8.7088e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2362. State = [[-0.20340022  0.21736506]]. Action = [[-0.09475935  0.06222215  0.15139136  0.26356387]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 2362 is [True, False, False, False, False, True]
State prediction error at timestep 2362 is tensor(2.3132e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2362 of -1
Current timestep = 2363. State = [[-0.20382588  0.21787716]]. Action = [[-0.17246044  0.15891492  0.06533679 -0.42719054]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 2363 is [True, False, False, False, False, True]
State prediction error at timestep 2363 is tensor(2.0397e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2364. State = [[-0.20439377  0.218654  ]]. Action = [[ 0.10465878 -0.05419786  0.03142944  0.8184624 ]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 2364 is [True, False, False, False, False, True]
State prediction error at timestep 2364 is tensor(7.6024e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2364 of -1
Current timestep = 2365. State = [[-0.20374906  0.21762748]]. Action = [[-0.00630756 -0.0725987  -0.0660162  -0.7695446 ]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 2365 is [True, False, False, False, False, True]
State prediction error at timestep 2365 is tensor(1.7508e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2366. State = [[-0.20350425  0.21714678]]. Action = [[-0.06524149  0.19051933 -0.03878674 -0.5556734 ]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 2366 is [True, False, False, False, False, True]
State prediction error at timestep 2366 is tensor(2.8849e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2367. State = [[-0.20307586  0.21649498]]. Action = [[-0.22774103  0.14724368  0.19387999  0.69143224]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 2367 is [True, False, False, False, False, True]
State prediction error at timestep 2367 is tensor(5.4077e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2368. State = [[-0.20281592  0.21602526]]. Action = [[-0.01508966 -0.17365515 -0.15638632 -0.79337555]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 2368 is [True, False, False, False, False, True]
State prediction error at timestep 2368 is tensor(5.1589e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2369. State = [[-0.20280719  0.21589674]]. Action = [[ 0.09148917 -0.2328325   0.15483052  0.84889483]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 2369 is [True, False, False, False, False, True]
State prediction error at timestep 2369 is tensor(3.9962e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2369 of 0
Current timestep = 2370. State = [[-0.2027815   0.21575241]]. Action = [[-0.07995243 -0.16186373  0.24153996  0.40176618]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 2370 is [True, False, False, False, False, True]
State prediction error at timestep 2370 is tensor(4.2689e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2371. State = [[-0.20250495  0.21530738]]. Action = [[-0.171913   -0.07441884  0.23696423  0.4107672 ]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 2371 is [True, False, False, False, False, True]
State prediction error at timestep 2371 is tensor(7.8520e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2372. State = [[-0.20245078  0.21511312]]. Action = [[ 0.10386747 -0.2057824   0.21317494  0.24728131]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 2372 is [True, False, False, False, False, True]
State prediction error at timestep 2372 is tensor(5.3668e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2373. State = [[-0.2023879   0.21491693]]. Action = [[-0.24572177 -0.13895766  0.05853364  0.95655847]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 2373 is [True, False, False, False, False, True]
State prediction error at timestep 2373 is tensor(4.7126e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2373 of 0
Current timestep = 2374. State = [[-0.20222907  0.21470684]]. Action = [[-0.18147379  0.15142971  0.06923997 -0.26272827]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 2374 is [True, False, False, False, False, True]
State prediction error at timestep 2374 is tensor(2.2413e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2375. State = [[-0.20215383  0.21444647]]. Action = [[ 0.11135045  0.22641927 -0.24363983  0.82050204]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 2375 is [True, False, False, False, False, True]
State prediction error at timestep 2375 is tensor(2.6762e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2376. State = [[-0.2019749   0.21401583]]. Action = [[ 0.09178382 -0.01032427 -0.19051667  0.20784736]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 2376 is [True, False, False, False, False, True]
State prediction error at timestep 2376 is tensor(5.9908e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2376 of 0
Current timestep = 2377. State = [[-0.2016919   0.21359596]]. Action = [[ 0.2282604   0.02060601  0.10078835 -0.2631117 ]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 2377 is [True, False, False, False, False, True]
State prediction error at timestep 2377 is tensor(1.1301e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2378. State = [[-0.2012257   0.21267995]]. Action = [[-0.08633792 -0.12795143 -0.15960981  0.5632374 ]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 2378 is [True, False, False, False, False, True]
State prediction error at timestep 2378 is tensor(8.6849e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2378 of 0
Current timestep = 2379. State = [[-0.20073862  0.21139334]]. Action = [[-0.07523355 -0.23299684 -0.19563802  0.5092344 ]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 2379 is [True, False, False, False, False, True]
State prediction error at timestep 2379 is tensor(1.4269e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2380. State = [[-0.20038588  0.21044002]]. Action = [[-0.21872261  0.16845226 -0.02618423 -0.6975388 ]]. Reward = [0.]
Curr episode timestep = 576
Scene graph at timestep 2380 is [True, False, False, False, False, True]
State prediction error at timestep 2380 is tensor(5.5271e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2381. State = [[-0.20003922  0.20980482]]. Action = [[-0.23899391 -0.15193966  0.06294605 -0.8511906 ]]. Reward = [0.]
Curr episode timestep = 577
Scene graph at timestep 2381 is [True, False, False, False, False, True]
State prediction error at timestep 2381 is tensor(5.1100e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2381 of 0
Current timestep = 2382. State = [[-0.19996749  0.20892844]]. Action = [[-0.01636268 -0.16909567  0.20577025  0.32577395]]. Reward = [0.]
Curr episode timestep = 578
Scene graph at timestep 2382 is [True, False, False, False, False, True]
State prediction error at timestep 2382 is tensor(8.5529e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2383. State = [[-0.19987144  0.20849617]]. Action = [[ 0.01054734  0.21827409  0.17429405 -0.8615303 ]]. Reward = [0.]
Curr episode timestep = 579
Scene graph at timestep 2383 is [True, False, False, False, False, True]
State prediction error at timestep 2383 is tensor(3.0275e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2384. State = [[-0.1993988  0.2073871]]. Action = [[ 0.09055784 -0.1830754   0.03205928 -0.8198786 ]]. Reward = [0.]
Curr episode timestep = 580
Scene graph at timestep 2384 is [True, False, False, False, False, True]
State prediction error at timestep 2384 is tensor(1.1505e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2385. State = [[-0.19913499  0.20591919]]. Action = [[-0.09065571 -0.01326476 -0.24595098  0.7275758 ]]. Reward = [0.]
Curr episode timestep = 581
Scene graph at timestep 2385 is [True, False, False, False, False, True]
State prediction error at timestep 2385 is tensor(7.4421e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2385 of 1
Current timestep = 2386. State = [[-0.19956914  0.20484528]]. Action = [[ 0.09752873  0.00432178  0.01349944 -0.06129044]]. Reward = [0.]
Curr episode timestep = 582
Scene graph at timestep 2386 is [True, False, False, False, False, True]
State prediction error at timestep 2386 is tensor(2.0082e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2386 of 1
Current timestep = 2387. State = [[-0.19945051  0.20427181]]. Action = [[-0.01761517 -0.13944729 -0.14506617  0.73818576]]. Reward = [0.]
Curr episode timestep = 583
Scene graph at timestep 2387 is [True, False, False, False, False, True]
State prediction error at timestep 2387 is tensor(5.6201e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2388. State = [[-0.19929321  0.20414265]]. Action = [[ 0.16180533 -0.12896205  0.20468292  0.84603274]]. Reward = [0.]
Curr episode timestep = 584
Scene graph at timestep 2388 is [True, False, False, False, False, True]
State prediction error at timestep 2388 is tensor(2.3433e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2389. State = [[-0.19919035  0.20403552]]. Action = [[ 0.07498753  0.17841965  0.16636276 -0.5791746 ]]. Reward = [0.]
Curr episode timestep = 585
Scene graph at timestep 2389 is [True, False, False, False, False, True]
State prediction error at timestep 2389 is tensor(1.7270e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2389 of 1
Current timestep = 2390. State = [[-0.19872293  0.20277056]]. Action = [[ 0.10741785 -0.11825284 -0.22531131  0.6840446 ]]. Reward = [0.]
Curr episode timestep = 586
Scene graph at timestep 2390 is [True, False, False, False, False, True]
State prediction error at timestep 2390 is tensor(3.6952e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2391. State = [[-0.19800776  0.20150128]]. Action = [[-0.22410072 -0.09354991  0.0009273  -0.00973141]]. Reward = [0.]
Curr episode timestep = 587
Scene graph at timestep 2391 is [True, False, False, False, False, True]
State prediction error at timestep 2391 is tensor(9.3148e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2392. State = [[-0.19729668  0.20031328]]. Action = [[-0.05728766 -0.16678141 -0.23360159  0.00923002]]. Reward = [0.]
Curr episode timestep = 588
Scene graph at timestep 2392 is [True, False, False, False, False, True]
State prediction error at timestep 2392 is tensor(2.5210e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2393. State = [[-0.1954244   0.19723018]]. Action = [[ 0.10882065 -0.12079039 -0.2054296  -0.34434187]]. Reward = [0.]
Curr episode timestep = 589
Scene graph at timestep 2393 is [True, False, False, False, False, True]
State prediction error at timestep 2393 is tensor(2.6331e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2393 of 1
Current timestep = 2394. State = [[-0.19412825  0.19528997]]. Action = [[-0.10424963 -0.20223148 -0.01533844  0.6970365 ]]. Reward = [0.]
Curr episode timestep = 590
Scene graph at timestep 2394 is [True, False, False, False, False, True]
State prediction error at timestep 2394 is tensor(3.9031e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2395. State = [[-0.19330494  0.19374326]]. Action = [[ 0.22274536 -0.1480259  -0.1192881  -0.48646212]]. Reward = [0.]
Curr episode timestep = 591
Scene graph at timestep 2395 is [True, False, False, False, False, True]
State prediction error at timestep 2395 is tensor(3.6492e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2396. State = [[-0.19263262  0.19238608]]. Action = [[-0.20503466  0.16744554 -0.11427794 -0.637245  ]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 2396 is [True, False, False, False, False, True]
State prediction error at timestep 2396 is tensor(2.7343e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2397. State = [[-0.1921875   0.19147871]]. Action = [[ 0.01908743  0.1438154   0.24295324 -0.12176949]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 2397 is [True, False, False, False, False, True]
State prediction error at timestep 2397 is tensor(5.7485e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2397 of 1
Current timestep = 2398. State = [[-0.19024365  0.18735358]]. Action = [[-0.05776897 -0.10627404  0.14218429 -0.25580585]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 2398 is [True, False, False, False, False, True]
State prediction error at timestep 2398 is tensor(7.2807e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2399. State = [[-0.18970832  0.18558802]]. Action = [[ 0.16774708  0.1472812  -0.02865209  0.1525209 ]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 2399 is [True, False, False, False, False, True]
State prediction error at timestep 2399 is tensor(4.2482e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2400. State = [[-0.18857667  0.18157718]]. Action = [[ 0.03458995 -0.10234749  0.12470922  0.2805016 ]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 2400 is [True, False, False, False, False, True]
State prediction error at timestep 2400 is tensor(6.3488e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2401. State = [[-0.18780135  0.17989807]]. Action = [[-0.04736188 -0.21291678  0.10962301  0.02317429]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 2401 is [True, False, False, False, False, True]
State prediction error at timestep 2401 is tensor(1.3430e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2402. State = [[-0.18748198  0.17857504]]. Action = [[-0.0661651  -0.2366146  -0.22019473  0.8079896 ]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 2402 is [True, False, False, False, False, True]
State prediction error at timestep 2402 is tensor(2.7841e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2403. State = [[-0.18725805  0.17762955]]. Action = [[-0.20637083  0.23679581 -0.17126282 -0.48890358]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 2403 is [True, False, False, False, False, True]
State prediction error at timestep 2403 is tensor(8.7283e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2404. State = [[-0.18668672  0.17614926]]. Action = [[ 0.17456496 -0.14503191  0.24048993 -0.50452816]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 2404 is [True, False, False, False, False, True]
State prediction error at timestep 2404 is tensor(5.3836e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2405. State = [[-0.18596251  0.17270744]]. Action = [[-0.11445031  0.01100755  0.13433588 -0.01196486]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 2405 is [True, False, False, False, False, True]
State prediction error at timestep 2405 is tensor(4.6551e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2405 of 1
Current timestep = 2406. State = [[-0.18599336  0.17248394]]. Action = [[0.02977794 0.22441769 0.20542943 0.6201602 ]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 2406 is [True, False, False, False, False, True]
State prediction error at timestep 2406 is tensor(7.6582e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2407. State = [[-0.18614742  0.17238788]]. Action = [[-0.13681282  0.01175895 -0.13067049 -0.14596617]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 2407 is [True, False, False, False, False, True]
State prediction error at timestep 2407 is tensor(5.4706e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2408. State = [[-0.18603703  0.1715522 ]]. Action = [[-0.05571961  0.00396028 -0.1846178  -0.42870378]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 2408 is [True, False, False, False, False, True]
State prediction error at timestep 2408 is tensor(2.5800e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2408 of 1
Current timestep = 2409. State = [[-0.1862604  0.1716285]]. Action = [[-0.21770723 -0.03333956 -0.22544344 -0.31412077]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 2409 is [True, False, False, False, False, True]
State prediction error at timestep 2409 is tensor(4.1618e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2410. State = [[-0.18685272  0.1711474 ]]. Action = [[ 0.02053353 -0.12905061  0.07851031 -0.27663267]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 2410 is [True, False, False, False, False, True]
State prediction error at timestep 2410 is tensor(1.0878e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2410 of 1
Current timestep = 2411. State = [[-0.1871084   0.16753134]]. Action = [[ 0.10606483  0.0970214   0.20524889 -0.7288513 ]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 2411 is [True, False, False, False, False, True]
State prediction error at timestep 2411 is tensor(5.6291e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2411 of 1
Current timestep = 2412. State = [[-0.18677229  0.1674421 ]]. Action = [[-0.011106    0.23068184  0.03126267 -0.00895405]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 2412 is [True, False, False, False, False, True]
State prediction error at timestep 2412 is tensor(8.7220e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2413. State = [[-0.18665619  0.1676508 ]]. Action = [[-0.10442123 -0.00389524 -0.07306725  0.24418116]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 2413 is [True, False, False, False, False, True]
State prediction error at timestep 2413 is tensor(1.0011e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2413 of 1
Current timestep = 2414. State = [[-0.18682563  0.167421  ]]. Action = [[-0.08538905 -0.20094503  0.03100988 -0.03003323]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 2414 is [True, False, False, False, False, True]
State prediction error at timestep 2414 is tensor(2.9196e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2415. State = [[-0.18689923  0.16762747]]. Action = [[-0.18169942  0.15051374 -0.08273058  0.236153  ]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 2415 is [True, False, False, False, False, True]
State prediction error at timestep 2415 is tensor(9.9172e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2416. State = [[-0.1870195   0.16772196]]. Action = [[-0.14449689 -0.11584142  0.22603714 -0.07029128]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 2416 is [True, False, False, False, False, True]
State prediction error at timestep 2416 is tensor(3.1598e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2417. State = [[-0.18720691  0.16760604]]. Action = [[-0.01708479 -0.2436221   0.18865216 -0.6011897 ]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 2417 is [True, False, False, False, False, True]
State prediction error at timestep 2417 is tensor(7.2374e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2418. State = [[-0.18707749  0.16762336]]. Action = [[-0.00613616 -0.22517261 -0.11579697  0.6586344 ]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 2418 is [True, False, False, False, False, True]
State prediction error at timestep 2418 is tensor(1.9654e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2418 of 1
Current timestep = 2419. State = [[-0.18729565  0.1678159 ]]. Action = [[-0.12162164  0.20495349 -0.1226064   0.20311558]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 2419 is [True, False, False, False, False, True]
State prediction error at timestep 2419 is tensor(7.9375e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2420. State = [[-0.18749048  0.16779147]]. Action = [[-0.14910732  0.10339692 -0.05375956 -0.10012603]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 2420 is [True, False, False, False, False, True]
State prediction error at timestep 2420 is tensor(5.5655e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2421. State = [[-0.18728843  0.16774617]]. Action = [[-0.19644007  0.03762108  0.07106823 -0.6321253 ]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 2421 is [True, False, False, False, False, True]
State prediction error at timestep 2421 is tensor(2.8022e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2422. State = [[-0.18738626  0.1676906 ]]. Action = [[-0.05793847  0.16416353 -0.0225433   0.75614226]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 2422 is [True, False, False, False, False, True]
State prediction error at timestep 2422 is tensor(2.5856e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2422 of 1
Current timestep = 2423. State = [[-0.18736354  0.16779254]]. Action = [[ 0.19444436 -0.06231233  0.20910889  0.79920805]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 2423 is [True, False, False, False, False, True]
State prediction error at timestep 2423 is tensor(2.2093e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2424. State = [[-0.18741617  0.16767997]]. Action = [[0.20100605 0.20545292 0.08645242 0.08169556]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 2424 is [True, False, False, False, False, True]
State prediction error at timestep 2424 is tensor(2.4508e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2425. State = [[-0.18741617  0.16767997]]. Action = [[-0.00088124 -0.23793401  0.20293185 -0.38235092]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 2425 is [True, False, False, False, False, True]
State prediction error at timestep 2425 is tensor(6.0876e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2426. State = [[-0.18749256  0.16757357]]. Action = [[-0.08390623 -0.12696148 -0.03811269 -0.5667974 ]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 2426 is [True, False, False, False, False, True]
State prediction error at timestep 2426 is tensor(1.7575e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2426 of 1
Current timestep = 2427. State = [[-0.18908653  0.1649332 ]]. Action = [[-0.13013823  0.03670093  0.11188856  0.04937053]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 2427 is [True, False, False, False, False, True]
State prediction error at timestep 2427 is tensor(1.8717e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2427 of 1
Current timestep = 2428. State = [[-0.19026695  0.16466986]]. Action = [[-0.02501743  0.18237513  0.03654206 -0.75759894]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 2428 is [True, False, False, False, False, True]
State prediction error at timestep 2428 is tensor(3.6638e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2429. State = [[-0.19142234  0.16457176]]. Action = [[-0.24073155 -0.21634659  0.23676959 -0.8006015 ]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 2429 is [True, False, False, False, False, True]
State prediction error at timestep 2429 is tensor(8.4920e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2430. State = [[-0.19560093  0.16318332]]. Action = [[-0.03908616 -0.09522673 -0.20208913  0.69457066]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 2430 is [True, False, False, False, False, True]
State prediction error at timestep 2430 is tensor(1.4454e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2430 of 1
Current timestep = 2431. State = [[-0.19677351  0.16212533]]. Action = [[-0.15525272 -0.17844959  0.19268435  0.03895187]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 2431 is [True, False, False, False, False, True]
State prediction error at timestep 2431 is tensor(4.7009e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2432. State = [[-0.19765781  0.1611317 ]]. Action = [[ 0.22999603  0.20546785 -0.20861748 -0.55694205]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 2432 is [True, False, False, False, False, True]
State prediction error at timestep 2432 is tensor(2.1364e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2433. State = [[-0.19844145  0.16049351]]. Action = [[-0.20018704  0.08543503  0.16229236 -0.24921751]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 2433 is [True, False, False, False, False, True]
State prediction error at timestep 2433 is tensor(4.8070e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2434. State = [[-0.19933854  0.1601951 ]]. Action = [[ 0.20909652 -0.01498608 -0.02471207  0.69835925]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 2434 is [True, False, False, False, False, True]
State prediction error at timestep 2434 is tensor(1.0121e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2434 of 1
Current timestep = 2435. State = [[-0.20074263  0.15917774]]. Action = [[-0.03404421  0.1701344  -0.0447008  -0.40107143]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 2435 is [True, False, False, False, False, True]
State prediction error at timestep 2435 is tensor(3.8170e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2436. State = [[-0.20172206  0.157961  ]]. Action = [[-0.24506107  0.22075301  0.21876353  0.47211552]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 2436 is [True, False, False, False, False, True]
State prediction error at timestep 2436 is tensor(1.1633e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2437. State = [[-0.20263259  0.1564636 ]]. Action = [[ 0.11459547  0.04313844 -0.12658916 -0.08460307]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 2437 is [True, False, False, False, False, True]
State prediction error at timestep 2437 is tensor(4.0168e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2437 of 1
Current timestep = 2438. State = [[-0.20228836  0.15660028]]. Action = [[ 0.0448471   0.24107668 -0.1467276   0.02419317]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 2438 is [True, False, False, False, False, True]
State prediction error at timestep 2438 is tensor(4.5472e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2439. State = [[-0.20210867  0.15658686]]. Action = [[-0.14210731 -0.12493098 -0.10531871  0.49607587]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 2439 is [True, False, False, False, False, True]
State prediction error at timestep 2439 is tensor(1.3512e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2440. State = [[-0.20205942  0.15653235]]. Action = [[ 0.02896047 -0.20135334  0.16848615  0.2211318 ]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 2440 is [True, False, False, False, False, True]
State prediction error at timestep 2440 is tensor(1.5951e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2441. State = [[-0.20206515  0.15636474]]. Action = [[ 0.18174934  0.22194624 -0.24663363 -0.562813  ]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 2441 is [True, False, False, False, False, True]
State prediction error at timestep 2441 is tensor(1.5057e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2442. State = [[-0.20179042  0.15634161]]. Action = [[ 0.21391755 -0.19480275 -0.10857615 -0.9278662 ]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 2442 is [True, False, False, False, False, True]
State prediction error at timestep 2442 is tensor(3.1301e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2442 of -1
Current timestep = 2443. State = [[-0.20181808  0.15639399]]. Action = [[ 0.15665135 -0.17538697  0.13389128 -0.3329631 ]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 2443 is [True, False, False, False, False, True]
State prediction error at timestep 2443 is tensor(1.7428e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2444. State = [[-0.20179817  0.15625891]]. Action = [[-0.22025043 -0.09531824 -0.20505841  0.87765   ]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 2444 is [True, False, False, False, False, True]
State prediction error at timestep 2444 is tensor(1.4533e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2445. State = [[-0.20169489  0.15616012]]. Action = [[ 0.19780642 -0.2065079  -0.17754793  0.4729247 ]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 2445 is [True, False, False, False, False, True]
State prediction error at timestep 2445 is tensor(1.6884e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2446. State = [[-0.20157489  0.15612729]]. Action = [[-0.08094513  0.19501641  0.07519719  0.63240504]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 2446 is [True, False, False, False, False, True]
State prediction error at timestep 2446 is tensor(1.5361e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2447. State = [[-0.20160592  0.15613154]]. Action = [[ 0.19575617 -0.00116894 -0.19013676  0.56091595]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 2447 is [True, False, False, False, False, True]
State prediction error at timestep 2447 is tensor(4.9772e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2447 of -1
Current timestep = 2448. State = [[-0.20163374  0.15588814]]. Action = [[-0.17776814  0.12584388 -0.01435551 -0.24857175]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 2448 is [True, False, False, False, False, True]
State prediction error at timestep 2448 is tensor(4.8889e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2449. State = [[-0.20153241  0.15592939]]. Action = [[-0.11645289 -0.18719016  0.07824805  0.341668  ]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 2449 is [True, False, False, False, False, True]
State prediction error at timestep 2449 is tensor(7.7776e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2450. State = [[-0.20156376  0.15598127]]. Action = [[-0.20595258  0.1512284   0.15469691  0.8955715 ]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 2450 is [True, False, False, False, False, True]
State prediction error at timestep 2450 is tensor(1.3914e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2451. State = [[-0.2015496  0.1559153]]. Action = [[0.0557988  0.1569404  0.02642721 0.21713328]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 2451 is [True, False, False, False, False, True]
State prediction error at timestep 2451 is tensor(2.8561e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2451 of -1
Current timestep = 2452. State = [[-0.20153546  0.15584932]]. Action = [[ 0.08357924 -0.14984457 -0.16867289 -0.03746819]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 2452 is [True, False, False, False, False, True]
State prediction error at timestep 2452 is tensor(2.1613e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2453. State = [[-0.2014754   0.15550777]]. Action = [[ 0.00335923 -0.12158346  0.10717165  0.56216073]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 2453 is [True, False, False, False, False, True]
State prediction error at timestep 2453 is tensor(2.3121e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2453 of -1
Current timestep = 2454. State = [[-0.20055988  0.1526568 ]]. Action = [[-0.06962056  0.06239307  0.21797156  0.917503  ]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 2454 is [True, False, False, False, False, True]
State prediction error at timestep 2454 is tensor(2.7344e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2454 of -1
Current timestep = 2455. State = [[-0.2011436   0.15218866]]. Action = [[-0.10497925 -0.11696452 -0.20363955 -0.09977329]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 2455 is [True, False, False, False, False, True]
State prediction error at timestep 2455 is tensor(9.1388e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2456. State = [[-0.20264187  0.15005474]]. Action = [[-0.08194464  0.12292832 -0.22877023 -0.17104018]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 2456 is [True, False, False, False, False, True]
State prediction error at timestep 2456 is tensor(2.9954e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2457. State = [[-0.20346643  0.15050976]]. Action = [[ 0.22091877  0.22295469 -0.1865272  -0.05424482]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 2457 is [True, False, False, False, False, True]
State prediction error at timestep 2457 is tensor(1.7175e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2458. State = [[-0.20416698  0.1511459 ]]. Action = [[ 0.09235001 -0.14340486  0.06988609  0.83798516]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 2458 is [True, False, False, False, False, True]
State prediction error at timestep 2458 is tensor(3.7384e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2458 of 1
Current timestep = 2459. State = [[-0.20470212  0.15175296]]. Action = [[ 0.06793079  0.20714128  0.1847431  -0.19506204]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 2459 is [True, False, False, False, False, True]
State prediction error at timestep 2459 is tensor(1.3844e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2460. State = [[-0.2054171   0.15188617]]. Action = [[-0.21514118 -0.21537694 -0.02224663  0.19443035]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 2460 is [True, False, False, False, False, True]
State prediction error at timestep 2460 is tensor(2.3874e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2461. State = [[-0.20613247  0.1522403 ]]. Action = [[-0.16227867  0.03983012  0.05038798 -0.14405191]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 2461 is [True, False, False, False, False, True]
State prediction error at timestep 2461 is tensor(1.6250e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2462. State = [[-0.20797217  0.15271936]]. Action = [[0.10526186 0.03646395 0.14765108 0.18989134]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 2462 is [True, False, False, False, False, True]
State prediction error at timestep 2462 is tensor(5.2720e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2462 of -1
Current timestep = 2463. State = [[-0.20820792  0.15270755]]. Action = [[-0.16149065 -0.0570212   0.03207132 -0.77520454]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 2463 is [True, False, False, False, False, True]
State prediction error at timestep 2463 is tensor(1.4626e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2464. State = [[-0.20838298  0.15294692]]. Action = [[-0.03819948  0.00839314 -0.04096876  0.4974234 ]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 2464 is [True, False, False, False, False, True]
State prediction error at timestep 2464 is tensor(1.0287e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2464 of -1
Current timestep = 2465. State = [[-0.20973463  0.15286198]]. Action = [[ 0.12173301  0.01356515 -0.0475865  -0.82447857]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 2465 is [True, False, False, False, False, True]
State prediction error at timestep 2465 is tensor(1.2740e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2465 of -1
Current timestep = 2466. State = [[-0.20947064  0.15300947]]. Action = [[-0.02626027 -0.20051742 -0.2026913   0.3432144 ]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 2466 is [True, False, False, False, False, True]
State prediction error at timestep 2466 is tensor(9.5588e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2467. State = [[-0.20907201  0.15339473]]. Action = [[-0.23641519  0.23439509  0.16852343 -0.54661053]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 2467 is [True, False, False, False, False, True]
State prediction error at timestep 2467 is tensor(8.4742e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2468. State = [[-0.20887978  0.15381931]]. Action = [[-0.00288117 -0.00291093 -0.23616342 -0.04296279]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 2468 is [True, False, False, False, False, True]
State prediction error at timestep 2468 is tensor(2.1727e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2468 of -1
Current timestep = 2469. State = [[-0.20875068  0.15406089]]. Action = [[-0.00626516 -0.03894287  0.23195457  0.8825064 ]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 2469 is [True, False, False, False, False, True]
State prediction error at timestep 2469 is tensor(2.2832e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2469 of -1
Current timestep = 2470. State = [[-0.20859006  0.15390135]]. Action = [[-0.17868887 -0.13780883 -0.2411848  -0.10441035]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 2470 is [True, False, False, False, False, True]
State prediction error at timestep 2470 is tensor(1.4952e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2471. State = [[-0.20839529  0.15388271]]. Action = [[-0.03654197 -0.03757353 -0.12675719 -0.5871685 ]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 2471 is [True, False, False, False, False, True]
State prediction error at timestep 2471 is tensor(2.8079e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2472. State = [[-0.20837313  0.15347825]]. Action = [[ 0.09463409  0.20481724  0.03801435 -0.72311574]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 2472 is [True, False, False, False, False, True]
State prediction error at timestep 2472 is tensor(1.4706e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2473. State = [[-0.20856836  0.15325318]]. Action = [[-0.14540674  0.1829881   0.151348    0.11994588]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 2473 is [True, False, False, False, False, True]
State prediction error at timestep 2473 is tensor(2.2855e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2474. State = [[-0.20863003  0.153155  ]]. Action = [[-0.18866481 -0.15272808 -0.21638712 -0.12213695]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 2474 is [True, False, False, False, False, True]
State prediction error at timestep 2474 is tensor(1.7343e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2475. State = [[-0.20874386  0.15307094]]. Action = [[-0.19778398  0.24502486 -0.07720059  0.6935365 ]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 2475 is [True, False, False, False, False, True]
State prediction error at timestep 2475 is tensor(5.4801e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2476. State = [[-0.20884097  0.15274581]]. Action = [[ 0.20022085  0.1603455  -0.18926299 -0.72360325]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 2476 is [True, False, False, False, False, True]
State prediction error at timestep 2476 is tensor(4.8117e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2477. State = [[-0.20883323  0.1528407 ]]. Action = [[-0.20530486  0.08418059 -0.02098353 -0.52755225]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 2477 is [True, False, False, False, False, True]
State prediction error at timestep 2477 is tensor(6.0794e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2477 of -1
Current timestep = 2478. State = [[-0.20878042  0.15279403]]. Action = [[ 0.15703356  0.15225422 -0.22282188 -0.57489675]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 2478 is [True, False, False, False, False, True]
State prediction error at timestep 2478 is tensor(4.4112e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2479. State = [[-0.20893726  0.15252785]]. Action = [[-0.18277659 -0.00353877 -0.1862811   0.5075656 ]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 2479 is [True, False, False, False, False, True]
State prediction error at timestep 2479 is tensor(4.3370e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2480. State = [[-0.20903528  0.15251788]]. Action = [[-0.13986516 -0.09969887  0.24286216  0.40022862]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 2480 is [True, False, False, False, False, True]
State prediction error at timestep 2480 is tensor(2.0514e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2481. State = [[-0.20922498  0.1523206 ]]. Action = [[-0.06371734 -0.08389851  0.06392595 -0.2653376 ]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 2481 is [True, False, False, False, False, True]
State prediction error at timestep 2481 is tensor(3.9860e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2481 of -1
Current timestep = 2482. State = [[-0.20952745  0.15141353]]. Action = [[-0.04122187  0.14580166  0.05841595  0.8149104 ]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 2482 is [True, False, False, False, False, True]
State prediction error at timestep 2482 is tensor(1.3367e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2483. State = [[-0.20981328  0.1508778 ]]. Action = [[ 0.03459853  0.24216416 -0.21446112 -0.56833106]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 2483 is [True, False, False, False, False, True]
State prediction error at timestep 2483 is tensor(1.3352e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2484. State = [[-0.21093312  0.14969414]]. Action = [[ 0.07477221  0.12610593  0.10495007 -0.8086878 ]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 2484 is [True, False, False, False, False, True]
State prediction error at timestep 2484 is tensor(9.1488e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2484 of -1
Current timestep = 2485. State = [[-0.21058315  0.15055083]]. Action = [[-0.24496765  0.14457655 -0.00611502  0.10855746]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 2485 is [True, False, False, False, False, True]
State prediction error at timestep 2485 is tensor(1.2359e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2486. State = [[-0.21040934  0.15079409]]. Action = [[ 0.04776138 -0.07816076  0.0705567   0.7463008 ]]. Reward = [0.]
Curr episode timestep = 682
Scene graph at timestep 2486 is [True, False, False, False, False, True]
State prediction error at timestep 2486 is tensor(4.0639e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2486 of -1
Current timestep = 2487. State = [[-0.21036527  0.15050076]]. Action = [[-0.0469739  -0.20936507  0.16902867  0.31249774]]. Reward = [0.]
Curr episode timestep = 683
Scene graph at timestep 2487 is [True, False, False, False, False, True]
State prediction error at timestep 2487 is tensor(2.1974e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2488. State = [[-0.21001287  0.1503001 ]]. Action = [[-1.4902554e-01 -2.6333332e-04 -1.5745638e-01 -5.2508301e-01]]. Reward = [0.]
Curr episode timestep = 684
Scene graph at timestep 2488 is [True, False, False, False, False, True]
State prediction error at timestep 2488 is tensor(7.7867e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2488 of -1
Current timestep = 2489. State = [[-0.20971301  0.15040492]]. Action = [[-0.2195538   0.18197507 -0.14658253 -0.1803484 ]]. Reward = [0.]
Curr episode timestep = 685
Scene graph at timestep 2489 is [True, False, False, False, False, True]
State prediction error at timestep 2489 is tensor(2.7600e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2490. State = [[-0.20903566  0.15009004]]. Action = [[ 0.08870858 -0.05277242  0.20306873  0.7332958 ]]. Reward = [0.]
Curr episode timestep = 686
Scene graph at timestep 2490 is [True, False, False, False, False, True]
State prediction error at timestep 2490 is tensor(3.9108e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2491. State = [[-0.20833142  0.14949846]]. Action = [[ 0.18092823  0.12230033 -0.12379926 -0.61514795]]. Reward = [0.]
Curr episode timestep = 687
Scene graph at timestep 2491 is [True, False, False, False, False, True]
State prediction error at timestep 2491 is tensor(1.2099e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2492. State = [[-0.20790893  0.1488723 ]]. Action = [[ 0.01343894 -0.23783322 -0.21141803 -0.15596223]]. Reward = [0.]
Curr episode timestep = 688
Scene graph at timestep 2492 is [True, False, False, False, False, True]
State prediction error at timestep 2492 is tensor(2.1911e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2493. State = [[-0.20659606  0.14746235]]. Action = [[ 0.02402785  0.06040043 -0.16217704  0.42213297]]. Reward = [0.]
Curr episode timestep = 689
Scene graph at timestep 2493 is [True, False, False, False, False, True]
State prediction error at timestep 2493 is tensor(3.5856e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2493 of -1
Current timestep = 2494. State = [[-0.20654231  0.14766386]]. Action = [[-0.08419831  0.2020868   0.12817854  0.7608663 ]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 2494 is [True, False, False, False, False, True]
State prediction error at timestep 2494 is tensor(4.4879e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2495. State = [[-0.20597333  0.14771909]]. Action = [[-0.08718979 -0.05738173 -0.05452122 -0.14874601]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 2495 is [True, False, False, False, False, True]
State prediction error at timestep 2495 is tensor(1.0331e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2495 of -1
Current timestep = 2496. State = [[-0.20610522  0.14717048]]. Action = [[-0.21958107 -0.02297407  0.05805159 -0.4381202 ]]. Reward = [0.]
Curr episode timestep = 692
Scene graph at timestep 2496 is [True, False, False, False, False, True]
State prediction error at timestep 2496 is tensor(7.9920e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2497. State = [[-0.20606431  0.14676106]]. Action = [[-0.14527667 -0.02004205 -0.00827439  0.8948922 ]]. Reward = [0.]
Curr episode timestep = 693
Scene graph at timestep 2497 is [True, False, False, False, False, True]
State prediction error at timestep 2497 is tensor(2.8723e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2498. State = [[-0.20587882  0.146439  ]]. Action = [[ 0.22560117 -0.1233137   0.00101399  0.58687246]]. Reward = [0.]
Curr episode timestep = 694
Scene graph at timestep 2498 is [True, False, False, False, False, True]
State prediction error at timestep 2498 is tensor(3.3442e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2499. State = [[-0.20563436  0.14564046]]. Action = [[ 0.12612158  0.10815883 -0.07807592 -0.1838857 ]]. Reward = [0.]
Curr episode timestep = 695
Scene graph at timestep 2499 is [True, False, False, False, False, True]
State prediction error at timestep 2499 is tensor(9.7514e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2499 of -1
Current timestep = 2500. State = [[-0.20563892  0.145968  ]]. Action = [[-0.17344725  0.15897945  0.22815162  0.44806957]]. Reward = [0.]
Curr episode timestep = 696
Scene graph at timestep 2500 is [True, False, False, False, False, True]
State prediction error at timestep 2500 is tensor(3.1963e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2501. State = [[-0.20532253  0.14649507]]. Action = [[-0.04896316  0.12645942  0.04984373  0.18422878]]. Reward = [0.]
Curr episode timestep = 697
Scene graph at timestep 2501 is [True, False, False, False, False, True]
State prediction error at timestep 2501 is tensor(1.6867e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2501 of -1
Current timestep = 2502. State = [[-0.20593825  0.1475929 ]]. Action = [[-0.22251163  0.05166265  0.24925697 -0.12129968]]. Reward = [0.]
Curr episode timestep = 698
Scene graph at timestep 2502 is [True, False, False, False, False, True]
State prediction error at timestep 2502 is tensor(1.5262e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2503. State = [[-0.20633899  0.14858483]]. Action = [[ 0.2246837   0.150179    0.10538217 -0.7310217 ]]. Reward = [0.]
Curr episode timestep = 699
Scene graph at timestep 2503 is [True, False, False, False, False, True]
State prediction error at timestep 2503 is tensor(4.3737e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2504. State = [[-0.20648201  0.14898811]]. Action = [[0.0315434  0.13804948 0.1436758  0.1940577 ]]. Reward = [0.]
Curr episode timestep = 700
Scene graph at timestep 2504 is [True, False, False, False, False, True]
State prediction error at timestep 2504 is tensor(3.1707e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2505. State = [[-0.20651814  0.14939627]]. Action = [[ 0.23507321  0.22896218  0.08228827 -0.880254  ]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 2505 is [True, False, False, False, False, True]
State prediction error at timestep 2505 is tensor(2.2959e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2505 of -1
Current timestep = 2506. State = [[-0.20682323  0.15025654]]. Action = [[-0.00111757  0.22310513 -0.00520971  0.7617042 ]]. Reward = [0.]
Curr episode timestep = 702
Scene graph at timestep 2506 is [True, False, False, False, False, True]
State prediction error at timestep 2506 is tensor(8.6657e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2507. State = [[-0.20697942  0.15062192]]. Action = [[-0.22646019  0.14277124 -0.17813988 -0.08269781]]. Reward = [0.]
Curr episode timestep = 703
Scene graph at timestep 2507 is [True, False, False, False, False, True]
State prediction error at timestep 2507 is tensor(1.0657e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2508. State = [[-0.20687401  0.15089545]]. Action = [[-0.16783865  0.15686274 -0.01683056  0.53338194]]. Reward = [0.]
Curr episode timestep = 704
Scene graph at timestep 2508 is [True, False, False, False, False, True]
State prediction error at timestep 2508 is tensor(1.2290e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2509. State = [[-0.2071058   0.15108573]]. Action = [[-0.23396006  0.18816358 -0.17688975 -0.6155373 ]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 2509 is [True, False, False, False, False, True]
State prediction error at timestep 2509 is tensor(3.7788e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2509 of -1
Current timestep = 2510. State = [[-0.20726016  0.15177403]]. Action = [[-0.20091769  0.12007469 -0.04540879  0.87427163]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 2510 is [True, False, False, False, False, True]
State prediction error at timestep 2510 is tensor(1.1244e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2511. State = [[-0.2071966   0.15195273]]. Action = [[-0.14688994  0.14160162  0.1757004   0.00407195]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 2511 is [True, False, False, False, False, True]
State prediction error at timestep 2511 is tensor(1.0744e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2512. State = [[-0.2073685   0.15258022]]. Action = [[-0.02627409  0.07557988  0.1272248  -0.9564342 ]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 2512 is [True, False, False, False, False, True]
State prediction error at timestep 2512 is tensor(4.2420e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2512 of -1
Current timestep = 2513. State = [[-0.20820029  0.15476274]]. Action = [[ 0.11212879 -0.11302596  0.24511772  0.3297975 ]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 2513 is [True, False, False, False, False, True]
State prediction error at timestep 2513 is tensor(4.8592e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2513 of -1
Current timestep = 2514. State = [[-0.20759651  0.15453956]]. Action = [[-0.16829716  0.05104899  0.23369172  0.5558348 ]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 2514 is [True, False, False, False, False, True]
State prediction error at timestep 2514 is tensor(7.3244e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2515. State = [[-0.20729426  0.1543566 ]]. Action = [[-0.13940859 -0.03803618  0.10597599  0.00735927]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 2515 is [True, False, False, False, False, True]
State prediction error at timestep 2515 is tensor(1.2383e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2515 of 1
Current timestep = 2516. State = [[-0.20665695  0.15423968]]. Action = [[-0.13101488 -0.05258642 -0.11663577  0.7936686 ]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 2516 is [True, False, False, False, False, True]
State prediction error at timestep 2516 is tensor(2.0455e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2517. State = [[-0.20660469  0.15411825]]. Action = [[-0.1377623  -0.17504495  0.22166371  0.68411875]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 2517 is [True, False, False, False, False, True]
State prediction error at timestep 2517 is tensor(2.7003e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2517 of 1
Current timestep = 2518. State = [[-0.2065855   0.15425742]]. Action = [[-0.16365078 -0.19490494 -0.03701189 -0.8337761 ]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 2518 is [True, False, False, False, False, True]
State prediction error at timestep 2518 is tensor(2.1290e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2519. State = [[-0.20666891  0.1542218 ]]. Action = [[ 0.18178278 -0.22768974  0.03292772 -0.29666835]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 2519 is [True, False, False, False, False, True]
State prediction error at timestep 2519 is tensor(4.7622e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2520. State = [[-0.20666234  0.15405238]]. Action = [[-0.12287515  0.1469872   0.15079343 -0.22344512]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 2520 is [True, False, False, False, False, True]
State prediction error at timestep 2520 is tensor(2.2273e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2521. State = [[-0.20655954  0.15395412]]. Action = [[-0.11945723  0.15588862 -0.04814546  0.4270084 ]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 2521 is [True, False, False, False, False, True]
State prediction error at timestep 2521 is tensor(7.0394e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2522. State = [[-0.20659319  0.15392163]]. Action = [[ 0.03495732 -0.18093945  0.16887248 -0.6783568 ]]. Reward = [0.]
Curr episode timestep = 718
Scene graph at timestep 2522 is [True, False, False, False, False, True]
State prediction error at timestep 2522 is tensor(1.5026e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2522 of 1
Current timestep = 2523. State = [[-0.20651574  0.153743  ]]. Action = [[-0.0723     -0.20404051 -0.23834264 -0.0497759 ]]. Reward = [0.]
Curr episode timestep = 719
Scene graph at timestep 2523 is [True, False, False, False, False, True]
State prediction error at timestep 2523 is tensor(4.3130e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2524. State = [[-0.20654716  0.15360376]]. Action = [[-0.12445548 -0.22182932  0.07564372  0.91565895]]. Reward = [0.]
Curr episode timestep = 720
Scene graph at timestep 2524 is [True, False, False, False, False, True]
State prediction error at timestep 2524 is tensor(1.0570e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2525. State = [[-0.20650664  0.15360822]]. Action = [[ 0.0103865   0.01513416  0.24513799 -0.79181767]]. Reward = [0.]
Curr episode timestep = 721
Scene graph at timestep 2525 is [True, False, False, False, False, True]
State prediction error at timestep 2525 is tensor(8.7268e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2525 of 1
Current timestep = 2526. State = [[-0.20647982  0.1536149 ]]. Action = [[ 0.05059975  0.15515077 -0.03290376 -0.03942043]]. Reward = [0.]
Curr episode timestep = 722
Scene graph at timestep 2526 is [True, False, False, False, False, True]
State prediction error at timestep 2526 is tensor(3.2080e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2527. State = [[-0.20646441  0.15350167]]. Action = [[-0.01571888 -0.05571645  0.0905084   0.6746291 ]]. Reward = [0.]
Curr episode timestep = 723
Scene graph at timestep 2527 is [True, False, False, False, False, True]
State prediction error at timestep 2527 is tensor(4.6030e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2527 of 1
Current timestep = 2528. State = [[-0.20636508  0.15303224]]. Action = [[-0.04913898  0.16364235 -0.21950556  0.27799845]]. Reward = [0.]
Curr episode timestep = 724
Scene graph at timestep 2528 is [True, False, False, False, False, True]
State prediction error at timestep 2528 is tensor(6.1858e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2529. State = [[-0.2063467   0.15284467]]. Action = [[-0.17101996 -0.23653667 -0.14538123  0.19562685]]. Reward = [0.]
Curr episode timestep = 725
Scene graph at timestep 2529 is [True, False, False, False, False, True]
State prediction error at timestep 2529 is tensor(6.3846e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2530. State = [[-0.2061899   0.15262206]]. Action = [[ 0.10366568 -0.19800612 -0.03705242 -0.60534036]]. Reward = [0.]
Curr episode timestep = 726
Scene graph at timestep 2530 is [True, False, False, False, False, True]
State prediction error at timestep 2530 is tensor(3.5683e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2531. State = [[-0.20623757  0.15252133]]. Action = [[0.02276245 0.22311339 0.23692852 0.25182605]]. Reward = [0.]
Curr episode timestep = 727
Scene graph at timestep 2531 is [True, False, False, False, False, True]
State prediction error at timestep 2531 is tensor(2.1678e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2531 of 1
Current timestep = 2532. State = [[-0.20618601  0.15197843]]. Action = [[-0.0249927   0.00382233 -0.22824152 -0.80762863]]. Reward = [0.]
Curr episode timestep = 728
Scene graph at timestep 2532 is [True, False, False, False, False, True]
State prediction error at timestep 2532 is tensor(2.6870e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2533. State = [[-0.20605947  0.15189701]]. Action = [[-0.1575211  -0.12457782 -0.22773084 -0.54470843]]. Reward = [0.]
Curr episode timestep = 729
Scene graph at timestep 2533 is [True, False, False, False, False, True]
State prediction error at timestep 2533 is tensor(1.3170e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2534. State = [[-0.20614508  0.15178148]]. Action = [[-0.14799741  0.19546741 -0.01528469 -0.76711255]]. Reward = [0.]
Curr episode timestep = 730
Scene graph at timestep 2534 is [True, False, False, False, False, True]
State prediction error at timestep 2534 is tensor(5.6703e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2535. State = [[-0.20610054  0.1516644 ]]. Action = [[-0.18426454  0.06934673 -0.10153809 -0.22483015]]. Reward = [0.]
Curr episode timestep = 731
Scene graph at timestep 2535 is [True, False, False, False, False, True]
State prediction error at timestep 2535 is tensor(1.5381e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2536. State = [[-0.20604889  0.15141271]]. Action = [[-0.01762581 -0.07346612 -0.21402189  0.5871688 ]]. Reward = [0.]
Curr episode timestep = 732
Scene graph at timestep 2536 is [True, False, False, False, False, True]
State prediction error at timestep 2536 is tensor(3.3135e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2536 of 1
Current timestep = 2537. State = [[-0.20596224  0.1509504 ]]. Action = [[ 0.0278264   0.16632336 -0.06859663 -0.7029302 ]]. Reward = [0.]
Curr episode timestep = 733
Scene graph at timestep 2537 is [True, False, False, False, False, True]
State prediction error at timestep 2537 is tensor(1.1187e-09, grad_fn=<MseLossBackward0>)
Current timestep = 2538. State = [[-0.20597139  0.15044293]]. Action = [[ 0.00858474  0.20049194 -0.14373277 -0.19485998]]. Reward = [0.]
Curr episode timestep = 734
Scene graph at timestep 2538 is [True, False, False, False, False, True]
State prediction error at timestep 2538 is tensor(6.5603e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2539. State = [[-0.205797    0.14996652]]. Action = [[-0.06213591 -0.21509305  0.20164871  0.62534237]]. Reward = [0.]
Curr episode timestep = 735
Scene graph at timestep 2539 is [True, False, False, False, False, True]
State prediction error at timestep 2539 is tensor(1.0089e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2540. State = [[-0.20568505  0.14866568]]. Action = [[ 0.01074773 -0.02488157 -0.00829357 -0.05937028]]. Reward = [0.]
Curr episode timestep = 736
Scene graph at timestep 2540 is [True, False, False, False, False, True]
State prediction error at timestep 2540 is tensor(7.4540e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2540 of 1
Current timestep = 2541. State = [[-0.20574021  0.14798929]]. Action = [[ 0.16394949  0.06477821 -0.01238057 -0.6897561 ]]. Reward = [0.]
Curr episode timestep = 737
Scene graph at timestep 2541 is [True, False, False, False, False, True]
State prediction error at timestep 2541 is tensor(4.6294e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2542. State = [[-0.2056066   0.14756098]]. Action = [[-0.14243701 -0.01371169  0.01048952  0.4103074 ]]. Reward = [0.]
Curr episode timestep = 738
Scene graph at timestep 2542 is [True, False, False, False, False, True]
State prediction error at timestep 2542 is tensor(1.5074e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2543. State = [[-0.2055237   0.14724672]]. Action = [[-0.08936386  0.22104785  0.1397833   0.03850019]]. Reward = [0.]
Curr episode timestep = 739
Scene graph at timestep 2543 is [True, False, False, False, False, True]
State prediction error at timestep 2543 is tensor(7.0776e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2544. State = [[-0.20560673  0.14685021]]. Action = [[-0.04284416 -0.1716756  -0.09006417 -0.5439417 ]]. Reward = [0.]
Curr episode timestep = 740
Scene graph at timestep 2544 is [True, False, False, False, False, True]
State prediction error at timestep 2544 is tensor(1.1188e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2544 of 1
Current timestep = 2545. State = [[-0.20543757  0.14650255]]. Action = [[ 0.13505161  0.10369435 -0.07434647  0.87474155]]. Reward = [0.]
Curr episode timestep = 741
Scene graph at timestep 2545 is [True, False, False, False, False, True]
State prediction error at timestep 2545 is tensor(1.9712e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2546. State = [[-0.20547499  0.14628404]]. Action = [[ 0.01713863  0.21458438 -0.15809645  0.48737955]]. Reward = [0.]
Curr episode timestep = 742
Scene graph at timestep 2546 is [True, False, False, False, False, True]
State prediction error at timestep 2546 is tensor(9.3259e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2547. State = [[-0.20542961  0.14574103]]. Action = [[ 0.04855806 -0.00988708 -0.23612887 -0.37617236]]. Reward = [0.]
Curr episode timestep = 743
Scene graph at timestep 2547 is [True, False, False, False, False, True]
State prediction error at timestep 2547 is tensor(1.0512e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2547 of 1
Current timestep = 2548. State = [[-0.20535396  0.14562479]]. Action = [[ 0.23398948 -0.22977708  0.23717856  0.25101566]]. Reward = [0.]
Curr episode timestep = 744
Scene graph at timestep 2548 is [True, False, False, False, False, True]
State prediction error at timestep 2548 is tensor(7.1763e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2549. State = [[-0.20515476  0.1448478 ]]. Action = [[ 0.07770723 -0.01854199  0.19397342  0.21931124]]. Reward = [0.]
Curr episode timestep = 745
Scene graph at timestep 2549 is [True, False, False, False, False, True]
State prediction error at timestep 2549 is tensor(1.2978e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2549 of 1
Current timestep = 2550. State = [[-0.20493414  0.14453173]]. Action = [[ 0.11856106 -0.2427295   0.09321404  0.85404456]]. Reward = [0.]
Curr episode timestep = 746
Scene graph at timestep 2550 is [True, False, False, False, False, True]
State prediction error at timestep 2550 is tensor(5.1321e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2551. State = [[-0.20428781  0.14386587]]. Action = [[-0.11938506  0.12960136  0.22854465 -0.64822626]]. Reward = [0.]
Curr episode timestep = 747
Scene graph at timestep 2551 is [True, False, False, False, False, True]
State prediction error at timestep 2551 is tensor(2.3707e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2552. State = [[-0.20486541  0.1448633 ]]. Action = [[ 0.1879901  -0.20021485  0.1490506  -0.17508143]]. Reward = [0.]
Curr episode timestep = 748
Scene graph at timestep 2552 is [True, False, False, False, False, True]
State prediction error at timestep 2552 is tensor(6.3030e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2553. State = [[-0.20510358  0.14535697]]. Action = [[-0.09551957 -0.1579849   0.1988498   0.25785244]]. Reward = [0.]
Curr episode timestep = 749
Scene graph at timestep 2553 is [True, False, False, False, False, True]
State prediction error at timestep 2553 is tensor(7.2446e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2554. State = [[-0.20521775  0.14578657]]. Action = [[-0.09052733  0.2305305  -0.2017451   0.08540845]]. Reward = [0.]
Curr episode timestep = 750
Scene graph at timestep 2554 is [True, False, False, False, False, True]
State prediction error at timestep 2554 is tensor(4.1066e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2555. State = [[-0.20532705  0.14570765]]. Action = [[ 0.08816332  0.22996289 -0.22004895 -0.2449525 ]]. Reward = [0.]
Curr episode timestep = 751
Scene graph at timestep 2555 is [True, False, False, False, False, True]
State prediction error at timestep 2555 is tensor(1.0486e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2556. State = [[-0.20541894  0.14616816]]. Action = [[-0.2318811   0.10370821  0.01304072 -0.7630205 ]]. Reward = [0.]
Curr episode timestep = 752
Scene graph at timestep 2556 is [True, False, False, False, False, True]
State prediction error at timestep 2556 is tensor(3.1010e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2557. State = [[-0.20551388  0.14639996]]. Action = [[-0.1850777   0.14078307 -0.08436266 -0.69585985]]. Reward = [0.]
Curr episode timestep = 753
Scene graph at timestep 2557 is [True, False, False, False, False, True]
State prediction error at timestep 2557 is tensor(1.2967e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2557 of 1
Current timestep = 2558. State = [[-0.2056017   0.14633223]]. Action = [[-0.13343386  0.03989562 -0.07690829 -0.1333059 ]]. Reward = [0.]
Curr episode timestep = 754
Scene graph at timestep 2558 is [True, False, False, False, False, True]
State prediction error at timestep 2558 is tensor(7.5320e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2559. State = [[-0.20564297  0.14653936]]. Action = [[0.17262536 0.15686774 0.07373983 0.97747445]]. Reward = [0.]
Curr episode timestep = 755
Scene graph at timestep 2559 is [True, False, False, False, False, True]
State prediction error at timestep 2559 is tensor(2.2357e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2560. State = [[-0.20587929  0.14697474]]. Action = [[ 0.10395646 -0.16075082 -0.24129456 -0.5891008 ]]. Reward = [0.]
Curr episode timestep = 756
Scene graph at timestep 2560 is [True, False, False, False, False, True]
State prediction error at timestep 2560 is tensor(1.4259e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2561. State = [[-0.2059505   0.14737087]]. Action = [[ 0.12515754  0.09339482 -0.06197011  0.5604985 ]]. Reward = [0.]
Curr episode timestep = 757
Scene graph at timestep 2561 is [True, False, False, False, False, True]
State prediction error at timestep 2561 is tensor(5.4694e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2561 of 1
Current timestep = 2562. State = [[-0.2059022   0.14763565]]. Action = [[ 0.0983651  -0.21163301 -0.00103037 -0.371148  ]]. Reward = [0.]
Curr episode timestep = 758
Scene graph at timestep 2562 is [True, False, False, False, False, True]
State prediction error at timestep 2562 is tensor(1.6204e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2563. State = [[-0.20571388  0.14858136]]. Action = [[ 0.1119771  -0.11900523 -0.1448765  -0.08326495]]. Reward = [0.]
Curr episode timestep = 759
Scene graph at timestep 2563 is [True, False, False, False, False, True]
State prediction error at timestep 2563 is tensor(1.4449e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2563 of 1
Current timestep = 2564. State = [[-0.20480795  0.14796646]]. Action = [[ 0.0938617   0.16027328 -0.20885816  0.6210873 ]]. Reward = [0.]
Curr episode timestep = 760
Scene graph at timestep 2564 is [True, False, False, False, False, True]
State prediction error at timestep 2564 is tensor(7.2652e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2565. State = [[-0.2041438   0.14725968]]. Action = [[ 0.0204857  -0.20583579  0.14585164 -0.6600924 ]]. Reward = [0.]
Curr episode timestep = 761
Scene graph at timestep 2565 is [True, False, False, False, False, True]
State prediction error at timestep 2565 is tensor(5.3814e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2566. State = [[-0.20387317  0.14714326]]. Action = [[-0.1551788   0.21554449 -0.14994429 -0.16783154]]. Reward = [0.]
Curr episode timestep = 762
Scene graph at timestep 2566 is [True, False, False, False, False, True]
State prediction error at timestep 2566 is tensor(1.4693e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2566 of 1
Current timestep = 2567. State = [[-0.20345834  0.14710101]]. Action = [[-0.2288931   0.04642817  0.19813275  0.878068  ]]. Reward = [0.]
Curr episode timestep = 763
Scene graph at timestep 2567 is [True, False, False, False, False, True]
State prediction error at timestep 2567 is tensor(5.7655e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2568. State = [[-0.20294651  0.14682385]]. Action = [[-0.1396727  -0.12575786  0.10773975  0.5948577 ]]. Reward = [0.]
Curr episode timestep = 764
Scene graph at timestep 2568 is [True, False, False, False, False, True]
State prediction error at timestep 2568 is tensor(2.5180e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2569. State = [[-0.20294447  0.14671913]]. Action = [[-0.05284256  0.19934025  0.18583071 -0.25326467]]. Reward = [0.]
Curr episode timestep = 765
Scene graph at timestep 2569 is [True, False, False, False, False, True]
State prediction error at timestep 2569 is tensor(1.2059e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2570. State = [[-0.20252064  0.1464692 ]]. Action = [[ 0.08150053  0.1048905   0.2350696  -0.5332363 ]]. Reward = [0.]
Curr episode timestep = 766
Scene graph at timestep 2570 is [True, False, False, False, False, True]
State prediction error at timestep 2570 is tensor(9.9988e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2570 of 1
Current timestep = 2571. State = [[-0.20195913  0.14696585]]. Action = [[ 0.00730246 -0.21822764  0.15424699  0.27963173]]. Reward = [0.]
Curr episode timestep = 767
Scene graph at timestep 2571 is [True, False, False, False, False, True]
State prediction error at timestep 2571 is tensor(1.4794e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2572. State = [[-0.20164275  0.147279  ]]. Action = [[0.09463552 0.13642234 0.20821512 0.00144684]]. Reward = [0.]
Curr episode timestep = 768
Scene graph at timestep 2572 is [True, False, False, False, False, True]
State prediction error at timestep 2572 is tensor(8.6871e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2572 of 1
Current timestep = 2573. State = [[-0.20112275  0.14756484]]. Action = [[-0.01318169 -0.21315256  0.15120018 -0.03891551]]. Reward = [0.]
Curr episode timestep = 769
Scene graph at timestep 2573 is [True, False, False, False, False, True]
State prediction error at timestep 2573 is tensor(2.3242e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2574. State = [[-0.19946074  0.14854394]]. Action = [[-0.00127126  0.05079722  0.11081055 -0.17569351]]. Reward = [0.]
Curr episode timestep = 770
Scene graph at timestep 2574 is [True, False, False, False, False, True]
State prediction error at timestep 2574 is tensor(7.7043e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2575. State = [[-0.19933479  0.14890775]]. Action = [[ 0.23225695 -0.08722305  0.16753632  0.39421415]]. Reward = [0.]
Curr episode timestep = 771
Scene graph at timestep 2575 is [True, False, False, False, False, True]
State prediction error at timestep 2575 is tensor(1.1288e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2576. State = [[-0.19917534  0.14932294]]. Action = [[-0.23354715 -0.14617838  0.1628094   0.2999996 ]]. Reward = [0.]
Curr episode timestep = 772
Scene graph at timestep 2576 is [True, False, False, False, False, True]
State prediction error at timestep 2576 is tensor(4.7049e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2577. State = [[-0.19868314  0.1497613 ]]. Action = [[ 0.14590424  0.17885536 -0.1020854   0.53714895]]. Reward = [0.]
Curr episode timestep = 773
Scene graph at timestep 2577 is [True, False, False, False, False, True]
State prediction error at timestep 2577 is tensor(5.5500e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2578. State = [[-0.19733162  0.15089941]]. Action = [[ 0.07113731 -0.00889005 -0.03331906 -0.49668252]]. Reward = [0.]
Curr episode timestep = 774
Scene graph at timestep 2578 is [True, False, False, False, False, True]
State prediction error at timestep 2578 is tensor(1.7983e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2578 of 1
Current timestep = 2579. State = [[-0.1967007   0.15097192]]. Action = [[-0.23461893 -0.23160332  0.23586875  0.6217532 ]]. Reward = [0.]
Curr episode timestep = 775
Scene graph at timestep 2579 is [True, False, False, False, False, True]
State prediction error at timestep 2579 is tensor(1.6085e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2580. State = [[-0.19631931  0.15109232]]. Action = [[ 0.2057131  -0.10969776  0.11855149 -0.09504962]]. Reward = [0.]
Curr episode timestep = 776
Scene graph at timestep 2580 is [True, False, False, False, False, True]
State prediction error at timestep 2580 is tensor(7.3900e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2581. State = [[-0.19575492  0.15134357]]. Action = [[-0.21654104  0.16989267 -0.00251389  0.18234086]]. Reward = [0.]
Curr episode timestep = 777
Scene graph at timestep 2581 is [True, False, False, False, False, True]
State prediction error at timestep 2581 is tensor(3.9770e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2582. State = [[-0.19441015  0.15215585]]. Action = [[-0.06597462  0.09187281  0.0968481   0.21710622]]. Reward = [0.]
Curr episode timestep = 778
Scene graph at timestep 2582 is [True, False, False, False, False, True]
State prediction error at timestep 2582 is tensor(9.3041e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2582 of 1
Current timestep = 2583. State = [[-0.19461907  0.15308143]]. Action = [[ 0.00490326  0.16783792 -0.23433739  0.04188657]]. Reward = [0.]
Curr episode timestep = 779
Scene graph at timestep 2583 is [True, False, False, False, False, True]
State prediction error at timestep 2583 is tensor(5.5386e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2584. State = [[-0.19467974  0.15374501]]. Action = [[-0.19585487 -0.16137043  0.22328758  0.14731288]]. Reward = [0.]
Curr episode timestep = 780
Scene graph at timestep 2584 is [True, False, False, False, False, True]
State prediction error at timestep 2584 is tensor(2.7522e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2584 of 1
Current timestep = 2585. State = [[-0.19462977  0.15409166]]. Action = [[ 0.17585969 -0.20365572 -0.18433827 -0.758414  ]]. Reward = [0.]
Curr episode timestep = 781
Scene graph at timestep 2585 is [True, False, False, False, False, True]
State prediction error at timestep 2585 is tensor(4.2906e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2586. State = [[-0.19461787  0.15422973]]. Action = [[-0.03352112  0.1989727  -0.01131824 -0.1098147 ]]. Reward = [0.]
Curr episode timestep = 782
Scene graph at timestep 2586 is [True, False, False, False, False, True]
State prediction error at timestep 2586 is tensor(3.5486e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2587. State = [[-0.1948544   0.15454094]]. Action = [[ 0.00871095  0.23394    -0.20649344 -0.7747103 ]]. Reward = [0.]
Curr episode timestep = 783
Scene graph at timestep 2587 is [True, False, False, False, False, True]
State prediction error at timestep 2587 is tensor(1.3122e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2588. State = [[-0.19492021  0.15498997]]. Action = [[-0.06117076  0.20779216  0.05336878 -0.93943244]]. Reward = [0.]
Curr episode timestep = 784
Scene graph at timestep 2588 is [True, False, False, False, False, True]
State prediction error at timestep 2588 is tensor(8.3234e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2589. State = [[-0.19491743  0.15507375]]. Action = [[-0.10199609  0.16762114  0.0779421  -0.80240685]]. Reward = [0.]
Curr episode timestep = 785
Scene graph at timestep 2589 is [True, False, False, False, False, True]
State prediction error at timestep 2589 is tensor(8.6993e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2590. State = [[-0.19491386  0.1555396 ]]. Action = [[-0.06626156 -0.01854464 -0.22962984  0.64556456]]. Reward = [0.]
Curr episode timestep = 786
Scene graph at timestep 2590 is [True, False, False, False, False, True]
State prediction error at timestep 2590 is tensor(2.5115e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2590 of 1
Current timestep = 2591. State = [[-0.19523905  0.15621328]]. Action = [[ 0.0228157   0.1119532  -0.08328734  0.3996166 ]]. Reward = [0.]
Curr episode timestep = 787
Scene graph at timestep 2591 is [True, False, False, False, False, True]
State prediction error at timestep 2591 is tensor(1.1271e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2591 of 1
Current timestep = 2592. State = [[-0.19565433  0.15730567]]. Action = [[-0.21306565  0.15500581  0.11997956  0.3649447 ]]. Reward = [0.]
Curr episode timestep = 788
Scene graph at timestep 2592 is [True, False, False, False, False, True]
State prediction error at timestep 2592 is tensor(4.6778e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2593. State = [[-0.19605339  0.15808077]]. Action = [[-0.16455063  0.2034725  -0.09400457 -0.5698004 ]]. Reward = [0.]
Curr episode timestep = 789
Scene graph at timestep 2593 is [True, False, False, False, False, True]
State prediction error at timestep 2593 is tensor(4.3026e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2594. State = [[-0.19629943  0.15865049]]. Action = [[ 0.09162027 -0.23535925 -0.05337343  0.05490303]]. Reward = [0.]
Curr episode timestep = 790
Scene graph at timestep 2594 is [True, False, False, False, False, True]
State prediction error at timestep 2594 is tensor(1.4844e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2595. State = [[-0.19651142  0.15901169]]. Action = [[-0.16302839 -0.01099183 -0.21697803  0.34023714]]. Reward = [0.]
Curr episode timestep = 791
Scene graph at timestep 2595 is [True, False, False, False, False, True]
State prediction error at timestep 2595 is tensor(4.5364e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2595 of 1
Current timestep = 2596. State = [[-0.19685526  0.15984012]]. Action = [[-0.19325888  0.04060435  0.21143046 -0.3624105 ]]. Reward = [0.]
Curr episode timestep = 792
Scene graph at timestep 2596 is [True, False, False, False, False, True]
State prediction error at timestep 2596 is tensor(5.8058e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2597. State = [[-0.19702698  0.16015884]]. Action = [[ 0.01122224  0.22003514 -0.11012471 -0.6788036 ]]. Reward = [0.]
Curr episode timestep = 793
Scene graph at timestep 2597 is [True, False, False, False, False, True]
State prediction error at timestep 2597 is tensor(8.5332e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2598. State = [[-0.19719829  0.16056715]]. Action = [[-0.12115347  0.18421146 -0.10520574 -0.84600383]]. Reward = [0.]
Curr episode timestep = 794
Scene graph at timestep 2598 is [True, False, False, False, False, True]
State prediction error at timestep 2598 is tensor(1.1954e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2599. State = [[-0.19730863  0.16088155]]. Action = [[ 0.21583343  0.10728183 -0.11846627 -0.4595195 ]]. Reward = [0.]
Curr episode timestep = 795
Scene graph at timestep 2599 is [True, False, False, False, False, True]
State prediction error at timestep 2599 is tensor(1.5354e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2600. State = [[-0.19740464  0.16094838]]. Action = [[0.22389108 0.21094936 0.0230445  0.4232942 ]]. Reward = [0.]
Curr episode timestep = 796
Scene graph at timestep 2600 is [True, False, False, False, False, True]
State prediction error at timestep 2600 is tensor(4.3558e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2600 of 1
Current timestep = 2601. State = [[-0.19756223  0.16129115]]. Action = [[-0.01738396 -0.22871251  0.06922266 -0.7762539 ]]. Reward = [0.]
Curr episode timestep = 797
Scene graph at timestep 2601 is [True, False, False, False, False, True]
State prediction error at timestep 2601 is tensor(7.0565e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2602. State = [[-0.19765589  0.16152877]]. Action = [[ 0.2086634   0.22074294  0.15530348 -0.50935835]]. Reward = [0.]
Curr episode timestep = 798
Scene graph at timestep 2602 is [True, False, False, False, False, True]
State prediction error at timestep 2602 is tensor(2.3629e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2603. State = [[-0.19770496  0.16156708]]. Action = [[-0.21257375  0.00290084 -0.20682083 -0.0398857 ]]. Reward = [0.]
Curr episode timestep = 799
Scene graph at timestep 2603 is [True, False, False, False, False, True]
State prediction error at timestep 2603 is tensor(1.6178e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2604. State = [[-0.19778576  0.16180347]]. Action = [[-2.4356516e-01 -4.4544041e-04 -4.8786104e-03  7.3894656e-01]]. Reward = [0.]
Curr episode timestep = 800
Scene graph at timestep 2604 is [True, False, False, False, False, True]
State prediction error at timestep 2604 is tensor(7.6010e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2604 of 1
Current timestep = 2605. State = [[-0.19771639  0.16216789]]. Action = [[-0.07073933  0.20270854  0.14977291  0.25498092]]. Reward = [0.]
Curr episode timestep = 801
Scene graph at timestep 2605 is [True, False, False, False, False, True]
State prediction error at timestep 2605 is tensor(1.9055e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2606. State = [[-0.19777338  0.16205846]]. Action = [[-0.16128542  0.22119576 -0.15122533 -0.62324494]]. Reward = [0.]
Curr episode timestep = 802
Scene graph at timestep 2606 is [True, False, False, False, False, True]
State prediction error at timestep 2606 is tensor(1.3477e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2607. State = [[-0.19780026  0.16205204]]. Action = [[0.13677087 0.22967935 0.19646293 0.687587  ]]. Reward = [0.]
Curr episode timestep = 803
Scene graph at timestep 2607 is [True, False, False, False, False, True]
State prediction error at timestep 2607 is tensor(6.1335e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2607 of 1
Current timestep = 2608. State = [[-0.19786079  0.16244827]]. Action = [[ 0.07011038  0.19437     0.2331121  -0.17620307]]. Reward = [0.]
Curr episode timestep = 804
Scene graph at timestep 2608 is [True, False, False, False, False, True]
State prediction error at timestep 2608 is tensor(1.5934e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2609. State = [[-0.197893    0.16264464]]. Action = [[ 0.08667734 -0.11554378  0.10365924 -0.7192045 ]]. Reward = [0.]
Curr episode timestep = 805
Scene graph at timestep 2609 is [True, False, False, False, False, True]
State prediction error at timestep 2609 is tensor(4.4831e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2610. State = [[-0.19739616  0.1621983 ]]. Action = [[-0.19318335  0.1181806  -0.08816475 -0.72879475]]. Reward = [0.]
Curr episode timestep = 806
Scene graph at timestep 2610 is [True, False, False, False, False, True]
State prediction error at timestep 2610 is tensor(1.3349e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2611. State = [[-0.1962167   0.16111177]]. Action = [[ 0.01153046 -0.04494916  0.14936781 -0.5520013 ]]. Reward = [0.]
Curr episode timestep = 807
Scene graph at timestep 2611 is [True, False, False, False, False, True]
State prediction error at timestep 2611 is tensor(5.8607e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2612. State = [[-0.19504718  0.15958294]]. Action = [[ 0.01076031 -0.01428622 -0.01546745 -0.21989644]]. Reward = [0.]
Curr episode timestep = 808
Scene graph at timestep 2612 is [True, False, False, False, False, True]
State prediction error at timestep 2612 is tensor(8.1230e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2613. State = [[-0.19420545  0.15859078]]. Action = [[-0.02929281  0.04656169 -0.1455241   0.08811402]]. Reward = [0.]
Curr episode timestep = 809
Scene graph at timestep 2613 is [True, False, False, False, False, True]
State prediction error at timestep 2613 is tensor(5.5289e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2614. State = [[-0.19411252  0.15844269]]. Action = [[-0.0696464   0.22225511  0.10237619  0.11983848]]. Reward = [0.]
Curr episode timestep = 810
Scene graph at timestep 2614 is [True, False, False, False, False, True]
State prediction error at timestep 2614 is tensor(7.8261e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2615. State = [[-0.1941464   0.15841554]]. Action = [[ 0.10468543 -0.19711536 -0.08743392 -0.15937561]]. Reward = [0.]
Curr episode timestep = 811
Scene graph at timestep 2615 is [True, False, False, False, False, True]
State prediction error at timestep 2615 is tensor(2.0623e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2615 of 1
Current timestep = 2616. State = [[-0.19417733  0.15855879]]. Action = [[ 0.22705352  0.0712204  -0.17859298 -0.58828545]]. Reward = [0.]
Curr episode timestep = 812
Scene graph at timestep 2616 is [True, False, False, False, False, True]
State prediction error at timestep 2616 is tensor(2.2100e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2617. State = [[-0.19402753  0.15848473]]. Action = [[ 0.1290541  -0.00740744 -0.16310616 -0.85982966]]. Reward = [0.]
Curr episode timestep = 813
Scene graph at timestep 2617 is [True, False, False, False, False, True]
State prediction error at timestep 2617 is tensor(1.3923e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2618. State = [[-0.19318849  0.15862913]]. Action = [[-0.19623466  0.21342027 -0.01054195  0.15401912]]. Reward = [0.]
Curr episode timestep = 814
Scene graph at timestep 2618 is [True, False, False, False, False, True]
State prediction error at timestep 2618 is tensor(6.2861e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2619. State = [[-0.19240335  0.15882117]]. Action = [[ 0.12744981  0.23043358 -0.20567735 -0.65218675]]. Reward = [0.]
Curr episode timestep = 815
Scene graph at timestep 2619 is [True, False, False, False, False, True]
State prediction error at timestep 2619 is tensor(2.9926e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2620. State = [[-0.19040574  0.15900408]]. Action = [[ 0.09899229 -0.03527318 -0.07356644 -0.18750322]]. Reward = [0.]
Curr episode timestep = 816
Scene graph at timestep 2620 is [True, False, False, False, False, True]
State prediction error at timestep 2620 is tensor(3.5896e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2621. State = [[-0.18904202  0.1585093 ]]. Action = [[ 0.19852555  0.12565663  0.12398708 -0.7669135 ]]. Reward = [0.]
Curr episode timestep = 817
Scene graph at timestep 2621 is [True, False, False, False, False, True]
State prediction error at timestep 2621 is tensor(2.4279e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2622. State = [[-0.18827187  0.15842678]]. Action = [[-0.10156649 -0.20652688 -0.14761004 -0.02107644]]. Reward = [0.]
Curr episode timestep = 818
Scene graph at timestep 2622 is [True, False, False, False, False, True]
State prediction error at timestep 2622 is tensor(2.9031e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2623. State = [[-0.18453854  0.15855443]]. Action = [[0.11382902 0.12089789 0.16068524 0.6092684 ]]. Reward = [0.]
Curr episode timestep = 819
Scene graph at timestep 2623 is [True, False, False, False, False, True]
State prediction error at timestep 2623 is tensor(1.3788e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2623 of 1
Current timestep = 2624. State = [[-0.17987858  0.16102205]]. Action = [[-8.3349645e-04  1.0597980e-01  1.7625445e-01 -9.7816813e-01]]. Reward = [0.]
Curr episode timestep = 820
Scene graph at timestep 2624 is [True, False, False, False, False, True]
State prediction error at timestep 2624 is tensor(9.0854e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2625. State = [[-0.17880562  0.16221626]]. Action = [[ 0.11577353  0.19411707  0.04866886 -0.08617938]]. Reward = [0.]
Curr episode timestep = 821
Scene graph at timestep 2625 is [True, False, False, False, False, True]
State prediction error at timestep 2625 is tensor(8.7465e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2626. State = [[-0.17821036  0.16321914]]. Action = [[-0.19290255 -0.04316719 -0.10910438 -0.85582465]]. Reward = [0.]
Curr episode timestep = 822
Scene graph at timestep 2626 is [True, False, False, False, False, True]
State prediction error at timestep 2626 is tensor(9.9665e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2627. State = [[-0.17744787  0.16383569]]. Action = [[-0.2094033  -0.10521339 -0.10184418  0.78752935]]. Reward = [0.]
Curr episode timestep = 823
Scene graph at timestep 2627 is [True, False, False, False, False, True]
State prediction error at timestep 2627 is tensor(7.8245e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2628. State = [[-0.17629628  0.16420446]]. Action = [[ 0.23376718  0.2300865   0.13421038 -0.8134769 ]]. Reward = [0.]
Curr episode timestep = 824
Scene graph at timestep 2628 is [True, False, False, False, False, True]
State prediction error at timestep 2628 is tensor(5.2712e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2629. State = [[-0.17548537  0.16472179]]. Action = [[0.14211613 0.05169019 0.125826   0.26110613]]. Reward = [0.]
Curr episode timestep = 825
Scene graph at timestep 2629 is [True, False, False, False, False, True]
State prediction error at timestep 2629 is tensor(2.4792e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2630. State = [[-0.17475635  0.1655115 ]]. Action = [[ 0.2140001  -0.14574143  0.23864084 -0.72424495]]. Reward = [0.]
Curr episode timestep = 826
Scene graph at timestep 2630 is [True, False, False, False, False, True]
State prediction error at timestep 2630 is tensor(2.9345e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2630 of 1
Current timestep = 2631. State = [[-0.17385377  0.16592388]]. Action = [[-0.03635912  0.20437592 -0.01351804 -0.18004024]]. Reward = [0.]
Curr episode timestep = 827
Scene graph at timestep 2631 is [True, False, False, False, False, True]
State prediction error at timestep 2631 is tensor(1.1038e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2632. State = [[-0.17306374  0.16604139]]. Action = [[ 0.18661559 -0.12728919 -0.1265022  -0.52255315]]. Reward = [0.]
Curr episode timestep = 828
Scene graph at timestep 2632 is [True, False, False, False, False, True]
State prediction error at timestep 2632 is tensor(4.7725e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2633. State = [[-0.17098291  0.16727877]]. Action = [[ 0.04249468 -0.03846565 -0.15292357  0.76993906]]. Reward = [0.]
Curr episode timestep = 829
Scene graph at timestep 2633 is [True, False, False, False, False, True]
State prediction error at timestep 2633 is tensor(2.2646e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2634. State = [[-0.17069565  0.16737643]]. Action = [[-0.13402577  0.04250219  0.17183107 -0.6964464 ]]. Reward = [0.]
Curr episode timestep = 830
Scene graph at timestep 2634 is [True, False, False, False, False, True]
State prediction error at timestep 2634 is tensor(1.1637e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2635. State = [[-0.17039402  0.16742663]]. Action = [[0.17674503 0.15386528 0.09822175 0.8106835 ]]. Reward = [0.]
Curr episode timestep = 831
Scene graph at timestep 2635 is [True, False, False, False, False, True]
State prediction error at timestep 2635 is tensor(7.3623e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2636. State = [[-0.17024226  0.16755094]]. Action = [[-0.06704207 -0.22422075  0.13933414 -0.93778825]]. Reward = [0.]
Curr episode timestep = 832
Scene graph at timestep 2636 is [True, False, False, False, False, True]
State prediction error at timestep 2636 is tensor(1.0416e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2637. State = [[-0.17006168  0.16750607]]. Action = [[ 0.14528614  0.02336994  0.0772191  -0.6419154 ]]. Reward = [0.]
Curr episode timestep = 833
Scene graph at timestep 2637 is [True, False, False, False, False, True]
State prediction error at timestep 2637 is tensor(3.2189e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2637 of 1
Current timestep = 2638. State = [[-0.1699081   0.16766416]]. Action = [[ 0.18355227 -0.1405429   0.16177154 -0.38295096]]. Reward = [0.]
Curr episode timestep = 834
Scene graph at timestep 2638 is [True, False, False, False, False, True]
State prediction error at timestep 2638 is tensor(5.1685e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2639. State = [[-0.1697833   0.16772313]]. Action = [[-0.12533039  0.07182896 -0.24721861  0.8072972 ]]. Reward = [0.]
Curr episode timestep = 835
Scene graph at timestep 2639 is [True, False, False, False, False, True]
State prediction error at timestep 2639 is tensor(1.3927e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2640. State = [[-0.17043822  0.16914181]]. Action = [[ 0.09628239 -0.08577265  0.07729733  0.64047647]]. Reward = [0.]
Curr episode timestep = 836
Scene graph at timestep 2640 is [True, False, False, False, False, True]
State prediction error at timestep 2640 is tensor(1.2862e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2641. State = [[-0.1702174   0.16889681]]. Action = [[ 0.0428659  -0.22031294  0.24530256  0.32139337]]. Reward = [0.]
Curr episode timestep = 837
Scene graph at timestep 2641 is [True, False, False, False, False, True]
State prediction error at timestep 2641 is tensor(5.2003e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2641 of 1
Current timestep = 2642. State = [[-0.17007083  0.16851683]]. Action = [[ 0.06301346 -0.2210713   0.1454522   0.8647319 ]]. Reward = [0.]
Curr episode timestep = 838
Scene graph at timestep 2642 is [True, False, False, False, False, True]
State prediction error at timestep 2642 is tensor(3.7961e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2643. State = [[-0.16983628  0.16828261]]. Action = [[ 0.12514818  0.02443328 -0.13463293  0.16493177]]. Reward = [0.]
Curr episode timestep = 839
Scene graph at timestep 2643 is [True, False, False, False, False, True]
State prediction error at timestep 2643 is tensor(3.2478e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2644. State = [[-0.16926493  0.16842297]]. Action = [[ 0.19961202 -0.2262594   0.12239286 -0.9650469 ]]. Reward = [0.]
Curr episode timestep = 840
Scene graph at timestep 2644 is [True, False, False, False, False, True]
State prediction error at timestep 2644 is tensor(7.5568e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2645. State = [[-0.1687255   0.16850021]]. Action = [[-0.24018733 -0.10600051 -0.2376615   0.7502761 ]]. Reward = [0.]
Curr episode timestep = 841
Scene graph at timestep 2645 is [True, False, False, False, False, True]
State prediction error at timestep 2645 is tensor(1.4054e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2646. State = [[-0.16711754  0.1689143 ]]. Action = [[-0.10058431  0.11973405  0.07252026  0.7167957 ]]. Reward = [0.]
Curr episode timestep = 842
Scene graph at timestep 2646 is [True, False, False, False, False, True]
State prediction error at timestep 2646 is tensor(5.4456e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2647. State = [[-0.1675056  0.1709501]]. Action = [[0.12946731 0.03194991 0.05442193 0.7918427 ]]. Reward = [0.]
Curr episode timestep = 843
Scene graph at timestep 2647 is [True, False, False, False, False, True]
State prediction error at timestep 2647 is tensor(9.7279e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2648. State = [[-0.16663319  0.17163932]]. Action = [[ 0.18303594  0.01574886 -0.2109538  -0.33834296]]. Reward = [0.]
Curr episode timestep = 844
Scene graph at timestep 2648 is [True, False, False, False, False, True]
State prediction error at timestep 2648 is tensor(5.3675e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2649. State = [[-0.16612037  0.17222896]]. Action = [[-0.14420743  0.20538023  0.24070504 -0.9596391 ]]. Reward = [0.]
Curr episode timestep = 845
Scene graph at timestep 2649 is [True, False, False, False, False, True]
State prediction error at timestep 2649 is tensor(1.1317e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2649 of 1
Current timestep = 2650. State = [[-0.16331275  0.17349675]]. Action = [[-0.027641   -0.04210472 -0.15925406 -0.04512864]]. Reward = [0.]
Curr episode timestep = 846
Scene graph at timestep 2650 is [True, False, False, False, False, True]
State prediction error at timestep 2650 is tensor(9.1389e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2651. State = [[-0.1615662  0.1740653]]. Action = [[ 0.08690736  0.03314805 -0.09949239 -0.3975854 ]]. Reward = [0.]
Curr episode timestep = 847
Scene graph at timestep 2651 is [True, False, False, False, False, True]
State prediction error at timestep 2651 is tensor(3.1668e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2652. State = [[-0.15931462  0.17497979]]. Action = [[ 0.07112938  0.1056402  -0.05325949  0.79556596]]. Reward = [0.]
Curr episode timestep = 848
Scene graph at timestep 2652 is [True, False, False, False, False, True]
State prediction error at timestep 2652 is tensor(1.0556e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2652 of 1
Current timestep = 2653. State = [[-0.15868703  0.17567158]]. Action = [[ 0.06453818 -0.19040497  0.24510244 -0.592413  ]]. Reward = [0.]
Curr episode timestep = 849
Scene graph at timestep 2653 is [True, False, False, False, False, True]
State prediction error at timestep 2653 is tensor(2.3669e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2654. State = [[-0.15829523  0.17618339]]. Action = [[-0.04678111  0.20355475  0.02077973 -0.9205236 ]]. Reward = [0.]
Curr episode timestep = 850
Scene graph at timestep 2654 is [True, False, False, False, False, True]
State prediction error at timestep 2654 is tensor(4.1554e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2655. State = [[-0.15544821  0.17819259]]. Action = [[0.04397252 0.11989674 0.00933746 0.7727158 ]]. Reward = [0.]
Curr episode timestep = 851
Scene graph at timestep 2655 is [True, False, False, False, False, True]
State prediction error at timestep 2655 is tensor(8.7657e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2655 of 1
Current timestep = 2656. State = [[-0.15196238  0.18306577]]. Action = [[ 0.09363958  0.03299415 -0.2274894  -0.79286844]]. Reward = [0.]
Curr episode timestep = 852
Scene graph at timestep 2656 is [True, False, False, False, False, True]
State prediction error at timestep 2656 is tensor(4.4933e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2656 of 1
Current timestep = 2657. State = [[-0.14759323  0.18694441]]. Action = [[-0.02042753  0.02388528  0.0512428  -0.49936318]]. Reward = [0.]
Curr episode timestep = 853
Scene graph at timestep 2657 is [True, False, False, False, False, True]
State prediction error at timestep 2657 is tensor(2.9746e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2657 of 1
Current timestep = 2658. State = [[-0.14424668  0.19071515]]. Action = [[ 0.10624036  0.0601387  -0.05788609  0.4238882 ]]. Reward = [0.]
Curr episode timestep = 854
Scene graph at timestep 2658 is [True, False, False, False, False, True]
State prediction error at timestep 2658 is tensor(3.9867e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2659. State = [[-0.14318404  0.19174361]]. Action = [[-0.18981573  0.08577666 -0.10296449 -0.9183334 ]]. Reward = [0.]
Curr episode timestep = 855
Scene graph at timestep 2659 is [True, False, False, False, False, True]
State prediction error at timestep 2659 is tensor(9.0936e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2660. State = [[-0.13923813  0.19467072]]. Action = [[-0.10410437 -0.02890143  0.16207188 -0.68286896]]. Reward = [0.]
Curr episode timestep = 856
Scene graph at timestep 2660 is [True, False, False, False, False, True]
State prediction error at timestep 2660 is tensor(4.4412e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2660 of 1
Current timestep = 2661. State = [[-0.13792297  0.19660172]]. Action = [[-0.03150985 -0.06379002  0.0593361  -0.6147217 ]]. Reward = [0.]
Curr episode timestep = 857
Scene graph at timestep 2661 is [True, False, False, False, False, True]
State prediction error at timestep 2661 is tensor(2.6533e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2661 of 1
Current timestep = 2662. State = [[-0.13817978  0.19647942]]. Action = [[-0.02718058  0.22000632  0.11090052  0.13370192]]. Reward = [0.]
Curr episode timestep = 858
Scene graph at timestep 2662 is [True, False, False, False, False, True]
State prediction error at timestep 2662 is tensor(1.0978e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2663. State = [[-0.13839194  0.19628039]]. Action = [[ 0.0422723  -0.09000617  0.21993792  0.41018665]]. Reward = [0.]
Curr episode timestep = 859
Scene graph at timestep 2663 is [True, False, False, False, False, True]
State prediction error at timestep 2663 is tensor(9.3545e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2663 of 1
Current timestep = 2664. State = [[-0.13801822  0.1956876 ]]. Action = [[-0.15013613 -0.06632087  0.18752134  0.25064886]]. Reward = [0.]
Curr episode timestep = 860
Scene graph at timestep 2664 is [True, False, False, False, False, True]
State prediction error at timestep 2664 is tensor(6.6849e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2665. State = [[-0.13754423  0.19466874]]. Action = [[-0.02638507 -0.11178064 -0.06001692 -0.30939263]]. Reward = [0.]
Curr episode timestep = 861
Scene graph at timestep 2665 is [True, False, False, False, False, True]
State prediction error at timestep 2665 is tensor(1.0868e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2666. State = [[-0.13635771  0.1916263 ]]. Action = [[-0.05532093 -0.04813559 -0.23523967 -0.36592323]]. Reward = [0.]
Curr episode timestep = 862
Scene graph at timestep 2666 is [True, False, False, False, False, True]
State prediction error at timestep 2666 is tensor(1.0287e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2667. State = [[-0.1361892   0.19062446]]. Action = [[-0.13906349 -0.01217219 -0.0059557  -0.3669449 ]]. Reward = [0.]
Curr episode timestep = 863
Scene graph at timestep 2667 is [True, False, False, False, False, True]
State prediction error at timestep 2667 is tensor(8.4060e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2668. State = [[-0.13594465  0.18989417]]. Action = [[-0.2005298   0.2490179   0.1841358  -0.14687467]]. Reward = [0.]
Curr episode timestep = 864
Scene graph at timestep 2668 is [True, False, False, False, False, True]
State prediction error at timestep 2668 is tensor(2.0098e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2668 of 1
Current timestep = 2669. State = [[-0.1357617   0.18917705]]. Action = [[ 0.10130766 -0.1526817   0.01536086 -0.01151353]]. Reward = [0.]
Curr episode timestep = 865
Scene graph at timestep 2669 is [True, False, False, False, False, True]
State prediction error at timestep 2669 is tensor(9.7626e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2670. State = [[-0.13548979  0.18737614]]. Action = [[ 0.0028441   0.11116982 -0.1796291  -0.81327134]]. Reward = [0.]
Curr episode timestep = 866
Scene graph at timestep 2670 is [True, False, False, False, False, True]
State prediction error at timestep 2670 is tensor(7.4352e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2670 of 1
Current timestep = 2671. State = [[-0.13574608  0.18769474]]. Action = [[0.21916968 0.21516526 0.22274601 0.34905982]]. Reward = [0.]
Curr episode timestep = 867
Scene graph at timestep 2671 is [True, False, False, False, False, True]
State prediction error at timestep 2671 is tensor(3.8722e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2672. State = [[-0.13615486  0.18849018]]. Action = [[-0.16153963 -0.03735189  0.20377415  0.09272456]]. Reward = [0.]
Curr episode timestep = 868
Scene graph at timestep 2672 is [True, False, False, False, False, True]
State prediction error at timestep 2672 is tensor(4.4266e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2673. State = [[-0.13650249  0.18877533]]. Action = [[ 0.08463943 -0.07362399 -0.14445862  0.35358   ]]. Reward = [0.]
Curr episode timestep = 869
Scene graph at timestep 2673 is [True, False, False, False, False, True]
State prediction error at timestep 2673 is tensor(8.1035e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2674. State = [[-0.13556767  0.18717082]]. Action = [[ 0.07357925  0.13001478  0.17078698 -0.18487883]]. Reward = [0.]
Curr episode timestep = 870
Scene graph at timestep 2674 is [True, False, False, False, False, True]
State prediction error at timestep 2674 is tensor(3.8620e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2675. State = [[-0.13528721  0.18775786]]. Action = [[-0.13201922 -0.05885077  0.20136535  0.3910159 ]]. Reward = [0.]
Curr episode timestep = 871
Scene graph at timestep 2675 is [True, False, False, False, False, True]
State prediction error at timestep 2675 is tensor(8.0292e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2675 of 1
Current timestep = 2676. State = [[-0.13543156  0.18794008]]. Action = [[ 0.03193715 -0.09463555  0.00604481  0.20346534]]. Reward = [0.]
Curr episode timestep = 872
Scene graph at timestep 2676 is [True, False, False, False, False, True]
State prediction error at timestep 2676 is tensor(1.0639e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2676 of 1
Current timestep = 2677. State = [[-0.13526753  0.18748437]]. Action = [[-0.2006218  -0.19824319 -0.17709456  0.12235987]]. Reward = [0.]
Curr episode timestep = 873
Scene graph at timestep 2677 is [True, False, False, False, False, True]
State prediction error at timestep 2677 is tensor(4.8301e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2678. State = [[-0.13518244  0.18715076]]. Action = [[-0.14839683 -0.05654564  0.23846388 -0.31648707]]. Reward = [0.]
Curr episode timestep = 874
Scene graph at timestep 2678 is [True, False, False, False, False, True]
State prediction error at timestep 2678 is tensor(9.2085e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2679. State = [[-0.1350138   0.18686093]]. Action = [[ 0.16241425 -0.23623721 -0.2091405  -0.74072266]]. Reward = [0.]
Curr episode timestep = 875
Scene graph at timestep 2679 is [True, False, False, False, False, True]
State prediction error at timestep 2679 is tensor(1.4776e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2680. State = [[-0.13460954  0.1859466 ]]. Action = [[-0.01101415 -0.11219868  0.20324385 -0.5241566 ]]. Reward = [0.]
Curr episode timestep = 876
Scene graph at timestep 2680 is [True, False, False, False, False, True]
State prediction error at timestep 2680 is tensor(9.6627e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2680 of 1
Current timestep = 2681. State = [[-0.13417335  0.18454368]]. Action = [[-0.23298007  0.20884615  0.17563075 -0.65274346]]. Reward = [0.]
Curr episode timestep = 877
Scene graph at timestep 2681 is [True, False, False, False, False, True]
State prediction error at timestep 2681 is tensor(1.0295e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2682. State = [[-0.13329647  0.18199116]]. Action = [[ 0.00273281  0.01985466 -0.10764952  0.37016225]]. Reward = [0.]
Curr episode timestep = 878
Scene graph at timestep 2682 is [True, False, False, False, False, True]
State prediction error at timestep 2682 is tensor(8.5627e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2682 of 1
Current timestep = 2683. State = [[-0.13267776  0.1807295 ]]. Action = [[ 0.05685303 -0.07377899  0.18042767 -0.8835126 ]]. Reward = [0.]
Curr episode timestep = 879
Scene graph at timestep 2683 is [True, False, False, False, False, True]
State prediction error at timestep 2683 is tensor(4.9909e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2683 of 1
Current timestep = 2684. State = [[-0.13170035  0.17851292]]. Action = [[ 0.13875395 -0.19005488  0.03491598  0.38113117]]. Reward = [0.]
Curr episode timestep = 880
Scene graph at timestep 2684 is [True, False, False, False, False, True]
State prediction error at timestep 2684 is tensor(9.0276e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2685. State = [[-0.13145016  0.177951  ]]. Action = [[-0.21000862 -0.13985391  0.20523596  0.17884386]]. Reward = [0.]
Curr episode timestep = 881
Scene graph at timestep 2685 is [True, False, False, False, False, True]
State prediction error at timestep 2685 is tensor(1.3294e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2686. State = [[-0.13131727  0.17763524]]. Action = [[-0.20016757  0.05041566  0.02483937  0.8658364 ]]. Reward = [0.]
Curr episode timestep = 882
Scene graph at timestep 2686 is [True, False, False, False, False, True]
State prediction error at timestep 2686 is tensor(1.4641e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2686 of 1
Current timestep = 2687. State = [[-0.13111863  0.17713153]]. Action = [[ 0.20327592 -0.1664577  -0.16317989 -0.10999513]]. Reward = [0.]
Curr episode timestep = 883
Scene graph at timestep 2687 is [True, False, False, False, False, True]
State prediction error at timestep 2687 is tensor(1.1117e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2688. State = [[-0.13075595  0.1764415 ]]. Action = [[-0.09879109  0.12806803  0.09973651 -0.9826834 ]]. Reward = [0.]
Curr episode timestep = 884
Scene graph at timestep 2688 is [True, False, False, False, False, True]
State prediction error at timestep 2688 is tensor(4.3423e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2689. State = [[-0.13190426  0.17797582]]. Action = [[-0.07697836 -0.12312278 -0.02255102  0.693478  ]]. Reward = [0.]
Curr episode timestep = 885
Scene graph at timestep 2689 is [True, False, False, False, False, True]
State prediction error at timestep 2689 is tensor(1.2217e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2690. State = [[-0.1318206   0.17761667]]. Action = [[-0.1544759   0.11059844  0.01263651  0.02028573]]. Reward = [0.]
Curr episode timestep = 886
Scene graph at timestep 2690 is [True, False, False, False, False, True]
State prediction error at timestep 2690 is tensor(7.7756e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2690 of 1
Current timestep = 2691. State = [[-0.13181369  0.17770621]]. Action = [[-0.19437753  0.24412555  0.07340866 -0.6918897 ]]. Reward = [0.]
Curr episode timestep = 887
Scene graph at timestep 2691 is [True, False, False, False, False, True]
State prediction error at timestep 2691 is tensor(5.5413e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2692. State = [[-0.13195698  0.17699516]]. Action = [[ 0.05144846 -0.11221126 -0.07468444  0.28982747]]. Reward = [0.]
Curr episode timestep = 888
Scene graph at timestep 2692 is [True, False, False, False, False, True]
State prediction error at timestep 2692 is tensor(1.1128e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2693. State = [[-0.13157141  0.1754304 ]]. Action = [[-0.20721208 -0.23482856  0.05016413 -0.3892436 ]]. Reward = [0.]
Curr episode timestep = 889
Scene graph at timestep 2693 is [True, False, False, False, False, True]
State prediction error at timestep 2693 is tensor(3.6681e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2694. State = [[-0.13126948  0.1745602 ]]. Action = [[-0.1749887   0.01576686  0.13874269 -0.27006578]]. Reward = [0.]
Curr episode timestep = 890
Scene graph at timestep 2694 is [True, False, False, False, False, True]
State prediction error at timestep 2694 is tensor(8.9193e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2695. State = [[-0.13071805  0.1721226 ]]. Action = [[-0.03740573 -0.04315934  0.11740702  0.42360604]]. Reward = [0.]
Curr episode timestep = 891
Scene graph at timestep 2695 is [True, False, False, False, False, True]
State prediction error at timestep 2695 is tensor(1.4239e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2696. State = [[-0.13076368  0.17141439]]. Action = [[ 0.03047547  0.14013529  0.13956091 -0.18651956]]. Reward = [0.]
Curr episode timestep = 892
Scene graph at timestep 2696 is [True, False, False, False, False, True]
State prediction error at timestep 2696 is tensor(3.5221e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2697. State = [[-0.13082594  0.17055383]]. Action = [[ 0.09958017  0.20827961 -0.03344969  0.11895669]]. Reward = [0.]
Curr episode timestep = 893
Scene graph at timestep 2697 is [True, False, False, False, False, True]
State prediction error at timestep 2697 is tensor(5.2852e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2698. State = [[-0.13077898  0.16968305]]. Action = [[-0.16888715  0.07287699  0.2426362  -0.25483668]]. Reward = [0.]
Curr episode timestep = 894
Scene graph at timestep 2698 is [True, False, False, False, False, True]
State prediction error at timestep 2698 is tensor(4.9868e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2699. State = [[-0.13075684  0.16890657]]. Action = [[-0.05101401 -0.20288227 -0.15294391 -0.1633572 ]]. Reward = [0.]
Curr episode timestep = 895
Scene graph at timestep 2699 is [True, False, False, False, False, True]
State prediction error at timestep 2699 is tensor(1.9575e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2700. State = [[-0.13086268  0.16833186]]. Action = [[ 0.14408478  0.00481221 -0.10760422  0.7144028 ]]. Reward = [0.]
Curr episode timestep = 896
Scene graph at timestep 2700 is [True, False, False, False, False, True]
State prediction error at timestep 2700 is tensor(1.5797e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2701. State = [[-0.13087353  0.16749173]]. Action = [[0.23424804 0.01504278 0.11857647 0.01006377]]. Reward = [0.]
Curr episode timestep = 897
Scene graph at timestep 2701 is [True, False, False, False, False, True]
State prediction error at timestep 2701 is tensor(8.3345e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2701 of 1
Current timestep = 2702. State = [[-0.13070549  0.16697615]]. Action = [[ 0.02837899  0.18147802 -0.10755609 -0.6405653 ]]. Reward = [0.]
Curr episode timestep = 898
Scene graph at timestep 2702 is [True, False, False, False, False, True]
State prediction error at timestep 2702 is tensor(1.0497e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2703. State = [[-0.13077363  0.16648158]]. Action = [[ 0.16364297  0.22539341 -0.05828167 -0.11003721]]. Reward = [0.]
Curr episode timestep = 899
Scene graph at timestep 2703 is [True, False, False, False, False, True]
State prediction error at timestep 2703 is tensor(7.0960e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2704. State = [[-0.13062368  0.16618639]]. Action = [[-0.23047252  0.15361351  0.15774181  0.9125891 ]]. Reward = [0.]
Curr episode timestep = 900
Scene graph at timestep 2704 is [True, False, False, False, False, True]
State prediction error at timestep 2704 is tensor(7.2752e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2705. State = [[-0.25777957  0.00806759]]. Action = [[-0.22609402  0.2411268  -0.01382168  0.7911427 ]]. Reward = [0.]
Curr episode timestep = 901
Scene graph at timestep 2705 is [True, False, False, False, True, False]
State prediction error at timestep 2705 is tensor(0.0209, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2705 of 1
Current timestep = 2706. State = [[-0.25777957  0.00806759]]. Action = [[-0.23048164 -0.05563852  0.22699124  0.28596961]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 2706 is [True, False, False, False, True, False]
State prediction error at timestep 2706 is tensor(1.1709e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2707. State = [[-0.25777957  0.00806759]]. Action = [[ 0.1476062  -0.17127347  0.21240595 -0.46289438]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 2707 is [True, False, False, False, True, False]
State prediction error at timestep 2707 is tensor(6.3281e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2708. State = [[-0.25777957  0.00806759]]. Action = [[ 0.15893859  0.1032612  -0.22817078  0.79634714]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2708 is [True, False, False, False, True, False]
State prediction error at timestep 2708 is tensor(3.3908e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2709. State = [[-0.25540593  0.01050443]]. Action = [[-0.03252569  0.02566934 -0.24182771 -0.51491416]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 2709 is [True, False, False, False, True, False]
State prediction error at timestep 2709 is tensor(2.1381e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2710. State = [[-0.25162187  0.01443717]]. Action = [[ 0.0210588   0.23771352 -0.15053286  0.68864226]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 2710 is [True, False, False, False, True, False]
State prediction error at timestep 2710 is tensor(9.8853e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2711. State = [[-0.24687396  0.0200366 ]]. Action = [[-0.11068127  0.21390063  0.04996365  0.38478482]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 2711 is [True, False, False, False, True, False]
State prediction error at timestep 2711 is tensor(9.5744e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2712. State = [[-0.24122784  0.02698357]]. Action = [[ 0.20856482  0.08290148 -0.1080904   0.44160986]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 2712 is [True, False, False, False, True, False]
State prediction error at timestep 2712 is tensor(3.3744e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2713. State = [[-0.23240402  0.03922211]]. Action = [[0.22764418 0.09350008 0.20242071 0.63905525]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 2713 is [True, False, False, False, True, False]
State prediction error at timestep 2713 is tensor(9.0538e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2714. State = [[-0.22473006  0.05410481]]. Action = [[-0.1739635  -0.1077641   0.04523185  0.51071453]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 2714 is [True, False, False, False, True, False]
State prediction error at timestep 2714 is tensor(8.5098e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2715. State = [[-0.217196    0.06913891]]. Action = [[-0.12035431  0.15151417 -0.05447029  0.5527723 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 2715 is [True, False, False, False, True, False]
State prediction error at timestep 2715 is tensor(8.7353e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2716. State = [[-0.20907347  0.081868  ]]. Action = [[-0.14714752  0.14459336 -0.20437653 -0.7070271 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 2716 is [True, False, False, False, True, False]
State prediction error at timestep 2716 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 2717. State = [[-0.2021945   0.09182082]]. Action = [[-0.10144837  0.16977823  0.01189592 -0.12852609]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 2717 is [True, False, False, False, True, False]
State prediction error at timestep 2717 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 2718. State = [[-0.19624609  0.10079779]]. Action = [[ 0.08220041 -0.17459357  0.13029134 -0.06875151]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 2718 is [True, False, False, False, True, False]
State prediction error at timestep 2718 is tensor(6.9069e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2719. State = [[-0.1905061   0.10856666]]. Action = [[-0.02354258  0.1930773  -0.0899158   0.4427315 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 2719 is [True, False, False, False, True, False]
State prediction error at timestep 2719 is tensor(3.9934e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2720. State = [[-0.18521336  0.11493802]]. Action = [[ 0.21999729  0.21428376 -0.00805293  0.30571055]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 2720 is [True, False, False, False, True, False]
State prediction error at timestep 2720 is tensor(4.1825e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2721. State = [[-0.18113044  0.12010379]]. Action = [[-0.2061593   0.17550987 -0.15048519  0.33185267]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 2721 is [True, False, False, False, True, False]
State prediction error at timestep 2721 is tensor(1.5860e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2722. State = [[-0.17829369  0.12364308]]. Action = [[-0.21217677 -0.01691039 -0.07885724 -0.8253048 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 2722 is [True, False, False, False, True, False]
State prediction error at timestep 2722 is tensor(4.3942e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2723. State = [[-0.17383152  0.12881006]]. Action = [[-0.01247765  0.2160033  -0.18205695  0.41310096]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 2723 is [True, False, False, False, False, True]
State prediction error at timestep 2723 is tensor(3.6092e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2724. State = [[-0.17010815  0.13381623]]. Action = [[-0.19053382  0.21780553 -0.24541885  0.5923257 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 2724 is [True, False, False, False, False, True]
State prediction error at timestep 2724 is tensor(1.6320e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2725. State = [[-0.16278656  0.14285608]]. Action = [[-0.10203142 -0.09526885  0.09435827  0.32572985]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 2725 is [True, False, False, False, False, True]
State prediction error at timestep 2725 is tensor(5.9038e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2726. State = [[-0.1620177   0.14343452]]. Action = [[-0.05927855 -0.1389225   0.1515888  -0.08937603]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 2726 is [True, False, False, False, False, True]
State prediction error at timestep 2726 is tensor(8.6504e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2727. State = [[-0.16152133  0.14394462]]. Action = [[ 0.23889244  0.15685189  0.07733002 -0.40526938]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 2727 is [True, False, False, False, False, True]
State prediction error at timestep 2727 is tensor(1.9844e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2728. State = [[-0.16123253  0.14412773]]. Action = [[-0.18530107  0.11794907 -0.1913042   0.3269881 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 2728 is [True, False, False, False, False, True]
State prediction error at timestep 2728 is tensor(2.5649e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2729. State = [[-0.16087879  0.14502406]]. Action = [[-0.02353212  0.22265956  0.04980582  0.5096376 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 2729 is [True, False, False, False, False, True]
State prediction error at timestep 2729 is tensor(8.1525e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2730. State = [[-0.16087203  0.14538607]]. Action = [[-0.20397897  0.2076537  -0.09570649  0.01229417]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 2730 is [True, False, False, False, False, True]
State prediction error at timestep 2730 is tensor(1.7452e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2731. State = [[-0.16122244  0.1458383 ]]. Action = [[-0.04791191  0.06981722  0.07025951  0.01294065]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 2731 is [True, False, False, False, False, True]
State prediction error at timestep 2731 is tensor(1.0514e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2732. State = [[-0.16147397  0.14616714]]. Action = [[ 0.04087934  0.15876484 -0.12611157 -0.93929625]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 2732 is [True, False, False, False, False, True]
State prediction error at timestep 2732 is tensor(1.3167e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2733. State = [[-0.16198447  0.1473455 ]]. Action = [[-0.10913733  0.15253058  0.07046032 -0.3801931 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 2733 is [True, False, False, False, False, True]
State prediction error at timestep 2733 is tensor(3.3695e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2734. State = [[-0.16211188  0.14775814]]. Action = [[-0.12493718  0.19452104  0.07145178 -0.8090032 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 2734 is [True, False, False, False, False, True]
State prediction error at timestep 2734 is tensor(2.2705e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2735. State = [[-0.16239572  0.14809713]]. Action = [[ 0.19876722  0.10464579 -0.1941947  -0.8443546 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 2735 is [True, False, False, False, False, True]
State prediction error at timestep 2735 is tensor(2.3592e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2736. State = [[-0.16269134  0.14837766]]. Action = [[ 0.01237983 -0.17995769 -0.00938338 -0.02278119]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 2736 is [True, False, False, False, False, True]
State prediction error at timestep 2736 is tensor(2.6236e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2737. State = [[-0.16303562  0.14935258]]. Action = [[-0.10418719 -0.20743349  0.20027453  0.50483894]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 2737 is [True, False, False, False, False, True]
State prediction error at timestep 2737 is tensor(5.9342e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2738. State = [[-0.16325507  0.14951576]]. Action = [[ 0.01408139 -0.18575914  0.12501094 -0.06063253]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 2738 is [True, False, False, False, False, True]
State prediction error at timestep 2738 is tensor(3.2223e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2739. State = [[-0.16346896  0.14978243]]. Action = [[-0.14357066  0.23876768  0.23161006 -0.81363964]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 2739 is [True, False, False, False, False, True]
State prediction error at timestep 2739 is tensor(1.7211e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2740. State = [[-0.16372801  0.15051073]]. Action = [[-0.01662394 -0.01397942 -0.13619067  0.26381063]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 2740 is [True, False, False, False, False, True]
State prediction error at timestep 2740 is tensor(1.2362e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2741. State = [[-0.16385677  0.15065692]]. Action = [[-0.12243992 -0.05814153  0.11903065 -0.35203516]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 2741 is [True, False, False, False, False, True]
State prediction error at timestep 2741 is tensor(1.7900e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2742. State = [[-0.1640627   0.15016095]]. Action = [[ 0.04466149 -0.05528501  0.05378398 -0.38950002]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 2742 is [True, False, False, False, False, True]
State prediction error at timestep 2742 is tensor(1.0418e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2743. State = [[-0.16399631  0.1496054 ]]. Action = [[-0.1283338   0.16735327 -0.12416939  0.02695525]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 2743 is [True, False, False, False, False, True]
State prediction error at timestep 2743 is tensor(5.2483e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2744. State = [[-0.1640093   0.14909615]]. Action = [[-0.06621939  0.17826515 -0.2321078  -0.20048529]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 2744 is [True, False, False, False, False, True]
State prediction error at timestep 2744 is tensor(7.8415e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2745. State = [[-0.1640342   0.14729832]]. Action = [[-0.10561635 -0.04363608 -0.01987188 -0.41985208]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 2745 is [True, False, False, False, False, True]
State prediction error at timestep 2745 is tensor(9.4181e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2746. State = [[-0.16408093  0.14684032]]. Action = [[ 0.12230146  0.19312248 -0.14402795 -0.46596503]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 2746 is [True, False, False, False, False, True]
State prediction error at timestep 2746 is tensor(1.0702e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2747. State = [[-0.16410439  0.14549778]]. Action = [[ 0.05515918 -0.11639425  0.17975307 -0.93710166]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 2747 is [True, False, False, False, False, True]
State prediction error at timestep 2747 is tensor(1.1750e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2748. State = [[-0.1639004   0.14441112]]. Action = [[-0.1843266   0.15656227  0.09095848  0.86479247]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 2748 is [True, False, False, False, False, True]
State prediction error at timestep 2748 is tensor(1.0580e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2749. State = [[-0.163831    0.14359592]]. Action = [[ 0.226957   -0.01343605 -0.1288556   0.6439731 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 2749 is [True, False, False, False, False, True]
State prediction error at timestep 2749 is tensor(6.0778e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2750. State = [[-0.16364142  0.14244251]]. Action = [[-0.12780109  0.20253357  0.01893604 -0.11647749]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 2750 is [True, False, False, False, False, True]
State prediction error at timestep 2750 is tensor(2.1979e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2751. State = [[-0.16350529  0.14161566]]. Action = [[-0.15777743  0.05561218  0.13593322  0.30935884]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 2751 is [True, False, False, False, False, True]
State prediction error at timestep 2751 is tensor(5.1114e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2752. State = [[-0.16362369  0.14016336]]. Action = [[-0.12748598  0.16703981 -0.20640557  0.5195458 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 2752 is [True, False, False, False, False, True]
State prediction error at timestep 2752 is tensor(5.9443e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2753. State = [[-0.16357335  0.1386602 ]]. Action = [[-0.14383839 -0.07468405  0.24221194  0.3639133 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 2753 is [True, False, False, False, False, True]
State prediction error at timestep 2753 is tensor(9.8699e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2754. State = [[-0.163631    0.13801944]]. Action = [[-0.13112377 -0.16698194 -0.12482834  0.73841584]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 2754 is [True, False, False, False, False, True]
State prediction error at timestep 2754 is tensor(1.0972e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2755. State = [[-0.1635848   0.13728362]]. Action = [[ 0.17312223 -0.13717508 -0.2343863  -0.6173856 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 2755 is [True, False, False, False, False, True]
State prediction error at timestep 2755 is tensor(1.2588e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2756. State = [[-0.16359176  0.13697192]]. Action = [[0.11828217 0.1441724  0.14266294 0.32817817]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 2756 is [True, False, False, False, False, True]
State prediction error at timestep 2756 is tensor(1.4161e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2757. State = [[-0.16360842  0.13659303]]. Action = [[-0.22904888  0.15655786  0.05477816 -0.88357395]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 2757 is [True, False, False, False, False, True]
State prediction error at timestep 2757 is tensor(6.9467e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2758. State = [[-0.16361539  0.1362808 ]]. Action = [[-0.06371194  0.19603077 -0.02214688 -0.6462399 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 2758 is [True, False, False, False, False, True]
State prediction error at timestep 2758 is tensor(3.8644e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2759. State = [[-0.16352014  0.13535514]]. Action = [[-0.04520734 -0.07839981 -0.21108319 -0.7406578 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 2759 is [True, False, False, False, False, True]
State prediction error at timestep 2759 is tensor(9.2400e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2759 of 1
Current timestep = 2760. State = [[-0.16342819  0.13443573]]. Action = [[ 0.13928229 -0.17302756  0.20576298  0.8559964 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 2760 is [True, False, False, False, False, True]
State prediction error at timestep 2760 is tensor(2.9425e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2761. State = [[-0.16331807  0.13376147]]. Action = [[ 0.15275785 -0.09627154 -0.16255084  0.23146296]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 2761 is [True, False, False, False, False, True]
State prediction error at timestep 2761 is tensor(1.0011e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2762. State = [[-0.16327092  0.1333479 ]]. Action = [[ 0.23007527  0.04357228 -0.14047787 -0.3926375 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 2762 is [True, False, False, False, False, True]
State prediction error at timestep 2762 is tensor(3.5076e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2763. State = [[-0.16312645  0.13190025]]. Action = [[0.08898473 0.01581994 0.2413922  0.6171347 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 2763 is [True, False, False, False, False, True]
State prediction error at timestep 2763 is tensor(1.9653e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2763 of 1
Current timestep = 2764. State = [[-0.16309133  0.13155364]]. Action = [[-0.14188963  0.22988117 -0.14350761  0.24624681]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 2764 is [True, False, False, False, False, True]
State prediction error at timestep 2764 is tensor(4.9071e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2764 of 1
Current timestep = 2765. State = [[-0.16287461  0.13030083]]. Action = [[-0.11625329 -0.12642114 -0.14863336  0.67263675]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 2765 is [True, False, False, False, False, True]
State prediction error at timestep 2765 is tensor(1.0961e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2766. State = [[-0.16284956  0.12916557]]. Action = [[ 0.16763884 -0.02953427 -0.01720382  0.14142084]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 2766 is [True, False, False, False, False, True]
State prediction error at timestep 2766 is tensor(1.0880e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2767. State = [[-0.16267176  0.1261991 ]]. Action = [[ 0.01413396  0.05628258 -0.04575431  0.5416763 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 2767 is [True, False, False, False, False, True]
State prediction error at timestep 2767 is tensor(8.2487e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2768. State = [[-0.16317485  0.12532972]]. Action = [[-0.09725496 -0.11453289 -0.17617977 -0.8330752 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 2768 is [True, False, False, False, False, True]
State prediction error at timestep 2768 is tensor(3.9043e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2769. State = [[-0.16349302  0.12447255]]. Action = [[0.23604888 0.18275124 0.22171777 0.7433256 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 2769 is [True, False, False, False, True, False]
State prediction error at timestep 2769 is tensor(7.7011e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2769 of 1
Current timestep = 2770. State = [[-0.16391535  0.12328515]]. Action = [[-0.11487213 -0.17933178  0.22034082 -0.0016492 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 2770 is [True, False, False, False, True, False]
State prediction error at timestep 2770 is tensor(2.6869e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2771. State = [[-0.1641445  0.1228331]]. Action = [[-0.16389486 -0.16666731 -0.13650417 -0.5543673 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 2771 is [True, False, False, False, True, False]
State prediction error at timestep 2771 is tensor(1.4596e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2772. State = [[-0.16557907  0.12042718]]. Action = [[ 0.01217675 -0.02963683 -0.15547961 -0.10628688]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 2772 is [True, False, False, False, True, False]
State prediction error at timestep 2772 is tensor(5.9919e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2773. State = [[-0.16579014  0.11864487]]. Action = [[0.08202833 0.06292579 0.01773649 0.26911247]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 2773 is [True, False, False, False, True, False]
State prediction error at timestep 2773 is tensor(1.0298e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2773 of 1
Current timestep = 2774. State = [[-0.16599804  0.11855845]]. Action = [[-0.17828342  0.07356337  0.24231696  0.67135334]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 2774 is [True, False, False, False, True, False]
State prediction error at timestep 2774 is tensor(8.7243e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2775. State = [[-0.16603948  0.11858077]]. Action = [[ 0.21191663  0.153462   -0.01657462 -0.7548403 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 2775 is [True, False, False, False, True, False]
State prediction error at timestep 2775 is tensor(1.2129e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2775 of 1
Current timestep = 2776. State = [[-0.16607659  0.11846977]]. Action = [[ 0.2385158   0.01860091  0.23297864 -0.9160273 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 2776 is [True, False, False, False, True, False]
State prediction error at timestep 2776 is tensor(8.1370e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2777. State = [[-0.16621155  0.11842671]]. Action = [[ 0.20583552  0.19137174  0.1639575  -0.9099443 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 2777 is [True, False, False, False, True, False]
State prediction error at timestep 2777 is tensor(1.7272e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2778. State = [[-0.16619855  0.11862453]]. Action = [[ 0.06412318  0.22976357 -0.06303103  0.56171095]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 2778 is [True, False, False, False, True, False]
State prediction error at timestep 2778 is tensor(4.2992e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2779. State = [[-0.16617773  0.11832995]]. Action = [[ 0.1891675 -0.0466252  0.137133  -0.6450976]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 2779 is [True, False, False, False, True, False]
State prediction error at timestep 2779 is tensor(3.2116e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2780. State = [[-0.1662517   0.11843087]]. Action = [[-0.21855757 -0.16129394  0.03673044  0.5467603 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 2780 is [True, False, False, False, True, False]
State prediction error at timestep 2780 is tensor(4.3641e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2781. State = [[-0.16647536  0.1184501 ]]. Action = [[ 0.02051359  0.02604467 -0.19695188  0.7584616 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 2781 is [True, False, False, False, True, False]
State prediction error at timestep 2781 is tensor(2.0402e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2781 of 1
Current timestep = 2782. State = [[-0.1665469   0.11853959]]. Action = [[ 0.02442494 -0.20380217 -0.23868214 -0.27905595]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 2782 is [True, False, False, False, True, False]
State prediction error at timestep 2782 is tensor(7.8014e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2783. State = [[-0.1666022   0.11865107]]. Action = [[-0.12252957  0.17003185 -0.05733779 -0.646101  ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 2783 is [True, False, False, False, True, False]
State prediction error at timestep 2783 is tensor(3.5291e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2784. State = [[-0.16665725  0.11876201]]. Action = [[-0.16498621 -0.04931198 -0.19401117  0.33862066]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 2784 is [True, False, False, False, True, False]
State prediction error at timestep 2784 is tensor(1.1199e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2784 of 1
Current timestep = 2785. State = [[-0.16668506  0.11881801]]. Action = [[ 0.20685288 -0.23629403 -0.04928395 -0.65876716]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 2785 is [True, False, False, False, True, False]
State prediction error at timestep 2785 is tensor(7.1126e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2786. State = [[-0.16692656  0.11910979]]. Action = [[0.04778767 0.06417501 0.00612536 0.46208382]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 2786 is [True, False, False, False, True, False]
State prediction error at timestep 2786 is tensor(4.1190e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2786 of 1
Current timestep = 2787. State = [[-0.16705751  0.11941605]]. Action = [[ 0.17661631  0.15839529  0.18124071 -0.74608785]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 2787 is [True, False, False, False, True, False]
State prediction error at timestep 2787 is tensor(3.2503e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2788. State = [[-0.16747448  0.12053253]]. Action = [[-0.00933003  0.01706526 -0.01735479 -0.36257243]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 2788 is [True, False, False, False, True, False]
State prediction error at timestep 2788 is tensor(2.1259e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2788 of 1
Current timestep = 2789. State = [[-0.16771819  0.1211217 ]]. Action = [[0.14987203 0.13285667 0.22764811 0.00166118]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 2789 is [True, False, False, False, True, False]
State prediction error at timestep 2789 is tensor(1.7234e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2790. State = [[-0.16795474  0.12137986]]. Action = [[ 0.17017296  0.17928976  0.08559731 -0.40883166]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 2790 is [True, False, False, False, True, False]
State prediction error at timestep 2790 is tensor(5.4974e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2791. State = [[-0.16823138  0.1222486 ]]. Action = [[-0.0298422  -0.0105315   0.13492465  0.8692968 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 2791 is [True, False, False, False, True, False]
State prediction error at timestep 2791 is tensor(1.0528e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2791 of 1
Current timestep = 2792. State = [[-0.168341    0.12239114]]. Action = [[-0.16971247 -0.01543245  0.08044797  0.2367121 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 2792 is [True, False, False, False, True, False]
State prediction error at timestep 2792 is tensor(7.4436e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2793. State = [[-0.16871956  0.12315504]]. Action = [[ 0.04868343  0.12453568  0.23971123 -0.05900651]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 2793 is [True, False, False, False, True, False]
State prediction error at timestep 2793 is tensor(9.5556e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2793 of 1
Current timestep = 2794. State = [[-0.16960205  0.12643379]]. Action = [[ 0.1147446  -0.03838724  0.21467277 -0.7611314 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 2794 is [True, False, False, False, False, True]
State prediction error at timestep 2794 is tensor(5.9299e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2794 of 1
Current timestep = 2795. State = [[-0.16909151  0.12672515]]. Action = [[-0.17282893  0.1555087   0.08338231 -0.17189085]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 2795 is [True, False, False, False, False, True]
State prediction error at timestep 2795 is tensor(6.1183e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2796. State = [[-0.16861103  0.12691419]]. Action = [[-0.1839847   0.19881299  0.2306853  -0.25699008]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 2796 is [True, False, False, False, False, True]
State prediction error at timestep 2796 is tensor(1.6271e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2797. State = [[-0.16825126  0.12703122]]. Action = [[ 0.1832     -0.19142471 -0.12891228  0.5047282 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 2797 is [True, False, False, False, False, True]
State prediction error at timestep 2797 is tensor(2.6047e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2798. State = [[-0.16800937  0.12731104]]. Action = [[ 0.20936963 -0.14105609 -0.0966824  -0.86413443]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 2798 is [True, False, False, False, False, True]
State prediction error at timestep 2798 is tensor(5.7025e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2798 of 1
Current timestep = 2799. State = [[-0.16757354  0.1275951 ]]. Action = [[-0.20998102 -0.22343963 -0.08951622 -0.87460905]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 2799 is [True, False, False, False, False, True]
State prediction error at timestep 2799 is tensor(6.4504e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2800. State = [[-0.16739835  0.12751976]]. Action = [[ 0.13125566  0.1401394  -0.19532919 -0.94761544]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 2800 is [True, False, False, False, False, True]
State prediction error at timestep 2800 is tensor(1.0170e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2801. State = [[-0.16735168  0.12742324]]. Action = [[-0.13935153  0.18216038 -0.10725506  0.55736494]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 2801 is [True, False, False, False, False, True]
State prediction error at timestep 2801 is tensor(3.9848e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2801 of 1
Current timestep = 2802. State = [[-0.16701509  0.12768844]]. Action = [[-0.18308076  0.20135623  0.24220961  0.58646595]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 2802 is [True, False, False, False, False, True]
State prediction error at timestep 2802 is tensor(4.5804e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2803. State = [[-0.16683143  0.12779693]]. Action = [[ 0.00414947  0.11669505 -0.18591957 -0.90290266]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 2803 is [True, False, False, False, False, True]
State prediction error at timestep 2803 is tensor(8.8255e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2804. State = [[-0.16702683  0.12876555]]. Action = [[-0.20730428 -0.13658604 -0.07708883  0.30528772]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 2804 is [True, False, False, False, False, True]
State prediction error at timestep 2804 is tensor(3.3818e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2805. State = [[-0.16704386  0.12979232]]. Action = [[ 0.22441521 -0.14861813  0.08016342  0.16307628]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 2805 is [True, False, False, False, False, True]
State prediction error at timestep 2805 is tensor(3.1324e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2806. State = [[-0.16727105  0.1303756 ]]. Action = [[ 0.14576104 -0.1970395   0.19432998  0.2738278 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 2806 is [True, False, False, False, False, True]
State prediction error at timestep 2806 is tensor(1.1658e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2807. State = [[-0.16667743  0.1321451 ]]. Action = [[0.07695666 0.00851911 0.12354195 0.8314142 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 2807 is [True, False, False, False, False, True]
State prediction error at timestep 2807 is tensor(1.7505e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2807 of 1
Current timestep = 2808. State = [[-0.16583161  0.13262585]]. Action = [[ 0.20699728  0.22620767 -0.01016672 -0.8990844 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 2808 is [True, False, False, False, False, True]
State prediction error at timestep 2808 is tensor(2.1999e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2809. State = [[-0.16314477  0.1338865 ]]. Action = [[ 0.06417024  0.06346422 -0.22082448  0.78324485]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 2809 is [True, False, False, False, False, True]
State prediction error at timestep 2809 is tensor(1.4487e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2809 of 1
Current timestep = 2810. State = [[-0.15949115  0.13655417]]. Action = [[ 0.13251281 -0.02274209 -0.19077858  0.37724066]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 2810 is [True, False, False, False, False, True]
State prediction error at timestep 2810 is tensor(3.9751e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2810 of 1
Current timestep = 2811. State = [[-0.1582      0.13700883]]. Action = [[ 0.1957174  -0.21277146  0.09519601 -0.47584903]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 2811 is [True, False, False, False, False, True]
State prediction error at timestep 2811 is tensor(6.1905e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2812. State = [[-0.1523623   0.13847494]]. Action = [[-0.11845122 -0.01837872 -0.1572862  -0.70349634]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 2812 is [True, False, False, False, False, True]
State prediction error at timestep 2812 is tensor(9.1677e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2812 of 1
Current timestep = 2813. State = [[-0.15185972  0.13869606]]. Action = [[-0.14826484 -0.15328942  0.06960753 -0.8620121 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 2813 is [True, False, False, False, False, True]
State prediction error at timestep 2813 is tensor(1.1646e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2814. State = [[-0.15159878  0.13856868]]. Action = [[0.17495745 0.20980838 0.23294196 0.43587875]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 2814 is [True, False, False, False, False, True]
State prediction error at timestep 2814 is tensor(7.9966e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2815. State = [[-0.15140219  0.13927762]]. Action = [[ 0.09156519  0.06881252 -0.22642529 -0.9038051 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 2815 is [True, False, False, False, False, True]
State prediction error at timestep 2815 is tensor(1.8696e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2815 of 1
Current timestep = 2816. State = [[-0.15118946  0.13951686]]. Action = [[-0.21096508 -0.101422    0.21118578  0.41942167]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 2816 is [True, False, False, False, False, True]
State prediction error at timestep 2816 is tensor(7.0038e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2817. State = [[-0.15006642  0.14025441]]. Action = [[ 0.04560238 -0.09523162  0.01618236  0.5962142 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 2817 is [True, False, False, False, False, True]
State prediction error at timestep 2817 is tensor(6.2826e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2817 of 1
Current timestep = 2818. State = [[-0.14778626  0.13951147]]. Action = [[-0.0672746  -0.04930234 -0.15798804 -0.06673455]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 2818 is [True, False, False, False, False, True]
State prediction error at timestep 2818 is tensor(5.2919e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2818 of 1
Current timestep = 2819. State = [[-0.14722998  0.1387779 ]]. Action = [[-0.1969163  -0.04989929 -0.06603172  0.72744966]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 2819 is [True, False, False, False, False, True]
State prediction error at timestep 2819 is tensor(4.5341e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2820. State = [[-0.14687923  0.13837808]]. Action = [[-0.05450349 -0.22443424 -0.20810531  0.53239465]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 2820 is [True, False, False, False, False, True]
State prediction error at timestep 2820 is tensor(1.3174e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2821. State = [[-0.14672865  0.13819212]]. Action = [[ 0.21492442 -0.00747773 -0.07135257  0.67827463]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 2821 is [True, False, False, False, False, True]
State prediction error at timestep 2821 is tensor(1.3078e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2822. State = [[-0.14680448  0.13817368]]. Action = [[ 0.15424752 -0.2051364   0.1432488  -0.32629   ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 2822 is [True, False, False, False, False, True]
State prediction error at timestep 2822 is tensor(2.2799e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2823. State = [[-0.14689061  0.13775517]]. Action = [[ 0.20923424 -0.10795492  0.12001321  0.54186153]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 2823 is [True, False, False, False, False, True]
State prediction error at timestep 2823 is tensor(5.2020e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2823 of 1
Current timestep = 2824. State = [[-0.14653873  0.13741952]]. Action = [[-0.0674973   0.15533039 -0.15701236  0.74560356]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 2824 is [True, False, False, False, False, True]
State prediction error at timestep 2824 is tensor(8.0078e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2825. State = [[-0.14668113  0.13725899]]. Action = [[-0.2305061  -0.10339999  0.16817445  0.4599465 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 2825 is [True, False, False, False, False, True]
State prediction error at timestep 2825 is tensor(2.0589e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2826. State = [[-0.14669977  0.13719611]]. Action = [[-0.12470984  0.2454037   0.2201441  -0.430138  ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 2826 is [True, False, False, False, False, True]
State prediction error at timestep 2826 is tensor(1.9588e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2827. State = [[-0.1466959   0.13690875]]. Action = [[-0.09271818  0.0266079   0.13672188  0.3763168 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 2827 is [True, False, False, False, False, True]
State prediction error at timestep 2827 is tensor(3.0611e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2827 of 1
Current timestep = 2828. State = [[-0.14685833  0.13704678]]. Action = [[-0.00670703  0.2032634   0.04135805  0.92537475]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 2828 is [True, False, False, False, False, True]
State prediction error at timestep 2828 is tensor(7.4642e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2829. State = [[-0.14704037  0.13734706]]. Action = [[-0.0066617  -0.02934536 -0.04022646  0.8141184 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 2829 is [True, False, False, False, False, True]
State prediction error at timestep 2829 is tensor(1.8940e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2830. State = [[-0.14702912  0.13727845]]. Action = [[ 0.23003504 -0.16497086  0.1098516  -0.48698986]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 2830 is [True, False, False, False, False, True]
State prediction error at timestep 2830 is tensor(2.6304e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2831. State = [[-0.14698426  0.13700448]]. Action = [[ 0.0287571  -0.08086234  0.2206558   0.46551394]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 2831 is [True, False, False, False, False, True]
State prediction error at timestep 2831 is tensor(3.2402e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2831 of 1
Current timestep = 2832. State = [[-0.14667328  0.1358312 ]]. Action = [[-0.10558549 -0.156942   -0.01739976  0.7211579 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 2832 is [True, False, False, False, False, True]
State prediction error at timestep 2832 is tensor(1.7575e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2833. State = [[-0.1465109   0.13464881]]. Action = [[ 0.12211865  0.0986712  -0.15633433  0.34886408]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 2833 is [True, False, False, False, False, True]
State prediction error at timestep 2833 is tensor(8.3974e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2833 of 1
Current timestep = 2834. State = [[-0.14616191  0.13488546]]. Action = [[-0.22252345  0.07603976  0.22075015 -0.7543437 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 2834 is [True, False, False, False, False, True]
State prediction error at timestep 2834 is tensor(1.3235e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2835. State = [[-0.14496471  0.13508783]]. Action = [[ 0.0932647  -0.06402552 -0.04755539  0.4438734 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 2835 is [True, False, False, False, False, True]
State prediction error at timestep 2835 is tensor(1.1664e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2835 of 1
Current timestep = 2836. State = [[-0.14401107  0.13473903]]. Action = [[ 0.13297093  0.1720987  -0.22997335 -0.4927764 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 2836 is [True, False, False, False, False, True]
State prediction error at timestep 2836 is tensor(1.1807e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2837. State = [[-0.14338441  0.13443103]]. Action = [[ 0.00333154 -0.17470516  0.09778589 -0.6774451 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 2837 is [True, False, False, False, False, True]
State prediction error at timestep 2837 is tensor(1.7941e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2837 of 1
Current timestep = 2838. State = [[-0.14096071  0.13446909]]. Action = [[-0.03670858  0.04101357  0.20535582 -0.39273888]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 2838 is [True, False, False, False, False, True]
State prediction error at timestep 2838 is tensor(2.9895e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2839. State = [[-0.14046611  0.13468508]]. Action = [[-0.1905133   0.01220369  0.04036576  0.5585232 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 2839 is [True, False, False, False, False, True]
State prediction error at timestep 2839 is tensor(3.4622e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2840. State = [[-0.14043844  0.1347559 ]]. Action = [[-0.13797602 -0.07096417 -0.08599499  0.16992939]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 2840 is [True, False, False, False, False, True]
State prediction error at timestep 2840 is tensor(5.9201e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2841. State = [[-0.14030074  0.13454023]]. Action = [[ 0.10177752 -0.11447278  0.13518667 -0.7353343 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 2841 is [True, False, False, False, False, True]
State prediction error at timestep 2841 is tensor(2.2327e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2842. State = [[-0.1394553   0.13366888]]. Action = [[0.09280151 0.16099808 0.08823815 0.64211917]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 2842 is [True, False, False, False, False, True]
State prediction error at timestep 2842 is tensor(8.3977e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2843. State = [[-0.13890667  0.13283913]]. Action = [[ 0.15364009 -0.08534911  0.17488316  0.02610087]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 2843 is [True, False, False, False, False, True]
State prediction error at timestep 2843 is tensor(3.4394e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2844. State = [[-0.13823298  0.13218382]]. Action = [[-0.20988394  0.04039368 -0.08478865 -0.06734616]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 2844 is [True, False, False, False, False, True]
State prediction error at timestep 2844 is tensor(4.2424e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2845. State = [[-0.13789064  0.13171883]]. Action = [[ 0.22134507 -0.06328163  0.24220738  0.0202769 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 2845 is [True, False, False, False, False, True]
State prediction error at timestep 2845 is tensor(6.5768e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2846. State = [[-0.13709342  0.13058464]]. Action = [[-0.07401478  0.03970167 -0.11362842 -0.4476056 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 2846 is [True, False, False, False, False, True]
State prediction error at timestep 2846 is tensor(1.2547e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2846 of 1
Current timestep = 2847. State = [[-0.13719393  0.13071895]]. Action = [[-0.14258282 -0.06888968 -0.11221217 -0.7773828 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 2847 is [True, False, False, False, False, True]
State prediction error at timestep 2847 is tensor(9.6131e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2848. State = [[-0.13736793  0.13085356]]. Action = [[ 0.22254539  0.03940707 -0.07216275 -0.85284525]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 2848 is [True, False, False, False, False, True]
State prediction error at timestep 2848 is tensor(1.5159e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2848 of 1
Current timestep = 2849. State = [[-0.13741383  0.1308382 ]]. Action = [[-0.10914096 -0.24705672 -0.00775357 -0.37594867]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 2849 is [True, False, False, False, False, True]
State prediction error at timestep 2849 is tensor(6.6348e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2850. State = [[-0.13736746  0.13052419]]. Action = [[-0.12110409 -0.11280175 -0.23072897  0.17274809]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 2850 is [True, False, False, False, False, True]
State prediction error at timestep 2850 is tensor(8.0975e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2851. State = [[-0.13727605  0.12980175]]. Action = [[0.17006806 0.21724749 0.17234743 0.7970967 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 2851 is [True, False, False, False, False, True]
State prediction error at timestep 2851 is tensor(1.0590e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2852. State = [[-0.13748199  0.12758225]]. Action = [[-0.03679997 -0.04943445 -0.21782687 -0.7213052 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 2852 is [True, False, False, False, False, True]
State prediction error at timestep 2852 is tensor(4.9468e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2853. State = [[-0.13741213  0.12653936]]. Action = [[-0.19881147 -0.08610329 -0.05796069 -0.17388856]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 2853 is [True, False, False, False, False, True]
State prediction error at timestep 2853 is tensor(3.0448e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2854. State = [[-0.1373675   0.12583478]]. Action = [[-0.00163876 -0.21871395 -0.08308485 -0.78209525]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 2854 is [True, False, False, False, False, True]
State prediction error at timestep 2854 is tensor(2.0202e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2855. State = [[-0.13729638  0.12499179]]. Action = [[ 0.18475163 -0.21646446  0.08384779  0.9343817 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 2855 is [True, False, False, False, True, False]
State prediction error at timestep 2855 is tensor(4.8423e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2856. State = [[-0.13734645  0.12440939]]. Action = [[ 0.18925413 -0.02710572  0.08414429  0.528947  ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 2856 is [True, False, False, False, True, False]
State prediction error at timestep 2856 is tensor(5.7163e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2856 of 1
Current timestep = 2857. State = [[-0.13744234  0.12282816]]. Action = [[ 0.10271972  0.1220597  -0.07799205  0.58813965]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 2857 is [True, False, False, False, True, False]
State prediction error at timestep 2857 is tensor(6.1839e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2858. State = [[-0.13762473  0.12321623]]. Action = [[0.23435849 0.1979534  0.09486163 0.21518147]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 2858 is [True, False, False, False, True, False]
State prediction error at timestep 2858 is tensor(3.1688e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2859. State = [[-0.13775513  0.12378196]]. Action = [[-0.19677778 -0.0397388   0.22493255 -0.80643976]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 2859 is [True, False, False, False, True, False]
State prediction error at timestep 2859 is tensor(1.0706e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2860. State = [[-0.13787085  0.12396353]]. Action = [[-0.19233435 -0.18787299  0.0719085  -0.8360527 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 2860 is [True, False, False, False, True, False]
State prediction error at timestep 2860 is tensor(3.8324e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2861. State = [[-0.13829777  0.12474202]]. Action = [[-0.12215078  0.02956617 -0.21399105  0.17297041]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 2861 is [True, False, False, False, True, False]
State prediction error at timestep 2861 is tensor(1.2418e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2861 of 1
Current timestep = 2862. State = [[-0.13870078  0.12535498]]. Action = [[-0.20000607  0.16126233  0.08110407  0.2781248 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 2862 is [True, False, False, False, False, True]
State prediction error at timestep 2862 is tensor(9.4946e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2863. State = [[-0.1389967   0.12610579]]. Action = [[-0.13952355  0.00743169 -0.17287327  0.20792699]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 2863 is [True, False, False, False, False, True]
State prediction error at timestep 2863 is tensor(1.0565e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2864. State = [[-0.13997284  0.12788107]]. Action = [[-0.10771754  0.1124447  -0.06537414 -0.90475804]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 2864 is [True, False, False, False, False, True]
State prediction error at timestep 2864 is tensor(2.6291e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2864 of 1
Current timestep = 2865. State = [[-0.14094836  0.12962219]]. Action = [[ 0.18590915 -0.04221439 -0.1895994  -0.43594003]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 2865 is [True, False, False, False, False, True]
State prediction error at timestep 2865 is tensor(6.5791e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2866. State = [[-0.14316447  0.13351549]]. Action = [[-0.11313963  0.00838017 -0.10383524  0.05860138]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 2866 is [True, False, False, False, False, True]
State prediction error at timestep 2866 is tensor(3.9578e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2866 of 1
Current timestep = 2867. State = [[-0.1439732  0.134976 ]]. Action = [[ 0.10250983 -0.14334376 -0.24200378  0.96750355]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 2867 is [True, False, False, False, False, True]
State prediction error at timestep 2867 is tensor(1.0160e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2868. State = [[-0.14613369  0.13793713]]. Action = [[-0.11134976  0.05774301  0.23193675 -0.78359634]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 2868 is [True, False, False, False, False, True]
State prediction error at timestep 2868 is tensor(8.2454e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2868 of 1
Current timestep = 2869. State = [[-0.14739962  0.13914348]]. Action = [[-0.12911598  0.22479653 -0.09406289 -0.540728  ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 2869 is [True, False, False, False, False, True]
State prediction error at timestep 2869 is tensor(4.8655e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2870. State = [[-0.14816928  0.14018947]]. Action = [[ 0.16169721 -0.16379753 -0.1250558   0.28420162]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 2870 is [True, False, False, False, False, True]
State prediction error at timestep 2870 is tensor(1.3994e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2870 of 1
Current timestep = 2871. State = [[-0.15149233  0.14293994]]. Action = [[-0.12852065 -0.00990808 -0.08480711  0.41672277]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 2871 is [True, False, False, False, False, True]
State prediction error at timestep 2871 is tensor(2.7644e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2872. State = [[-0.1528152   0.14364012]]. Action = [[ 0.09621176 -0.16837107  0.19595784 -0.77163976]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 2872 is [True, False, False, False, False, True]
State prediction error at timestep 2872 is tensor(1.5530e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2873. State = [[-0.15390217  0.14407176]]. Action = [[0.02259243 0.23424286 0.0558953  0.07545757]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 2873 is [True, False, False, False, False, True]
State prediction error at timestep 2873 is tensor(9.4219e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2874. State = [[-0.15766557  0.14541915]]. Action = [[-0.07674372 -0.04976575 -0.0568032  -0.7417063 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 2874 is [True, False, False, False, False, True]
State prediction error at timestep 2874 is tensor(1.6005e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2875. State = [[-0.16097207  0.14465798]]. Action = [[-0.06011137 -0.08019601  0.02399698 -0.52703804]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 2875 is [True, False, False, False, False, True]
State prediction error at timestep 2875 is tensor(9.5816e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2876. State = [[-0.16217439  0.14351499]]. Action = [[ 0.07475084  0.23913437 -0.23420882 -0.5187615 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 2876 is [True, False, False, False, False, True]
State prediction error at timestep 2876 is tensor(6.2258e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2877. State = [[-0.16294087  0.1426955 ]]. Action = [[ 0.00176662  0.18868327  0.20707262 -0.7190763 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 2877 is [True, False, False, False, False, True]
State prediction error at timestep 2877 is tensor(1.0079e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2878. State = [[-0.16352439  0.14239697]]. Action = [[-0.23156187  0.08996111 -0.22698367  0.82359076]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 2878 is [True, False, False, False, False, True]
State prediction error at timestep 2878 is tensor(6.6314e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2879. State = [[-0.16451262  0.14193322]]. Action = [[ 0.0270676  -0.14291956 -0.03670746 -0.32482976]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 2879 is [True, False, False, False, False, True]
State prediction error at timestep 2879 is tensor(4.8019e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2880. State = [[-0.1659071   0.14097692]]. Action = [[ 0.06785116  0.23281541 -0.2268104   0.10679686]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 2880 is [True, False, False, False, False, True]
State prediction error at timestep 2880 is tensor(4.3287e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2880 of -1
Current timestep = 2881. State = [[-0.1664413   0.14092067]]. Action = [[-0.1716636   0.19179332  0.00707445 -0.13896382]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 2881 is [True, False, False, False, False, True]
State prediction error at timestep 2881 is tensor(4.3621e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2882. State = [[-0.16861442  0.13950223]]. Action = [[ 0.05693436 -0.08799782 -0.20848116  0.17267668]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 2882 is [True, False, False, False, False, True]
State prediction error at timestep 2882 is tensor(6.4096e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2882 of -1
Current timestep = 2883. State = [[-0.16881506  0.13809444]]. Action = [[-0.1838309  -0.09901923 -0.2146669  -0.11388242]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 2883 is [True, False, False, False, False, True]
State prediction error at timestep 2883 is tensor(7.0522e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2884. State = [[-0.16907805  0.13499986]]. Action = [[ 0.07067281 -0.11149943  0.05358428 -0.8730386 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 2884 is [True, False, False, False, False, True]
State prediction error at timestep 2884 is tensor(4.5050e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2884 of -1
Current timestep = 2885. State = [[-0.16796109  0.12945934]]. Action = [[-0.0277562  -0.10283411 -0.08504111 -0.932176  ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 2885 is [True, False, False, False, False, True]
State prediction error at timestep 2885 is tensor(1.7524e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2885 of -1
Current timestep = 2886. State = [[-0.16740113  0.12732479]]. Action = [[ 0.15124995  0.02826789 -0.09572983 -0.75318015]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 2886 is [True, False, False, False, False, True]
State prediction error at timestep 2886 is tensor(3.0235e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2887. State = [[-0.16701518  0.1229682 ]]. Action = [[ 0.05956447 -0.03417902 -0.01345393 -0.7449273 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 2887 is [True, False, False, False, True, False]
State prediction error at timestep 2887 is tensor(4.3378e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2887 of -1
Current timestep = 2888. State = [[-0.16639902  0.1188437 ]]. Action = [[-0.07508287  0.0729917   0.01123935  0.14122951]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 2888 is [True, False, False, False, True, False]
State prediction error at timestep 2888 is tensor(6.2437e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2888 of -1
Current timestep = 2889. State = [[-0.16641721  0.11875059]]. Action = [[-0.15541701 -0.06515038  0.09028426 -0.17119741]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 2889 is [True, False, False, False, True, False]
State prediction error at timestep 2889 is tensor(4.5098e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2890. State = [[-0.16667753  0.11887368]]. Action = [[-0.21804549 -0.01196267  0.11888906 -0.78702474]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 2890 is [True, False, False, False, True, False]
State prediction error at timestep 2890 is tensor(5.5918e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2891. State = [[-0.16693266  0.11879704]]. Action = [[-0.20883588  0.20239946 -0.16531108 -0.33854783]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 2891 is [True, False, False, False, True, False]
State prediction error at timestep 2891 is tensor(2.6438e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2891 of -1
Current timestep = 2892. State = [[-0.16665466  0.11854887]]. Action = [[-0.09855372 -0.20781492 -0.08185278 -0.21957946]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 2892 is [True, False, False, False, True, False]
State prediction error at timestep 2892 is tensor(1.9700e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2893. State = [[-0.16688317  0.11855198]]. Action = [[ 0.08945727 -0.01517673 -0.22568332  0.5798681 ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 2893 is [True, False, False, False, True, False]
State prediction error at timestep 2893 is tensor(2.9645e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2894. State = [[-0.16667186  0.118432  ]]. Action = [[ 0.2024427  -0.03024733 -0.21091859 -0.16799432]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 2894 is [True, False, False, False, True, False]
State prediction error at timestep 2894 is tensor(2.9949e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2895. State = [[-0.16603081  0.11817839]]. Action = [[ 0.13301134 -0.02782388 -0.08010207  0.2892226 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 2895 is [True, False, False, False, True, False]
State prediction error at timestep 2895 is tensor(5.3132e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2895 of -1
Current timestep = 2896. State = [[-0.16532676  0.11766344]]. Action = [[-0.01269332 -0.20627505 -0.16142961 -0.33420134]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 2896 is [True, False, False, False, True, False]
State prediction error at timestep 2896 is tensor(3.8544e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2897. State = [[-0.16501908  0.11744273]]. Action = [[ 0.18393892 -0.03556767  0.18739969  0.4733963 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 2897 is [True, False, False, False, True, False]
State prediction error at timestep 2897 is tensor(5.0536e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2898. State = [[-0.16469473  0.11722504]]. Action = [[ 0.24052462  0.03853074 -0.0447792   0.25093424]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 2898 is [True, False, False, False, True, False]
State prediction error at timestep 2898 is tensor(1.1076e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2899. State = [[-0.16451731  0.11709558]]. Action = [[ 0.23727578 -0.23161314  0.08182541 -0.73948294]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 2899 is [True, False, False, False, True, False]
State prediction error at timestep 2899 is tensor(3.8367e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2900. State = [[-0.16428857  0.11687208]]. Action = [[ 0.1563484   0.08782053 -0.24008314  0.6376574 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 2900 is [True, False, False, False, True, False]
State prediction error at timestep 2900 is tensor(4.6753e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2901. State = [[-0.1639402   0.11656204]]. Action = [[-0.21760854 -0.18278511  0.05767187 -0.5563953 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 2901 is [True, False, False, False, True, False]
State prediction error at timestep 2901 is tensor(9.5568e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2902. State = [[-0.16351014  0.11602152]]. Action = [[-0.02098241  0.07197067 -0.14371389  0.4686041 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 2902 is [True, False, False, False, True, False]
State prediction error at timestep 2902 is tensor(5.2056e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2902 of -1
Current timestep = 2903. State = [[-0.16387239  0.11663787]]. Action = [[-0.02628459  0.09600788  0.0706996   0.62826324]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 2903 is [True, False, False, False, True, False]
State prediction error at timestep 2903 is tensor(6.5214e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2903 of -1
Current timestep = 2904. State = [[-0.16442698  0.1179464 ]]. Action = [[-0.14146216  0.11477137  0.14184022  0.4879266 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 2904 is [True, False, False, False, True, False]
State prediction error at timestep 2904 is tensor(1.2920e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2905. State = [[-0.16493522  0.11960394]]. Action = [[-0.08770519 -0.05914482 -0.1379545  -0.8617197 ]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 2905 is [True, False, False, False, True, False]
State prediction error at timestep 2905 is tensor(5.1843e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2905 of -1
Current timestep = 2906. State = [[-0.16524367  0.12024328]]. Action = [[0.06491381 0.11791867 0.21627623 0.8549428 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 2906 is [True, False, False, False, True, False]
State prediction error at timestep 2906 is tensor(1.9795e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2906 of -1
Current timestep = 2907. State = [[-0.16558523  0.12119064]]. Action = [[-0.12136692 -0.202073    0.16016486  0.5208056 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 2907 is [True, False, False, False, True, False]
State prediction error at timestep 2907 is tensor(3.0412e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2908. State = [[-0.1662715   0.12310883]]. Action = [[ 0.03014362 -0.04100953 -0.03464113  0.5895879 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 2908 is [True, False, False, False, True, False]
State prediction error at timestep 2908 is tensor(1.9156e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2909. State = [[-0.16646072  0.12338213]]. Action = [[ 0.21587718  0.1752724   0.00604939 -0.50900555]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 2909 is [True, False, False, False, True, False]
State prediction error at timestep 2909 is tensor(2.2902e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2910. State = [[-0.16637969  0.12323667]]. Action = [[-0.08718324 -0.1364204  -0.23372565  0.49711585]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 2910 is [True, False, False, False, True, False]
State prediction error at timestep 2910 is tensor(1.9843e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2911. State = [[-0.16629314  0.12319517]]. Action = [[-0.16815948 -0.12429208  0.03200686 -0.1244638 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 2911 is [True, False, False, False, True, False]
State prediction error at timestep 2911 is tensor(3.0239e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2912. State = [[-0.16642693  0.12356405]]. Action = [[-0.01813537  0.00861499 -0.1965696   0.05547321]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 2912 is [True, False, False, False, True, False]
State prediction error at timestep 2912 is tensor(1.1823e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2912 of -1
Current timestep = 2913. State = [[-0.16651322  0.12360498]]. Action = [[ 0.17787832 -0.22633472  0.23333889 -0.32398373]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 2913 is [True, False, False, False, True, False]
State prediction error at timestep 2913 is tensor(7.1253e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2913 of -1
Current timestep = 2914. State = [[-0.16665135  0.12405857]]. Action = [[-0.08689776  0.0923827  -0.11888249 -0.64838487]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 2914 is [True, False, False, False, True, False]
State prediction error at timestep 2914 is tensor(2.7863e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2915. State = [[-0.16714731  0.12525189]]. Action = [[ 0.21577317 -0.23846933 -0.1953245  -0.5160153 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 2915 is [True, False, False, False, False, True]
State prediction error at timestep 2915 is tensor(7.9532e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2916. State = [[-0.1682027  0.1274658]]. Action = [[-0.11928615 -0.02135769  0.22522894  0.6551348 ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 2916 is [True, False, False, False, False, True]
State prediction error at timestep 2916 is tensor(5.3399e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2917. State = [[-0.16969304  0.12989996]]. Action = [[-0.01181982  0.04707918  0.20512703 -0.6720032 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 2917 is [True, False, False, False, False, True]
State prediction error at timestep 2917 is tensor(6.5302e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2918. State = [[-0.17028686  0.13073187]]. Action = [[ 0.13838089 -0.21597894 -0.04761127  0.30322683]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 2918 is [True, False, False, False, False, True]
State prediction error at timestep 2918 is tensor(3.4503e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2919. State = [[-0.1705991   0.13155626]]. Action = [[-0.08249396  0.13394219 -0.24017528  0.4448223 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 2919 is [True, False, False, False, False, True]
State prediction error at timestep 2919 is tensor(1.0579e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2919 of -1
Current timestep = 2920. State = [[-0.17092876  0.13223863]]. Action = [[-0.15141763  0.06792012 -0.02152187 -0.85967636]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 2920 is [True, False, False, False, False, True]
State prediction error at timestep 2920 is tensor(1.8146e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2921. State = [[-0.17125349  0.13262932]]. Action = [[-0.08393019  0.21226007  0.16648853  0.13684869]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 2921 is [True, False, False, False, False, True]
State prediction error at timestep 2921 is tensor(8.4999e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2922. State = [[-0.17209084  0.13404956]]. Action = [[-0.02046657 -0.10543323 -0.04922496 -0.55610454]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 2922 is [True, False, False, False, False, True]
State prediction error at timestep 2922 is tensor(3.9500e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2922 of -1
Current timestep = 2923. State = [[-0.17227118  0.1334108 ]]. Action = [[-0.14857343  0.12129128 -0.22460543  0.53286994]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 2923 is [True, False, False, False, False, True]
State prediction error at timestep 2923 is tensor(2.0643e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2924. State = [[-0.17245685  0.13277029]]. Action = [[ 0.22215289 -0.041813   -0.0494937  -0.9560166 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 2924 is [True, False, False, False, False, True]
State prediction error at timestep 2924 is tensor(3.3826e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2925. State = [[-0.17239252  0.13254718]]. Action = [[ 0.13786274 -0.09979647 -0.21812207  0.502264  ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 2925 is [True, False, False, False, False, True]
State prediction error at timestep 2925 is tensor(6.3763e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2926. State = [[-0.17253907  0.13258173]]. Action = [[ 0.15561217  0.17109525 -0.24326593  0.68894076]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 2926 is [True, False, False, False, False, True]
State prediction error at timestep 2926 is tensor(1.2028e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2926 of -1
Current timestep = 2927. State = [[-0.17263323  0.13197523]]. Action = [[-0.12861636  0.22181267  0.08811811 -0.75055104]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 2927 is [True, False, False, False, False, True]
State prediction error at timestep 2927 is tensor(7.1620e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2928. State = [[-0.17275299  0.13191935]]. Action = [[-0.06400251  0.2258983  -0.21704839  0.6880598 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 2928 is [True, False, False, False, False, True]
State prediction error at timestep 2928 is tensor(6.9290e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2929. State = [[-0.17272563  0.13189851]]. Action = [[-0.00541142  0.14686447 -0.22524755  0.21823263]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 2929 is [True, False, False, False, False, True]
State prediction error at timestep 2929 is tensor(4.3583e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2930. State = [[-0.17278923  0.13179244]]. Action = [[-0.20055522 -0.05757038 -0.16831037 -0.03733724]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 2930 is [True, False, False, False, False, True]
State prediction error at timestep 2930 is tensor(5.9188e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2931. State = [[-0.17300472  0.13155028]]. Action = [[-0.11420618 -0.05983637  0.10981995 -0.6297053 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 2931 is [True, False, False, False, False, True]
State prediction error at timestep 2931 is tensor(1.7239e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2931 of -1
Current timestep = 2932. State = [[-0.17366669  0.13083091]]. Action = [[-1.6362062e-01 -6.6846609e-05 -2.3017439e-01  2.8712535e-01]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 2932 is [True, False, False, False, False, True]
State prediction error at timestep 2932 is tensor(2.6639e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2933. State = [[-0.17398174  0.13032874]]. Action = [[ 0.06685373 -0.1966902  -0.08314377  0.92935586]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 2933 is [True, False, False, False, False, True]
State prediction error at timestep 2933 is tensor(3.0258e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2934. State = [[-0.17414917  0.13015975]]. Action = [[ 0.14560035 -0.02053678  0.2472733  -0.62154937]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 2934 is [True, False, False, False, False, True]
State prediction error at timestep 2934 is tensor(1.1200e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2935. State = [[-0.17436719  0.13004237]]. Action = [[ 0.24085104  0.04369444 -0.22888596 -0.23066467]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 2935 is [True, False, False, False, False, True]
State prediction error at timestep 2935 is tensor(2.6215e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2935 of -1
Current timestep = 2936. State = [[-0.17486     0.12944871]]. Action = [[-0.02578972 -0.1971577  -0.11160746  0.5288372 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 2936 is [True, False, False, False, False, True]
State prediction error at timestep 2936 is tensor(3.9476e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2937. State = [[-0.17486107  0.12925002]]. Action = [[-0.03052127  0.13649356  0.04751027 -0.7829552 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 2937 is [True, False, False, False, False, True]
State prediction error at timestep 2937 is tensor(9.8454e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2938. State = [[-0.17524116  0.12867135]]. Action = [[-0.09561911  0.06067041 -0.18343751 -0.02492225]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 2938 is [True, False, False, False, False, True]
State prediction error at timestep 2938 is tensor(4.6641e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2938 of -1
Current timestep = 2939. State = [[-0.17780434  0.13024831]]. Action = [[ 0.11089942  0.06716058 -0.18171646  0.07488596]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 2939 is [True, False, False, False, False, True]
State prediction error at timestep 2939 is tensor(1.3479e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2939 of -1
Current timestep = 2940. State = [[-0.17818649  0.13112494]]. Action = [[-0.03254245 -0.17716664 -0.2272421   0.18874395]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 2940 is [True, False, False, False, False, True]
State prediction error at timestep 2940 is tensor(4.9773e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2941. State = [[-0.17837977  0.13151601]]. Action = [[ 0.22850776  0.08969307 -0.24343313  0.9063499 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 2941 is [True, False, False, False, False, True]
State prediction error at timestep 2941 is tensor(4.8768e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2942. State = [[-0.17902721  0.13270363]]. Action = [[ 0.04407683 -0.0066883   0.18724316 -0.7473551 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 2942 is [True, False, False, False, False, True]
State prediction error at timestep 2942 is tensor(1.5305e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2942 of -1
Current timestep = 2943. State = [[-0.17914115  0.1327259 ]]. Action = [[-0.2146178  -0.18249035 -0.08192143 -0.12152928]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 2943 is [True, False, False, False, False, True]
State prediction error at timestep 2943 is tensor(2.6745e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2944. State = [[-0.17907164  0.13267471]]. Action = [[-0.18308014 -0.17163949  0.20730588 -0.8202094 ]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 2944 is [True, False, False, False, False, True]
State prediction error at timestep 2944 is tensor(7.1896e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2944 of -1
Current timestep = 2945. State = [[-0.17910326  0.13279952]]. Action = [[ 0.02342135  0.21852997 -0.02286094 -0.17745405]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 2945 is [True, False, False, False, False, True]
State prediction error at timestep 2945 is tensor(4.1401e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2946. State = [[-0.17913222  0.1328537 ]]. Action = [[0.14731207 0.00864506 0.22838742 0.26329136]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 2946 is [True, False, False, False, False, True]
State prediction error at timestep 2946 is tensor(7.8955e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2947. State = [[-0.17935145  0.13315135]]. Action = [[-0.02191956  0.11508042 -0.18836285 -0.47982907]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 2947 is [True, False, False, False, False, True]
State prediction error at timestep 2947 is tensor(1.3957e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2947 of -1
Current timestep = 2948. State = [[-0.179882    0.13416305]]. Action = [[-0.23052008  0.12799153  0.16562635 -0.5248712 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 2948 is [True, False, False, False, False, True]
State prediction error at timestep 2948 is tensor(1.6732e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2949. State = [[-0.18106689  0.13652083]]. Action = [[ 0.06907994 -0.09410208 -0.01098396 -0.59237313]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 2949 is [True, False, False, False, False, True]
State prediction error at timestep 2949 is tensor(3.4264e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2949 of -1
Current timestep = 2950. State = [[-0.18068999  0.13608666]]. Action = [[ 0.02555498  0.00658503 -0.10221839  0.48819423]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 2950 is [True, False, False, False, False, True]
State prediction error at timestep 2950 is tensor(2.3688e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2950 of -1
Current timestep = 2951. State = [[-0.18053365  0.13594668]]. Action = [[-0.230704   -0.15805276  0.10723549  0.58567   ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 2951 is [True, False, False, False, False, True]
State prediction error at timestep 2951 is tensor(3.2868e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2952. State = [[-0.18030187  0.13565755]]. Action = [[-0.20597658  0.03581378 -0.22334512  0.81071126]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 2952 is [True, False, False, False, False, True]
State prediction error at timestep 2952 is tensor(2.9170e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2953. State = [[-0.18019406  0.13555558]]. Action = [[ 0.03038713  0.23397964 -0.19190979 -0.9013468 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 2953 is [True, False, False, False, False, True]
State prediction error at timestep 2953 is tensor(2.0806e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2954. State = [[-0.18026516  0.13561632]]. Action = [[-0.00785816  0.16656917 -0.21935683  0.44766152]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 2954 is [True, False, False, False, False, True]
State prediction error at timestep 2954 is tensor(4.6380e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2955. State = [[-0.18041566  0.13585052]]. Action = [[ 0.17811105  0.09775615 -0.15038091  0.3157798 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 2955 is [True, False, False, False, False, True]
State prediction error at timestep 2955 is tensor(4.3915e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2955 of -1
Current timestep = 2956. State = [[-0.18008974  0.13572408]]. Action = [[ 0.12016553 -0.14632241 -0.06336762  0.62474537]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 2956 is [True, False, False, False, False, True]
State prediction error at timestep 2956 is tensor(4.4399e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2957. State = [[-0.17998919  0.13566427]]. Action = [[ 0.22668326  0.16479045  0.0231868  -0.31367153]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 2957 is [True, False, False, False, False, True]
State prediction error at timestep 2957 is tensor(5.6540e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2958. State = [[-0.18007557  0.13564274]]. Action = [[-0.02729568  0.1710366  -0.16953361  0.0352751 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 2958 is [True, False, False, False, False, True]
State prediction error at timestep 2958 is tensor(2.1121e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2959. State = [[-0.1800112  0.1356895]]. Action = [[ 0.10196313  0.16473913  0.15601587 -0.89182854]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 2959 is [True, False, False, False, False, True]
State prediction error at timestep 2959 is tensor(1.7653e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2959 of -1
Current timestep = 2960. State = [[-0.17987798  0.13561286]]. Action = [[ 0.00558573  0.21389657 -0.17709981  0.5797651 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 2960 is [True, False, False, False, False, True]
State prediction error at timestep 2960 is tensor(6.3012e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2961. State = [[-0.17999034  0.13550021]]. Action = [[ 0.03944889  0.1893937   0.05121154 -0.17347312]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 2961 is [True, False, False, False, False, True]
State prediction error at timestep 2961 is tensor(1.6450e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2962. State = [[-0.17986749  0.13556272]]. Action = [[ 0.15945074 -0.06372625  0.24580348  0.28813303]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 2962 is [True, False, False, False, False, True]
State prediction error at timestep 2962 is tensor(5.1915e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2963. State = [[-0.17976345  0.13550508]]. Action = [[-0.03023192  0.0408453  -0.02680229  0.75930524]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 2963 is [True, False, False, False, False, True]
State prediction error at timestep 2963 is tensor(8.8047e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2963 of -1
Current timestep = 2964. State = [[-0.1800144   0.13603131]]. Action = [[0.13312817 0.10835034 0.21045709 0.08895814]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 2964 is [True, False, False, False, False, True]
State prediction error at timestep 2964 is tensor(9.9479e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2964 of -1
Current timestep = 2965. State = [[-0.17983212  0.13646695]]. Action = [[-0.21795239 -0.04028359 -0.05322646  0.5523714 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 2965 is [True, False, False, False, False, True]
State prediction error at timestep 2965 is tensor(1.9447e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2966. State = [[-0.17945206  0.13715805]]. Action = [[-0.19350229  0.00227788  0.2092019   0.06994939]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 2966 is [True, False, False, False, False, True]
State prediction error at timestep 2966 is tensor(4.9565e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2966 of -1
Current timestep = 2967. State = [[-0.1791164   0.13757645]]. Action = [[-0.21145016  0.06948555 -0.19059041 -0.6728497 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 2967 is [True, False, False, False, False, True]
State prediction error at timestep 2967 is tensor(4.0481e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2968. State = [[-0.17865308  0.13828631]]. Action = [[ 0.14561671 -0.16450879 -0.00914899  0.17422962]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 2968 is [True, False, False, False, False, True]
State prediction error at timestep 2968 is tensor(2.7674e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2969. State = [[-0.17860472  0.13880588]]. Action = [[0.20732117 0.02714226 0.22290984 0.13942337]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 2969 is [True, False, False, False, False, True]
State prediction error at timestep 2969 is tensor(8.9087e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2970. State = [[-0.1785964   0.13895854]]. Action = [[-0.0361997  -0.22099161 -0.08688813  0.1756779 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 2970 is [True, False, False, False, False, True]
State prediction error at timestep 2970 is tensor(1.1804e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2970 of -1
Current timestep = 2971. State = [[-0.17838591  0.13978219]]. Action = [[ 0.01627871 -0.00783208  0.1528373  -0.83985394]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 2971 is [True, False, False, False, False, True]
State prediction error at timestep 2971 is tensor(7.6150e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2972. State = [[-0.1781897   0.13995613]]. Action = [[ 0.22582912  0.09349108  0.24289066 -0.49582815]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 2972 is [True, False, False, False, False, True]
State prediction error at timestep 2972 is tensor(1.5862e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2972 of -1
Current timestep = 2973. State = [[-0.17803872  0.14022812]]. Action = [[ 0.08770582  0.15708491  0.17416912 -0.87213796]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 2973 is [True, False, False, False, False, True]
State prediction error at timestep 2973 is tensor(1.5211e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2974. State = [[-0.17808859  0.14033006]]. Action = [[ 0.13422918 -0.22294961 -0.08447644 -0.8490777 ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 2974 is [True, False, False, False, False, True]
State prediction error at timestep 2974 is tensor(6.8042e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2975. State = [[-0.17790832  0.14045286]]. Action = [[ 0.19164455  0.18675849  0.12240928 -0.11149043]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 2975 is [True, False, False, False, False, True]
State prediction error at timestep 2975 is tensor(1.7276e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2976. State = [[-0.17771626  0.14056413]]. Action = [[-0.21849479 -0.24272276  0.13882726 -0.7839306 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 2976 is [True, False, False, False, False, True]
State prediction error at timestep 2976 is tensor(3.9500e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2977. State = [[-0.17773148  0.14060915]]. Action = [[ 0.20737964  0.0631282   0.21835184 -0.04845655]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 2977 is [True, False, False, False, False, True]
State prediction error at timestep 2977 is tensor(8.2980e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2977 of -1
Current timestep = 2978. State = [[-0.17771941  0.14054184]]. Action = [[-0.1696261   0.13409662  0.04548791 -0.8366289 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 2978 is [True, False, False, False, False, True]
State prediction error at timestep 2978 is tensor(3.2442e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2979. State = [[-0.17760654  0.1409931 ]]. Action = [[-0.08898604  0.10030195  0.01519012 -0.29891163]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 2979 is [True, False, False, False, False, True]
State prediction error at timestep 2979 is tensor(1.9115e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2979 of -1
Current timestep = 2980. State = [[-0.17806816  0.1417263 ]]. Action = [[-0.18046103  0.21660823  0.21917564  0.25175738]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 2980 is [True, False, False, False, False, True]
State prediction error at timestep 2980 is tensor(8.7572e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2981. State = [[-0.1784288   0.14266197]]. Action = [[0.14161307 0.00285763 0.2047981  0.72150135]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 2981 is [True, False, False, False, False, True]
State prediction error at timestep 2981 is tensor(2.5526e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2982. State = [[-0.17866325  0.14339231]]. Action = [[-0.13422057  0.02903736  0.2247073  -0.73888475]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 2982 is [True, False, False, False, False, True]
State prediction error at timestep 2982 is tensor(2.0324e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2982 of -1
Current timestep = 2983. State = [[-0.17949598  0.14456597]]. Action = [[0.16454315 0.243545   0.03594711 0.88768053]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 2983 is [True, False, False, False, False, True]
State prediction error at timestep 2983 is tensor(1.2350e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2984. State = [[-0.18044959  0.14668632]]. Action = [[-0.00507075  0.13041177 -0.08584413 -0.6735362 ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 2984 is [True, False, False, False, False, True]
State prediction error at timestep 2984 is tensor(3.1838e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2984 of -1
Current timestep = 2985. State = [[-0.18110833  0.1481852 ]]. Action = [[-0.2160417   0.17586064 -0.16880172  0.4666183 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 2985 is [True, False, False, False, False, True]
State prediction error at timestep 2985 is tensor(8.0648e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2986. State = [[-0.18272524  0.1520259 ]]. Action = [[ 0.09699702  0.04810101 -0.07579678 -0.92132556]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 2986 is [True, False, False, False, False, True]
State prediction error at timestep 2986 is tensor(1.5209e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2986 of -1
Current timestep = 2987. State = [[-0.18205848  0.15526779]]. Action = [[-0.04760244  0.12960613 -0.06979741  0.6426015 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 2987 is [True, False, False, False, False, True]
State prediction error at timestep 2987 is tensor(8.0035e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2988. State = [[-0.18236746  0.15746187]]. Action = [[-0.07830434  0.22744226  0.18737215  0.9002521 ]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 2988 is [True, False, False, False, False, True]
State prediction error at timestep 2988 is tensor(1.2275e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2989. State = [[-0.18298416  0.16212198]]. Action = [[-0.01586555  0.02851349  0.03567111  0.06019402]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 2989 is [True, False, False, False, False, True]
State prediction error at timestep 2989 is tensor(3.4617e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2990. State = [[-0.1836595   0.16340452]]. Action = [[ 0.04182965  0.1740455  -0.19102712  0.7760174 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 2990 is [True, False, False, False, False, True]
State prediction error at timestep 2990 is tensor(9.7632e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2991. State = [[-0.1841161   0.16411288]]. Action = [[0.11344755 0.21504414 0.22002998 0.38315463]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 2991 is [True, False, False, False, False, True]
State prediction error at timestep 2991 is tensor(1.5889e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2992. State = [[-0.18511553  0.16659357]]. Action = [[ 0.01431435 -0.0940066   0.20263508  0.3714701 ]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 2992 is [True, False, False, False, False, True]
State prediction error at timestep 2992 is tensor(1.3372e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2993. State = [[-0.18482822  0.16652201]]. Action = [[ 0.00207049  0.01489031 -0.02610117 -0.88822395]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 2993 is [True, False, False, False, False, True]
State prediction error at timestep 2993 is tensor(6.3703e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2994. State = [[-0.18460965  0.16635934]]. Action = [[ 0.10490099 -0.11870614 -0.0917052  -0.677789  ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 2994 is [True, False, False, False, False, True]
State prediction error at timestep 2994 is tensor(1.0424e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2995. State = [[-0.18252432  0.16387583]]. Action = [[ 0.01273072 -0.04206623 -0.19064535  0.5597415 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 2995 is [True, False, False, False, False, True]
State prediction error at timestep 2995 is tensor(4.9002e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2995 of -1
Current timestep = 2996. State = [[-0.18115643  0.16160955]]. Action = [[-0.109312   -0.06694414 -0.02736989 -0.5475587 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 2996 is [True, False, False, False, False, True]
State prediction error at timestep 2996 is tensor(1.8646e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2996 of -1
Current timestep = 2997. State = [[-0.18101569  0.1608387 ]]. Action = [[ 0.02943486  0.1894579  -0.21351297 -0.8788325 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 2997 is [True, False, False, False, False, True]
State prediction error at timestep 2997 is tensor(4.9200e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2998. State = [[-0.18076953  0.16020417]]. Action = [[ 0.14915961  0.15960765 -0.14481698  0.44785285]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 2998 is [True, False, False, False, False, True]
State prediction error at timestep 2998 is tensor(3.5801e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2999. State = [[-0.18063141  0.15960288]]. Action = [[-0.22404385  0.12755263  0.16699666 -0.5173145 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 2999 is [True, False, False, False, False, True]
State prediction error at timestep 2999 is tensor(5.8849e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2999 of -1
Current timestep = 3000. State = [[-0.18025728  0.15714414]]. Action = [[ 0.11657453 -0.00411545  0.2134983   0.01891661]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 3000 is [True, False, False, False, False, True]
State prediction error at timestep 3000 is tensor(1.8497e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3001. State = [[-0.1799462   0.15674552]]. Action = [[-0.11582723  0.2065121   0.03327543  0.75375533]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 3001 is [True, False, False, False, False, True]
State prediction error at timestep 3001 is tensor(2.3019e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3002. State = [[-0.17954773  0.15632327]]. Action = [[ 0.20589226  0.09635806  0.02215895 -0.33974326]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 3002 is [True, False, False, False, False, True]
State prediction error at timestep 3002 is tensor(8.1036e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3003. State = [[-0.17925549  0.15598479]]. Action = [[ 0.23104733 -0.08695123 -0.15143095  0.26113045]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 3003 is [True, False, False, False, False, True]
State prediction error at timestep 3003 is tensor(4.4259e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3004. State = [[-0.1792359  0.1556447]]. Action = [[ 0.02984598 -0.13954592  0.23241287 -0.64236724]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 3004 is [True, False, False, False, False, True]
State prediction error at timestep 3004 is tensor(6.1680e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3005. State = [[-0.17888835  0.15553617]]. Action = [[-0.14707367  0.05277187  0.19190109  0.2674898 ]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 3005 is [True, False, False, False, False, True]
State prediction error at timestep 3005 is tensor(3.6375e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3005 of -1
Current timestep = 3006. State = [[-0.17858076  0.15538517]]. Action = [[ 0.05182901  0.24017122 -0.12068829 -0.55130816]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 3006 is [True, False, False, False, False, True]
State prediction error at timestep 3006 is tensor(1.8495e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3007. State = [[-0.17862238  0.15524355]]. Action = [[-0.19238722  0.12500402 -0.21848589  0.5459403 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 3007 is [True, False, False, False, False, True]
State prediction error at timestep 3007 is tensor(2.1238e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3008. State = [[-0.17808504  0.15531595]]. Action = [[-0.24488367  0.07835695 -0.07165317 -0.5139807 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 3008 is [True, False, False, False, False, True]
State prediction error at timestep 3008 is tensor(2.3329e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3009. State = [[-0.17817673  0.15513211]]. Action = [[-0.1890764   0.22720823  0.20748422  0.3306228 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 3009 is [True, False, False, False, False, True]
State prediction error at timestep 3009 is tensor(4.3756e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3010. State = [[-0.177756    0.15504338]]. Action = [[ 0.11470148  0.10528132 -0.16382684  0.6985476 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 3010 is [True, False, False, False, False, True]
State prediction error at timestep 3010 is tensor(7.4959e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3010 of -1
Current timestep = 3011. State = [[-0.1766899  0.1557608]]. Action = [[0.0731242  0.20011428 0.02177566 0.891654  ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 3011 is [True, False, False, False, False, True]
State prediction error at timestep 3011 is tensor(4.2453e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3011 of -1
Current timestep = 3012. State = [[-0.1759959   0.15614259]]. Action = [[ 0.13079488  0.14813942 -0.20763552  0.13617098]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 3012 is [True, False, False, False, False, True]
State prediction error at timestep 3012 is tensor(1.8306e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3013. State = [[-0.1736336  0.157448 ]]. Action = [[-0.0826249   0.13180566 -0.15494967  0.1898408 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 3013 is [True, False, False, False, False, True]
State prediction error at timestep 3013 is tensor(7.8895e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3014. State = [[-0.17377438  0.1587212 ]]. Action = [[ 0.1379162   0.21460277 -0.01914693  0.01186311]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 3014 is [True, False, False, False, False, True]
State prediction error at timestep 3014 is tensor(6.6851e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3015. State = [[-0.17377976  0.16012737]]. Action = [[-0.11684251 -0.24663435  0.04235443 -0.5446089 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 3015 is [True, False, False, False, False, True]
State prediction error at timestep 3015 is tensor(4.2232e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3016. State = [[-0.1736214   0.16109815]]. Action = [[0.18687838 0.16390431 0.14406374 0.7252629 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 3016 is [True, False, False, False, False, True]
State prediction error at timestep 3016 is tensor(9.8097e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3017. State = [[-0.1735148   0.16308248]]. Action = [[-0.08638403 -0.11858574 -0.19545728  0.805078  ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 3017 is [True, False, False, False, False, True]
State prediction error at timestep 3017 is tensor(2.6405e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3017 of -1
Current timestep = 3018. State = [[-0.17350852  0.16301161]]. Action = [[-0.07633595  0.17944723  0.23528337  0.46473348]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 3018 is [True, False, False, False, False, True]
State prediction error at timestep 3018 is tensor(7.8731e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3019. State = [[-0.17342964  0.16283448]]. Action = [[ 0.20846105 -0.08745936 -0.02205719 -0.6280842 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 3019 is [True, False, False, False, False, True]
State prediction error at timestep 3019 is tensor(1.0068e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3019 of -1
Current timestep = 3020. State = [[-0.17334399  0.16257031]]. Action = [[-0.16294843 -0.21666248  0.14518332 -0.5031231 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 3020 is [True, False, False, False, False, True]
State prediction error at timestep 3020 is tensor(1.3118e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3021. State = [[-0.17345515  0.16280277]]. Action = [[-0.13560529 -0.07201454 -0.11332963 -0.93326867]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 3021 is [True, False, False, False, False, True]
State prediction error at timestep 3021 is tensor(1.7417e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3022. State = [[-0.17345515  0.16280277]]. Action = [[-0.20878208 -0.02106483  0.12744918  0.812858  ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 3022 is [True, False, False, False, False, True]
State prediction error at timestep 3022 is tensor(9.8983e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3023. State = [[-0.1733613  0.1625575]]. Action = [[ 0.13706169  0.1944679  -0.2116156   0.51837647]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 3023 is [True, False, False, False, False, True]
State prediction error at timestep 3023 is tensor(9.2323e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3023 of -1
Current timestep = 3024. State = [[-0.1733613  0.1625575]]. Action = [[ 0.14347374 -0.0189583   0.10024872  0.3381884 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 3024 is [True, False, False, False, False, True]
State prediction error at timestep 3024 is tensor(3.0979e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3025. State = [[-0.17339364  0.1626128 ]]. Action = [[ 0.1625148   0.20915356  0.14306256 -0.30967343]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 3025 is [True, False, False, False, False, True]
State prediction error at timestep 3025 is tensor(1.5118e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3026. State = [[-0.17337902  0.16254567]]. Action = [[ 0.10496825  0.24029404 -0.14731191  0.23240459]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 3026 is [True, False, False, False, False, True]
State prediction error at timestep 3026 is tensor(2.9042e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3026 of -1
Current timestep = 3027. State = [[-0.17334668  0.16249035]]. Action = [[-0.05228661 -0.13434954  0.16144866  0.51559854]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 3027 is [True, False, False, False, False, True]
State prediction error at timestep 3027 is tensor(6.2042e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3028. State = [[-0.17339632  0.16253284]]. Action = [[-0.09787306  0.01667184  0.21337691  0.5969982 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 3028 is [True, False, False, False, False, True]
State prediction error at timestep 3028 is tensor(9.8358e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3029. State = [[-0.17356026  0.16271183]]. Action = [[ 0.1964044  -0.22722796  0.19195187 -0.36545604]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 3029 is [True, False, False, False, False, True]
State prediction error at timestep 3029 is tensor(2.8629e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3030. State = [[-0.17375062  0.16312133]]. Action = [[-0.17519836  0.10209721  0.03504312 -0.61461985]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 3030 is [True, False, False, False, False, True]
State prediction error at timestep 3030 is tensor(1.1005e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3031. State = [[-0.17380041  0.1631642 ]]. Action = [[-0.17426616  0.02375633  0.0848169  -0.0146513 ]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 3031 is [True, False, False, False, False, True]
State prediction error at timestep 3031 is tensor(9.1965e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3032. State = [[-0.1738333   0.16313061]]. Action = [[ 0.10002664 -0.14942099 -0.10304552  0.42965817]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 3032 is [True, False, False, False, False, True]
State prediction error at timestep 3032 is tensor(4.0745e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3033. State = [[-0.17392989  0.16329531]]. Action = [[0.1570707  0.10415611 0.14580649 0.42176247]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 3033 is [True, False, False, False, False, True]
State prediction error at timestep 3033 is tensor(1.1571e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3033 of -1
Current timestep = 3034. State = [[-0.17404136  0.16352756]]. Action = [[-0.2125118  -0.21040535 -0.20974144 -0.02988821]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 3034 is [True, False, False, False, False, True]
State prediction error at timestep 3034 is tensor(7.6065e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3035. State = [[-0.17407562  0.16349299]]. Action = [[-0.06776996  0.24008217  0.19056898 -0.43506837]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 3035 is [True, False, False, False, False, True]
State prediction error at timestep 3035 is tensor(2.7534e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3036. State = [[-0.17410775  0.1635477 ]]. Action = [[ 0.13946983 -0.13886829  0.00379035 -0.67083275]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 3036 is [True, False, False, False, False, True]
State prediction error at timestep 3036 is tensor(4.1668e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3037. State = [[-0.17415504  0.16367048]]. Action = [[ 0.23747534  0.13829815 -0.05204244 -0.19765484]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 3037 is [True, False, False, False, False, True]
State prediction error at timestep 3037 is tensor(4.7404e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3037 of -1
Current timestep = 3038. State = [[-0.17417233  0.16365762]]. Action = [[-0.07742012  0.23634106  0.21799746  0.7960261 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 3038 is [True, False, False, False, False, True]
State prediction error at timestep 3038 is tensor(8.4986e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3039. State = [[-0.17420447  0.16371232]]. Action = [[ 0.10849684  0.22592157 -0.05334692  0.4463371 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 3039 is [True, False, False, False, False, True]
State prediction error at timestep 3039 is tensor(3.6799e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3040. State = [[-0.17422205  0.16369997]]. Action = [[-0.00419159 -0.11114284  0.09795132  0.01418245]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 3040 is [True, False, False, False, False, True]
State prediction error at timestep 3040 is tensor(1.5099e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3040 of -1
Current timestep = 3041. State = [[-0.17408867  0.16309233]]. Action = [[ 0.0126749  -0.16659938 -0.20444363 -0.38356972]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 3041 is [True, False, False, False, False, True]
State prediction error at timestep 3041 is tensor(5.3608e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3042. State = [[-0.17397822  0.1623973 ]]. Action = [[-0.17726056 -0.21639468 -0.10541394  0.20069659]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 3042 is [True, False, False, False, False, True]
State prediction error at timestep 3042 is tensor(5.2014e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3043. State = [[-0.17381547  0.16197528]]. Action = [[-0.18777178  0.21059918  0.2081621   0.5049187 ]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 3043 is [True, False, False, False, False, True]
State prediction error at timestep 3043 is tensor(3.8222e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3044. State = [[-0.17376637  0.16171199]]. Action = [[-0.16857816  0.04236767 -0.15312022 -0.53705126]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 3044 is [True, False, False, False, False, True]
State prediction error at timestep 3044 is tensor(5.4071e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3044 of -1
Current timestep = 3045. State = [[-0.17362516  0.1604843 ]]. Action = [[ 0.07225561  0.10173863 -0.08423626  0.27697825]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 3045 is [True, False, False, False, False, True]
State prediction error at timestep 3045 is tensor(2.9706e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3045 of -1
Current timestep = 3046. State = [[-0.17367798  0.16082914]]. Action = [[-0.22139153  0.22590351  0.12822843  0.12782407]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 3046 is [True, False, False, False, False, True]
State prediction error at timestep 3046 is tensor(1.4015e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3047. State = [[-0.17375645  0.16100629]]. Action = [[-0.23009136  0.22488639  0.09653625  0.66965306]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 3047 is [True, False, False, False, False, True]
State prediction error at timestep 3047 is tensor(3.5414e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3048. State = [[-0.17393388  0.16126858]]. Action = [[-0.08300839  0.0442107  -0.02679157 -0.6307396 ]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 3048 is [True, False, False, False, False, True]
State prediction error at timestep 3048 is tensor(9.5313e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3048 of -1
Current timestep = 3049. State = [[-0.17427552  0.16182373]]. Action = [[-0.07773688  0.23188537  0.1621851  -0.5688327 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 3049 is [True, False, False, False, False, True]
State prediction error at timestep 3049 is tensor(9.5912e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3050. State = [[-0.1745228  0.1624037]]. Action = [[-0.21426818 -0.14374547 -0.16027288  0.15681493]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 3050 is [True, False, False, False, False, True]
State prediction error at timestep 3050 is tensor(1.0050e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3051. State = [[-0.17476273  0.16285536]]. Action = [[-0.19479115 -0.15203096 -0.08786303 -0.95028   ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 3051 is [True, False, False, False, False, True]
State prediction error at timestep 3051 is tensor(2.4096e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3052. State = [[-0.17585404  0.1645402 ]]. Action = [[-0.13284679  0.12624049 -0.16000922 -0.16986173]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 3052 is [True, False, False, False, False, True]
State prediction error at timestep 3052 is tensor(4.3702e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3052 of -1
Current timestep = 3053. State = [[-0.17656836  0.16572385]]. Action = [[-0.20797616  0.08117267 -0.07163362 -0.8505868 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 3053 is [True, False, False, False, False, True]
State prediction error at timestep 3053 is tensor(6.1614e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3054. State = [[-0.17753908  0.167602  ]]. Action = [[-0.0118273  -0.17464726 -0.20004445 -0.17833257]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 3054 is [True, False, False, False, False, True]
State prediction error at timestep 3054 is tensor(3.4171e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3055. State = [[-0.17814362  0.16887504]]. Action = [[ 0.17729837  0.1847505  -0.20491849 -0.48398775]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 3055 is [True, False, False, False, False, True]
State prediction error at timestep 3055 is tensor(6.9405e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3055 of -1
Current timestep = 3056. State = [[-0.17903371  0.16977863]]. Action = [[-0.17546025 -0.01728891 -0.07904708 -0.9234063 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 3056 is [True, False, False, False, False, True]
State prediction error at timestep 3056 is tensor(3.9103e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3057. State = [[-0.17956609  0.17071612]]. Action = [[ 0.14496714 -0.13467866  0.21862459  0.37160933]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 3057 is [True, False, False, False, False, True]
State prediction error at timestep 3057 is tensor(2.5510e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3058. State = [[-0.18028559  0.17223132]]. Action = [[-0.20635638 -0.18849865  0.19363278  0.71381545]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 3058 is [True, False, False, False, False, True]
State prediction error at timestep 3058 is tensor(1.3667e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3059. State = [[-0.18103221  0.17301993]]. Action = [[ 0.12625146 -0.18022168 -0.22278616 -0.10229713]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 3059 is [True, False, False, False, False, True]
State prediction error at timestep 3059 is tensor(2.7900e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3059 of -1
Current timestep = 3060. State = [[-0.18232729  0.17537105]]. Action = [[-0.10697314  0.04013076  0.09042495 -0.04403639]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 3060 is [True, False, False, False, False, True]
State prediction error at timestep 3060 is tensor(1.1734e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3060 of -1
Current timestep = 3061. State = [[-0.18285678  0.17595652]]. Action = [[ 0.13710412 -0.13165915 -0.12381411 -0.02605546]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 3061 is [True, False, False, False, False, True]
State prediction error at timestep 3061 is tensor(1.0881e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3062. State = [[-0.18368268  0.17696694]]. Action = [[-0.2231124  -0.15760319 -0.07007369 -0.2515564 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 3062 is [True, False, False, False, False, True]
State prediction error at timestep 3062 is tensor(1.0170e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3063. State = [[-0.18403928  0.17758228]]. Action = [[-0.10646686 -0.18731126  0.20329225 -0.0761618 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 3063 is [True, False, False, False, False, True]
State prediction error at timestep 3063 is tensor(2.1296e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3064. State = [[-0.18453415  0.17797084]]. Action = [[0.13540158 0.19987491 0.12131381 0.4480996 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 3064 is [True, False, False, False, False, True]
State prediction error at timestep 3064 is tensor(1.6523e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3065. State = [[-0.18517534  0.17870022]]. Action = [[-0.15332638  0.04805937 -0.21486461  0.13467336]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 3065 is [True, False, False, False, False, True]
State prediction error at timestep 3065 is tensor(4.7683e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3065 of -1
Current timestep = 3066. State = [[-0.18646675  0.18028788]]. Action = [[-0.04786739 -0.01330705 -0.20926195 -0.37587965]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 3066 is [True, False, False, False, False, True]
State prediction error at timestep 3066 is tensor(2.1390e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3067. State = [[-0.18698002  0.18045981]]. Action = [[-0.11623967  0.21630594 -0.02644578  0.92496395]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 3067 is [True, False, False, False, False, True]
State prediction error at timestep 3067 is tensor(1.3580e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3068. State = [[-0.1877753   0.18082933]]. Action = [[ 0.01998356 -0.09427157  0.24499518 -0.8179895 ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 3068 is [True, False, False, False, False, True]
State prediction error at timestep 3068 is tensor(8.2904e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3069. State = [[-0.18791074  0.17867516]]. Action = [[-0.06006363  0.11167061  0.1210227   0.20254397]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 3069 is [True, False, False, False, False, True]
State prediction error at timestep 3069 is tensor(3.9414e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3070. State = [[-0.18965614  0.18019535]]. Action = [[ 0.09161106  0.05422071 -0.1733309  -0.6687757 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 3070 is [True, False, False, False, False, True]
State prediction error at timestep 3070 is tensor(8.6544e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3070 of -1
Current timestep = 3071. State = [[-0.19001007  0.1809458 ]]. Action = [[0.22529334 0.08769739 0.10516477 0.66886616]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 3071 is [True, False, False, False, False, True]
State prediction error at timestep 3071 is tensor(3.5389e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3072. State = [[-0.19018397  0.18142153]]. Action = [[-0.0593967   0.19429746 -0.04785639  0.8619182 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 3072 is [True, False, False, False, False, True]
State prediction error at timestep 3072 is tensor(1.0143e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3073. State = [[-0.19043216  0.18146951]]. Action = [[-0.19659305 -0.1632426   0.14927238  0.45594072]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 3073 is [True, False, False, False, False, True]
State prediction error at timestep 3073 is tensor(4.4478e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3074. State = [[-0.1909449  0.1820146]]. Action = [[ 0.1170705  -0.09557995 -0.16143294  0.05695546]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 3074 is [True, False, False, False, False, True]
State prediction error at timestep 3074 is tensor(3.4925e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3074 of -1
Current timestep = 3075. State = [[-0.19052804  0.1814444 ]]. Action = [[-0.04698947 -0.1796633   0.06213906  0.985548  ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 3075 is [True, False, False, False, False, True]
State prediction error at timestep 3075 is tensor(2.4112e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3076. State = [[-0.19001284  0.17952456]]. Action = [[ 0.00667101  0.0337742   0.04172027 -0.33093297]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 3076 is [True, False, False, False, False, True]
State prediction error at timestep 3076 is tensor(7.6987e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3076 of -1
Current timestep = 3077. State = [[-0.18996957  0.17938548]]. Action = [[-0.19595784 -0.21705724 -0.15966454 -0.7570328 ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 3077 is [True, False, False, False, False, True]
State prediction error at timestep 3077 is tensor(4.3110e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3078. State = [[-0.18991849  0.17926507]]. Action = [[-0.17573749 -0.10727289 -0.2029968   0.45283818]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 3078 is [True, False, False, False, False, True]
State prediction error at timestep 3078 is tensor(2.0474e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3079. State = [[-0.18988562  0.17922115]]. Action = [[0.16918308 0.19587243 0.04670721 0.6997409 ]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 3079 is [True, False, False, False, False, True]
State prediction error at timestep 3079 is tensor(3.1049e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3080. State = [[-0.1898092   0.17880371]]. Action = [[-0.11781952 -0.09204186 -0.16998884 -0.10640669]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 3080 is [True, False, False, False, False, True]
State prediction error at timestep 3080 is tensor(7.7290e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3080 of -1
Current timestep = 3081. State = [[-0.18976213  0.1782122 ]]. Action = [[ 0.02730817 -0.2077085   0.18198356  0.15560508]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 3081 is [True, False, False, False, False, True]
State prediction error at timestep 3081 is tensor(3.9349e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3082. State = [[-0.18965273  0.17781533]]. Action = [[-0.11740299  0.15618634 -0.24006742  0.6673813 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 3082 is [True, False, False, False, False, True]
State prediction error at timestep 3082 is tensor(1.9198e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3083. State = [[-0.18942751  0.17666942]]. Action = [[-0.06853113  0.03150576 -0.12172401  0.16729426]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 3083 is [True, False, False, False, False, True]
State prediction error at timestep 3083 is tensor(2.8379e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3083 of -1
Current timestep = 3084. State = [[-0.19012013  0.17759264]]. Action = [[-0.05525529  0.12639606  0.23778415 -0.7030502 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 3084 is [True, False, False, False, False, True]
State prediction error at timestep 3084 is tensor(9.1903e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3085. State = [[-0.19074939  0.17852502]]. Action = [[ 0.14703786  0.22747388 -0.19803782  0.44763052]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 3085 is [True, False, False, False, False, True]
State prediction error at timestep 3085 is tensor(1.0487e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3085 of -1
Current timestep = 3086. State = [[-0.191594    0.18016899]]. Action = [[ 0.14710039  0.21150988 -0.02579175  0.44827843]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 3086 is [True, False, False, False, False, True]
State prediction error at timestep 3086 is tensor(3.3744e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3087. State = [[-0.19202565  0.18070441]]. Action = [[ 0.13735336 -0.0713827  -0.09109722 -0.13479602]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 3087 is [True, False, False, False, False, True]
State prediction error at timestep 3087 is tensor(1.7021e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3088. State = [[-0.1927682   0.18148713]]. Action = [[-0.16947694  0.10106143  0.09769604 -0.69043887]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 3088 is [True, False, False, False, False, True]
State prediction error at timestep 3088 is tensor(9.9018e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3089. State = [[-0.19323626  0.1825394 ]]. Action = [[ 0.13373965 -0.13822842 -0.04395209 -0.29806137]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 3089 is [True, False, False, False, False, True]
State prediction error at timestep 3089 is tensor(1.1332e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3090. State = [[-0.19360414  0.18313095]]. Action = [[-0.04503825 -0.17953622  0.02106205  0.37271094]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 3090 is [True, False, False, False, False, True]
State prediction error at timestep 3090 is tensor(2.5263e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3090 of -1
Current timestep = 3091. State = [[-0.19403103  0.18343283]]. Action = [[-0.2162445  -0.15470447  0.13400483  0.17509091]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 3091 is [True, False, False, False, False, True]
State prediction error at timestep 3091 is tensor(1.2020e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3092. State = [[-0.19495898  0.18501703]]. Action = [[0.13355863 0.2214427  0.16097307 0.8176906 ]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 3092 is [True, False, False, False, False, True]
State prediction error at timestep 3092 is tensor(3.9693e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3093. State = [[-0.19506012  0.18508536]]. Action = [[ 0.15965152  0.24415445 -0.17294517  0.4387331 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 3093 is [True, False, False, False, False, True]
State prediction error at timestep 3093 is tensor(2.6798e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3094. State = [[-0.19514994  0.18511638]]. Action = [[ 0.1632762   0.18872094  0.18638098 -0.4160694 ]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 3094 is [True, False, False, False, False, True]
State prediction error at timestep 3094 is tensor(2.1913e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3095. State = [[-0.1955242  0.1856771]]. Action = [[ 0.01469398  0.12829709  0.21177724 -0.86357874]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 3095 is [True, False, False, False, False, True]
State prediction error at timestep 3095 is tensor(3.7760e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3095 of -1
Current timestep = 3096. State = [[-0.19622473  0.18687283]]. Action = [[ 0.24116358 -0.06768693 -0.20638196 -0.5295248 ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 3096 is [True, False, False, False, False, True]
State prediction error at timestep 3096 is tensor(5.0995e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3097. State = [[-0.19681257  0.18795502]]. Action = [[-0.19961363 -0.16897742  0.02596027 -0.20636404]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 3097 is [True, False, False, False, False, True]
State prediction error at timestep 3097 is tensor(1.6941e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3097 of -1
Current timestep = 3098. State = [[-0.19724797  0.18835908]]. Action = [[ 0.1265207   0.1336729  -0.07727782  0.20849967]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 3098 is [True, False, False, False, False, True]
State prediction error at timestep 3098 is tensor(1.0978e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3099. State = [[-0.19763348  0.18894862]]. Action = [[-0.1139971   0.17332542 -0.11647625 -0.13225675]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 3099 is [True, False, False, False, False, True]
State prediction error at timestep 3099 is tensor(5.6664e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3100. State = [[-0.1979565   0.18971029]]. Action = [[0.1295861  0.15972248 0.11500952 0.67084396]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 3100 is [True, False, False, False, False, True]
State prediction error at timestep 3100 is tensor(8.3151e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3101. State = [[-0.19838272  0.19010417]]. Action = [[-0.15789485  0.22284445  0.15811408  0.23406553]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 3101 is [True, False, False, False, False, True]
State prediction error at timestep 3101 is tensor(8.8756e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3102. State = [[-0.19857444  0.19035251]]. Action = [[-0.03421038 -0.22981393 -0.00097345 -0.4163915 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 3102 is [True, False, False, False, False, True]
State prediction error at timestep 3102 is tensor(1.9767e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3102 of -1
Current timestep = 3103. State = [[-0.1991525   0.19139765]]. Action = [[-0.00939849 -0.01299907 -0.06479916  0.06189132]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 3103 is [True, False, False, False, False, True]
State prediction error at timestep 3103 is tensor(9.0079e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3104. State = [[-0.19937986  0.19169766]]. Action = [[4.2435527e-04 9.5905662e-03 1.2207565e-01 5.0716734e-01]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 3104 is [True, False, False, False, False, True]
State prediction error at timestep 3104 is tensor(1.6480e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3104 of -1
Current timestep = 3105. State = [[-0.19945024  0.19180237]]. Action = [[-0.182844    0.10062224  0.06896478 -0.5194281 ]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 3105 is [True, False, False, False, False, True]
State prediction error at timestep 3105 is tensor(1.1977e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3106. State = [[-0.19959384  0.19195014]]. Action = [[-0.18414295  0.01775235 -0.12740989 -0.3862056 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 3106 is [True, False, False, False, False, True]
State prediction error at timestep 3106 is tensor(1.7200e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3106 of -1
Current timestep = 3107. State = [[-0.19962578  0.19206335]]. Action = [[ 0.10805929 -0.23165989  0.17261583 -0.0545783 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 3107 is [True, False, False, False, False, True]
State prediction error at timestep 3107 is tensor(1.0418e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3108. State = [[-0.19971496  0.19223432]]. Action = [[-0.22783488  0.05908883 -0.13413377  0.44087076]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 3108 is [True, False, False, False, False, True]
State prediction error at timestep 3108 is tensor(1.4001e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3109. State = [[-0.19973162  0.19222058]]. Action = [[-0.23132886  0.01173908  0.1051476  -0.6266186 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 3109 is [True, False, False, False, False, True]
State prediction error at timestep 3109 is tensor(5.7968e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3110. State = [[-0.19990887  0.19256824]]. Action = [[ 0.11414629 -0.01075673 -0.11213326 -0.54188603]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 3110 is [True, False, False, False, False, True]
State prediction error at timestep 3110 is tensor(1.0651e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3110 of -1
Current timestep = 3111. State = [[-0.19982556  0.192506  ]]. Action = [[ 0.1772772  -0.07032922 -0.21125191 -0.8882282 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 3111 is [True, False, False, False, False, True]
State prediction error at timestep 3111 is tensor(5.0299e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3112. State = [[-0.199679    0.19226462]]. Action = [[-0.11453626 -0.11281613  0.10557124  0.13042879]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 3112 is [True, False, False, False, False, True]
State prediction error at timestep 3112 is tensor(3.9844e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3112 of -1
Current timestep = 3113. State = [[-0.19916843  0.19047941]]. Action = [[-0.00905031 -0.01269464 -0.21569805 -0.01595455]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 3113 is [True, False, False, False, False, True]
State prediction error at timestep 3113 is tensor(1.0196e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3113 of -1
Current timestep = 3114. State = [[-0.19903615  0.18992601]]. Action = [[-0.21761133 -0.00052513 -0.02871431 -0.4324968 ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 3114 is [True, False, False, False, False, True]
State prediction error at timestep 3114 is tensor(2.1968e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3115. State = [[-0.1989449   0.18967073]]. Action = [[-0.16994041  0.06499714  0.21468088  0.9810178 ]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 3115 is [True, False, False, False, False, True]
State prediction error at timestep 3115 is tensor(1.6419e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3116. State = [[-0.19887237  0.18941496]]. Action = [[ 0.02670416 -0.2012943  -0.23765917  0.97725034]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 3116 is [True, False, False, False, False, True]
State prediction error at timestep 3116 is tensor(4.5075e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3117. State = [[-0.19885398  0.18934858]]. Action = [[ 0.07731819  0.21457607  0.19900763 -0.42523444]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 3117 is [True, False, False, False, False, True]
State prediction error at timestep 3117 is tensor(8.8514e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3117 of -1
Current timestep = 3118. State = [[-0.19874296  0.18894152]]. Action = [[ 0.0658012  -0.14969195  0.11641771  0.11670208]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 3118 is [True, False, False, False, False, True]
State prediction error at timestep 3118 is tensor(3.2795e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3119. State = [[-0.19861493  0.1884773 ]]. Action = [[ 0.08307576  0.08220035  0.00154674 -0.32783318]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 3119 is [True, False, False, False, False, True]
State prediction error at timestep 3119 is tensor(1.7688e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3120. State = [[-0.19868718  0.1886715 ]]. Action = [[-0.16142373 -0.14767012 -0.00475959  0.17624116]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 3120 is [True, False, False, False, False, True]
State prediction error at timestep 3120 is tensor(2.9582e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3121. State = [[-0.19868718  0.1886715 ]]. Action = [[ 0.15345627 -0.03088589  0.13823897 -0.59863734]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 3121 is [True, False, False, False, False, True]
State prediction error at timestep 3121 is tensor(1.2165e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3122. State = [[-0.19863419  0.18855293]]. Action = [[ 0.00661728  0.24874109 -0.05899873  0.21971011]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 3122 is [True, False, False, False, False, True]
State prediction error at timestep 3122 is tensor(1.1492e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3123. State = [[-0.19863872  0.18875362]]. Action = [[ 0.12556887  0.11873823 -0.15453395  0.743258  ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 3123 is [True, False, False, False, False, True]
State prediction error at timestep 3123 is tensor(2.4743e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3124. State = [[-0.1985079   0.18920577]]. Action = [[-0.19160534 -0.08914715 -0.1360713   0.33252227]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 3124 is [True, False, False, False, False, True]
State prediction error at timestep 3124 is tensor(3.8395e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3125. State = [[-0.19850822  0.18982705]]. Action = [[-0.17051286  0.04080653 -0.00683855 -0.13998073]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 3125 is [True, False, False, False, False, True]
State prediction error at timestep 3125 is tensor(1.6736e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3126. State = [[-0.19856542  0.1901755 ]]. Action = [[ 0.21262717  0.13406807 -0.13221584 -0.2537371 ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 3126 is [True, False, False, False, False, True]
State prediction error at timestep 3126 is tensor(2.7254e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3126 of -1
Current timestep = 3127. State = [[-0.19825423  0.19052513]]. Action = [[ 0.09619904 -0.20894524  0.08285445 -0.44908643]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 3127 is [True, False, False, False, False, True]
State prediction error at timestep 3127 is tensor(9.2968e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3128. State = [[-0.19815664  0.19071177]]. Action = [[ 0.08025423 -0.21640666  0.16405118  0.72666025]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 3128 is [True, False, False, False, False, True]
State prediction error at timestep 3128 is tensor(1.1577e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3129. State = [[-0.19832766  0.19080305]]. Action = [[ 0.16314608 -0.02736589 -0.23527293  0.5100615 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 3129 is [True, False, False, False, False, True]
State prediction error at timestep 3129 is tensor(7.5796e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3130. State = [[-0.19833703  0.19122297]]. Action = [[ 0.02231508  0.20381567  0.15883082 -0.38215256]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 3130 is [True, False, False, False, False, True]
State prediction error at timestep 3130 is tensor(2.0912e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3130 of -1
Current timestep = 3131. State = [[-0.19841027  0.19142386]]. Action = [[ 0.22045314 -0.1836886   0.04841024  0.5298253 ]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 3131 is [True, False, False, False, False, True]
State prediction error at timestep 3131 is tensor(7.6177e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3132. State = [[-0.19828427  0.1917853 ]]. Action = [[-0.1058031  -0.05274037  0.15293223 -0.8394227 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 3132 is [True, False, False, False, False, True]
State prediction error at timestep 3132 is tensor(3.3820e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3133. State = [[-0.19831765  0.1916907 ]]. Action = [[-0.05158807  0.14389107  0.03978941  0.12466109]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 3133 is [True, False, False, False, False, True]
State prediction error at timestep 3133 is tensor(2.9523e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3134. State = [[-0.1982463   0.19187762]]. Action = [[ 0.13857591 -0.14541557 -0.11296898  0.28769398]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 3134 is [True, False, False, False, False, True]
State prediction error at timestep 3134 is tensor(9.5957e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3135. State = [[-0.19829321  0.19200967]]. Action = [[-0.10745114 -0.19720659  0.11948189  0.14399958]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 3135 is [True, False, False, False, False, True]
State prediction error at timestep 3135 is tensor(5.1177e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3136. State = [[-0.19839507  0.19193254]]. Action = [[-0.10864383  0.17631692  0.0586673  -0.5441178 ]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 3136 is [True, False, False, False, False, True]
State prediction error at timestep 3136 is tensor(4.9545e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3137. State = [[-0.19832487  0.1919786 ]]. Action = [[ 0.15899277 -0.12029496  0.22605205 -0.5505623 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 3137 is [True, False, False, False, False, True]
State prediction error at timestep 3137 is tensor(6.9640e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3137 of -1
Current timestep = 3138. State = [[-0.19831671  0.19198257]]. Action = [[-0.07142934  0.13985395  0.24567935 -0.30092323]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 3138 is [True, False, False, False, False, True]
State prediction error at timestep 3138 is tensor(2.0944e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3139. State = [[-0.19837141  0.19195981]]. Action = [[-0.02466272  0.21421316  0.13530743 -0.49139804]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 3139 is [True, False, False, False, False, True]
State prediction error at timestep 3139 is tensor(7.1133e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3140. State = [[-0.19831671  0.19198257]]. Action = [[-0.19512564 -0.17997843  0.1703502   0.38058865]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 3140 is [True, False, False, False, False, True]
State prediction error at timestep 3140 is tensor(1.0505e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3141. State = [[-0.19831671  0.19198257]]. Action = [[-0.05023475 -0.23361759  0.23096842 -0.7673242 ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 3141 is [True, False, False, False, False, True]
State prediction error at timestep 3141 is tensor(1.4451e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3142. State = [[-0.19837141  0.19195981]]. Action = [[ 0.11644018  0.23358577 -0.20930675 -0.77418995]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 3142 is [True, False, False, False, False, True]
State prediction error at timestep 3142 is tensor(1.4606e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3142 of -1
Current timestep = 3143. State = [[-0.19826318  0.19185285]]. Action = [[ 0.10989654 -0.07305214  0.20268959 -0.7445401 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 3143 is [True, False, False, False, False, True]
State prediction error at timestep 3143 is tensor(1.8777e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3144. State = [[-0.19787832  0.19144098]]. Action = [[-0.04246588  0.1876879   0.02176842 -0.19434959]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 3144 is [True, False, False, False, False, True]
State prediction error at timestep 3144 is tensor(1.3748e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3145. State = [[-0.19755764  0.19098552]]. Action = [[-0.14652805 -0.08548866  0.23194993 -0.61421144]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 3145 is [True, False, False, False, False, True]
State prediction error at timestep 3145 is tensor(5.0166e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3146. State = [[-0.19739594  0.19061968]]. Action = [[ 0.09678748 -0.16584213  0.22052407 -0.22996593]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 3146 is [True, False, False, False, False, True]
State prediction error at timestep 3146 is tensor(4.7920e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3147. State = [[-0.19681889  0.18979786]]. Action = [[ 0.10154939 -0.11423105  0.03391054 -0.6360076 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 3147 is [True, False, False, False, False, True]
State prediction error at timestep 3147 is tensor(3.2911e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3147 of -1
Current timestep = 3148. State = [[-0.19601163  0.18880042]]. Action = [[ 0.16033182 -0.16753009  0.17573088 -0.9156566 ]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 3148 is [True, False, False, False, False, True]
State prediction error at timestep 3148 is tensor(4.3768e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3149. State = [[-0.1950345   0.18777601]]. Action = [[-0.085163    0.21783939  0.13010478 -0.5756166 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 3149 is [True, False, False, False, False, True]
State prediction error at timestep 3149 is tensor(1.8587e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3150. State = [[-0.19310652  0.18513806]]. Action = [[ 0.0016     -0.07889277  0.2477592   0.12021244]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 3150 is [True, False, False, False, False, True]
State prediction error at timestep 3150 is tensor(1.9977e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3150 of -1
Current timestep = 3151. State = [[-0.19266729  0.18409713]]. Action = [[ 0.19988418  0.08077773 -0.17295386  0.8874649 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 3151 is [True, False, False, False, False, True]
State prediction error at timestep 3151 is tensor(1.9547e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3152. State = [[-0.19202809  0.18267559]]. Action = [[ 0.02088499 -0.13699314  0.13416892 -0.25006533]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 3152 is [True, False, False, False, False, True]
State prediction error at timestep 3152 is tensor(1.0215e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3152 of -1
Current timestep = 3153. State = [[-0.19157436  0.1814495 ]]. Action = [[ 0.05516022  0.20940709  0.14581463 -0.7909935 ]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 3153 is [True, False, False, False, False, True]
State prediction error at timestep 3153 is tensor(1.2396e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3154. State = [[-0.19140232  0.18062016]]. Action = [[ 0.1400452   0.08978742 -0.15992416  0.6716243 ]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 3154 is [True, False, False, False, False, True]
State prediction error at timestep 3154 is tensor(3.1072e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3155. State = [[-0.1908252   0.17954573]]. Action = [[0.09785229 0.19605875 0.10386044 0.60805583]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 3155 is [True, False, False, False, False, True]
State prediction error at timestep 3155 is tensor(7.1752e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3155 of 1
Current timestep = 3156. State = [[-0.19055611  0.17892505]]. Action = [[-0.23320888 -0.13422994  0.01013121 -0.6778251 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 3156 is [True, False, False, False, False, True]
State prediction error at timestep 3156 is tensor(1.3361e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3157. State = [[-0.19031458  0.17854005]]. Action = [[0.20385984 0.14134753 0.05503005 0.2866714 ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 3157 is [True, False, False, False, False, True]
State prediction error at timestep 3157 is tensor(4.0486e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3158. State = [[-0.1901461   0.17814524]]. Action = [[ 0.09074295 -0.21280974  0.10100263 -0.9431852 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 3158 is [True, False, False, False, False, True]
State prediction error at timestep 3158 is tensor(7.3086e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3159. State = [[-0.18951319  0.17720442]]. Action = [[-0.10517415  0.01528892 -0.03192532  0.41677022]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 3159 is [True, False, False, False, False, True]
State prediction error at timestep 3159 is tensor(7.7333e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3160. State = [[-0.18948878  0.17736971]]. Action = [[-0.19131882 -0.1786516   0.11243519 -0.82394785]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 3160 is [True, False, False, False, False, True]
State prediction error at timestep 3160 is tensor(2.0405e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3160 of 1
Current timestep = 3161. State = [[-0.18966457  0.17757297]]. Action = [[-0.13383538  0.08063334  0.16868359 -0.6646762 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 3161 is [True, False, False, False, False, True]
State prediction error at timestep 3161 is tensor(4.9364e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3162. State = [[-0.18969846  0.17762658]]. Action = [[ 0.15271604 -0.02518633 -0.05568787 -0.7406731 ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 3162 is [True, False, False, False, False, True]
State prediction error at timestep 3162 is tensor(3.9833e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3163. State = [[-0.189631    0.17751986]]. Action = [[-0.20443341 -0.12499249 -0.14023615 -0.6724673 ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 3163 is [True, False, False, False, False, True]
State prediction error at timestep 3163 is tensor(4.6082e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3164. State = [[-0.18961431  0.1774535 ]]. Action = [[ 0.08502281  0.15325612 -0.22551277 -0.04993075]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 3164 is [True, False, False, False, False, True]
State prediction error at timestep 3164 is tensor(2.1037e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3165. State = [[-0.189631    0.17751986]]. Action = [[ 0.14644772 -0.19731234  0.18225962 -0.4448179 ]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 3165 is [True, False, False, False, False, True]
State prediction error at timestep 3165 is tensor(1.5470e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3165 of 1
Current timestep = 3166. State = [[-0.18946661  0.17730671]]. Action = [[ 0.11042908 -0.09386742  0.20975757 -0.02114218]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 3166 is [True, False, False, False, False, True]
State prediction error at timestep 3166 is tensor(7.3822e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3166 of 1
Current timestep = 3167. State = [[-0.18797828  0.17473027]]. Action = [[-0.03599533 -0.02912867  0.12794381 -0.45369136]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 3167 is [True, False, False, False, False, True]
State prediction error at timestep 3167 is tensor(5.3117e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3167 of 1
Current timestep = 3168. State = [[-0.1873129   0.17286937]]. Action = [[-0.12699859 -0.11294276 -0.15069288  0.30935383]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 3168 is [True, False, False, False, False, True]
State prediction error at timestep 3168 is tensor(3.5448e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3168 of 1
Current timestep = 3169. State = [[-0.18733944  0.1716956 ]]. Action = [[-0.20088457 -0.10396862 -0.07275613 -0.458906  ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 3169 is [True, False, False, False, False, True]
State prediction error at timestep 3169 is tensor(1.6551e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3170. State = [[-0.18748136  0.17045957]]. Action = [[-0.04354526 -0.23342448  0.1340673   0.54002166]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 3170 is [True, False, False, False, False, True]
State prediction error at timestep 3170 is tensor(4.6993e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3171. State = [[-0.18765253  0.16739947]]. Action = [[ 0.05087972  0.04917449 -0.15108088  0.17441928]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 3171 is [True, False, False, False, False, True]
State prediction error at timestep 3171 is tensor(3.4129e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3171 of 1
Current timestep = 3172. State = [[-0.18747234  0.16719526]]. Action = [[ 0.03511798 -0.19542748 -0.13696107  0.10271025]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 3172 is [True, False, False, False, False, True]
State prediction error at timestep 3172 is tensor(9.1351e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3173. State = [[-0.1875891  0.1672717]]. Action = [[-0.2070465  -0.10995418 -0.2119769  -0.10344428]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 3173 is [True, False, False, False, False, True]
State prediction error at timestep 3173 is tensor(1.1217e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3173 of 1
Current timestep = 3174. State = [[-0.18773492  0.16663836]]. Action = [[-0.2202698   0.08111218  0.22125235  0.5812118 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 3174 is [True, False, False, False, False, True]
State prediction error at timestep 3174 is tensor(1.4062e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3175. State = [[-0.1875082   0.16631134]]. Action = [[ 0.21108288 -0.18427195 -0.18761384 -0.6947039 ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 3175 is [True, False, False, False, False, True]
State prediction error at timestep 3175 is tensor(9.0168e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3176. State = [[-0.18722765  0.16588636]]. Action = [[-0.04104039 -0.19819711 -0.15369302 -0.24460447]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 3176 is [True, False, False, False, False, True]
State prediction error at timestep 3176 is tensor(5.2335e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3177. State = [[-0.18740743  0.16570209]]. Action = [[ 0.18990794  0.1268403   0.21050942 -0.37176818]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 3177 is [True, False, False, False, False, True]
State prediction error at timestep 3177 is tensor(2.3461e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3178. State = [[-0.18743262  0.165599  ]]. Action = [[-0.23126063  0.02074432 -0.0557456  -0.7374474 ]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 3178 is [True, False, False, False, False, True]
State prediction error at timestep 3178 is tensor(3.3389e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3178 of 1
Current timestep = 3179. State = [[-0.18727876  0.16522846]]. Action = [[-0.14268886  0.0801737  -0.02192469 -0.04643238]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 3179 is [True, False, False, False, False, True]
State prediction error at timestep 3179 is tensor(1.5436e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3180. State = [[-0.18721916  0.16454941]]. Action = [[-0.08853629 -0.10839102 -0.2315632  -0.48639864]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 3180 is [True, False, False, False, False, True]
State prediction error at timestep 3180 is tensor(1.3739e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3180 of 1
Current timestep = 3181. State = [[-0.18760486  0.16301416]]. Action = [[0.15363622 0.2117314  0.14399683 0.61466384]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 3181 is [True, False, False, False, False, True]
State prediction error at timestep 3181 is tensor(2.9993e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3182. State = [[-0.18845087  0.16135447]]. Action = [[ 0.0288637   0.08408403  0.16885677 -0.43125874]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 3182 is [True, False, False, False, False, True]
State prediction error at timestep 3182 is tensor(7.3803e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3182 of 1
Current timestep = 3183. State = [[-0.18845941  0.161726  ]]. Action = [[ 0.10920763  0.19903654 -0.02710025 -0.5129172 ]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 3183 is [True, False, False, False, False, True]
State prediction error at timestep 3183 is tensor(7.1083e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3184. State = [[-0.18853709  0.16216421]]. Action = [[ 0.21523845 -0.0290911   0.17923695  0.5995929 ]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 3184 is [True, False, False, False, False, True]
State prediction error at timestep 3184 is tensor(3.5184e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3185. State = [[-0.18870784  0.16230345]]. Action = [[0.01556146 0.17412648 0.22772586 0.1784668 ]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 3185 is [True, False, False, False, False, True]
State prediction error at timestep 3185 is tensor(5.9037e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3185 of 1
Current timestep = 3186. State = [[-0.1889765  0.1623914]]. Action = [[-0.00091068 -0.00135097 -0.17264366  0.75232434]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 3186 is [True, False, False, False, False, True]
State prediction error at timestep 3186 is tensor(4.6872e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3186 of 1
Current timestep = 3187. State = [[-0.18887037  0.16236645]]. Action = [[-0.13154985  0.19850925  0.08966267  0.47911274]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 3187 is [True, False, False, False, False, True]
State prediction error at timestep 3187 is tensor(8.3176e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3188. State = [[-0.18887037  0.16236645]]. Action = [[ 0.03135806 -0.01051491  0.09186348 -0.02346313]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 3188 is [True, False, False, False, False, True]
State prediction error at timestep 3188 is tensor(2.6953e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3189. State = [[-0.18891056  0.16227858]]. Action = [[-0.2234049  -0.18716691 -0.07332306 -0.64328635]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 3189 is [True, False, False, False, False, True]
State prediction error at timestep 3189 is tensor(8.8273e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3190. State = [[-0.18883123  0.16210485]]. Action = [[ 0.16272229  0.10525799  0.0566099  -0.65370524]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 3190 is [True, False, False, False, False, True]
State prediction error at timestep 3190 is tensor(8.6454e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3191. State = [[-0.18877596  0.16212587]]. Action = [[-0.05461715  0.17714614 -0.12002927 -0.6778948 ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 3191 is [True, False, False, False, False, True]
State prediction error at timestep 3191 is tensor(9.7690e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3192. State = [[-0.18874504  0.1619718 ]]. Action = [[ 0.05234724 -0.05150503 -0.11912277  0.77704775]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 3192 is [True, False, False, False, False, True]
State prediction error at timestep 3192 is tensor(6.8044e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3192 of 1
Current timestep = 3193. State = [[-0.18846983  0.16160518]]. Action = [[-0.16381602  0.01935026  0.2142995   0.88552785]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 3193 is [True, False, False, False, False, True]
State prediction error at timestep 3193 is tensor(1.4560e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3194. State = [[-0.18830971  0.16134754]]. Action = [[-0.2176859  -0.22717007  0.1217171   0.3389294 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 3194 is [True, False, False, False, False, True]
State prediction error at timestep 3194 is tensor(1.2952e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3195. State = [[-0.18802834  0.16062568]]. Action = [[-0.00898749  0.06711128 -0.00291865  0.11671376]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 3195 is [True, False, False, False, False, True]
State prediction error at timestep 3195 is tensor(6.6397e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3195 of 1
Current timestep = 3196. State = [[-0.18812527  0.16064   ]]. Action = [[-0.21281962 -0.09942991 -0.23068197  0.79059124]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 3196 is [True, False, False, False, False, True]
State prediction error at timestep 3196 is tensor(3.1305e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3196 of 1
Current timestep = 3197. State = [[-0.18829682  0.1612002 ]]. Action = [[ 0.09684193  0.18948492 -0.07626425 -0.16672337]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 3197 is [True, False, False, False, False, True]
State prediction error at timestep 3197 is tensor(1.1690e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3198. State = [[-0.18822502  0.16099462]]. Action = [[-0.00128113  0.11417621  0.18677622  0.90677047]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 3198 is [True, False, False, False, False, True]
State prediction error at timestep 3198 is tensor(2.3946e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3199. State = [[-0.18877381  0.1620151 ]]. Action = [[ 0.01880848  0.15846813 -0.23815712 -0.4491012 ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 3199 is [True, False, False, False, False, True]
State prediction error at timestep 3199 is tensor(3.0171e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3200. State = [[-0.1891754  0.1628392]]. Action = [[ 0.1447193   0.13178372  0.17929417 -0.69384944]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 3200 is [True, False, False, False, False, True]
State prediction error at timestep 3200 is tensor(5.4405e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3201. State = [[-0.18964987  0.16410436]]. Action = [[-0.00071302  0.05829495 -0.10417685 -0.29617602]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 3201 is [True, False, False, False, False, True]
State prediction error at timestep 3201 is tensor(7.1062e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3202. State = [[-0.19010039  0.1648926 ]]. Action = [[-0.14071426 -0.17624481 -0.02174439 -0.09879625]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 3202 is [True, False, False, False, False, True]
State prediction error at timestep 3202 is tensor(3.3062e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3203. State = [[-0.1907086  0.1664223]]. Action = [[ 0.00411287 -0.00713864 -0.19964498 -0.80604506]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 3203 is [True, False, False, False, False, True]
State prediction error at timestep 3203 is tensor(3.5832e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3204. State = [[-0.19080497  0.16666229]]. Action = [[-0.09169057  0.14439559 -0.1511387  -0.89670265]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 3204 is [True, False, False, False, False, True]
State prediction error at timestep 3204 is tensor(8.8293e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3205. State = [[-0.19089252  0.16669387]]. Action = [[ 0.05700925  0.147448    0.01612157 -0.75678664]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 3205 is [True, False, False, False, False, True]
State prediction error at timestep 3205 is tensor(9.8787e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3206. State = [[-0.19119358  0.1674514 ]]. Action = [[ 0.00948903  0.07186389 -0.11203825 -0.42767614]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 3206 is [True, False, False, False, False, True]
State prediction error at timestep 3206 is tensor(9.6458e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3207. State = [[-0.19141856  0.1680726 ]]. Action = [[ 0.11875671  0.2153014  -0.02176417 -0.872945  ]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 3207 is [True, False, False, False, False, True]
State prediction error at timestep 3207 is tensor(2.4093e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3208. State = [[-0.19166267  0.16859171]]. Action = [[ 0.08063197  0.22797805 -0.03979883 -0.7999584 ]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 3208 is [True, False, False, False, False, True]
State prediction error at timestep 3208 is tensor(1.8986e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3209. State = [[-0.19190685  0.16910969]]. Action = [[-0.01850352  0.1724174   0.1463474   0.76411974]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 3209 is [True, False, False, False, False, True]
State prediction error at timestep 3209 is tensor(2.3747e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3210. State = [[-0.19211975  0.16949457]]. Action = [[ 0.17616111  0.10298958  0.01343921 -0.16624403]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 3210 is [True, False, False, False, False, True]
State prediction error at timestep 3210 is tensor(2.1135e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3211. State = [[-0.19230013  0.16982648]]. Action = [[-0.01389794 -0.18164007 -0.11055252 -0.22010827]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 3211 is [True, False, False, False, False, True]
State prediction error at timestep 3211 is tensor(3.1126e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3212. State = [[-0.19247541  0.17010137]]. Action = [[-0.0124318  -0.18115397 -0.2034576   0.79442286]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 3212 is [True, False, False, False, False, True]
State prediction error at timestep 3212 is tensor(3.5530e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3213. State = [[-0.19257662  0.17039421]]. Action = [[-0.04200739  0.18917608  0.08675224 -0.75284195]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 3213 is [True, False, False, False, False, True]
State prediction error at timestep 3213 is tensor(4.6333e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3214. State = [[-0.19283895  0.17098708]]. Action = [[ 0.04878837 -0.07610381 -0.09803128 -0.2647369 ]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 3214 is [True, False, False, False, False, True]
State prediction error at timestep 3214 is tensor(4.5269e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3214 of -1
Current timestep = 3215. State = [[-0.19272788  0.17081542]]. Action = [[-0.12552752  0.143264   -0.14033681 -0.8716631 ]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 3215 is [True, False, False, False, False, True]
State prediction error at timestep 3215 is tensor(1.2603e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3216. State = [[-0.19249028  0.17078857]]. Action = [[-0.20132744 -0.11560002 -0.22604342  0.4290675 ]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 3216 is [True, False, False, False, False, True]
State prediction error at timestep 3216 is tensor(1.9300e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3217. State = [[-0.19239211  0.17089978]]. Action = [[-0.08333144  0.04549623  0.2038393   0.6943933 ]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 3217 is [True, False, False, False, False, True]
State prediction error at timestep 3217 is tensor(3.4973e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3217 of -1
Current timestep = 3218. State = [[-0.19269446  0.17131586]]. Action = [[-0.03211325  0.07481959 -0.0679979   0.12223232]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 3218 is [True, False, False, False, False, True]
State prediction error at timestep 3218 is tensor(6.8987e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3218 of -1
Current timestep = 3219. State = [[-0.19318984  0.1721371 ]]. Action = [[ 0.06154099 -0.17076212  0.03063911 -0.17470694]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 3219 is [True, False, False, False, False, True]
State prediction error at timestep 3219 is tensor(9.9296e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3220. State = [[-0.19346428  0.1727593 ]]. Action = [[ 0.037709    0.19749409 -0.13218045 -0.9605809 ]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 3220 is [True, False, False, False, False, True]
State prediction error at timestep 3220 is tensor(8.7861e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3221. State = [[-0.19365422  0.17300256]]. Action = [[-0.04339813  0.22345066 -0.01724386 -0.8376882 ]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 3221 is [True, False, False, False, False, True]
State prediction error at timestep 3221 is tensor(1.1907e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3222. State = [[-0.19392826  0.17321691]]. Action = [[ 0.09550422 -0.18078007  0.23474407 -0.34762836]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 3222 is [True, False, False, False, False, True]
State prediction error at timestep 3222 is tensor(8.8803e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3222 of -1
Current timestep = 3223. State = [[-0.194328    0.17413828]]. Action = [[ 0.1527937   0.163585   -0.20125982 -0.77247226]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 3223 is [True, False, False, False, False, True]
State prediction error at timestep 3223 is tensor(3.5999e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3224. State = [[-0.19515735  0.1754011 ]]. Action = [[ 0.11056966  0.02553397  0.19482386 -0.7526162 ]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 3224 is [True, False, False, False, False, True]
State prediction error at timestep 3224 is tensor(1.2021e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3225. State = [[-0.19460586  0.17558622]]. Action = [[ 0.05887043 -0.12631376  0.03704849  0.2662295 ]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 3225 is [True, False, False, False, False, True]
State prediction error at timestep 3225 is tensor(9.3540e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3226. State = [[-0.19265245  0.17407916]]. Action = [[ 0.08536854  0.03744632  0.00258836 -0.5435089 ]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 3226 is [True, False, False, False, False, True]
State prediction error at timestep 3226 is tensor(3.6323e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3227. State = [[-0.1917019   0.17418414]]. Action = [[-0.24879022 -0.22722416  0.095458   -0.14382821]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 3227 is [True, False, False, False, False, True]
State prediction error at timestep 3227 is tensor(1.6682e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3228. State = [[-0.18887286  0.17477536]]. Action = [[-0.05204369  0.00937772 -0.14599793  0.07719874]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 3228 is [True, False, False, False, False, True]
State prediction error at timestep 3228 is tensor(1.6769e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3228 of -1
Current timestep = 3229. State = [[-0.18853298  0.17478636]]. Action = [[ 0.20171225  0.05165729  0.11670408 -0.64472526]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 3229 is [True, False, False, False, False, True]
State prediction error at timestep 3229 is tensor(4.9580e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3230. State = [[-0.18840258  0.17490917]]. Action = [[-0.20063618 -0.23066115  0.02897835  0.47602534]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 3230 is [True, False, False, False, False, True]
State prediction error at timestep 3230 is tensor(2.5750e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3230 of -1
Current timestep = 3231. State = [[-0.1882999   0.17518404]]. Action = [[-0.03735566 -0.23894218 -0.2223679   0.9638345 ]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 3231 is [True, False, False, False, False, True]
State prediction error at timestep 3231 is tensor(7.6375e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3232. State = [[-0.18822165  0.17525998]]. Action = [[-0.15982628  0.1546064   0.05346972  0.3189336 ]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 3232 is [True, False, False, False, False, True]
State prediction error at timestep 3232 is tensor(1.7605e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3233. State = [[-0.18812528  0.1751961 ]]. Action = [[ 0.11501008 -0.1813251   0.16409773  0.8256204 ]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 3233 is [True, False, False, False, False, True]
State prediction error at timestep 3233 is tensor(2.1138e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3234. State = [[-0.18789722  0.17530379]]. Action = [[-0.1283982  -0.10557151  0.10171565  0.25363743]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 3234 is [True, False, False, False, False, True]
State prediction error at timestep 3234 is tensor(4.2263e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3234 of -1
Current timestep = 3235. State = [[-0.18774605  0.17460628]]. Action = [[0.20923847 0.13095382 0.0411666  0.02111328]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 3235 is [True, False, False, False, False, True]
State prediction error at timestep 3235 is tensor(1.0774e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3236. State = [[-0.18773805  0.17416218]]. Action = [[ 0.22206527  0.19425923 -0.1630323   0.7422323 ]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 3236 is [True, False, False, False, False, True]
State prediction error at timestep 3236 is tensor(3.8058e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3237. State = [[-0.18758965  0.1740015 ]]. Action = [[ 0.00785482 -0.19515103 -0.11846325 -0.8983964 ]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 3237 is [True, False, False, False, False, True]
State prediction error at timestep 3237 is tensor(6.4300e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3238. State = [[-0.1874907  0.1738891]]. Action = [[ 0.1355946   0.14321092 -0.1880936   0.23723757]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 3238 is [True, False, False, False, False, True]
State prediction error at timestep 3238 is tensor(9.1708e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3238 of -1
Current timestep = 3239. State = [[-0.18767278  0.17365547]]. Action = [[-0.1709674   0.17385411  0.17940187  0.31362772]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 3239 is [True, False, False, False, False, True]
State prediction error at timestep 3239 is tensor(2.5612e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3240. State = [[-0.18742453  0.17326291]]. Action = [[ 0.23142686 -0.1516958  -0.003506    0.6368027 ]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 3240 is [True, False, False, False, False, True]
State prediction error at timestep 3240 is tensor(1.8231e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3241. State = [[-0.18737464  0.17314224]]. Action = [[ 0.1587066  -0.1748558   0.19738829 -0.36260676]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 3241 is [True, False, False, False, False, True]
State prediction error at timestep 3241 is tensor(4.7149e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3242. State = [[-0.18730123  0.17271642]]. Action = [[ 0.05361095 -0.11789322 -0.07365394  0.2563753 ]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 3242 is [True, False, False, False, False, True]
State prediction error at timestep 3242 is tensor(3.1176e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3242 of -1
Current timestep = 3243. State = [[-0.18634647  0.17107484]]. Action = [[ 0.20582783 -0.12048849  0.2229712   0.38739634]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 3243 is [True, False, False, False, False, True]
State prediction error at timestep 3243 is tensor(1.2221e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3244. State = [[-0.18601127  0.1702094 ]]. Action = [[ 0.00331396  0.15448284  0.09249759 -0.29180336]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 3244 is [True, False, False, False, False, True]
State prediction error at timestep 3244 is tensor(1.8010e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3245. State = [[-0.18591116  0.16975503]]. Action = [[ 0.04716557 -0.17058721  0.24889386  0.00773585]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 3245 is [True, False, False, False, False, True]
State prediction error at timestep 3245 is tensor(8.4095e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3245 of -1
Current timestep = 3246. State = [[-0.18578982  0.16930558]]. Action = [[ 0.07108971 -0.24248713 -0.13656192 -0.38236463]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 3246 is [True, False, False, False, False, True]
State prediction error at timestep 3246 is tensor(4.0070e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3247. State = [[-0.18555798  0.16845357]]. Action = [[-0.04497585  0.22150365 -0.04027244 -0.45322382]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 3247 is [True, False, False, False, False, True]
State prediction error at timestep 3247 is tensor(1.4512e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3248. State = [[-0.185042    0.16640396]]. Action = [[ 0.0033116  -0.05958021 -0.10762858  0.83660626]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 3248 is [True, False, False, False, False, True]
State prediction error at timestep 3248 is tensor(1.9344e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3248 of -1
Current timestep = 3249. State = [[-0.18475433  0.1655464 ]]. Action = [[ 0.15836823 -0.1410885  -0.22677344 -0.6902802 ]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 3249 is [True, False, False, False, False, True]
State prediction error at timestep 3249 is tensor(5.5722e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3250. State = [[-0.18447374  0.1648392 ]]. Action = [[ 0.1478988   0.01205498 -0.02003202  0.1316762 ]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 3250 is [True, False, False, False, False, True]
State prediction error at timestep 3250 is tensor(2.3548e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3251. State = [[-0.18428653  0.16411185]]. Action = [[ 0.17800885  0.09276116 -0.19790469  0.4689219 ]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 3251 is [True, False, False, False, False, True]
State prediction error at timestep 3251 is tensor(1.3870e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3251 of 1
Current timestep = 3252. State = [[-0.18413217  0.16298564]]. Action = [[-0.1358401   0.16762868  0.06831563  0.28056931]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 3252 is [True, False, False, False, False, True]
State prediction error at timestep 3252 is tensor(9.7631e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3253. State = [[-0.18389282  0.16229276]]. Action = [[ 0.12994218 -0.16226529  0.0055609   0.1756612 ]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 3253 is [True, False, False, False, False, True]
State prediction error at timestep 3253 is tensor(9.8023e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3254. State = [[-0.1837879   0.16181539]]. Action = [[ 0.21707246  0.1847052  -0.13710904 -0.43546557]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 3254 is [True, False, False, False, False, True]
State prediction error at timestep 3254 is tensor(9.1112e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3255. State = [[-0.18366717  0.16128942]]. Action = [[ 0.05028194 -0.16939239  0.14304155 -0.79401994]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 3255 is [True, False, False, False, False, True]
State prediction error at timestep 3255 is tensor(8.3462e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3255 of 1
Current timestep = 3256. State = [[-0.1836075   0.16061401]]. Action = [[-0.06880277  0.12310299  0.15547377 -0.64568096]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 3256 is [True, False, False, False, False, True]
State prediction error at timestep 3256 is tensor(2.9919e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3256 of 1
Current timestep = 3257. State = [[-0.18414286  0.16149494]]. Action = [[ 0.02535024  0.22530031  0.14580786 -0.4837408 ]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 3257 is [True, False, False, False, False, True]
State prediction error at timestep 3257 is tensor(6.5847e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3258. State = [[-0.18450786  0.16224241]]. Action = [[-0.21410489 -0.1965018  -0.01902378 -0.3232255 ]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 3258 is [True, False, False, False, False, True]
State prediction error at timestep 3258 is tensor(2.8693e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3259. State = [[-0.18479364  0.16272171]]. Action = [[ 0.14162284 -0.2477352   0.18519509  0.7230923 ]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 3259 is [True, False, False, False, False, True]
State prediction error at timestep 3259 is tensor(9.9027e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3260. State = [[-0.18553092  0.16365871]]. Action = [[-0.12762351 -0.01954719 -0.03189492  0.61663735]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 3260 is [True, False, False, False, False, True]
State prediction error at timestep 3260 is tensor(1.7132e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3260 of 1
Current timestep = 3261. State = [[-0.18729356  0.1644937 ]]. Action = [[-0.01052688  0.00892407  0.15675226 -0.25312734]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 3261 is [True, False, False, False, False, True]
State prediction error at timestep 3261 is tensor(3.5878e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3261 of 1
Current timestep = 3262. State = [[-0.1875521   0.16479631]]. Action = [[ 0.1798985  -0.01215483  0.11181983  0.73301935]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 3262 is [True, False, False, False, False, True]
State prediction error at timestep 3262 is tensor(7.3638e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3263. State = [[-0.18787535  0.1651303 ]]. Action = [[ 0.17703557 -0.10371386 -0.19492157  0.3565563 ]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 3263 is [True, False, False, False, False, True]
State prediction error at timestep 3263 is tensor(1.3379e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3264. State = [[-0.18801752  0.16528657]]. Action = [[-0.19910611 -0.04011895 -0.20017819  0.8513737 ]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 3264 is [True, False, False, False, False, True]
State prediction error at timestep 3264 is tensor(3.2514e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3265. State = [[-0.18868925  0.16580817]]. Action = [[ 0.00569758 -0.04527915  0.21453515 -0.44543993]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 3265 is [True, False, False, False, False, True]
State prediction error at timestep 3265 is tensor(2.0026e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3265 of 1
Current timestep = 3266. State = [[-0.18866445  0.16553211]]. Action = [[-0.12088034  0.14035416  0.17434251 -0.6639405 ]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 3266 is [True, False, False, False, False, True]
State prediction error at timestep 3266 is tensor(3.8407e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3267. State = [[-0.18879981  0.1652441 ]]. Action = [[ 0.23273173 -0.16659908  0.11382613 -0.03372622]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 3267 is [True, False, False, False, False, True]
State prediction error at timestep 3267 is tensor(5.6549e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3268. State = [[-0.18884803  0.16536453]]. Action = [[-0.24161209 -0.0854333  -0.08903107  0.7590647 ]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 3268 is [True, False, False, False, False, True]
State prediction error at timestep 3268 is tensor(5.0247e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3268 of 1
Current timestep = 3269. State = [[-0.18889634  0.16541308]]. Action = [[-0.00295751  0.2196747   0.00882447  0.679245  ]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 3269 is [True, False, False, False, False, True]
State prediction error at timestep 3269 is tensor(1.2042e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3270. State = [[-0.18892065  0.1652486 ]]. Action = [[ 0.1615017   0.2215333  -0.02231809 -0.26398635]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 3270 is [True, False, False, False, False, True]
State prediction error at timestep 3270 is tensor(2.7349e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3271. State = [[-0.18915279  0.16545764]]. Action = [[-0.01423331  0.12989125  0.15231061  0.4054811 ]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 3271 is [True, False, False, False, False, True]
State prediction error at timestep 3271 is tensor(3.9903e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3271 of 1
Current timestep = 3272. State = [[-0.18959032  0.16639991]]. Action = [[-0.24256212  0.20977044  0.18770307 -0.5130227 ]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 3272 is [True, False, False, False, False, True]
State prediction error at timestep 3272 is tensor(2.1581e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3273. State = [[-0.19080532  0.16839436]]. Action = [[ 0.08377099 -0.10655662  0.1426996   0.8111062 ]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 3273 is [True, False, False, False, False, True]
State prediction error at timestep 3273 is tensor(7.6332e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3274. State = [[-0.19065365  0.16811757]]. Action = [[-0.21427877  0.12778866 -0.20404935 -0.02101684]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 3274 is [True, False, False, False, False, True]
State prediction error at timestep 3274 is tensor(3.2911e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3275. State = [[-0.19042687  0.16769664]]. Action = [[ 0.23807895  0.20918411  0.14709729 -0.3942331 ]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 3275 is [True, False, False, False, False, True]
State prediction error at timestep 3275 is tensor(8.1226e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3276. State = [[-0.19014397  0.16702901]]. Action = [[-0.18329982  0.19104552 -0.19337334 -0.62403214]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 3276 is [True, False, False, False, False, True]
State prediction error at timestep 3276 is tensor(6.8330e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3276 of -1
Current timestep = 3277. State = [[-0.19036946  0.16740705]]. Action = [[-0.21734738  0.18669349 -0.20356973 -0.7209622 ]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 3277 is [True, False, False, False, False, True]
State prediction error at timestep 3277 is tensor(7.8225e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3278. State = [[-0.19023602  0.16672464]]. Action = [[-0.08050218 -0.11420542 -0.04337905  0.9822259 ]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 3278 is [True, False, False, False, False, True]
State prediction error at timestep 3278 is tensor(1.4646e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3279. State = [[-0.19040889  0.16415925]]. Action = [[-0.08404487  0.04993954  0.09193778 -0.04713881]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 3279 is [True, False, False, False, False, True]
State prediction error at timestep 3279 is tensor(5.4821e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3279 of -1
Current timestep = 3280. State = [[-0.19203821  0.16366632]]. Action = [[-0.10482374 -0.06091295 -0.14384378 -0.76635057]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 3280 is [True, False, False, False, False, True]
State prediction error at timestep 3280 is tensor(5.7010e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3281. State = [[-0.19505161  0.16200174]]. Action = [[ 0.01779431  0.09944564 -0.11641884  0.7246628 ]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 3281 is [True, False, False, False, False, True]
State prediction error at timestep 3281 is tensor(2.4720e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3282. State = [[-0.19602886  0.1623188 ]]. Action = [[ 0.16793439 -0.13049756  0.18606701 -0.74368   ]]. Reward = [0.]
Curr episode timestep = 576
Scene graph at timestep 3282 is [True, False, False, False, False, True]
State prediction error at timestep 3282 is tensor(7.8655e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3283. State = [[-0.19802512  0.16326419]]. Action = [[-0.02187665  0.09389403 -0.05919355  0.43797934]]. Reward = [0.]
Curr episode timestep = 577
Scene graph at timestep 3283 is [True, False, False, False, False, True]
State prediction error at timestep 3283 is tensor(9.6873e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3283 of -1
Current timestep = 3284. State = [[-0.20111293  0.16798875]]. Action = [[ 7.5259805e-04  8.1165105e-02 -3.5612941e-02  7.6315546e-01]]. Reward = [0.]
Curr episode timestep = 578
Scene graph at timestep 3284 is [True, False, False, False, False, True]
State prediction error at timestep 3284 is tensor(1.2550e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3285. State = [[-0.20157261  0.16882089]]. Action = [[-0.05160396  0.15095139  0.22398067  0.80164933]]. Reward = [0.]
Curr episode timestep = 579
Scene graph at timestep 3285 is [True, False, False, False, False, True]
State prediction error at timestep 3285 is tensor(2.0798e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3286. State = [[-0.20188287  0.1694435 ]]. Action = [[-0.21915409  0.13700134  0.16324884 -0.8685675 ]]. Reward = [0.]
Curr episode timestep = 580
Scene graph at timestep 3286 is [True, False, False, False, False, True]
State prediction error at timestep 3286 is tensor(2.1655e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3287. State = [[-0.20232162  0.17033385]]. Action = [[ 0.00239626 -0.15381506  0.10421616  0.43002224]]. Reward = [0.]
Curr episode timestep = 581
Scene graph at timestep 3287 is [True, False, False, False, False, True]
State prediction error at timestep 3287 is tensor(1.2691e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3288. State = [[-0.20283699  0.17086013]]. Action = [[-0.13960771 -0.129575   -0.06056666 -0.95611537]]. Reward = [0.]
Curr episode timestep = 582
Scene graph at timestep 3288 is [True, False, False, False, False, True]
State prediction error at timestep 3288 is tensor(1.7351e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3289. State = [[-0.2032994   0.17158686]]. Action = [[ 0.01513949 -0.21305044 -0.2113214  -0.7575308 ]]. Reward = [0.]
Curr episode timestep = 583
Scene graph at timestep 3289 is [True, False, False, False, False, True]
State prediction error at timestep 3289 is tensor(1.5992e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3290. State = [[-0.20382874  0.17259192]]. Action = [[ 0.22450045 -0.06401658  0.17150211  0.19727015]]. Reward = [0.]
Curr episode timestep = 584
Scene graph at timestep 3290 is [True, False, False, False, False, True]
State prediction error at timestep 3290 is tensor(5.9561e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3290 of -1
Current timestep = 3291. State = [[-0.20494659  0.17414376]]. Action = [[-0.1218586   0.1268228   0.23856252  0.60782266]]. Reward = [0.]
Curr episode timestep = 585
Scene graph at timestep 3291 is [True, False, False, False, False, True]
State prediction error at timestep 3291 is tensor(1.4591e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3292. State = [[-0.20570248  0.17533691]]. Action = [[ 0.08377358  0.2047652  -0.10020654  0.93965554]]. Reward = [0.]
Curr episode timestep = 586
Scene graph at timestep 3292 is [True, False, False, False, False, True]
State prediction error at timestep 3292 is tensor(1.0839e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3293. State = [[-0.2066597   0.17701533]]. Action = [[ 0.22616047 -0.15262064  0.14187562 -0.94229347]]. Reward = [0.]
Curr episode timestep = 587
Scene graph at timestep 3293 is [True, False, False, False, False, True]
State prediction error at timestep 3293 is tensor(1.0800e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3294. State = [[-0.20745067  0.178228  ]]. Action = [[ 0.0246067  -0.20084262 -0.02727288 -0.70688266]]. Reward = [0.]
Curr episode timestep = 588
Scene graph at timestep 3294 is [True, False, False, False, False, True]
State prediction error at timestep 3294 is tensor(5.2485e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3295. State = [[-0.20828496  0.17913395]]. Action = [[-0.20805945 -0.20386258  0.22676855  0.7249255 ]]. Reward = [0.]
Curr episode timestep = 589
Scene graph at timestep 3295 is [True, False, False, False, False, True]
State prediction error at timestep 3295 is tensor(1.3800e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3296. State = [[-0.20922019  0.18075693]]. Action = [[ 0.18921971  0.19968957 -0.15229395 -0.46880198]]. Reward = [0.]
Curr episode timestep = 590
Scene graph at timestep 3296 is [True, False, False, False, False, True]
State prediction error at timestep 3296 is tensor(3.6175e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3296 of -1
Current timestep = 3297. State = [[-0.21000618  0.18179987]]. Action = [[-0.15178299  0.05194286 -0.13562939  0.8499923 ]]. Reward = [0.]
Curr episode timestep = 591
Scene graph at timestep 3297 is [True, False, False, False, False, True]
State prediction error at timestep 3297 is tensor(3.4272e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3298. State = [[-0.21140398  0.18355374]]. Action = [[ 0.12400115 -0.10240559  0.01419583 -0.32170606]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 3298 is [True, False, False, False, False, True]
State prediction error at timestep 3298 is tensor(1.4142e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3299. State = [[-0.21072824  0.1830315 ]]. Action = [[-0.02684593  0.20527297  0.21670961 -0.9793782 ]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 3299 is [True, False, False, False, False, True]
State prediction error at timestep 3299 is tensor(3.4711e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3300. State = [[-0.21036743  0.18259524]]. Action = [[-0.10612901 -0.22276555 -0.18978749  0.89099526]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 3300 is [True, False, False, False, False, True]
State prediction error at timestep 3300 is tensor(2.9358e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3301. State = [[-0.21011749  0.18163614]]. Action = [[-0.07705177 -0.05259615 -0.20695968 -0.74657184]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 3301 is [True, False, False, False, False, True]
State prediction error at timestep 3301 is tensor(3.0159e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3301 of -1
Current timestep = 3302. State = [[-0.21007611  0.1805833 ]]. Action = [[-0.12701227 -0.2086126   0.17741841 -0.935799  ]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 3302 is [True, False, False, False, False, True]
State prediction error at timestep 3302 is tensor(5.5601e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3303. State = [[-0.21023281  0.18009996]]. Action = [[ 0.23659545 -0.07241423  0.24451146 -0.46276933]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 3303 is [True, False, False, False, False, True]
State prediction error at timestep 3303 is tensor(3.5699e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3304. State = [[-0.21004215  0.17991258]]. Action = [[-0.19496538  0.12321752  0.03987223  0.8243331 ]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 3304 is [True, False, False, False, False, True]
State prediction error at timestep 3304 is tensor(5.3718e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3305. State = [[-0.20978455  0.17944345]]. Action = [[-0.16237171 -0.18680683 -0.16576     0.26695788]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 3305 is [True, False, False, False, False, True]
State prediction error at timestep 3305 is tensor(1.7233e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3305 of -1
Current timestep = 3306. State = [[-0.209564    0.17783967]]. Action = [[ 0.09774554  0.03838411  0.07971007 -0.00705254]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 3306 is [True, False, False, False, False, True]
State prediction error at timestep 3306 is tensor(1.6655e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3307. State = [[-0.20943107  0.17775102]]. Action = [[-0.1713256  -0.08715171 -0.0052235   0.9665978 ]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 3307 is [True, False, False, False, False, True]
State prediction error at timestep 3307 is tensor(1.7280e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3307 of -1
Current timestep = 3308. State = [[-0.2094652   0.17775103]]. Action = [[-0.070379    0.17446193  0.04225636 -0.6806206 ]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 3308 is [True, False, False, False, False, True]
State prediction error at timestep 3308 is tensor(1.3943e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3309. State = [[-0.20952635  0.17764258]]. Action = [[-0.09222619 -0.19279972  0.16058448  0.5519173 ]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 3309 is [True, False, False, False, False, True]
State prediction error at timestep 3309 is tensor(3.0680e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3310. State = [[-0.20941699  0.17759444]]. Action = [[ 0.1022321  -0.1902738  -0.19589695  0.84138584]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 3310 is [True, False, False, False, False, True]
State prediction error at timestep 3310 is tensor(1.2868e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3311. State = [[-0.20922092  0.17759885]]. Action = [[-0.23265557  0.23558563 -0.19761223 -0.1755805 ]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 3311 is [True, False, False, False, False, True]
State prediction error at timestep 3311 is tensor(8.0380e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3312. State = [[-0.20917858  0.17750905]]. Action = [[-0.23683524  0.2101934   0.10166582  0.07488835]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 3312 is [True, False, False, False, False, True]
State prediction error at timestep 3312 is tensor(4.2734e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3312 of -1
Current timestep = 3313. State = [[-0.20910479  0.17732497]]. Action = [[-0.04215157 -0.1732377   0.16268948 -0.35267276]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 3313 is [True, False, False, False, False, True]
State prediction error at timestep 3313 is tensor(8.8605e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3314. State = [[-0.20908578  0.1770365 ]]. Action = [[ 0.04724205  0.08381522 -0.16936742 -0.96885335]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 3314 is [True, False, False, False, False, True]
State prediction error at timestep 3314 is tensor(1.4690e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3314 of -1
Current timestep = 3315. State = [[-0.20922528  0.17737839]]. Action = [[0.13134575 0.16639131 0.00837651 0.6377604 ]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 3315 is [True, False, False, False, False, True]
State prediction error at timestep 3315 is tensor(4.6264e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3316. State = [[-0.20934233  0.1776933 ]]. Action = [[-0.19155443 -0.21734823 -0.04337382  0.741554  ]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 3316 is [True, False, False, False, False, True]
State prediction error at timestep 3316 is tensor(2.9259e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3317. State = [[-0.20925972  0.17809056]]. Action = [[-0.03805035  0.04720643 -0.06204247 -0.05417097]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 3317 is [True, False, False, False, False, True]
State prediction error at timestep 3317 is tensor(8.7970e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3317 of -1
Current timestep = 3318. State = [[-0.20962289  0.17875217]]. Action = [[-0.10512108 -0.23126917  0.11673355  0.91046834]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 3318 is [True, False, False, False, False, True]
State prediction error at timestep 3318 is tensor(6.3404e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3319. State = [[-0.2097745   0.17894322]]. Action = [[-0.10514267  0.14133841 -0.02350035  0.3520372 ]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 3319 is [True, False, False, False, False, True]
State prediction error at timestep 3319 is tensor(1.9214e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3320. State = [[-0.20990969  0.17914785]]. Action = [[0.09231478 0.17706615 0.09286624 0.5427151 ]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 3320 is [True, False, False, False, False, True]
State prediction error at timestep 3320 is tensor(1.9276e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3320 of -1
Current timestep = 3321. State = [[-0.21014689  0.17950645]]. Action = [[-0.02162184 -0.23431747  0.17548639  0.09494281]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 3321 is [True, False, False, False, False, True]
State prediction error at timestep 3321 is tensor(8.9413e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3322. State = [[-0.21026617  0.1797251 ]]. Action = [[ 0.18586016  0.09035712 -0.02692141  0.4355942 ]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 3322 is [True, False, False, False, False, True]
State prediction error at timestep 3322 is tensor(2.8791e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3323. State = [[-0.21035023  0.1798134 ]]. Action = [[-0.10886765  0.22753114 -0.22413978  0.38058233]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 3323 is [True, False, False, False, False, True]
State prediction error at timestep 3323 is tensor(2.0331e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3324. State = [[-0.21055375  0.18012023]]. Action = [[-0.16313231  0.07426327  0.20342201 -0.06330514]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 3324 is [True, False, False, False, False, True]
State prediction error at timestep 3324 is tensor(1.1173e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3324 of -1
Current timestep = 3325. State = [[-0.21072361  0.18037598]]. Action = [[0.1697934  0.157489   0.11165881 0.11699295]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 3325 is [True, False, False, False, False, True]
State prediction error at timestep 3325 is tensor(9.9052e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3326. State = [[-0.2107743   0.18050075]]. Action = [[-0.13700327 -0.13659593 -0.1475192  -0.30737293]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 3326 is [True, False, False, False, False, True]
State prediction error at timestep 3326 is tensor(1.1508e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3327. State = [[-0.21089402  0.18071917]]. Action = [[-0.22453588 -0.19792321 -0.23479064 -0.4910468 ]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 3327 is [True, False, False, False, False, True]
State prediction error at timestep 3327 is tensor(5.9495e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3328. State = [[-0.2110204   0.18103236]]. Action = [[-0.02337559 -0.00529222  0.01251796 -0.03545576]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 3328 is [True, False, False, False, False, True]
State prediction error at timestep 3328 is tensor(1.3842e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3328 of -1
Current timestep = 3329. State = [[-0.21108021  0.18114443]]. Action = [[0.1990928  0.21180362 0.02512965 0.73467875]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 3329 is [True, False, False, False, False, True]
State prediction error at timestep 3329 is tensor(8.3889e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3330. State = [[-0.21109632  0.18113023]]. Action = [[-0.19848552 -0.23358059 -0.05711147 -0.253779  ]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 3330 is [True, False, False, False, False, True]
State prediction error at timestep 3330 is tensor(1.5287e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3331. State = [[-0.21113059  0.18118162]]. Action = [[ 0.15859848 -0.13094358  0.00906369 -0.01092809]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 3331 is [True, False, False, False, False, True]
State prediction error at timestep 3331 is tensor(5.1973e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3332. State = [[-0.21113059  0.18118162]]. Action = [[-0.22539866  0.18611038  0.19308269 -0.32955897]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 3332 is [True, False, False, False, False, True]
State prediction error at timestep 3332 is tensor(4.6291e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3332 of -1
Current timestep = 3333. State = [[-0.21113059  0.18118162]]. Action = [[ 0.1984201  -0.16170205 -0.1378636  -0.6381837 ]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 3333 is [True, False, False, False, False, True]
State prediction error at timestep 3333 is tensor(1.0287e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3334. State = [[-0.21123049  0.18133141]]. Action = [[-0.09217279 -0.18095535  0.15231201 -0.29556334]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 3334 is [True, False, False, False, False, True]
State prediction error at timestep 3334 is tensor(5.4734e-09, grad_fn=<MseLossBackward0>)
Current timestep = 3335. State = [[-0.2112823   0.18144742]]. Action = [[-0.23855735 -0.0380815  -0.24711388  0.5923568 ]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 3335 is [True, False, False, False, False, True]
State prediction error at timestep 3335 is tensor(3.4578e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3336. State = [[-0.21126443  0.18138231]]. Action = [[-0.03951348 -0.13528115  0.09995568  0.14401603]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 3336 is [True, False, False, False, False, True]
State prediction error at timestep 3336 is tensor(6.4311e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3336 of -1
Current timestep = 3337. State = [[-0.2113327   0.18148457]]. Action = [[ 0.07306612 -0.18079469  0.21423918 -0.5912876 ]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 3337 is [True, False, False, False, False, True]
State prediction error at timestep 3337 is tensor(1.0854e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3338. State = [[-0.21134967  0.18155834]]. Action = [[ 0.13112554 -0.02371474  0.19338825 -0.78673434]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 3338 is [True, False, False, False, False, True]
State prediction error at timestep 3338 is tensor(3.5386e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3338 of -1
Current timestep = 3339. State = [[-0.21107388  0.18172361]]. Action = [[-0.0191083   0.22666782  0.18915474  0.69324636]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 3339 is [True, False, False, False, False, True]
State prediction error at timestep 3339 is tensor(9.2686e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3340. State = [[-0.21089718  0.18181701]]. Action = [[-0.23594074 -0.06654182 -0.1284035  -0.59049183]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 3340 is [True, False, False, False, False, True]
State prediction error at timestep 3340 is tensor(1.1592e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3341. State = [[-0.2107943  0.1818161]]. Action = [[-0.0398455  -0.17623456 -0.05874331 -0.2154553 ]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 3341 is [True, False, False, False, False, True]
State prediction error at timestep 3341 is tensor(1.0010e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3342. State = [[-0.21011882  0.18172647]]. Action = [[-0.02872105 -0.07442179  0.15259564 -0.8327449 ]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 3342 is [True, False, False, False, False, True]
State prediction error at timestep 3342 is tensor(2.6420e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3342 of -1
Current timestep = 3343. State = [[-0.20986809  0.18121634]]. Action = [[-0.17274638 -0.13168012 -0.04497135 -0.29684925]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 3343 is [True, False, False, False, False, True]
State prediction error at timestep 3343 is tensor(2.6322e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3344. State = [[-0.20981802  0.18081972]]. Action = [[ 0.20905697 -0.18177001  0.17626202 -0.8274891 ]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 3344 is [True, False, False, False, False, True]
State prediction error at timestep 3344 is tensor(1.5014e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3345. State = [[-0.20934215  0.17964858]]. Action = [[ 0.05996644  0.12903166  0.12774944 -0.10569769]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 3345 is [True, False, False, False, False, True]
State prediction error at timestep 3345 is tensor(1.4905e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3345 of -1
Current timestep = 3346. State = [[-0.20938471  0.18011492]]. Action = [[-0.01703991  0.21901578  0.05375987 -0.34374046]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 3346 is [True, False, False, False, False, True]
State prediction error at timestep 3346 is tensor(5.1667e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3347. State = [[-0.20948954  0.18084672]]. Action = [[ 0.184995    0.07865474  0.21745253 -0.18648177]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 3347 is [True, False, False, False, False, True]
State prediction error at timestep 3347 is tensor(6.3565e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3347 of -1
Current timestep = 3348. State = [[-0.20918727  0.18105239]]. Action = [[ 0.17255574  0.13462543  0.07206929 -0.53206056]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 3348 is [True, False, False, False, False, True]
State prediction error at timestep 3348 is tensor(6.5489e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3349. State = [[-0.20928074  0.18159938]]. Action = [[-0.07572664  0.03162652 -0.16674198  0.5457647 ]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 3349 is [True, False, False, False, False, True]
State prediction error at timestep 3349 is tensor(5.9776e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3350. State = [[-0.20964716  0.18234578]]. Action = [[ 0.10073557 -0.07602127  0.17465067 -0.46875656]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 3350 is [True, False, False, False, False, True]
State prediction error at timestep 3350 is tensor(1.1003e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3351. State = [[-0.20930453  0.18228899]]. Action = [[-0.19409378 -0.12648161 -0.054721    0.5188222 ]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 3351 is [True, False, False, False, False, True]
State prediction error at timestep 3351 is tensor(3.6356e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3352. State = [[-0.20863275  0.18216391]]. Action = [[-0.0558641  -0.11083519  0.02600721  0.56831634]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 3352 is [True, False, False, False, False, True]
State prediction error at timestep 3352 is tensor(8.8711e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3352 of -1
Current timestep = 3353. State = [[-0.20821434  0.18155839]]. Action = [[ 0.13657564  0.24148291 -0.16429417  0.09111261]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 3353 is [True, False, False, False, False, True]
State prediction error at timestep 3353 is tensor(8.4867e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3354. State = [[-0.2078009   0.18058313]]. Action = [[-0.06143671 -0.19301032 -0.06552631 -0.7472981 ]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 3354 is [True, False, False, False, False, True]
State prediction error at timestep 3354 is tensor(8.0834e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3355. State = [[-0.20754638  0.18021823]]. Action = [[ 0.17798346  0.0129115   0.06462002 -0.6502446 ]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 3355 is [True, False, False, False, False, True]
State prediction error at timestep 3355 is tensor(5.0364e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3355 of -1
Current timestep = 3356. State = [[-0.2075535  0.1800008]]. Action = [[-0.12962759  0.22226405  0.0905793  -0.41925597]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 3356 is [True, False, False, False, False, True]
State prediction error at timestep 3356 is tensor(6.4032e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3357. State = [[-0.20722932  0.17929499]]. Action = [[ 0.06967092  0.08355856  0.20045668 -0.677453  ]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 3357 is [True, False, False, False, False, True]
State prediction error at timestep 3357 is tensor(2.0439e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3357 of -1
Current timestep = 3358. State = [[-0.20697564  0.17979336]]. Action = [[ 0.01655656  0.05076638  0.186225   -0.15172791]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 3358 is [True, False, False, False, False, True]
State prediction error at timestep 3358 is tensor(9.4349e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3359. State = [[-0.20692311  0.18001704]]. Action = [[-0.13890511  0.23132563  0.22615892 -0.8307752 ]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 3359 is [True, False, False, False, False, True]
State prediction error at timestep 3359 is tensor(3.5106e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3360. State = [[-0.20691451  0.18024409]]. Action = [[ 0.00740296 -0.20300683 -0.151824    0.12034893]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 3360 is [True, False, False, False, False, True]
State prediction error at timestep 3360 is tensor(7.2900e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3361. State = [[-0.20698507  0.18026985]]. Action = [[-0.2125873  -0.09273365 -0.01538612  0.2998426 ]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 3361 is [True, False, False, False, False, True]
State prediction error at timestep 3361 is tensor(2.9201e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3362. State = [[-0.20672998  0.18044713]]. Action = [[ 0.04934767 -0.1616369  -0.19378975  0.3784877 ]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 3362 is [True, False, False, False, False, True]
State prediction error at timestep 3362 is tensor(8.9287e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3363. State = [[-0.20654017  0.18050653]]. Action = [[-0.15365195 -0.09011462 -0.10631716 -0.70219445]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 3363 is [True, False, False, False, False, True]
State prediction error at timestep 3363 is tensor(1.1657e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3364. State = [[-0.20643227  0.18074773]]. Action = [[-0.23100843  0.2121644   0.06848067 -0.32257557]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 3364 is [True, False, False, False, False, True]
State prediction error at timestep 3364 is tensor(1.2055e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3364 of -1
Current timestep = 3365. State = [[-0.20609267  0.181001  ]]. Action = [[-0.17605393  0.13667035 -0.08331262  0.2831005 ]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 3365 is [True, False, False, False, False, True]
State prediction error at timestep 3365 is tensor(2.8368e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3366. State = [[-0.2061279  0.1809466]]. Action = [[ 0.14390266  0.03866664  0.04930875 -0.6955018 ]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 3366 is [True, False, False, False, False, True]
State prediction error at timestep 3366 is tensor(1.0034e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3367. State = [[-0.20605785  0.18111157]]. Action = [[-0.20242517 -0.16877078  0.09689814 -0.23428315]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 3367 is [True, False, False, False, False, True]
State prediction error at timestep 3367 is tensor(5.1799e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3368. State = [[-0.20601402  0.1811895 ]]. Action = [[-0.20162092  0.22323123  0.21164289 -0.90203995]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 3368 is [True, False, False, False, False, True]
State prediction error at timestep 3368 is tensor(5.5559e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3368 of -1
Current timestep = 3369. State = [[-0.20576799  0.18160044]]. Action = [[0.01294282 0.07981101 0.0785231  0.5097208 ]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 3369 is [True, False, False, False, False, True]
State prediction error at timestep 3369 is tensor(1.4062e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3370. State = [[-0.20584318  0.18230328]]. Action = [[-0.19630638 -0.18237215 -0.11448866 -0.7145723 ]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 3370 is [True, False, False, False, False, True]
State prediction error at timestep 3370 is tensor(6.8462e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3371. State = [[-0.2061044   0.18256167]]. Action = [[ 0.12690836 -0.17349    -0.00415136 -0.03727156]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 3371 is [True, False, False, False, False, True]
State prediction error at timestep 3371 is tensor(4.3018e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3372. State = [[-0.20619296  0.1827866 ]]. Action = [[-0.16458315  0.14204848 -0.1665395  -0.24382955]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 3372 is [True, False, False, False, False, True]
State prediction error at timestep 3372 is tensor(5.7671e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3373. State = [[-0.20603248  0.18312646]]. Action = [[ 0.1965614  -0.00964554  0.01848361 -0.23224986]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 3373 is [True, False, False, False, False, True]
State prediction error at timestep 3373 is tensor(4.2373e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3374. State = [[-0.20610347  0.1832128 ]]. Action = [[0.14508176 0.18669862 0.23723575 0.2971406 ]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 3374 is [True, False, False, False, False, True]
State prediction error at timestep 3374 is tensor(8.0423e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3375. State = [[-0.205948    0.18396509]]. Action = [[-0.06283045  0.10215572 -0.09249666 -0.3236928 ]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 3375 is [True, False, False, False, False, True]
State prediction error at timestep 3375 is tensor(6.3179e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3375 of -1
Current timestep = 3376. State = [[-0.20672698  0.18522486]]. Action = [[ 0.15361848 -0.14862028 -0.07963353  0.8563945 ]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 3376 is [True, False, False, False, False, True]
State prediction error at timestep 3376 is tensor(1.6742e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3377. State = [[-0.20722966  0.18647552]]. Action = [[ 0.16550592  0.08430615  0.14293107 -0.23179299]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 3377 is [True, False, False, False, False, True]
State prediction error at timestep 3377 is tensor(3.8481e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3377 of -1
Current timestep = 3378. State = [[-0.20733832  0.18692778]]. Action = [[ 0.09081951  0.16201645 -0.00049675  0.13418674]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 3378 is [True, False, False, False, False, True]
State prediction error at timestep 3378 is tensor(2.2729e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3379. State = [[-0.20769255  0.18757996]]. Action = [[ 0.18174401  0.16502422  0.19811478 -0.4269551 ]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 3379 is [True, False, False, False, False, True]
State prediction error at timestep 3379 is tensor(1.3925e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3380. State = [[-0.20800637  0.18813235]]. Action = [[-0.08942908  0.1702655   0.18026924  0.32132995]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 3380 is [True, False, False, False, False, True]
State prediction error at timestep 3380 is tensor(5.4144e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3381. State = [[-0.20843582  0.18948013]]. Action = [[ 0.00894043  0.08962983  0.07624671 -0.40862226]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 3381 is [True, False, False, False, False, True]
State prediction error at timestep 3381 is tensor(1.0122e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3381 of -1
Current timestep = 3382. State = [[-0.20873041  0.19070414]]. Action = [[ 0.18805707 -0.09836313 -0.19548956  0.93370295]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 3382 is [True, False, False, False, False, True]
State prediction error at timestep 3382 is tensor(5.3204e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3383. State = [[-0.20867702  0.19131254]]. Action = [[ 0.03150597  0.2220735  -0.10321124 -0.6666611 ]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 3383 is [True, False, False, False, False, True]
State prediction error at timestep 3383 is tensor(1.8224e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3384. State = [[-0.20872149  0.19199105]]. Action = [[ 0.13795617  0.13987035  0.08535519 -0.47816873]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 3384 is [True, False, False, False, False, True]
State prediction error at timestep 3384 is tensor(6.7401e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3384 of -1
Current timestep = 3385. State = [[-0.20884456  0.19225748]]. Action = [[-0.18096665 -0.16296223  0.09547034 -0.0121969 ]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 3385 is [True, False, False, False, False, True]
State prediction error at timestep 3385 is tensor(4.3059e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3386. State = [[-0.20876662  0.19328645]]. Action = [[-0.20218556 -0.08170852  0.035943    0.28213012]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 3386 is [True, False, False, False, False, True]
State prediction error at timestep 3386 is tensor(6.7530e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3387. State = [[-0.20878327  0.19386551]]. Action = [[ 0.04065073  0.16310072 -0.17745252  0.19416451]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 3387 is [True, False, False, False, False, True]
State prediction error at timestep 3387 is tensor(7.0361e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3388. State = [[-0.20882949  0.1944353 ]]. Action = [[-0.06952104 -0.19056949 -0.10801099 -0.66635245]]. Reward = [0.]
Curr episode timestep = 682
Scene graph at timestep 3388 is [True, False, False, False, False, True]
State prediction error at timestep 3388 is tensor(4.5068e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3388 of -1
Current timestep = 3389. State = [[-0.208612    0.19512096]]. Action = [[ 0.11599296 -0.16663045  0.10373461  0.71304536]]. Reward = [0.]
Curr episode timestep = 683
Scene graph at timestep 3389 is [True, False, False, False, False, True]
State prediction error at timestep 3389 is tensor(9.5308e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3390. State = [[-0.20866008  0.19534668]]. Action = [[0.21601433 0.13213241 0.21155578 0.51321673]]. Reward = [0.]
Curr episode timestep = 684
Scene graph at timestep 3390 is [True, False, False, False, False, True]
State prediction error at timestep 3390 is tensor(5.2893e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3391. State = [[-0.20844904  0.19594331]]. Action = [[-0.19237985 -0.1524014   0.06871986  0.13262987]]. Reward = [0.]
Curr episode timestep = 685
Scene graph at timestep 3391 is [True, False, False, False, False, True]
State prediction error at timestep 3391 is tensor(8.5522e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3392. State = [[-0.20828468  0.19611329]]. Action = [[ 0.24499458 -0.1854969  -0.14082494 -0.12671101]]. Reward = [0.]
Curr episode timestep = 686
Scene graph at timestep 3392 is [True, False, False, False, False, True]
State prediction error at timestep 3392 is tensor(2.6614e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3393. State = [[-0.20838827  0.1963146 ]]. Action = [[-0.19354102 -0.08973527 -0.05318946 -0.22489995]]. Reward = [0.]
Curr episode timestep = 687
Scene graph at timestep 3393 is [True, False, False, False, False, True]
State prediction error at timestep 3393 is tensor(3.4567e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3393 of -1
Current timestep = 3394. State = [[-0.2079975  0.1968594]]. Action = [[ 0.0616644  -0.09049353  0.09894502 -0.52576226]]. Reward = [0.]
Curr episode timestep = 688
Scene graph at timestep 3394 is [True, False, False, False, False, True]
State prediction error at timestep 3394 is tensor(4.8270e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3394 of -1
Current timestep = 3395. State = [[-0.20669924  0.1964987 ]]. Action = [[0.00616801 0.05503297 0.1974433  0.48808742]]. Reward = [0.]
Curr episode timestep = 689
Scene graph at timestep 3395 is [True, False, False, False, False, True]
State prediction error at timestep 3395 is tensor(1.9674e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3396. State = [[-0.20584439  0.19711918]]. Action = [[-0.04873101  0.10141695  0.05578727  0.812655  ]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 3396 is [True, False, False, False, False, True]
State prediction error at timestep 3396 is tensor(9.1702e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3397. State = [[-0.20593071  0.19759938]]. Action = [[-0.15425979 -0.1987475   0.10479596 -0.29633278]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 3397 is [True, False, False, False, False, True]
State prediction error at timestep 3397 is tensor(4.5994e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3398. State = [[-0.20612785  0.19871269]]. Action = [[ 0.15421164 -0.13624898 -0.19835418  0.77988195]]. Reward = [0.]
Curr episode timestep = 692
Scene graph at timestep 3398 is [True, False, False, False, False, True]
State prediction error at timestep 3398 is tensor(6.0368e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3398 of -1
Current timestep = 3399. State = [[-0.20622313  0.19914079]]. Action = [[-0.10740498  0.16000915  0.16445786 -0.434242  ]]. Reward = [0.]
Curr episode timestep = 693
Scene graph at timestep 3399 is [True, False, False, False, False, True]
State prediction error at timestep 3399 is tensor(4.3169e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3400. State = [[-0.20651676  0.20028552]]. Action = [[ 0.1119      0.08105424  0.10843551 -0.6368002 ]]. Reward = [0.]
Curr episode timestep = 694
Scene graph at timestep 3400 is [True, False, False, False, False, True]
State prediction error at timestep 3400 is tensor(1.6307e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3400 of -1
Current timestep = 3401. State = [[-0.20574814  0.20143133]]. Action = [[-0.24489391 -0.16661422 -0.1777143   0.57736135]]. Reward = [0.]
Curr episode timestep = 695
Scene graph at timestep 3401 is [True, False, False, False, False, True]
State prediction error at timestep 3401 is tensor(4.8084e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3402. State = [[-0.2050587   0.20221545]]. Action = [[ 0.2286481   0.24511182 -0.00127938 -0.97673184]]. Reward = [0.]
Curr episode timestep = 696
Scene graph at timestep 3402 is [True, False, False, False, False, True]
State prediction error at timestep 3402 is tensor(1.1127e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3403. State = [[-0.20452935  0.20254597]]. Action = [[-0.09228042 -0.13436207 -0.13572918  0.16768193]]. Reward = [0.]
Curr episode timestep = 697
Scene graph at timestep 3403 is [True, False, False, False, False, True]
State prediction error at timestep 3403 is tensor(5.0120e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3404. State = [[-0.20137906  0.20495597]]. Action = [[-0.01333322 -0.1310614   0.0537135   0.40526414]]. Reward = [0.]
Curr episode timestep = 698
Scene graph at timestep 3404 is [True, False, False, False, False, True]
State prediction error at timestep 3404 is tensor(1.2662e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3404 of -1
Current timestep = 3405. State = [[-0.20045769  0.20491348]]. Action = [[ 0.2394008  -0.24549949  0.07290313  0.00056112]]. Reward = [0.]
Curr episode timestep = 699
Scene graph at timestep 3405 is [True, False, False, False, False, True]
State prediction error at timestep 3405 is tensor(5.5202e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3406. State = [[-0.19913362  0.20468894]]. Action = [[ 0.11497763  0.0299145  -0.03668815  0.42696965]]. Reward = [0.]
Curr episode timestep = 700
Scene graph at timestep 3406 is [True, False, False, False, False, True]
State prediction error at timestep 3406 is tensor(4.4640e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3406 of -1
Current timestep = 3407. State = [[-0.1982642   0.20479673]]. Action = [[0.20428532 0.24330634 0.01827571 0.2774644 ]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 3407 is [True, False, False, False, False, True]
State prediction error at timestep 3407 is tensor(1.5886e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3408. State = [[-0.19747245  0.20493527]]. Action = [[-0.21221113  0.06244272  0.01001766  0.40551198]]. Reward = [0.]
Curr episode timestep = 702
Scene graph at timestep 3408 is [True, False, False, False, False, True]
State prediction error at timestep 3408 is tensor(2.1865e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3409. State = [[-0.19521715  0.20576508]]. Action = [[-0.0589353   0.03623936  0.08677804  0.5635009 ]]. Reward = [0.]
Curr episode timestep = 703
Scene graph at timestep 3409 is [True, False, False, False, False, True]
State prediction error at timestep 3409 is tensor(8.8226e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3409 of -1
Current timestep = 3410. State = [[-0.19490445  0.20587051]]. Action = [[ 0.09187859 -0.03801443  0.16943559 -0.243034  ]]. Reward = [0.]
Curr episode timestep = 704
Scene graph at timestep 3410 is [True, False, False, False, False, True]
State prediction error at timestep 3410 is tensor(8.3400e-09, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3410 of -1
Current timestep = 3411. State = [[-0.1945699   0.20575462]]. Action = [[0.04839996 0.19451246 0.04845819 0.7504101 ]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 3411 is [True, False, False, False, False, True]
State prediction error at timestep 3411 is tensor(2.3260e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3412. State = [[-0.1943914   0.20571502]]. Action = [[-0.05493587  0.19472685 -0.07403392  0.9730735 ]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 3412 is [True, False, False, False, False, True]
State prediction error at timestep 3412 is tensor(6.5174e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3413. State = [[-0.19429246  0.20536892]]. Action = [[-0.02757281 -0.09027949  0.21166652  0.39563394]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 3413 is [True, False, False, False, False, True]
State prediction error at timestep 3413 is tensor(3.5798e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3413 of -1
Current timestep = 3414. State = [[-0.19385739  0.20458454]]. Action = [[-0.14575475  0.19078732  0.09696564 -0.9724137 ]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 3414 is [True, False, False, False, False, True]
State prediction error at timestep 3414 is tensor(4.7862e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3415. State = [[-0.19359466  0.20411783]]. Action = [[ 0.24605241 -0.04663971  0.20182991 -0.8478746 ]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 3415 is [True, False, False, False, False, True]
State prediction error at timestep 3415 is tensor(3.5567e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3415 of -1
Current timestep = 3416. State = [[-0.1934072   0.20378357]]. Action = [[-0.10408586 -0.18198864  0.12669611  0.1596427 ]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 3416 is [True, False, False, False, False, True]
State prediction error at timestep 3416 is tensor(2.3156e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3417. State = [[-0.19331738  0.2035277 ]]. Action = [[ 0.21122128 -0.19330823  0.00975275 -0.93005425]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 3417 is [True, False, False, False, False, True]
State prediction error at timestep 3417 is tensor(1.9666e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3418. State = [[-0.19321595  0.20319825]]. Action = [[-0.21583606  0.19428581 -0.10080421  0.78782225]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 3418 is [True, False, False, False, False, True]
State prediction error at timestep 3418 is tensor(7.5464e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3419. State = [[-0.19311124  0.20295273]]. Action = [[-0.15528248  0.2390266   0.08277673 -0.07060277]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 3419 is [True, False, False, False, False, True]
State prediction error at timestep 3419 is tensor(5.1225e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3420. State = [[-0.19312716  0.2029395 ]]. Action = [[ 0.17401373 -0.23176365 -0.00451851 -0.7709331 ]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 3420 is [True, False, False, False, False, True]
State prediction error at timestep 3420 is tensor(1.2886e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3421. State = [[-0.19303457  0.20276798]]. Action = [[ 0.21601751  0.16344818 -0.2218347  -0.7463484 ]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 3421 is [True, False, False, False, False, True]
State prediction error at timestep 3421 is tensor(3.9415e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3421 of -1
Current timestep = 3422. State = [[-0.19288903  0.20239371]]. Action = [[-0.09960911  0.03617457  0.11819962  0.45283175]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 3422 is [True, False, False, False, False, True]
State prediction error at timestep 3422 is tensor(2.3909e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3422 of -1
Current timestep = 3423. State = [[-0.19298993  0.20255724]]. Action = [[-0.06489557  0.22660834 -0.14855944 -0.6727162 ]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 3423 is [True, False, False, False, False, True]
State prediction error at timestep 3423 is tensor(4.5703e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3424. State = [[-0.19308265  0.20272881]]. Action = [[ 0.03210309 -0.19962613  0.17242914  0.8425585 ]]. Reward = [0.]
Curr episode timestep = 718
Scene graph at timestep 3424 is [True, False, False, False, False, True]
State prediction error at timestep 3424 is tensor(1.6598e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3425. State = [[-0.19310938  0.20279859]]. Action = [[ 0.22084615 -0.0730781   0.22133788  0.6044245 ]]. Reward = [0.]
Curr episode timestep = 719
Scene graph at timestep 3425 is [True, False, False, False, False, True]
State prediction error at timestep 3425 is tensor(8.1113e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3426. State = [[-0.19308627  0.20264533]]. Action = [[-0.06975873 -0.20543374  0.13177478 -0.24268317]]. Reward = [0.]
Curr episode timestep = 720
Scene graph at timestep 3426 is [True, False, False, False, False, True]
State prediction error at timestep 3426 is tensor(2.2532e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3427. State = [[-0.19308145  0.20273776]]. Action = [[-0.22039811 -0.08062544  0.14511362 -0.4947996 ]]. Reward = [0.]
Curr episode timestep = 721
Scene graph at timestep 3427 is [True, False, False, False, False, True]
State prediction error at timestep 3427 is tensor(2.0845e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3428. State = [[-0.19330135  0.20300993]]. Action = [[-0.11842452  0.11174655  0.16003045 -0.6652453 ]]. Reward = [0.]
Curr episode timestep = 722
Scene graph at timestep 3428 is [True, False, False, False, False, True]
State prediction error at timestep 3428 is tensor(8.0686e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3428 of -1
Current timestep = 3429. State = [[-0.19568376  0.20641263]]. Action = [[ 0.07841989 -0.02687867  0.22187129 -0.4120921 ]]. Reward = [0.]
Curr episode timestep = 723
Scene graph at timestep 3429 is [True, False, False, False, False, True]
State prediction error at timestep 3429 is tensor(1.0905e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3429 of -1
Current timestep = 3430. State = [[-0.19579941  0.20678975]]. Action = [[ 0.04379433 -0.00994278  0.19571689 -0.3615393 ]]. Reward = [0.]
Curr episode timestep = 724
Scene graph at timestep 3430 is [True, False, False, False, False, True]
State prediction error at timestep 3430 is tensor(8.7284e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3430 of -1
Current timestep = 3431. State = [[-0.19578809  0.20695682]]. Action = [[-0.11177883  0.21538278 -0.04833834 -0.6341577 ]]. Reward = [0.]
Curr episode timestep = 725
Scene graph at timestep 3431 is [True, False, False, False, False, True]
State prediction error at timestep 3431 is tensor(3.2600e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3432. State = [[-0.19583236  0.20675442]]. Action = [[-0.1790777  -0.07534252  0.1372832  -0.8400876 ]]. Reward = [0.]
Curr episode timestep = 726
Scene graph at timestep 3432 is [True, False, False, False, False, True]
State prediction error at timestep 3432 is tensor(8.4041e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3433. State = [[-0.19579013  0.20655674]]. Action = [[ 0.0340637  -0.19102974 -0.16544302  0.52500963]]. Reward = [0.]
Curr episode timestep = 727
Scene graph at timestep 3433 is [True, False, False, False, False, True]
State prediction error at timestep 3433 is tensor(5.6093e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3434. State = [[-0.19574219  0.20687647]]. Action = [[0.12141913 0.12394926 0.16870856 0.9036944 ]]. Reward = [0.]
Curr episode timestep = 728
Scene graph at timestep 3434 is [True, False, False, False, False, True]
State prediction error at timestep 3434 is tensor(3.0432e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3434 of -1
Current timestep = 3435. State = [[-0.19543639  0.20742132]]. Action = [[ 0.19197312  0.22033024  0.12651014 -0.74899644]]. Reward = [0.]
Curr episode timestep = 729
Scene graph at timestep 3435 is [True, False, False, False, False, True]
State prediction error at timestep 3435 is tensor(1.2514e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3436. State = [[-0.19519562  0.20808746]]. Action = [[-0.05116671 -0.22035296  0.0480102  -0.7863351 ]]. Reward = [0.]
Curr episode timestep = 730
Scene graph at timestep 3436 is [True, False, False, False, False, True]
State prediction error at timestep 3436 is tensor(2.3550e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3437. State = [[-0.19497061  0.20827292]]. Action = [[-0.07078403 -0.16432633 -0.17987405 -0.47200817]]. Reward = [0.]
Curr episode timestep = 731
Scene graph at timestep 3437 is [True, False, False, False, False, True]
State prediction error at timestep 3437 is tensor(1.1318e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3438. State = [[-0.19462597  0.20841321]]. Action = [[-0.23062374 -0.18747349  0.11557877 -0.07991993]]. Reward = [0.]
Curr episode timestep = 732
Scene graph at timestep 3438 is [True, False, False, False, False, True]
State prediction error at timestep 3438 is tensor(1.3689e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3438 of -1
Current timestep = 3439. State = [[-0.19333597  0.20977688]]. Action = [[-0.10995661 -0.0469479   0.20388862 -0.7885554 ]]. Reward = [0.]
Curr episode timestep = 733
Scene graph at timestep 3439 is [True, False, False, False, False, True]
State prediction error at timestep 3439 is tensor(5.6932e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3440. State = [[-0.19358522  0.21018416]]. Action = [[-0.11211136  0.03492546  0.24071774  0.07916641]]. Reward = [0.]
Curr episode timestep = 734
Scene graph at timestep 3440 is [True, False, False, False, False, True]
State prediction error at timestep 3440 is tensor(1.5025e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3441. State = [[-0.19514172  0.21226244]]. Action = [[-0.03798924 -0.01392365  0.17978603  0.41482067]]. Reward = [0.]
Curr episode timestep = 735
Scene graph at timestep 3441 is [True, False, False, False, False, True]
State prediction error at timestep 3441 is tensor(1.2894e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3441 of -1
Current timestep = 3442. State = [[-0.19599047  0.21342534]]. Action = [[ 0.04673475 -0.08780074  0.01477066  0.8345337 ]]. Reward = [0.]
Curr episode timestep = 736
Scene graph at timestep 3442 is [True, False, False, False, False, True]
State prediction error at timestep 3442 is tensor(9.5244e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3443. State = [[-0.19582538  0.21320371]]. Action = [[-0.17823943  0.21532571 -0.01191604 -0.660489  ]]. Reward = [0.]
Curr episode timestep = 737
Scene graph at timestep 3443 is [True, False, False, False, False, True]
State prediction error at timestep 3443 is tensor(1.2839e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3444. State = [[-0.19577543  0.21306767]]. Action = [[ 0.02595556 -0.19011873  0.01657081 -0.19058216]]. Reward = [0.]
Curr episode timestep = 738
Scene graph at timestep 3444 is [True, False, False, False, False, True]
State prediction error at timestep 3444 is tensor(4.5505e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3445. State = [[-0.19560981  0.21262112]]. Action = [[ 0.05736381  0.01838988  0.03858173 -0.18830317]]. Reward = [0.]
Curr episode timestep = 739
Scene graph at timestep 3445 is [True, False, False, False, False, True]
State prediction error at timestep 3445 is tensor(1.2709e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3446. State = [[-0.19558841  0.21255538]]. Action = [[-0.0957613  -0.16251531 -0.21866317  0.04933953]]. Reward = [0.]
Curr episode timestep = 740
Scene graph at timestep 3446 is [True, False, False, False, False, True]
State prediction error at timestep 3446 is tensor(1.0933e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3447. State = [[-0.19560981  0.21262112]]. Action = [[ 0.20495749  0.08621496  0.225182   -0.80823755]]. Reward = [0.]
Curr episode timestep = 741
Scene graph at timestep 3447 is [True, False, False, False, False, True]
State prediction error at timestep 3447 is tensor(5.5546e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3448. State = [[-0.19560981  0.21262112]]. Action = [[-0.07954851 -0.17591375  0.22251254  0.59840417]]. Reward = [0.]
Curr episode timestep = 742
Scene graph at timestep 3448 is [True, False, False, False, False, True]
State prediction error at timestep 3448 is tensor(1.6655e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3449. State = [[-0.19558841  0.21255538]]. Action = [[ 0.0730443   0.05050427 -0.15190133  0.4561447 ]]. Reward = [0.]
Curr episode timestep = 743
Scene graph at timestep 3449 is [True, False, False, False, False, True]
State prediction error at timestep 3449 is tensor(2.9380e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3449 of -1
Current timestep = 3450. State = [[-0.19566308  0.21266086]]. Action = [[-0.12814987  0.07504544  0.10067454 -0.00236893]]. Reward = [0.]
Curr episode timestep = 744
Scene graph at timestep 3450 is [True, False, False, False, False, True]
State prediction error at timestep 3450 is tensor(1.8985e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3451. State = [[-0.19604182  0.2132661 ]]. Action = [[0.18153155 0.12982589 0.08859986 0.8284657 ]]. Reward = [0.]
Curr episode timestep = 745
Scene graph at timestep 3451 is [True, False, False, False, False, True]
State prediction error at timestep 3451 is tensor(4.6003e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3452. State = [[-0.19648033  0.21398911]]. Action = [[ 0.19614014  0.16764116  0.01112384 -0.10457236]]. Reward = [0.]
Curr episode timestep = 746
Scene graph at timestep 3452 is [True, False, False, False, False, True]
State prediction error at timestep 3452 is tensor(2.9960e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3453. State = [[-0.19673324  0.21437423]]. Action = [[ 0.21316355  0.14622638 -0.12443031  0.5094445 ]]. Reward = [0.]
Curr episode timestep = 747
Scene graph at timestep 3453 is [True, False, False, False, False, True]
State prediction error at timestep 3453 is tensor(2.3696e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3454. State = [[-0.19695574  0.21451508]]. Action = [[ 0.17769122 -0.07496805 -0.21701586  0.45225108]]. Reward = [0.]
Curr episode timestep = 748
Scene graph at timestep 3454 is [True, False, False, False, False, True]
State prediction error at timestep 3454 is tensor(8.8873e-09, grad_fn=<MseLossBackward0>)
Current timestep = 3455. State = [[-0.19714914  0.21485636]]. Action = [[-0.02315702 -0.15185311 -0.06631288 -0.05942965]]. Reward = [0.]
Curr episode timestep = 749
Scene graph at timestep 3455 is [True, False, False, False, False, True]
State prediction error at timestep 3455 is tensor(2.0786e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3456. State = [[-0.19745456  0.2153643 ]]. Action = [[-0.22985077  0.02380031  0.03294334 -0.5507216 ]]. Reward = [0.]
Curr episode timestep = 750
Scene graph at timestep 3456 is [True, False, False, False, False, True]
State prediction error at timestep 3456 is tensor(9.0162e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3456 of -1
Current timestep = 3457. State = [[-0.19767448  0.21544123]]. Action = [[-0.10638961 -0.14137556 -0.00168411 -0.8947276 ]]. Reward = [0.]
Curr episode timestep = 751
Scene graph at timestep 3457 is [True, False, False, False, False, True]
State prediction error at timestep 3457 is tensor(1.3428e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3458. State = [[-0.19769111  0.21557884]]. Action = [[ 0.16147923  0.20582646 -0.001399    0.27315927]]. Reward = [0.]
Curr episode timestep = 752
Scene graph at timestep 3458 is [True, False, False, False, False, True]
State prediction error at timestep 3458 is tensor(1.6939e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3459. State = [[-0.19790569  0.21599373]]. Action = [[-0.16260682  0.23789316 -0.06280243  0.7041125 ]]. Reward = [0.]
Curr episode timestep = 753
Scene graph at timestep 3459 is [True, False, False, False, False, True]
State prediction error at timestep 3459 is tensor(7.4268e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3460. State = [[-0.1979759  0.2160109]]. Action = [[-0.09046231  0.14355797  0.07452971  0.13410306]]. Reward = [0.]
Curr episode timestep = 754
Scene graph at timestep 3460 is [True, False, False, False, False, True]
State prediction error at timestep 3460 is tensor(7.8216e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3461. State = [[-0.19808881  0.21616815]]. Action = [[ 0.08111924 -0.21299925 -0.09830296  0.05000496]]. Reward = [0.]
Curr episode timestep = 755
Scene graph at timestep 3461 is [True, False, False, False, False, True]
State prediction error at timestep 3461 is tensor(3.2597e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3461 of -1
Current timestep = 3462. State = [[-0.19841623  0.21657315]]. Action = [[-0.12595294  0.00307176 -0.01127288 -0.7296595 ]]. Reward = [0.]
Curr episode timestep = 756
Scene graph at timestep 3462 is [True, False, False, False, False, True]
State prediction error at timestep 3462 is tensor(1.4329e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3463. State = [[-0.19865444  0.21695435]]. Action = [[ 0.04895884  0.18324983 -0.21026331 -0.5263509 ]]. Reward = [0.]
Curr episode timestep = 757
Scene graph at timestep 3463 is [True, False, False, False, False, True]
State prediction error at timestep 3463 is tensor(1.0375e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3464. State = [[-0.19905424  0.21754356]]. Action = [[-0.07029524  0.16001207  0.20286351  0.33803976]]. Reward = [0.]
Curr episode timestep = 758
Scene graph at timestep 3464 is [True, False, False, False, False, True]
State prediction error at timestep 3464 is tensor(3.1676e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3464 of -1
Current timestep = 3465. State = [[-0.20027356  0.21921486]]. Action = [[-0.12395138 -0.03239283 -0.20295474 -0.45540017]]. Reward = [0.]
Curr episode timestep = 759
Scene graph at timestep 3465 is [True, False, False, False, False, True]
State prediction error at timestep 3465 is tensor(1.9322e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3466. State = [[-0.20198797  0.22050862]]. Action = [[ 0.22679648 -0.15574226  0.07381403 -0.4126506 ]]. Reward = [0.]
Curr episode timestep = 760
Scene graph at timestep 3466 is [True, False, False, False, False, True]
State prediction error at timestep 3466 is tensor(5.5286e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3467. State = [[-0.20236364  0.22089912]]. Action = [[-0.18736298 -0.13666466 -0.07933205  0.89947724]]. Reward = [0.]
Curr episode timestep = 761
Scene graph at timestep 3467 is [True, False, False, False, False, True]
State prediction error at timestep 3467 is tensor(4.7426e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3468. State = [[-0.20248593  0.22104639]]. Action = [[ 0.1507023   0.23970285 -0.12856069  0.16602623]]. Reward = [0.]
Curr episode timestep = 762
Scene graph at timestep 3468 is [True, False, False, False, False, True]
State prediction error at timestep 3468 is tensor(1.8707e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3469. State = [[-0.20345092  0.22167999]]. Action = [[-0.06841439 -0.03003979  0.22547466 -0.66245973]]. Reward = [0.]
Curr episode timestep = 763
Scene graph at timestep 3469 is [True, False, False, False, False, True]
State prediction error at timestep 3469 is tensor(1.0547e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3469 of -1
Current timestep = 3470. State = [[-0.20414281  0.22148742]]. Action = [[-0.17492878  0.02070522  0.14598614 -0.7578609 ]]. Reward = [0.]
Curr episode timestep = 764
Scene graph at timestep 3470 is [True, False, False, False, False, True]
State prediction error at timestep 3470 is tensor(2.2088e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3471. State = [[-0.20476022  0.2213556 ]]. Action = [[ 0.14686936  0.05296972  0.04087961 -0.6567012 ]]. Reward = [0.]
Curr episode timestep = 765
Scene graph at timestep 3471 is [True, False, False, False, False, True]
State prediction error at timestep 3471 is tensor(4.4654e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3472. State = [[-0.20496759  0.22147009]]. Action = [[ 0.23456934  0.19552395 -0.20505708 -0.47649646]]. Reward = [0.]
Curr episode timestep = 766
Scene graph at timestep 3472 is [True, False, False, False, False, True]
State prediction error at timestep 3472 is tensor(1.5410e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3473. State = [[-0.20557739  0.22149457]]. Action = [[-0.19314323  0.19788212  0.12747571 -0.3017286 ]]. Reward = [0.]
Curr episode timestep = 767
Scene graph at timestep 3473 is [True, False, False, False, False, True]
State prediction error at timestep 3473 is tensor(2.6996e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3474. State = [[-0.20598342  0.22126924]]. Action = [[-0.16205622 -0.11940913  0.21828035  0.8011539 ]]. Reward = [0.]
Curr episode timestep = 768
Scene graph at timestep 3474 is [True, False, False, False, False, True]
State prediction error at timestep 3474 is tensor(1.7936e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3474 of -1
Current timestep = 3475. State = [[-0.20662421  0.22117724]]. Action = [[-0.1205785   0.22421026  0.17038351 -0.54248124]]. Reward = [0.]
Curr episode timestep = 769
Scene graph at timestep 3475 is [True, False, False, False, False, True]
State prediction error at timestep 3475 is tensor(1.0997e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3476. State = [[-0.20712289  0.22112198]]. Action = [[ 0.20710665 -0.13680471  0.0262382  -0.37274092]]. Reward = [0.]
Curr episode timestep = 770
Scene graph at timestep 3476 is [True, False, False, False, False, True]
State prediction error at timestep 3476 is tensor(4.8088e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3477. State = [[-0.20754796  0.2209398 ]]. Action = [[-0.13119231 -0.15032049  0.22380406  0.88703537]]. Reward = [0.]
Curr episode timestep = 771
Scene graph at timestep 3477 is [True, False, False, False, False, True]
State prediction error at timestep 3477 is tensor(5.6281e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3478. State = [[-0.20785294  0.221083  ]]. Action = [[ 0.17036694  0.21864742 -0.17873497  0.1788032 ]]. Reward = [0.]
Curr episode timestep = 772
Scene graph at timestep 3478 is [True, False, False, False, False, True]
State prediction error at timestep 3478 is tensor(1.5414e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3479. State = [[-0.20801328  0.22098944]]. Action = [[0.2230447  0.06452388 0.0261727  0.63152456]]. Reward = [0.]
Curr episode timestep = 773
Scene graph at timestep 3479 is [True, False, False, False, False, True]
State prediction error at timestep 3479 is tensor(1.3222e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3479 of -1
Current timestep = 3480. State = [[-0.20821981  0.22093089]]. Action = [[-0.22380511  0.06880674 -0.04193544 -0.10584265]]. Reward = [0.]
Curr episode timestep = 774
Scene graph at timestep 3480 is [True, False, False, False, False, True]
State prediction error at timestep 3480 is tensor(3.3445e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3481. State = [[-0.20873524  0.22101931]]. Action = [[ 0.11431611  0.05418706  0.08905119 -0.37961835]]. Reward = [0.]
Curr episode timestep = 775
Scene graph at timestep 3481 is [True, False, False, False, False, True]
State prediction error at timestep 3481 is tensor(7.9808e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3481 of -1
Current timestep = 3482. State = [[-0.20870735  0.22111274]]. Action = [[-0.08315566  0.2084651  -0.14467396 -0.47561276]]. Reward = [0.]
Curr episode timestep = 776
Scene graph at timestep 3482 is [True, False, False, False, False, True]
State prediction error at timestep 3482 is tensor(1.1968e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3483. State = [[-0.20873524  0.22101931]]. Action = [[-0.24186249  0.10434079  0.22738922  0.03305471]]. Reward = [0.]
Curr episode timestep = 777
Scene graph at timestep 3483 is [True, False, False, False, False, True]
State prediction error at timestep 3483 is tensor(5.7544e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3484. State = [[-0.20873524  0.22101931]]. Action = [[-0.02890705 -0.21115457 -0.0600502   0.8936758 ]]. Reward = [0.]
Curr episode timestep = 778
Scene graph at timestep 3484 is [True, False, False, False, False, True]
State prediction error at timestep 3484 is tensor(2.1414e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3485. State = [[-0.20868428  0.22104545]]. Action = [[ 0.13168019  0.10055545 -0.08836192 -0.04504782]]. Reward = [0.]
Curr episode timestep = 779
Scene graph at timestep 3485 is [True, False, False, False, False, True]
State prediction error at timestep 3485 is tensor(1.5081e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3485 of -1
Current timestep = 3486. State = [[-0.20823646  0.22216341]]. Action = [[-0.06335399  0.11934888 -0.12707445  0.8979361 ]]. Reward = [0.]
Curr episode timestep = 780
Scene graph at timestep 3486 is [True, False, False, False, False, True]
State prediction error at timestep 3486 is tensor(8.3698e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3486 of -1
Current timestep = 3487. State = [[-0.20889147  0.22326228]]. Action = [[ 0.1387214  -0.14949527 -0.01534367  0.5971093 ]]. Reward = [0.]
Curr episode timestep = 781
Scene graph at timestep 3487 is [True, False, False, False, False, True]
State prediction error at timestep 3487 is tensor(4.1482e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3488. State = [[-0.20951714  0.22431506]]. Action = [[-0.11739527 -0.15218799  0.24811321  0.00477266]]. Reward = [0.]
Curr episode timestep = 782
Scene graph at timestep 3488 is [True, False, False, False, False, True]
State prediction error at timestep 3488 is tensor(4.1800e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3489. State = [[-0.21004966  0.22500786]]. Action = [[ 0.04028141  0.16670096 -0.03743923  0.06723952]]. Reward = [0.]
Curr episode timestep = 783
Scene graph at timestep 3489 is [True, False, False, False, False, True]
State prediction error at timestep 3489 is tensor(3.6023e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3490. State = [[-0.21136284  0.2271333 ]]. Action = [[ 0.08560646 -0.03448555 -0.21521837  0.36567307]]. Reward = [0.]
Curr episode timestep = 784
Scene graph at timestep 3490 is [True, False, False, False, False, True]
State prediction error at timestep 3490 is tensor(3.0597e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3490 of -1
Current timestep = 3491. State = [[-0.21087304  0.22762309]]. Action = [[ 0.13952011 -0.1529026   0.14740488  0.17870569]]. Reward = [0.]
Curr episode timestep = 785
Scene graph at timestep 3491 is [True, False, False, False, False, True]
State prediction error at timestep 3491 is tensor(3.5238e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3492. State = [[-0.2106889   0.22775531]]. Action = [[ 0.18858415 -0.10877283 -0.19254746 -0.60965455]]. Reward = [0.]
Curr episode timestep = 786
Scene graph at timestep 3492 is [True, False, False, False, False, True]
State prediction error at timestep 3492 is tensor(5.4204e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3493. State = [[-0.21011534  0.22801284]]. Action = [[0.07908928 0.07556832 0.24277458 0.47697031]]. Reward = [0.]
Curr episode timestep = 787
Scene graph at timestep 3493 is [True, False, False, False, False, True]
State prediction error at timestep 3493 is tensor(2.1291e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3493 of -1
Current timestep = 3494. State = [[-0.20820892  0.22965884]]. Action = [[-0.02935344 -0.02569556  0.22515059  0.46020222]]. Reward = [0.]
Curr episode timestep = 788
Scene graph at timestep 3494 is [True, False, False, False, False, True]
State prediction error at timestep 3494 is tensor(3.1421e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3495. State = [[-0.20800503  0.22977692]]. Action = [[ 0.13061717  0.15815729 -0.0103475  -0.17855227]]. Reward = [0.]
Curr episode timestep = 789
Scene graph at timestep 3495 is [True, False, False, False, False, True]
State prediction error at timestep 3495 is tensor(4.1189e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3495 of -1
Current timestep = 3496. State = [[-0.20738308  0.23069511]]. Action = [[-0.10489665  0.09843859 -0.09959246  0.27294493]]. Reward = [0.]
Curr episode timestep = 790
Scene graph at timestep 3496 is [True, False, False, False, False, True]
State prediction error at timestep 3496 is tensor(9.9644e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3497. State = [[-0.2078357   0.23162252]]. Action = [[-0.039297   -0.16674146  0.1673162  -0.22364932]]. Reward = [0.]
Curr episode timestep = 791
Scene graph at timestep 3497 is [True, False, False, False, False, True]
State prediction error at timestep 3497 is tensor(1.1054e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3498. State = [[-0.20933284  0.23405008]]. Action = [[ 0.07185057 -0.10292661  0.21611503 -0.16457379]]. Reward = [0.]
Curr episode timestep = 792
Scene graph at timestep 3498 is [True, False, False, False, False, True]
State prediction error at timestep 3498 is tensor(5.9073e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3498 of -1
Current timestep = 3499. State = [[-0.20879546  0.23381038]]. Action = [[ 0.04972589 -0.0737907   0.1959039  -0.54866445]]. Reward = [0.]
Curr episode timestep = 793
Scene graph at timestep 3499 is [True, False, False, False, False, True]
State prediction error at timestep 3499 is tensor(8.1544e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3499 of -1
Current timestep = 3500. State = [[-0.20814838  0.23329721]]. Action = [[-0.19683495 -0.1302203   0.19792908  0.6756263 ]]. Reward = [0.]
Curr episode timestep = 794
Scene graph at timestep 3500 is [True, False, False, False, False, True]
State prediction error at timestep 3500 is tensor(1.3919e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3501. State = [[-0.20770568  0.23286676]]. Action = [[0.16778839 0.12987664 0.11764961 0.3950374 ]]. Reward = [0.]
Curr episode timestep = 795
Scene graph at timestep 3501 is [True, False, False, False, False, True]
State prediction error at timestep 3501 is tensor(2.5480e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3502. State = [[-0.20697132  0.23184912]]. Action = [[-0.07381931 -0.0588192  -0.16550763  0.16620743]]. Reward = [0.]
Curr episode timestep = 796
Scene graph at timestep 3502 is [True, False, False, False, False, True]
State prediction error at timestep 3502 is tensor(1.0533e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3502 of -1
Current timestep = 3503. State = [[-0.20670252  0.23132236]]. Action = [[-0.22773086 -0.18706068  0.14476997 -0.88355106]]. Reward = [0.]
Curr episode timestep = 797
Scene graph at timestep 3503 is [True, False, False, False, False, True]
State prediction error at timestep 3503 is tensor(2.7078e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3504. State = [[-0.2067107   0.23122191]]. Action = [[-0.1127326   0.17210329  0.1837889   0.7250583 ]]. Reward = [0.]
Curr episode timestep = 798
Scene graph at timestep 3504 is [True, False, False, False, False, True]
State prediction error at timestep 3504 is tensor(2.2917e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3505. State = [[-0.20646393  0.2305856 ]]. Action = [[-0.01027782  0.00860891  0.1983763  -0.01878512]]. Reward = [0.]
Curr episode timestep = 799
Scene graph at timestep 3505 is [True, False, False, False, False, True]
State prediction error at timestep 3505 is tensor(6.8855e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3505 of -1
Current timestep = 3506. State = [[-0.20646393  0.2305856 ]]. Action = [[-0.09023234  0.23459047  0.23066539  0.37758255]]. Reward = [0.]
Curr episode timestep = 800
Scene graph at timestep 3506 is [True, False, False, False, False, True]
State prediction error at timestep 3506 is tensor(4.8622e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3507. State = [[-0.20646393  0.2305856 ]]. Action = [[-0.16685681  0.24355191  0.22569817 -0.11730832]]. Reward = [0.]
Curr episode timestep = 801
Scene graph at timestep 3507 is [True, False, False, False, False, True]
State prediction error at timestep 3507 is tensor(5.4628e-09, grad_fn=<MseLossBackward0>)
Current timestep = 3508. State = [[-0.20651846  0.23062381]]. Action = [[ 0.18143395 -0.14460692 -0.24600509  0.23079967]]. Reward = [0.]
Curr episode timestep = 802
Scene graph at timestep 3508 is [True, False, False, False, False, True]
State prediction error at timestep 3508 is tensor(6.5963e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3509. State = [[-0.20643218  0.2304424 ]]. Action = [[-0.23473813 -0.12027374 -0.05869886 -0.9425577 ]]. Reward = [0.]
Curr episode timestep = 803
Scene graph at timestep 3509 is [True, False, False, False, False, True]
State prediction error at timestep 3509 is tensor(1.4763e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3510. State = [[-0.2063932   0.23039101]]. Action = [[ 0.15048444  0.12185943  0.02834508 -0.4760257 ]]. Reward = [0.]
Curr episode timestep = 804
Scene graph at timestep 3510 is [True, False, False, False, False, True]
State prediction error at timestep 3510 is tensor(3.4311e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3511. State = [[-0.20643218  0.2304424 ]]. Action = [[-0.14752011 -0.16475834  0.11214891  0.8142016 ]]. Reward = [0.]
Curr episode timestep = 805
Scene graph at timestep 3511 is [True, False, False, False, False, True]
State prediction error at timestep 3511 is tensor(4.5759e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3511 of -1
Current timestep = 3512. State = [[-0.20638227  0.23047155]]. Action = [[ 0.1168991   0.09028006  0.20166963 -0.3216933 ]]. Reward = [0.]
Curr episode timestep = 806
Scene graph at timestep 3512 is [True, False, False, False, False, True]
State prediction error at timestep 3512 is tensor(3.0638e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3513. State = [[-0.20634095  0.23053364]]. Action = [[ 0.20300117 -0.14799123 -0.1142519   0.910143  ]]. Reward = [0.]
Curr episode timestep = 807
Scene graph at timestep 3513 is [True, False, False, False, False, True]
State prediction error at timestep 3513 is tensor(3.2973e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3513 of -1
Current timestep = 3514. State = [[-0.20621185  0.23071088]]. Action = [[-0.20040134  0.2428118  -0.01104118 -0.39448524]]. Reward = [0.]
Curr episode timestep = 808
Scene graph at timestep 3514 is [True, False, False, False, False, True]
State prediction error at timestep 3514 is tensor(3.7105e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3515. State = [[-0.20578204  0.2311919 ]]. Action = [[-0.08337642 -0.12297036  0.1857596  -0.5116355 ]]. Reward = [0.]
Curr episode timestep = 809
Scene graph at timestep 3515 is [True, False, False, False, False, True]
State prediction error at timestep 3515 is tensor(7.9124e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3516. State = [[-0.20536967  0.23074189]]. Action = [[-0.19668625 -0.12035897  0.00263357 -0.6022467 ]]. Reward = [0.]
Curr episode timestep = 810
Scene graph at timestep 3516 is [True, False, False, False, False, True]
State prediction error at timestep 3516 is tensor(1.3684e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3517. State = [[-0.20504011  0.23026323]]. Action = [[ 0.19307756  0.10421273 -0.23217517 -0.10787034]]. Reward = [0.]
Curr episode timestep = 811
Scene graph at timestep 3517 is [True, False, False, False, False, True]
State prediction error at timestep 3517 is tensor(2.2328e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3518. State = [[-0.20498498  0.23014168]]. Action = [[ 0.19408566 -0.07359645 -0.11825815 -0.28394562]]. Reward = [0.]
Curr episode timestep = 812
Scene graph at timestep 3518 is [True, False, False, False, False, True]
State prediction error at timestep 3518 is tensor(2.8420e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3518 of -1
Current timestep = 3519. State = [[-0.20479743  0.22979179]]. Action = [[ 0.01943806  0.08124724  0.17609939 -0.2778209 ]]. Reward = [0.]
Curr episode timestep = 813
Scene graph at timestep 3519 is [True, False, False, False, False, True]
State prediction error at timestep 3519 is tensor(1.0433e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3520. State = [[-0.2049146   0.22994694]]. Action = [[ 0.16405126 -0.09178323 -0.00306989  0.94125295]]. Reward = [0.]
Curr episode timestep = 814
Scene graph at timestep 3520 is [True, False, False, False, False, True]
State prediction error at timestep 3520 is tensor(1.3829e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3521. State = [[-0.20501637  0.23011556]]. Action = [[ 0.15206632 -0.14660183 -0.18910003 -0.9293376 ]]. Reward = [0.]
Curr episode timestep = 815
Scene graph at timestep 3521 is [True, False, False, False, False, True]
State prediction error at timestep 3521 is tensor(6.6487e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3522. State = [[-0.20496811  0.2301805 ]]. Action = [[-0.1351922   0.2010889   0.18044001  0.65653634]]. Reward = [0.]
Curr episode timestep = 816
Scene graph at timestep 3522 is [True, False, False, False, False, True]
State prediction error at timestep 3522 is tensor(1.2993e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3523. State = [[-0.20489581  0.23017278]]. Action = [[-0.22705084  0.19903892  0.2356562  -0.24702406]]. Reward = [0.]
Curr episode timestep = 817
Scene graph at timestep 3523 is [True, False, False, False, False, True]
State prediction error at timestep 3523 is tensor(1.5711e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3524. State = [[-0.20476712  0.23010725]]. Action = [[ 0.19532603 -0.03750902 -0.14015791  0.40540648]]. Reward = [0.]
Curr episode timestep = 818
Scene graph at timestep 3524 is [True, False, False, False, False, True]
State prediction error at timestep 3524 is tensor(3.0379e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3525. State = [[-0.20487939  0.23019482]]. Action = [[-0.08223861  0.16403854 -0.01619241 -0.627539  ]]. Reward = [0.]
Curr episode timestep = 819
Scene graph at timestep 3525 is [True, False, False, False, False, True]
State prediction error at timestep 3525 is tensor(2.7804e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3525 of -1
Current timestep = 3526. State = [[-0.20483527  0.2302311 ]]. Action = [[0.20141262 0.19212294 0.00321522 0.4242612 ]]. Reward = [0.]
Curr episode timestep = 820
Scene graph at timestep 3526 is [True, False, False, False, False, True]
State prediction error at timestep 3526 is tensor(6.3359e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3527. State = [[-0.20487745  0.2301184 ]]. Action = [[-0.05174805 -0.15773787 -0.14048144 -0.64354545]]. Reward = [0.]
Curr episode timestep = 821
Scene graph at timestep 3527 is [True, False, False, False, False, True]
State prediction error at timestep 3527 is tensor(2.1296e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3528. State = [[-0.20490728  0.23018712]]. Action = [[ 0.02584279 -0.23423351  0.24541247 -0.71424073]]. Reward = [0.]
Curr episode timestep = 822
Scene graph at timestep 3528 is [True, False, False, False, False, True]
State prediction error at timestep 3528 is tensor(2.1549e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3529. State = [[-0.2047564   0.23003776]]. Action = [[ 0.13118237 -0.09372905 -0.04999915 -0.7732741 ]]. Reward = [0.]
Curr episode timestep = 823
Scene graph at timestep 3529 is [True, False, False, False, False, True]
State prediction error at timestep 3529 is tensor(5.3194e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3529 of -1
Current timestep = 3530. State = [[-0.20392294  0.22945628]]. Action = [[ 0.12089509 -0.22147831  0.14352262  0.18612766]]. Reward = [0.]
Curr episode timestep = 824
Scene graph at timestep 3530 is [True, False, False, False, False, True]
State prediction error at timestep 3530 is tensor(7.8539e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3531. State = [[-0.20336959  0.22901663]]. Action = [[ 0.02919367 -0.23291962  0.23835313 -0.2371803 ]]. Reward = [0.]
Curr episode timestep = 825
Scene graph at timestep 3531 is [True, False, False, False, False, True]
State prediction error at timestep 3531 is tensor(1.0390e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3532. State = [[-0.20288058  0.22845387]]. Action = [[ 0.024957   -0.21673653 -0.12596002 -0.4783889 ]]. Reward = [0.]
Curr episode timestep = 826
Scene graph at timestep 3532 is [True, False, False, False, False, True]
State prediction error at timestep 3532 is tensor(1.6895e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3533. State = [[-0.20264612  0.22818385]]. Action = [[ 0.21660623 -0.01560783  0.22307765 -0.10476351]]. Reward = [0.]
Curr episode timestep = 827
Scene graph at timestep 3533 is [True, False, False, False, False, True]
State prediction error at timestep 3533 is tensor(1.3929e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3533 of -1
Current timestep = 3534. State = [[-0.20138267  0.22673866]]. Action = [[ 0.0082486  -0.13151936  0.19756663 -0.49001455]]. Reward = [0.]
Curr episode timestep = 828
Scene graph at timestep 3534 is [True, False, False, False, False, True]
State prediction error at timestep 3534 is tensor(5.4211e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3535. State = [[-0.19882442  0.22214729]]. Action = [[ 0.00471434 -0.09691182  0.19677144  0.68077993]]. Reward = [0.]
Curr episode timestep = 829
Scene graph at timestep 3535 is [True, False, False, False, False, True]
State prediction error at timestep 3535 is tensor(2.0922e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3536. State = [[-0.19798231  0.22072798]]. Action = [[ 0.18048218  0.05832452 -0.05498895  0.8131678 ]]. Reward = [0.]
Curr episode timestep = 830
Scene graph at timestep 3536 is [True, False, False, False, False, True]
State prediction error at timestep 3536 is tensor(6.8976e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3537. State = [[-0.19744441  0.21991217]]. Action = [[-0.07164618  0.14294988 -0.19262527 -0.7937031 ]]. Reward = [0.]
Curr episode timestep = 831
Scene graph at timestep 3537 is [True, False, False, False, False, True]
State prediction error at timestep 3537 is tensor(8.7240e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3538. State = [[-0.19688529  0.21876651]]. Action = [[ 0.23599273 -0.21049164 -0.20214058 -0.10477161]]. Reward = [0.]
Curr episode timestep = 832
Scene graph at timestep 3538 is [True, False, False, False, False, True]
State prediction error at timestep 3538 is tensor(4.8125e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3539. State = [[-0.19665515  0.21772665]]. Action = [[-0.21206349 -0.07340227  0.1912041  -0.06319791]]. Reward = [0.]
Curr episode timestep = 833
Scene graph at timestep 3539 is [True, False, False, False, False, True]
State prediction error at timestep 3539 is tensor(5.0288e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3539 of -1
Current timestep = 3540. State = [[-0.1956348  0.2139179]]. Action = [[-0.10886088  0.00549412  0.05774429 -0.72609824]]. Reward = [0.]
Curr episode timestep = 834
Scene graph at timestep 3540 is [True, False, False, False, False, True]
State prediction error at timestep 3540 is tensor(5.4449e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3541. State = [[-0.19549793  0.21376316]]. Action = [[-0.09029859  0.22063434  0.16055256 -0.6334552 ]]. Reward = [0.]
Curr episode timestep = 835
Scene graph at timestep 3541 is [True, False, False, False, False, True]
State prediction error at timestep 3541 is tensor(4.4419e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3542. State = [[-0.19539863  0.2137425 ]]. Action = [[ 0.15592813 -0.23587735 -0.12214488  0.13183141]]. Reward = [0.]
Curr episode timestep = 836
Scene graph at timestep 3542 is [True, False, False, False, False, True]
State prediction error at timestep 3542 is tensor(8.7585e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3543. State = [[-0.19520986  0.2126865 ]]. Action = [[ 0.07425389 -0.0370841   0.14880264 -0.3910039 ]]. Reward = [0.]
Curr episode timestep = 837
Scene graph at timestep 3543 is [True, False, False, False, False, True]
State prediction error at timestep 3543 is tensor(3.6637e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3543 of -1
Current timestep = 3544. State = [[-0.19499561  0.21217912]]. Action = [[0.10743439 0.22324133 0.21910346 0.86163163]]. Reward = [0.]
Curr episode timestep = 838
Scene graph at timestep 3544 is [True, False, False, False, False, True]
State prediction error at timestep 3544 is tensor(1.5586e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3545. State = [[-0.19471397  0.21158971]]. Action = [[-0.01432532  0.19846517  0.0200493   0.47046804]]. Reward = [0.]
Curr episode timestep = 839
Scene graph at timestep 3545 is [True, False, False, False, False, True]
State prediction error at timestep 3545 is tensor(3.4271e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3546. State = [[-0.19442894  0.21114118]]. Action = [[ 0.07815284  0.17505127  0.09727937 -0.39520502]]. Reward = [0.]
Curr episode timestep = 840
Scene graph at timestep 3546 is [True, False, False, False, False, True]
State prediction error at timestep 3546 is tensor(5.7210e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3547. State = [[-0.19454393  0.21116246]]. Action = [[0.0177134  0.24040824 0.16182148 0.8168013 ]]. Reward = [0.]
Curr episode timestep = 841
Scene graph at timestep 3547 is [True, False, False, False, False, True]
State prediction error at timestep 3547 is tensor(3.1771e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3548. State = [[-0.1942917   0.21075152]]. Action = [[-0.22099161 -0.12932086  0.1419152  -0.02996409]]. Reward = [0.]
Curr episode timestep = 842
Scene graph at timestep 3548 is [True, False, False, False, False, True]
State prediction error at timestep 3548 is tensor(1.7214e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3549. State = [[-0.19425023  0.21046647]]. Action = [[-0.16194038 -0.13711296 -0.16587164 -0.6222291 ]]. Reward = [0.]
Curr episode timestep = 843
Scene graph at timestep 3549 is [True, False, False, False, False, True]
State prediction error at timestep 3549 is tensor(6.3993e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3550. State = [[-0.19414128  0.2103836 ]]. Action = [[ 0.11476198 -0.17324674  0.16325074 -0.29530883]]. Reward = [0.]
Curr episode timestep = 844
Scene graph at timestep 3550 is [True, False, False, False, False, True]
State prediction error at timestep 3550 is tensor(7.3861e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3550 of -1
Current timestep = 3551. State = [[-0.1941657   0.21020426]]. Action = [[ 0.01262695 -0.15882675 -0.09270081  0.2974367 ]]. Reward = [0.]
Curr episode timestep = 845
Scene graph at timestep 3551 is [True, False, False, False, False, True]
State prediction error at timestep 3551 is tensor(5.1423e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3552. State = [[-0.19407323  0.21010163]]. Action = [[-0.11379214 -0.14075695  0.05046728  0.6812587 ]]. Reward = [0.]
Curr episode timestep = 846
Scene graph at timestep 3552 is [True, False, False, False, False, True]
State prediction error at timestep 3552 is tensor(5.3945e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3553. State = [[-0.19412364  0.21007358]]. Action = [[ 0.20896614  0.10055771 -0.21677466 -0.41200006]]. Reward = [0.]
Curr episode timestep = 847
Scene graph at timestep 3553 is [True, False, False, False, False, True]
State prediction error at timestep 3553 is tensor(1.5799e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3554. State = [[-0.19407038  0.20994042]]. Action = [[-0.04383317  0.11829561 -0.1770126  -0.8021802 ]]. Reward = [0.]
Curr episode timestep = 848
Scene graph at timestep 3554 is [True, False, False, False, False, True]
State prediction error at timestep 3554 is tensor(9.5597e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3554 of -1
Current timestep = 3555. State = [[-0.19470562  0.21075948]]. Action = [[ 0.21472615  0.1812036  -0.09742275 -0.41923904]]. Reward = [0.]
Curr episode timestep = 849
Scene graph at timestep 3555 is [True, False, False, False, False, True]
State prediction error at timestep 3555 is tensor(1.1721e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3556. State = [[-0.19526416  0.2114304 ]]. Action = [[ 0.13063729 -0.071291    0.23245972 -0.727361  ]]. Reward = [0.]
Curr episode timestep = 850
Scene graph at timestep 3556 is [True, False, False, False, False, True]
State prediction error at timestep 3556 is tensor(8.0447e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3556 of -1
Current timestep = 3557. State = [[-0.19485353  0.21122721]]. Action = [[-0.21506779  0.23537931  0.06296432  0.428208  ]]. Reward = [0.]
Curr episode timestep = 851
Scene graph at timestep 3557 is [True, False, False, False, False, True]
State prediction error at timestep 3557 is tensor(1.0795e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3558. State = [[-0.19459927  0.21096927]]. Action = [[ 0.22571123  0.13688296 -0.23944241 -0.7197094 ]]. Reward = [0.]
Curr episode timestep = 852
Scene graph at timestep 3558 is [True, False, False, False, False, True]
State prediction error at timestep 3558 is tensor(4.7382e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3559. State = [[-0.19428715  0.21041442]]. Action = [[-0.15010267 -0.09706485 -0.13733083  0.92763484]]. Reward = [0.]
Curr episode timestep = 853
Scene graph at timestep 3559 is [True, False, False, False, False, True]
State prediction error at timestep 3559 is tensor(1.2189e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3560. State = [[-0.194377    0.21045922]]. Action = [[-0.1573086   0.02913204  0.03236777  0.45589554]]. Reward = [0.]
Curr episode timestep = 854
Scene graph at timestep 3560 is [True, False, False, False, False, True]
State prediction error at timestep 3560 is tensor(9.6208e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3561. State = [[-0.19390781  0.20993137]]. Action = [[-0.09585553 -0.07612312 -0.04596081 -0.46855354]]. Reward = [0.]
Curr episode timestep = 855
Scene graph at timestep 3561 is [True, False, False, False, False, True]
State prediction error at timestep 3561 is tensor(2.4697e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3562. State = [[-0.19387306  0.20979543]]. Action = [[ 0.06297788  0.17253107  0.20578787 -0.39820874]]. Reward = [0.]
Curr episode timestep = 856
Scene graph at timestep 3562 is [True, False, False, False, False, True]
State prediction error at timestep 3562 is tensor(1.4382e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3563. State = [[-0.19376135  0.20942236]]. Action = [[-0.2275664   0.00510174 -0.22448342  0.63579774]]. Reward = [0.]
Curr episode timestep = 857
Scene graph at timestep 3563 is [True, False, False, False, False, True]
State prediction error at timestep 3563 is tensor(1.9072e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3563 of -1
Current timestep = 3564. State = [[-0.19356188  0.20913817]]. Action = [[-0.14523782  0.165048    0.15932202 -0.14847386]]. Reward = [0.]
Curr episode timestep = 858
Scene graph at timestep 3564 is [True, False, False, False, False, True]
State prediction error at timestep 3564 is tensor(1.5954e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3565. State = [[-0.19361915  0.20919318]]. Action = [[ 0.16687614 -0.08443645 -0.02825047  0.4228816 ]]. Reward = [0.]
Curr episode timestep = 859
Scene graph at timestep 3565 is [True, False, False, False, False, True]
State prediction error at timestep 3565 is tensor(7.5621e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3566. State = [[-0.19356541  0.20905276]]. Action = [[-0.08349301  0.20807749 -0.22574666 -0.9344055 ]]. Reward = [0.]
Curr episode timestep = 860
Scene graph at timestep 3566 is [True, False, False, False, False, True]
State prediction error at timestep 3566 is tensor(7.3630e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3567. State = [[-0.19339173  0.20869744]]. Action = [[-0.06008482  0.20243037  0.2460429   0.4458034 ]]. Reward = [0.]
Curr episode timestep = 861
Scene graph at timestep 3567 is [True, False, False, False, False, True]
State prediction error at timestep 3567 is tensor(8.4283e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3568. State = [[-0.19334851  0.20856234]]. Action = [[-0.17709683 -0.08921254 -0.18141991 -0.26671976]]. Reward = [0.]
Curr episode timestep = 862
Scene graph at timestep 3568 is [True, False, False, False, False, True]
State prediction error at timestep 3568 is tensor(1.6202e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3568 of -1
Current timestep = 3569. State = [[-0.19333002  0.20841397]]. Action = [[ 0.05937076 -0.1680912  -0.18279305  0.22712767]]. Reward = [0.]
Curr episode timestep = 863
Scene graph at timestep 3569 is [True, False, False, False, False, True]
State prediction error at timestep 3569 is tensor(3.1276e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3570. State = [[-0.19326721  0.20821731]]. Action = [[ 0.14387181 -0.19715796 -0.22104229  0.9320986 ]]. Reward = [0.]
Curr episode timestep = 864
Scene graph at timestep 3570 is [True, False, False, False, False, True]
State prediction error at timestep 3570 is tensor(5.2061e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3571. State = [[-0.19318883  0.20803389]]. Action = [[ 0.16845828 -0.08110346 -0.10702413  0.4707706 ]]. Reward = [0.]
Curr episode timestep = 865
Scene graph at timestep 3571 is [True, False, False, False, False, True]
State prediction error at timestep 3571 is tensor(1.1739e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3572. State = [[-0.19318137  0.20803873]]. Action = [[-0.17581844 -0.21118242 -0.09303401 -0.4322636 ]]. Reward = [0.]
Curr episode timestep = 866
Scene graph at timestep 3572 is [True, False, False, False, False, True]
State prediction error at timestep 3572 is tensor(3.2034e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3573. State = [[-0.19324167  0.20807432]]. Action = [[ 0.17258143 -0.10600986 -0.097868   -0.73652536]]. Reward = [0.]
Curr episode timestep = 867
Scene graph at timestep 3573 is [True, False, False, False, False, True]
State prediction error at timestep 3573 is tensor(9.6227e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3574. State = [[-0.19319248  0.2077963 ]]. Action = [[-0.229614    0.11795604  0.1183722   0.02587306]]. Reward = [0.]
Curr episode timestep = 868
Scene graph at timestep 3574 is [True, False, False, False, False, True]
State prediction error at timestep 3574 is tensor(3.1607e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3575. State = [[-0.19307698  0.2077109 ]]. Action = [[ 0.17920843  0.17655769 -0.16104248 -0.48375487]]. Reward = [0.]
Curr episode timestep = 869
Scene graph at timestep 3575 is [True, False, False, False, False, True]
State prediction error at timestep 3575 is tensor(1.2895e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3576. State = [[-0.19307585  0.20744097]]. Action = [[-0.05339436  0.07952577 -0.11057685  0.16806233]]. Reward = [0.]
Curr episode timestep = 870
Scene graph at timestep 3576 is [True, False, False, False, False, True]
State prediction error at timestep 3576 is tensor(1.2533e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3576 of -1
Current timestep = 3577. State = [[-0.1934368   0.20785257]]. Action = [[-0.04233479  0.17581019 -0.21906672  0.20243251]]. Reward = [0.]
Curr episode timestep = 871
Scene graph at timestep 3577 is [True, False, False, False, False, True]
State prediction error at timestep 3577 is tensor(1.3763e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3578. State = [[-0.19375992  0.20848812]]. Action = [[-0.18217175 -0.21947776 -0.04085124  0.4704982 ]]. Reward = [0.]
Curr episode timestep = 872
Scene graph at timestep 3578 is [True, False, False, False, False, True]
State prediction error at timestep 3578 is tensor(5.4360e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3579. State = [[-0.19386612  0.208632  ]]. Action = [[-0.22703579  0.18584847  0.19142365 -0.3382386 ]]. Reward = [0.]
Curr episode timestep = 873
Scene graph at timestep 3579 is [True, False, False, False, False, True]
State prediction error at timestep 3579 is tensor(1.3163e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3580. State = [[-0.19400537  0.20863312]]. Action = [[-0.23700318  0.17265767 -0.03222077  0.69243133]]. Reward = [0.]
Curr episode timestep = 874
Scene graph at timestep 3580 is [True, False, False, False, False, True]
State prediction error at timestep 3580 is tensor(2.3490e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3581. State = [[-0.19402683  0.20892136]]. Action = [[-0.15042801 -0.05252844 -0.01809072 -0.4998535 ]]. Reward = [0.]
Curr episode timestep = 875
Scene graph at timestep 3581 is [True, False, False, False, False, True]
State prediction error at timestep 3581 is tensor(7.0265e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3581 of -1
Current timestep = 3582. State = [[-0.19412553  0.20884782]]. Action = [[ 0.19302225  0.11012021  0.07617316 -0.55195045]]. Reward = [0.]
Curr episode timestep = 876
Scene graph at timestep 3582 is [True, False, False, False, False, True]
State prediction error at timestep 3582 is tensor(1.0970e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3583. State = [[-0.19420683  0.20909996]]. Action = [[ 0.22872788  0.19132388 -0.02499968  0.08958507]]. Reward = [0.]
Curr episode timestep = 877
Scene graph at timestep 3583 is [True, False, False, False, False, True]
State prediction error at timestep 3583 is tensor(5.3781e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3584. State = [[-0.1943093   0.20926636]]. Action = [[ 0.23834002 -0.2284326  -0.23466112  0.8434086 ]]. Reward = [0.]
Curr episode timestep = 878
Scene graph at timestep 3584 is [True, False, False, False, False, True]
State prediction error at timestep 3584 is tensor(1.2769e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3585. State = [[-0.19432294  0.2092505 ]]. Action = [[ 0.01330078  0.18481761 -0.22198547 -0.22368157]]. Reward = [0.]
Curr episode timestep = 879
Scene graph at timestep 3585 is [True, False, False, False, False, True]
State prediction error at timestep 3585 is tensor(1.2014e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3586. State = [[-0.19429445  0.20918988]]. Action = [[ 0.06415933 -0.14218748  0.03079671  0.11752117]]. Reward = [0.]
Curr episode timestep = 880
Scene graph at timestep 3586 is [True, False, False, False, False, True]
State prediction error at timestep 3586 is tensor(1.6744e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3587. State = [[-0.19438915  0.20936063]]. Action = [[-0.1699521   0.04582348 -0.16950206 -0.4610113 ]]. Reward = [0.]
Curr episode timestep = 881
Scene graph at timestep 3587 is [True, False, False, False, False, True]
State prediction error at timestep 3587 is tensor(5.2616e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3588. State = [[-0.19445485  0.20947435]]. Action = [[-0.23525307  0.09593025 -0.0505686  -0.04005253]]. Reward = [0.]
Curr episode timestep = 882
Scene graph at timestep 3588 is [True, False, False, False, False, True]
State prediction error at timestep 3588 is tensor(6.7871e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3589. State = [[-0.19432491  0.20921616]]. Action = [[ 0.12834525 -0.10409337 -0.09009968 -0.7329506 ]]. Reward = [0.]
Curr episode timestep = 883
Scene graph at timestep 3589 is [True, False, False, False, False, True]
State prediction error at timestep 3589 is tensor(1.9666e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3589 of -1
Current timestep = 3590. State = [[-0.19372614  0.20835562]]. Action = [[ 0.1999945  -0.24054925  0.15076709  0.74151325]]. Reward = [0.]
Curr episode timestep = 884
Scene graph at timestep 3590 is [True, False, False, False, False, True]
State prediction error at timestep 3590 is tensor(6.3780e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3591. State = [[-0.19329517  0.20762987]]. Action = [[ 0.00139678 -0.2254671  -0.13126616  0.07149994]]. Reward = [0.]
Curr episode timestep = 885
Scene graph at timestep 3591 is [True, False, False, False, False, True]
State prediction error at timestep 3591 is tensor(1.7069e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3592. State = [[-0.19307625  0.20723224]]. Action = [[-0.01792683 -0.24613275  0.05719182 -0.43788707]]. Reward = [0.]
Curr episode timestep = 886
Scene graph at timestep 3592 is [True, False, False, False, False, True]
State prediction error at timestep 3592 is tensor(8.8016e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3593. State = [[-0.19292033  0.20670927]]. Action = [[-0.17713886 -0.04305989 -0.20377937  0.8965751 ]]. Reward = [0.]
Curr episode timestep = 887
Scene graph at timestep 3593 is [True, False, False, False, False, True]
State prediction error at timestep 3593 is tensor(4.3265e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3594. State = [[-0.19269583  0.20639037]]. Action = [[-0.02675314  0.23965433  0.19238809  0.5865786 ]]. Reward = [0.]
Curr episode timestep = 888
Scene graph at timestep 3594 is [True, False, False, False, False, True]
State prediction error at timestep 3594 is tensor(1.6646e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3595. State = [[-0.19237484  0.2055873 ]]. Action = [[ 0.07236087  0.03434327  0.14282304 -0.33408952]]. Reward = [0.]
Curr episode timestep = 889
Scene graph at timestep 3595 is [True, False, False, False, False, True]
State prediction error at timestep 3595 is tensor(1.6981e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3595 of -1
Current timestep = 3596. State = [[-0.1922797   0.20555854]]. Action = [[-0.23359458  0.08404982 -0.20578472 -0.9598908 ]]. Reward = [0.]
Curr episode timestep = 890
Scene graph at timestep 3596 is [True, False, False, False, False, True]
State prediction error at timestep 3596 is tensor(3.1312e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3597. State = [[-0.19216342  0.20549501]]. Action = [[ 0.14691728 -0.1867363   0.02017885 -0.61036277]]. Reward = [0.]
Curr episode timestep = 891
Scene graph at timestep 3597 is [True, False, False, False, False, True]
State prediction error at timestep 3597 is tensor(9.9337e-09, grad_fn=<MseLossBackward0>)
Current timestep = 3598. State = [[-0.19214731  0.20539723]]. Action = [[-0.07782826 -0.20866522  0.10629278 -0.5949527 ]]. Reward = [0.]
Curr episode timestep = 892
Scene graph at timestep 3598 is [True, False, False, False, False, True]
State prediction error at timestep 3598 is tensor(1.2949e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3599. State = [[-0.1920384  0.2053789]]. Action = [[-0.0609408  -0.23069136  0.19924268 -0.12329996]]. Reward = [0.]
Curr episode timestep = 893
Scene graph at timestep 3599 is [True, False, False, False, False, True]
State prediction error at timestep 3599 is tensor(1.1092e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3600. State = [[-0.19203572  0.20539054]]. Action = [[-0.1941372   0.14813769  0.23935118 -0.85966146]]. Reward = [0.]
Curr episode timestep = 894
Scene graph at timestep 3600 is [True, False, False, False, False, True]
State prediction error at timestep 3600 is tensor(1.6496e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3601. State = [[-0.19204181  0.2054204 ]]. Action = [[0.23888767 0.09679434 0.12078169 0.30527508]]. Reward = [0.]
Curr episode timestep = 895
Scene graph at timestep 3601 is [True, False, False, False, False, True]
State prediction error at timestep 3601 is tensor(2.7045e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3602. State = [[-0.19194525  0.20554264]]. Action = [[ 0.08611986 -0.18446814 -0.13857782 -0.20459926]]. Reward = [0.]
Curr episode timestep = 896
Scene graph at timestep 3602 is [True, False, False, False, False, True]
State prediction error at timestep 3602 is tensor(3.4445e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3603. State = [[-0.19181783  0.20550184]]. Action = [[-0.22281013  0.19486192  0.23545563  0.3913281 ]]. Reward = [0.]
Curr episode timestep = 897
Scene graph at timestep 3603 is [True, False, False, False, False, True]
State prediction error at timestep 3603 is tensor(1.6028e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3604. State = [[-0.1918244   0.20525569]]. Action = [[ 0.1560877  -0.08799881  0.02309638  0.45832753]]. Reward = [0.]
Curr episode timestep = 898
Scene graph at timestep 3604 is [True, False, False, False, False, True]
State prediction error at timestep 3604 is tensor(3.8877e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3605. State = [[-0.19171853  0.20540355]]. Action = [[-0.10513504  0.14346272 -0.04242246  0.47230613]]. Reward = [0.]
Curr episode timestep = 899
Scene graph at timestep 3605 is [True, False, False, False, False, True]
State prediction error at timestep 3605 is tensor(3.6615e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3606. State = [[-0.19175015  0.20530078]]. Action = [[-0.22279449  0.22509909  0.05646849  0.9052968 ]]. Reward = [0.]
Curr episode timestep = 900
Scene graph at timestep 3606 is [True, False, False, False, False, True]
State prediction error at timestep 3606 is tensor(3.1377e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3607. State = [[-0.25758055  0.00824798]]. Action = [[ 0.02271613 -0.22900203  0.05539775 -0.7509018 ]]. Reward = [0.]
Curr episode timestep = 901
Scene graph at timestep 3607 is [True, False, False, False, True, False]
State prediction error at timestep 3607 is tensor(0.0218, grad_fn=<MseLossBackward0>)
Current timestep = 3608. State = [[-0.25566936  0.01036926]]. Action = [[-0.03892159 -0.09377226 -0.14302963  0.62258637]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 3608 is [True, False, False, False, True, False]
State prediction error at timestep 3608 is tensor(6.6975e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3609. State = [[-0.25377765  0.01243562]]. Action = [[-0.20761804  0.13465554  0.21715325  0.22989893]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 3609 is [True, False, False, False, True, False]
State prediction error at timestep 3609 is tensor(2.6045e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3610. State = [[-0.25058135  0.01684207]]. Action = [[ 0.22269231 -0.06327097 -0.22660309 -0.36362934]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 3610 is [True, False, False, False, True, False]
State prediction error at timestep 3610 is tensor(1.3869e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3611. State = [[-0.24752693  0.02248614]]. Action = [[-0.16446607  0.19744462 -0.06738205  0.4306321 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3611 is [True, False, False, False, True, False]
State prediction error at timestep 3611 is tensor(3.7489e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3612. State = [[-0.24438949  0.02980597]]. Action = [[ 0.13564444  0.20889568  0.19805896 -0.73052144]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 3612 is [True, False, False, False, True, False]
State prediction error at timestep 3612 is tensor(3.0375e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3613. State = [[-0.23019534  0.08556431]]. Action = [[ 0.05721796  0.0295749   0.03589609 -0.84967697]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 3613 is [True, False, False, False, True, False]
State prediction error at timestep 3613 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 3614. State = [[-0.22827712  0.09467   ]]. Action = [[ 0.10006663 -0.17823125  0.14011806 -0.44616997]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 3614 is [True, False, False, False, True, False]
State prediction error at timestep 3614 is tensor(2.6986e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3615. State = [[-0.22481994  0.10653955]]. Action = [[0.0609259  0.1757527  0.16851631 0.7868686 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 3615 is [True, False, False, False, True, False]
State prediction error at timestep 3615 is tensor(7.6173e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3616. State = [[-0.21084864  0.14443721]]. Action = [[ 0.06180742 -0.12919645  0.02391872 -0.08003604]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 3616 is [True, False, False, False, False, True]
State prediction error at timestep 3616 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 3617. State = [[-0.20766403  0.14888722]]. Action = [[ 0.23703    -0.1718351   0.19667047 -0.33043265]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 3617 is [True, False, False, False, False, True]
State prediction error at timestep 3617 is tensor(1.5644e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3618. State = [[-0.19709973  0.16276373]]. Action = [[ 0.12463215  0.07315728  0.14252934 -0.71630317]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 3618 is [True, False, False, False, False, True]
State prediction error at timestep 3618 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 3619. State = [[-0.1955388  0.1645483]]. Action = [[ 0.2072488   0.08712956 -0.203521   -0.5745636 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 3619 is [True, False, False, False, False, True]
State prediction error at timestep 3619 is tensor(3.1941e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3620. State = [[-0.18801045  0.17433071]]. Action = [[-0.04767632 -0.07094353 -0.21651666 -0.9299328 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 3620 is [True, False, False, False, False, True]
State prediction error at timestep 3620 is tensor(6.8272e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3621. State = [[-0.18335144  0.17841919]]. Action = [[ 0.03072581 -0.10093527  0.14373356  0.42790163]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 3621 is [True, False, False, False, False, True]
State prediction error at timestep 3621 is tensor(1.8941e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3622. State = [[-0.18241757  0.17807439]]. Action = [[ 0.07410389 -0.15699095 -0.08507842 -0.73391294]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 3622 is [True, False, False, False, False, True]
State prediction error at timestep 3622 is tensor(3.9151e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3623. State = [[-0.18143158  0.17768626]]. Action = [[-0.23417091 -0.15837243 -0.20921108  0.01591945]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 3623 is [True, False, False, False, False, True]
State prediction error at timestep 3623 is tensor(1.5688e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3624. State = [[-0.18116997  0.17762528]]. Action = [[ 0.15158367 -0.04023392 -0.16690959  0.21723354]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 3624 is [True, False, False, False, False, True]
State prediction error at timestep 3624 is tensor(3.5911e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3625. State = [[-0.18100014  0.17777339]]. Action = [[ 0.13571352 -0.1593169  -0.07214794 -0.7363698 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 3625 is [True, False, False, False, False, True]
State prediction error at timestep 3625 is tensor(7.5445e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3626. State = [[-0.18109448  0.17782593]]. Action = [[-0.05232349  0.23619866 -0.17395198 -0.8497927 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 3626 is [True, False, False, False, False, True]
State prediction error at timestep 3626 is tensor(3.5034e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3627. State = [[-0.18013456  0.17774375]]. Action = [[ 4.6932697e-04  2.1997029e-01  2.4869865e-01 -5.4462534e-01]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 3627 is [True, False, False, False, False, True]
State prediction error at timestep 3627 is tensor(2.4664e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3628. State = [[-0.1797401   0.17744634]]. Action = [[ 0.2046836  -0.19536667  0.0848543   0.908648  ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 3628 is [True, False, False, False, False, True]
State prediction error at timestep 3628 is tensor(1.6138e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3629. State = [[-0.17925717  0.17782101]]. Action = [[-0.17574674 -0.15470971  0.10018969 -0.00613183]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 3629 is [True, False, False, False, False, True]
State prediction error at timestep 3629 is tensor(1.1353e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3630. State = [[-0.17877091  0.17808153]]. Action = [[-0.12182081  0.24133748 -0.18007116 -0.81517595]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 3630 is [True, False, False, False, False, True]
State prediction error at timestep 3630 is tensor(4.3395e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3631. State = [[-0.17844757  0.17782377]]. Action = [[-0.17468633 -0.17702794  0.03542766  0.0995717 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 3631 is [True, False, False, False, False, True]
State prediction error at timestep 3631 is tensor(2.0488e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3632. State = [[-0.17799713  0.17770536]]. Action = [[ 0.05299449  0.19494987 -0.21072221 -0.82983327]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 3632 is [True, False, False, False, False, True]
State prediction error at timestep 3632 is tensor(2.8670e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3633. State = [[-0.17722254  0.17816098]]. Action = [[-0.20056137 -0.08247824  0.04701185 -0.6420163 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 3633 is [True, False, False, False, False, True]
State prediction error at timestep 3633 is tensor(4.8405e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3634. State = [[-0.17704672  0.17835145]]. Action = [[-0.16993187  0.19161564  0.03879562  0.44233656]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 3634 is [True, False, False, False, False, True]
State prediction error at timestep 3634 is tensor(3.6885e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3635. State = [[-0.17639291  0.17853315]]. Action = [[-0.04182126  0.2165192  -0.099255    0.32552934]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 3635 is [True, False, False, False, False, True]
State prediction error at timestep 3635 is tensor(1.0480e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3636. State = [[-0.17585017  0.1785766 ]]. Action = [[-0.17712831 -0.1996257  -0.16367349  0.6704694 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 3636 is [True, False, False, False, False, True]
State prediction error at timestep 3636 is tensor(6.5607e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3637. State = [[-0.17541127  0.1786527 ]]. Action = [[ 0.15374833 -0.0457876   0.16558355  0.5142715 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 3637 is [True, False, False, False, False, True]
State prediction error at timestep 3637 is tensor(4.1075e-06, grad_fn=<MseLossBackward0>)
