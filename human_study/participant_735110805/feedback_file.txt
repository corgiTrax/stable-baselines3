Current timestep = 0. State = [[-0.25648904  0.00764321]]. Action = [[-0.06292832 -0.00783072  0.21038541 -0.03610903]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Current timestep = 1. State = [[-0.25647798  0.00757334]]. Action = [[ 0.02804708  0.09211323 -0.17953753 -0.24135602]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0394, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.256745    0.00941847]]. Action = [[-0.15283313  0.10317436  0.08250085  0.97395396]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
Current timestep = 3. State = [[-0.25866908  0.01336706]]. Action = [[-0.15658152  0.18879378 -0.05679892 -0.21903908]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0419, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.2626386   0.02014536]]. Action = [[-0.06515831 -0.21350987 -0.2270246   0.91687274]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
Current timestep = 5. State = [[-0.26628724  0.02049333]]. Action = [[-0.19404647  0.20411986 -0.19760707 -0.8448607 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
Current timestep = 6. State = [[-0.27174187  0.02494621]]. Action = [[ 0.24527463 -0.10609299 -0.19202721 -0.2252906 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
Current timestep = 7. State = [[-0.27260363  0.02528753]]. Action = [[-0.13173239  0.19834805 -0.12744653  0.7123525 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0533, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.2751102   0.03105901]]. Action = [[ 0.18874025  0.10747921 -0.21657972 -0.08161724]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
Current timestep = 9. State = [[-0.27586836  0.03593986]]. Action = [[-0.17556798 -0.14458303  0.2087945   0.24431825]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0459, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.27691403  0.035999  ]]. Action = [[ 0.16702765 -0.20503464 -0.198931   -0.67114556]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0355, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.27613658  0.02932342]]. Action = [[-0.02317014 -0.1613437   0.12703657 -0.9684069 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
Current timestep = 12. State = [[-0.2760692   0.02364656]]. Action = [[-0.02471386 -0.24597779  0.20099166 -0.614913  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0271, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.27531767  0.0136586 ]]. Action = [[ 0.22643995 -0.13567218  0.19493079  0.24568582]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0447, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.27245072  0.00320714]]. Action = [[ 0.18065628  0.03531045 -0.23489961 -0.4160719 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
Human Feedback received at timestep 14 of -1
Current timestep = 15. State = [[-0.26774865 -0.00212229]]. Action = [[-0.1477273  -0.12928894 -0.13149224  0.7692232 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
Current timestep = 16. State = [[-0.2666453  -0.00844091]]. Action = [[-0.22118598  0.18397707 -0.03226592  0.7894316 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0483, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of -1
Current timestep = 17. State = [[-0.2683833  -0.00843474]]. Action = [[ 0.01967832  0.2151779  -0.18030106 -0.33679414]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
Current timestep = 18. State = [[-0.26977336 -0.00385975]]. Action = [[-0.08254196  0.20234978 -0.12120283 -0.731575  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
Human Feedback received at timestep 18 of 1
Current timestep = 19. State = [[-0.27248675  0.00335936]]. Action = [[ 0.00782257 -0.09399566  0.12785363 -0.25821555]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0355, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.27409703  0.00639793]]. Action = [[ 0.15812397  0.14410222 -0.09557876  0.49787724]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
Current timestep = 21. State = [[-0.27433336  0.01065545]]. Action = [[ 0.14182323  0.01168245 -0.18392065  0.501621  ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
Current timestep = 22. State = [[-0.27282336  0.01382241]]. Action = [[ 0.1688351   0.24548686  0.09852752 -0.9514527 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.26907912  0.02212223]]. Action = [[-0.05702102  0.17219979  0.14208743  0.03624845]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0472, grad_fn=<MseLossBackward0>)
Current timestep = 24. State = [[-0.26653564  0.03246795]]. Action = [[ 0.08372721  0.24889523  0.13376862 -0.682824  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
Current timestep = 25. State = [[-0.26280156  0.04449267]]. Action = [[-0.11555925 -0.20196092  0.24720162  0.63734305]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
Current timestep = 26. State = [[-0.26147544  0.04778228]]. Action = [[ 0.11335847  0.04647794 -0.01763617 -0.01341397]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0455, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.259079    0.05063232]]. Action = [[-0.1318131  -0.15813036  0.01346835 -0.01878482]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0393, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 27 of 1
Current timestep = 28. State = [[-0.2588972   0.05035243]]. Action = [[-0.18267702  0.03736448 -0.0018138  -0.91120285]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
Current timestep = 29. State = [[-0.2599163   0.05129492]]. Action = [[ 0.12679619  0.13841623 -0.04986307 -0.4208697 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
Current timestep = 30. State = [[-0.26126614  0.05458924]]. Action = [[-0.0342588   0.1584205  -0.23644787 -0.95380545]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
Current timestep = 31. State = [[-0.2632777   0.05971106]]. Action = [[-0.05987707  0.03214163 -0.05190513  0.84736323]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
Current timestep = 32. State = [[-0.26537383  0.0641169 ]]. Action = [[ 0.00109294 -0.17708966 -0.18974707 -0.4909749 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
Current timestep = 33. State = [[-0.26523507  0.0636196 ]]. Action = [[-0.19922154 -0.19781938  0.21593627 -0.9374181 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
Human Feedback received at timestep 33 of 1
Current timestep = 34. State = [[-0.2667578   0.05977789]]. Action = [[ 0.05743226  0.03883946 -0.23645777  0.09854746]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
Current timestep = 35. State = [[-0.2673447  0.0574484]]. Action = [[-0.0594888  -0.08989099  0.08439064  0.6699455 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
Current timestep = 36. State = [[-0.26802063  0.05421679]]. Action = [[ 0.23280835 -0.02098373  0.16651341  0.02484822]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
Current timestep = 37. State = [[-0.26759604  0.05088225]]. Action = [[ 0.16575176 -0.11865516  0.24713773  0.314659  ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
Current timestep = 38. State = [[-0.26488978  0.04612036]]. Action = [[-0.08253792 -0.17259337 -0.16319866 -0.56067777]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
Current timestep = 39. State = [[-0.2632028   0.03910293]]. Action = [[-0.0535952  -0.0970277  -0.09475388  0.5556381 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
Current timestep = 40. State = [[-0.26251444  0.03312178]]. Action = [[-0.22650304  0.23190928 -0.19892998  0.32077074]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[-0.26461264  0.03360163]]. Action = [[ 0.10872632 -0.09703131  0.20763269 -0.17703742]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0367, grad_fn=<MseLossBackward0>)
Current timestep = 42. State = [[-0.26498187  0.03157225]]. Action = [[-0.09357411  0.01792291  0.154369    0.46081853]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0477, grad_fn=<MseLossBackward0>)
Current timestep = 43. State = [[-0.26600417  0.03056728]]. Action = [[ 0.15594012  0.13645342  0.0622842  -0.7390872 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
Human Feedback received at timestep 43 of -1
Current timestep = 44. State = [[-0.26636493  0.03204958]]. Action = [[-0.02267635  0.18309161  0.1395728  -0.0730992 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0459, grad_fn=<MseLossBackward0>)
Current timestep = 45. State = [[-0.26762602  0.03725981]]. Action = [[ 0.11342704 -0.04019237  0.05631548 -0.51758724]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0332, grad_fn=<MseLossBackward0>)
Current timestep = 46. State = [[-0.2668591   0.03936535]]. Action = [[0.09529731 0.10571244 0.22975743 0.02757192]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0451, grad_fn=<MseLossBackward0>)
Current timestep = 47. State = [[-0.2644168   0.04254028]]. Action = [[-0.03294587 -0.06545065  0.09615448  0.755185  ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
Current timestep = 48. State = [[-0.26330858  0.04343143]]. Action = [[-0.1157887   0.05567047 -0.20120142  0.37288344]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
Human Feedback received at timestep 48 of 1
Current timestep = 49. State = [[-0.26415813  0.04601948]]. Action = [[ 0.1134553   0.07748425 -0.20218235  0.8847685 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
Current timestep = 50. State = [[-0.2642665   0.04833408]]. Action = [[-0.07827386 -0.20051333 -0.12017211 -0.02700591]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
Current timestep = 51. State = [[-0.26352164  0.04774072]]. Action = [[ 0.2068848   0.22652376 -0.03586359  0.1242373 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0532, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 51 of 1
Current timestep = 52. State = [[-0.26109403  0.05018663]]. Action = [[ 0.18189603 -0.20764373  0.18816039  0.5712166 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
Current timestep = 53. State = [[-0.2557065   0.04909964]]. Action = [[-0.11441588 -0.05674538 -0.12370075 -0.8974389 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
Current timestep = 54. State = [[-0.2536044   0.04751193]]. Action = [[-0.12237573 -0.2230823   0.20830828  0.16367233]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
Human Feedback received at timestep 54 of 1
Current timestep = 55. State = [[-0.25250104  0.04066461]]. Action = [[-0.08687785 -0.17729017 -0.08210889  0.7207265 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
Current timestep = 56. State = [[-0.25274485  0.03300487]]. Action = [[ 0.05953512  0.11114797 -0.14817959  0.91396666]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0426, grad_fn=<MseLossBackward0>)
Current timestep = 57. State = [[-0.2526802   0.02906548]]. Action = [[ 0.23208168 -0.15677391  0.2330029  -0.27012765]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
Current timestep = 58. State = [[-0.2503574   0.02293028]]. Action = [[-0.01688263 -0.17942126 -0.09049878  0.02913284]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0377, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 58 of 1
Current timestep = 59. State = [[-0.24803749  0.01499521]]. Action = [[-0.22462757  0.22966427  0.03413746 -0.01006943]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
Current timestep = 60. State = [[-0.2485651   0.01584345]]. Action = [[ 0.08144125  0.15033007 -0.04957311  0.36233544]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
Current timestep = 61. State = [[-0.2489033   0.01831873]]. Action = [[ 0.21871018 -0.00888601  0.11972737  0.6086904 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
Human Feedback received at timestep 61 of 1
Current timestep = 62. State = [[-0.24705943  0.0190604 ]]. Action = [[-0.015443   -0.1623418  -0.07818675 -0.59949154]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
Current timestep = 63. State = [[-0.24518682  0.01739124]]. Action = [[ 0.09055582  0.1572401   0.16502541 -0.9037737 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
Current timestep = 64. State = [[-0.24273421  0.01916648]]. Action = [[-0.11106697  0.02211285 -0.24506785 -0.55001134]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
Current timestep = 65. State = [[-0.24257953  0.02078736]]. Action = [[-0.18644704  0.01333836  0.17273095 -0.73126566]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
Human Feedback received at timestep 65 of 1
Current timestep = 66. State = [[-0.24326083  0.02156341]]. Action = [[ 0.07175013 -0.1642149  -0.24593687 -0.78596085]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(0.0256, grad_fn=<MseLossBackward0>)
Current timestep = 67. State = [[-0.24275172  0.01931151]]. Action = [[ 0.24854136 -0.08151072  0.20405278  0.7270007 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0358, grad_fn=<MseLossBackward0>)
Current timestep = 68. State = [[-0.24032125  0.01682162]]. Action = [[0.03160676 0.21968237 0.19586185 0.43771553]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
Current timestep = 69. State = [[-0.237918    0.01869032]]. Action = [[-0.20742673 -0.13721892  0.14055955  0.3013432 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
Current timestep = 70. State = [[-0.23827623  0.01795807]]. Action = [[-0.03636354 -0.05990322 -0.18529083  0.02888215]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0383, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of 1
Current timestep = 71. State = [[-0.23822372  0.01623364]]. Action = [[ 0.01221383 -0.01296236  0.090105   -0.54344183]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
Current timestep = 72. State = [[-0.23828782  0.01473372]]. Action = [[-0.16936138 -0.06833398 -0.01987658  0.9019809 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
Current timestep = 73. State = [[-0.24024312  0.01275984]]. Action = [[ 0.04703778  0.2433297   0.10549861 -0.14095664]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
Current timestep = 74. State = [[-0.24172251  0.01583962]]. Action = [[-0.04591274 -0.21344574  0.05896783 -0.5593137 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
Human Feedback received at timestep 74 of 1
Current timestep = 75. State = [[-0.24267463  0.01275566]]. Action = [[-0.11358589 -0.18174565  0.24147367 -0.39439988]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
Current timestep = 76. State = [[-0.24520609  0.00693894]]. Action = [[-9.40460265e-02  4.54038382e-04 -1.00999266e-01  7.13565111e-01]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
Human Feedback received at timestep 76 of 1
Current timestep = 77. State = [[-0.24875715  0.00240996]]. Action = [[-0.03371632 -0.19152136 -0.07343674 -0.311545  ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
Current timestep = 78. State = [[-0.25279424 -0.00577181]]. Action = [[ 0.022044   -0.2426457   0.10017937  0.51455307]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
Current timestep = 79. State = [[-0.25510266 -0.0162579 ]]. Action = [[-0.2102649  -0.13816953 -0.21217577 -0.64967614]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0261, grad_fn=<MseLossBackward0>)
Current timestep = 80. State = [[-0.26043218 -0.02801861]]. Action = [[-0.0068455  -0.16103208 -0.06041899 -0.23225856]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
Current timestep = 81. State = [[-0.2636567  -0.03877045]]. Action = [[ 0.13097224 -0.0734749  -0.16514692 -0.47577   ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
Current timestep = 82. State = [[-0.26450372 -0.04701849]]. Action = [[-0.202117    0.23224825 -0.12801798 -0.83660877]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
Current timestep = 83. State = [[-0.2673782  -0.04782117]]. Action = [[ 0.2389484   0.02112851  0.07512534 -0.7967107 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
Current timestep = 84. State = [[-0.26706073 -0.0477092 ]]. Action = [[ 0.02619484 -0.00192858 -0.2153943  -0.9035846 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
Current timestep = 85. State = [[-0.2664285  -0.04753654]]. Action = [[ 0.09412518  0.08068794 -0.08480367 -0.30874026]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(0.0367, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.26568368 -0.04667308]]. Action = [[-0.04189269 -0.13539053  0.167485   -0.28540534]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
Current timestep = 87. State = [[-0.26489192 -0.04801717]]. Action = [[-0.13219015 -0.04103981  0.03815037 -0.12867564]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
Current timestep = 88. State = [[-0.26496488 -0.0496446 ]]. Action = [[-0.00802706 -0.01777503 -0.12589616  0.3568728 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0484, grad_fn=<MseLossBackward0>)
Current timestep = 89. State = [[-0.26546165 -0.05165752]]. Action = [[-1.2616846e-01 -2.2613351e-01  7.9822540e-04  9.8299289e-01]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0421, grad_fn=<MseLossBackward0>)
Current timestep = 90. State = [[-0.26821813 -0.05822058]]. Action = [[-0.17308962  0.20423535 -0.04385433  0.30456305]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(0.0498, grad_fn=<MseLossBackward0>)
Current timestep = 91. State = [[-0.27338842 -0.05823426]]. Action = [[-0.24324673  0.13268635 -0.24387646 -0.79303753]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
Current timestep = 92. State = [[-0.28221044 -0.05579638]]. Action = [[ 0.05326334 -0.16724561  0.03834906 -0.19529057]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
Current timestep = 93. State = [[-0.287963  -0.0571472]]. Action = [[ 0.00920275  0.15086064 -0.07823598  0.37502968]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
Human Feedback received at timestep 93 of -1
Current timestep = 94. State = [[-0.29124257 -0.05571673]]. Action = [[0.07915059 0.01438561 0.04716134 0.10213578]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0515, grad_fn=<MseLossBackward0>)
Current timestep = 95. State = [[-0.2913728  -0.05466923]]. Action = [[-0.07474667  0.10044095 -0.15577957 -0.81921107]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
Current timestep = 96. State = [[-0.2927605  -0.05107919]]. Action = [[-0.02428597  0.07535601 -0.1852176  -0.04370892]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
Current timestep = 97. State = [[-0.29401112 -0.04705255]]. Action = [[ 0.20194349  0.10286158  0.20833257 -0.6593675 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
Current timestep = 98. State = [[-0.2944333  -0.04218434]]. Action = [[-0.09913734  0.04422888  0.02979702  0.92147255]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
Human Feedback received at timestep 98 of -1
Current timestep = 99. State = [[-0.2954295  -0.03825232]]. Action = [[-0.09856635  0.0674566  -0.21167783 -0.16999316]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
Current timestep = 100. State = [[-0.29664722 -0.03381706]]. Action = [[-0.15079565 -0.21940632 -0.02369845  0.11940181]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(0.0489, grad_fn=<MseLossBackward0>)
Current timestep = 101. State = [[-0.29920453 -0.03531531]]. Action = [[-0.22904678 -0.02748613  0.116041    0.28773856]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
Human Feedback received at timestep 101 of -1
Current timestep = 102. State = [[-0.3047047  -0.03651181]]. Action = [[0.09275281 0.10883743 0.06741947 0.2768674 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
Current timestep = 103. State = [[-0.30986863 -0.03528089]]. Action = [[ 0.22295952 -0.02079807 -0.20881678 -0.28873467]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
Current timestep = 104. State = [[-0.31027257 -0.03532858]]. Action = [[-0.10683371 -0.1037007   0.21886036 -0.7239744 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
Current timestep = 105. State = [[-0.31122795 -0.03691472]]. Action = [[-0.0370544  -0.08820483 -0.06997551 -0.9768368 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
Current timestep = 106. State = [[-0.31247732 -0.03953112]]. Action = [[ 0.20893937 -0.03371015  0.20521957 -0.11461991]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0500, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 106 of -1
Current timestep = 107. State = [[-0.31117782 -0.04180107]]. Action = [[-0.02727693 -0.0242108   0.22461662 -0.08456141]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
Current timestep = 108. State = [[-0.31011108 -0.04379379]]. Action = [[-0.03981924 -0.0557791  -0.02126198 -0.52216256]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
Current timestep = 109. State = [[-0.30967754 -0.04594097]]. Action = [[ 0.14718503  0.10361809 -0.11176175 -0.8177222 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
Human Feedback received at timestep 109 of -1
Current timestep = 110. State = [[-0.30829513 -0.04555042]]. Action = [[ 0.20218009  0.0075199   0.15887168 -0.9461646 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
Current timestep = 111. State = [[-0.30454606 -0.04489692]]. Action = [[ 0.20505464  0.19873732 -0.13456756  0.9906614 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
Current timestep = 112. State = [[-0.298171  -0.0416916]]. Action = [[ 0.2132181  -0.03087531  0.17487937 -0.96350336]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
Human Feedback received at timestep 112 of -1
Current timestep = 113. State = [[-0.28873876 -0.03978142]]. Action = [[-0.1854473   0.08423623  0.15571609  0.84478045]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0514, grad_fn=<MseLossBackward0>)
Current timestep = 114. State = [[-0.2845026  -0.03697834]]. Action = [[ 0.17291403  0.24345651 -0.08439955  0.2454791 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
Current timestep = 115. State = [[-0.2792384  -0.02842757]]. Action = [[-0.02897206  0.20259047  0.20263237 -0.38037252]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
Human Feedback received at timestep 115 of -1
Current timestep = 116. State = [[-0.27619916 -0.01914022]]. Action = [[ 0.08030078 -0.07574013 -0.17948721  0.67524683]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
Current timestep = 117. State = [[-0.27279195 -0.01418861]]. Action = [[0.14925262 0.21274069 0.03794622 0.4118315 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
Current timestep = 118. State = [[-0.26787218 -0.006241  ]]. Action = [[-0.0137002   0.07842419  0.22548386  0.8506783 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
Human Feedback received at timestep 118 of 1
Current timestep = 119. State = [[-0.26320633  0.00127489]]. Action = [[ 0.18740013  0.12822524  0.15111178 -0.5467224 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0303, grad_fn=<MseLossBackward0>)
Current timestep = 120. State = [[-0.2560927   0.00937116]]. Action = [[ 0.03344995 -0.03253202 -0.18300799 -0.27900124]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
Current timestep = 121. State = [[-0.25054997  0.01413597]]. Action = [[-0.1797661   0.15656316  0.21257979 -0.14114434]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
Current timestep = 122. State = [[-0.24978082  0.02089033]]. Action = [[ 0.21754336 -0.03369781 -0.02604298  0.5711534 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
Human Feedback received at timestep 122 of 1
Current timestep = 123. State = [[-0.24629268  0.02387907]]. Action = [[-0.04699215 -0.08934003  0.03357944  0.10692334]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
Current timestep = 124. State = [[-0.24365658  0.02433158]]. Action = [[ 0.04003665 -0.05808204  0.1481753   0.21763086]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0389, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.24188457  0.0242075 ]]. Action = [[-0.1978653   0.02526319 -0.0407638  -0.05132365]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.24211285  0.02464852]]. Action = [[ 0.17063957 -0.01758155  0.12360758 -0.04624939]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(0.0351, grad_fn=<MseLossBackward0>)
Current timestep = 127. State = [[-0.24148174  0.02470941]]. Action = [[-0.11885005  0.01701701 -0.03999653  0.01879144]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
Current timestep = 128. State = [[-0.24178785  0.02484235]]. Action = [[-0.24058092  0.05086353 -0.04525606  0.18736851]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
Human Feedback received at timestep 128 of 1
Current timestep = 129. State = [[-0.24478558  0.02717611]]. Action = [[-0.24276508  0.19436371 -0.2091362  -0.909334  ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
Current timestep = 130. State = [[-0.25216073  0.03256977]]. Action = [[-0.212147   -0.20059066  0.02537453  0.97973585]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
Current timestep = 131. State = [[-0.26094076  0.03245009]]. Action = [[ 0.06131846  0.21765405  0.14906627 -0.33817333]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0391, grad_fn=<MseLossBackward0>)
Current timestep = 132. State = [[-0.2677688   0.03684483]]. Action = [[-3.9282441e-04 -1.8119034e-01 -6.9955885e-02 -9.3395925e-01]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
Current timestep = 133. State = [[-0.270834    0.03547338]]. Action = [[ 0.04398781 -0.1731964   0.03767967  0.3322369 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
Current timestep = 134. State = [[-0.27147093  0.03010775]]. Action = [[ 0.20921946 -0.22330736 -0.07237035 -0.1830532 ]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0403, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of -1
Current timestep = 135. State = [[-0.26974595  0.0220782 ]]. Action = [[-0.10398477  0.06595671 -0.08796173 -0.263579  ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
Current timestep = 136. State = [[-0.26970425  0.01866415]]. Action = [[ 0.10536161 -0.0464419   0.116898   -0.5455249 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0306, grad_fn=<MseLossBackward0>)
Current timestep = 137. State = [[-0.26918864  0.01509801]]. Action = [[ 0.11343995  0.10054204 -0.23627533 -0.8251441 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
Human Feedback received at timestep 137 of -1
Current timestep = 138. State = [[-0.26770923  0.01433502]]. Action = [[-0.02240205 -0.21621992  0.01480576 -0.6011182 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
Current timestep = 139. State = [[-0.26576307  0.0086665 ]]. Action = [[ 0.00360641 -0.24350345 -0.21222901  0.4732709 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0450, grad_fn=<MseLossBackward0>)
Current timestep = 140. State = [[-2.6349288e-01  1.9378524e-04]]. Action = [[ 0.21982399  0.1689884  -0.12315628  0.75975275]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
Human Feedback received at timestep 140 of -1
Current timestep = 141. State = [[-0.2597833  -0.00114828]]. Action = [[ 0.1033833   0.22558865 -0.03103516 -0.06988275]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
Current timestep = 142. State = [[-0.25604102  0.00217151]]. Action = [[-0.0604167   0.17406571  0.14437044  0.17399645]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
Current timestep = 143. State = [[-0.2540053   0.00849665]]. Action = [[ 0.10276279  0.22698045 -0.21996301 -0.52428895]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
Current timestep = 144. State = [[-0.25079334  0.01649003]]. Action = [[ 0.15435696 -0.02193694 -0.171608   -0.70893675]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
Human Feedback received at timestep 144 of 1
Current timestep = 145. State = [[-0.24453345  0.02098176]]. Action = [[ 0.21616817 -0.06358007 -0.03994004  0.4767716 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0402, grad_fn=<MseLossBackward0>)
Current timestep = 146. State = [[-0.23511782  0.02274006]]. Action = [[ 0.00769588 -0.09368058 -0.1350201   0.24625003]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
Current timestep = 147. State = [[-0.22828227  0.02296922]]. Action = [[-0.18548284  0.04960519  0.105371   -0.9420088 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
Current timestep = 148. State = [[-0.2269156   0.02398704]]. Action = [[-0.05987763  0.03108501 -0.16109483 -0.21106172]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0323, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of 1
Current timestep = 149. State = [[-0.22744349  0.02553393]]. Action = [[-0.04991546  0.1909563   0.15661633 -0.31137848]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, True, False]
Current timestep = 150. State = [[-0.22905141  0.03061412]]. Action = [[ 0.01599884 -0.11815986 -0.0712131  -0.23423773]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, True, False]
Current timestep = 151. State = [[-0.22942673  0.03180144]]. Action = [[-0.08839943  0.19495249  0.03480902 -0.5698542 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, True, False]
Current timestep = 152. State = [[-0.23157616  0.03738918]]. Action = [[-0.21367085 -0.1041984   0.20831594 -0.30824268]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, True, False]
Human Feedback received at timestep 152 of 1
Current timestep = 153. State = [[-0.23402658  0.03896819]]. Action = [[-0.04773346 -0.14984342 -0.06717011 -0.49667573]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, True, False]
Current timestep = 154. State = [[-0.23709278  0.03599944]]. Action = [[-0.04414341 -0.2276486   0.2119298   0.677847  ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0370, grad_fn=<MseLossBackward0>)
Current timestep = 155. State = [[-0.24076527  0.02900498]]. Action = [[-0.16135721 -0.07142517 -0.20095442 -0.58711743]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0288, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of -1
Current timestep = 156. State = [[-0.24591757  0.0219971 ]]. Action = [[ 0.24078846 -0.1651571   0.21897483  0.2608112 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
Human Feedback received at timestep 156 of -1
Current timestep = 157. State = [[-0.24560694  0.01425293]]. Action = [[-0.14201003 -0.1747597  -0.1260021   0.16281736]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
Current timestep = 158. State = [[-0.2471106   0.00467389]]. Action = [[-0.04772323 -0.1936308   0.18407002 -0.32034814]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0257, grad_fn=<MseLossBackward0>)
Current timestep = 159. State = [[-0.24903135 -0.00632683]]. Action = [[ 0.20126098 -0.18334974 -0.09023497  0.94406533]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0342, grad_fn=<MseLossBackward0>)
Current timestep = 160. State = [[-0.24762554 -0.0188169 ]]. Action = [[ 0.06107664 -0.01960391  0.06645605 -0.8740796 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0189, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of -1
Current timestep = 161. State = [[-0.24530925 -0.02695782]]. Action = [[0.19856533 0.08630535 0.1669032  0.36968005]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
Current timestep = 162. State = [[-0.24189302 -0.02987982]]. Action = [[-0.02202916 -0.0201029   0.1141777  -0.33206046]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0272, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.23968834 -0.03234162]]. Action = [[-0.08309558  0.14805996 -0.175633    0.6142738 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
Current timestep = 164. State = [[-0.23961002 -0.03099023]]. Action = [[ 0.08665794  0.09695548 -0.22014058 -0.14046508]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
Current timestep = 165. State = [[-0.23849837 -0.02797333]]. Action = [[ 0.2477481   0.14907333 -0.16502199 -0.28255308]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0324, grad_fn=<MseLossBackward0>)
Current timestep = 166. State = [[-0.23343533 -0.02294624]]. Action = [[ 0.08860108  0.03889459  0.03688529 -0.71008694]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0202, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of -1
Current timestep = 167. State = [[-0.2280223  -0.01873402]]. Action = [[ 0.15250719 -0.04388523 -0.20775358 -0.9477239 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
Current timestep = 168. State = [[-0.22074775 -0.01616777]]. Action = [[ 0.13893068  0.17110065 -0.16432643  0.3619163 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0384, grad_fn=<MseLossBackward0>)
Current timestep = 169. State = [[-0.21117705 -0.01106884]]. Action = [[ 0.21873432 -0.2376918   0.13337493 -0.6846394 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0135, grad_fn=<MseLossBackward0>)
Current timestep = 170. State = [[-0.20020998 -0.01237851]]. Action = [[ 0.00389519  0.1709145  -0.05306125 -0.57995296]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
Human Feedback received at timestep 170 of 1
Current timestep = 171. State = [[-0.19358121 -0.0100257 ]]. Action = [[ 0.12400207  0.0537281   0.10981166 -0.50004685]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
Current timestep = 172. State = [[-0.18603057 -0.00662321]]. Action = [[-0.1143738   0.15419835 -0.00956962 -0.32204366]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
Current timestep = 173. State = [[-0.18206158 -0.00202952]]. Action = [[-0.10765925 -0.24071933  0.11024138  0.6125511 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
Human Feedback received at timestep 173 of 1
Current timestep = 174. State = [[-0.18190117 -0.00323   ]]. Action = [[-0.17602418  0.03908932  0.22933602  0.65946627]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
Current timestep = 175. State = [[-0.1827634 -0.002967 ]]. Action = [[-0.00339782  0.09062034  0.18818784 -0.50893474]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0148, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.18357633 -0.00098598]]. Action = [[ 0.17917651 -0.03283335  0.23054433 -0.30285203]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0157, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of 1
Current timestep = 177. State = [[-0.18363452 -0.00081032]]. Action = [[ 0.00195935  0.18051466 -0.01837017 -0.06658101]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
Current timestep = 178. State = [[-0.18427035  0.00388631]]. Action = [[ 0.08475149  0.16424847  0.05031881 -0.03266454]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
Human Feedback received at timestep 178 of 1
Current timestep = 179. State = [[-0.1832142   0.00982766]]. Action = [[ 0.07929349 -0.21524003  0.06381196 -0.6378873 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
Current timestep = 180. State = [[-0.18117069  0.00954575]]. Action = [[-0.05665383 -0.11533035 -0.13117161  0.69155   ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
Current timestep = 181. State = [[-0.17986324  0.00721179]]. Action = [[-0.12755789  0.08888021  0.14275604  0.8183851 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
Human Feedback received at timestep 181 of -1
Current timestep = 182. State = [[-0.17986986  0.00738666]]. Action = [[ 0.10639477 -0.22023146  0.18306822 -0.8239961 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
Current timestep = 183. State = [[-0.17913589  0.00350198]]. Action = [[-0.17693232  0.21541941  0.19861531 -0.18445301]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0214, grad_fn=<MseLossBackward0>)
Current timestep = 184. State = [[-0.17994656  0.00566121]]. Action = [[0.06473324 0.06303507 0.11400709 0.8529171 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
Current timestep = 185. State = [[-0.18048622  0.00814013]]. Action = [[ 0.16424423  0.04459471 -0.20879027 -0.38379776]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
Current timestep = 186. State = [[-0.17940731  0.00943399]]. Action = [[-0.14548479 -0.16777208  0.02824131 -0.94576174]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
Human Feedback received at timestep 186 of 1
Current timestep = 187. State = [[-0.17888884  0.00827111]]. Action = [[0.18637186 0.2318497  0.13847488 0.47669947]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0260, grad_fn=<MseLossBackward0>)
Current timestep = 188. State = [[-0.1775033   0.01159167]]. Action = [[ 0.13150212  0.00635302  0.02051318 -0.57161003]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0141, grad_fn=<MseLossBackward0>)
Current timestep = 189. State = [[-0.17338747  0.01406581]]. Action = [[0.21966851 0.23917645 0.16514823 0.1119225 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
Current timestep = 190. State = [[-0.16583039  0.02110395]]. Action = [[-0.13592981 -0.21215163 -0.08386201  0.78376555]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
Current timestep = 191. State = [[-0.16155028  0.02170685]]. Action = [[-0.01555954 -0.02376109  0.07114136 -0.7951282 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
Human Feedback received at timestep 191 of 1
Current timestep = 192. State = [[-0.15919465  0.02160908]]. Action = [[0.19617379 0.09813368 0.16204616 0.23868382]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(0.0208, grad_fn=<MseLossBackward0>)
Current timestep = 193. State = [[-0.15389001  0.02253002]]. Action = [[ 0.21188438 -0.24325876  0.18500295 -0.85029477]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
Human Feedback received at timestep 193 of 1
Current timestep = 194. State = [[-0.14609826  0.0196231 ]]. Action = [[ 0.0614987   0.23780507 -0.0826121  -0.0240348 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
Current timestep = 195. State = [[-0.13919504  0.02222275]]. Action = [[-0.06690162 -0.17114142  0.12228313  0.64145064]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
Current timestep = 196. State = [[-0.13482565  0.02012084]]. Action = [[-0.04993281 -0.15844724  0.22818685 -0.46609902]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
Human Feedback received at timestep 196 of 1
Current timestep = 197. State = [[-0.13282312  0.01578734]]. Action = [[-0.07283235  0.24509746  0.24752    -0.04217392]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0149, grad_fn=<MseLossBackward0>)
Current timestep = 198. State = [[-0.13295801  0.01835161]]. Action = [[-0.01295364  0.05526581 -0.23472893 -0.13174242]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
Current timestep = 199. State = [[-0.1336977   0.02108307]]. Action = [[0.03442681 0.07938659 0.20709142 0.46658075]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
Current timestep = 200. State = [[-0.13333419  0.02345135]]. Action = [[ 0.22167093 -0.15932208 -0.12230529  0.8047106 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
Current timestep = 201. State = [[-0.13037747  0.02240713]]. Action = [[-0.24909319  0.11405769 -0.18466775 -0.7303585 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
Current timestep = 202. State = [[-0.1305815   0.02486989]]. Action = [[ 0.03432423  0.1850369   0.06134829 -0.37497103]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
Current timestep = 203. State = [[-0.13073693  0.02998285]]. Action = [[0.07699198 0.07341385 0.20333654 0.88116693]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0150, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of 1
Current timestep = 204. State = [[-0.13000159  0.03562265]]. Action = [[-0.23884155  0.01859024  0.18437639  0.56615937]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
Current timestep = 205. State = [[-0.13178673  0.04019545]]. Action = [[ 0.19656172 -0.03054556  0.1307362  -0.00350201]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0147, grad_fn=<MseLossBackward0>)
Current timestep = 206. State = [[-0.13107046  0.04118564]]. Action = [[-0.06895259 -0.07097568 -0.12628926  0.43885493]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
Human Feedback received at timestep 206 of 1
Current timestep = 207. State = [[-0.13116984  0.04042407]]. Action = [[-0.16641675 -0.11855742 -0.23823376  0.9427171 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0180, grad_fn=<MseLossBackward0>)
Current timestep = 208. State = [[-0.13125178  0.03787136]]. Action = [[ 0.2343004  -0.13623904 -0.21723409  0.99166226]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
Current timestep = 209. State = [[-0.12997045  0.03360785]]. Action = [[ 0.04622212 -0.10608259 -0.02175751  0.32107306]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
Current timestep = 210. State = [[-0.12842815  0.02859245]]. Action = [[ 0.1895284   0.19233215 -0.06150877  0.27240837]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
Current timestep = 211. State = [[-0.12523587  0.02933253]]. Action = [[ 0.01404774 -0.03061989 -0.14222047 -0.4178825 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
Current timestep = 212. State = [[-0.1229999   0.02999513]]. Action = [[-0.24554722  0.21270025  0.02171019  0.60448813]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[-0.1243645   0.03469909]]. Action = [[-0.09468235  0.13363963 -0.13187072 -0.6106347 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
Current timestep = 214. State = [[-0.12647216  0.04127927]]. Action = [[ 0.20487124  0.16343334 -0.24176301 -0.37116373]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
Human Feedback received at timestep 214 of 1
Current timestep = 215. State = [[-0.12608732  0.04801171]]. Action = [[-0.02497853  0.05948555 -0.2479739  -0.535717  ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
Current timestep = 216. State = [[-0.12492555  0.05484477]]. Action = [[ 0.13009328  0.15356517 -0.04910804 -0.7833507 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
Current timestep = 217. State = [[-0.12258755  0.06237662]]. Action = [[-0.23795587 -0.03682694  0.21681976  0.03574967]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
Current timestep = 218. State = [[-0.12342793  0.06667358]]. Action = [[ 0.16888863 -0.10855861  0.12292001 -0.55490094]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0121, grad_fn=<MseLossBackward0>)
Current timestep = 219. State = [[-0.1222093   0.06706991]]. Action = [[-0.05478552  0.04259902  0.08141324  0.1606834 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
Current timestep = 220. State = [[-0.1217856   0.06771593]]. Action = [[ 0.03532082 -0.02958338  0.19590226 -0.653043  ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
Current timestep = 221. State = [[-0.12141255  0.06857471]]. Action = [[-0.11177251  0.20285508  0.24604356  0.81247103]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, True, False]
Current timestep = 222. State = [[-0.12331878  0.07408535]]. Action = [[-0.21468942  0.20655793  0.19606614  0.8246758 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, True, False]
Human Feedback received at timestep 222 of -1
Current timestep = 223. State = [[-0.12706134  0.08339224]]. Action = [[-0.21628301  0.19669035  0.238145    0.93394005]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
Current timestep = 224. State = [[-0.13307236  0.09599802]]. Action = [[-0.0909445   0.23682892  0.14686653 -0.4123146 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0231, grad_fn=<MseLossBackward0>)
Current timestep = 225. State = [[-0.13955203  0.11015201]]. Action = [[ 0.24408162 -0.00055966 -0.07960401  0.51452816]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
Human Feedback received at timestep 225 of -1
Current timestep = 226. State = [[-0.14246075  0.1182274 ]]. Action = [[-0.14413036  0.08864668 -0.1309307   0.5289302 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
Current timestep = 227. State = [[-0.14549772  0.12482585]]. Action = [[ 0.19259417 -0.21434656 -0.21341625  0.08602011]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
Human Feedback received at timestep 227 of -1
Current timestep = 228. State = [[-0.14418073  0.12411487]]. Action = [[ 0.07690215 -0.1414266  -0.227726   -0.70641214]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0266, grad_fn=<MseLossBackward0>)
Current timestep = 229. State = [[-0.14224334  0.12095664]]. Action = [[-0.2028042   0.20139301  0.13227218 -0.54819727]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
Human Feedback received at timestep 229 of -1
Current timestep = 230. State = [[-0.1435949  0.1238517]]. Action = [[-0.05069827 -0.12104085  0.21157914  0.9498532 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(0.0261, grad_fn=<MseLossBackward0>)
Current timestep = 231. State = [[-0.1438048   0.12318053]]. Action = [[ 0.22333002  0.08914188 -0.07450783 -0.5562528 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0272, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of -1
Current timestep = 232. State = [[-0.14267212  0.12284992]]. Action = [[ 0.20445591 -0.21000135 -0.06569147  0.75736856]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
Current timestep = 233. State = [[-0.13904683  0.11889146]]. Action = [[-0.20776093  0.24428481 -0.00255047  0.25492954]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, True, False]
Human Feedback received at timestep 233 of -1
Current timestep = 234. State = [[-0.13941097  0.12150422]]. Action = [[ 0.23407203 -0.12511943 -0.17534637  0.01931775]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, True, False]
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(0.0275, grad_fn=<MseLossBackward0>)
Current timestep = 235. State = [[-0.13727961  0.12039112]]. Action = [[-0.13189304  0.09068683 -0.10971878 -0.12875408]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, True, False]
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(0.0299, grad_fn=<MseLossBackward0>)
Current timestep = 236. State = [[-0.1372442  0.1212733]]. Action = [[ 0.12616518 -0.24618413  0.21612778 -0.9721306 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, True, False]
Current timestep = 237. State = [[-0.13475882  0.11667939]]. Action = [[ 0.1558631   0.15834898 -0.16642994  0.4381032 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, True, False]
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(0.0311, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 237 of -1
Current timestep = 238. State = [[-0.13054188  0.1177524 ]]. Action = [[ 0.13382673  0.16866386 -0.21316151  0.43462968]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, True, False]
Current timestep = 239. State = [[-0.12392982  0.12247311]]. Action = [[ 0.10598162  0.14920929 -0.17376275 -0.61793566]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
Current timestep = 240. State = [[-0.11614834  0.12872578]]. Action = [[ 0.08344057 -0.11720499 -0.19889396  0.7587408 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
Current timestep = 241. State = [[-0.10930498  0.1301475 ]]. Action = [[ 0.01065865 -0.02331588  0.11452988  0.20876658]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, False, True]
Current timestep = 242. State = [[-0.10382385  0.13028996]]. Action = [[-0.09266159 -0.16566388  0.22805709  0.36524403]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, False, True]
Current timestep = 243. State = [[-0.10085766  0.12765606]]. Action = [[-0.10562342  0.14856115 -0.17018382 -0.5759227 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, False, False, True]
Current timestep = 244. State = [[-0.10143587  0.12961605]]. Action = [[-0.06522837  0.10756516 -0.09153476  0.02485895]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, False, False, True]
Scene graph at timestep 244 is [True, False, False, False, False, True]
State prediction error at timestep 244 is tensor(0.0269, grad_fn=<MseLossBackward0>)
Current timestep = 245. State = [[-0.10329897  0.13311546]]. Action = [[-0.0735945   0.20464015 -0.06401554  0.70086753]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, False, False, True]
Human Feedback received at timestep 245 of 1
Current timestep = 246. State = [[-0.10614876  0.1399123 ]]. Action = [[ 0.16232067  0.0087887  -0.17061064  0.01838219]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, False, False, True]
Scene graph at timestep 246 is [True, False, False, False, False, True]
State prediction error at timestep 246 is tensor(0.0283, grad_fn=<MseLossBackward0>)
Current timestep = 247. State = [[-0.10671306  0.1444212 ]]. Action = [[-0.24501383  0.20665208  0.13832492  0.6473994 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, False, True]
Human Feedback received at timestep 247 of 1
Current timestep = 248. State = [[-0.11096001  0.15285683]]. Action = [[-0.16684389  0.10406646 -0.14902791  0.14702559]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, False, True]
Current timestep = 249. State = [[-0.11573731  0.1614028 ]]. Action = [[-0.0837819  -0.24446994 -0.16581596 -0.7860285 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, False, True]
Human Feedback received at timestep 249 of 1
Current timestep = 250. State = [[-0.11827052  0.1609669 ]]. Action = [[-0.16087376 -0.22170264  0.00859144  0.39860976]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, False, True]
Current timestep = 251. State = [[-0.1215295   0.15611054]]. Action = [[ 0.21816403  0.11784059 -0.01532617 -0.43543458]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, False, True]
Current timestep = 252. State = [[-0.12194498  0.15507062]]. Action = [[-0.16015641 -0.03660324 -0.04082091  0.7022412 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, False, True]
Scene graph at timestep 252 is [True, False, False, False, False, True]
State prediction error at timestep 252 is tensor(0.0323, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.12324399  0.1542941 ]]. Action = [[-0.24579757 -0.03500755  0.05094171  0.02466536]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, False, True]
Current timestep = 254. State = [[-0.12730877  0.15351939]]. Action = [[ 0.13049084 -0.05805054  0.11064038 -0.7885103 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, False, True]
Current timestep = 255. State = [[-0.12947397  0.15166724]]. Action = [[-0.06627987  0.22305524  0.23126104 -0.8168889 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, False, True]
Current timestep = 256. State = [[-0.13284369  0.1558753 ]]. Action = [[-0.05675259  0.09552652  0.04346678 -0.8023792 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, False, True]
Scene graph at timestep 256 is [True, False, False, False, False, True]
State prediction error at timestep 256 is tensor(0.0327, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 256 of -1
Current timestep = 257. State = [[-0.13748105  0.16190903]]. Action = [[ 0.09580943  0.22550112  0.2285018  -0.8241176 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, False, True]
Scene graph at timestep 257 is [True, False, False, False, False, True]
State prediction error at timestep 257 is tensor(0.0345, grad_fn=<MseLossBackward0>)
Current timestep = 258. State = [[-0.14147823  0.17013526]]. Action = [[-0.00741103  0.06764382 -0.0429339  -0.7659553 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, False, True]
Current timestep = 259. State = [[-0.14503546  0.17688344]]. Action = [[-0.1414631   0.14259434  0.17767245  0.68668246]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, False, True]
Current timestep = 260. State = [[-0.14913596  0.18494871]]. Action = [[ 0.1674281   0.24226135 -0.01459828  0.94628024]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, False, True]
Current timestep = 261. State = [[-0.15133463  0.19393192]]. Action = [[ 0.14681676 -0.2141407   0.14839897  0.53379023]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, False, True]
Current timestep = 262. State = [[-0.14940087  0.1953953 ]]. Action = [[-0.13455038 -0.05033401  0.15135208  0.30620217]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, False, True]
Human Feedback received at timestep 262 of -1
Current timestep = 263. State = [[-0.14937787  0.19619848]]. Action = [[-0.2331721   0.23358351 -0.22470807 -0.0293349 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, False, True]
Scene graph at timestep 263 is [True, False, False, False, False, True]
State prediction error at timestep 263 is tensor(0.0540, grad_fn=<MseLossBackward0>)
Current timestep = 264. State = [[-0.1535067   0.20254755]]. Action = [[-0.14683115  0.14240545  0.05516675 -0.84896934]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, False, True]
Current timestep = 265. State = [[-0.15926863  0.21093655]]. Action = [[-0.11730245 -0.19366647  0.16625956  0.10845613]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, False, True]
Current timestep = 266. State = [[-0.16325025  0.21145655]]. Action = [[-0.03388691 -0.22867149 -0.15199074  0.563499  ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, False, True]
Human Feedback received at timestep 266 of -1
Current timestep = 267. State = [[-0.16626722  0.20584735]]. Action = [[ 0.14198232 -0.22215258  0.16073337 -0.20960104]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, False, True]
Current timestep = 268. State = [[-0.16651364  0.19740576]]. Action = [[-0.19415663  0.04800582 -0.18125214  0.76058364]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, False, True]
Current timestep = 269. State = [[-0.17047918  0.19334443]]. Action = [[0.00544444 0.05199674 0.24074498 0.89136255]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, False, True]
Scene graph at timestep 269 is [True, False, False, False, False, True]
State prediction error at timestep 269 is tensor(0.0467, grad_fn=<MseLossBackward0>)
Current timestep = 270. State = [[-0.17322905  0.19182372]]. Action = [[-0.04561377 -0.01057202 -0.0559461   0.37617683]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, False, True]
Current timestep = 271. State = [[-0.17490572  0.19093426]]. Action = [[ 0.09351668 -0.02025799 -0.24399182  0.11308277]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, False, True]
Current timestep = 272. State = [[-0.1746955   0.19037169]]. Action = [[-0.10475013  0.19848222  0.14973074  0.3330425 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, False, True]
Human Feedback received at timestep 272 of -1
Current timestep = 273. State = [[-0.17695405  0.19428115]]. Action = [[ 0.03401643 -0.11779597 -0.1482825   0.6510427 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, False, True]
Current timestep = 274. State = [[-0.17786388  0.19458021]]. Action = [[-0.0346882   0.22812301 -0.23656636 -0.71032465]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, False, True]
Scene graph at timestep 274 is [True, False, False, False, False, True]
State prediction error at timestep 274 is tensor(0.0557, grad_fn=<MseLossBackward0>)
Current timestep = 275. State = [[-0.18067248  0.19927193]]. Action = [[ 0.14904675  0.2025336   0.13415867 -0.0348435 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, False, True]
Scene graph at timestep 275 is [True, False, False, False, False, True]
State prediction error at timestep 275 is tensor(0.0523, grad_fn=<MseLossBackward0>)
Current timestep = 276. State = [[-0.1825606   0.20592421]]. Action = [[-0.19725594 -0.12320578 -0.01666395 -0.3679859 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, False, True]
Scene graph at timestep 276 is [True, False, False, False, False, True]
State prediction error at timestep 276 is tensor(0.0460, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of -1
Current timestep = 277. State = [[-0.1848477   0.20880787]]. Action = [[ 0.051844   -0.23044468  0.23371124  0.75566316]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, False, True]
Current timestep = 278. State = [[-0.18472072  0.20605104]]. Action = [[-0.13019408  0.20826912 -0.05643457  0.65749335]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, False, True]
Human Feedback received at timestep 278 of -1
Current timestep = 279. State = [[-0.1870075   0.20914246]]. Action = [[-0.13500734 -0.04399964  0.05741155 -0.36770654]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, False, True]
Current timestep = 280. State = [[-0.19014925  0.21026629]]. Action = [[-0.11497733 -0.14535372 -0.06744662 -0.09917009]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, False, True]
Current timestep = 281. State = [[-0.1941882   0.20699383]]. Action = [[ 0.12718004 -0.04739977  0.00409278  0.49590337]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, False, True]
Scene graph at timestep 281 is [True, False, False, False, False, True]
State prediction error at timestep 281 is tensor(0.0547, grad_fn=<MseLossBackward0>)
Current timestep = 282. State = [[-0.19670019  0.20228224]]. Action = [[ 0.17700568 -0.21997114 -0.11565176  0.41281605]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, False, True]
Human Feedback received at timestep 282 of -1
Current timestep = 283. State = [[-0.19350769  0.19139017]]. Action = [[-0.07046781  0.0016174  -0.15857404  0.8113942 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, False, True]
Current timestep = 284. State = [[-0.19450292  0.18825182]]. Action = [[-0.24727397 -0.00294057 -0.02004679 -0.0380137 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, False, True]
Human Feedback received at timestep 284 of -1
Current timestep = 285. State = [[-0.19950137  0.18569063]]. Action = [[ 0.12173414  0.00119254 -0.0334174  -0.27904463]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, False, True]
Current timestep = 286. State = [[-0.20083362  0.18349077]]. Action = [[-0.18390064 -0.19500923  0.0542081  -0.07798249]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, False, True]
Human Feedback received at timestep 286 of -1
Current timestep = 287. State = [[-0.20341457  0.17924878]]. Action = [[-0.07817903  0.24974644 -0.05386518 -0.5866436 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, False, True]
Scene graph at timestep 287 is [True, False, False, False, False, True]
State prediction error at timestep 287 is tensor(0.0516, grad_fn=<MseLossBackward0>)
Current timestep = 288. State = [[-0.20888664  0.18225914]]. Action = [[ 0.01028073  0.22316831  0.10373178 -0.09009129]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, False, True]
Current timestep = 289. State = [[-0.21333681  0.1884743 ]]. Action = [[-0.1008929  -0.08855754 -0.20721525 -0.2635529 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, False, True]
Current timestep = 290. State = [[-0.21804139  0.19025175]]. Action = [[ 0.24335366 -0.17864884 -0.16622287 -0.02795124]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, False, True]
Scene graph at timestep 290 is [True, False, False, False, False, True]
State prediction error at timestep 290 is tensor(0.0549, grad_fn=<MseLossBackward0>)
Current timestep = 291. State = [[-0.21717723  0.18648453]]. Action = [[-0.14284828  0.02666262  0.14307636 -0.7663967 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, False, True]
Current timestep = 292. State = [[-0.21798006  0.1853312 ]]. Action = [[-0.15636575 -0.1805887  -0.21037896  0.49005258]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, False, True]
Human Feedback received at timestep 292 of -1
Current timestep = 293. State = [[-0.2215486   0.17943539]]. Action = [[ 0.11829767 -0.18967219  0.0033237   0.20205629]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, False, True]
Current timestep = 294. State = [[-0.22239149  0.17225711]]. Action = [[0.11746955 0.11282527 0.12952775 0.29335082]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, False, True]
Current timestep = 295. State = [[-0.22111171  0.1706879 ]]. Action = [[-0.1979075   0.00405869  0.04390317  0.7200632 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, False, True]
Current timestep = 296. State = [[-0.22282197  0.17017724]]. Action = [[-0.22503477  0.02290606  0.04141948  0.8843081 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, False, True]
Current timestep = 297. State = [[-0.22678101  0.17058866]]. Action = [[ 0.22907895  0.21820211  0.0883787  -0.98925376]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, False, True]
Scene graph at timestep 297 is [True, False, False, False, False, True]
State prediction error at timestep 297 is tensor(0.0483, grad_fn=<MseLossBackward0>)
Current timestep = 298. State = [[-0.22909662  0.17478436]]. Action = [[-0.16847706  0.2031959   0.20241317  0.87305737]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, False, True]
Scene graph at timestep 298 is [True, False, False, False, False, True]
State prediction error at timestep 298 is tensor(0.0598, grad_fn=<MseLossBackward0>)
Current timestep = 299. State = [[-0.23344685  0.18193734]]. Action = [[ 0.14989555 -0.05269396  0.04854783 -0.16872978]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, False, True]
Current timestep = 300. State = [[-0.23427406  0.18489677]]. Action = [[ 0.06016815 -0.06185862 -0.23609644  0.8243147 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, False, True]
Human Feedback received at timestep 300 of -1
Current timestep = 301. State = [[-0.2337995   0.18491443]]. Action = [[-0.12334317 -0.08496915 -0.04075986 -0.41738147]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, False, True]
Current timestep = 302. State = [[-0.23331827  0.18381323]]. Action = [[-0.04655159 -0.23836389  0.11342287 -0.6467637 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, False, True]
Human Feedback received at timestep 302 of -1
Current timestep = 303. State = [[-0.2338757  0.1790633]]. Action = [[0.00282103 0.23250079 0.20972288 0.01337445]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, False, True]
Current timestep = 304. State = [[-0.23517348  0.18046749]]. Action = [[ 0.22818995  0.09646788 -0.03094155 -0.5939901 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, False, True]
Scene graph at timestep 304 is [True, False, False, False, False, True]
State prediction error at timestep 304 is tensor(0.0531, grad_fn=<MseLossBackward0>)
Current timestep = 305. State = [[-0.23453736  0.1811573 ]]. Action = [[ 0.03143471 -0.02668905 -0.14678264 -0.13458979]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, False, True]
Scene graph at timestep 305 is [True, False, False, False, False, True]
State prediction error at timestep 305 is tensor(0.0580, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of -1
Current timestep = 306. State = [[-0.2336124   0.18130413]]. Action = [[ 0.03555176 -0.04896116  0.07185972  0.42453074]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, False, True]
Current timestep = 307. State = [[-0.23241591  0.18114091]]. Action = [[-0.05673547  0.1568076   0.15336204 -0.7118628 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, False, True]
Current timestep = 308. State = [[-0.23313172  0.18402395]]. Action = [[-0.19176978 -0.03002229 -0.09161752 -0.57948595]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, False, True]
Current timestep = 309. State = [[-0.23449431  0.18611367]]. Action = [[-0.04748781 -0.05484791 -0.20752318 -0.24825007]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, False, True]
Current timestep = 310. State = [[-0.2352917  0.1870788]]. Action = [[-0.0192827   0.22745982  0.01754797  0.89527404]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, False, True]
Human Feedback received at timestep 310 of -1
Current timestep = 311. State = [[-0.23870112  0.19221674]]. Action = [[ 0.24598312  0.23363417  0.19990122 -0.2632689 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, False, True]
Scene graph at timestep 311 is [True, False, False, False, False, True]
State prediction error at timestep 311 is tensor(0.0588, grad_fn=<MseLossBackward0>)
Current timestep = 312. State = [[-0.23829272  0.19982034]]. Action = [[ 0.0868603   0.14239407 -0.12051478 -0.28462952]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, False, True]
Scene graph at timestep 312 is [True, False, False, False, False, True]
State prediction error at timestep 312 is tensor(0.0651, grad_fn=<MseLossBackward0>)
Current timestep = 313. State = [[-0.23595238  0.20853901]]. Action = [[-0.09736218  0.10388878  0.1806775  -0.3176986 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, False, True]
Scene graph at timestep 313 is [True, False, False, False, False, True]
State prediction error at timestep 313 is tensor(0.0609, grad_fn=<MseLossBackward0>)
Current timestep = 314. State = [[-0.23620528  0.2177451 ]]. Action = [[0.15163898 0.22656208 0.1745058  0.03274632]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, False, True]
Current timestep = 315. State = [[-0.23510092  0.22740917]]. Action = [[ 0.01717269  0.12485924 -0.08647761 -0.52691865]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, False, True]
Current timestep = 316. State = [[-0.23288524  0.23737237]]. Action = [[ 0.23792255 -0.19138168  0.05486271  0.9859054 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, False, True]
Current timestep = 317. State = [[-0.22672243  0.2401296 ]]. Action = [[-0.08270381  0.16747987 -0.05599672 -0.44100857]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, False, True]
Current timestep = 318. State = [[-0.2236604   0.24560599]]. Action = [[-0.00962296 -0.21153666  0.23658735  0.538484  ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, False, True]
Scene graph at timestep 318 is [True, False, False, False, False, True]
State prediction error at timestep 318 is tensor(0.0648, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 318 of -1
Current timestep = 319. State = [[-0.22119118  0.24496435]]. Action = [[ 0.1986165   0.07936335  0.09900945 -0.489398  ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, False, True]
Current timestep = 320. State = [[-0.21766777  0.24546917]]. Action = [[-0.02845798 -0.10386884 -0.10968763 -0.07388496]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, False, True]
Current timestep = 321. State = [[-0.21505603  0.24423814]]. Action = [[-0.0990124  -0.06469022  0.11580673  0.7758266 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, False, True]
Current timestep = 322. State = [[-0.21404202  0.24311318]]. Action = [[0.2382291  0.06340122 0.09194005 0.23380637]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, False, True]
Current timestep = 323. State = [[-0.21118137  0.24240051]]. Action = [[-0.09507224 -0.08900395  0.19966567  0.73209715]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, False, True]
Human Feedback received at timestep 323 of -1
Current timestep = 324. State = [[-0.20966563  0.24132669]]. Action = [[-0.21417637  0.10341573 -0.2203437  -0.65301824]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, False, True]
Scene graph at timestep 324 is [True, False, False, False, False, True]
State prediction error at timestep 324 is tensor(0.0702, grad_fn=<MseLossBackward0>)
Current timestep = 325. State = [[-0.21102233  0.24279682]]. Action = [[ 0.01872355 -0.16982254 -0.15279731 -0.74373543]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, False, True]
Current timestep = 326. State = [[-0.21077545  0.24073333]]. Action = [[ 0.02318859  0.10557556 -0.21522062 -0.96729946]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, False, True]
Current timestep = 327. State = [[-0.21077035  0.24046278]]. Action = [[-0.0694325  -0.22754604  0.21761382  0.63733697]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, False, True]
Human Feedback received at timestep 327 of -1
Current timestep = 328. State = [[-0.21074876  0.23627116]]. Action = [[-0.24432375 -0.1452278   0.18519777 -0.22927636]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, False, True]
Current timestep = 329. State = [[-0.21494171  0.2314593 ]]. Action = [[-0.2005955   0.16930556  0.18864614  0.21889639]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, False, True]
Current timestep = 330. State = [[-0.2215672   0.23070736]]. Action = [[ 0.20111847 -0.16314597  0.07994545  0.34611845]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, False, True]
Current timestep = 331. State = [[-0.22315419  0.22657162]]. Action = [[-0.23933971  0.17342329 -0.09801787 -0.01418412]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, False, True]
Human Feedback received at timestep 331 of -1
Current timestep = 332. State = [[-0.22633824  0.22894458]]. Action = [[-0.00810161  0.03671473  0.18093175 -0.9650372 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, False, True]
Current timestep = 333. State = [[-0.22954147  0.23164263]]. Action = [[ 0.12924582  0.2283318   0.12670863 -0.9666131 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, False, True]
Scene graph at timestep 333 is [True, False, False, False, False, True]
State prediction error at timestep 333 is tensor(0.0658, grad_fn=<MseLossBackward0>)
Current timestep = 334. State = [[-0.23278715  0.23613389]]. Action = [[-0.03061958 -0.19909368 -0.17821075 -0.23841894]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, False, True]
Current timestep = 335. State = [[-0.23288111  0.23595895]]. Action = [[-0.05106708 -0.00397305 -0.14977463 -0.24857134]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, False, True]
Current timestep = 336. State = [[-0.23328885  0.23620403]]. Action = [[-0.09272014  0.08565399  0.12230873  0.544919  ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, False, True]
Scene graph at timestep 336 is [True, False, False, False, False, True]
State prediction error at timestep 336 is tensor(0.0727, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of -1
Current timestep = 337. State = [[-0.23562385  0.2381032 ]]. Action = [[-0.04950137 -0.21603273 -0.24416505 -0.21847332]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, False, True]
Current timestep = 338. State = [[-0.23816298  0.23521024]]. Action = [[-0.16546598  0.23410219  0.11023152 -0.73228985]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, False, True]
Current timestep = 339. State = [[-0.24391946  0.2380784 ]]. Action = [[-0.13661401 -0.21665384  0.10210997 -0.43282878]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, False, True]
Current timestep = 340. State = [[-0.25114504  0.2348977 ]]. Action = [[ 0.14101002  0.08166957  0.11080635 -0.7109239 ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, False, True]
Current timestep = 341. State = [[-0.2546187  0.2339055]]. Action = [[ 0.18061775  0.04637587  0.22132954 -0.53498703]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, False, True]
Scene graph at timestep 341 is [True, False, False, False, False, True]
State prediction error at timestep 341 is tensor(0.0649, grad_fn=<MseLossBackward0>)
Current timestep = 342. State = [[-0.25407177  0.233753  ]]. Action = [[0.0029926  0.01291969 0.2294766  0.68729067]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, False, True]
Scene graph at timestep 342 is [True, False, False, False, False, True]
State prediction error at timestep 342 is tensor(0.0731, grad_fn=<MseLossBackward0>)
Current timestep = 343. State = [[-0.2535222   0.23377621]]. Action = [[ 0.12324908 -0.08229987 -0.13664523 -0.6164951 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, False, True]
Scene graph at timestep 343 is [True, False, False, False, False, True]
State prediction error at timestep 343 is tensor(0.0683, grad_fn=<MseLossBackward0>)
Current timestep = 344. State = [[-0.25132245  0.23182935]]. Action = [[ 0.11365283 -0.0664736   0.04917908  0.604738  ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, False, True]
Current timestep = 345. State = [[-0.24805069  0.22836934]]. Action = [[ 0.05304357 -0.21711743  0.01088193 -0.15997213]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, False, True]
Current timestep = 346. State = [[-0.24367392  0.22283335]]. Action = [[-0.06937063  0.21108481 -0.15869917 -0.35764176]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, False, True]
Current timestep = 347. State = [[-0.24331942  0.22281852]]. Action = [[0.01284745 0.01944196 0.11819389 0.38699365]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, False, True]
Current timestep = 348. State = [[-0.24303861  0.22287752]]. Action = [[ 0.13355124 -0.03734936  0.19594842 -0.44285524]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, False, True]
Current timestep = 349. State = [[-0.24086764  0.22152753]]. Action = [[ 0.02317917 -0.15682155  0.20274532 -0.64070034]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, False, True]
Human Feedback received at timestep 349 of -1
Current timestep = 350. State = [[-0.23737565  0.21762353]]. Action = [[-0.0895699  -0.00805157 -0.13619357 -0.32054746]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, False, True]
Current timestep = 351. State = [[-0.23633291  0.21618661]]. Action = [[-3.59720737e-02 -3.32266092e-04  1.02556944e-01  5.37557602e-01]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, False, True]
Human Feedback received at timestep 351 of -1
Current timestep = 352. State = [[-0.23621947  0.21532206]]. Action = [[-0.072612    0.07750195  0.04819214 -0.9101254 ]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, False, True]
Current timestep = 353. State = [[-0.23705937  0.21579014]]. Action = [[-0.09469867 -0.24609025 -0.00074832 -0.48478913]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, False, True]
Scene graph at timestep 353 is [True, False, False, False, False, True]
State prediction error at timestep 353 is tensor(0.0543, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of 1
Current timestep = 354. State = [[-0.23818608  0.21112378]]. Action = [[-0.22265866 -0.00817369 -0.23016062  0.7360873 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, False, True]
Current timestep = 355. State = [[-0.24211045  0.20878682]]. Action = [[-0.08099765  0.03628013 -0.06126581 -0.24832118]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, False, True]
Scene graph at timestep 355 is [True, False, False, False, False, True]
State prediction error at timestep 355 is tensor(0.0643, grad_fn=<MseLossBackward0>)
Current timestep = 356. State = [[-0.24657314  0.20802858]]. Action = [[0.18267804 0.15306312 0.11147755 0.0860337 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, False, True]
Current timestep = 357. State = [[-0.24733385  0.20935655]]. Action = [[ 0.14349246 -0.21327515  0.12247166  0.9602027 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, False, True]
Current timestep = 358. State = [[-0.24471833  0.20510383]]. Action = [[ 0.1433684 -0.1500123  0.1905356  0.6758517]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, False, True]
Scene graph at timestep 358 is [True, False, False, False, False, True]
State prediction error at timestep 358 is tensor(0.0610, grad_fn=<MseLossBackward0>)
Current timestep = 359. State = [[-0.24063936  0.19918758]]. Action = [[0.05090523 0.10076836 0.03580984 0.12570786]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, False, True]
Human Feedback received at timestep 359 of -1
Current timestep = 360. State = [[-0.23897785  0.19733828]]. Action = [[ 0.13462365  0.23287162  0.00525573 -0.61892647]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, False, True]
Current timestep = 361. State = [[-0.23626827  0.20112826]]. Action = [[-0.2031908   0.17001104 -0.164652    0.8156121 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, False, True]
Current timestep = 362. State = [[-0.23705274  0.207397  ]]. Action = [[-0.03897995  0.03425282 -0.1394416  -0.35039037]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, False, True]
Current timestep = 363. State = [[-0.23862599  0.21158493]]. Action = [[ 0.18064845 -0.12550405 -0.05125922  0.3761872 ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, False, True]
Human Feedback received at timestep 363 of 1
Current timestep = 364. State = [[-0.23684967  0.21030642]]. Action = [[ 0.10928142 -0.08886972  0.11869407 -0.97263044]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, False, True]
Current timestep = 365. State = [[-0.23329872  0.20729677]]. Action = [[-0.07885459 -0.16705854 -0.22025645 -0.27600598]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, False, True]
Current timestep = 366. State = [[-0.23097059  0.2042204 ]]. Action = [[ 0.11501312  0.17025846 -0.16781598  0.84440506]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, False, True]
Current timestep = 367. State = [[-0.22973195  0.20428233]]. Action = [[-0.07545272 -0.19695397 -0.16551189  0.6478708 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, False, True]
Current timestep = 368. State = [[-0.22825415  0.20166153]]. Action = [[ 0.23848069  0.20635632 -0.16398728 -0.5728567 ]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, False, True]
Scene graph at timestep 368 is [True, False, False, False, False, True]
State prediction error at timestep 368 is tensor(0.0643, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of 1
Current timestep = 369. State = [[-0.2245207   0.20246726]]. Action = [[ 0.1024386  -0.17311867 -0.14773993  0.02762902]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, False, True]
Current timestep = 370. State = [[-0.21982457  0.19955944]]. Action = [[ 0.12162054  0.15833396  0.1017946  -0.6584465 ]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, False, True]
Current timestep = 371. State = [[-0.21445327  0.2011192 ]]. Action = [[-0.2306885   0.02579218  0.24652606  0.10843635]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, False, True]
Current timestep = 372. State = [[-0.21322738  0.20369026]]. Action = [[-0.10513338 -0.1524755  -0.1738475  -0.32867497]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, False, True]
Human Feedback received at timestep 372 of 1
Current timestep = 373. State = [[-0.21288224  0.20249812]]. Action = [[ 0.06044602  0.04359072 -0.12441617  0.37088263]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, False, False, True]
Current timestep = 374. State = [[-0.21242574  0.20175715]]. Action = [[ 0.07123291  0.12582636 -0.02907166  0.45122695]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, False, False, True]
Human Feedback received at timestep 374 of 1
Current timestep = 375. State = [[-0.21261206  0.20345734]]. Action = [[-0.03237647  0.06646734 -0.19660771 -0.46888477]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, False, False, True]
Scene graph at timestep 375 is [True, False, False, False, False, True]
State prediction error at timestep 375 is tensor(0.0584, grad_fn=<MseLossBackward0>)
Current timestep = 376. State = [[-0.21284054  0.20552838]]. Action = [[ 0.1948365  -0.17575721  0.1984961  -0.7294052 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, False, False, True]
Current timestep = 377. State = [[-0.21004538  0.2028425 ]]. Action = [[ 0.04070619  0.14198223  0.00413352 -0.41296673]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, False, False, True]
Current timestep = 378. State = [[-0.20703784  0.20416595]]. Action = [[ 0.06249788 -0.21649724 -0.2370868  -0.3394674 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, False, False, True]
Human Feedback received at timestep 378 of 1
Current timestep = 379. State = [[-0.20353901  0.20110983]]. Action = [[-0.24891637  0.08087739  0.12442592 -0.5708886 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, False, False, True]
Current timestep = 380. State = [[-0.20385104  0.20159051]]. Action = [[ 0.22872257 -0.16502248 -0.17737074 -0.7401458 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, False, False, True]
Human Feedback received at timestep 380 of 1
Current timestep = 381. State = [[-0.20084202  0.19768763]]. Action = [[ 0.01084745  0.16277838  0.22258627 -0.710396  ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, False, False, True]
Current timestep = 382. State = [[-0.19867903  0.19847451]]. Action = [[-0.08200383 -0.16086976  0.06939167  0.7218077 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, False, False, True]
Human Feedback received at timestep 382 of 1
Current timestep = 383. State = [[-0.19734304  0.19636066]]. Action = [[ 0.07652029 -0.24158217 -0.15613778 -0.47275913]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, False, False, True]
Current timestep = 384. State = [[-0.19309598  0.18896885]]. Action = [[ 0.22622702 -0.21811932 -0.08395199  0.09765232]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, False, False, True]
Current timestep = 385. State = [[-0.18583082  0.17795198]]. Action = [[-0.17512873 -0.24138223 -0.07343137  0.06793427]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, False, False, True]
Current timestep = 386. State = [[-0.18154944  0.16717151]]. Action = [[ 0.02392405  0.17742503 -0.18236668 -0.30797338]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, False, False, True]
Scene graph at timestep 386 is [True, False, False, False, False, True]
State prediction error at timestep 386 is tensor(0.0469, grad_fn=<MseLossBackward0>)
Current timestep = 387. State = [[-0.17975229  0.16362229]]. Action = [[-0.22826855  0.17682889 -0.10021573 -0.82999414]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, False, False, True]
Human Feedback received at timestep 387 of 1
Current timestep = 388. State = [[-0.18212841  0.16553232]]. Action = [[-0.14683391 -0.13267958 -0.11738974  0.54345584]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, False, False, True]
Current timestep = 389. State = [[-0.18626668  0.1634949 ]]. Action = [[-0.14410399  0.14523447  0.230968    0.0535357 ]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, False, False, True]
Current timestep = 390. State = [[-0.19324417  0.16557729]]. Action = [[-0.00436376  0.00788525 -0.02925782  0.12712479]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, False, False, True]
Current timestep = 391. State = [[-0.19891413  0.1667448 ]]. Action = [[ 0.00680155 -0.07619743  0.13877463 -0.9681096 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, False, False, True]
Current timestep = 392. State = [[-0.2022764   0.16545562]]. Action = [[-0.13603221 -0.04600768 -0.16513947 -0.70569754]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, False, False, True]
Current timestep = 393. State = [[-0.20558545  0.16396008]]. Action = [[ 0.19411168 -0.07861337 -0.06811731 -0.9625329 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, False, False, True]
Current timestep = 394. State = [[-0.20493016  0.16119447]]. Action = [[-0.2149664  -0.10418855  0.08010542  0.9566463 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, False, False, True]
Scene graph at timestep 394 is [True, False, False, False, False, True]
State prediction error at timestep 394 is tensor(0.0470, grad_fn=<MseLossBackward0>)
Current timestep = 395. State = [[-0.20662715  0.15698461]]. Action = [[-0.0307807  -0.06188348 -0.09164914  0.16604269]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, False, False, True]
Human Feedback received at timestep 395 of -1
Current timestep = 396. State = [[-0.20942593  0.15206814]]. Action = [[-0.10154295 -0.07955533  0.05182603  0.74991536]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, False, False, True]
Current timestep = 397. State = [[-0.21427636  0.14656086]]. Action = [[-0.13271114 -0.15138914  0.00269243  0.01375067]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, False, False, True]
Human Feedback received at timestep 397 of -1
Current timestep = 398. State = [[-0.22066084  0.1394435 ]]. Action = [[-0.02145794  0.07253754  0.12534273  0.7597914 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, False, False, True]
Current timestep = 399. State = [[-0.22490498  0.13658276]]. Action = [[ 0.03946671 -0.0701433   0.16483977  0.5673311 ]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, False, False, True]
Current timestep = 400. State = [[-0.22791786  0.13236125]]. Action = [[-0.0214601  -0.17681594 -0.01717755  0.90279615]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, False, False, True]
Current timestep = 401. State = [[-0.23096694  0.12537557]]. Action = [[-0.03051876  0.00969654 -0.17877291 -0.4020986 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, False, False, True]
Scene graph at timestep 401 is [True, False, False, False, False, True]
State prediction error at timestep 401 is tensor(0.0439, grad_fn=<MseLossBackward0>)
Current timestep = 402. State = [[-0.23452662  0.120246  ]]. Action = [[-0.1797141  -0.06812574  0.2368921  -0.3986498 ]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, False, False, True]
Human Feedback received at timestep 402 of -1
Current timestep = 403. State = [[-0.2387203   0.11611081]]. Action = [[ 0.12693655 -0.02220267 -0.13935295 -0.8169536 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, False, True, False]
Current timestep = 404. State = [[-0.23970006  0.11235654]]. Action = [[ 0.15387756 -0.22228073 -0.02099338  0.85012555]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, False, True, False]
Current timestep = 405. State = [[-0.23700756  0.10560296]]. Action = [[ 0.16062409  0.11864492 -0.14652179 -0.4720571 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, False, True, False]
Scene graph at timestep 405 is [True, False, False, False, True, False]
State prediction error at timestep 405 is tensor(0.0429, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of -1
Current timestep = 406. State = [[-0.23403867  0.10211805]]. Action = [[ 0.04444024 -0.03668763  0.16816202  0.31221056]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, False, True, False]
Scene graph at timestep 406 is [True, False, False, False, True, False]
State prediction error at timestep 406 is tensor(0.0442, grad_fn=<MseLossBackward0>)
Current timestep = 407. State = [[-0.23188783  0.09951854]]. Action = [[-0.1751818   0.03859329  0.18841594 -0.5621452 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, False, True, False]
Human Feedback received at timestep 407 of -1
Current timestep = 408. State = [[-0.23158284  0.09943943]]. Action = [[-0.03189123 -0.14287917 -0.03860381  0.22244465]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, False, True, False]
Current timestep = 409. State = [[-0.23143919  0.09506164]]. Action = [[ 0.0998385  -0.22945124  0.00485605 -0.21691102]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, False, True, False]
Current timestep = 410. State = [[-0.23109923  0.08620462]]. Action = [[-0.16516352 -0.24781941 -0.03480011  0.8801396 ]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, False, True, False]
Current timestep = 411. State = [[-0.23413149  0.07333988]]. Action = [[-0.10200152 -0.1742281   0.24381262  0.9302002 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, False, True, False]
Human Feedback received at timestep 411 of -1
Current timestep = 412. State = [[-0.23845133  0.06198549]]. Action = [[ 0.04059905  0.23550892  0.22864729 -0.9161777 ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, False, True, False]
Current timestep = 413. State = [[-0.23887084  0.0599395 ]]. Action = [[ 0.06609264 -0.10250154  0.16600943 -0.70916575]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, False, True, False]
Scene graph at timestep 413 is [True, False, False, False, True, False]
State prediction error at timestep 413 is tensor(0.0241, grad_fn=<MseLossBackward0>)
Current timestep = 414. State = [[-0.23912914  0.05627853]]. Action = [[-0.14116438  0.06443331  0.07274228  0.9251492 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, False, True, False]
Scene graph at timestep 414 is [True, False, False, False, True, False]
State prediction error at timestep 414 is tensor(0.0412, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 414 of -1
Current timestep = 415. State = [[-0.24265566  0.05388445]]. Action = [[ 0.11702269 -0.08997047  0.12269658  0.98881316]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, False, True, False]
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(0.0369, grad_fn=<MseLossBackward0>)
Current timestep = 416. State = [[-0.24381368  0.05080336]]. Action = [[-0.1556667  -0.00815974  0.18672442 -0.4628172 ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, False, True, False]
Scene graph at timestep 416 is [True, False, False, False, True, False]
State prediction error at timestep 416 is tensor(0.0302, grad_fn=<MseLossBackward0>)
Current timestep = 417. State = [[-0.24643369  0.04842144]]. Action = [[ 0.20032644  0.0009011   0.03591508 -0.21147603]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, False, True, False]
Human Feedback received at timestep 417 of 1
Current timestep = 418. State = [[-0.24548264  0.04679484]]. Action = [[-0.20415716 -0.09453255 -0.01780581 -0.70637006]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, False, True, False]
Current timestep = 419. State = [[-0.24747667  0.04426666]]. Action = [[ 0.01213333  0.245601   -0.07274039  0.3252722 ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, False, True, False]
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(0.0494, grad_fn=<MseLossBackward0>)
Current timestep = 420. State = [[-0.24902488  0.04758127]]. Action = [[ 0.10508773  0.15494996  0.16751337 -0.16977727]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, False, True, False]
Current timestep = 421. State = [[-0.25029987  0.05204793]]. Action = [[-0.12738976 -0.05455816 -0.07382244  0.7509217 ]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, False, True, False]
Current timestep = 422. State = [[-0.2514374   0.05465176]]. Action = [[ 0.15636319  0.01558101 -0.09015742 -0.98159015]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 422 is [True, False, False, False, True, False]
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(0.0294, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 422 of -1
Current timestep = 423. State = [[-0.25142124  0.05586668]]. Action = [[-0.00808617  0.06896672  0.07461068  0.7456614 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 423 is [True, False, False, False, True, False]
Current timestep = 424. State = [[-0.2513454   0.05822957]]. Action = [[0.23140824 0.18644017 0.08775014 0.90157545]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 424 is [True, False, False, False, True, False]
Current timestep = 425. State = [[-0.2473861  0.0632839]]. Action = [[-0.00242688 -0.2050744   0.09519666 -0.8076555 ]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 425 is [True, False, False, False, True, False]
Scene graph at timestep 425 is [True, False, False, False, True, False]
State prediction error at timestep 425 is tensor(0.0250, grad_fn=<MseLossBackward0>)
Current timestep = 426. State = [[-0.24429119  0.06347238]]. Action = [[ 0.17395669  0.00738421 -0.14558955  0.488878  ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 426 is [True, False, False, False, True, False]
Human Feedback received at timestep 426 of -1
Current timestep = 427. State = [[-0.23996605  0.06404106]]. Action = [[-0.05712351  0.13394368 -0.22933695  0.5688182 ]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 427 is [True, False, False, False, True, False]
Scene graph at timestep 427 is [True, False, False, False, True, False]
State prediction error at timestep 427 is tensor(0.0475, grad_fn=<MseLossBackward0>)
Current timestep = 428. State = [[-0.23696466  0.06687512]]. Action = [[-0.15509167 -0.12963893  0.0948441  -0.68762636]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 428 is [True, False, False, False, True, False]
Current timestep = 429. State = [[-0.23706922  0.06731907]]. Action = [[ 0.10866195  0.23302346  0.10088041 -0.13253349]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 429 is [True, False, False, False, True, False]
Human Feedback received at timestep 429 of 1
Current timestep = 430. State = [[-0.23554018  0.07143953]]. Action = [[-0.01928495  0.04566443  0.18790436 -0.68535745]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 430 is [True, False, False, False, True, False]
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(0.0282, grad_fn=<MseLossBackward0>)
Current timestep = 431. State = [[-0.23461668  0.07492971]]. Action = [[ 0.14192837 -0.05185299  0.18181825 -0.34598404]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 431 is [True, False, False, False, True, False]
Human Feedback received at timestep 431 of 1
Current timestep = 432. State = [[-0.23247153  0.07625609]]. Action = [[ 0.10484713 -0.06039947  0.12234759 -0.7631167 ]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 432 is [True, False, False, False, True, False]
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(0.0260, grad_fn=<MseLossBackward0>)
Current timestep = 433. State = [[-0.22944158  0.07565659]]. Action = [[-0.06716004 -0.23789637  0.04336601  0.866186  ]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 433 is [True, False, False, False, True, False]
Human Feedback received at timestep 433 of 1
Current timestep = 434. State = [[-0.2273485   0.07060987]]. Action = [[-0.21122716 -0.03249183 -0.04530647  0.6640625 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 434 is [True, False, False, False, True, False]
Current timestep = 435. State = [[-0.22855186  0.06590028]]. Action = [[-0.18218133 -0.21247989 -0.02945906 -0.01479512]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 435 is [True, False, False, False, True, False]
Scene graph at timestep 435 is [True, False, False, False, True, False]
State prediction error at timestep 435 is tensor(0.0330, grad_fn=<MseLossBackward0>)
Current timestep = 436. State = [[-0.2324311   0.05944186]]. Action = [[-0.14760782  0.1435127   0.07144693 -0.5950826 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 436 is [True, False, False, False, True, False]
Scene graph at timestep 436 is [True, False, False, False, True, False]
State prediction error at timestep 436 is tensor(0.0306, grad_fn=<MseLossBackward0>)
Current timestep = 437. State = [[-0.23849098  0.05777801]]. Action = [[-0.24135432  0.00660768  0.11792356  0.5140873 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 437 is [True, False, False, False, True, False]
Human Feedback received at timestep 437 of 1
Current timestep = 438. State = [[-0.2458657   0.05717273]]. Action = [[-0.24296084  0.04713529  0.13506544  0.5001869 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 438 is [True, False, False, False, True, False]
Current timestep = 439. State = [[-0.25639382  0.05779598]]. Action = [[ 0.16672134 -0.162823    0.16041815  0.8095763 ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 439 is [True, False, False, False, True, False]
Human Feedback received at timestep 439 of 1
Current timestep = 440. State = [[-0.26153675  0.05441255]]. Action = [[ 0.08356905  0.04535866 -0.04137643 -0.2590834 ]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 440 is [True, False, False, False, True, False]
Current timestep = 441. State = [[-0.2629823   0.05274851]]. Action = [[-0.23432253  0.24298966 -0.07279786  0.4045037 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 441 is [True, False, False, False, True, False]
Human Feedback received at timestep 441 of -1
Current timestep = 442. State = [[-0.26706588  0.05746967]]. Action = [[-0.13331445  0.02270925 -0.14265238 -0.8108663 ]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 442 is [True, False, False, False, True, False]
Current timestep = 443. State = [[-0.27359748  0.06158192]]. Action = [[-0.17030028  0.16813299  0.20133796 -0.4437071 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 443 is [True, False, False, False, True, False]
Human Feedback received at timestep 443 of -1
Current timestep = 444. State = [[-0.28243262  0.06896975]]. Action = [[-0.01781528  0.24882448 -0.16059689 -0.62653923]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 444 is [True, False, False, False, True, False]
Current timestep = 445. State = [[-0.28993994  0.07956655]]. Action = [[ 0.2425853   0.13633883  0.21823502 -0.4946915 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 445 is [True, False, False, False, True, False]
Current timestep = 446. State = [[-0.29311174  0.08837404]]. Action = [[-0.04554483  0.1853447   0.00446767  0.66037965]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 446 is [True, False, False, False, True, False]
Current timestep = 447. State = [[-0.29604116  0.09896307]]. Action = [[ 0.00617546  0.14103985 -0.04783675 -0.25697613]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 447 is [True, False, False, False, True, False]
Human Feedback received at timestep 447 of -1
Current timestep = 448. State = [[-0.29802102  0.10861503]]. Action = [[-0.08574274 -0.20266843  0.17045274 -0.5842132 ]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 448 is [True, False, False, False, True, False]
Current timestep = 449. State = [[-0.2995481   0.11135457]]. Action = [[ 0.1843074   0.13383171 -0.19538096  0.09421408]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 449 is [True, False, False, False, True, False]
Human Feedback received at timestep 449 of -1
Current timestep = 450. State = [[-0.29838407  0.11432871]]. Action = [[ 0.04803711 -0.1806341   0.16375673 -0.8079358 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 450 is [True, False, False, False, True, False]
Current timestep = 451. State = [[-0.29684153  0.11286145]]. Action = [[-0.24195147  0.13638946 -0.0764208  -0.043814  ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 451 is [True, False, False, False, True, False]
Current timestep = 452. State = [[-0.29858178  0.11563555]]. Action = [[-0.11817312 -0.04910173 -0.09910917  0.09428   ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 452 is [True, False, False, False, True, False]
Current timestep = 453. State = [[-0.30139342  0.1170811 ]]. Action = [[ 0.16199023  0.06500524 -0.03648323  0.7845838 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 453 is [True, False, False, False, True, False]
Current timestep = 454. State = [[-0.30187002  0.1184781 ]]. Action = [[ 0.0627746  -0.10686931 -0.23287979 -0.248559  ]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 454 is [True, False, False, False, True, False]
Scene graph at timestep 454 is [True, False, False, False, True, False]
State prediction error at timestep 454 is tensor(0.0642, grad_fn=<MseLossBackward0>)
Current timestep = 455. State = [[-0.3009473   0.11773282]]. Action = [[ 0.16276848 -0.10310346  0.0623056  -0.12591517]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 455 is [True, False, False, False, True, False]
Current timestep = 456. State = [[-0.29795548  0.1145483 ]]. Action = [[-0.24391    -0.08484645  0.01959884 -0.76892203]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 456 is [True, False, False, False, True, False]
Current timestep = 457. State = [[-0.2985633   0.11167822]]. Action = [[-0.03895494  0.01946715 -0.20645802  0.6488712 ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 457 is [True, False, False, False, True, False]
Current timestep = 458. State = [[-0.30066264  0.10971646]]. Action = [[-0.13026607 -0.08395931  0.20529595 -0.6983514 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 458 is [True, False, False, False, True, False]
Current timestep = 459. State = [[-0.30396906  0.10709798]]. Action = [[-0.22454587  0.03126729  0.21581826 -0.62494725]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 459 is [True, False, False, False, True, False]
Current timestep = 460. State = [[-0.30902737  0.10565819]]. Action = [[ 0.1756208  -0.18159893 -0.20362978  0.6815803 ]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 460 is [True, False, False, False, True, False]
Scene graph at timestep 460 is [True, False, False, False, True, False]
State prediction error at timestep 460 is tensor(0.0650, grad_fn=<MseLossBackward0>)
Current timestep = 461. State = [[-0.31029385  0.100949  ]]. Action = [[0.20156771 0.18728667 0.10493171 0.7849951 ]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 461 is [True, False, False, False, True, False]
Current timestep = 462. State = [[-0.3088662   0.10080321]]. Action = [[ 0.0872153 -0.1474008  0.218436  -0.9389544]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 462 is [True, False, False, False, True, False]
Human Feedback received at timestep 462 of -1
Current timestep = 463. State = [[-0.30600715  0.09783928]]. Action = [[ 0.08508813  0.09982932 -0.10306492  0.9115062 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 463 is [True, False, False, False, True, False]
Current timestep = 464. State = [[-0.3031291   0.09732258]]. Action = [[-0.06755148 -0.12809858 -0.20226677 -0.50400436]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 464 is [True, False, False, False, True, False]
Human Feedback received at timestep 464 of -1
Current timestep = 465. State = [[-0.30119914  0.09579156]]. Action = [[ 0.11539328  0.10330862 -0.24787587  0.290493  ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 465 is [True, False, False, False, True, False]
Scene graph at timestep 465 is [True, False, False, False, True, False]
State prediction error at timestep 465 is tensor(0.0705, grad_fn=<MseLossBackward0>)
Current timestep = 466. State = [[-0.29964638  0.09603815]]. Action = [[ 0.04056418 -0.01214768 -0.21841021 -0.45286238]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 466 is [True, False, False, False, True, False]
Scene graph at timestep 466 is [True, False, False, False, True, False]
State prediction error at timestep 466 is tensor(0.0578, grad_fn=<MseLossBackward0>)
Current timestep = 467. State = [[-0.29809585  0.09609861]]. Action = [[ 0.14638942 -0.13852952 -0.1340348   0.9220178 ]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 467 is [True, False, False, False, True, False]
Current timestep = 468. State = [[-0.29462463  0.09285257]]. Action = [[-2.1429318e-01  9.1437519e-02 -2.4491310e-02  3.2782555e-05]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 468 is [True, False, False, False, True, False]
Current timestep = 469. State = [[-0.29449174  0.09277081]]. Action = [[-0.24736682  0.00972143 -0.07826209 -0.03090715]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 469 is [True, False, False, False, True, False]
Scene graph at timestep 469 is [True, False, False, False, True, False]
State prediction error at timestep 469 is tensor(0.0586, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 469 of -1
Current timestep = 470. State = [[-0.2969412   0.09321107]]. Action = [[ 0.14793366 -0.08892524 -0.15738969  0.50279486]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 470 is [True, False, False, False, True, False]
Current timestep = 471. State = [[-0.29650733  0.09224024]]. Action = [[ 0.16928887  0.07225603 -0.08331299  0.06915569]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 471 is [True, False, False, False, True, False]
Current timestep = 472. State = [[-0.29492456  0.0920372 ]]. Action = [[ 0.19418341 -0.10820723 -0.10971439  0.86710835]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 472 is [True, False, False, False, True, False]
Current timestep = 473. State = [[-0.2911145   0.08878505]]. Action = [[-0.20004244 -0.1937932  -0.01084034 -0.4200771 ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 473 is [True, False, False, False, True, False]
Scene graph at timestep 473 is [True, False, False, False, True, False]
State prediction error at timestep 473 is tensor(0.0442, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 473 of -1
Current timestep = 474. State = [[-0.28883842  0.08291454]]. Action = [[-0.05063462 -0.01138559 -0.23105957  0.59956586]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 474 is [True, False, False, False, True, False]
Current timestep = 475. State = [[-0.28823155  0.07872158]]. Action = [[-0.09686744 -0.19204725  0.17605042  0.22140479]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 475 is [True, False, False, False, True, False]
Human Feedback received at timestep 475 of 1
Current timestep = 476. State = [[-0.28977606  0.07206347]]. Action = [[-0.0464218   0.21445626  0.10505283  0.17878878]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 476 is [True, False, False, False, True, False]
Current timestep = 477. State = [[-0.2916536   0.07478309]]. Action = [[-0.14448692  0.09155661  0.01225483  0.28349924]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 477 is [True, False, False, False, True, False]
Current timestep = 478. State = [[-0.2963777   0.08079787]]. Action = [[ 0.05441555 -0.2397646   0.16713846 -0.93396616]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 478 is [True, False, False, False, True, False]
Current timestep = 479. State = [[-0.3003166   0.08516753]]. Action = [[ 0.17676306  0.09630769  0.09337574 -0.90368253]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 479 is [True, False, False, False, True, False]
Current timestep = 480. State = [[-0.30180037  0.08600092]]. Action = [[ 0.09238619  0.04907864 -0.0660819  -0.05145156]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 480 is [True, False, False, False, True, False]
Current timestep = 481. State = [[-0.30200672  0.08805231]]. Action = [[-0.07462344 -0.01106577 -0.16599704 -0.39157444]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 481 is [True, False, False, False, True, False]
Scene graph at timestep 481 is [True, False, False, False, True, False]
State prediction error at timestep 481 is tensor(0.0563, grad_fn=<MseLossBackward0>)
Current timestep = 482. State = [[-0.30167204  0.08593241]]. Action = [[-0.2100802  -0.17320034  0.17348045 -0.65935105]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 482 is [True, False, False, False, True, False]
Current timestep = 483. State = [[-0.3042551   0.08586749]]. Action = [[ 0.24261522  0.19293755 -0.12320736 -0.646887  ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 483 is [True, False, False, False, True, False]
Current timestep = 484. State = [[-0.30395758  0.08535916]]. Action = [[-0.09868596  0.1732691  -0.02579735 -0.07366562]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 484 is [True, False, False, False, True, False]
Current timestep = 485. State = [[-0.30419564  0.08715916]]. Action = [[-0.20529999  0.1505124  -0.05472447 -0.70521474]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 485 is [True, False, False, False, True, False]
Current timestep = 486. State = [[-0.30589464  0.08866105]]. Action = [[-0.06315516 -0.05081935 -0.21406282 -0.6612668 ]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 486 is [True, False, False, False, True, False]
Human Feedback received at timestep 486 of -1
Current timestep = 487. State = [[-0.30873597  0.09226977]]. Action = [[ 0.20945865  0.14631575 -0.180806    0.6093693 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 487 is [True, False, False, False, True, False]
Current timestep = 488. State = [[-0.30902395  0.095777  ]]. Action = [[ 0.03802082  0.19752127  0.23060888 -0.1810801 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 488 is [True, False, False, False, True, False]
Human Feedback received at timestep 488 of -1
Current timestep = 489. State = [[-0.30780494  0.10025941]]. Action = [[ 0.1951774  -0.11257923 -0.24902055 -0.40708613]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 489 is [True, False, False, False, True, False]
Current timestep = 490. State = [[-0.30421996  0.10157661]]. Action = [[ 0.14756972  0.11423475  0.01739287 -0.23237932]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 490 is [True, False, False, False, True, False]
Scene graph at timestep 490 is [True, False, False, False, True, False]
State prediction error at timestep 490 is tensor(0.0612, grad_fn=<MseLossBackward0>)
Current timestep = 491. State = [[-0.29808602  0.10499397]]. Action = [[ 0.17007661 -0.03119515 -0.15369466 -0.24252224]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 491 is [True, False, False, False, True, False]
Scene graph at timestep 491 is [True, False, False, False, True, False]
State prediction error at timestep 491 is tensor(0.0602, grad_fn=<MseLossBackward0>)
Current timestep = 492. State = [[-0.29068133  0.10708599]]. Action = [[ 0.016774    0.12405986  0.12829551 -0.35204238]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 492 is [True, False, False, False, True, False]
Human Feedback received at timestep 492 of -1
Current timestep = 493. State = [[-0.2850231   0.11147332]]. Action = [[ 0.01919547  0.15387732 -0.14717856 -0.71714187]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 493 is [True, False, False, False, True, False]
Scene graph at timestep 493 is [True, False, False, False, True, False]
State prediction error at timestep 493 is tensor(0.0536, grad_fn=<MseLossBackward0>)
Current timestep = 494. State = [[-0.27905834  0.11807228]]. Action = [[-0.01882751 -0.1945853   0.16711682 -0.20175159]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 494 is [True, False, False, False, True, False]
Human Feedback received at timestep 494 of 1
Current timestep = 495. State = [[-0.2765119   0.11881723]]. Action = [[0.054216   0.04818913 0.20721889 0.06876302]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 495 is [True, False, False, False, True, False]
Current timestep = 496. State = [[-0.27446866  0.11997019]]. Action = [[ 0.1532036  -0.17948508 -0.11086451 -0.14420271]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 496 is [True, False, False, False, True, False]
Current timestep = 497. State = [[-0.2704239   0.11614748]]. Action = [[ 0.21972027 -0.04684529  0.00900519 -0.15000159]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 497 is [True, False, False, False, True, False]
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(0.0509, grad_fn=<MseLossBackward0>)
Current timestep = 498. State = [[-0.26235804  0.11150384]]. Action = [[ 0.24150032 -0.19868612 -0.01153383 -0.6509905 ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 498 is [True, False, False, False, True, False]
Human Feedback received at timestep 498 of 1
Current timestep = 499. State = [[-0.2520865   0.10486419]]. Action = [[ 0.04589212  0.20889896  0.16493988 -0.0060041 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 499 is [True, False, False, False, True, False]
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(0.0510, grad_fn=<MseLossBackward0>)
Current timestep = 500. State = [[-0.2413763   0.10524151]]. Action = [[ 0.02223107 -0.03900665  0.10535368 -0.44918495]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 500 is [True, False, False, False, True, False]
Human Feedback received at timestep 500 of 1
Current timestep = 501. State = [[-0.23308301  0.10513224]]. Action = [[ 0.08185288  0.06973344 -0.09892207  0.17022693]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 501 is [True, False, False, False, True, False]
Scene graph at timestep 501 is [True, False, False, False, True, False]
State prediction error at timestep 501 is tensor(0.0485, grad_fn=<MseLossBackward0>)
Current timestep = 502. State = [[-0.22574306  0.10642426]]. Action = [[ 0.24194854 -0.14707738  0.23148847  0.52146196]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 502 is [True, False, False, False, True, False]
Human Feedback received at timestep 502 of 1
Current timestep = 503. State = [[-0.21719079  0.10478329]]. Action = [[ 0.12516835  0.17008898  0.08257702 -0.11701512]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 503 is [True, False, False, False, True, False]
Current timestep = 504. State = [[-0.20819047  0.10803728]]. Action = [[-0.14216511  0.15857524  0.12417001  0.72881794]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 504 is [True, False, False, False, True, False]
Scene graph at timestep 504 is [True, False, False, False, True, False]
State prediction error at timestep 504 is tensor(0.0415, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 504 of 1
Current timestep = 505. State = [[-0.20315346  0.11356173]]. Action = [[-0.13052323 -0.08570004 -0.24482742  0.7778163 ]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 505 is [True, False, False, False, True, False]
Current timestep = 506. State = [[-0.20331846  0.11525593]]. Action = [[-0.06290603 -0.0523534  -0.02305762  0.774933  ]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 506 is [True, False, False, False, True, False]
Scene graph at timestep 506 is [True, False, False, False, True, False]
State prediction error at timestep 506 is tensor(0.0398, grad_fn=<MseLossBackward0>)
Current timestep = 507. State = [[-0.20377937  0.11561694]]. Action = [[-0.04581495  0.18805265 -0.21451959  0.9059336 ]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 507 is [True, False, False, False, True, False]
Current timestep = 508. State = [[-0.20545077  0.11863784]]. Action = [[-0.06144807 -0.13408285  0.1531328  -0.37792307]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 508 is [True, False, False, False, True, False]
Scene graph at timestep 508 is [True, False, False, False, True, False]
State prediction error at timestep 508 is tensor(0.0307, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 508 of 1
Current timestep = 509. State = [[-0.2057903   0.11890578]]. Action = [[ 0.22552568  0.11342824 -0.07478799 -0.7929649 ]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 509 is [True, False, False, False, True, False]
Current timestep = 510. State = [[-0.20395379  0.12014528]]. Action = [[ 0.2378596  -0.22268087 -0.0039973   0.21174324]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 510 is [True, False, False, False, True, False]
Scene graph at timestep 510 is [True, False, False, False, True, False]
State prediction error at timestep 510 is tensor(0.0369, grad_fn=<MseLossBackward0>)
Current timestep = 511. State = [[-0.19925217  0.11637795]]. Action = [[ 0.1785301  -0.05539821 -0.0166025  -0.3633207 ]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 511 is [True, False, False, False, True, False]
Human Feedback received at timestep 511 of 1
Current timestep = 512. State = [[-0.1909115   0.11219035]]. Action = [[ 0.18280542 -0.13161212 -0.01811567  0.7472129 ]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 512 is [True, False, False, False, True, False]
Current timestep = 513. State = [[-0.181489    0.10596597]]. Action = [[ 0.07058221 -0.16850795  0.19575632  0.34089684]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 513 is [True, False, False, False, True, False]
Human Feedback received at timestep 513 of 1
Current timestep = 514. State = [[-0.17262596  0.09788681]]. Action = [[-0.19868205  0.03488263 -0.05384767 -0.17455852]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 514 is [True, False, False, False, True, False]
Current timestep = 515. State = [[-0.16904038  0.09438869]]. Action = [[-0.12610453  0.09480616 -0.17506039 -0.8972438 ]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 515 is [True, False, False, False, True, False]
Current timestep = 516. State = [[-0.16953653  0.09469889]]. Action = [[-0.08797374  0.03380442  0.18540049 -0.97962   ]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 516 is [True, False, False, False, True, False]
Human Feedback received at timestep 516 of 1
Current timestep = 517. State = [[-0.17050664  0.09642233]]. Action = [[ 0.11669856  0.1320824  -0.04591486 -0.28167975]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 517 is [True, False, False, False, True, False]
Scene graph at timestep 517 is [True, False, False, False, True, False]
State prediction error at timestep 517 is tensor(0.0282, grad_fn=<MseLossBackward0>)
Current timestep = 518. State = [[-0.17089897  0.09904767]]. Action = [[-0.21001357 -0.17042549 -0.18052405  0.69756424]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 518 is [True, False, False, False, True, False]
Scene graph at timestep 518 is [True, False, False, False, True, False]
State prediction error at timestep 518 is tensor(0.0315, grad_fn=<MseLossBackward0>)
Current timestep = 519. State = [[-0.1726497   0.09785541]]. Action = [[-0.21026182  0.11435351  0.10572684 -0.5958358 ]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 519 is [True, False, False, False, True, False]
Current timestep = 520. State = [[-0.17651717  0.10035807]]. Action = [[ 0.18198892  0.21296823 -0.21728365  0.5744257 ]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 520 is [True, False, False, False, True, False]
Current timestep = 521. State = [[-0.17869118  0.10547231]]. Action = [[ 0.17249465 -0.0972231   0.06593147 -0.8461533 ]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 521 is [True, False, False, False, True, False]
Current timestep = 522. State = [[-0.17756751  0.10702269]]. Action = [[ 0.1980896   0.07211959 -0.12764546  0.8008146 ]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 522 is [True, False, False, False, True, False]
Human Feedback received at timestep 522 of 1
Current timestep = 523. State = [[-0.17385598  0.1094175 ]]. Action = [[-0.23837489  0.01616195  0.08610138 -0.17469162]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 523 is [True, False, False, False, True, False]
Current timestep = 524. State = [[-0.17388806  0.11083847]]. Action = [[ 0.09139162 -0.16622569 -0.02575503 -0.7645536 ]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 524 is [True, False, False, False, True, False]
Current timestep = 525. State = [[-0.1724349   0.10782634]]. Action = [[ 0.23805493 -0.16336086  0.14325732  0.04693067]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 525 is [True, False, False, False, True, False]
Current timestep = 526. State = [[-0.16930762  0.10240159]]. Action = [[-0.09763256 -0.0859997  -0.1786597  -0.47537816]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 526 is [True, False, False, False, True, False]
Current timestep = 527. State = [[-0.16723587  0.097037  ]]. Action = [[-0.14333826  0.12155938  0.14068815  0.57581973]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 527 is [True, False, False, False, True, False]
Scene graph at timestep 527 is [True, False, False, False, True, False]
State prediction error at timestep 527 is tensor(0.0311, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 527 of 1
Current timestep = 528. State = [[-0.16733885  0.09667948]]. Action = [[ 0.04105204 -0.06818169  0.17867416  0.18196094]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 528 is [True, False, False, False, True, False]
Scene graph at timestep 528 is [True, False, False, False, True, False]
State prediction error at timestep 528 is tensor(0.0273, grad_fn=<MseLossBackward0>)
Current timestep = 529. State = [[-0.16690697  0.09565878]]. Action = [[0.22933465 0.16353047 0.06065506 0.7012117 ]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 529 is [True, False, False, False, True, False]
Scene graph at timestep 529 is [True, False, False, False, True, False]
State prediction error at timestep 529 is tensor(0.0305, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 529 of 1
Current timestep = 530. State = [[-0.16386998  0.09865844]]. Action = [[-0.18652782  0.24641186  0.15717071 -0.6181594 ]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 530 is [True, False, False, False, True, False]
Scene graph at timestep 530 is [True, False, False, False, True, False]
State prediction error at timestep 530 is tensor(0.0270, grad_fn=<MseLossBackward0>)
Current timestep = 531. State = [[-0.1635738  0.105404 ]]. Action = [[-0.2017196  -0.14954865  0.13919604 -0.479398  ]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 531 is [True, False, False, False, True, False]
Human Feedback received at timestep 531 of 1
Current timestep = 532. State = [[-0.16510798  0.10759028]]. Action = [[0.11464921 0.1015709  0.03831959 0.1331358 ]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 532 is [True, False, False, False, True, False]
Current timestep = 533. State = [[-0.16622794  0.10990562]]. Action = [[-0.02400817 -0.10724506 -0.06793514  0.34462285]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 533 is [True, False, False, False, True, False]
Scene graph at timestep 533 is [True, False, False, False, True, False]
State prediction error at timestep 533 is tensor(0.0316, grad_fn=<MseLossBackward0>)
Current timestep = 534. State = [[-0.16615728  0.10968666]]. Action = [[-0.22576392 -0.09523493  0.11853978 -0.5598222 ]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 534 is [True, False, False, False, True, False]
Current timestep = 535. State = [[-0.16851614  0.10732462]]. Action = [[ 0.07431227 -0.22029541 -0.04019919  0.572804  ]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 535 is [True, False, False, False, True, False]
Current timestep = 536. State = [[-0.16938739  0.10076805]]. Action = [[ 0.20732218 -0.01823473  0.14406353  0.77230334]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 536 is [True, False, False, False, True, False]
Scene graph at timestep 536 is [True, False, False, False, True, False]
State prediction error at timestep 536 is tensor(0.0278, grad_fn=<MseLossBackward0>)
Current timestep = 537. State = [[-0.16762364  0.09639193]]. Action = [[ 0.17735213  0.15619886 -0.12443236 -0.9806415 ]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 537 is [True, False, False, False, True, False]
Human Feedback received at timestep 537 of 1
Current timestep = 538. State = [[-0.16474402  0.09578244]]. Action = [[ 0.08237797 -0.24776885  0.2297352  -0.5560324 ]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 538 is [True, False, False, False, True, False]
Current timestep = 539. State = [[-0.16132383  0.08902312]]. Action = [[-0.02100278 -0.18018982 -0.1365863  -0.2840904 ]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 539 is [True, False, False, False, True, False]
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(0.0222, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 539 of 1
Current timestep = 540. State = [[-0.15587114  0.07635903]]. Action = [[ 0.1786348   0.18855733 -0.10191348 -0.42179275]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 540 is [True, False, False, False, True, False]
Current timestep = 541. State = [[-0.15190743  0.07779266]]. Action = [[-0.15800156  0.12579215 -0.1145232  -0.83937025]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 541 is [True, False, False, False, True, False]
Human Feedback received at timestep 541 of 1
Current timestep = 542. State = [[-0.15151386  0.08113797]]. Action = [[ 0.04239613 -0.2118715  -0.24594831  0.9404931 ]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 542 is [True, False, False, False, True, False]
Scene graph at timestep 542 is [True, False, False, False, True, False]
State prediction error at timestep 542 is tensor(0.0236, grad_fn=<MseLossBackward0>)
Current timestep = 543. State = [[-0.15037532  0.07793739]]. Action = [[ 0.18419802 -0.04475266  0.18721068 -0.4721111 ]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 543 is [True, False, False, False, True, False]
Scene graph at timestep 543 is [True, False, False, False, True, False]
State prediction error at timestep 543 is tensor(0.0164, grad_fn=<MseLossBackward0>)
Current timestep = 544. State = [[-0.14713608  0.07509121]]. Action = [[-0.08926003  0.18714717 -0.13211553  0.67991495]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 544 is [True, False, False, False, True, False]
Current timestep = 545. State = [[-0.14571786  0.07762424]]. Action = [[-0.02751818 -0.18567309 -0.22950725  0.47194982]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 545 is [True, False, False, False, True, False]
Current timestep = 546. State = [[-0.14478022  0.07529837]]. Action = [[-0.10688855  0.17343944 -0.16105685 -0.33416986]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 546 is [True, False, False, False, True, False]
Current timestep = 547. State = [[-0.1456975  0.076685 ]]. Action = [[-0.12083858 -0.23448345  0.18883306  0.6217084 ]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 547 is [True, False, False, False, True, False]
Human Feedback received at timestep 547 of 1
Current timestep = 548. State = [[-0.1473782   0.07388322]]. Action = [[ 0.20880455  0.18555042 -0.05337209  0.40902376]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 548 is [True, False, False, False, True, False]
Current timestep = 549. State = [[-0.14698794  0.07500989]]. Action = [[-0.06999183  0.15804946 -0.05642207 -0.9365571 ]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 549 is [True, False, False, False, True, False]
Scene graph at timestep 549 is [True, False, False, False, True, False]
State prediction error at timestep 549 is tensor(0.0209, grad_fn=<MseLossBackward0>)
Current timestep = 550. State = [[-0.1473903   0.07941719]]. Action = [[-0.13782454  0.04142034 -0.09000301 -0.13116324]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 550 is [True, False, False, False, True, False]
Current timestep = 551. State = [[-0.14935511  0.08368623]]. Action = [[-0.07865392  0.14416191  0.04802203 -0.41768026]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 551 is [True, False, False, False, True, False]
Current timestep = 552. State = [[-0.15242262  0.09051444]]. Action = [[-0.07892999  0.09243715  0.21339324 -0.23048872]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 552 is [True, False, False, False, True, False]
Scene graph at timestep 552 is [True, False, False, False, True, False]
State prediction error at timestep 552 is tensor(0.0235, grad_fn=<MseLossBackward0>)
Current timestep = 553. State = [[-0.15537448  0.0972423 ]]. Action = [[ 0.01053554 -0.10245159  0.16419423 -0.13289559]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 553 is [True, False, False, False, True, False]
Human Feedback received at timestep 553 of 1
Current timestep = 554. State = [[-0.15671621  0.09936403]]. Action = [[ 0.18084103  0.23736876 -0.08786491 -0.6156328 ]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 554 is [True, False, False, False, True, False]
Current timestep = 555. State = [[-0.15625685  0.10523777]]. Action = [[-0.02257316  0.03447723  0.09040177  0.25241554]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 555 is [True, False, False, False, True, False]
Current timestep = 556. State = [[-0.1564231   0.10948307]]. Action = [[-0.16129948 -0.22028895 -0.01457649 -0.21468842]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 556 is [True, False, False, False, True, False]
Scene graph at timestep 556 is [True, False, False, False, True, False]
State prediction error at timestep 556 is tensor(0.0230, grad_fn=<MseLossBackward0>)
Current timestep = 557. State = [[-0.15732183  0.10910034]]. Action = [[ 0.07921985  0.15136033  0.15375474 -0.7844381 ]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 557 is [True, False, False, False, True, False]
Human Feedback received at timestep 557 of 1
Current timestep = 558. State = [[-0.15777667  0.11017242]]. Action = [[ 0.1400348  -0.1341649  -0.11740103 -0.17573798]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 558 is [True, False, False, False, True, False]
Current timestep = 559. State = [[-0.1561521   0.10788584]]. Action = [[ 0.12940493 -0.22598855  0.2238755  -0.8369373 ]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 559 is [True, False, False, False, True, False]
Current timestep = 560. State = [[-0.1530295   0.10170954]]. Action = [[-0.09594466  0.1131652   0.01615497  0.9375632 ]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 560 is [True, False, False, False, True, False]
Current timestep = 561. State = [[-0.15253891  0.10055182]]. Action = [[-0.03549595  0.02889591 -0.17950073 -0.05836856]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 561 is [True, False, False, False, True, False]
Current timestep = 562. State = [[-0.15262182  0.10137502]]. Action = [[-0.06373809 -0.01006353 -0.18389715 -0.59274405]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 562 is [True, False, False, False, True, False]
Human Feedback received at timestep 562 of 1
Current timestep = 563. State = [[-0.15279828  0.1015415 ]]. Action = [[ 0.23186335  0.00451005 -0.05632952 -0.06339782]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 563 is [True, False, False, False, True, False]
Current timestep = 564. State = [[-0.15060718  0.10160273]]. Action = [[ 0.18733478  0.15535033 -0.12618428  0.3664478 ]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 564 is [True, False, False, False, True, False]
Current timestep = 565. State = [[-0.14614236  0.10544121]]. Action = [[ 0.05882326  0.15602076  0.19833595 -0.92032325]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 565 is [True, False, False, False, True, False]
Human Feedback received at timestep 565 of 1
Current timestep = 566. State = [[-0.14019667  0.11187247]]. Action = [[-0.00132343 -0.02032606  0.13755494 -0.08056515]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 566 is [True, False, False, False, True, False]
Scene graph at timestep 566 is [True, False, False, False, True, False]
State prediction error at timestep 566 is tensor(0.0248, grad_fn=<MseLossBackward0>)
Current timestep = 567. State = [[-0.13619359  0.1160108 ]]. Action = [[ 0.12188232  0.07244617 -0.00852616  0.5677208 ]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 567 is [True, False, False, False, True, False]
Current timestep = 568. State = [[-0.1309986   0.12021659]]. Action = [[-0.04343671  0.07826638  0.23010781 -0.34987712]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 568 is [True, False, False, False, True, False]
Current timestep = 569. State = [[-0.12726012  0.12582459]]. Action = [[ 0.21018043  0.20339939 -0.24825466  0.7516494 ]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 569 is [True, False, False, False, True, False]
Human Feedback received at timestep 569 of 1
Current timestep = 570. State = [[-0.12196802  0.13300332]]. Action = [[ 0.03463662 -0.07182667  0.24192262 -0.23393238]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 570 is [True, False, False, False, False, True]
Current timestep = 571. State = [[-0.11632764  0.13645072]]. Action = [[ 0.05305985 -0.02714472  0.24651307  0.7352762 ]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 571 is [True, False, False, False, False, True]
Current timestep = 572. State = [[-0.11120454  0.13897829]]. Action = [[0.10364476 0.23666209 0.22456706 0.7069384 ]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 572 is [True, False, False, False, False, True]
Current timestep = 573. State = [[-0.10614259  0.14549786]]. Action = [[-0.19775985 -0.03949223 -0.07997288  0.6663703 ]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 573 is [True, False, False, False, False, True]
Current timestep = 574. State = [[-0.10468813  0.14915529]]. Action = [[ 0.01297385 -0.1521466   0.01692721  0.27689946]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 574 is [True, False, False, False, False, True]
Current timestep = 575. State = [[-0.10424311  0.14793165]]. Action = [[-0.02547908  0.0609023  -0.19900103  0.74184394]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 575 is [True, False, False, False, False, True]
Current timestep = 576. State = [[-0.10464271  0.14852813]]. Action = [[-0.1538654   0.18401375  0.21903223 -0.4359668 ]]. Reward = [0.]
Curr episode timestep = 576
Scene graph at timestep 576 is [True, False, False, False, False, True]
Current timestep = 577. State = [[-0.1068093   0.15297706]]. Action = [[-0.15473592 -0.11074394 -0.08022904  0.48023248]]. Reward = [0.]
Curr episode timestep = 577
Scene graph at timestep 577 is [True, False, False, False, False, True]
Human Feedback received at timestep 577 of 1
Current timestep = 578. State = [[-0.10865675  0.15437655]]. Action = [[-0.02706447  0.0979614  -0.18020105 -0.76718426]]. Reward = [0.]
Curr episode timestep = 578
Scene graph at timestep 578 is [True, False, False, False, False, True]
Current timestep = 579. State = [[-0.11086046  0.15792502]]. Action = [[ 0.10244116  0.17377275 -0.1314235  -0.5215869 ]]. Reward = [0.]
Curr episode timestep = 579
Scene graph at timestep 579 is [True, False, False, False, False, True]
Current timestep = 580. State = [[-0.11275053  0.16234861]]. Action = [[-0.15026227 -0.24889606  0.09054112 -0.52445275]]. Reward = [0.]
Curr episode timestep = 580
Scene graph at timestep 580 is [True, False, False, False, False, True]
Current timestep = 581. State = [[-0.11422673  0.161252  ]]. Action = [[ 0.1219165   0.13577196  0.18960276 -0.16657639]]. Reward = [0.]
Curr episode timestep = 581
Scene graph at timestep 581 is [True, False, False, False, False, True]
Current timestep = 582. State = [[-0.11474795  0.16200367]]. Action = [[ 0.06359801 -0.15612105  0.09561139  0.7543514 ]]. Reward = [0.]
Curr episode timestep = 582
Scene graph at timestep 582 is [True, False, False, False, False, True]
Human Feedback received at timestep 582 of 1
Current timestep = 583. State = [[-0.11406825  0.16039243]]. Action = [[-0.19551086  0.22903675  0.12966108 -0.16540015]]. Reward = [0.]
Curr episode timestep = 583
Scene graph at timestep 583 is [True, False, False, False, False, True]
Scene graph at timestep 583 is [True, False, False, False, False, True]
State prediction error at timestep 583 is tensor(0.0353, grad_fn=<MseLossBackward0>)
Current timestep = 584. State = [[-0.11660331  0.16473451]]. Action = [[0.11450946 0.1088964  0.09708428 0.53009677]]. Reward = [0.]
Curr episode timestep = 584
Scene graph at timestep 584 is [True, False, False, False, False, True]
Current timestep = 585. State = [[-0.11815327  0.16882278]]. Action = [[ 0.00389311 -0.1423532   0.06656387 -0.6675591 ]]. Reward = [0.]
Curr episode timestep = 585
Scene graph at timestep 585 is [True, False, False, False, False, True]
Scene graph at timestep 585 is [True, False, False, False, False, True]
State prediction error at timestep 585 is tensor(0.0292, grad_fn=<MseLossBackward0>)
Current timestep = 586. State = [[-0.11816332  0.16877078]]. Action = [[ 0.01339215 -0.00781775  0.10383835 -0.9825558 ]]. Reward = [0.]
Curr episode timestep = 586
Scene graph at timestep 586 is [True, False, False, False, False, True]
Current timestep = 587. State = [[-0.11788549  0.1680862 ]]. Action = [[-0.14343518 -0.04909077  0.21350414  0.19064772]]. Reward = [0.]
Curr episode timestep = 587
Scene graph at timestep 587 is [True, False, False, False, False, True]
Current timestep = 588. State = [[-0.11835153  0.16733463]]. Action = [[ 0.01694098 -0.18817452  0.12727135 -0.74670553]]. Reward = [0.]
Curr episode timestep = 588
Scene graph at timestep 588 is [True, False, False, False, False, True]
Human Feedback received at timestep 588 of 1
Current timestep = 589. State = [[-0.11840583  0.16316405]]. Action = [[0.11171588 0.06201649 0.13590872 0.48469043]]. Reward = [0.]
Curr episode timestep = 589
Scene graph at timestep 589 is [True, False, False, False, False, True]
Scene graph at timestep 589 is [True, False, False, False, False, True]
State prediction error at timestep 589 is tensor(0.0316, grad_fn=<MseLossBackward0>)
Current timestep = 590. State = [[-0.11844413  0.16179807]]. Action = [[-0.22630368  0.23911506  0.16190666  0.5644152 ]]. Reward = [0.]
Curr episode timestep = 590
Scene graph at timestep 590 is [True, False, False, False, False, True]
Human Feedback received at timestep 590 of 1
Current timestep = 591. State = [[-0.12155081  0.16651616]]. Action = [[-0.16309208  0.03782687 -0.0087077   0.4672519 ]]. Reward = [0.]
Curr episode timestep = 591
Scene graph at timestep 591 is [True, False, False, False, False, True]
Scene graph at timestep 591 is [True, False, False, False, False, True]
State prediction error at timestep 591 is tensor(0.0348, grad_fn=<MseLossBackward0>)
Current timestep = 592. State = [[-0.12666869  0.17043845]]. Action = [[-0.05490589 -0.07524747 -0.14057654 -0.02628273]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 592 is [True, False, False, False, False, True]
Scene graph at timestep 592 is [True, False, False, False, False, True]
State prediction error at timestep 592 is tensor(0.0351, grad_fn=<MseLossBackward0>)
Current timestep = 593. State = [[-0.1322016   0.17125311]]. Action = [[-0.0475446  -0.06569022  0.20779032  0.9892664 ]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 593 is [True, False, False, False, False, True]
Current timestep = 594. State = [[-0.13565914  0.17015068]]. Action = [[-0.16194421 -0.19369729 -0.22107413  0.07841158]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 594 is [True, False, False, False, False, True]
Current timestep = 595. State = [[-0.14060712  0.16451958]]. Action = [[ 0.06801319  0.12666547 -0.09735    -0.49601126]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 595 is [True, False, False, False, False, True]
Current timestep = 596. State = [[-0.1449189   0.16472699]]. Action = [[-0.06951576  0.22386187  0.05112332  0.9023584 ]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 596 is [True, False, False, False, False, True]
Current timestep = 597. State = [[-0.14828597  0.17018299]]. Action = [[-0.23644106  0.10237205 -0.16851139 -0.36075795]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 597 is [True, False, False, False, False, True]
Current timestep = 598. State = [[-0.15564923  0.17648773]]. Action = [[-0.02142888 -0.00450788 -0.07033056 -0.37717366]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 598 is [True, False, False, False, False, True]
Scene graph at timestep 598 is [True, False, False, False, False, True]
State prediction error at timestep 598 is tensor(0.0387, grad_fn=<MseLossBackward0>)
Current timestep = 599. State = [[-0.16317919  0.18046582]]. Action = [[-0.05240636  0.07503346 -0.20851307 -0.4948995 ]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 599 is [True, False, False, False, False, True]
Human Feedback received at timestep 599 of -1
Current timestep = 600. State = [[-0.16937406  0.18564908]]. Action = [[-0.0783914   0.16786003 -0.23579869  0.8964596 ]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 600 is [True, False, False, False, False, True]
Scene graph at timestep 600 is [True, False, False, False, False, True]
State prediction error at timestep 600 is tensor(0.0516, grad_fn=<MseLossBackward0>)
Current timestep = 601. State = [[-0.17420374  0.19315718]]. Action = [[-0.06627104 -0.04771453 -0.08753651 -0.8332626 ]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 601 is [True, False, False, False, False, True]
Human Feedback received at timestep 601 of -1
Current timestep = 602. State = [[-0.17876801  0.1961444 ]]. Action = [[-0.24460356 -0.1961294   0.0555099  -0.53988665]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 602 is [True, False, False, False, False, True]
Current timestep = 603. State = [[-0.18658112  0.19300172]]. Action = [[ 0.15143484 -0.05320913  0.00804147  0.02834129]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 603 is [True, False, False, False, False, True]
Current timestep = 604. State = [[-0.19140667  0.18938038]]. Action = [[ 0.23332101  0.05629408 -0.16058815  0.6058736 ]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 604 is [True, False, False, False, False, True]
Current timestep = 605. State = [[-0.19124298  0.18814856]]. Action = [[-0.1010344   0.18280694 -0.10029194  0.3625015 ]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 605 is [True, False, False, False, False, True]
Current timestep = 606. State = [[-0.19271219  0.19127454]]. Action = [[-0.01843186 -0.02690311  0.20265156  0.25468063]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 606 is [True, False, False, False, False, True]
Human Feedback received at timestep 606 of -1
Current timestep = 607. State = [[-0.19380765  0.19310223]]. Action = [[0.10294506 0.07902786 0.05448288 0.14701807]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 607 is [True, False, False, False, False, True]
Current timestep = 608. State = [[-0.19413196  0.19558069]]. Action = [[-0.23599887  0.24973676 -0.0660747   0.16068304]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 608 is [True, False, False, False, False, True]
Scene graph at timestep 608 is [True, False, False, False, False, True]
State prediction error at timestep 608 is tensor(0.0601, grad_fn=<MseLossBackward0>)
Current timestep = 609. State = [[-0.19785704  0.20297277]]. Action = [[ 0.15990245 -0.09295401 -0.12623551  0.05816877]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 609 is [True, False, False, False, False, True]
Scene graph at timestep 609 is [True, False, False, False, False, True]
State prediction error at timestep 609 is tensor(0.0546, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 609 of -1
Current timestep = 610. State = [[-0.19934684  0.20572223]]. Action = [[-0.18366855  0.23217547 -0.14259335 -0.72642225]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 610 is [True, False, False, False, False, True]
Current timestep = 611. State = [[-0.20277911  0.21293257]]. Action = [[ 0.00210562 -0.2164334   0.2460886   0.8458307 ]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 611 is [True, False, False, False, False, True]
Current timestep = 612. State = [[-0.2048832   0.21387133]]. Action = [[-0.04084554  0.22595298  0.11179438 -0.06595856]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 612 is [True, False, False, False, False, True]
Human Feedback received at timestep 612 of -1
Current timestep = 613. State = [[-0.20781615  0.21887745]]. Action = [[-0.20890483  0.18773663 -0.05163728 -0.32690245]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 613 is [True, False, False, False, False, True]
Current timestep = 614. State = [[-0.21395968  0.22775832]]. Action = [[ 0.04758346 -0.02343817 -0.13978577  0.22672677]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 614 is [True, False, False, False, False, True]
Current timestep = 615. State = [[-0.21754447  0.23283216]]. Action = [[ 0.1194959  -0.14071475  0.17298272  0.3957746 ]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 615 is [True, False, False, False, False, True]
Current timestep = 616. State = [[-0.2170072   0.23217209]]. Action = [[ 0.12776822 -0.16404466 -0.23594528  0.7569829 ]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 616 is [True, False, False, False, False, True]
Current timestep = 617. State = [[-0.214087   0.2283437]]. Action = [[ 0.21410573  0.24846673 -0.04731901 -0.47485673]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 617 is [True, False, False, False, False, True]
Human Feedback received at timestep 617 of -1
Current timestep = 618. State = [[-0.21051404  0.23084249]]. Action = [[ 0.23376334  0.07527491 -0.05598892 -0.97987205]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 618 is [True, False, False, False, False, True]
Current timestep = 619. State = [[-0.2030195   0.23491387]]. Action = [[ 0.09210536  0.0951165  -0.05133903  0.23448598]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 619 is [True, False, False, False, False, True]
Current timestep = 620. State = [[-0.19516212  0.24064632]]. Action = [[0.24741578 0.07058072 0.15193832 0.98905706]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 620 is [True, False, False, False, False, True]
Current timestep = 621. State = [[-0.18482375  0.24550633]]. Action = [[ 0.18668681  0.01454845 -0.11912122 -0.9938354 ]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 621 is [True, False, False, False, False, True]
Scene graph at timestep 621 is [True, False, False, False, False, True]
State prediction error at timestep 621 is tensor(0.0670, grad_fn=<MseLossBackward0>)
Current timestep = 622. State = [[-0.17264172  0.24954562]]. Action = [[ 0.00247243  0.24167424 -0.1394255   0.7894455 ]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 622 is [True, False, False, False, False, True]
Scene graph at timestep 622 is [True, False, False, False, False, True]
State prediction error at timestep 622 is tensor(0.0698, grad_fn=<MseLossBackward0>)
Current timestep = 623. State = [[-0.1633319   0.25840226]]. Action = [[-0.08956347 -0.1487249  -0.16533308  0.44535136]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 623 is [True, False, False, False, False, True]
Scene graph at timestep 623 is [True, False, False, False, False, True]
State prediction error at timestep 623 is tensor(0.0634, grad_fn=<MseLossBackward0>)
Current timestep = 624. State = [[-0.15737231  0.261418  ]]. Action = [[-0.01416804  0.16606814  0.20132905  0.6773325 ]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 624 is [True, False, False, False, False, True]
Current timestep = 625. State = [[-0.15592274  0.26623157]]. Action = [[-0.10749906 -0.10964328  0.23875105 -0.15665197]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 625 is [True, False, False, False, False, True]
Current timestep = 626. State = [[-0.15602797  0.26720133]]. Action = [[0.19697303 0.04981488 0.04912344 0.95648146]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 626 is [True, False, False, False, False, True]
Scene graph at timestep 626 is [True, False, False, False, False, True]
State prediction error at timestep 626 is tensor(0.0657, grad_fn=<MseLossBackward0>)
Current timestep = 627. State = [[-0.15401004  0.2686126 ]]. Action = [[ 0.22110456  0.09243843  0.06665534 -0.3434568 ]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 627 is [True, False, False, False, False, True]
Scene graph at timestep 627 is [True, False, False, False, False, True]
State prediction error at timestep 627 is tensor(0.0639, grad_fn=<MseLossBackward0>)
Current timestep = 628. State = [[-0.14875552  0.27111563]]. Action = [[-0.14245589 -0.19319794  0.12756926 -0.55647093]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 628 is [True, False, False, False, False, True]
Current timestep = 629. State = [[-0.14414427  0.2694135 ]]. Action = [[ 0.12736744  0.09176791  0.21899831 -0.05748993]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 629 is [True, False, False, False, False, True]
Current timestep = 630. State = [[-0.1403719   0.27021652]]. Action = [[-0.13340378  0.21842596 -0.014736   -0.03152585]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 630 is [True, False, False, False, False, True]
Human Feedback received at timestep 630 of 0
Current timestep = 631. State = [[-0.13956094  0.2764526 ]]. Action = [[ 0.06807792  0.02590865 -0.09467317  0.4979713 ]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 631 is [True, False, False, False, False, True]
Current timestep = 632. State = [[-0.13784824  0.28225088]]. Action = [[ 0.03987572  0.24741483 -0.19949023  0.42594278]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 632 is [True, False, False, False, False, True]
Current timestep = 633. State = [[-0.13613428  0.2911865 ]]. Action = [[-0.13807969 -0.10167028  0.0350396  -0.12818384]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 633 is [True, False, False, False, False, True]
Scene graph at timestep 633 is [True, False, False, False, False, True]
State prediction error at timestep 633 is tensor(0.0647, grad_fn=<MseLossBackward0>)
Current timestep = 634. State = [[-0.13612103  0.2948064 ]]. Action = [[-0.02362852 -0.11273018 -0.11125937 -0.60036016]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 634 is [True, False, False, False, False, True]
Scene graph at timestep 634 is [True, False, False, False, False, True]
State prediction error at timestep 634 is tensor(0.0669, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of 0
Current timestep = 635. State = [[-0.1361854   0.29423407]]. Action = [[ 0.17106158 -0.1954109   0.20208788 -0.2783476 ]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 635 is [True, False, False, False, False, True]
Scene graph at timestep 635 is [True, False, False, False, False, True]
State prediction error at timestep 635 is tensor(0.0623, grad_fn=<MseLossBackward0>)
Current timestep = 636. State = [[-0.13267654  0.2888317 ]]. Action = [[ 0.09556997 -0.21258259  0.1376695  -0.7730674 ]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 636 is [True, False, False, False, False, True]
Scene graph at timestep 636 is [True, False, False, False, False, True]
State prediction error at timestep 636 is tensor(0.0627, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 636 of 0
Current timestep = 637. State = [[-0.12691273  0.28059462]]. Action = [[ 0.17885911  0.24346274 -0.117429    0.05149865]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 637 is [True, False, False, False, False, True]
Scene graph at timestep 637 is [True, False, False, False, False, True]
State prediction error at timestep 637 is tensor(0.0726, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 637 of 0
Current timestep = 638. State = [[-0.12173132  0.2792689 ]]. Action = [[-0.20491713 -0.21175018  0.14208704 -0.17770249]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 638 is [True, False, False, False, False, True]
Current timestep = 639. State = [[-0.12013734  0.2759545 ]]. Action = [[-0.00979988  0.06006718  0.00948066  0.28788435]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 639 is [True, False, False, False, False, True]
Human Feedback received at timestep 639 of 0
Current timestep = 640. State = [[-0.12003747  0.27467617]]. Action = [[ 0.13135302 -0.17710559 -0.06823993 -0.51357603]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 640 is [True, False, False, False, False, True]
Current timestep = 641. State = [[-0.11788349  0.26979253]]. Action = [[ 0.10871106  0.13041186  0.20070702 -0.21810913]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 641 is [True, False, False, False, False, True]
Current timestep = 642. State = [[-0.11614815  0.2688648 ]]. Action = [[-0.15748082  0.18636617  0.21041018 -0.2744394 ]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 642 is [True, False, False, False, False, True]
Current timestep = 643. State = [[-0.11682703  0.27216902]]. Action = [[-0.1280213  -0.16203566  0.2315675   0.7658851 ]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 643 is [True, False, False, False, False, True]
Scene graph at timestep 643 is [True, False, False, False, False, True]
State prediction error at timestep 643 is tensor(0.0565, grad_fn=<MseLossBackward0>)
Current timestep = 644. State = [[-0.11745832  0.27244902]]. Action = [[ 0.11856163  0.1088008  -0.06358472 -0.6083065 ]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 644 is [True, False, False, False, False, True]
Human Feedback received at timestep 644 of 1
Current timestep = 645. State = [[-0.11736012  0.2728479 ]]. Action = [[ 0.24048033 -0.2470339   0.21527588 -0.95769733]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 645 is [True, False, False, False, False, True]
Current timestep = 646. State = [[-0.11362737  0.26847333]]. Action = [[ 0.13097757  0.23264799 -0.02653024 -0.35257006]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 646 is [True, False, False, False, False, True]
Human Feedback received at timestep 646 of 1
Current timestep = 647. State = [[-0.10803237  0.27087164]]. Action = [[0.22051093 0.14653808 0.09829792 0.40050292]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 647 is [True, False, False, False, False, True]
Current timestep = 648. State = [[-0.10043868  0.275687  ]]. Action = [[-0.1028821  -0.18432993 -0.17289497  0.2803011 ]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 648 is [True, False, False, False, False, True]
Current timestep = 649. State = [[-0.09410135  0.27421296]]. Action = [[-0.02633889 -0.04734035 -0.09582409 -0.72565717]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 649 is [True, False, False, False, False, True]
Current timestep = 650. State = [[-0.09059259  0.2731277 ]]. Action = [[-0.16314486  0.11210591 -0.08270523 -0.3789481 ]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 650 is [True, False, False, False, False, True]
Scene graph at timestep 650 is [True, False, False, False, False, True]
State prediction error at timestep 650 is tensor(0.0616, grad_fn=<MseLossBackward0>)
Current timestep = 651. State = [[-0.09126794  0.2749631 ]]. Action = [[0.1904572  0.15517706 0.20952737 0.34899652]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 651 is [True, False, False, False, False, True]
Scene graph at timestep 651 is [True, False, False, False, False, True]
State prediction error at timestep 651 is tensor(0.0594, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 651 of 1
Current timestep = 652. State = [[-0.09013502  0.28014833]]. Action = [[-0.15856707  0.17195868  0.20495015 -0.76596206]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 652 is [True, False, False, False, False, True]
Current timestep = 653. State = [[-0.0908535  0.2878971]]. Action = [[ 0.01973653  0.1812458  -0.03709273 -0.37702894]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 653 is [True, False, False, False, False, True]
Current timestep = 654. State = [[-0.09134361  0.29705235]]. Action = [[0.11861724 0.09259516 0.19951886 0.26188624]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 654 is [True, False, False, False, False, True]
Current timestep = 655. State = [[-0.08987871  0.30478045]]. Action = [[ 0.07440853 -0.22078376 -0.15641183  0.3069737 ]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 655 is [True, False, False, False, False, True]
Current timestep = 656. State = [[-0.08722124  0.30506024]]. Action = [[-0.10027209  0.15523922  0.07164839 -0.20109618]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 656 is [True, False, False, False, False, True]
Scene graph at timestep 656 is [True, False, False, False, False, True]
State prediction error at timestep 656 is tensor(0.0715, grad_fn=<MseLossBackward0>)
Current timestep = 657. State = [[-0.08651748  0.30876842]]. Action = [[ 0.08588445 -0.23154326 -0.23711471  0.18249917]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 657 is [True, False, False, False, False, True]
Human Feedback received at timestep 657 of 0
Current timestep = 658. State = [[-0.08424894  0.30549747]]. Action = [[-0.07948305 -0.02448566 -0.1684084  -0.7875047 ]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 658 is [True, False, False, False, False, True]
Scene graph at timestep 658 is [True, False, False, False, False, True]
State prediction error at timestep 658 is tensor(0.0759, grad_fn=<MseLossBackward0>)
Current timestep = 659. State = [[-0.08333222  0.3030631 ]]. Action = [[ 0.10827249 -0.15150808 -0.09623295  0.5375166 ]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 659 is [True, False, False, False, False, True]
Scene graph at timestep 659 is [True, False, False, False, False, True]
State prediction error at timestep 659 is tensor(0.0662, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 659 of 0
Current timestep = 660. State = [[-0.0797281   0.29804325]]. Action = [[ 0.23728037 -0.04260106 -0.18446212  0.4644935 ]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 660 is [True, False, False, False, False, True]
Current timestep = 661. State = [[-0.07491745  0.29277825]]. Action = [[-0.06199308 -0.15457131 -0.22053213  0.82555366]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 661 is [True, False, False, False, False, True]
Human Feedback received at timestep 661 of 0
Current timestep = 662. State = [[-0.06987601  0.28579426]]. Action = [[ 0.04562125 -0.04574406  0.14351577 -0.8011281 ]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 662 is [True, False, False, False, False, True]
Current timestep = 663. State = [[-0.06629641  0.28058612]]. Action = [[0.01849285 0.11712441 0.0042415  0.51387954]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 663 is [True, False, False, False, False, True]
Scene graph at timestep 663 is [True, False, False, False, False, True]
State prediction error at timestep 663 is tensor(0.0597, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 663 of 1
Current timestep = 664. State = [[-0.06404971  0.27944958]]. Action = [[0.24324054 0.02767491 0.02958417 0.60273874]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 664 is [True, False, False, False, False, True]
Current timestep = 665. State = [[-0.05962256  0.2787947 ]]. Action = [[ 0.11746392  0.06906191  0.13832495 -0.0470311 ]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 665 is [True, False, False, False, False, True]
Human Feedback received at timestep 665 of 1
Current timestep = 666. State = [[-0.0529537  0.2803261]]. Action = [[ 0.2210032  -0.05768766 -0.05330688 -0.3390838 ]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 666 is [True, False, False, False, False, True]
Current timestep = 667. State = [[-0.04419762  0.27989358]]. Action = [[ 0.21619236  0.24062234 -0.04799493 -0.69255114]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 667 is [True, False, False, False, False, True]
Scene graph at timestep 667 is [False, True, False, False, False, True]
State prediction error at timestep 667 is tensor(0.0709, grad_fn=<MseLossBackward0>)
Current timestep = 668. State = [[-0.03254604  0.28591266]]. Action = [[0.24980634 0.15913135 0.13131034 0.5988679 ]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 668 is [False, True, False, False, False, True]
Current timestep = 669. State = [[-0.0201291   0.29322004]]. Action = [[ 0.08981201  0.03010434 -0.15868874  0.7943357 ]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 669 is [False, True, False, False, False, True]
Current timestep = 670. State = [[-0.00883629  0.29901692]]. Action = [[-0.08673131  0.17565519  0.15842956 -0.45133263]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 670 is [False, True, False, False, False, True]
Current timestep = 671. State = [[-0.00083164  0.30862707]]. Action = [[ 0.07011095  0.19317007  0.08042333 -0.16600358]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 671 is [False, True, False, False, False, True]
Current timestep = 672. State = [[0.00425714 0.3187649 ]]. Action = [[-0.16273072  0.11026764 -0.00346802 -0.2180605 ]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 672 is [False, True, False, False, False, True]
Human Feedback received at timestep 672 of -1
Current timestep = 673. State = [[0.00648096 0.32963997]]. Action = [[ 0.24689412  0.22710586 -0.17670733 -0.3289113 ]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 673 is [False, True, False, False, False, True]
Current timestep = 674. State = [[0.007225   0.33637056]]. Action = [[-0.24438646 -0.10880038 -0.01388672  0.19812512]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 674 is [False, True, False, False, False, True]
Current timestep = 675. State = [[0.00756384 0.3400526 ]]. Action = [[ 0.21729475 -0.23939815 -0.10889874 -0.87877107]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 675 is [False, True, False, False, False, True]
Current timestep = 676. State = [[0.00776048 0.34269437]]. Action = [[-0.0157513 -0.1563954  0.0662885  0.4445219]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 676 is [False, True, False, False, False, True]
Current timestep = 677. State = [[0.00838607 0.344383  ]]. Action = [[-0.01128617 -0.24264643  0.10156944 -0.04356968]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 677 is [False, True, False, False, False, True]
Current timestep = 678. State = [[0.00856685 0.34529668]]. Action = [[-0.16507025 -0.08093208 -0.07873289  0.23532486]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 678 is [False, True, False, False, False, True]
Scene graph at timestep 678 is [False, True, False, False, False, True]
State prediction error at timestep 678 is tensor(0.0770, grad_fn=<MseLossBackward0>)
Current timestep = 679. State = [[0.00858667 0.34546468]]. Action = [[ 0.00464347  0.21787134 -0.11605191  0.56651926]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 679 is [False, True, False, False, False, True]
Scene graph at timestep 679 is [False, True, False, False, False, True]
State prediction error at timestep 679 is tensor(0.0859, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 679 of 0
Current timestep = 680. State = [[0.00877485 0.34574142]]. Action = [[-0.16466518 -0.07836971  0.15014863 -0.2472384 ]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 680 is [False, True, False, False, False, True]
Current timestep = 681. State = [[0.00879943 0.3459999 ]]. Action = [[ 0.01281831  0.17539448  0.22017998 -0.14791906]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 681 is [False, True, False, False, False, True]
Scene graph at timestep 681 is [False, True, False, False, False, True]
State prediction error at timestep 681 is tensor(0.0885, grad_fn=<MseLossBackward0>)
Current timestep = 682. State = [[0.00871383 0.34619915]]. Action = [[-0.0527406   0.0794535  -0.03328604  0.44601274]]. Reward = [0.]
Curr episode timestep = 682
Scene graph at timestep 682 is [False, True, False, False, False, True]
Scene graph at timestep 682 is [False, True, False, False, False, True]
State prediction error at timestep 682 is tensor(0.0814, grad_fn=<MseLossBackward0>)
Current timestep = 683. State = [[0.00866557 0.34624842]]. Action = [[ 0.18178815 -0.05398175 -0.24323778  0.024351  ]]. Reward = [0.]
Curr episode timestep = 683
Scene graph at timestep 683 is [False, True, False, False, False, True]
Scene graph at timestep 683 is [False, True, False, False, False, True]
State prediction error at timestep 683 is tensor(0.0850, grad_fn=<MseLossBackward0>)
Current timestep = 684. State = [[0.00904523 0.34655285]]. Action = [[ 0.15806061 -0.09221403 -0.03596419 -0.15575051]]. Reward = [0.]
Curr episode timestep = 684
Scene graph at timestep 684 is [False, True, False, False, False, True]
Scene graph at timestep 684 is [False, True, False, False, False, True]
State prediction error at timestep 684 is tensor(0.0825, grad_fn=<MseLossBackward0>)
Current timestep = 685. State = [[0.00960184 0.34689385]]. Action = [[-0.15209395 -0.18336506  0.12977165  0.07509732]]. Reward = [0.]
Curr episode timestep = 685
Scene graph at timestep 685 is [False, True, False, False, False, True]
Current timestep = 686. State = [[0.01002601 0.34736216]]. Action = [[ 0.10684466 -0.13364784  0.18746209 -0.43095022]]. Reward = [0.]
Curr episode timestep = 686
Scene graph at timestep 686 is [False, True, False, False, False, True]
Current timestep = 687. State = [[0.01033687 0.34778047]]. Action = [[ 0.24955028  0.075221   -0.07978448 -0.7360014 ]]. Reward = [0.]
Curr episode timestep = 687
Scene graph at timestep 687 is [False, True, False, False, False, True]
Current timestep = 688. State = [[0.01063148 0.34741083]]. Action = [[-0.02487865  0.1128687   0.1672585  -0.42158002]]. Reward = [0.]
Curr episode timestep = 688
Scene graph at timestep 688 is [False, True, False, False, False, True]
Scene graph at timestep 688 is [False, True, False, False, False, True]
State prediction error at timestep 688 is tensor(0.0902, grad_fn=<MseLossBackward0>)
Current timestep = 689. State = [[0.0105614  0.34732142]]. Action = [[ 0.07421616  0.23484373 -0.23465884 -0.4237852 ]]. Reward = [0.]
Curr episode timestep = 689
Scene graph at timestep 689 is [False, True, False, False, False, True]
Current timestep = 690. State = [[0.01051106 0.34711477]]. Action = [[-0.00612791 -0.16856296 -0.20805486 -0.7228733 ]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 690 is [False, True, False, False, False, True]
Current timestep = 691. State = [[0.01055275 0.34697616]]. Action = [[-0.23147222  0.21899182 -0.14442816 -0.78427863]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 691 is [False, True, False, False, False, True]
Current timestep = 692. State = [[0.0106508  0.34696344]]. Action = [[ 0.0057984  -0.12313953 -0.13740398 -0.5128294 ]]. Reward = [0.]
Curr episode timestep = 692
Scene graph at timestep 692 is [False, True, False, False, False, True]
Current timestep = 693. State = [[0.01062849 0.34713075]]. Action = [[ 0.0510141   0.08111447 -0.1034281  -0.6444013 ]]. Reward = [0.]
Curr episode timestep = 693
Scene graph at timestep 693 is [False, True, False, False, False, True]
Current timestep = 694. State = [[0.01070118 0.34703162]]. Action = [[ 0.02270657 -0.20372215 -0.0103014  -0.7098818 ]]. Reward = [0.]
Curr episode timestep = 694
Scene graph at timestep 694 is [False, True, False, False, False, True]
Current timestep = 695. State = [[0.01070365 0.3470858 ]]. Action = [[0.22445577 0.13397762 0.11681056 0.5158378 ]]. Reward = [0.]
Curr episode timestep = 695
Scene graph at timestep 695 is [False, True, False, False, False, True]
Current timestep = 696. State = [[0.01070365 0.3470858 ]]. Action = [[-0.0042533   0.12886083 -0.08205609 -0.12882173]]. Reward = [0.]
Curr episode timestep = 696
Scene graph at timestep 696 is [False, True, False, False, False, True]
Current timestep = 697. State = [[0.01070365 0.3470858 ]]. Action = [[-0.04383203  0.04907629  0.15509224  0.9793967 ]]. Reward = [0.]
Curr episode timestep = 697
Scene graph at timestep 697 is [False, True, False, False, False, True]
Current timestep = 698. State = [[0.01068985 0.34712166]]. Action = [[-0.15301853  0.03667089  0.15976596  0.10217381]]. Reward = [0.]
Curr episode timestep = 698
Scene graph at timestep 698 is [False, True, False, False, False, True]
Human Feedback received at timestep 698 of -1
Current timestep = 699. State = [[0.01073665 0.3472608 ]]. Action = [[-0.19851218 -0.02677473  0.10524881  0.60022473]]. Reward = [0.]
Curr episode timestep = 699
Scene graph at timestep 699 is [False, True, False, False, False, True]
Current timestep = 700. State = [[0.01072733 0.3473891 ]]. Action = [[-0.24464552 -0.01249008  0.04427364 -0.9246324 ]]. Reward = [0.]
Curr episode timestep = 700
Scene graph at timestep 700 is [False, True, False, False, False, True]
Current timestep = 701. State = [[0.01071933 0.34738338]]. Action = [[-0.23024112 -0.17841642  0.21196038 -0.1754458 ]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 701 is [False, True, False, False, False, True]
Current timestep = 702. State = [[0.0107252  0.34736165]]. Action = [[ 0.20863679  0.18663678 -0.10527799  0.30291736]]. Reward = [0.]
Curr episode timestep = 702
Scene graph at timestep 702 is [False, True, False, False, False, True]
Current timestep = 703. State = [[0.01072758 0.34741586]]. Action = [[ 0.09309798 -0.04412025  0.03187272 -0.7877661 ]]. Reward = [0.]
Curr episode timestep = 703
Scene graph at timestep 703 is [False, True, False, False, False, True]
Scene graph at timestep 703 is [False, True, False, False, False, True]
State prediction error at timestep 703 is tensor(0.0950, grad_fn=<MseLossBackward0>)
Current timestep = 704. State = [[0.01072758 0.34741586]]. Action = [[-0.16742733 -0.09123483 -0.22752582 -0.1631676 ]]. Reward = [0.]
Curr episode timestep = 704
Scene graph at timestep 704 is [False, True, False, False, False, True]
Scene graph at timestep 704 is [False, True, False, False, False, True]
State prediction error at timestep 704 is tensor(0.0840, grad_fn=<MseLossBackward0>)
Current timestep = 705. State = [[0.01072758 0.34741586]]. Action = [[ 0.10941783 -0.15614775  0.03443623  0.67180943]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 705 is [False, True, False, False, False, True]
Current timestep = 706. State = [[0.01072758 0.34741586]]. Action = [[ 0.16800433 -0.17492715  0.22999826  0.08968353]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 706 is [False, True, False, False, False, True]
Current timestep = 707. State = [[0.01072758 0.34741586]]. Action = [[-0.23069966 -0.12444732  0.1328507   0.1369803 ]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 707 is [False, True, False, False, False, True]
Current timestep = 708. State = [[0.01072758 0.34741586]]. Action = [[-0.13344541  0.08565611  0.1334149  -0.5549263 ]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 708 is [False, True, False, False, False, True]
Human Feedback received at timestep 708 of -1
Current timestep = 709. State = [[0.01072758 0.34741586]]. Action = [[ 0.00769788 -0.23312892  0.03734273  0.81189466]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 709 is [False, True, False, False, False, True]
Scene graph at timestep 709 is [False, True, False, False, False, True]
State prediction error at timestep 709 is tensor(0.0803, grad_fn=<MseLossBackward0>)
Current timestep = 710. State = [[0.0107345  0.34740782]]. Action = [[-0.10919952 -0.18442012 -0.16934286 -0.3440084 ]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 710 is [False, True, False, False, False, True]
Current timestep = 711. State = [[0.0107345  0.34740782]]. Action = [[ 0.03655696 -0.20208588  0.04096729 -0.04511029]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 711 is [False, True, False, False, False, True]
Current timestep = 712. State = [[0.0107345  0.34740782]]. Action = [[ 0.1916601   0.18669808 -0.04312854  0.33489728]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 712 is [False, True, False, False, False, True]
Current timestep = 713. State = [[0.0107345  0.34740782]]. Action = [[-0.18474014  0.05879143 -0.21523991 -0.47716224]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 713 is [False, True, False, False, False, True]
Current timestep = 714. State = [[0.0107345  0.34740782]]. Action = [[ 0.04891935 -0.20801209 -0.23005041 -0.07414454]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 714 is [False, True, False, False, False, True]
Current timestep = 715. State = [[0.01065176 0.34741372]]. Action = [[-0.11364159 -0.04373091  0.19317985  0.02751064]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 715 is [False, True, False, False, False, True]
Scene graph at timestep 715 is [False, True, False, False, False, True]
State prediction error at timestep 715 is tensor(0.0801, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 715 of -1
Current timestep = 716. State = [[0.01065176 0.34741372]]. Action = [[-0.06834874  0.17092255 -0.12566595  0.3334744 ]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 716 is [False, True, False, False, False, True]
Scene graph at timestep 716 is [False, True, False, False, False, True]
State prediction error at timestep 716 is tensor(0.0863, grad_fn=<MseLossBackward0>)
Current timestep = 717. State = [[0.01065176 0.34741372]]. Action = [[-0.10210627 -0.16717601 -0.06563997  0.08878958]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 717 is [False, True, False, False, False, True]
Current timestep = 718. State = [[0.01065176 0.34741372]]. Action = [[ 0.08557743 -0.08039938 -0.16359204  0.04506803]]. Reward = [0.]
Curr episode timestep = 718
Scene graph at timestep 718 is [False, True, False, False, False, True]
Current timestep = 719. State = [[0.01065176 0.34741372]]. Action = [[0.08999345 0.11754483 0.02205402 0.84846234]]. Reward = [0.]
Curr episode timestep = 719
Scene graph at timestep 719 is [False, True, False, False, False, True]
Human Feedback received at timestep 719 of -1
Current timestep = 720. State = [[0.01065176 0.34741372]]. Action = [[ 0.11322334  0.08206767  0.1686604  -0.18432838]]. Reward = [0.]
Curr episode timestep = 720
Scene graph at timestep 720 is [False, True, False, False, False, True]
Scene graph at timestep 720 is [False, True, False, False, False, True]
State prediction error at timestep 720 is tensor(0.0877, grad_fn=<MseLossBackward0>)
Current timestep = 721. State = [[0.01065176 0.34741372]]. Action = [[ 0.23360693 -0.1660935  -0.12117746 -0.3738433 ]]. Reward = [0.]
Curr episode timestep = 721
Scene graph at timestep 721 is [False, True, False, False, False, True]
Human Feedback received at timestep 721 of -1
Current timestep = 722. State = [[0.01065176 0.34741372]]. Action = [[ 0.13436812 -0.16057388  0.05842999 -0.09684998]]. Reward = [0.]
Curr episode timestep = 722
Scene graph at timestep 722 is [False, True, False, False, False, True]
Current timestep = 723. State = [[0.01062409 0.34741572]]. Action = [[ 0.23973328 -0.08119285 -0.11897516 -0.9610952 ]]. Reward = [0.]
Curr episode timestep = 723
Scene graph at timestep 723 is [False, True, False, False, False, True]
Current timestep = 724. State = [[0.01062409 0.34741572]]. Action = [[0.06880879 0.16226733 0.04843533 0.9721887 ]]. Reward = [0.]
Curr episode timestep = 724
Scene graph at timestep 724 is [False, True, False, False, False, True]
Current timestep = 725. State = [[0.01062409 0.34741572]]. Action = [[-0.1466868  -0.08775264  0.2339052   0.3132459 ]]. Reward = [0.]
Curr episode timestep = 725
Scene graph at timestep 725 is [False, True, False, False, False, True]
Current timestep = 726. State = [[0.01062409 0.34741572]]. Action = [[ 0.16844094  0.19172043  0.16449723 -0.7576656 ]]. Reward = [0.]
Curr episode timestep = 726
Scene graph at timestep 726 is [False, True, False, False, False, True]
Current timestep = 727. State = [[0.01062409 0.34741572]]. Action = [[0.18131995 0.11057684 0.20161936 0.20903766]]. Reward = [0.]
Curr episode timestep = 727
Scene graph at timestep 727 is [False, True, False, False, False, True]
Current timestep = 728. State = [[0.01061044 0.34760207]]. Action = [[-0.12760387 -0.16288365  0.04745474 -0.22109938]]. Reward = [0.]
Curr episode timestep = 728
Scene graph at timestep 728 is [False, True, False, False, False, True]
Scene graph at timestep 728 is [False, True, False, False, False, True]
State prediction error at timestep 728 is tensor(0.0793, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of -1
Current timestep = 729. State = [[0.01061044 0.34760207]]. Action = [[-0.24419774 -0.01928043  0.09853455  0.26353765]]. Reward = [0.]
Curr episode timestep = 729
Scene graph at timestep 729 is [False, True, False, False, False, True]
Current timestep = 730. State = [[0.01061044 0.34760207]]. Action = [[-0.12332939  0.166785    0.20782089  0.07376039]]. Reward = [0.]
Curr episode timestep = 730
Scene graph at timestep 730 is [False, True, False, False, False, True]
Current timestep = 731. State = [[0.01061044 0.34760207]]. Action = [[-0.13686977  0.18140236  0.23056877 -0.6693413 ]]. Reward = [0.]
Curr episode timestep = 731
Scene graph at timestep 731 is [False, True, False, False, False, True]
Current timestep = 732. State = [[0.01060587 0.3476643 ]]. Action = [[-0.1250225  -0.1172938  -0.06925973 -0.23385882]]. Reward = [0.]
Curr episode timestep = 732
Scene graph at timestep 732 is [False, True, False, False, False, True]
Human Feedback received at timestep 732 of -1
Current timestep = 733. State = [[0.01060587 0.3476643 ]]. Action = [[0.2373994  0.03674316 0.23376173 0.0703851 ]]. Reward = [0.]
Curr episode timestep = 733
Scene graph at timestep 733 is [False, True, False, False, False, True]
Scene graph at timestep 733 is [False, True, False, False, False, True]
State prediction error at timestep 733 is tensor(0.0831, grad_fn=<MseLossBackward0>)
Current timestep = 734. State = [[0.01060587 0.3476643 ]]. Action = [[0.24542138 0.08361459 0.1760467  0.63652277]]. Reward = [0.]
Curr episode timestep = 734
Scene graph at timestep 734 is [False, True, False, False, False, True]
Human Feedback received at timestep 734 of -1
Current timestep = 735. State = [[0.01060587 0.3476643 ]]. Action = [[-0.03461727 -0.16929424  0.02693757 -0.25449574]]. Reward = [0.]
Curr episode timestep = 735
Scene graph at timestep 735 is [False, True, False, False, False, True]
Current timestep = 736. State = [[0.01060587 0.3476643 ]]. Action = [[-0.2196649   0.01351902  0.18974176  0.66461205]]. Reward = [0.]
Curr episode timestep = 736
Scene graph at timestep 736 is [False, True, False, False, False, True]
Current timestep = 737. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.11528757 -0.02169792 -0.02243543 -0.8090815 ]]. Reward = [0.]
Curr episode timestep = 737
Scene graph at timestep 737 is [False, True, False, False, False, True]
Scene graph at timestep 737 is [False, True, False, False, False, True]
State prediction error at timestep 737 is tensor(0.0973, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 737 of -1
Current timestep = 738. State = [[0.01060587 0.3476643 ]]. Action = [[-0.08197361 -0.22005728 -0.10242213 -0.8233887 ]]. Reward = [0.]
Curr episode timestep = 738
Scene graph at timestep 738 is [False, True, False, False, False, True]
Human Feedback received at timestep 738 of -1
Current timestep = 739. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.10136387  0.10676363 -0.17265831 -0.4175024 ]]. Reward = [0.]
Curr episode timestep = 739
Scene graph at timestep 739 is [False, True, False, False, False, True]
Current timestep = 740. State = [[0.01060587 0.3476643 ]]. Action = [[0.24941373 0.11082608 0.20208919 0.7284478 ]]. Reward = [0.]
Curr episode timestep = 740
Scene graph at timestep 740 is [False, True, False, False, False, True]
Human Feedback received at timestep 740 of -1
Current timestep = 741. State = [[0.01060587 0.3476643 ]]. Action = [[-0.09068599 -0.16689725  0.01641995 -0.42877495]]. Reward = [0.]
Curr episode timestep = 741
Scene graph at timestep 741 is [False, True, False, False, False, True]
Current timestep = 742. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.04471397  0.16317901 -0.14173123 -0.30008316]]. Reward = [0.]
Curr episode timestep = 742
Scene graph at timestep 742 is [False, True, False, False, False, True]
Current timestep = 743. State = [[0.01060587 0.3476643 ]]. Action = [[-0.23763363 -0.07534486 -0.08664505 -0.3036222 ]]. Reward = [0.]
Curr episode timestep = 743
Scene graph at timestep 743 is [False, True, False, False, False, True]
Current timestep = 744. State = [[0.01060587 0.3476643 ]]. Action = [[-0.14863728 -0.06021032  0.10218012  0.7404125 ]]. Reward = [0.]
Curr episode timestep = 744
Scene graph at timestep 744 is [False, True, False, False, False, True]
Human Feedback received at timestep 744 of -1
Current timestep = 745. State = [[0.01060587 0.3476643 ]]. Action = [[-0.0816271  -0.15300035 -0.1973653   0.08400738]]. Reward = [0.]
Curr episode timestep = 745
Scene graph at timestep 745 is [False, True, False, False, False, True]
Current timestep = 746. State = [[0.01060587 0.3476643 ]]. Action = [[-0.06858607 -0.23489936  0.0266777   0.58146596]]. Reward = [0.]
Curr episode timestep = 746
Scene graph at timestep 746 is [False, True, False, False, False, True]
Human Feedback received at timestep 746 of -1
Current timestep = 747. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.23944002 -0.01034637  0.06326604 -0.2073139 ]]. Reward = [0.]
Curr episode timestep = 747
Scene graph at timestep 747 is [False, True, False, False, False, True]
Current timestep = 748. State = [[0.01060587 0.3476643 ]]. Action = [[0.09887415 0.1784724  0.22213429 0.8553473 ]]. Reward = [0.]
Curr episode timestep = 748
Scene graph at timestep 748 is [False, True, False, False, False, True]
Current timestep = 749. State = [[0.01060587 0.3476643 ]]. Action = [[-0.10905883 -0.12918726 -0.1898415  -0.15699548]]. Reward = [0.]
Curr episode timestep = 749
Scene graph at timestep 749 is [False, True, False, False, False, True]
Current timestep = 750. State = [[0.01060587 0.3476643 ]]. Action = [[-0.04441613 -0.11529791  0.14051569 -0.72059715]]. Reward = [0.]
Curr episode timestep = 750
Scene graph at timestep 750 is [False, True, False, False, False, True]
Current timestep = 751. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.05649081  0.03133926 -0.17047311 -0.27660632]]. Reward = [0.]
Curr episode timestep = 751
Scene graph at timestep 751 is [False, True, False, False, False, True]
Human Feedback received at timestep 751 of -1
Current timestep = 752. State = [[0.01060587 0.3476643 ]]. Action = [[-0.0248559   0.00183821 -0.03341283  0.43566608]]. Reward = [0.]
Curr episode timestep = 752
Scene graph at timestep 752 is [False, True, False, False, False, True]
Current timestep = 753. State = [[0.01060587 0.3476643 ]]. Action = [[-0.15525256 -0.11615881  0.19448486 -0.01682848]]. Reward = [0.]
Curr episode timestep = 753
Scene graph at timestep 753 is [False, True, False, False, False, True]
Current timestep = 754. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.22291467  0.0520727   0.08855551 -0.4560727 ]]. Reward = [0.]
Curr episode timestep = 754
Scene graph at timestep 754 is [False, True, False, False, False, True]
Current timestep = 755. State = [[0.01060587 0.3476643 ]]. Action = [[0.08112898 0.19858852 0.01995742 0.46081185]]. Reward = [0.]
Curr episode timestep = 755
Scene graph at timestep 755 is [False, True, False, False, False, True]
Current timestep = 756. State = [[0.01060587 0.3476643 ]]. Action = [[-0.10525197  0.03525501 -0.13347949 -0.8798099 ]]. Reward = [0.]
Curr episode timestep = 756
Scene graph at timestep 756 is [False, True, False, False, False, True]
Human Feedback received at timestep 756 of -1
Current timestep = 757. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.07167816 -0.0211546  -0.1380668  -0.57500964]]. Reward = [0.]
Curr episode timestep = 757
Scene graph at timestep 757 is [False, True, False, False, False, True]
Current timestep = 758. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.03253973  0.15462095 -0.15445897 -0.13088995]]. Reward = [0.]
Curr episode timestep = 758
Scene graph at timestep 758 is [False, True, False, False, False, True]
Scene graph at timestep 758 is [False, True, False, False, False, True]
State prediction error at timestep 758 is tensor(0.0915, grad_fn=<MseLossBackward0>)
Current timestep = 759. State = [[0.01060587 0.3476643 ]]. Action = [[-0.06723514 -0.15429452  0.23027688 -0.41441768]]. Reward = [0.]
Curr episode timestep = 759
Scene graph at timestep 759 is [False, True, False, False, False, True]
Current timestep = 760. State = [[0.01060587 0.3476643 ]]. Action = [[-0.20934694 -0.21207319  0.16085437 -0.47703707]]. Reward = [0.]
Curr episode timestep = 760
Scene graph at timestep 760 is [False, True, False, False, False, True]
Current timestep = 761. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.05799845  0.00945646 -0.12979665  0.40574253]]. Reward = [0.]
Curr episode timestep = 761
Scene graph at timestep 761 is [False, True, False, False, False, True]
Human Feedback received at timestep 761 of -1
Current timestep = 762. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.21534654 -0.02794601 -0.12229593  0.0889492 ]]. Reward = [0.]
Curr episode timestep = 762
Scene graph at timestep 762 is [False, True, False, False, False, True]
Current timestep = 763. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.22939539  0.2072607  -0.16978075  0.16887808]]. Reward = [0.]
Curr episode timestep = 763
Scene graph at timestep 763 is [False, True, False, False, False, True]
Scene graph at timestep 763 is [False, True, False, False, False, True]
State prediction error at timestep 763 is tensor(0.0896, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 763 of -1
Current timestep = 764. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.21104458 -0.13194153 -0.12348215  0.35092437]]. Reward = [0.]
Curr episode timestep = 764
Scene graph at timestep 764 is [False, True, False, False, False, True]
Current timestep = 765. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.24723703 -0.22774833  0.12995231 -0.9062925 ]]. Reward = [0.]
Curr episode timestep = 765
Scene graph at timestep 765 is [False, True, False, False, False, True]
Current timestep = 766. State = [[0.01060587 0.3476643 ]]. Action = [[-0.07579112 -0.01893899 -0.04533091 -0.3386684 ]]. Reward = [0.]
Curr episode timestep = 766
Scene graph at timestep 766 is [False, True, False, False, False, True]
Human Feedback received at timestep 766 of -1
Current timestep = 767. State = [[0.01060587 0.3476643 ]]. Action = [[0.21856976 0.03323016 0.24534905 0.3423338 ]]. Reward = [0.]
Curr episode timestep = 767
Scene graph at timestep 767 is [False, True, False, False, False, True]
Current timestep = 768. State = [[0.01060587 0.3476643 ]]. Action = [[-0.20873857  0.15127134  0.13981682 -0.71676725]]. Reward = [0.]
Curr episode timestep = 768
Scene graph at timestep 768 is [False, True, False, False, False, True]
Scene graph at timestep 768 is [False, True, False, False, False, True]
State prediction error at timestep 768 is tensor(0.0928, grad_fn=<MseLossBackward0>)
Current timestep = 769. State = [[0.01060587 0.3476643 ]]. Action = [[-0.23713462  0.1141454   0.1059621   0.00723088]]. Reward = [0.]
Curr episode timestep = 769
Scene graph at timestep 769 is [False, True, False, False, False, True]
Current timestep = 770. State = [[0.01060587 0.3476643 ]]. Action = [[-0.23661429 -0.03899311  0.09401107  0.98291266]]. Reward = [0.]
Curr episode timestep = 770
Scene graph at timestep 770 is [False, True, False, False, False, True]
Current timestep = 771. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.20522922 -0.05826716 -0.03171112 -0.04953921]]. Reward = [0.]
Curr episode timestep = 771
Scene graph at timestep 771 is [False, True, False, False, False, True]
Scene graph at timestep 771 is [False, True, False, False, False, True]
State prediction error at timestep 771 is tensor(0.0829, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 771 of -1
Current timestep = 772. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.15098694  0.17937082  0.13345394 -0.8955244 ]]. Reward = [0.]
Curr episode timestep = 772
Scene graph at timestep 772 is [False, True, False, False, False, True]
Scene graph at timestep 772 is [False, True, False, False, False, True]
State prediction error at timestep 772 is tensor(0.0999, grad_fn=<MseLossBackward0>)
Current timestep = 773. State = [[0.01060587 0.3476643 ]]. Action = [[-0.07684277 -0.19175513  0.21294141 -0.0803296 ]]. Reward = [0.]
Curr episode timestep = 773
Scene graph at timestep 773 is [False, True, False, False, False, True]
Human Feedback received at timestep 773 of -1
Current timestep = 774. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.1958322   0.13118374 -0.03345583  0.6510874 ]]. Reward = [0.]
Curr episode timestep = 774
Scene graph at timestep 774 is [False, True, False, False, False, True]
Current timestep = 775. State = [[0.01060587 0.3476643 ]]. Action = [[-0.01607351  0.12739474  0.19898343  0.5468526 ]]. Reward = [0.]
Curr episode timestep = 775
Scene graph at timestep 775 is [False, True, False, False, False, True]
Scene graph at timestep 775 is [False, True, False, False, False, True]
State prediction error at timestep 775 is tensor(0.0838, grad_fn=<MseLossBackward0>)
Current timestep = 776. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.11952055  0.18410924  0.2209177  -0.09134418]]. Reward = [0.]
Curr episode timestep = 776
Scene graph at timestep 776 is [False, True, False, False, False, True]
Current timestep = 777. State = [[0.01060587 0.3476643 ]]. Action = [[-0.13178778  0.08138368 -0.22114272  0.31260288]]. Reward = [0.]
Curr episode timestep = 777
Scene graph at timestep 777 is [False, True, False, False, False, True]
Human Feedback received at timestep 777 of -1
Current timestep = 778. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.22746685 -0.14466788  0.17701459  0.64170885]]. Reward = [0.]
Curr episode timestep = 778
Scene graph at timestep 778 is [False, True, False, False, False, True]
Current timestep = 779. State = [[0.01060587 0.3476643 ]]. Action = [[-0.10112876 -0.09549549 -0.05488679 -0.8567645 ]]. Reward = [0.]
Curr episode timestep = 779
Scene graph at timestep 779 is [False, True, False, False, False, True]
Scene graph at timestep 779 is [False, True, False, False, False, True]
State prediction error at timestep 779 is tensor(0.0950, grad_fn=<MseLossBackward0>)
Current timestep = 780. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.07066715 -0.1994737  -0.06635685  0.29480922]]. Reward = [0.]
Curr episode timestep = 780
Scene graph at timestep 780 is [False, True, False, False, False, True]
Current timestep = 781. State = [[0.01060587 0.3476643 ]]. Action = [[-0.14772804  0.03176078 -0.00267649 -0.4156394 ]]. Reward = [0.]
Curr episode timestep = 781
Scene graph at timestep 781 is [False, True, False, False, False, True]
Current timestep = 782. State = [[0.01060587 0.3476643 ]]. Action = [[-0.11096108  0.0452641  -0.12767416  0.1762631 ]]. Reward = [0.]
Curr episode timestep = 782
Scene graph at timestep 782 is [False, True, False, False, False, True]
Human Feedback received at timestep 782 of -1
Current timestep = 783. State = [[0.01060587 0.3476643 ]]. Action = [[-0.1010676   0.24330002 -0.11607048 -0.01336288]]. Reward = [0.]
Curr episode timestep = 783
Scene graph at timestep 783 is [False, True, False, False, False, True]
Current timestep = 784. State = [[0.01060587 0.3476643 ]]. Action = [[-0.19395904  0.23385462 -0.22275564 -0.8942189 ]]. Reward = [0.]
Curr episode timestep = 784
Scene graph at timestep 784 is [False, True, False, False, False, True]
Human Feedback received at timestep 784 of -1
Current timestep = 785. State = [[0.01060587 0.3476643 ]]. Action = [[-0.07778469  0.2104226   0.0745692  -0.22903812]]. Reward = [0.]
Curr episode timestep = 785
Scene graph at timestep 785 is [False, True, False, False, False, True]
Current timestep = 786. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.03439915  0.16002601 -0.06646325 -0.55082744]]. Reward = [0.]
Curr episode timestep = 786
Scene graph at timestep 786 is [False, True, False, False, False, True]
Human Feedback received at timestep 786 of -1
Current timestep = 787. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.00318256 -0.09730253 -0.11753304  0.04489005]]. Reward = [0.]
Curr episode timestep = 787
Scene graph at timestep 787 is [False, True, False, False, False, True]
Scene graph at timestep 787 is [False, True, False, False, False, True]
State prediction error at timestep 787 is tensor(0.0806, grad_fn=<MseLossBackward0>)
Current timestep = 788. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.10722053 -0.15796152 -0.12379405  0.23667431]]. Reward = [0.]
Curr episode timestep = 788
Scene graph at timestep 788 is [False, True, False, False, False, True]
Current timestep = 789. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.08479118 -0.02306654 -0.2389254  -0.35315073]]. Reward = [0.]
Curr episode timestep = 789
Scene graph at timestep 789 is [False, True, False, False, False, True]
Human Feedback received at timestep 789 of -1
Current timestep = 790. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.17732245 -0.10528974 -0.14644177 -0.5083091 ]]. Reward = [0.]
Curr episode timestep = 790
Scene graph at timestep 790 is [False, True, False, False, False, True]
Current timestep = 791. State = [[0.01060587 0.3476643 ]]. Action = [[-0.20258613 -0.07698721  0.01837048 -0.17037821]]. Reward = [0.]
Curr episode timestep = 791
Scene graph at timestep 791 is [False, True, False, False, False, True]
Human Feedback received at timestep 791 of -1
Current timestep = 792. State = [[0.01060587 0.3476643 ]]. Action = [[-0.1781875   0.08450991  0.04386595 -0.2509128 ]]. Reward = [0.]
Curr episode timestep = 792
Scene graph at timestep 792 is [False, True, False, False, False, True]
Current timestep = 793. State = [[0.01060587 0.3476643 ]]. Action = [[-0.19547072 -0.1854766  -0.23382545 -0.45124555]]. Reward = [0.]
Curr episode timestep = 793
Scene graph at timestep 793 is [False, True, False, False, False, True]
Current timestep = 794. State = [[0.01060587 0.3476643 ]]. Action = [[-0.12892842  0.12975347  0.04684162  0.83721507]]. Reward = [0.]
Curr episode timestep = 794
Scene graph at timestep 794 is [False, True, False, False, False, True]
Human Feedback received at timestep 794 of -1
Current timestep = 795. State = [[0.01060587 0.3476643 ]]. Action = [[-0.04137313 -0.0894403   0.1374608   0.6436479 ]]. Reward = [0.]
Curr episode timestep = 795
Scene graph at timestep 795 is [False, True, False, False, False, True]
Current timestep = 796. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.01089039  0.16449451 -0.099931    0.34108114]]. Reward = [0.]
Curr episode timestep = 796
Scene graph at timestep 796 is [False, True, False, False, False, True]
Current timestep = 797. State = [[0.01060587 0.3476643 ]]. Action = [[-0.04904716 -0.10134652  0.2495512   0.4180622 ]]. Reward = [0.]
Curr episode timestep = 797
Scene graph at timestep 797 is [False, True, False, False, False, True]
Current timestep = 798. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.00871336  0.04898271 -0.0521917   0.9446156 ]]. Reward = [0.]
Curr episode timestep = 798
Scene graph at timestep 798 is [False, True, False, False, False, True]
Human Feedback received at timestep 798 of -1
Current timestep = 799. State = [[0.01060587 0.3476643 ]]. Action = [[-0.23093823  0.14974171  0.11066303 -0.86301434]]. Reward = [0.]
Curr episode timestep = 799
Scene graph at timestep 799 is [False, True, False, False, False, True]
Current timestep = 800. State = [[0.01060587 0.3476643 ]]. Action = [[-0.07751425 -0.14889406 -0.08264202  0.42639923]]. Reward = [0.]
Curr episode timestep = 800
Scene graph at timestep 800 is [False, True, False, False, False, True]
Human Feedback received at timestep 800 of -1
Current timestep = 801. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.06578648 -0.1864992   0.10199988  0.64242697]]. Reward = [0.]
Curr episode timestep = 801
Scene graph at timestep 801 is [False, True, False, False, False, True]
Current timestep = 802. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.03267607  0.19381064 -0.11265546 -0.45114148]]. Reward = [0.]
Curr episode timestep = 802
Scene graph at timestep 802 is [False, True, False, False, False, True]
Human Feedback received at timestep 802 of -1
Current timestep = 803. State = [[0.01060587 0.3476643 ]]. Action = [[-0.05006772 -0.07850619 -0.23095971 -0.79141146]]. Reward = [0.]
Curr episode timestep = 803
Scene graph at timestep 803 is [False, True, False, False, False, True]
Current timestep = 804. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.21683434  0.2213679   0.02797493 -0.23511785]]. Reward = [0.]
Curr episode timestep = 804
Scene graph at timestep 804 is [False, True, False, False, False, True]
Human Feedback received at timestep 804 of -1
Current timestep = 805. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.16310036  0.20589542  0.15120247 -0.8650296 ]]. Reward = [0.]
Curr episode timestep = 805
Scene graph at timestep 805 is [False, True, False, False, False, True]
Scene graph at timestep 805 is [False, True, False, False, False, True]
State prediction error at timestep 805 is tensor(0.0996, grad_fn=<MseLossBackward0>)
Current timestep = 806. State = [[0.01060587 0.3476643 ]]. Action = [[-0.09112519  0.14926207  0.03752792 -0.29706693]]. Reward = [0.]
Curr episode timestep = 806
Scene graph at timestep 806 is [False, True, False, False, False, True]
Current timestep = 807. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.00478011  0.13801098 -0.22894435  0.85993576]]. Reward = [0.]
Curr episode timestep = 807
Scene graph at timestep 807 is [False, True, False, False, False, True]
Current timestep = 808. State = [[0.01060587 0.3476643 ]]. Action = [[-0.02095254 -0.10026231 -0.16266009 -0.8830146 ]]. Reward = [0.]
Curr episode timestep = 808
Scene graph at timestep 808 is [False, True, False, False, False, True]
Current timestep = 809. State = [[0.01060587 0.3476643 ]]. Action = [[0.12412441 0.14276046 0.18254727 0.22307944]]. Reward = [0.]
Curr episode timestep = 809
Scene graph at timestep 809 is [False, True, False, False, False, True]
Current timestep = 810. State = [[0.01060587 0.3476643 ]]. Action = [[-0.23779951 -0.10670486  0.20464426  0.1658225 ]]. Reward = [0.]
Curr episode timestep = 810
Scene graph at timestep 810 is [False, True, False, False, False, True]
Current timestep = 811. State = [[0.01060587 0.3476643 ]]. Action = [[-0.24619709 -0.02183965 -0.15547667 -0.58889747]]. Reward = [0.]
Curr episode timestep = 811
Scene graph at timestep 811 is [False, True, False, False, False, True]
Human Feedback received at timestep 811 of -1
Current timestep = 812. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.1444557  -0.10731182  0.24247733  0.5534167 ]]. Reward = [0.]
Curr episode timestep = 812
Scene graph at timestep 812 is [False, True, False, False, False, True]
Scene graph at timestep 812 is [False, True, False, False, False, True]
State prediction error at timestep 812 is tensor(0.0763, grad_fn=<MseLossBackward0>)
Current timestep = 813. State = [[0.01060587 0.3476643 ]]. Action = [[-0.15423948  0.03803131  0.1776067   0.14616466]]. Reward = [0.]
Curr episode timestep = 813
Scene graph at timestep 813 is [False, True, False, False, False, True]
Current timestep = 814. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.13812968 -0.15344799  0.07691148  0.6213716 ]]. Reward = [0.]
Curr episode timestep = 814
Scene graph at timestep 814 is [False, True, False, False, False, True]
Current timestep = 815. State = [[0.01060587 0.3476643 ]]. Action = [[-0.19120649  0.19587564 -0.19123287 -0.9107446 ]]. Reward = [0.]
Curr episode timestep = 815
Scene graph at timestep 815 is [False, True, False, False, False, True]
Scene graph at timestep 815 is [False, True, False, False, False, True]
State prediction error at timestep 815 is tensor(0.1073, grad_fn=<MseLossBackward0>)
Current timestep = 816. State = [[0.01060587 0.3476643 ]]. Action = [[0.08282626 0.18790102 0.18958151 0.3436234 ]]. Reward = [0.]
Curr episode timestep = 816
Scene graph at timestep 816 is [False, True, False, False, False, True]
Current timestep = 817. State = [[0.01060587 0.3476643 ]]. Action = [[-0.03061564  0.01792908 -0.15995961  0.6208749 ]]. Reward = [0.]
Curr episode timestep = 817
Scene graph at timestep 817 is [False, True, False, False, False, True]
Scene graph at timestep 817 is [False, True, False, False, False, True]
State prediction error at timestep 817 is tensor(0.0830, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 817 of -1
Current timestep = 818. State = [[0.01060587 0.3476643 ]]. Action = [[-0.15452561  0.1546349  -0.22909272 -0.02620709]]. Reward = [0.]
Curr episode timestep = 818
Scene graph at timestep 818 is [False, True, False, False, False, True]
Current timestep = 819. State = [[0.01060587 0.3476643 ]]. Action = [[-0.1557588   0.15877903 -0.01594977 -0.5510592 ]]. Reward = [0.]
Curr episode timestep = 819
Scene graph at timestep 819 is [False, True, False, False, False, True]
Human Feedback received at timestep 819 of -1
Current timestep = 820. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.10383651 -0.1008717  -0.08971922  0.20208788]]. Reward = [0.]
Curr episode timestep = 820
Scene graph at timestep 820 is [False, True, False, False, False, True]
Current timestep = 821. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.07190174  0.16149306  0.2457332  -0.1201269 ]]. Reward = [0.]
Curr episode timestep = 821
Scene graph at timestep 821 is [False, True, False, False, False, True]
Human Feedback received at timestep 821 of -1
Current timestep = 822. State = [[0.01060587 0.3476643 ]]. Action = [[-0.00689781 -0.03481403 -0.20147029  0.35883915]]. Reward = [0.]
Curr episode timestep = 822
Scene graph at timestep 822 is [False, True, False, False, False, True]
Current timestep = 823. State = [[0.01060587 0.3476643 ]]. Action = [[0.0152683  0.04053921 0.03242469 0.2921095 ]]. Reward = [0.]
Curr episode timestep = 823
Scene graph at timestep 823 is [False, True, False, False, False, True]
Current timestep = 824. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.23889863  0.21136016  0.17208403 -0.63137627]]. Reward = [0.]
Curr episode timestep = 824
Scene graph at timestep 824 is [False, True, False, False, False, True]
Current timestep = 825. State = [[0.01060587 0.3476643 ]]. Action = [[-0.22125824  0.06836566  0.05920208 -0.5202895 ]]. Reward = [0.]
Curr episode timestep = 825
Scene graph at timestep 825 is [False, True, False, False, False, True]
Current timestep = 826. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.08298686  0.09173298  0.11985281 -0.20287979]]. Reward = [0.]
Curr episode timestep = 826
Scene graph at timestep 826 is [False, True, False, False, False, True]
Scene graph at timestep 826 is [False, True, False, False, False, True]
State prediction error at timestep 826 is tensor(0.0882, grad_fn=<MseLossBackward0>)
Current timestep = 827. State = [[0.01060587 0.3476643 ]]. Action = [[-0.01084651 -0.08867992 -0.24276026  0.21912348]]. Reward = [0.]
Curr episode timestep = 827
Scene graph at timestep 827 is [False, True, False, False, False, True]
Current timestep = 828. State = [[0.01060587 0.3476643 ]]. Action = [[-0.03357676  0.17309323 -0.18931717  0.42798185]]. Reward = [0.]
Curr episode timestep = 828
Scene graph at timestep 828 is [False, True, False, False, False, True]
Current timestep = 829. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.24473366 -0.19068101 -0.10698374 -0.56486595]]. Reward = [0.]
Curr episode timestep = 829
Scene graph at timestep 829 is [False, True, False, False, False, True]
Current timestep = 830. State = [[0.01060587 0.3476643 ]]. Action = [[0.15775043 0.03598836 0.06388235 0.6298704 ]]. Reward = [0.]
Curr episode timestep = 830
Scene graph at timestep 830 is [False, True, False, False, False, True]
Current timestep = 831. State = [[0.01060587 0.3476643 ]]. Action = [[-0.01490763  0.07929119 -0.06180395 -0.93713677]]. Reward = [0.]
Curr episode timestep = 831
Scene graph at timestep 831 is [False, True, False, False, False, True]
Human Feedback received at timestep 831 of -1
Current timestep = 832. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.15324986 -0.20470272 -0.22636856  0.6372268 ]]. Reward = [0.]
Curr episode timestep = 832
Scene graph at timestep 832 is [False, True, False, False, False, True]
Current timestep = 833. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.09258634 -0.08173594 -0.05256239 -0.84256804]]. Reward = [0.]
Curr episode timestep = 833
Scene graph at timestep 833 is [False, True, False, False, False, True]
Current timestep = 834. State = [[0.01060587 0.3476643 ]]. Action = [[-0.09158349  0.05811948  0.15416172  0.645414  ]]. Reward = [0.]
Curr episode timestep = 834
Scene graph at timestep 834 is [False, True, False, False, False, True]
Current timestep = 835. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.11461872  0.13953042 -0.1555098   0.7376232 ]]. Reward = [0.]
Curr episode timestep = 835
Scene graph at timestep 835 is [False, True, False, False, False, True]
Current timestep = 836. State = [[0.01060587 0.3476643 ]]. Action = [[-3.0866265e-04  5.0708860e-02 -5.1482558e-02  5.3654909e-01]]. Reward = [0.]
Curr episode timestep = 836
Scene graph at timestep 836 is [False, True, False, False, False, True]
Current timestep = 837. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.16416329  0.11927867  0.23129246 -0.13291478]]. Reward = [0.]
Curr episode timestep = 837
Scene graph at timestep 837 is [False, True, False, False, False, True]
Scene graph at timestep 837 is [False, True, False, False, False, True]
State prediction error at timestep 837 is tensor(0.0880, grad_fn=<MseLossBackward0>)
Current timestep = 838. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.19058213  0.00078651 -0.08712961  0.18038046]]. Reward = [0.]
Curr episode timestep = 838
Scene graph at timestep 838 is [False, True, False, False, False, True]
Current timestep = 839. State = [[0.01060587 0.3476643 ]]. Action = [[ 0.22407082  0.17436248 -0.07456365  0.0684495 ]]. Reward = [0.]
Curr episode timestep = 839
Scene graph at timestep 839 is [False, True, False, False, False, True]
Current timestep = 840. State = [[0.01060587 0.3476643 ]]. Action = [[-0.16409512 -0.13239533 -0.01702628 -0.08991122]]. Reward = [0.]
Curr episode timestep = 840
Scene graph at timestep 840 is [False, True, False, False, False, True]
