Current timestep = 0. State = [[-0.2573131   0.00593717]]. Action = [[ 0.05919206 -0.07896695  0.10002005  0.69164634]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0415, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-0.25518376  0.00404067]]. Action = [[-0.2317478   0.16154382 -0.17458123 -0.90845263]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0198, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.25691184  0.00683668]]. Action = [[ 0.10771903 -0.17335859  0.23816806 -0.8268405 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0095, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.25698465  0.00372539]]. Action = [[-0.15476477 -0.21614817 -0.09075923 -0.9427277 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.25916052 -0.00368596]]. Action = [[-0.22326693 -0.07973875 -0.01635273 -0.9672796 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-0.2653162 -0.0111912]]. Action = [[-0.15619195  0.22388244 -0.17905962 -0.8184555 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.27237725 -0.01024133]]. Action = [[-0.22808771 -0.23531578 -0.01785474 -0.9420157 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.28095606 -0.01524325]]. Action = [[ 0.24833083 -0.15899217  0.10209155 -0.44670177]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.28526482 -0.02263047]]. Action = [[-0.12110019 -0.00842741  0.22881049  0.6864947 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0109, grad_fn=<MseLossBackward0>)
Current timestep = 9. State = [[-0.28941116 -0.02711547]]. Action = [[0.11077788 0.22055244 0.17680833 0.39458668]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0108, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.290495   -0.02494345]]. Action = [[-0.11412372 -0.11471045 -0.12569857 -0.8625013 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.29197684 -0.02604841]]. Action = [[ 0.09045309  0.00988105 -0.18701668 -0.26439083]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Current timestep = 12. State = [[-0.29190254 -0.02651581]]. Action = [[ 0.12473738 -0.05173434  0.24319726 -0.14952362]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.29166916 -0.02755545]]. Action = [[-0.15793583 -0.07624364 -0.11188099 -0.10313833]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.2923101  -0.03063872]]. Action = [[-0.19222927 -0.08823842  0.19590592 -0.14861548]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-0.29612327 -0.03587443]]. Action = [[-0.22863764 -0.1479592  -0.22792372 -0.0902428 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0058, grad_fn=<MseLossBackward0>)
