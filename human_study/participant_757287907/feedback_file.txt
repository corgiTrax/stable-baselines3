Current timestep = 0. State = [[-0.25770724  0.00781734]]. Action = [[ 0.05922416 -0.07896233  0.10002762  0.6916282 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0417, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-0.2557349   0.00619931]]. Action = [[-0.23175828  0.16157135 -0.17457658 -0.90838706]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0202, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.25735882  0.00848491]]. Action = [[ 0.10776287 -0.1733665   0.23816895 -0.8267299 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0097, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.25748768  0.00570181]]. Action = [[-0.15479423 -0.21615426 -0.09074351 -0.94270766]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.2598512  -0.00231693]]. Action = [[-0.22328691 -0.079748   -0.01633672 -0.9672672 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-0.26596755 -0.00855149]]. Action = [[-0.15621468  0.22388864 -0.1790616  -0.8184279 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.27232015 -0.00738129]]. Action = [[-0.2281097  -0.23532566 -0.01785083 -0.9420046 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.28139403 -0.0131668 ]]. Action = [[ 0.24833441 -0.15900682  0.10210484 -0.4466617 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.28545037 -0.02069452]]. Action = [[-0.1211254  -0.00841351  0.22881496  0.68653154]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0109, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.28994584 -0.02579258]]. Action = [[0.11080506 0.22056705 0.17681193 0.39462113]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0109, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.29087684 -0.02365551]]. Action = [[-0.11413616 -0.11471158 -0.12570974 -0.862528  ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 10 of -1
Current timestep = 11. State = [[-0.29228267 -0.02463116]]. Action = [[ 0.09046763  0.00989705 -0.18703476 -0.26446342]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Current timestep = 12. State = [[-0.29226637 -0.0249014 ]]. Action = [[ 0.12475726 -0.0517142   0.2431908  -0.14966518]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.29188558 -0.02605734]]. Action = [[-0.15792714 -0.07620393 -0.11199418 -0.10334468]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.29256204 -0.02922556]]. Action = [[-0.19221433 -0.08817467  0.19579518 -0.14895695]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-0.2968318 -0.0345532]]. Action = [[-0.22862478 -0.1478816  -0.22796053 -0.09079516]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 15 of -1
Current timestep = 16. State = [[-0.30419606 -0.04164801]]. Action = [[ 0.02626854  0.04819843 -0.15904853 -0.68845403]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 17. State = [[-0.30858767 -0.04535253]]. Action = [[ 0.17059374  0.15236518  0.24373543 -0.9491984 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 17 of -1
Current timestep = 18. State = [[-0.3087905  -0.04338971]]. Action = [[ 0.09091011 -0.07543081  0.18253088  0.5604265 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 18 of -1
Current timestep = 19. State = [[-0.30787486 -0.04354265]]. Action = [[ 0.19140208 -0.04321693 -0.21274988  0.78752637]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.30344707 -0.04435387]]. Action = [[ 0.19281876 -0.02007684 -0.2010747  -0.00596052]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0078, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 20 of -1
Current timestep = 21. State = [[-0.29756775 -0.04504477]]. Action = [[0.16819322 0.06928396 0.23907706 0.6945286 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Current timestep = 22. State = [[-0.29089418 -0.04516362]]. Action = [[-0.10463849 -0.20457841  0.0914965   0.8416076 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.28769353 -0.04925192]]. Action = [[ 0.0209842   0.05062315 -0.03114028 -0.90146166]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 23 of 1
Current timestep = 24. State = [[-0.2860925  -0.05089732]]. Action = [[-0.14737085 -0.13787815 -0.01845458  0.38613403]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Current timestep = 25. State = [[-0.28650433 -0.05624365]]. Action = [[-0.1139636  -0.17825858  0.15808648  0.666126  ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.28924072 -0.06538633]]. Action = [[-0.18256435 -0.17323524 -0.03577892 -0.28264976]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 26 of 1
Current timestep = 27. State = [[-0.29453713 -0.075643  ]]. Action = [[-0.11206684 -0.13291955 -0.17262289  0.68588185]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.30010542 -0.08550388]]. Action = [[-0.19128779  0.08443347  0.09602836  0.96425605]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 28 of 1
Current timestep = 29. State = [[-0.3076963  -0.09106536]]. Action = [[-0.0180058  -0.06515563 -0.1988702  -0.18293637]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0093, grad_fn=<MseLossBackward0>)
Current timestep = 30. State = [[-0.31209773 -0.09562228]]. Action = [[-0.1317469   0.0288201   0.20061442  0.44810176]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 30 of -1
Current timestep = 31. State = [[-0.32015875 -0.09853781]]. Action = [[ 0.00501102 -0.16262342 -0.10627463 -0.9658226 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 32. State = [[-0.3247265 -0.1025977]]. Action = [[0.14487049 0.20296013 0.20169955 0.6618588 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 32 of -1
Current timestep = 33. State = [[-0.32434765 -0.10134827]]. Action = [[-0.06165516 -0.16773194 -0.07573842 -0.50800973]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0068, grad_fn=<MseLossBackward0>)
Current timestep = 34. State = [[-0.32431695 -0.10394488]]. Action = [[-0.05101851 -0.18469884  0.01155448 -0.17239314]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0088, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.3252094  -0.11004519]]. Action = [[0.06982955 0.20706838 0.18221033 0.6303997 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Current timestep = 36. State = [[-0.32482567 -0.10961944]]. Action = [[ 0.1834833  -0.16219892  0.02449784 -0.7863368 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 36 of -1
Current timestep = 37. State = [[-0.3229363  -0.11167431]]. Action = [[ 0.23016599  0.17059016 -0.10103974 -0.4331761 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Current timestep = 38. State = [[-0.3185124  -0.11055648]]. Action = [[-0.04075924 -0.21361452 -0.11544961 -0.690791  ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 38 of -1
Current timestep = 39. State = [[-0.31520566 -0.11376007]]. Action = [[ 0.13814089 -0.00411102 -0.09663363  0.2915094 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Current timestep = 40. State = [[-0.3113561  -0.11670446]]. Action = [[ 0.09163097 -0.1659935  -0.12090388  0.47076833]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 40 of 1
Current timestep = 41. State = [[-0.30708647 -0.12255961]]. Action = [[-0.04300369 -0.13656837  0.24573088 -0.24150693]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Current timestep = 42. State = [[-0.30436948 -0.13072102]]. Action = [[-0.12650104 -0.00148396 -0.09582517 -0.6612852 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.30431575 -0.13571854]]. Action = [[ 0.10186666  0.15837649 -0.1523842  -0.7863142 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, True, False, False]
State prediction error at timestep 43 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.3037418  -0.13491423]]. Action = [[ 0.00138056  0.1866017   0.05382222 -0.86345756]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, True, False, False]
State prediction error at timestep 44 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 45. State = [[-0.30292067 -0.12988807]]. Action = [[-0.14384001  0.05748263 -0.24239686 -0.50017625]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, True, False, False]
State prediction error at timestep 45 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 45 of 1
Current timestep = 46. State = [[-0.3035793 -0.1258131]]. Action = [[-0.15081786  0.14260125  0.1522932   0.22089839]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, True, False, False]
State prediction error at timestep 46 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Current timestep = 47. State = [[-0.30654332 -0.11944361]]. Action = [[-0.12718351  0.04025221 -0.06886393  0.6750517 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.3110293  -0.11394405]]. Action = [[ 0.15630114 -0.13408178 -0.19675809  0.5819596 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 49. State = [[-0.31131285 -0.11386915]]. Action = [[-0.19624683 -0.03935647 -0.23976733 -0.7302193 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 50. State = [[-0.31315207 -0.11468919]]. Action = [[-0.0364539  -0.01159014  0.17326021 -0.62802607]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 51. State = [[-0.3148684  -0.11662644]]. Action = [[-0.23185435 -0.19169374  0.09430489  0.48537445]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 51 of -1
Current timestep = 52. State = [[-0.32004157 -0.1229615 ]]. Action = [[ 0.1648711  -0.21168756 -0.22220103 -0.32248688]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Current timestep = 53. State = [[-0.32149413 -0.12994063]]. Action = [[0.15969464 0.14718646 0.19306144 0.7828585 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, True, False, False]
State prediction error at timestep 53 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 53 of 1
Current timestep = 54. State = [[-0.31984746 -0.13189788]]. Action = [[ 0.07370704 -0.21369183  0.1474343  -0.06158334]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, True, False, False]
State prediction error at timestep 54 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Current timestep = 55. State = [[-0.31767964 -0.13815849]]. Action = [[-0.05885783  0.23815918  0.01297826 -0.01158458]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, True, False, False]
State prediction error at timestep 55 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.31740302 -0.13694857]]. Action = [[0.23165789 0.15301818 0.22718623 0.56176805]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, True, False, False]
State prediction error at timestep 56 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of 1
Current timestep = 57. State = [[-0.3143971  -0.13191928]]. Action = [[-0.04594371 -0.12096369  0.08702683 -0.9118565 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, True, False, False]
State prediction error at timestep 57 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 58. State = [[-0.31366283 -0.131841  ]]. Action = [[ 0.12343872 -0.185396   -0.16781446  0.91422224]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, True, False, False]
State prediction error at timestep 58 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 59. State = [[-0.31075287 -0.13580307]]. Action = [[ 0.02667058 -0.02340589 -0.02441241  0.9055822 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, True, False, False]
State prediction error at timestep 59 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 59 of -1
Current timestep = 60. State = [[-0.30781093 -0.13878317]]. Action = [[0.21422264 0.07828701 0.01029259 0.26680696]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, True, False, False]
State prediction error at timestep 60 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Current timestep = 61. State = [[-0.30345842 -0.13944764]]. Action = [[-0.16258885 -0.11836608  0.08663601 -0.11100537]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, True, False, False]
State prediction error at timestep 61 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 61 of -1
Current timestep = 62. State = [[-0.30295587 -0.14219254]]. Action = [[ 0.23085111 -0.14048555 -0.01440172  0.38453817]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, True, False, False]
State prediction error at timestep 62 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Current timestep = 63. State = [[-0.29829746 -0.14777437]]. Action = [[-0.11704355 -0.04616766 -0.20098443  0.8685218 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, True, False, False]
State prediction error at timestep 63 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 63 of 1
Current timestep = 64. State = [[-0.2966105  -0.15303761]]. Action = [[-0.03722395 -0.048747    0.20397466 -0.5519893 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, True, False, False]
State prediction error at timestep 64 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 65. State = [[-0.29633704 -0.1565226 ]]. Action = [[ 0.14990735  0.02191234 -0.23746307 -0.62988377]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, True, False, False]
State prediction error at timestep 65 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 66. State = [[-0.29348707 -0.15884966]]. Action = [[-0.16288276 -0.06886306 -0.23355277 -0.8142298 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, True, False, False]
State prediction error at timestep 66 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 67. State = [[-0.29394385 -0.1616402 ]]. Action = [[-0.2039562   0.01102203 -0.22039552 -0.3704604 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, True, False, False]
State prediction error at timestep 67 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of 1
Current timestep = 68. State = [[-0.2968378  -0.16452828]]. Action = [[-0.12378544  0.23723322  0.10921836  0.8492849 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, True, False, False]
State prediction error at timestep 68 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Current timestep = 69. State = [[-0.30100325 -0.16099608]]. Action = [[ 0.19787127 -0.02122669  0.170896    0.1727252 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, True, False, False]
State prediction error at timestep 69 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of 1
Current timestep = 70. State = [[-0.30092737 -0.15943305]]. Action = [[ 0.04078948  0.11757517 -0.13659346 -0.19555068]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, True, False, False]
State prediction error at timestep 70 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 71. State = [[-0.30070543 -0.15531893]]. Action = [[-0.22491454  0.20078737 -0.2383985   0.49170542]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, True, False, False]
State prediction error at timestep 71 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 72. State = [[-0.30341938 -0.14725542]]. Action = [[ 0.19637448 -0.01153858 -0.13408835  0.6300858 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, True, False, False]
State prediction error at timestep 72 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 73. State = [[-0.3034149  -0.14252356]]. Action = [[-0.16326624 -0.04491659 -0.07304443  0.08711874]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, True, False, False]
State prediction error at timestep 73 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-0.30452174 -0.14067447]]. Action = [[ 0.15759408 -0.01486075 -0.13825598  0.21180797]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, True, False, False]
State prediction error at timestep 74 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.30433312 -0.14000103]]. Action = [[0.11508569 0.03517693 0.02566308 0.9880396 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, True, False, False]
State prediction error at timestep 75 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 76. State = [[-0.30342463 -0.13899544]]. Action = [[-0.12587197 -0.23650594 -0.06081086  0.58079565]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, True, False, False]
State prediction error at timestep 76 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 77. State = [[-0.30354023 -0.1426188 ]]. Action = [[ 0.12849945  0.05241686 -0.10828388 -0.01145762]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, True, False, False]
State prediction error at timestep 77 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 77 of -1
Current timestep = 78. State = [[-0.30191103 -0.14337502]]. Action = [[ 0.11226115 -0.04533578 -0.10172614  0.35690236]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, True, False, False]
State prediction error at timestep 78 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 79. State = [[-0.2979747  -0.14458376]]. Action = [[-0.19917928  0.13194215  0.22007003 -0.32755613]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, True, False, False]
State prediction error at timestep 79 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of 1
Current timestep = 80. State = [[-0.29801083 -0.14386965]]. Action = [[ 0.10995692 -0.21171343  0.22814965 -0.7985786 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, True, False, False]
State prediction error at timestep 80 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 81. State = [[-0.29753774 -0.14764817]]. Action = [[ 0.06382361  0.02192339  0.14726967 -0.4911483 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, True, False, False]
State prediction error at timestep 81 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 81 of 1
Current timestep = 82. State = [[-0.2961687  -0.14903736]]. Action = [[0.07657897 0.11621892 0.2313031  0.0575819 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, True, False, False]
State prediction error at timestep 82 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 82 of 1
Current timestep = 83. State = [[-0.29363346 -0.14829029]]. Action = [[-0.10857716  0.05253142  0.24113345 -0.6384575 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, True, False, False]
State prediction error at timestep 83 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 84. State = [[-0.29347467 -0.14688179]]. Action = [[-0.21887809  0.17726868 -0.06418757 -0.64945054]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, True, False, False]
State prediction error at timestep 84 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of 1
Current timestep = 85. State = [[-0.295316   -0.14133032]]. Action = [[ 0.05404142 -0.03387725  0.15180126 -0.5135653 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 86. State = [[-0.2956415  -0.13896623]]. Action = [[-0.05140781  0.18835536 -0.17795686  0.20280051]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, True, False, False]
State prediction error at timestep 86 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 87. State = [[-0.2969936  -0.13203341]]. Action = [[-0.17526834  0.19638237 -0.22447754  0.23278749]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, True, False, False]
State prediction error at timestep 87 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 88. State = [[-0.30221236 -0.12175395]]. Action = [[ 0.05603313  0.02202436  0.20136666 -0.3370738 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.30496508 -0.11466494]]. Action = [[-0.01163219  0.15899277  0.07166186 -0.67228544]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 90. State = [[-0.3063159  -0.10669871]]. Action = [[ 0.17620358 -0.05507939 -0.06108053 -0.9363062 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(5.5055e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of -1
Current timestep = 91. State = [[-0.3058537  -0.10290984]]. Action = [[-0.19768336 -0.0991329   0.07005566 -0.19249034]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
State prediction error at timestep 91 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 92. State = [[-0.30700457 -0.1034505 ]]. Action = [[-0.01657891  0.02757573  0.10700136  0.8987887 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
State prediction error at timestep 92 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 92 of -1
Current timestep = 93. State = [[-0.30791518 -0.10331113]]. Action = [[ 0.08477294 -0.12188503  0.21664917  0.22470939]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
State prediction error at timestep 93 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 94. State = [[-0.30811918 -0.10456692]]. Action = [[0.0932214  0.15540868 0.20837218 0.26568282]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of -1
Current timestep = 95. State = [[-0.3081823  -0.10305572]]. Action = [[-0.17644227 -0.03955966 -0.12810317  0.83835375]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
State prediction error at timestep 95 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 96. State = [[-0.30878705 -0.10343021]]. Action = [[-0.21471766 -0.1833624  -0.08669853 -0.71783936]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
State prediction error at timestep 96 is tensor(7.8072e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 96 of 1
Current timestep = 97. State = [[-0.31269178 -0.10719289]]. Action = [[ 0.08127171  0.12036681 -0.17674282 -0.64552844]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
State prediction error at timestep 97 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 97 of 1
Current timestep = 98. State = [[-0.31379497 -0.10698879]]. Action = [[-0.21064422  0.22551423 -0.21969366 -0.15294069]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
State prediction error at timestep 98 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 98 of 1
Current timestep = 99. State = [[-0.31706098 -0.10115395]]. Action = [[-0.09316242  0.17015228  0.15974963 -0.5108572 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
State prediction error at timestep 99 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 100. State = [[-0.32222804 -0.09233797]]. Action = [[-0.17662813 -0.07483327 -0.03663513 -0.4597633 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 100 of 1
Current timestep = 101. State = [[-0.3289308  -0.08932524]]. Action = [[ 0.2363967  -0.1461723  -0.1735601   0.63414764]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
State prediction error at timestep 101 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 102. State = [[-0.32889587 -0.08899414]]. Action = [[-0.1167087   0.05999783  0.13658422 -0.84469265]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
State prediction error at timestep 102 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 103. State = [[-0.3295646  -0.08908708]]. Action = [[-0.08017749 -0.09456007 -0.19517188  0.04426432]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
State prediction error at timestep 103 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 103 of -1
Current timestep = 104. State = [[-0.33078742 -0.09032194]]. Action = [[ 0.23798555  0.07745683 -0.20660388 -0.79590464]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
State prediction error at timestep 104 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 105. State = [[-0.32971266 -0.0900388 ]]. Action = [[ 0.10378364  0.11473599 -0.22076476 -0.8142806 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
State prediction error at timestep 105 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 105 of -1
Current timestep = 106. State = [[-0.32773945 -0.08826122]]. Action = [[-0.16632165  0.22624177 -0.05587016 -0.01961231]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 107. State = [[-0.3281724  -0.08520979]]. Action = [[ 0.02494246 -0.19025296 -0.22928078  0.24301815]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 107 of -1
Current timestep = 108. State = [[-0.32853532 -0.08511257]]. Action = [[-0.01528525  0.09549624  0.19421893  0.96403646]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of -1
Current timestep = 109. State = [[-0.32902375 -0.08439822]]. Action = [[ 0.05061817 -0.03477974  0.11223876 -0.834709  ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
State prediction error at timestep 109 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 110. State = [[-0.32891262 -0.08411463]]. Action = [[-0.15556291 -0.16714336 -0.19453661 -0.07504833]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
State prediction error at timestep 110 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 110 of -1
Current timestep = 111. State = [[-0.32994282 -0.08573741]]. Action = [[ 0.186284    0.03077194  0.16917196 -0.9300889 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
State prediction error at timestep 111 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 112. State = [[-0.32967743 -0.08559635]]. Action = [[-0.15119082 -0.00535946  0.06092435  0.12945461]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
State prediction error at timestep 112 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 112 of -1
Current timestep = 113. State = [[-0.32970056 -0.08585643]]. Action = [[ 0.24299198 -0.04262963 -0.21383277  0.5320041 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of -1
Current timestep = 114. State = [[-0.32776758 -0.08597512]]. Action = [[ 0.12168026 -0.03020558 -0.11469615  0.5336952 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
State prediction error at timestep 114 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 115. State = [[-0.32485056 -0.08638388]]. Action = [[-0.09935087 -0.06671768 -0.21473049  0.9287727 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
State prediction error at timestep 115 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 115 of -1
Current timestep = 116. State = [[-0.32426345 -0.08673918]]. Action = [[ 0.16643652  0.12057099 -0.24100856 -0.11140257]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 117. State = [[-0.32227367 -0.08570201]]. Action = [[-0.14473075  0.17233497  0.04685351  0.14749491]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
State prediction error at timestep 117 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 117 of -1
Current timestep = 118. State = [[-0.3227888  -0.08176345]]. Action = [[-0.04128136  0.10388279  0.16022575 -0.15842128]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
State prediction error at timestep 118 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 119. State = [[-0.32343075 -0.07645243]]. Action = [[ 0.23212647 -0.12044258 -0.11387789 -0.02494866]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.3210445  -0.07481834]]. Action = [[0.00940806 0.20263076 0.11923453 0.8432349 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
State prediction error at timestep 120 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 121. State = [[-0.31934732 -0.07071398]]. Action = [[ 0.1843954  -0.20698513  0.16900137  0.95741796]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(8.8251e-05, grad_fn=<MseLossBackward0>)
Current timestep = 122. State = [[-0.31529638 -0.07150532]]. Action = [[ 0.06410295 -0.02815312 -0.21494617  0.86510587]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
State prediction error at timestep 122 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 123. State = [[-0.31113234 -0.07216161]]. Action = [[-0.14410527 -0.0084929   0.12380806 -0.53841126]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of 1
Current timestep = 124. State = [[-0.31030262 -0.07251137]]. Action = [[ 0.11462757  0.22294524 -0.14496     0.51973104]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.3088377  -0.06848552]]. Action = [[ 0.19254732  0.22536957 -0.12254846 -0.9359174 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
State prediction error at timestep 125 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 126. State = [[-0.30432287 -0.0608053 ]]. Action = [[-0.24322136  0.02370596  0.09729028  0.19634604]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.30522728 -0.05416309]]. Action = [[ 0.1635685   0.15939057  0.22299647 -0.77907556]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 127 of 1
Current timestep = 128. State = [[-0.3036582  -0.04592806]]. Action = [[0.04874411 0.1381228  0.12510681 0.23261237]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
State prediction error at timestep 128 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 129. State = [[-0.3004169 -0.0358973]]. Action = [[-0.06551948  0.19340724  0.01999852  0.62230825]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
State prediction error at timestep 129 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 130. State = [[-0.29955235 -0.02551905]]. Action = [[ 0.19080073 -0.1378415  -0.13615541 -0.11639667]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 130 of 1
Current timestep = 131. State = [[-0.29551318 -0.02296985]]. Action = [[-0.17851761 -0.10076538 -0.0237543  -0.8247591 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 132. State = [[-0.2952224  -0.02327927]]. Action = [[-0.05392343  0.061979    0.23376423 -0.8626319 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 132 of 1
Current timestep = 133. State = [[-0.29550573 -0.02249307]]. Action = [[ 0.04678112 -0.12526977  0.19914275 -0.94603044]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 134. State = [[-0.2953413 -0.0243139]]. Action = [[ 0.11535552 -0.17647773  0.05607593 -0.7137838 ]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.29339176 -0.02965637]]. Action = [[-0.05011559  0.07189387  0.01021618 -0.630675  ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 136. State = [[-0.29281658 -0.03031114]]. Action = [[0.1082249  0.18184444 0.16504991 0.13542151]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.29075933 -0.02760023]]. Action = [[ 0.00909373 -0.01502132 -0.06019197 -0.59697706]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 137 of 1
Current timestep = 138. State = [[-0.28974032 -0.02669321]]. Action = [[-0.0205951  -0.06904905 -0.11064869  0.7578496 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 139. State = [[-0.28956547 -0.02642016]]. Action = [[-0.16517405  0.20583466 -0.18726973 -0.7816196 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 139 of 1
Current timestep = 140. State = [[-0.29111078 -0.02139248]]. Action = [[-0.13877383  0.2190674  -0.11755548 -0.6366123 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 141. State = [[-0.2946581  -0.01247309]]. Action = [[ 0.09601206 -0.01441891  0.12749285 -0.5654514 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.29602876 -0.00741928]]. Action = [[-0.07197052 -0.07106267 -0.13425614 -0.52156985]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 143. State = [[-0.29735073 -0.00622681]]. Action = [[-0.23634785 -0.09219128  0.03279388  0.20473337]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
State prediction error at timestep 143 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 144. State = [[-0.30217844 -0.00790288]]. Action = [[-0.04544589 -0.10471162 -0.01381242  0.32325602]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 144 of 1
Current timestep = 145. State = [[-0.3064271 -0.0114888]]. Action = [[0.16620249 0.20048249 0.20370948 0.5172708 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.3074373  -0.00906479]]. Action = [[-0.19175123 -0.21032669  0.1813632   0.5440346 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 147. State = [[-0.30941516 -0.01329197]]. Action = [[-0.07947752 -0.061221   -0.1661974   0.1003803 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 147 of -1
Current timestep = 148. State = [[-0.3125801  -0.01762383]]. Action = [[-0.15736324 -0.11612171  0.1144931   0.76506126]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.31930113 -0.02411626]]. Action = [[-0.03135036 -0.11721909 -0.07730344  0.5971596 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 149 of -1
Current timestep = 150. State = [[-0.32411733 -0.02926302]]. Action = [[0.07854974 0.23303977 0.18903157 0.43528116]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 151. State = [[-0.32491785 -0.02836866]]. Action = [[-0.22736593  0.23793203  0.15657288  0.74213433]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, True, False]
State prediction error at timestep 151 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 151 of -1
Current timestep = 152. State = [[-0.32925135 -0.02433648]]. Action = [[-0.1476388   0.20097569  0.00268653 -0.8246194 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 153. State = [[-0.33581057 -0.01738059]]. Action = [[0.12168133 0.15960705 0.01488045 0.84526753]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 154. State = [[-0.33869395 -0.00853513]]. Action = [[-0.17954697 -0.12245429 -0.20342135  0.7537594 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of -1
Current timestep = 155. State = [[-0.34225297 -0.00333161]]. Action = [[ 0.03903207 -0.12372921  0.05403057  0.7700925 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 156. State = [[-0.34401685 -0.00174804]]. Action = [[ 0.24472511  0.21444416  0.23186547 -0.99741495]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of -1
Current timestep = 157. State = [[-0.34305516  0.00230554]]. Action = [[-0.17901416 -0.03237914  0.10452896  0.58742046]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Current timestep = 158. State = [[-0.34418824  0.00475278]]. Action = [[ 0.10929161  0.16886884  0.05984339 -0.8739771 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of -1
Current timestep = 159. State = [[-0.3446951  0.0093198]]. Action = [[ 0.22374752  0.09070021  0.00760528 -0.5074305 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Current timestep = 160. State = [[-0.34197262  0.0153953 ]]. Action = [[-0.03412691  0.09090194 -0.16904494 -0.80976766]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of -1
Current timestep = 161. State = [[-0.3414451  0.0219655]]. Action = [[-0.04043745  0.21508628  0.13228717 -0.7951023 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Current timestep = 162. State = [[-0.342151    0.03130871]]. Action = [[-0.20952886 -0.03786816 -0.242527    0.5951288 ]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Current timestep = 163. State = [[-0.34498465  0.03714122]]. Action = [[ 0.10717875  0.17176911 -0.16480985  0.02166724]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0083, grad_fn=<MseLossBackward0>)
Current timestep = 164. State = [[-0.34752938  0.04364818]]. Action = [[-0.09675628  0.1203517   0.06261483 -0.707617  ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 164 of -1
Current timestep = 165. State = [[-0.35146812  0.05193278]]. Action = [[ 0.07419664  0.2250737  -0.20700699 -0.5542397 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0086, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.35367626  0.06263224]]. Action = [[-0.18988723 -0.05781704 -0.19002922 -0.48128772]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0085, grad_fn=<MseLossBackward0>)
Current timestep = 167. State = [[-0.357802    0.06947201]]. Action = [[ 0.14358097  0.07411438  0.05778384 -0.12783313]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0112, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 167 of -1
Current timestep = 168. State = [[-0.35954934  0.07555452]]. Action = [[-1.21904165e-01  3.17798853e-02  5.62369823e-05  5.95620632e-01]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Current timestep = 169. State = [[-0.36292455  0.08139182]]. Action = [[ 0.03297895  0.22611266 -0.14704888 -0.8586812 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of -1
Current timestep = 170. State = [[-0.3644      0.08916495]]. Action = [[ 0.22472018 -0.07839599 -0.03670847  0.8988646 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(0.0103, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 170 of -1
Current timestep = 171. State = [[-0.36102468  0.095314  ]]. Action = [[-0.06019837  0.17217416  0.07028833  0.88364387]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(0.0105, grad_fn=<MseLossBackward0>)
Current timestep = 172. State = [[-0.35875472  0.10200296]]. Action = [[ 0.02729017  0.14865339  0.13352475 -0.957266  ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0093, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 172 of -1
Current timestep = 173. State = [[-0.35585132  0.10947119]]. Action = [[ 0.11769557 -0.19073617 -0.10571659  0.6355177 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(0.0135, grad_fn=<MseLossBackward0>)
Current timestep = 174. State = [[-0.35239956  0.11154225]]. Action = [[ 0.04289788 -0.12331279  0.16487488 -0.96385795]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Current timestep = 175. State = [[-0.34987688  0.11120436]]. Action = [[-0.23057204 -0.07862759 -0.11170895 -0.92579985]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of -1
Current timestep = 176. State = [[-0.35058343  0.10940181]]. Action = [[ 0.12730017  0.16544935  0.24476337 -0.71668226]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0103, grad_fn=<MseLossBackward0>)
Current timestep = 177. State = [[-0.3505127  0.1096972]]. Action = [[ 0.181084   -0.01849371  0.19189337 -0.21886718]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0118, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 177 of -1
Current timestep = 178. State = [[-0.347295    0.11063635]]. Action = [[-0.09838384 -0.15090589  0.01670274 -0.6849829 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0086, grad_fn=<MseLossBackward0>)
Current timestep = 179. State = [[-0.3461343  0.1091482]]. Action = [[-0.14182313 -0.00219782 -0.21378607  0.57556677]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0112, grad_fn=<MseLossBackward0>)
Current timestep = 180. State = [[-0.34670883  0.10763402]]. Action = [[-0.21446605  0.14782375  0.01734662 -0.00887638]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0121, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of -1
Current timestep = 181. State = [[-0.35063636  0.10740697]]. Action = [[-0.04346865 -0.15370029  0.04460874  0.05829382]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0119, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.3542735  0.1046277]]. Action = [[-0.06616375  0.1669848  -0.07812601  0.05915368]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
State prediction error at timestep 182 is tensor(0.0126, grad_fn=<MseLossBackward0>)
Current timestep = 183. State = [[-0.35697243  0.10521062]]. Action = [[-0.18240854 -0.06429273  0.00115755 -0.6937049 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0078, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of 1
Current timestep = 184. State = [[-0.36215195  0.10492013]]. Action = [[-0.23236877  0.01003972  0.20535305  0.75273395]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0088, grad_fn=<MseLossBackward0>)
Current timestep = 185. State = [[-0.36944917  0.10450038]]. Action = [[ 0.08124077  0.23331636 -0.08893071  0.828166  ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(0.0102, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 185 of -1
Current timestep = 186. State = [[-0.37457728  0.10829956]]. Action = [[ 0.01512402 -0.12888502  0.05366814  0.38185263]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(0.0117, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 186 of -1
Current timestep = 187. State = [[-0.3780763   0.11047173]]. Action = [[ 0.13814706 -0.13195336  0.16208446  0.99069726]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0090, grad_fn=<MseLossBackward0>)
Current timestep = 188. State = [[-0.37793866  0.10903133]]. Action = [[ 0.14585876 -0.1357757  -0.10109273 -0.36555636]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0115, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of -1
Current timestep = 189. State = [[-0.37678856  0.10547312]]. Action = [[ 0.15377584  0.20329466 -0.22874857  0.10753632]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0135, grad_fn=<MseLossBackward0>)
Current timestep = 190. State = [[-0.37280625  0.10671068]]. Action = [[ 0.00093374 -0.1218389   0.0432772   0.8951993 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0085, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of -1
Current timestep = 191. State = [[-0.3696834   0.10607475]]. Action = [[ 0.24474037 -0.06195717  0.06626979  0.58678675]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(0.0092, grad_fn=<MseLossBackward0>)
Current timestep = 192. State = [[-0.3638201   0.10636811]]. Action = [[ 0.10660982  0.06627902  0.02150646 -0.9576305 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Current timestep = 193. State = [[-0.35822406  0.10804925]]. Action = [[ 0.06707236 -0.05109674  0.06085703  0.6604984 ]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
State prediction error at timestep 193 is tensor(0.0078, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 193 of -1
Current timestep = 194. State = [[-0.35409445  0.1089282 ]]. Action = [[ 0.17878237  0.17781079  0.20102817 -0.7551242 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
State prediction error at timestep 194 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 194 of 1
Current timestep = 195. State = [[-0.34731564  0.11181199]]. Action = [[-0.10980931 -0.10143161 -0.1906746  -0.9295307 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Current timestep = 196. State = [[-0.34416264  0.11003489]]. Action = [[-0.15098593 -0.09709051  0.19259086  0.6751692 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Current timestep = 197. State = [[-0.3431916  0.1077444]]. Action = [[ 0.22714543 -0.14813459 -0.13938011 -0.10101181]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0085, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.34075913  0.10522583]]. Action = [[-0.03910209  0.21910131 -0.08327073 -0.6961737 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Current timestep = 199. State = [[-0.33905715  0.10579918]]. Action = [[-0.24382158  0.07884353 -0.2166543  -0.38349617]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of 1
Current timestep = 200. State = [[-0.34062424  0.10783152]]. Action = [[ 0.11645201 -0.17281576  0.08442125  0.39077783]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 200 of 1
Current timestep = 201. State = [[-0.34073484  0.10718022]]. Action = [[ 0.00305423 -0.19397691  0.19856972 -0.5223052 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
State prediction error at timestep 201 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Current timestep = 202. State = [[-0.34018412  0.10346375]]. Action = [[-0.10802259 -0.18500587 -0.18060474  0.31170487]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 202 of 1
Current timestep = 203. State = [[-0.34217626  0.09536301]]. Action = [[-0.0502373   0.20121813  0.05411804  0.27778983]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Current timestep = 204. State = [[-0.34346345  0.09470087]]. Action = [[ 0.19424272  0.2152541  -0.19765712  0.85962915]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Current timestep = 205. State = [[-0.342865    0.09680846]]. Action = [[-0.05947247 -0.01515804  0.01733217  0.05860794]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Current timestep = 206. State = [[-0.34291428  0.09759022]]. Action = [[-0.04980767  0.01443574  0.00535539 -0.79677564]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of 1
Current timestep = 207. State = [[-0.34299845  0.09824353]]. Action = [[ 0.1213387   0.08912718 -0.21441917  0.7262461 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Current timestep = 208. State = [[-0.34220374  0.10023689]]. Action = [[ 0.03662008  0.1650582  -0.11247635 -0.9498154 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 208 of 1
Current timestep = 209. State = [[-0.34003818  0.10567021]]. Action = [[-0.21314536 -0.02221197 -0.18962896 -0.9718215 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 210. State = [[-0.3425771   0.10975081]]. Action = [[-0.1298102  -0.06429785  0.00579903 -0.39513803]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Current timestep = 211. State = [[-0.3451529   0.11181363]]. Action = [[ 0.08021295 -0.10547142 -0.05979237  0.47724068]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 211 of 1
Current timestep = 212. State = [[-0.34518692  0.10966342]]. Action = [[ 0.12578323 -0.22771014 -0.16856523 -0.4297006 ]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Current timestep = 213. State = [[-0.34248978  0.10290182]]. Action = [[ 0.18719864 -0.03147149  0.18352443 -0.0570333 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Current timestep = 214. State = [[-0.33891124  0.09872849]]. Action = [[ 0.20818874  0.04360896 -0.01634434 -0.4101094 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 215. State = [[-0.33192155  0.09669642]]. Action = [[-0.17263421  0.22712633  0.20020491 -0.8262479 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 215 of -1
Current timestep = 216. State = [[-0.3308683   0.10041334]]. Action = [[-0.13319497  0.15138689  0.12140065  0.03025043]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 216 of 1
Current timestep = 217. State = [[-0.33366597  0.10620945]]. Action = [[-0.1957063  -0.19139479 -0.10492086  0.05245769]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Current timestep = 218. State = [[-0.33692232  0.10722494]]. Action = [[-0.09132081  0.18763736  0.13504407 -0.25241292]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 219. State = [[-0.34178758  0.112517  ]]. Action = [[-0.06905612  0.05149084 -0.09908018 -0.84313476]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of 1
Current timestep = 220. State = [[-0.34602332  0.11716202]]. Action = [[ 0.04443884 -0.22612548  0.10123396  0.14052689]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 221. State = [[-0.34693837  0.11351712]]. Action = [[-0.19647573 -0.06987688  0.09849775 -0.7653288 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, True, False]
State prediction error at timestep 221 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 221 of 1
Current timestep = 222. State = [[-0.3512739   0.10913701]]. Action = [[-0.11769004 -0.23029888 -0.08615115  0.7901268 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 222 of -1
Current timestep = 223. State = [[-0.3581987   0.10146105]]. Action = [[-0.15585957  0.21655324  0.00297916 -0.57677084]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
State prediction error at timestep 223 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 224. State = [[-0.3662598   0.09926394]]. Action = [[-0.05396155  0.12393537 -0.14546046 -0.89307356]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of -1
Current timestep = 225. State = [[-0.37208748  0.10099881]]. Action = [[-0.1020066   0.1166243   0.22065651  0.05345142]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 225 of -1
Current timestep = 226. State = [[-0.37657294  0.10475749]]. Action = [[-0.1400474  -0.11742434 -0.02143779 -0.02873224]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Current timestep = 227. State = [[-0.38340804  0.10580347]]. Action = [[ 0.09568897 -0.21132702  0.1881684  -0.3116868 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
State prediction error at timestep 227 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 227 of -1
Current timestep = 228. State = [[-0.38601005  0.10316993]]. Action = [[-0.02127068 -0.06364217 -0.07005966 -0.8885259 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 229. State = [[-0.38755116  0.10068907]]. Action = [[-0.2023499   0.08198407 -0.23547874  0.15876448]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 229 of -1
Current timestep = 230. State = [[-0.3890233   0.09986269]]. Action = [[ 0.18109024 -0.19476058 -0.07421598 -0.6727602 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 230 of -1
Current timestep = 231. State = [[-0.38835213  0.09628784]]. Action = [[-0.00109914  0.02037525  0.00612983 -0.12992424]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 232. State = [[-0.3877412  0.0935693]]. Action = [[-0.22954132  0.14355779  0.13665184 -0.20888221]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
State prediction error at timestep 232 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 232 of -1
Current timestep = 233. State = [[-0.38747534  0.09230299]]. Action = [[-0.08070916  0.20724851  0.22328413  0.5719619 ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, True, False]
State prediction error at timestep 233 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 234. State = [[-0.38739175  0.09177689]]. Action = [[ 0.03928006 -0.1278171   0.11042297 -0.25168788]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 234 of -1
Current timestep = 235. State = [[-0.3869821   0.08932004]]. Action = [[ 0.03061238 -0.151849   -0.0917566  -0.7941776 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 235 of -1
Current timestep = 236. State = [[-0.38627017  0.08583543]]. Action = [[-0.23304985 -0.21291646  0.00371003 -0.25491148]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, True, False]
State prediction error at timestep 236 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 237. State = [[-0.38554138  0.08228791]]. Action = [[-0.1859918   0.1640011   0.06150234 -0.36334968]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 237 of -1
Current timestep = 238. State = [[-0.38477775  0.08040226]]. Action = [[ 0.168636    0.22340387 -0.193687   -0.35700828]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, True, False]
State prediction error at timestep 238 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 239. State = [[-0.3815469   0.08236479]]. Action = [[ 0.05109584  0.06039858 -0.01318297 -0.45150405]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
State prediction error at timestep 239 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 240. State = [[-0.37914038  0.08377143]]. Action = [[ 0.07181078  0.13490713 -0.14285839  0.9703026 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 240 of -1
Current timestep = 241. State = [[-0.37661895  0.08611   ]]. Action = [[ 0.01898384  0.00504792 -0.01819538  0.19671082]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
State prediction error at timestep 241 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 241 of -1
Current timestep = 242. State = [[-0.37521577  0.08694576]]. Action = [[ 0.23836336  0.12747598 -0.13907701 -0.11390334]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
State prediction error at timestep 242 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 243. State = [[-0.37046814  0.09074678]]. Action = [[-0.03299867  0.12859523 -0.08249046 -0.53168297]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, False, True, False]
State prediction error at timestep 243 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 244. State = [[-0.36609694  0.09614983]]. Action = [[ 0.14478505  0.08587521  0.14303201 -0.64597344]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of -1
Current timestep = 245. State = [[-0.36111262  0.10285023]]. Action = [[-0.1762112   0.2285198  -0.19216593 -0.9241948 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, False, True, False]
State prediction error at timestep 245 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 246. State = [[-0.3591999   0.11083157]]. Action = [[-0.19203128  0.00144756 -0.03009672 -0.3034218 ]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, False, True, False]
State prediction error at timestep 246 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 247. State = [[-0.36317447  0.11820474]]. Action = [[-0.13915597  0.14101234 -0.04200684  0.25970745]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
State prediction error at timestep 247 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 248. State = [[-0.3669547   0.12417173]]. Action = [[ 0.01632679 -0.21325873  0.21063623 -0.96792144]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 249. State = [[-0.36849695  0.12448853]]. Action = [[-0.16530478  0.21409988 -0.1305021  -0.44001675]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
State prediction error at timestep 249 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 249 of 1
Current timestep = 250. State = [[-0.3724815   0.12913252]]. Action = [[ 0.1104826  -0.17844504 -0.06420623 -0.4452498 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, False, True]
State prediction error at timestep 250 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 251. State = [[-0.37256733  0.12864678]]. Action = [[-0.24166697 -0.10282938  0.21881598  0.88727283]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, False, True]
State prediction error at timestep 251 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 251 of -1
Current timestep = 252. State = [[-0.3727446   0.12838127]]. Action = [[-0.19354329 -0.13848615  0.15769619 -0.5102788 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, False, True]
State prediction error at timestep 252 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 253. State = [[-0.37599614  0.12483551]]. Action = [[-0.20095612 -0.23975827  0.12073135 -0.25485945]]. Reward = [0.]
Curr episode timestep = 253
Human Feedback received at timestep 253 of -1
Current timestep = 254. State = [[-0.38349894  0.11637386]]. Action = [[-0.21644953 -0.06082702  0.21727532  0.66474926]]. Reward = [0.]
Curr episode timestep = 254
Human Feedback received at timestep 254 of -1
Current timestep = 255. State = [[-0.39003518  0.1108597 ]]. Action = [[ 0.23767737  0.11728486 -0.13371079  0.6016637 ]]. Reward = [0.]
Curr episode timestep = 255
Current timestep = 256. State = [[-0.3901369   0.11118273]]. Action = [[-0.23080115 -0.19540031  0.08248502 -0.5420805 ]]. Reward = [0.]
Curr episode timestep = 256
Human Feedback received at timestep 256 of -1
Current timestep = 257. State = [[-0.3900294   0.11115919]]. Action = [[-0.15922073  0.20022449  0.19335496 -0.67052275]]. Reward = [0.]
Curr episode timestep = 257
Human Feedback received at timestep 257 of -1
Current timestep = 258. State = [[-0.38995862  0.11102464]]. Action = [[ 0.18218452 -0.14183626 -0.06766424 -0.7581401 ]]. Reward = [0.]
Curr episode timestep = 258
Current timestep = 259. State = [[-0.38863558  0.10974923]]. Action = [[ 0.03629318  0.03814337 -0.16071273  0.3950982 ]]. Reward = [0.]
Curr episode timestep = 259
Human Feedback received at timestep 259 of -1
Current timestep = 260. State = [[-0.38747594  0.10930455]]. Action = [[-0.00516847  0.2196117   0.07976747  0.27910495]]. Reward = [0.]
Curr episode timestep = 260
Current timestep = 261. State = [[-0.38702852  0.10889889]]. Action = [[-0.00069408  0.0674997   0.09957582  0.05479312]]. Reward = [0.]
Curr episode timestep = 261
Current timestep = 262. State = [[-0.38664934  0.10911682]]. Action = [[ 0.1956023  -0.16868858  0.06562638  0.66958857]]. Reward = [0.]
Curr episode timestep = 262
Current timestep = 263. State = [[-0.38398412  0.10835491]]. Action = [[-0.20435177  0.08866784 -0.15330066 -0.02077633]]. Reward = [0.]
Curr episode timestep = 263
Current timestep = 264. State = [[-0.38203055  0.10719519]]. Action = [[-0.24535707 -0.07728796  0.17959875 -0.8303226 ]]. Reward = [0.]
Curr episode timestep = 264
Current timestep = 265. State = [[-0.38114756  0.10624217]]. Action = [[-0.03236915 -0.10689038  0.16954637  0.9417889 ]]. Reward = [0.]
Curr episode timestep = 265
Current timestep = 266. State = [[-0.3805904   0.10423989]]. Action = [[-0.17490402  0.16379389  0.12890917  0.71264005]]. Reward = [0.]
Curr episode timestep = 266
Current timestep = 267. State = [[-0.3802239   0.10357841]]. Action = [[ 0.15738696  0.20758355 -0.20016047 -0.83820504]]. Reward = [0.]
Curr episode timestep = 267
Current timestep = 268. State = [[-0.37723133  0.10542493]]. Action = [[0.14422137 0.11193886 0.09928465 0.15237463]]. Reward = [0.]
Curr episode timestep = 268
Human Feedback received at timestep 268 of -1
Current timestep = 269. State = [[-0.3711797   0.10971625]]. Action = [[-0.18070479 -0.00967386 -0.16732585 -0.54351956]]. Reward = [0.]
Curr episode timestep = 269
Current timestep = 270. State = [[-0.36417532  0.10996962]]. Action = [[ 0.20201916 -0.12773307  0.16538289 -0.27849853]]. Reward = [0.]
Curr episode timestep = 270
Current timestep = 271. State = [[-0.35572192  0.10615316]]. Action = [[-0.07537127  0.01657867  0.10929042 -0.06538367]]. Reward = [0.]
Curr episode timestep = 271
Current timestep = 272. State = [[-0.35073927  0.10423442]]. Action = [[-0.07252625 -0.12147228 -0.01959737 -0.9658831 ]]. Reward = [0.]
Curr episode timestep = 272
Current timestep = 273. State = [[-0.35083786  0.1042685 ]]. Action = [[-0.0150325  -0.09091675  0.18261093  0.8200518 ]]. Reward = [0.]
Curr episode timestep = 273
Human Feedback received at timestep 273 of 1
Current timestep = 274. State = [[-0.35124665  0.10312301]]. Action = [[-0.05677974 -0.20661587  0.23406592 -0.55731195]]. Reward = [0.]
Curr episode timestep = 274
Current timestep = 275. State = [[-0.35156712  0.0977433 ]]. Action = [[-0.00213462  0.14060342  0.14833033 -0.6787388 ]]. Reward = [0.]
Curr episode timestep = 275
Current timestep = 276. State = [[-0.3526757   0.09729782]]. Action = [[-0.20483436  0.19716913  0.06221732 -0.74916553]]. Reward = [0.]
Curr episode timestep = 276
Current timestep = 277. State = [[-0.35527658  0.09955288]]. Action = [[ 0.14065486  0.1029554   0.12657535 -0.8169581 ]]. Reward = [0.]
Curr episode timestep = 277
Current timestep = 278. State = [[-0.35630298  0.10199162]]. Action = [[ 0.2192955   0.14273298 -0.21935919  0.30837107]]. Reward = [0.]
Curr episode timestep = 278
Current timestep = 279. State = [[-0.35193908  0.10360542]]. Action = [[-0.0938786  -0.09673232 -0.13888885 -0.97014385]]. Reward = [0.]
Curr episode timestep = 279
Current timestep = 280. State = [[-0.35091418  0.10391344]]. Action = [[ 0.12960255 -0.04483144 -0.24712951 -0.37031007]]. Reward = [0.]
Curr episode timestep = 280
Current timestep = 281. State = [[-0.34824     0.10388736]]. Action = [[-0.103948    0.13486803 -0.20444152  0.30531192]]. Reward = [0.]
Curr episode timestep = 281
Current timestep = 282. State = [[-0.3472731   0.10476431]]. Action = [[ 0.1501978  -0.15919648  0.00536186  0.00969338]]. Reward = [0.]
Curr episode timestep = 282
Current timestep = 283. State = [[-0.34437642  0.10388723]]. Action = [[ 0.0328227   0.02905643 -0.0986459  -0.3566153 ]]. Reward = [0.]
Curr episode timestep = 283
Current timestep = 284. State = [[-0.3420382   0.10396538]]. Action = [[-0.18495534 -0.22966966  0.12857592  0.63531256]]. Reward = [0.]
Curr episode timestep = 284
Human Feedback received at timestep 284 of 1
Current timestep = 285. State = [[-0.34275448  0.1026483 ]]. Action = [[-0.15928547  0.19637328 -0.13685058  0.7070887 ]]. Reward = [0.]
Curr episode timestep = 285
Current timestep = 286. State = [[-0.34470046  0.10398259]]. Action = [[ 0.1479721   0.09619236 -0.08515137 -0.09258151]]. Reward = [0.]
Curr episode timestep = 286
Current timestep = 287. State = [[-0.34565088  0.10594817]]. Action = [[-0.02334157  0.22466147 -0.2316262   0.8441448 ]]. Reward = [0.]
Curr episode timestep = 287
Current timestep = 288. State = [[-0.34591904  0.10920356]]. Action = [[-0.02134994  0.10286328  0.23935223 -0.8158665 ]]. Reward = [0.]
Curr episode timestep = 288
Current timestep = 289. State = [[-0.3458029   0.11372343]]. Action = [[ 0.07140359 -0.16485894  0.01868817  0.9063889 ]]. Reward = [0.]
Curr episode timestep = 289
Current timestep = 290. State = [[-0.34489584  0.11342202]]. Action = [[-0.06404056 -0.17809093 -0.05521294  0.27234328]]. Reward = [0.]
Curr episode timestep = 290
Current timestep = 291. State = [[-0.3448935   0.11333697]]. Action = [[ 0.04737905  0.24684641  0.099756   -0.31938976]]. Reward = [0.]
Curr episode timestep = 291
Current timestep = 292. State = [[-0.344418    0.11477874]]. Action = [[ 0.02464637  0.22141695 -0.18096994  0.6417415 ]]. Reward = [0.]
Curr episode timestep = 292
Current timestep = 293. State = [[-0.34291464  0.12098522]]. Action = [[ 0.21758065  0.09740534 -0.23303016 -0.01471651]]. Reward = [0.]
Curr episode timestep = 293
Current timestep = 294. State = [[-0.3380362   0.12750705]]. Action = [[ 0.06891602  0.06193322  0.02946734 -0.1520077 ]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, False, True]
State prediction error at timestep 294 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 295. State = [[-0.33182648  0.13464688]]. Action = [[-0.22984336  0.20589355 -0.20261319 -0.50842416]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, False, True]
State prediction error at timestep 295 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 296. State = [[-0.3325283   0.14379814]]. Action = [[-0.20263584 -0.03918138 -0.14633256 -0.03505784]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, False, True]
State prediction error at timestep 296 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 297. State = [[-0.33724326  0.14977153]]. Action = [[ 0.16438144  0.02457735 -0.19723159  0.3172531 ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, False, True]
State prediction error at timestep 297 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 298. State = [[-0.33845642  0.1530805 ]]. Action = [[-0.06929678  0.13707983 -0.1405107  -0.770103  ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, False, True]
State prediction error at timestep 298 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 299. State = [[-0.3401953   0.15796708]]. Action = [[ 0.01117826 -0.2283085  -0.18397263 -0.21776694]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, False, True]
State prediction error at timestep 299 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 299 of 1
Current timestep = 300. State = [[-0.3401253   0.15757741]]. Action = [[-0.13220318  0.14862019  0.02941608 -0.7896537 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, False, True]
State prediction error at timestep 300 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 300 of -1
Current timestep = 301. State = [[-0.3422851   0.15988833]]. Action = [[ 0.2239306  -0.21658076  0.22615093 -0.8759701 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, False, True]
State prediction error at timestep 301 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 302. State = [[-0.33949924  0.15632032]]. Action = [[ 0.15537292 -0.10020715 -0.22544606  0.31165695]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, False, True]
State prediction error at timestep 302 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 303. State = [[-0.33475465  0.15206932]]. Action = [[-0.13451438  0.02818009  0.21607041 -0.843155  ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, False, True]
State prediction error at timestep 303 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 304. State = [[-0.33416054  0.15138002]]. Action = [[-0.2222299   0.12167194 -0.0172206   0.55724406]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, False, True]
State prediction error at timestep 304 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 305. State = [[-0.33675617  0.15353456]]. Action = [[ 0.05674201 -0.16558222 -0.17113534 -0.5613956 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, False, True]
State prediction error at timestep 305 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 306. State = [[-0.33733395  0.15154262]]. Action = [[ 0.24488658  0.23517066 -0.20715137  0.53674984]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, False, True]
State prediction error at timestep 306 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 307. State = [[-0.33475375  0.15318139]]. Action = [[ 0.1651482  -0.02867457  0.04453823  0.3371278 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, False, True]
State prediction error at timestep 307 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 307 of 1
Current timestep = 308. State = [[-0.330025   0.1547369]]. Action = [[-0.06606734  0.09486538 -0.08415869  0.2614212 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, False, True]
State prediction error at timestep 308 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 309. State = [[-0.32728392  0.15802027]]. Action = [[ 0.14729077 -0.03426906  0.20111442 -0.70768815]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, False, True]
State prediction error at timestep 309 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 310. State = [[-0.322847    0.16035067]]. Action = [[-0.16819334  0.19141567  0.23357382  0.858469  ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, False, True]
State prediction error at timestep 310 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 310 of -1
Current timestep = 311. State = [[-0.3233772   0.16524585]]. Action = [[-0.08174351 -0.09943894  0.22597706 -0.34931576]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, False, True]
State prediction error at timestep 311 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 311 of 1
Current timestep = 312. State = [[-0.324428   0.1661487]]. Action = [[ 0.21707547  0.09229112 -0.04767619 -0.86116296]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, False, True]
State prediction error at timestep 312 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 312 of 1
Current timestep = 313. State = [[-0.32211065  0.168322  ]]. Action = [[-0.20865507 -0.00372155 -0.24726333 -0.58856547]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, False, True]
State prediction error at timestep 313 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 314. State = [[-0.32379103  0.17043394]]. Action = [[-0.19278964  0.15531945 -0.09152842  0.6711339 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, False, True]
State prediction error at timestep 314 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 314 of 1
Current timestep = 315. State = [[-0.32983434  0.17724305]]. Action = [[ 0.2114459   0.13714954 -0.11470905 -0.00625336]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, False, True]
State prediction error at timestep 315 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 316. State = [[-0.32998294  0.18233407]]. Action = [[ 0.13185361  0.03708839 -0.2424478   0.30089474]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, False, True]
State prediction error at timestep 316 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 317. State = [[-0.32630357  0.186814  ]]. Action = [[-0.03393225  0.03745264 -0.06319366  0.5353956 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, False, True]
State prediction error at timestep 317 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 317 of 1
Current timestep = 318. State = [[-0.3233808   0.19223249]]. Action = [[-0.1531841   0.0581854  -0.18553358  0.36694396]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, False, True]
State prediction error at timestep 318 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 319. State = [[-0.3251237   0.19762558]]. Action = [[-0.04851051  0.07099789 -0.05226757 -0.51878136]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, False, True]
State prediction error at timestep 319 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 319 of -1
Current timestep = 320. State = [[-0.32911545  0.20290367]]. Action = [[-0.1635831   0.00880393 -0.05363107  0.26704097]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, False, True]
State prediction error at timestep 320 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 320 of -1
Current timestep = 321. State = [[-0.3336923   0.20787345]]. Action = [[ 0.18486607  0.21995491 -0.13100094 -0.9624533 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, False, True]
State prediction error at timestep 321 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 321 of -1
Current timestep = 322. State = [[-0.33423117  0.21333066]]. Action = [[ 0.20798975 -0.11723538  0.11217567  0.78688586]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, False, True]
State prediction error at timestep 322 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 323. State = [[-0.3300939   0.21446736]]. Action = [[-0.17452727 -0.1056537   0.04861367 -0.7823631 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, False, True]
State prediction error at timestep 323 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 323 of -1
Current timestep = 324. State = [[-0.32934484  0.21366882]]. Action = [[ 0.19403893 -0.08216605 -0.08857006  0.48498535]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, False, True]
State prediction error at timestep 324 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 325. State = [[-0.3258451   0.21114272]]. Action = [[ 0.00818264  0.1872127  -0.16676292  0.38142323]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, False, True]
State prediction error at timestep 325 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 325 of -1
Current timestep = 326. State = [[-0.32377034  0.2133088 ]]. Action = [[-0.23740749 -0.2225907  -0.17588724 -0.5301088 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, False, True]
State prediction error at timestep 326 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of -1
Current timestep = 327. State = [[-0.32458848  0.21212186]]. Action = [[-0.13003261  0.04306933  0.17143923  0.9472008 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, False, True]
State prediction error at timestep 327 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 327 of 1
Current timestep = 328. State = [[-0.32707047  0.21212322]]. Action = [[ 0.1103816  -0.1740778  -0.22368906  0.5702913 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, False, True]
State prediction error at timestep 328 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 329. State = [[-0.32669246  0.20784979]]. Action = [[ 0.06482127  0.05732274 -0.02205086 -0.9287522 ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, False, True]
State prediction error at timestep 329 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 329 of -1
Current timestep = 330. State = [[-0.32594395  0.20581792]]. Action = [[-0.21494566 -0.24217944  0.0763973  -0.844676  ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, False, True]
State prediction error at timestep 330 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 330 of -1
Current timestep = 331. State = [[-0.3289428   0.19814187]]. Action = [[ 0.17151845 -0.15197307 -0.22879656 -0.70248437]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, False, True]
State prediction error at timestep 331 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 332. State = [[-0.32709563  0.18790777]]. Action = [[-0.080475   -0.20083219 -0.13425072 -0.38626933]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, False, True]
State prediction error at timestep 332 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 332 of -1
Current timestep = 333. State = [[-0.32734835  0.17729372]]. Action = [[ 0.08643666 -0.04858692  0.03277889 -0.23483276]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, False, True]
State prediction error at timestep 333 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 333 of -1
Current timestep = 334. State = [[-0.32723254  0.16887993]]. Action = [[-0.07172042 -0.12985358 -0.18485443  0.8132026 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, False, True]
State prediction error at timestep 334 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 335. State = [[-0.32795215  0.16007936]]. Action = [[ 0.08427635 -0.19404237  0.18925804  0.7266507 ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, False, True]
State prediction error at timestep 335 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 335 of -1
Current timestep = 336. State = [[-0.32693762  0.14821364]]. Action = [[ 0.21026278 -0.07663456 -0.12364964  0.46453357]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, False, True]
State prediction error at timestep 336 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 337. State = [[-0.32056564  0.13997538]]. Action = [[-0.10841525  0.19441137  0.20247293 -0.23388201]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, False, True]
State prediction error at timestep 337 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 337 of 1
Current timestep = 338. State = [[-0.3196724   0.13912685]]. Action = [[-0.23536357 -0.20013306  0.1007981   0.35264456]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, False, True]
State prediction error at timestep 338 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 338 of 1
Current timestep = 339. State = [[-0.32326913  0.13501632]]. Action = [[-0.08081934  0.04381093 -0.1101546  -0.95246947]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, False, True]
State prediction error at timestep 339 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 339 of 1
Current timestep = 340. State = [[-0.32799315  0.13322744]]. Action = [[-0.15835997  0.18174583 -0.03475891 -0.45699906]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, False, True]
State prediction error at timestep 340 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 340 of 1
Current timestep = 341. State = [[-0.33523282  0.1370046 ]]. Action = [[ 0.11170441 -0.04641131 -0.10093491  0.8334992 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, False, True]
State prediction error at timestep 341 is tensor(7.7438e-05, grad_fn=<MseLossBackward0>)
Current timestep = 342. State = [[-0.33713427  0.13754281]]. Action = [[ 0.11395651 -0.04565747  0.02572197 -0.48975587]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, False, True]
State prediction error at timestep 342 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 342 of 1
Current timestep = 343. State = [[-0.33654684  0.13673252]]. Action = [[-0.10317545 -0.0022015   0.19131705 -0.82374996]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, False, True]
State prediction error at timestep 343 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 343 of 1
Current timestep = 344. State = [[-0.33746624  0.13658795]]. Action = [[-0.10788175  0.21954101 -0.14043191 -0.22873825]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, False, True]
State prediction error at timestep 344 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 344 of 1
Current timestep = 345. State = [[-0.3412309   0.14150608]]. Action = [[ 0.215105    0.06587082 -0.15726611  0.20218122]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, False, True]
State prediction error at timestep 345 is tensor(5.8850e-06, grad_fn=<MseLossBackward0>)
Current timestep = 346. State = [[-0.3412194   0.14468636]]. Action = [[-0.06346905  0.1723009  -0.03247654 -0.5626429 ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, False, True]
State prediction error at timestep 346 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 347. State = [[-0.34268084  0.15094027]]. Action = [[-0.07097423 -0.00347884 -0.09576072  0.17158294]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, False, True]
State prediction error at timestep 347 is tensor(7.1467e-05, grad_fn=<MseLossBackward0>)
Current timestep = 348. State = [[-0.34571347  0.15532528]]. Action = [[ 0.10817975  0.07190359 -0.03329819 -0.8919467 ]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, False, True]
State prediction error at timestep 348 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 349. State = [[-0.34528556  0.15881443]]. Action = [[ 0.23309472  0.06996471 -0.2274793   0.37980568]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, False, True]
State prediction error at timestep 349 is tensor(6.2314e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 349 of -1
Current timestep = 350. State = [[-0.33965886  0.16333614]]. Action = [[0.17651165 0.17382479 0.00691634 0.864197  ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, False, True]
State prediction error at timestep 350 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 351. State = [[-0.33113414  0.17181806]]. Action = [[ 0.12889624  0.24602494  0.03087071 -0.41556883]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, False, True]
State prediction error at timestep 351 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 351 of -1
Current timestep = 352. State = [[-0.3218644   0.18423581]]. Action = [[ 0.00561213  0.10041615 -0.23749848  0.10770261]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, False, True]
State prediction error at timestep 352 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 352 of -1
Current timestep = 353. State = [[-0.31517807  0.19552922]]. Action = [[-0.09264404 -0.00986591  0.08675587  0.9147229 ]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, False, True]
State prediction error at timestep 353 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 354. State = [[-0.3134714   0.20198663]]. Action = [[-0.19949414  0.06448919  0.14297998 -0.19570035]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, False, True]
State prediction error at timestep 354 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 354 of 1
Current timestep = 355. State = [[-0.31689292  0.20771487]]. Action = [[-0.2274261  -0.13644145  0.0346022   0.9400543 ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, False, True]
State prediction error at timestep 355 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 356. State = [[-0.3213744   0.20989706]]. Action = [[0.2196669  0.21895263 0.03764367 0.0170747 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, False, True]
State prediction error at timestep 356 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 357. State = [[-0.3223897   0.21363875]]. Action = [[0.19023556 0.04939783 0.17900285 0.82288957]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, False, True]
State prediction error at timestep 357 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 357 of 1
Current timestep = 358. State = [[-0.31929633  0.21746081]]. Action = [[-0.02331953 -0.13301426 -0.01191515 -0.31095755]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, False, True]
State prediction error at timestep 358 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 359. State = [[-0.31804934  0.21659975]]. Action = [[-0.01662509  0.12673736  0.15158242 -0.16643757]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, False, True]
State prediction error at timestep 359 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 359 of -1
Current timestep = 360. State = [[-0.31814352  0.21861668]]. Action = [[-0.17028734  0.0661875  -0.12350595  0.27420628]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, False, True]
State prediction error at timestep 360 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 360 of -1
Current timestep = 361. State = [[-0.32143643  0.22219746]]. Action = [[ 0.00450999 -0.15616176  0.04968065 -0.8998309 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, False, True]
State prediction error at timestep 361 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 362. State = [[-0.32153925  0.22184618]]. Action = [[-0.00161415 -0.2398339  -0.06877965 -0.43377817]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, False, True]
State prediction error at timestep 362 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 362 of -1
Current timestep = 363. State = [[-0.32111344  0.21577433]]. Action = [[-0.05201697 -0.08967572 -0.05379769  0.49031055]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, False, True]
State prediction error at timestep 363 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 363 of -1
Current timestep = 364. State = [[-0.32109058  0.21050382]]. Action = [[ 0.00894654 -0.04482567 -0.09331763  0.75815654]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, False, True]
State prediction error at timestep 364 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 365. State = [[-0.3214089   0.20586433]]. Action = [[ 0.01156208  0.05145812  0.11723822 -0.47145617]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, False, True]
State prediction error at timestep 365 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 366. State = [[-0.32157326  0.20386957]]. Action = [[-0.04087155  0.05848432 -0.02671148  0.7213043 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, False, True]
State prediction error at timestep 366 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 366 of -1
Current timestep = 367. State = [[-0.32283497  0.20409471]]. Action = [[-0.23041634  0.23153627 -0.17573701  0.2952056 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, False, True]
State prediction error at timestep 367 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 368. State = [[-0.32865086  0.20964761]]. Action = [[ 0.213049   -0.16404487  0.1503874  -0.25210834]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, False, True]
State prediction error at timestep 368 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of 1
Current timestep = 369. State = [[-0.3285609   0.20929398]]. Action = [[-0.1668477   0.08886161  0.05547139  0.83387053]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, False, True]
State prediction error at timestep 369 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 369 of 1
Current timestep = 370. State = [[-0.3314704   0.21128538]]. Action = [[ 0.06130362  0.10543922 -0.18073137  0.04482031]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, False, True]
State prediction error at timestep 370 is tensor(8.5087e-05, grad_fn=<MseLossBackward0>)
Current timestep = 371. State = [[-0.3336435   0.21417482]]. Action = [[ 0.24312851  0.14822519  0.08865058 -0.75617874]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, False, True]
State prediction error at timestep 371 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 372. State = [[-0.33037063  0.21913406]]. Action = [[ 0.1575554   0.19894078 -0.10961911  0.17603648]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, False, True]
State prediction error at timestep 372 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 372 of -1
Current timestep = 373. State = [[-0.32403713  0.22764358]]. Action = [[-0.10324842 -0.19982006  0.24629092 -0.23768306]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, False, False, True]
State prediction error at timestep 373 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 374. State = [[-0.32328457  0.22828257]]. Action = [[-0.20899652 -0.05599637 -0.19981529 -0.78138894]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, False, False, True]
State prediction error at timestep 374 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 374 of -1
Current timestep = 375. State = [[-0.3253576   0.22879261]]. Action = [[ 0.04624611  0.21200049 -0.05545488  0.05010402]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, False, False, True]
State prediction error at timestep 375 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of -1
Current timestep = 376. State = [[-0.32797405  0.23235697]]. Action = [[-0.01258884 -0.1554537   0.05893749  0.47575772]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, False, False, True]
State prediction error at timestep 376 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 377. State = [[-0.32813352  0.23267615]]. Action = [[-0.09046136  0.13442639 -0.07989851 -0.6432939 ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, False, False, True]
State prediction error at timestep 377 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of -1
Current timestep = 378. State = [[-0.33085123  0.23505497]]. Action = [[-0.14208177 -0.23318778 -0.17376363 -0.7069876 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, False, False, True]
State prediction error at timestep 378 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 378 of -1
Current timestep = 379. State = [[-0.3345237   0.23186873]]. Action = [[-0.05240558 -0.20799224  0.08197182  0.6981158 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, False, False, True]
State prediction error at timestep 379 is tensor(7.3523e-05, grad_fn=<MseLossBackward0>)
Current timestep = 380. State = [[-0.33681807  0.22556025]]. Action = [[0.18182719 0.14634189 0.20473576 0.49295664]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, False, False, True]
State prediction error at timestep 380 is tensor(3.3538e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 380 of -1
Current timestep = 381. State = [[-0.3356225   0.22503668]]. Action = [[0.21512526 0.20052916 0.24162647 0.6020709 ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, False, False, True]
State prediction error at timestep 381 is tensor(6.4707e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 381 of 1
Current timestep = 382. State = [[-0.3321867   0.22803697]]. Action = [[-0.18688445 -0.08311117  0.1056309   0.72790873]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, False, False, True]
State prediction error at timestep 382 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 383. State = [[-0.33293673  0.22884405]]. Action = [[ 0.07805824  0.05099535  0.2036514  -0.3490975 ]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, False, False, True]
State prediction error at timestep 383 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 384. State = [[-0.33281782  0.22874513]]. Action = [[-0.13455147  0.01576152 -0.14396136 -0.03295302]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, False, False, True]
State prediction error at timestep 384 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 385. State = [[-0.334215    0.23016126]]. Action = [[-0.21574917 -0.08828166 -0.01762241  0.8672222 ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, False, False, True]
State prediction error at timestep 385 is tensor(5.4255e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 385 of -1
Current timestep = 386. State = [[-0.3382565   0.23111211]]. Action = [[ 0.20883545  0.16295642 -0.16272362  0.10963106]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, False, False, True]
State prediction error at timestep 386 is tensor(1.0734e-05, grad_fn=<MseLossBackward0>)
Current timestep = 387. State = [[-0.33912244  0.23308854]]. Action = [[-0.1072745  -0.03815639  0.23093224 -0.82866263]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, False, False, True]
State prediction error at timestep 387 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 388. State = [[-0.340716    0.23461719]]. Action = [[-0.15428403  0.00776711  0.21455687  0.9335668 ]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, False, False, True]
State prediction error at timestep 388 is tensor(8.7855e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 388 of -1
Current timestep = 389. State = [[-0.34475338  0.23745015]]. Action = [[ 0.21234664  0.08827528 -0.00329497  0.06677198]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, False, False, True]
State prediction error at timestep 389 is tensor(2.2882e-05, grad_fn=<MseLossBackward0>)
Current timestep = 390. State = [[-0.34432736  0.23854929]]. Action = [[-0.0321473  -0.0414381  -0.16714808 -0.6815593 ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, False, False, True]
State prediction error at timestep 390 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 390 of -1
Current timestep = 391. State = [[-0.3443454   0.23865823]]. Action = [[ 0.08002639  0.07897663 -0.24208833 -0.80479527]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, False, False, True]
State prediction error at timestep 391 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 392. State = [[-0.34357047  0.23999842]]. Action = [[-0.13364165 -0.13836175  0.11825702  0.3796773 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, False, False, True]
State prediction error at timestep 392 is tensor(5.0853e-05, grad_fn=<MseLossBackward0>)
Current timestep = 393. State = [[-0.34401512  0.23927853]]. Action = [[ 0.05862162 -0.10212684  0.04088989 -0.89793855]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, False, False, True]
State prediction error at timestep 393 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 393 of -1
Current timestep = 394. State = [[-0.34287637  0.23689167]]. Action = [[-0.1887121  -0.08112499  0.19169772 -0.83244884]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, False, False, True]
State prediction error at timestep 394 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 395. State = [[-0.3453815   0.23388062]]. Action = [[-0.11057673 -0.00557619 -0.12218769 -0.8965118 ]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, False, False, True]
State prediction error at timestep 395 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 395 of -1
Current timestep = 396. State = [[-0.34952465  0.23123044]]. Action = [[-0.0795857  -0.06844272 -0.05263111 -0.82541823]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, False, False, True]
State prediction error at timestep 396 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 397. State = [[-0.3550639   0.22686043]]. Action = [[-0.2017868  -0.2205778  -0.21382998  0.02626657]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, False, False, True]
State prediction error at timestep 397 is tensor(3.4312e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 397 of -1
Current timestep = 398. State = [[-0.3628405   0.21917671]]. Action = [[ 0.10817897  0.18942809 -0.16004717  0.5156088 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, False, False, True]
State prediction error at timestep 398 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 399. State = [[-0.3662477   0.21866803]]. Action = [[ 0.01780277 -0.16528715 -0.12815042  0.12093103]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, False, False, True]
State prediction error at timestep 399 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 399 of -1
Current timestep = 400. State = [[-0.36766675  0.21498773]]. Action = [[-0.21953607  0.05137733  0.08988422 -0.37101138]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, False, False, True]
State prediction error at timestep 400 is tensor(5.8959e-05, grad_fn=<MseLossBackward0>)
Current timestep = 401. State = [[-0.37287015  0.21335462]]. Action = [[ 0.17463231 -0.17772615  0.18196726 -0.6974298 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, False, False, True]
State prediction error at timestep 401 is tensor(3.3053e-05, grad_fn=<MseLossBackward0>)
Current timestep = 402. State = [[-0.37386647  0.2076879 ]]. Action = [[ 0.07688612  0.24421275  0.18275172 -0.28871465]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, False, False, True]
State prediction error at timestep 402 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 402 of -1
Current timestep = 403. State = [[-0.37403753  0.20889479]]. Action = [[ 0.06493533  0.22796744  0.14200747 -0.4714136 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, False, False, True]
State prediction error at timestep 403 is tensor(6.4123e-05, grad_fn=<MseLossBackward0>)
Current timestep = 404. State = [[-0.37290612  0.21470614]]. Action = [[-0.17353617 -0.00991827 -0.19955015  0.9693556 ]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, False, False, True]
State prediction error at timestep 404 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 404 of -1
Current timestep = 405. State = [[-0.37628427  0.2193339 ]]. Action = [[ 0.12390319  0.11010844  0.23952124 -0.6848007 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, False, False, True]
State prediction error at timestep 405 is tensor(8.5927e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of -1
Current timestep = 406. State = [[-0.37694556  0.22346435]]. Action = [[-0.1011636  -0.0192939   0.2062251   0.37254715]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, False, False, True]
State prediction error at timestep 406 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 407. State = [[-0.3791271   0.22588538]]. Action = [[-0.16815828 -0.03555724 -0.16962202 -0.29530936]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, False, False, True]
State prediction error at timestep 407 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 407 of -1
Current timestep = 408. State = [[-0.38072255  0.22740723]]. Action = [[-0.22770995  0.15212771 -0.06289272  0.8480973 ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, False, False, True]
State prediction error at timestep 408 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 409. State = [[-0.3814236   0.22801192]]. Action = [[-0.22627638  0.08201653 -0.12650824  0.28117132]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, False, False, True]
State prediction error at timestep 409 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 409 of -1
Current timestep = 410. State = [[-0.38172606  0.22870621]]. Action = [[ 0.06476483  0.15813798 -0.08806843  0.06814778]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, False, False, True]
State prediction error at timestep 410 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 411. State = [[-0.38015473  0.23251261]]. Action = [[ 0.18894762  0.05471945 -0.10604514  0.29738176]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, False, False, True]
State prediction error at timestep 411 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 411 of -1
Current timestep = 412. State = [[-0.37552533  0.23687297]]. Action = [[ 0.24587476 -0.06153616  0.14113325 -0.510307  ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, False, False, True]
State prediction error at timestep 412 is tensor(1.1060e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 412 of -1
Current timestep = 413. State = [[-0.36775926  0.23809522]]. Action = [[ 0.21944234 -0.1989061   0.07444859 -0.79297525]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, False, False, True]
State prediction error at timestep 413 is tensor(4.8480e-06, grad_fn=<MseLossBackward0>)
Current timestep = 414. State = [[-0.3582246  0.2331253]]. Action = [[-0.22984867 -0.03885721 -0.17498659 -0.06567645]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, False, False, True]
State prediction error at timestep 414 is tensor(1.2465e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 414 of -1
Current timestep = 415. State = [[-0.35649058  0.23114236]]. Action = [[-0.17524351  0.19245255 -0.24021661 -0.62672997]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, False, False, True]
State prediction error at timestep 415 is tensor(6.8834e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of -1
Current timestep = 416. State = [[-0.35869324  0.23332685]]. Action = [[ 0.15244642 -0.22895339 -0.06888224 -0.8078922 ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, False, False, True]
State prediction error at timestep 416 is tensor(2.9768e-05, grad_fn=<MseLossBackward0>)
Current timestep = 417. State = [[-0.35568523  0.2305052 ]]. Action = [[ 0.17413071  0.14640784  0.02067372 -0.9106388 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, False, False, True]
State prediction error at timestep 417 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 417 of 1
Current timestep = 418. State = [[-0.35202748  0.23042665]]. Action = [[-0.11623877 -0.23190598 -0.14044799 -0.76658994]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, False, False, True]
State prediction error at timestep 418 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 419. State = [[-0.3495631   0.22684091]]. Action = [[ 0.20116192  0.13981706 -0.01871999 -0.4270873 ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, False, False, True]
State prediction error at timestep 419 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 419 of 1
Current timestep = 420. State = [[-0.34565744  0.22584552]]. Action = [[-0.11403161 -0.09062228 -0.04504429 -0.48247927]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, False, False, True]
State prediction error at timestep 420 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 421. State = [[-0.34445164  0.22442013]]. Action = [[-0.04600936 -0.23436448  0.13878375 -0.70209384]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, False, False, True]
State prediction error at timestep 421 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 421 of -1
Current timestep = 422. State = [[-0.34296474  0.21818726]]. Action = [[ 0.20937526 -0.06408781  0.06206644  0.64778256]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 422 is [True, False, False, False, False, True]
State prediction error at timestep 422 is tensor(6.0650e-05, grad_fn=<MseLossBackward0>)
Current timestep = 423. State = [[-0.33811855  0.21096557]]. Action = [[ 0.0207864  -0.21217135 -0.18974814 -0.23646224]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 423 is [True, False, False, False, False, True]
State prediction error at timestep 423 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 423 of -1
Current timestep = 424. State = [[-0.33192608  0.20119898]]. Action = [[ 0.23018807  0.05410451  0.02896708 -0.4704976 ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 424 is [True, False, False, False, False, True]
State prediction error at timestep 424 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 424 of -1
Current timestep = 425. State = [[-0.32555377  0.19530015]]. Action = [[-0.17953211 -0.02601527 -0.04681021 -0.82029986]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 425 is [True, False, False, False, False, True]
State prediction error at timestep 425 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 426. State = [[-0.3243998   0.19341561]]. Action = [[-0.18332054  0.17726526 -0.19160898  0.81431556]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 426 is [True, False, False, False, False, True]
State prediction error at timestep 426 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 426 of 1
Current timestep = 427. State = [[-0.3276552   0.19626336]]. Action = [[-0.23847695 -0.03772378  0.16489246  0.07033265]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 427 is [True, False, False, False, False, True]
State prediction error at timestep 427 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 427 of 1
Current timestep = 428. State = [[-0.33420148  0.19851528]]. Action = [[0.0865736  0.0025396  0.07411769 0.39269757]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 428 is [True, False, False, False, False, True]
State prediction error at timestep 428 is tensor(4.1285e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 428 of 1
Current timestep = 429. State = [[-0.3361496  0.1996116]]. Action = [[ 0.24327254  0.09237444 -0.12173179  0.10113847]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 429 is [True, False, False, False, False, True]
State prediction error at timestep 429 is tensor(3.5666e-05, grad_fn=<MseLossBackward0>)
Current timestep = 430. State = [[-0.3344802   0.20032047]]. Action = [[ 0.10728085 -0.07262048  0.14193842  0.23480988]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 430 is [True, False, False, False, False, True]
State prediction error at timestep 430 is tensor(8.4622e-05, grad_fn=<MseLossBackward0>)
Current timestep = 431. State = [[-0.33227727  0.19910839]]. Action = [[-0.1420802  -0.13624658  0.02259061 -0.66820014]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 431 is [True, False, False, False, False, True]
State prediction error at timestep 431 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 431 of 1
Current timestep = 432. State = [[-0.33129987  0.19697925]]. Action = [[0.05242738 0.20813757 0.21842197 0.0391171 ]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 432 is [True, False, False, False, False, True]
State prediction error at timestep 432 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 433. State = [[-0.33110544  0.19915201]]. Action = [[ 0.21479824  0.23134819 -0.14086215 -0.18178415]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 433 is [True, False, False, False, False, True]
State prediction error at timestep 433 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 434. State = [[-0.3263283   0.20681317]]. Action = [[ 0.11928517  0.15844607 -0.02021864 -0.11219603]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 434 is [True, False, False, False, False, True]
State prediction error at timestep 434 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 434 of 1
Current timestep = 435. State = [[-0.31987926  0.21668197]]. Action = [[-0.21374796 -0.05522251 -0.1807853   0.75845814]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 435 is [True, False, False, False, False, True]
State prediction error at timestep 435 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 436. State = [[-0.31953534  0.22116971]]. Action = [[0.18093914 0.07104596 0.18522713 0.3420086 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 436 is [True, False, False, False, False, True]
State prediction error at timestep 436 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 436 of 1
Current timestep = 437. State = [[-0.31696102  0.22525096]]. Action = [[ 0.22390401  0.13748544 -0.22602235  0.22313583]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 437 is [True, False, False, False, False, True]
State prediction error at timestep 437 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 438. State = [[-0.31162634  0.23072673]]. Action = [[-0.15321705  0.23405093 -0.0143849  -0.8950347 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 438 is [True, False, False, False, False, True]
State prediction error at timestep 438 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 439. State = [[-0.309323    0.24290161]]. Action = [[-0.21619923  0.19064876 -0.02638273  0.06217158]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 439 is [True, False, False, False, False, True]
State prediction error at timestep 439 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 440. State = [[-0.31181148  0.2542134 ]]. Action = [[-0.18138053  0.06527138 -0.0606709   0.36512828]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 440 is [True, False, False, False, False, True]
State prediction error at timestep 440 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 440 of 1
Current timestep = 441. State = [[-0.3187272   0.26423877]]. Action = [[ 0.0819205   0.22104394  0.1945796  -0.25818193]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 441 is [True, False, False, False, False, True]
State prediction error at timestep 441 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 442. State = [[-0.32277584  0.2747999 ]]. Action = [[-0.17275201  0.09653765 -0.09453663  0.03359985]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 442 is [True, False, False, False, False, True]
State prediction error at timestep 442 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 442 of -1
Current timestep = 443. State = [[-0.3278082   0.28486928]]. Action = [[-0.05226758 -0.04562393 -0.00350538  0.44489217]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 443 is [True, False, False, False, False, True]
State prediction error at timestep 443 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 444. State = [[-0.33315936  0.28983995]]. Action = [[ 0.07307035  0.02697885 -0.08654585 -0.25591528]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 444 is [True, False, False, False, False, True]
State prediction error at timestep 444 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 444 of -1
Current timestep = 445. State = [[-0.33516023  0.29303265]]. Action = [[-0.19378874  0.20311663  0.11828542  0.9056426 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 445 is [True, False, False, False, False, True]
State prediction error at timestep 445 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 445 of -1
Current timestep = 446. State = [[-0.34030157  0.3006356 ]]. Action = [[ 0.05531892 -0.13260667 -0.02415448 -0.40722454]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 446 is [True, False, False, False, False, True]
State prediction error at timestep 446 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 446 of -1
Current timestep = 447. State = [[-0.34248853  0.30257043]]. Action = [[0.06073329 0.10280776 0.153548   0.7046733 ]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 447 is [True, False, False, False, False, True]
State prediction error at timestep 447 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 448. State = [[-0.34284005  0.30478176]]. Action = [[ 0.15147322  0.19416657  0.08990833 -0.09113961]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 448 is [True, False, False, False, False, True]
State prediction error at timestep 448 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 448 of -1
Current timestep = 449. State = [[-0.34260696  0.30616358]]. Action = [[0.19724143 0.06285149 0.07833153 0.6989211 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 449 is [True, False, False, False, False, True]
State prediction error at timestep 449 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 449 of -1
Current timestep = 450. State = [[-0.33971915  0.30860096]]. Action = [[-0.08323723 -0.0890851  -0.20010333 -0.9074334 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 450 is [True, False, False, False, False, True]
State prediction error at timestep 450 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 451. State = [[-0.33876488  0.3089007 ]]. Action = [[ 0.08618301 -0.08147517 -0.06025229 -0.2189027 ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 451 is [True, False, False, False, False, True]
State prediction error at timestep 451 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 451 of -1
Current timestep = 452. State = [[-0.33652508  0.30747882]]. Action = [[ 0.16309455  0.05626076 -0.1862342  -0.20632589]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 452 is [True, False, False, False, False, True]
State prediction error at timestep 452 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 452 of -1
Current timestep = 453. State = [[-0.33166552  0.30741262]]. Action = [[-0.23072633  0.1889317  -0.02653413  0.4110031 ]]. Reward = [0.]
Curr episode timestep = 453
Current timestep = 454. State = [[-0.3280754   0.30742732]]. Action = [[-0.04558516 -0.22823974 -0.14567532  0.5617399 ]]. Reward = [0.]
Curr episode timestep = 454
Human Feedback received at timestep 454 of -1
Current timestep = 455. State = [[-0.32435268  0.303672  ]]. Action = [[ 0.1574089   0.03733021 -0.07444543  0.56482744]]. Reward = [0.]
Curr episode timestep = 455
Human Feedback received at timestep 455 of -1
Current timestep = 456. State = [[-0.32035205  0.3006063 ]]. Action = [[-0.03215913  0.05029914  0.07103834 -0.19667685]]. Reward = [0.]
Curr episode timestep = 456
Current timestep = 457. State = [[-0.3187343  0.2999306]]. Action = [[ 0.10718191  0.11912337 -0.17454527  0.83506596]]. Reward = [0.]
Curr episode timestep = 457
Human Feedback received at timestep 457 of -1
Current timestep = 458. State = [[-0.3162424   0.30161718]]. Action = [[ 0.00413445  0.02287617 -0.12150472  0.73408794]]. Reward = [0.]
Curr episode timestep = 458
Human Feedback received at timestep 458 of -1
Current timestep = 459. State = [[-0.31364796  0.30395952]]. Action = [[ 0.20868778  0.20622146 -0.14495432 -0.0868293 ]]. Reward = [0.]
Curr episode timestep = 459
Current timestep = 460. State = [[-0.3118763   0.30534747]]. Action = [[-0.14485757 -0.17021282  0.04112056  0.45858884]]. Reward = [0.]
Curr episode timestep = 460
Current timestep = 461. State = [[-0.31187123  0.30448675]]. Action = [[-0.22689764 -0.09217398  0.0249117  -0.59041727]]. Reward = [0.]
Curr episode timestep = 461
Current timestep = 462. State = [[-0.31462     0.30198595]]. Action = [[-0.14996524 -0.23345715 -0.0877434   0.58876574]]. Reward = [0.]
Curr episode timestep = 462
Current timestep = 463. State = [[-0.31827998  0.29562694]]. Action = [[-0.10156289  0.09015253  0.19161499  0.3256476 ]]. Reward = [0.]
Curr episode timestep = 463
Current timestep = 464. State = [[-0.32482183  0.29275328]]. Action = [[ 0.2019622   0.09738499 -0.20582086 -0.33386987]]. Reward = [0.]
Curr episode timestep = 464
Current timestep = 465. State = [[-0.32524657  0.2926893 ]]. Action = [[ 0.24100947 -0.03889042  0.19140679 -0.23937744]]. Reward = [0.]
Curr episode timestep = 465
Current timestep = 466. State = [[-0.32177514  0.29003754]]. Action = [[ 0.2470203  -0.07723311 -0.07177301  0.8394693 ]]. Reward = [0.]
Curr episode timestep = 466
Current timestep = 467. State = [[-0.31553152  0.2857804 ]]. Action = [[-0.07761616  0.09043056 -0.03031327 -0.03553838]]. Reward = [0.]
Curr episode timestep = 467
Current timestep = 468. State = [[-0.3119416   0.28521362]]. Action = [[-0.00689894 -0.16020535  0.04617763 -0.84136784]]. Reward = [0.]
Curr episode timestep = 468
Current timestep = 469. State = [[-0.3087366  0.2822439]]. Action = [[0.15337664 0.1016809  0.21238473 0.04363656]]. Reward = [0.]
Curr episode timestep = 469
Current timestep = 470. State = [[-0.3053908  0.2812469]]. Action = [[ 0.19607699  0.10414097 -0.00878233 -0.32978797]]. Reward = [0.]
Curr episode timestep = 470
Current timestep = 471. State = [[-0.2989488   0.28243837]]. Action = [[ 0.0235436  -0.1534239  -0.11242671 -0.28172863]]. Reward = [0.]
Curr episode timestep = 471
Current timestep = 472. State = [[-0.2927273   0.28053588]]. Action = [[ 0.18143272  0.01001063 -0.16883442  0.93304706]]. Reward = [0.]
Curr episode timestep = 472
Current timestep = 473. State = [[-0.2844493   0.27924994]]. Action = [[-0.19677688 -0.03243175 -0.05329892 -0.6849434 ]]. Reward = [0.]
Curr episode timestep = 473
Current timestep = 474. State = [[-0.28229287  0.27798355]]. Action = [[ 0.20418793 -0.03695984 -0.04981267  0.52587855]]. Reward = [0.]
Curr episode timestep = 474
Current timestep = 475. State = [[-0.27803156  0.27542526]]. Action = [[2.2913331e-01 2.1818930e-01 1.6635656e-04 9.1732836e-01]]. Reward = [0.]
Curr episode timestep = 475
Current timestep = 476. State = [[-0.27073717  0.27845916]]. Action = [[-0.04126947 -0.2007571  -0.22338043 -0.31681383]]. Reward = [0.]
Curr episode timestep = 476
Current timestep = 477. State = [[-0.2650123   0.27732322]]. Action = [[-0.00838138 -0.05460918 -0.06763229 -0.5513605 ]]. Reward = [0.]
Curr episode timestep = 477
Current timestep = 478. State = [[-0.26049235  0.27488998]]. Action = [[ 0.05607998 -0.08001184 -0.13269499 -0.21839881]]. Reward = [0.]
Curr episode timestep = 478
Current timestep = 479. State = [[-0.25651553  0.2711796 ]]. Action = [[ 0.00468013 -0.00310071  0.11175361  0.22052777]]. Reward = [0.]
Curr episode timestep = 479
Current timestep = 480. State = [[-0.2543052   0.26921552]]. Action = [[ 0.00692576  0.1054723  -0.10153565  0.51352894]]. Reward = [0.]
Curr episode timestep = 480
Current timestep = 481. State = [[-0.2533208   0.27088657]]. Action = [[-0.16893412  0.0741483  -0.06630787  0.9370129 ]]. Reward = [0.]
Curr episode timestep = 481
Current timestep = 482. State = [[-0.25453866  0.27402607]]. Action = [[ 0.21083367  0.09498903 -0.0536297  -0.5588825 ]]. Reward = [0.]
Curr episode timestep = 482
Current timestep = 483. State = [[-0.25231084  0.27755454]]. Action = [[ 0.23750943  0.2249021  -0.22490527 -0.87296474]]. Reward = [0.]
Curr episode timestep = 483
Current timestep = 484. State = [[-0.2466425   0.28503084]]. Action = [[-0.18069562 -0.09084994 -0.19217958  0.15186286]]. Reward = [0.]
Curr episode timestep = 484
Current timestep = 485. State = [[-0.24474171  0.28915828]]. Action = [[-0.04506223 -0.01243438 -0.13926134  0.95438194]]. Reward = [0.]
Curr episode timestep = 485
Current timestep = 486. State = [[-0.24416809  0.2907933 ]]. Action = [[ 0.10652584 -0.06095336  0.11862987  0.47260714]]. Reward = [0.]
Curr episode timestep = 486
Current timestep = 487. State = [[-0.24255876  0.2898634 ]]. Action = [[-0.2224054  -0.16308439  0.0184263   0.95808816]]. Reward = [0.]
Curr episode timestep = 487
Current timestep = 488. State = [[-0.24220373  0.28775585]]. Action = [[ 0.22376916 -0.12167203  0.03482991  0.6951617 ]]. Reward = [0.]
Curr episode timestep = 488
Current timestep = 489. State = [[-0.23829854  0.28273737]]. Action = [[ 0.03078952 -0.12481618  0.12351543  0.6038194 ]]. Reward = [0.]
Curr episode timestep = 489
Current timestep = 490. State = [[-0.2341234   0.27685988]]. Action = [[ 0.07521164  0.16070801 -0.13157104 -0.6293773 ]]. Reward = [0.]
Curr episode timestep = 490
Current timestep = 491. State = [[-0.23177813  0.2754331 ]]. Action = [[ 0.20300817  0.16161227 -0.06396328  0.87593985]]. Reward = [0.]
Curr episode timestep = 491
Current timestep = 492. State = [[-0.22775283  0.27765882]]. Action = [[-0.17819868 -0.20661116 -0.22528444  0.53942156]]. Reward = [0.]
Curr episode timestep = 492
Current timestep = 493. State = [[-0.22608842  0.27553302]]. Action = [[-0.21006736 -0.13080172  0.078825    0.5819564 ]]. Reward = [0.]
Curr episode timestep = 493
Current timestep = 494. State = [[-0.22649397  0.2724694 ]]. Action = [[ 0.0609265  -0.10304591  0.06444594  0.2789178 ]]. Reward = [0.]
Curr episode timestep = 494
Current timestep = 495. State = [[-0.22556639  0.26840332]]. Action = [[ 0.14941147  0.16495624  0.12557036 -0.9520291 ]]. Reward = [0.]
Curr episode timestep = 495
Current timestep = 496. State = [[-0.22460392  0.26762146]]. Action = [[-0.15324055 -0.0465385  -0.1485728   0.11129022]]. Reward = [0.]
Curr episode timestep = 496
Current timestep = 497. State = [[-0.22496401  0.2674948 ]]. Action = [[-0.18968399 -0.1647562   0.20680755 -0.5728207 ]]. Reward = [0.]
Curr episode timestep = 497
Current timestep = 498. State = [[-0.22716033  0.26433912]]. Action = [[0.13434619 0.17494336 0.01993215 0.3338697 ]]. Reward = [0.]
Curr episode timestep = 498
Current timestep = 499. State = [[-0.22838014  0.26508877]]. Action = [[-0.03490482  0.21215254 -0.16261773  0.6749394 ]]. Reward = [0.]
Curr episode timestep = 499
Current timestep = 500. State = [[-0.23127909  0.26963097]]. Action = [[-0.23503576 -0.16155258 -0.01923878  0.02934408]]. Reward = [0.]
Curr episode timestep = 500
Current timestep = 501. State = [[-0.236368    0.27060533]]. Action = [[-0.232001    0.14789897  0.24399614  0.32195735]]. Reward = [0.]
Curr episode timestep = 501
Current timestep = 502. State = [[-0.24438381  0.2748249 ]]. Action = [[-0.09161386 -0.1387816  -0.01456006  0.9629983 ]]. Reward = [0.]
Curr episode timestep = 502
Current timestep = 503. State = [[-0.252747    0.27424556]]. Action = [[ 0.025594    0.08960125 -0.08325458 -0.01313782]]. Reward = [0.]
Curr episode timestep = 503
Current timestep = 504. State = [[-0.2591004   0.27509275]]. Action = [[ 0.20378557 -0.01396826 -0.05913322  0.89811516]]. Reward = [0.]
Curr episode timestep = 504
Current timestep = 505. State = [[-0.2593028   0.27524334]]. Action = [[-0.04546307  0.14928955  0.190059    0.22807837]]. Reward = [0.]
Curr episode timestep = 505
Current timestep = 506. State = [[-0.26112494  0.27771413]]. Action = [[-0.20124637  0.18213338  0.21781412  0.8261602 ]]. Reward = [0.]
Curr episode timestep = 506
Current timestep = 507. State = [[-0.26613578  0.28438696]]. Action = [[ 0.21955198  0.19765541 -0.17765547  0.03649342]]. Reward = [0.]
Curr episode timestep = 507
Current timestep = 508. State = [[-0.26722282  0.29237688]]. Action = [[ 0.24035141  0.06639743 -0.09192154  0.73813915]]. Reward = [0.]
Curr episode timestep = 508
Current timestep = 509. State = [[-0.2632334   0.29928166]]. Action = [[0.17814997 0.00404873 0.11237377 0.05767751]]. Reward = [0.]
Curr episode timestep = 509
Current timestep = 510. State = [[-0.256574    0.30402517]]. Action = [[0.19054562 0.18791133 0.19049013 0.20777953]]. Reward = [0.]
Curr episode timestep = 510
Current timestep = 511. State = [[-0.24780956  0.31126064]]. Action = [[ 0.24776113 -0.22364666 -0.13372453  0.19312406]]. Reward = [0.]
Curr episode timestep = 511
Current timestep = 512. State = [[-0.23611222  0.30989334]]. Action = [[-0.22492638 -0.20157893 -0.1948808  -0.322039  ]]. Reward = [0.]
Curr episode timestep = 512
Current timestep = 513. State = [[-0.2301365   0.30646604]]. Action = [[ 0.18430823 -0.11208227  0.09312356 -0.09140378]]. Reward = [0.]
Curr episode timestep = 513
Current timestep = 514. State = [[-0.22313935  0.30063853]]. Action = [[ 0.02827761  0.1034379   0.10691082 -0.03452307]]. Reward = [0.]
Curr episode timestep = 514
Current timestep = 515. State = [[-0.2185336  0.2990198]]. Action = [[-0.23850201 -0.17372005 -0.16017453 -0.2968567 ]]. Reward = [0.]
Curr episode timestep = 515
Current timestep = 516. State = [[-0.21785691  0.296406  ]]. Action = [[-0.17001225  0.0274846   0.23945373  0.09481966]]. Reward = [0.]
Curr episode timestep = 516
Current timestep = 517. State = [[-0.21997556  0.29554263]]. Action = [[-0.21430627 -0.24361126 -0.11019304  0.6639569 ]]. Reward = [0.]
Curr episode timestep = 517
Current timestep = 518. State = [[-0.223546  0.288913]]. Action = [[-0.17525293 -0.2149676   0.03016543  0.04994774]]. Reward = [0.]
Curr episode timestep = 518
Current timestep = 519. State = [[-0.23024541  0.27968824]]. Action = [[ 0.2274946   0.13088948 -0.1940507  -0.14115971]]. Reward = [0.]
Curr episode timestep = 519
Current timestep = 520. State = [[-0.2334836  0.275038 ]]. Action = [[ 0.21712536  0.08300003  0.03256524 -0.28654522]]. Reward = [0.]
Curr episode timestep = 520
Current timestep = 521. State = [[-0.23130453  0.27333122]]. Action = [[0.15850085 0.02713361 0.01081604 0.25506306]]. Reward = [0.]
Curr episode timestep = 521
Current timestep = 522. State = [[-0.22823934  0.27215394]]. Action = [[-0.16436602 -0.15220042  0.11466551  0.5757878 ]]. Reward = [0.]
Curr episode timestep = 522
Current timestep = 523. State = [[-0.22711743  0.2691067 ]]. Action = [[ 0.05889934 -0.1804991  -0.03033325 -0.4365815 ]]. Reward = [0.]
Curr episode timestep = 523
Current timestep = 524. State = [[-0.22483982  0.26366988]]. Action = [[0.18401843 0.11942428 0.05704638 0.89533854]]. Reward = [0.]
Curr episode timestep = 524
Current timestep = 525. State = [[-0.22155832  0.2608975 ]]. Action = [[-0.15917873 -0.23372138  0.20673937  0.3267815 ]]. Reward = [0.]
Curr episode timestep = 525
Current timestep = 526. State = [[-0.21878149  0.2552746 ]]. Action = [[ 0.2193788  -0.1469445   0.21193647 -0.4766189 ]]. Reward = [0.]
Curr episode timestep = 526
Current timestep = 527. State = [[-0.21360688  0.24818932]]. Action = [[ 0.23634675  0.15733081  0.1561575  -0.08956397]]. Reward = [0.]
Curr episode timestep = 527
Current timestep = 528. State = [[-0.20715687  0.24484265]]. Action = [[ 0.20733997 -0.08995308 -0.24506631 -0.9264649 ]]. Reward = [0.]
Curr episode timestep = 528
Current timestep = 529. State = [[-0.19777504  0.23895791]]. Action = [[-0.00986205 -0.18085644  0.17938977  0.9900911 ]]. Reward = [0.]
Curr episode timestep = 529
Current timestep = 530. State = [[-0.19173451  0.23151913]]. Action = [[-0.19541629  0.00744343  0.10250261  0.5666809 ]]. Reward = [0.]
Curr episode timestep = 530
Current timestep = 531. State = [[-0.19010185  0.22842692]]. Action = [[-0.15447052 -0.05032468  0.04404333  0.8452029 ]]. Reward = [0.]
Curr episode timestep = 531
Current timestep = 532. State = [[-0.19181667  0.22542259]]. Action = [[-0.0964196  -0.20766895  0.06127912  0.71802616]]. Reward = [0.]
Curr episode timestep = 532
Current timestep = 533. State = [[-0.19377057  0.21987127]]. Action = [[-0.07089306  0.10095602 -0.05295564 -0.74109447]]. Reward = [0.]
Curr episode timestep = 533
Current timestep = 534. State = [[-0.19601515  0.21805365]]. Action = [[ 0.20532906 -0.22938395 -0.23149332  0.73306966]]. Reward = [0.]
Curr episode timestep = 534
Current timestep = 535. State = [[-0.19544266  0.2104801 ]]. Action = [[ 0.11323744 -0.09328379  0.2464911  -0.7766362 ]]. Reward = [0.]
Curr episode timestep = 535
Current timestep = 536. State = [[-0.192129    0.20300056]]. Action = [[ 0.02255216 -0.11989065 -0.17938632 -0.28041553]]. Reward = [0.]
Curr episode timestep = 536
Current timestep = 537. State = [[-0.18815349  0.19506763]]. Action = [[ 0.21408889 -0.22712527 -0.11155897 -0.04831022]]. Reward = [0.]
Curr episode timestep = 537
Current timestep = 538. State = [[-0.18187119  0.18309051]]. Action = [[ 0.24488097 -0.17854914  0.13908729 -0.73903424]]. Reward = [0.]
Curr episode timestep = 538
Current timestep = 539. State = [[-0.17380548  0.17077312]]. Action = [[ 0.20978594  0.20521629  0.10930881 -0.6147729 ]]. Reward = [0.]
Curr episode timestep = 539
Current timestep = 540. State = [[-0.16445626  0.16727507]]. Action = [[ 0.22609729  0.24202824  0.01815695 -0.592392  ]]. Reward = [0.]
Curr episode timestep = 540
Current timestep = 541. State = [[-0.15276423  0.1718115 ]]. Action = [[-0.23516019  0.03578514  0.01334214 -0.45387644]]. Reward = [0.]
Curr episode timestep = 541
Current timestep = 542. State = [[-0.14852895  0.17584145]]. Action = [[-0.05692409  0.02290148  0.08901566  0.8770255 ]]. Reward = [0.]
Curr episode timestep = 542
Current timestep = 543. State = [[-0.14811897  0.17888568]]. Action = [[-0.24622728  0.080055    0.17839831  0.0037179 ]]. Reward = [0.]
Curr episode timestep = 543
Current timestep = 544. State = [[-0.15074413  0.18308496]]. Action = [[ 0.09132186 -0.03046638 -0.01838738  0.6827489 ]]. Reward = [0.]
Curr episode timestep = 544
Current timestep = 545. State = [[-0.15144329  0.18388933]]. Action = [[-0.05020599 -0.21881624 -0.07938574  0.84702015]]. Reward = [0.]
Curr episode timestep = 545
Current timestep = 546. State = [[-0.15150544  0.18025793]]. Action = [[-0.24155293 -0.09630868  0.09637821  0.4202845 ]]. Reward = [0.]
Curr episode timestep = 546
Current timestep = 547. State = [[-0.1547057   0.17540324]]. Action = [[ 0.17471194 -0.20174313 -0.22578448 -0.49312937]]. Reward = [0.]
Curr episode timestep = 547
Current timestep = 548. State = [[-0.15580632  0.16641276]]. Action = [[ 0.07265815 -0.17907988  0.15392125 -0.80444604]]. Reward = [0.]
Curr episode timestep = 548
Current timestep = 549. State = [[-0.1540917  0.1576349]]. Action = [[-0.08217536 -0.01906374  0.04523444  0.5044935 ]]. Reward = [0.]
Curr episode timestep = 549
Current timestep = 550. State = [[-0.15388511  0.15083776]]. Action = [[-0.18322448 -0.01928492 -0.13669252 -0.618009  ]]. Reward = [0.]
Curr episode timestep = 550
Current timestep = 551. State = [[-0.15737222  0.14516523]]. Action = [[-0.05722868 -0.23967631  0.1428482   0.4325378 ]]. Reward = [0.]
Curr episode timestep = 551
Current timestep = 552. State = [[-0.16177857  0.13592452]]. Action = [[-0.14648731 -0.06275427  0.18287778  0.70785   ]]. Reward = [0.]
Curr episode timestep = 552
Current timestep = 553. State = [[-0.16806886  0.12837882]]. Action = [[-0.23058943  0.09331778 -0.23157644  0.41117573]]. Reward = [0.]
Curr episode timestep = 553
Current timestep = 554. State = [[-0.176248  0.125269]]. Action = [[ 0.11096528 -0.22293682  0.05821049  0.27202904]]. Reward = [0.]
Curr episode timestep = 554
Current timestep = 555. State = [[-0.18257493  0.11731766]]. Action = [[ 0.03093371  0.22636211 -0.03582336 -0.24365562]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 555 is [True, False, False, False, True, False]
State prediction error at timestep 555 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Current timestep = 556. State = [[-0.18433529  0.11849161]]. Action = [[-0.21045853  0.09484112  0.21862337 -0.8451052 ]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 556 is [True, False, False, False, True, False]
State prediction error at timestep 556 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Current timestep = 557. State = [[-0.18824455  0.12211983]]. Action = [[ 0.026209   -0.13611376 -0.03344882  0.520684  ]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 557 is [True, False, False, False, True, False]
State prediction error at timestep 557 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 558. State = [[-0.19146828  0.1202049 ]]. Action = [[-0.08228758 -0.03067905 -0.14509095  0.4933957 ]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 558 is [True, False, False, False, True, False]
State prediction error at timestep 558 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 559. State = [[-0.19580577  0.11773429]]. Action = [[ 0.12043217 -0.07312331  0.16816169 -0.85505325]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 559 is [True, False, False, False, True, False]
State prediction error at timestep 559 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Current timestep = 560. State = [[-0.19736445  0.11413807]]. Action = [[ 0.19696218 -0.10562891 -0.13130596  0.7963507 ]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 560 is [True, False, False, False, True, False]
State prediction error at timestep 560 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 561. State = [[-0.19491999  0.1092788 ]]. Action = [[-0.14319597  0.07210517 -0.14962786  0.12099969]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 561 is [True, False, False, False, True, False]
State prediction error at timestep 561 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 562. State = [[-0.19523449  0.10869393]]. Action = [[ 0.16891724 -0.09378621  0.00653964  0.16364622]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 562 is [True, False, False, False, True, False]
State prediction error at timestep 562 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 563. State = [[-0.193558    0.10552585]]. Action = [[0.06079015 0.02648103 0.1309399  0.00223815]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 563 is [True, False, False, False, True, False]
State prediction error at timestep 563 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Current timestep = 564. State = [[-0.1923857   0.10355528]]. Action = [[ 0.15811262  0.0334084  -0.14624126 -0.8277922 ]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 564 is [True, False, False, False, True, False]
State prediction error at timestep 564 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 564 of 1
Current timestep = 565. State = [[-0.18874975  0.10260377]]. Action = [[ 0.1080336  -0.23033941 -0.04065745 -0.34578478]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 565 is [True, False, False, False, True, False]
State prediction error at timestep 565 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Current timestep = 566. State = [[-0.18358546  0.0957674 ]]. Action = [[ 0.24290025  0.19766065  0.0359295  -0.07907087]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 566 is [True, False, False, False, True, False]
State prediction error at timestep 566 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Current timestep = 567. State = [[-0.17435291  0.09730988]]. Action = [[0.2095936  0.24826619 0.08071542 0.8847549 ]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 567 is [True, False, False, False, True, False]
State prediction error at timestep 567 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 567 of 1
Current timestep = 568. State = [[-0.16153593  0.10554048]]. Action = [[ 0.21937495  0.06205621 -0.22464147 -0.57049876]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 568 is [True, False, False, False, True, False]
State prediction error at timestep 568 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Current timestep = 569. State = [[-0.14824742  0.11272088]]. Action = [[-0.19313246  0.22600621  0.16464502  0.3118744 ]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 569 is [True, False, False, False, True, False]
State prediction error at timestep 569 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Current timestep = 570. State = [[-0.142473    0.12257783]]. Action = [[-0.24654686 -0.16708726  0.02741578  0.9510461 ]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 570 is [True, False, False, False, True, False]
State prediction error at timestep 570 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 570 of 1
Current timestep = 571. State = [[-0.14399269  0.12434393]]. Action = [[ 0.04294425 -0.13576591 -0.06741211 -0.74533796]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 571 is [True, False, False, False, True, False]
State prediction error at timestep 571 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 571 of 1
Current timestep = 572. State = [[-0.14389688  0.12230375]]. Action = [[ 0.1934858   0.13673538 -0.20881909 -0.9254616 ]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 572 is [True, False, False, False, True, False]
State prediction error at timestep 572 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Current timestep = 573. State = [[-0.14210957  0.12335881]]. Action = [[-0.24875797 -0.11175635 -0.13888493 -0.4680364 ]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 573 is [True, False, False, False, True, False]
State prediction error at timestep 573 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Current timestep = 574. State = [[-0.14381304  0.12266085]]. Action = [[-0.11283869  0.12571669  0.03228769 -0.7819474 ]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 574 is [True, False, False, False, True, False]
State prediction error at timestep 574 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Current timestep = 575. State = [[-0.14663093  0.12549822]]. Action = [[ 0.02592373  0.20978475  0.05223745 -0.08812755]]. Reward = [0.]
Curr episode timestep = 575
Current timestep = 576. State = [[-0.14964852  0.13176921]]. Action = [[0.01002854 0.12850264 0.1964432  0.84367776]]. Reward = [0.]
Curr episode timestep = 576
Human Feedback received at timestep 576 of 1
Current timestep = 577. State = [[-0.15272233  0.13911685]]. Action = [[-0.08507362  0.09568429 -0.05902451  0.31064975]]. Reward = [0.]
Curr episode timestep = 577
Current timestep = 578. State = [[-0.1558261   0.14587986]]. Action = [[-0.23264618 -0.08175057 -0.18862571  0.58740735]]. Reward = [0.]
Curr episode timestep = 578
Current timestep = 579. State = [[-0.16128471  0.14990509]]. Action = [[-0.10661659  0.14708662  0.13436878 -0.59577775]]. Reward = [0.]
Curr episode timestep = 579
Current timestep = 580. State = [[-0.16624483  0.15597269]]. Action = [[ 0.16089493 -0.17296748  0.20682907  0.09608269]]. Reward = [0.]
Curr episode timestep = 580
Current timestep = 581. State = [[-0.16871463  0.15542875]]. Action = [[-0.03539115  0.2005406  -0.11544105 -0.9717294 ]]. Reward = [0.]
Curr episode timestep = 581
Current timestep = 582. State = [[-0.17098525  0.15929826]]. Action = [[-0.2164583   0.16040426  0.05120379 -0.5444632 ]]. Reward = [0.]
Curr episode timestep = 582
Current timestep = 583. State = [[-0.176033   0.1666111]]. Action = [[0.09132731 0.13881668 0.21681952 0.08393812]]. Reward = [0.]
Curr episode timestep = 583
Current timestep = 584. State = [[-0.18041162  0.17424202]]. Action = [[ 0.06486523 -0.00450607  0.12357813  0.7563398 ]]. Reward = [0.]
Curr episode timestep = 584
Current timestep = 585. State = [[-0.18267438  0.17834064]]. Action = [[-0.23311695  0.10999727 -0.17603791  0.80326486]]. Reward = [0.]
Curr episode timestep = 585
Current timestep = 586. State = [[-0.18684901  0.1846569 ]]. Action = [[ 0.18791741 -0.20513476  0.18498892 -0.8270376 ]]. Reward = [0.]
Curr episode timestep = 586
Current timestep = 587. State = [[-0.18668194  0.18343143]]. Action = [[ 0.17287448 -0.16216567 -0.22310847 -0.27628118]]. Reward = [0.]
Curr episode timestep = 587
Current timestep = 588. State = [[-0.18302336  0.17761996]]. Action = [[-3.3958256e-04 -2.2268301e-01  1.4215058e-01  8.3395064e-01]]. Reward = [0.]
Curr episode timestep = 588
Current timestep = 589. State = [[-0.17921185  0.17025118]]. Action = [[-0.04556632  0.22096622  0.05661508 -0.32658535]]. Reward = [0.]
Curr episode timestep = 589
Current timestep = 590. State = [[-0.17937246  0.1701322 ]]. Action = [[-0.2129453   0.17504227 -0.20973504 -0.7290657 ]]. Reward = [0.]
Curr episode timestep = 590
Current timestep = 591. State = [[-0.1823149  0.174466 ]]. Action = [[ 0.11061594 -0.02069473  0.15815163 -0.77968866]]. Reward = [0.]
Curr episode timestep = 591
Current timestep = 592. State = [[-0.18304174  0.17573372]]. Action = [[ 0.12112078 -0.22168112  0.13607639 -0.8452773 ]]. Reward = [0.]
Curr episode timestep = 592
Current timestep = 593. State = [[-0.18079792  0.17185734]]. Action = [[-0.06648603 -0.05452058 -0.11843604  0.09645164]]. Reward = [0.]
Curr episode timestep = 593
Current timestep = 594. State = [[-0.17964365  0.1693266 ]]. Action = [[0.13985977 0.16228542 0.0191859  0.90540886]]. Reward = [0.]
Curr episode timestep = 594
Current timestep = 595. State = [[-0.17888348  0.16978507]]. Action = [[-0.09781276 -0.22283314 -0.13507195 -0.73513037]]. Reward = [0.]
Curr episode timestep = 595
Current timestep = 596. State = [[-0.17805381  0.16607662]]. Action = [[-0.18349257 -0.02225001 -0.17329092  0.8208115 ]]. Reward = [0.]
Curr episode timestep = 596
Current timestep = 597. State = [[-0.17939916  0.16304302]]. Action = [[-0.18109822 -0.22252047 -0.23987825 -0.15541148]]. Reward = [0.]
Curr episode timestep = 597
Current timestep = 598. State = [[-0.1842458   0.15544611]]. Action = [[-0.21897092  0.21745372 -0.12335789 -0.26083398]]. Reward = [0.]
Curr episode timestep = 598
Current timestep = 599. State = [[-0.19220483  0.15618236]]. Action = [[ 0.00763142  0.01428047  0.07017398 -0.5017445 ]]. Reward = [0.]
Curr episode timestep = 599
Current timestep = 600. State = [[-0.19923708  0.15604232]]. Action = [[ 0.09272617 -0.13412964 -0.12917267 -0.8855415 ]]. Reward = [0.]
Curr episode timestep = 600
Current timestep = 601. State = [[-0.20151885  0.15343241]]. Action = [[-0.22330205  0.06230792 -0.02416363 -0.72215766]]. Reward = [0.]
Curr episode timestep = 601
Current timestep = 602. State = [[-0.20486844  0.15361215]]. Action = [[-0.24754354 -0.24162255  0.01211807 -0.6959484 ]]. Reward = [0.]
Curr episode timestep = 602
Current timestep = 603. State = [[-0.21265048  0.14726444]]. Action = [[ 0.13950962  0.18965632 -0.03849457  0.23725843]]. Reward = [0.]
Curr episode timestep = 603
Current timestep = 604. State = [[-0.21743192  0.14807795]]. Action = [[ 0.1807656   0.15567476  0.18486738 -0.21869302]]. Reward = [0.]
Curr episode timestep = 604
Current timestep = 605. State = [[-0.21820742  0.1510363 ]]. Action = [[ 0.09248406 -0.08200334 -0.07048476  0.8043772 ]]. Reward = [0.]
Curr episode timestep = 605
Current timestep = 606. State = [[-0.21799664  0.15149358]]. Action = [[-0.02755906  0.17609078 -0.01274213  0.47827697]]. Reward = [0.]
Curr episode timestep = 606
Current timestep = 607. State = [[-0.21862845  0.15509537]]. Action = [[ 0.1405223  -0.23382941  0.18102908  0.02643049]]. Reward = [0.]
Curr episode timestep = 607
Current timestep = 608. State = [[-0.21597677  0.15210755]]. Action = [[ 0.10486487 -0.1725681   0.09389105 -0.20814121]]. Reward = [0.]
Curr episode timestep = 608
Current timestep = 609. State = [[-0.21177565  0.14614296]]. Action = [[-0.06364836  0.18810606  0.14418584  0.94014335]]. Reward = [0.]
Curr episode timestep = 609
Current timestep = 610. State = [[-0.2109921   0.14631243]]. Action = [[-0.08112898  0.01648569 -0.04713634 -0.8796189 ]]. Reward = [0.]
Curr episode timestep = 610
Current timestep = 611. State = [[-0.21165404  0.14771736]]. Action = [[-0.09072725  0.10976231  0.13674134 -0.53637385]]. Reward = [0.]
Curr episode timestep = 611
Current timestep = 612. State = [[-0.21342361  0.15098962]]. Action = [[ 0.10848808 -0.11313081  0.12673992  0.88316536]]. Reward = [0.]
Curr episode timestep = 612
Current timestep = 613. State = [[-0.21294993  0.15065485]]. Action = [[0.18005317 0.20450187 0.1416974  0.12617135]]. Reward = [0.]
Curr episode timestep = 613
Current timestep = 614. State = [[-0.20991714  0.15436253]]. Action = [[ 0.2482368  -0.12914409  0.08463997 -0.88977796]]. Reward = [0.]
Curr episode timestep = 614
Current timestep = 615. State = [[-0.20301941  0.15336338]]. Action = [[-0.07246517 -0.21748899  0.18794906 -0.8155318 ]]. Reward = [0.]
Curr episode timestep = 615
Current timestep = 616. State = [[-0.19798817  0.14747968]]. Action = [[ 0.19509596 -0.22719681  0.12105471  0.8186674 ]]. Reward = [0.]
Curr episode timestep = 616
Current timestep = 617. State = [[-0.19145794  0.13869679]]. Action = [[0.15299541 0.20866865 0.1656022  0.4305284 ]]. Reward = [0.]
Curr episode timestep = 617
Current timestep = 618. State = [[-0.1826952   0.13690041]]. Action = [[ 0.01905012 -0.21343394  0.03223404  0.26623404]]. Reward = [0.]
Curr episode timestep = 618
Current timestep = 619. State = [[-0.1767537   0.13067517]]. Action = [[-0.17553064 -0.10575262 -0.10369667  0.9286579 ]]. Reward = [0.]
Curr episode timestep = 619
Current timestep = 620. State = [[-0.17451334  0.12473461]]. Action = [[-0.03178456  0.00404695 -0.23512141  0.3961588 ]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 620 is [True, False, False, False, True, False]
State prediction error at timestep 620 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 621. State = [[-0.17404574  0.12144732]]. Action = [[-0.05223811 -0.14215994 -0.11383718 -0.55499214]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 621 is [True, False, False, False, True, False]
State prediction error at timestep 621 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 622. State = [[-0.17428903  0.11545361]]. Action = [[-0.04764497 -0.1884547   0.22359502  0.17444563]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 622 is [True, False, False, False, True, False]
State prediction error at timestep 622 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 623. State = [[-0.1752861   0.10695408]]. Action = [[-0.11481968 -0.00491634  0.19664955  0.11266291]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 623 is [True, False, False, False, True, False]
State prediction error at timestep 623 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 624. State = [[-0.1772732   0.10124793]]. Action = [[ 0.23991361 -0.22181064 -0.08068907  0.33078265]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 624 is [True, False, False, False, True, False]
State prediction error at timestep 624 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 625. State = [[-0.17645907  0.0922671 ]]. Action = [[0.09995219 0.21602422 0.06223363 0.7705108 ]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 625 is [True, False, False, False, True, False]
State prediction error at timestep 625 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 625 of -1
Current timestep = 626. State = [[-0.17319705  0.09108728]]. Action = [[ 0.05687991  0.06852689  0.15592849 -0.15976846]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 626 is [True, False, False, False, True, False]
State prediction error at timestep 626 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 627. State = [[-0.17011602  0.0923609 ]]. Action = [[-0.22002459  0.0063324   0.02443171  0.83800757]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 627 is [True, False, False, False, True, False]
State prediction error at timestep 627 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 628. State = [[-0.1708086   0.09357782]]. Action = [[-0.21049564 -0.12247527 -0.09243125  0.29881942]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 628 is [True, False, False, False, True, False]
State prediction error at timestep 628 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 628 of 1
Current timestep = 629. State = [[-0.17603292  0.09091774]]. Action = [[ 0.0494723   0.15550861 -0.00055797 -0.2503754 ]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 629 is [True, False, False, False, True, False]
State prediction error at timestep 629 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 629 of 1
Current timestep = 630. State = [[-0.17756002  0.09259805]]. Action = [[-0.08402255 -0.21531388 -0.19952059  0.9707167 ]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 630 is [True, False, False, False, True, False]
State prediction error at timestep 630 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 630 of 1
Current timestep = 631. State = [[-0.18017206  0.08838087]]. Action = [[-0.09218797 -0.05436176  0.15451753  0.8664675 ]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 631 is [True, False, False, False, True, False]
State prediction error at timestep 631 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 632. State = [[-0.18373075  0.08505508]]. Action = [[-0.1991963   0.02778858 -0.10306735  0.4229008 ]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 632 is [True, False, False, False, True, False]
State prediction error at timestep 632 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 632 of 1
Current timestep = 633. State = [[-0.1897789  0.0829943]]. Action = [[-0.01697683 -0.02860186 -0.08931255  0.41399217]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 633 is [True, False, False, False, True, False]
State prediction error at timestep 633 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 634. State = [[-0.19386338  0.08135517]]. Action = [[-0.14658782  0.05112693  0.10662588 -0.15205151]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 634 is [True, False, False, False, True, False]
State prediction error at timestep 634 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 635. State = [[-0.19969234  0.08192745]]. Action = [[ 0.22637004  0.20913082  0.11078054 -0.81320053]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 635 is [True, False, False, False, True, False]
State prediction error at timestep 635 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 636. State = [[-0.20141391  0.08651554]]. Action = [[ 0.12151292 -0.16068684 -0.22372454 -0.82752407]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 636 is [True, False, False, False, True, False]
State prediction error at timestep 636 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 637. State = [[-0.20059887  0.08554927]]. Action = [[-0.15367004  0.06929162  0.00724387  0.79934883]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 637 is [True, False, False, False, True, False]
State prediction error at timestep 637 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 637 of -1
Current timestep = 638. State = [[-0.2018275  0.087735 ]]. Action = [[0.16708407 0.16332304 0.03856698 0.72078395]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 638 is [True, False, False, False, True, False]
State prediction error at timestep 638 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 639. State = [[-0.20092916  0.09265766]]. Action = [[ 0.110535    0.21340978  0.09156746 -0.9250862 ]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 639 is [True, False, False, False, True, False]
State prediction error at timestep 639 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 640. State = [[-0.1970487   0.10111327]]. Action = [[-0.1946536  -0.20200752  0.1819765   0.50861216]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 640 is [True, False, False, False, True, False]
State prediction error at timestep 640 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 640 of -1
Current timestep = 641. State = [[-0.19840269  0.10198992]]. Action = [[-0.2012168   0.14877081 -0.17533447 -0.13006997]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 641 is [True, False, False, False, True, False]
State prediction error at timestep 641 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 641 of -1
Current timestep = 642. State = [[-0.20233001  0.1069433 ]]. Action = [[ 0.2289393   0.23201114  0.2351622  -0.8974135 ]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 642 is [True, False, False, False, True, False]
State prediction error at timestep 642 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 643. State = [[-0.20317042  0.11461952]]. Action = [[ 0.19387186 -0.13612062 -0.09528217  0.8803568 ]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 643 is [True, False, False, False, True, False]
State prediction error at timestep 643 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 644. State = [[-0.20007358  0.1161066 ]]. Action = [[ 0.13693696 -0.00584674 -0.08489338  0.95590377]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 644 is [True, False, False, False, True, False]
State prediction error at timestep 644 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 644 of -1
Current timestep = 645. State = [[-0.19582872  0.11658564]]. Action = [[-0.15250629 -0.10788184 -0.06173028  0.8980044 ]]. Reward = [0.]
Curr episode timestep = 645
Human Feedback received at timestep 645 of -1
Current timestep = 646. State = [[-0.19493224  0.11460421]]. Action = [[-0.23492041  0.04041147  0.18464386  0.8936546 ]]. Reward = [0.]
Curr episode timestep = 646
Current timestep = 647. State = [[-0.19809332  0.11451983]]. Action = [[-0.11510263  0.04082602 -0.10370469 -0.24899173]]. Reward = [0.]
Curr episode timestep = 647
Human Feedback received at timestep 647 of -1
Current timestep = 648. State = [[-0.2022086   0.11579927]]. Action = [[-0.24595429  0.11557847  0.1833084   0.17487216]]. Reward = [0.]
Curr episode timestep = 648
Current timestep = 649. State = [[-0.20925088  0.12104125]]. Action = [[-0.03753236  0.23078823  0.17622763 -0.9045464 ]]. Reward = [0.]
Curr episode timestep = 649
Human Feedback received at timestep 649 of -1
Current timestep = 650. State = [[-0.21524112  0.13038714]]. Action = [[ 0.10346854 -0.18490821  0.08296046 -0.19055814]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 650 is [True, False, False, False, False, True]
State prediction error at timestep 650 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 650 of -1
Current timestep = 651. State = [[-0.21813756  0.13096362]]. Action = [[ 0.12118018  0.10765666  0.22684246 -0.8339898 ]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 651 is [True, False, False, False, False, True]
State prediction error at timestep 651 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 652. State = [[-0.21850803  0.13267529]]. Action = [[-0.2360944   0.0480707   0.0635587   0.86086357]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 652 is [True, False, False, False, False, True]
State prediction error at timestep 652 is tensor(6.5649e-05, grad_fn=<MseLossBackward0>)
Current timestep = 653. State = [[-0.22192475  0.13612415]]. Action = [[ 0.19923508 -0.24354658  0.07563132  0.97936463]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 653 is [True, False, False, False, False, True]
State prediction error at timestep 653 is tensor(8.2824e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 653 of -1
Current timestep = 654. State = [[-0.2213204   0.13226576]]. Action = [[ 0.05310968  0.14540088 -0.16835938  0.6808287 ]]. Reward = [0.]
Curr episode timestep = 654
Human Feedback received at timestep 654 of -1
Current timestep = 655. State = [[-0.2209691   0.13264693]]. Action = [[0.1604836  0.1521908  0.10679662 0.17185116]]. Reward = [0.]
Curr episode timestep = 655
Current timestep = 656. State = [[-0.21787776  0.13627072]]. Action = [[-0.19356711 -0.18065283 -0.1414924   0.12031889]]. Reward = [0.]
Curr episode timestep = 656
Human Feedback received at timestep 656 of -1
Current timestep = 657. State = [[-0.2178384  0.135301 ]]. Action = [[ 0.06390426 -0.16101302 -0.14018606  0.01442575]]. Reward = [0.]
Curr episode timestep = 657
Human Feedback received at timestep 657 of -1
Current timestep = 658. State = [[-0.21713178  0.13077793]]. Action = [[ 0.11280963  0.10454836  0.16189018 -0.35588068]]. Reward = [0.]
Curr episode timestep = 658
Current timestep = 659. State = [[-0.21612278  0.12953822]]. Action = [[-0.21723056 -0.17982692 -0.03472352 -0.23275733]]. Reward = [0.]
Curr episode timestep = 659
Human Feedback received at timestep 659 of -1
Current timestep = 660. State = [[-0.21762271  0.12488432]]. Action = [[ 0.14760947 -0.18794145  0.2117421   0.43217993]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 660 is [True, False, False, False, True, False]
State prediction error at timestep 660 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 661. State = [[-0.21767765  0.11739913]]. Action = [[-0.18876366  0.15587443 -0.14114834  0.52092695]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 661 is [True, False, False, False, True, False]
State prediction error at timestep 661 is tensor(2.2711e-05, grad_fn=<MseLossBackward0>)
Current timestep = 662. State = [[-0.21845104  0.11600516]]. Action = [[ 0.24152225 -0.16387507  0.04618198  0.6311805 ]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 662 is [True, False, False, False, True, False]
State prediction error at timestep 662 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 663. State = [[-0.21586527  0.11080488]]. Action = [[-0.00569233  0.09576494 -0.04774842  0.77891827]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 663 is [True, False, False, False, True, False]
State prediction error at timestep 663 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 664. State = [[-0.21495058  0.10941685]]. Action = [[ 0.01572579 -0.04985127  0.15553823 -0.85029036]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 664 is [True, False, False, False, True, False]
State prediction error at timestep 664 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 665. State = [[-0.21381184  0.10771973]]. Action = [[ 0.23444676  0.0452846  -0.15207477  0.92159104]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 665 is [True, False, False, False, True, False]
State prediction error at timestep 665 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 665 of -1
Current timestep = 666. State = [[-0.20911862  0.10661092]]. Action = [[-0.17771104 -0.20086642 -0.0966275   0.20368326]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 666 is [True, False, False, False, True, False]
State prediction error at timestep 666 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 666 of -1
Current timestep = 667. State = [[-0.20635696  0.09958041]]. Action = [[0.15241599 0.0942955  0.08090356 0.33238697]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 667 is [True, False, False, False, True, False]
State prediction error at timestep 667 is tensor(6.0499e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 667 of -1
Current timestep = 668. State = [[-0.20480035  0.09895725]]. Action = [[ 0.18247902 -0.18320407  0.08704102 -0.31434077]]. Reward = [0.]
Curr episode timestep = 668
Current timestep = 669. State = [[-0.20020315  0.09374146]]. Action = [[ 0.22174841 -0.00865737 -0.00921433 -0.9063502 ]]. Reward = [0.]
Curr episode timestep = 669
Human Feedback received at timestep 669 of -1
Current timestep = 670. State = [[-0.19104275  0.09030076]]. Action = [[0.12550354 0.03809202 0.03098837 0.1339382 ]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 670 is [True, False, False, False, True, False]
State prediction error at timestep 670 is tensor(6.3317e-05, grad_fn=<MseLossBackward0>)
Current timestep = 671. State = [[-0.1799149   0.08938189]]. Action = [[ 0.05983788  0.22656652 -0.22128631  0.3410263 ]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 671 is [True, False, False, False, True, False]
State prediction error at timestep 671 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 672. State = [[-0.17120662  0.0941548 ]]. Action = [[ 0.15051556 -0.16205913  0.16358674 -0.08895129]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 672 is [True, False, False, False, True, False]
State prediction error at timestep 672 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 672 of -1
Current timestep = 673. State = [[-0.16231743  0.09289347]]. Action = [[-0.1115298  -0.17755738  0.1346463  -0.9250246 ]]. Reward = [0.]
Curr episode timestep = 673
Human Feedback received at timestep 673 of 1
Current timestep = 674. State = [[-0.15878059  0.08772983]]. Action = [[ 0.11123115 -0.10334697  0.16479349 -0.56578386]]. Reward = [0.]
Curr episode timestep = 674
Human Feedback received at timestep 674 of 1
Current timestep = 675. State = [[-0.15533534  0.08219584]]. Action = [[ 0.09657052  0.16887373 -0.03692392  0.37360215]]. Reward = [0.]
Curr episode timestep = 675
Human Feedback received at timestep 675 of 1
Current timestep = 676. State = [[-0.1503596   0.08205767]]. Action = [[ 0.15780807 -0.15219088 -0.218253    0.4177525 ]]. Reward = [0.]
Curr episode timestep = 676
Current timestep = 677. State = [[-0.1441878   0.07854919]]. Action = [[ 0.23469603 -0.02091479 -0.12271412 -0.23058271]]. Reward = [0.]
Curr episode timestep = 677
Current timestep = 678. State = [[-0.13323008  0.07515065]]. Action = [[ 0.1649724  -0.11783633 -0.15383182  0.2035662 ]]. Reward = [0.]
Curr episode timestep = 678
Human Feedback received at timestep 678 of 1
Current timestep = 679. State = [[-0.12270602  0.07071392]]. Action = [[-0.00143866 -0.03573862 -0.0806874  -0.9085753 ]]. Reward = [0.]
Curr episode timestep = 679
Human Feedback received at timestep 679 of 1
Current timestep = 680. State = [[-0.11360055  0.06714062]]. Action = [[ 0.00377145  0.20109916 -0.21560743  0.08776999]]. Reward = [0.]
Curr episode timestep = 680
Human Feedback received at timestep 680 of 1
Current timestep = 681. State = [[-0.10787165  0.06912483]]. Action = [[-0.03573677 -0.2122181  -0.15467578 -0.90876245]]. Reward = [0.]
Curr episode timestep = 681
Human Feedback received at timestep 681 of 1
Current timestep = 682. State = [[-0.10511246  0.06545535]]. Action = [[-0.07096231 -0.12136877  0.14422733 -0.968309  ]]. Reward = [0.]
Curr episode timestep = 682
Human Feedback received at timestep 682 of 1
Current timestep = 683. State = [[-0.10343242  0.06007016]]. Action = [[ 0.06735325 -0.16706397 -0.03284192  0.49726617]]. Reward = [0.]
Curr episode timestep = 683
Current timestep = 684. State = [[-0.10083933  0.05316734]]. Action = [[ 0.20947918  0.1929143  -0.07547902  0.46601272]]. Reward = [0.]
Curr episode timestep = 684
Current timestep = 685. State = [[-0.09610278  0.05305627]]. Action = [[-0.05763182  0.0835999  -0.21964225 -0.44886804]]. Reward = [0.]
Curr episode timestep = 685
Human Feedback received at timestep 685 of 1
Current timestep = 686. State = [[-0.09384326  0.05519805]]. Action = [[-0.18815091  0.11518538 -0.14839546 -0.5275343 ]]. Reward = [0.]
Curr episode timestep = 686
Human Feedback received at timestep 686 of 1
Current timestep = 687. State = [[-0.09535212  0.05994191]]. Action = [[-0.1259996   0.15838984 -0.21403152 -0.76128536]]. Reward = [0.]
Curr episode timestep = 687
Human Feedback received at timestep 687 of 1
Current timestep = 688. State = [[-0.09813891  0.06720991]]. Action = [[-0.03482524  0.22637269  0.20251101  0.07450461]]. Reward = [0.]
Curr episode timestep = 688
Human Feedback received at timestep 688 of 1
Current timestep = 689. State = [[-0.10156199  0.0768046 ]]. Action = [[-0.14341018 -0.07705532  0.20065829 -0.9565937 ]]. Reward = [0.]
Curr episode timestep = 689
Current timestep = 690. State = [[-0.10501757  0.08161263]]. Action = [[ 0.09191108 -0.22036067  0.05573884  0.4439894 ]]. Reward = [0.]
Curr episode timestep = 690
Current timestep = 691. State = [[-0.10648702  0.07929695]]. Action = [[ 0.10843217  0.07105726 -0.05895492  0.15104437]]. Reward = [0.]
Curr episode timestep = 691
Human Feedback received at timestep 691 of 1
Current timestep = 692. State = [[-0.10624872  0.07883921]]. Action = [[ 0.0626035   0.01936665  0.10072765 -0.679249  ]]. Reward = [0.]
Curr episode timestep = 692
Human Feedback received at timestep 692 of 1
Current timestep = 693. State = [[-0.10604636  0.07874393]]. Action = [[-0.21693245 -0.20195703  0.18931824 -0.8473025 ]]. Reward = [0.]
Curr episode timestep = 693
Human Feedback received at timestep 693 of 1
Current timestep = 694. State = [[-0.10762107  0.074394  ]]. Action = [[-0.18228315  0.02142623 -0.15409043  0.40726852]]. Reward = [0.]
Curr episode timestep = 694
Current timestep = 695. State = [[-0.11083414  0.07278448]]. Action = [[ 0.19231814  0.18879205 -0.2138226   0.15834582]]. Reward = [0.]
Curr episode timestep = 695
Current timestep = 696. State = [[-0.11148438  0.07571782]]. Action = [[-0.23531696  0.191701    0.02176669 -0.41338193]]. Reward = [0.]
Curr episode timestep = 696
Human Feedback received at timestep 696 of 1
Current timestep = 697. State = [[-0.11471727  0.08310419]]. Action = [[0.12481987 0.1452896  0.15829337 0.24926186]]. Reward = [0.]
Curr episode timestep = 697
Human Feedback received at timestep 697 of 1
Current timestep = 698. State = [[-0.11737663  0.09083711]]. Action = [[-0.23079923 -0.23269641  0.2108711  -0.10194051]]. Reward = [0.]
Curr episode timestep = 698
Current timestep = 699. State = [[-0.12070181  0.09084772]]. Action = [[ 0.19893372  0.03979802 -0.14372146  0.4101894 ]]. Reward = [0.]
Curr episode timestep = 699
Human Feedback received at timestep 699 of 1
Current timestep = 700. State = [[-0.12079131  0.09098651]]. Action = [[ 0.06980658 -0.07636526  0.08237952  0.07526529]]. Reward = [0.]
Curr episode timestep = 700
Human Feedback received at timestep 700 of 1
Current timestep = 701. State = [[-0.12006295  0.08928744]]. Action = [[ 0.02217799 -0.23555145 -0.16587347  0.27079296]]. Reward = [0.]
Curr episode timestep = 701
Current timestep = 702. State = [[-0.11839105  0.08307261]]. Action = [[ 0.23931244  0.22503993 -0.03341983 -0.23434258]]. Reward = [0.]
Curr episode timestep = 702
Human Feedback received at timestep 702 of 1
Current timestep = 703. State = [[-0.114144    0.08382656]]. Action = [[ 0.2282607  -0.00477323  0.1522947  -0.7863223 ]]. Reward = [0.]
Curr episode timestep = 703
Human Feedback received at timestep 703 of 1
Current timestep = 704. State = [[-0.10743252  0.08441157]]. Action = [[-0.23124819  0.18142691  0.15450203 -0.1931085 ]]. Reward = [0.]
Curr episode timestep = 704
Current timestep = 705. State = [[-0.10573217  0.08920912]]. Action = [[ 0.00076243  0.00939167  0.22428873 -0.4508171 ]]. Reward = [0.]
Curr episode timestep = 705
Current timestep = 706. State = [[-0.10568073  0.09186079]]. Action = [[0.14282304 0.10193127 0.08127016 0.4861096 ]]. Reward = [0.]
Curr episode timestep = 706
Human Feedback received at timestep 706 of 1
Current timestep = 707. State = [[-0.10254528  0.09664056]]. Action = [[ 0.02614823  0.03136975  0.15089723 -0.4067279 ]]. Reward = [0.]
Curr episode timestep = 707
Human Feedback received at timestep 707 of 1
Current timestep = 708. State = [[-0.09991845  0.09992722]]. Action = [[ 0.20167449 -0.11209254 -0.06325686  0.9050058 ]]. Reward = [0.]
Curr episode timestep = 708
Current timestep = 709. State = [[-0.09470543  0.10014226]]. Action = [[-0.21420261  0.1473127   0.05813995 -0.3150009 ]]. Reward = [0.]
Curr episode timestep = 709
Human Feedback received at timestep 709 of 1
Current timestep = 710. State = [[-0.09392557  0.10347039]]. Action = [[ 0.19278845  0.03218275 -0.09946525 -0.63654786]]. Reward = [0.]
Curr episode timestep = 710
Human Feedback received at timestep 710 of 1
Current timestep = 711. State = [[-0.09113158  0.10723215]]. Action = [[ 0.1429823   0.20545036 -0.18193288  0.9666953 ]]. Reward = [0.]
Curr episode timestep = 711
Human Feedback received at timestep 711 of 1
Current timestep = 712. State = [[-0.08544915  0.11525641]]. Action = [[ 0.23066708  0.08868995  0.09028441 -0.66927093]]. Reward = [0.]
Curr episode timestep = 712
Current timestep = 713. State = [[-0.07684781  0.12277517]]. Action = [[ 0.1458661   0.0272111  -0.11650223 -0.29548204]]. Reward = [0.]
Curr episode timestep = 713
Human Feedback received at timestep 713 of 1
Current timestep = 714. State = [[-0.06724399  0.1278667 ]]. Action = [[ 0.08343133 -0.14893104 -0.18543851  0.32755494]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 714 is [True, False, False, False, False, True]
State prediction error at timestep 714 is tensor(2.1814e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 714 of 1
Current timestep = 715. State = [[-0.05638552  0.1262284 ]]. Action = [[ 0.13647494 -0.19888976 -0.11600021  0.03802133]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 715 is [True, False, False, False, False, True]
State prediction error at timestep 715 is tensor(3.2722e-05, grad_fn=<MseLossBackward0>)
Current timestep = 716. State = [[-0.04687457  0.12036724]]. Action = [[ 0.2173639   0.01254839  0.03976893 -0.12064946]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 716 is [False, True, False, False, True, False]
State prediction error at timestep 716 is tensor(8.8144e-05, grad_fn=<MseLossBackward0>)
Current timestep = 717. State = [[-0.257983    0.00806101]]. Action = [[-0.07891406  0.04898638  0.02347553 -0.09848297]]. Reward = [1.]
Curr episode timestep = 717
Current timestep = 718. State = [[-0.25223264  0.01266111]]. Action = [[ 0.13787633 -0.21918175 -0.0166214  -0.35815465]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 719. State = [[-0.21333872  0.03932357]]. Action = [[-0.07880677 -0.19013213 -0.1786185  -0.53961325]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 720. State = [[-0.16516705  0.05364697]]. Action = [[-0.03188357 -0.23013596 -0.01171888 -0.8878183 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 721. State = [[-0.1282121   0.05580923]]. Action = [[ 0.1772471   0.16288367 -0.11617577  0.6495787 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 722. State = [[-0.09606109  0.06256849]]. Action = [[-0.05049251 -0.16901717 -0.02561432 -0.19720387]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 723. State = [[-0.07228596  0.06274859]]. Action = [[ 0.1456973   0.10021561  0.07355988 -0.15688437]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 724. State = [[-0.05170066  0.06460817]]. Action = [[-0.17485873 -0.14310344 -0.08126649  0.47211802]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 725. State = [[-0.03924195  0.06220358]]. Action = [[-0.08730072 -0.1758613   0.08160794 -0.8874395 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 725 is [False, True, False, False, True, False]
State prediction error at timestep 725 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 726. State = [[-0.25715932  0.00779812]]. Action = [[ 0.18932745 -0.0385901  -0.21152773  0.6474724 ]]. Reward = [1.]
Curr episode timestep = 8
Current timestep = 727. State = [[-0.24905239  0.01038147]]. Action = [[ 0.17859584 -0.05041423 -0.0941333  -0.63669884]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 728. State = [[-0.20727286  0.02119255]]. Action = [[-0.01275155  0.07756296  0.2301538   0.68760514]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 729. State = [[-0.15179712  0.03198084]]. Action = [[-0.12802418  0.22827029  0.1207431   0.8304659 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 730. State = [[-0.11170173  0.04340554]]. Action = [[-0.00844936 -0.08891785  0.14919746  0.66242456]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 731. State = [[-0.08490251  0.04792226]]. Action = [[-0.00728786  0.19225925  0.15636533 -0.4633559 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 732. State = [[-0.06263194  0.05616831]]. Action = [[ 0.20595735  0.18026829 -0.01878351 -0.88232726]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 733. State = [[-0.04484669  0.06532909]]. Action = [[-0.08306912 -0.24129137 -0.21186121  0.786628  ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 733 is [False, True, False, False, True, False]
State prediction error at timestep 733 is tensor(6.8112e-05, grad_fn=<MseLossBackward0>)
Current timestep = 734. State = [[-0.25699833  0.00765907]]. Action = [[ 0.00382993  0.10789478 -0.14626303 -0.929561  ]]. Reward = [1.]
Curr episode timestep = 7
Current timestep = 735. State = [[-0.24971841  0.01182361]]. Action = [[ 0.17021972 -0.04265711 -0.19164866 -0.20703197]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 736. State = [[-0.20982426  0.03008168]]. Action = [[ 0.11149153 -0.14117669 -0.09597072 -0.6906675 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 737. State = [[-0.15536837  0.04159423]]. Action = [[ 0.2138238  -0.0817183  -0.15946038  0.7935877 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 738. State = [[-0.10560113  0.04540014]]. Action = [[-0.10394572  0.07561398 -0.06109729 -0.11114997]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 739. State = [[-0.07506659  0.05004084]]. Action = [[ 0.11685529  0.20092529  0.15887457 -0.8854791 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 740. State = [[-0.05013273  0.05747835]]. Action = [[-0.03062375  0.23740819  0.17902309 -0.90946877]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 741. State = [[-0.25704968  0.0076444 ]]. Action = [[0.05870876 0.12947941 0.02074793 0.29312432]]. Reward = [1.]
Curr episode timestep = 6
Current timestep = 742. State = [[-0.24956846  0.01322241]]. Action = [[-0.13154118 -0.15421125  0.03433195  0.04360652]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 743. State = [[-0.20988786  0.03605542]]. Action = [[-0.06053415  0.24513012  0.18531758  0.06921136]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 744. State = [[-0.15896918  0.06150492]]. Action = [[-0.06671399 -0.20591299  0.11502811  0.9753511 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 745. State = [[-0.11261636  0.07255667]]. Action = [[ 0.10780162  0.18891466  0.11824083 -0.7082259 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 746. State = [[-0.0817752   0.08270359]]. Action = [[ 0.17359573  0.07700792  0.2006551  -0.8702771 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 747. State = [[-0.052998    0.09119052]]. Action = [[ 0.2228877  -0.07585081  0.12222409 -0.25562656]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 748. State = [[-0.25690594  0.00782264]]. Action = [[-0.14475752 -0.1094645   0.06465963 -0.13581783]]. Reward = [1.]
Curr episode timestep = 6
Current timestep = 749. State = [[-0.24999008  0.01294666]]. Action = [[-0.24569966  0.19031271 -0.09093149 -0.01120377]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 750. State = [[-0.2134038   0.04106418]]. Action = [[ 0.20584995  0.21788484  0.04304603 -0.9560114 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 751. State = [[-0.15691638  0.073346  ]]. Action = [[ 0.14823052 -0.21642055 -0.1145802   0.40680218]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 752. State = [[-0.10855531  0.08561893]]. Action = [[-0.21542048  0.07295513 -0.00256252 -0.8066004 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 753. State = [[-0.07500568  0.09477258]]. Action = [[ 0.15574026 -0.13967586 -0.05635349 -0.53669566]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 754. State = [[-0.04953116  0.09758137]]. Action = [[ 0.18382329  0.21124601  0.12075371 -0.28272748]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 754 is [False, True, False, False, True, False]
State prediction error at timestep 754 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 755. State = [[-0.2579107   0.00968684]]. Action = [[-0.18998072  0.15095478 -0.04276769 -0.71342325]]. Reward = [1.]
Curr episode timestep = 6
Current timestep = 756. State = [[-0.25074384  0.01534467]]. Action = [[-0.09258141  0.18273339  0.03114742  0.1459732 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 757. State = [[-0.21612966  0.04676024]]. Action = [[ 0.05785197  0.13621962 -0.01074506  0.40506577]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 758. State = [[-0.16700646  0.08399568]]. Action = [[ 0.10789108  0.08161464 -0.07602152  0.47426796]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 759. State = [[-0.11431371  0.11132182]]. Action = [[ 0.02304867  0.14683408 -0.16778919  0.43769455]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 760. State = [[-0.07826102  0.13096929]]. Action = [[ 0.08281812  0.04427269 -0.20334788 -0.44526124]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 760 is [True, False, False, False, False, True]
State prediction error at timestep 760 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 761. State = [[-0.05160695  0.14363557]]. Action = [[0.08754781 0.19279617 0.14770454 0.7159238 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 761 is [True, False, False, False, False, True]
State prediction error at timestep 761 is tensor(6.0275e-05, grad_fn=<MseLossBackward0>)
Current timestep = 762. State = [[-0.0286372   0.15634742]]. Action = [[ 0.2191256   0.00264546  0.10298046 -0.95845115]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 762 is [False, True, False, False, False, True]
State prediction error at timestep 762 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 763. State = [[-0.00466826  0.16483697]]. Action = [[ 0.11898869 -0.1310875   0.20530403 -0.94056535]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 763 is [False, True, False, False, False, True]
State prediction error at timestep 763 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 764. State = [[0.01784403 0.16632865]]. Action = [[ 0.24695176 -0.22147152  0.23906827 -0.76952946]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 764 is [False, True, False, False, False, True]
State prediction error at timestep 764 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 765. State = [[0.04195511 0.16187465]]. Action = [[ 0.19807094  0.24250978 -0.17553607 -0.375414  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 765 is [False, True, False, False, False, True]
State prediction error at timestep 765 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 766. State = [[0.06378353 0.16701277]]. Action = [[ 0.05517694  0.06832573 -0.03482364 -0.14265221]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 766 is [False, False, True, False, False, True]
State prediction error at timestep 766 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 767. State = [[0.08013662 0.17223977]]. Action = [[-0.07564723 -0.22140265 -0.15523396  0.24669886]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 767 is [False, False, True, False, False, True]
State prediction error at timestep 767 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 768. State = [[0.09452259 0.17012475]]. Action = [[ 0.06789684 -0.13123272 -0.01640886 -0.8003222 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 768 is [False, False, True, False, False, True]
State prediction error at timestep 768 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 769. State = [[0.10517642 0.16619015]]. Action = [[-0.20919512  0.08191335  0.01505476  0.79147744]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 769 is [False, False, True, False, False, True]
State prediction error at timestep 769 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 770. State = [[0.1119168 0.1659495]]. Action = [[ 0.16532266  0.19579077 -0.06695345  0.57336307]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 770 is [False, False, True, False, False, True]
State prediction error at timestep 770 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 771. State = [[0.11368137 0.16924404]]. Action = [[ 0.23217845 -0.00675653  0.22115493 -0.4086299 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 771 is [False, False, True, False, False, True]
State prediction error at timestep 771 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 772. State = [[0.1143415 0.1675049]]. Action = [[-0.02234963 -0.05216211  0.05188766 -0.26603544]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 772 is [False, False, True, False, False, True]
State prediction error at timestep 772 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 773. State = [[0.11464304 0.16533908]]. Action = [[ 0.05091479  0.20450968 -0.21220385 -0.756644  ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 773 is [False, False, True, False, False, True]
State prediction error at timestep 773 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 774. State = [[0.11378545 0.16985014]]. Action = [[-0.22552902  0.24502295  0.02148169 -0.68259716]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 774 is [False, False, True, False, False, True]
State prediction error at timestep 774 is tensor(8.3102e-05, grad_fn=<MseLossBackward0>)
Current timestep = 775. State = [[0.10999859 0.1813721 ]]. Action = [[-0.11025476  0.238713   -0.13567396 -0.3409719 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 775 is [False, False, True, False, False, True]
State prediction error at timestep 775 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 776. State = [[0.10436586 0.19500932]]. Action = [[-0.10944614 -0.14197671  0.12563169 -0.27641207]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 776 is [False, False, True, False, False, True]
State prediction error at timestep 776 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 777. State = [[0.10181643 0.20017481]]. Action = [[-0.0636501   0.0480963   0.07085517 -0.0008415 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 777 is [False, False, True, False, False, True]
State prediction error at timestep 777 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 778. State = [[0.09941741 0.20484474]]. Action = [[ 0.18247354 -0.09820876  0.22388816 -0.01203632]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 778 is [False, False, True, False, False, True]
State prediction error at timestep 778 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 779. State = [[0.09985988 0.20322922]]. Action = [[ 0.1736888   0.12181321 -0.06804085  0.9034538 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 779 is [False, False, True, False, False, True]
State prediction error at timestep 779 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 780. State = [[0.1002632  0.20260441]]. Action = [[ 0.13734126 -0.1424379  -0.05648237 -0.4401052 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 780 is [False, False, True, False, False, True]
State prediction error at timestep 780 is tensor(8.3732e-05, grad_fn=<MseLossBackward0>)
Current timestep = 781. State = [[0.10212455 0.1984288 ]]. Action = [[-0.13472879 -0.02855092 -0.12978938 -0.624627  ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 781 is [False, False, True, False, False, True]
State prediction error at timestep 781 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 782. State = [[0.10234782 0.19718705]]. Action = [[-0.1928261  -0.030267    0.21689689 -0.01644772]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 782 is [False, False, True, False, False, True]
State prediction error at timestep 782 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 783. State = [[0.10234588 0.19653463]]. Action = [[ 0.08629218 -0.22803374 -0.23935674  0.94096327]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 783 is [False, False, True, False, False, True]
State prediction error at timestep 783 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 784. State = [[0.10478251 0.18985644]]. Action = [[-0.13820411 -0.24645445 -0.07404479  0.1159122 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 784 is [False, False, True, False, False, True]
State prediction error at timestep 784 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 785. State = [[0.10750792 0.18087013]]. Action = [[-0.16795512 -0.22556248 -0.11598995 -0.6027031 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 785 is [False, False, True, False, False, True]
State prediction error at timestep 785 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 786. State = [[0.10957059 0.1716956 ]]. Action = [[ 0.08539551 -0.0941958   0.07341149 -0.24324614]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 786 is [False, False, True, False, False, True]
State prediction error at timestep 786 is tensor(6.3324e-05, grad_fn=<MseLossBackward0>)
Current timestep = 787. State = [[0.11192049 0.16346493]]. Action = [[0.06687397 0.06975096 0.08435139 0.7845938 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 787 is [False, False, True, False, False, True]
State prediction error at timestep 787 is tensor(1.3909e-05, grad_fn=<MseLossBackward0>)
Current timestep = 788. State = [[0.11276181 0.16016069]]. Action = [[ 0.05580819  0.04856664 -0.1203696   0.60011864]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 788 is [False, False, True, False, False, True]
State prediction error at timestep 788 is tensor(3.3825e-05, grad_fn=<MseLossBackward0>)
Current timestep = 789. State = [[0.1130604  0.15878005]]. Action = [[-0.13624467 -0.01933709  0.1491127   0.9016974 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 789 is [False, False, True, False, False, True]
State prediction error at timestep 789 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 790. State = [[0.11307909 0.15847798]]. Action = [[0.07290727 0.03401765 0.24257296 0.85527945]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 790 is [False, False, True, False, False, True]
State prediction error at timestep 790 is tensor(4.7643e-06, grad_fn=<MseLossBackward0>)
Current timestep = 791. State = [[0.11303292 0.15870188]]. Action = [[-0.227462    0.22871095  0.18803185  0.7587676 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 791 is [False, False, True, False, False, True]
State prediction error at timestep 791 is tensor(1.1994e-05, grad_fn=<MseLossBackward0>)
Current timestep = 792. State = [[0.1099073  0.16642638]]. Action = [[0.01907602 0.05108625 0.17893839 0.03628147]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 792 is [False, False, True, False, False, True]
State prediction error at timestep 792 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 793. State = [[0.10768608 0.17201339]]. Action = [[-0.01712193  0.11918393 -0.06108309  0.4907298 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 793 is [False, False, True, False, False, True]
State prediction error at timestep 793 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 794. State = [[0.1050067  0.17861918]]. Action = [[ 0.06989247  0.22773567 -0.14790137 -0.71065646]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 794 is [False, False, True, False, False, True]
State prediction error at timestep 794 is tensor(9.2221e-05, grad_fn=<MseLossBackward0>)
Current timestep = 795. State = [[0.10143942 0.1873942 ]]. Action = [[ 0.19371247 -0.19216922 -0.17749646 -0.94973105]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 795 is [False, False, True, False, False, True]
State prediction error at timestep 795 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 796. State = [[0.10219552 0.18588775]]. Action = [[ 0.12580603  0.16166067 -0.14672652 -0.65098834]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 796 is [False, False, True, False, False, True]
State prediction error at timestep 796 is tensor(1.4505e-05, grad_fn=<MseLossBackward0>)
Current timestep = 797. State = [[0.10224679 0.18720053]]. Action = [[-0.08531216 -0.06834371 -0.07484366 -0.66748464]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 797 is [False, False, True, False, False, True]
State prediction error at timestep 797 is tensor(5.2834e-05, grad_fn=<MseLossBackward0>)
Current timestep = 798. State = [[0.10226052 0.18706799]]. Action = [[-0.08866635 -0.13081293  0.223391    0.41469526]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 798 is [False, False, True, False, False, True]
State prediction error at timestep 798 is tensor(1.1220e-05, grad_fn=<MseLossBackward0>)
Current timestep = 799. State = [[0.10313298 0.18441333]]. Action = [[ 0.21510172 -0.22107461 -0.22739607 -0.910183  ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 799 is [False, False, True, False, False, True]
State prediction error at timestep 799 is tensor(4.4682e-07, grad_fn=<MseLossBackward0>)
Current timestep = 800. State = [[0.10601557 0.17628986]]. Action = [[-0.10830641  0.12613523 -0.04493284  0.13734818]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 800 is [False, False, True, False, False, True]
State prediction error at timestep 800 is tensor(2.9247e-05, grad_fn=<MseLossBackward0>)
Current timestep = 801. State = [[0.10586946 0.17581993]]. Action = [[ 0.10175508  0.11225617 -0.04003504  0.30539095]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 801 is [False, False, True, False, False, True]
State prediction error at timestep 801 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 802. State = [[0.10557833 0.17678812]]. Action = [[ 0.03181821 -0.05642019 -0.2096479  -0.08226228]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 802 is [False, False, True, False, False, True]
State prediction error at timestep 802 is tensor(8.4266e-06, grad_fn=<MseLossBackward0>)
Current timestep = 803. State = [[0.10610324 0.17604855]]. Action = [[-0.20109232  0.0690057  -0.0948      0.08266008]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 803 is [False, False, True, False, False, True]
State prediction error at timestep 803 is tensor(8.5466e-05, grad_fn=<MseLossBackward0>)
Current timestep = 804. State = [[0.10478733 0.17897734]]. Action = [[-0.13722377  0.07320619 -0.22038978  0.7997465 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 804 is [False, False, True, False, False, True]
State prediction error at timestep 804 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 805. State = [[0.10299349 0.18319786]]. Action = [[-0.21679385 -0.07000688 -0.08096789  0.7431091 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 805 is [False, False, True, False, False, True]
State prediction error at timestep 805 is tensor(9.0742e-05, grad_fn=<MseLossBackward0>)
Current timestep = 806. State = [[0.10155742 0.18609798]]. Action = [[ 0.01974475 -0.24036346  0.14396772 -0.01886618]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 806 is [False, False, True, False, False, True]
State prediction error at timestep 806 is tensor(7.8175e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 806 of -1
Current timestep = 807. State = [[0.10268553 0.18241224]]. Action = [[ 0.22791433  0.1476208  -0.2192783  -0.6092981 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 807 is [False, False, True, False, False, True]
State prediction error at timestep 807 is tensor(6.6339e-06, grad_fn=<MseLossBackward0>)
Current timestep = 808. State = [[0.10276133 0.18227574]]. Action = [[ 0.11512965  0.11513934  0.0859502  -0.1251247 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 808 is [False, False, True, False, False, True]
State prediction error at timestep 808 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 809. State = [[0.10242313 0.18324971]]. Action = [[-0.24642606  0.04622769 -0.13100386  0.0027703 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 809 is [False, False, True, False, False, True]
State prediction error at timestep 809 is tensor(9.0888e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 809 of -1
Current timestep = 810. State = [[0.10079021 0.18735696]]. Action = [[0.22262824 0.00854319 0.19025248 0.94466805]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 810 is [False, False, True, False, False, True]
State prediction error at timestep 810 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 811. State = [[0.10099186 0.18716256]]. Action = [[-0.18276997 -0.23384583 -0.16108832 -0.5828173 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 811 is [False, False, True, False, False, True]
State prediction error at timestep 811 is tensor(1.8166e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 811 of -1
Current timestep = 812. State = [[0.10224973 0.18384936]]. Action = [[-0.09865315  0.05864558 -0.06261358  0.40301847]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 812 is [False, False, True, False, False, True]
State prediction error at timestep 812 is tensor(9.5809e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 812 of -1
Current timestep = 813. State = [[0.10199002 0.18453872]]. Action = [[ 0.10610482  0.07546252  0.0886299  -0.66194797]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 813 is [False, False, True, False, False, True]
State prediction error at timestep 813 is tensor(4.5420e-05, grad_fn=<MseLossBackward0>)
Current timestep = 814. State = [[0.10138864 0.18622339]]. Action = [[-0.14025429  0.24316984 -0.23525515 -0.5245042 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 814 is [False, False, True, False, False, True]
State prediction error at timestep 814 is tensor(5.5093e-05, grad_fn=<MseLossBackward0>)
Current timestep = 815. State = [[0.09845666 0.19305769]]. Action = [[ 0.23116028 -0.21429105  0.00724095  0.12758684]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 815 is [False, False, True, False, False, True]
State prediction error at timestep 815 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 816. State = [[0.09949132 0.19069782]]. Action = [[ 0.13710767  0.24716616  0.1396653  -0.21748257]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 816 is [False, False, True, False, False, True]
State prediction error at timestep 816 is tensor(3.9829e-05, grad_fn=<MseLossBackward0>)
Current timestep = 817. State = [[0.09843814 0.1936048 ]]. Action = [[-0.05905415  0.14886197 -0.06180362 -0.9702055 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 817 is [False, False, True, False, False, True]
State prediction error at timestep 817 is tensor(4.5107e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 817 of -1
Current timestep = 818. State = [[0.09588589 0.19966067]]. Action = [[ 0.13710645  0.22212479 -0.09728298 -0.41472328]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 818 is [False, False, True, False, False, True]
State prediction error at timestep 818 is tensor(3.3840e-05, grad_fn=<MseLossBackward0>)
Current timestep = 819. State = [[0.09369282 0.20642872]]. Action = [[-0.09100977 -0.24695669  0.05062562  0.556617  ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 819 is [False, False, True, False, False, True]
State prediction error at timestep 819 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 819 of -1
Current timestep = 820. State = [[0.09408048 0.2051491 ]]. Action = [[0.18676269 0.01193991 0.09932935 0.8486862 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 820 is [False, False, True, False, False, True]
State prediction error at timestep 820 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 820 of -1
Current timestep = 821. State = [[0.09556245 0.20396806]]. Action = [[-0.14676882  0.20632625 -0.17658247 -0.8644423 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 821 is [False, False, True, False, False, True]
State prediction error at timestep 821 is tensor(6.8624e-06, grad_fn=<MseLossBackward0>)
Current timestep = 822. State = [[0.09366903 0.20932372]]. Action = [[-0.10081665 -0.16404428  0.03535646 -0.87946475]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 822 is [False, False, True, False, False, True]
State prediction error at timestep 822 is tensor(4.8772e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 822 of -1
Current timestep = 823. State = [[0.09359223 0.20925577]]. Action = [[-0.15575106 -0.07817158 -0.1164932  -0.2986064 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 823 is [False, False, True, False, False, True]
State prediction error at timestep 823 is tensor(3.9644e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 823 of -1
Current timestep = 824. State = [[0.09360627 0.20878966]]. Action = [[-0.16403142 -0.14790264  0.22688812 -0.07772243]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 824 is [False, False, True, False, False, True]
State prediction error at timestep 824 is tensor(2.5067e-05, grad_fn=<MseLossBackward0>)
Current timestep = 825. State = [[0.09353246 0.20730132]]. Action = [[-0.18131332  0.22939575  0.09368515 -0.1295566 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 825 is [False, False, True, False, False, True]
State prediction error at timestep 825 is tensor(6.2312e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 825 of -1
Current timestep = 826. State = [[0.09035629 0.21339428]]. Action = [[ 0.22336972 -0.02642396 -0.16367246  0.59389925]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 826 is [False, False, True, False, False, True]
State prediction error at timestep 826 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 826 of -1
Current timestep = 827. State = [[0.09134975 0.21354797]]. Action = [[ 0.22088385 -0.21964404 -0.13958822  0.10181928]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 827 is [False, False, True, False, False, True]
State prediction error at timestep 827 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 827 of -1
Current timestep = 828. State = [[0.09558295 0.20705582]]. Action = [[ 0.09552434 -0.12269041  0.00631419  0.38690555]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 828 is [False, False, True, False, False, True]
State prediction error at timestep 828 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 829. State = [[0.09937301 0.19950385]]. Action = [[ 0.12459615  0.10175711 -0.10733896 -0.9946342 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 829 is [False, False, True, False, False, True]
State prediction error at timestep 829 is tensor(3.5182e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 829 of -1
Current timestep = 830. State = [[0.10097425 0.19655773]]. Action = [[-0.0547259  -0.00104575  0.20980692 -0.14074504]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 830 is [False, False, True, False, False, True]
State prediction error at timestep 830 is tensor(3.7468e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 830 of -1
Current timestep = 831. State = [[0.10139696 0.1956485 ]]. Action = [[-0.00874057  0.01573431  0.14102262  0.11894965]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 831 is [False, False, True, False, False, True]
State prediction error at timestep 831 is tensor(1.3477e-05, grad_fn=<MseLossBackward0>)
Current timestep = 832. State = [[0.10155478 0.19536011]]. Action = [[-0.21147919 -0.22068617  0.22154826 -0.65196663]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 832 is [False, False, True, False, False, True]
State prediction error at timestep 832 is tensor(6.1707e-06, grad_fn=<MseLossBackward0>)
Current timestep = 833. State = [[0.10281119 0.19164118]]. Action = [[ 0.07722238 -0.10937665 -0.06204179  0.8659662 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 833 is [False, False, True, False, False, True]
State prediction error at timestep 833 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 834. State = [[0.10498864 0.18583174]]. Action = [[-0.22274159 -0.03477871 -0.10645197  0.36943388]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 834 is [False, False, True, False, False, True]
State prediction error at timestep 834 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 835. State = [[0.10488635 0.18440385]]. Action = [[-0.01046607 -0.0218682   0.20395559  0.01025748]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 835 is [False, False, True, False, False, True]
State prediction error at timestep 835 is tensor(1.2695e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 835 of -1
Current timestep = 836. State = [[0.10509728 0.18321308]]. Action = [[0.10459772 0.02182931 0.20794868 0.82166886]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 836 is [False, False, True, False, False, True]
State prediction error at timestep 836 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 837. State = [[0.10543902 0.18204175]]. Action = [[-0.17228033 -0.04447657 -0.21817197 -0.95634615]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 837 is [False, False, True, False, False, True]
State prediction error at timestep 837 is tensor(2.0207e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 837 of -1
Current timestep = 838. State = [[0.10496846 0.18200506]]. Action = [[ 0.09452146  0.12827271  0.24090844 -0.47214997]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 838 is [False, False, True, False, False, True]
State prediction error at timestep 838 is tensor(9.7907e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 838 of -1
Current timestep = 839. State = [[0.10433523 0.18353388]]. Action = [[ 0.23291689  0.19062713  0.2053811  -0.69527066]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 839 is [False, False, True, False, False, True]
State prediction error at timestep 839 is tensor(9.2005e-05, grad_fn=<MseLossBackward0>)
Current timestep = 840. State = [[0.10345592 0.1862727 ]]. Action = [[ 0.15973043  0.15592548  0.08293515 -0.19829208]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 840 is [False, False, True, False, False, True]
State prediction error at timestep 840 is tensor(9.2049e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 840 of -1
Current timestep = 841. State = [[0.10228816 0.19065098]]. Action = [[-0.19318303  0.16130278  0.02978766 -0.49191165]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 841 is [False, False, True, False, False, True]
State prediction error at timestep 841 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 842. State = [[0.09899824 0.19880986]]. Action = [[-0.14355548  0.13972467 -0.02359034  0.02710986]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 842 is [False, False, True, False, False, True]
State prediction error at timestep 842 is tensor(9.9286e-05, grad_fn=<MseLossBackward0>)
Current timestep = 843. State = [[0.09451424 0.20916341]]. Action = [[-0.00993751  0.0418157   0.13043445  0.4241519 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 843 is [False, False, True, False, False, True]
State prediction error at timestep 843 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 843 of -1
Current timestep = 844. State = [[0.0917226 0.2153614]]. Action = [[ 0.1376189  -0.15196817 -0.21444084  0.13514245]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 844 is [False, False, True, False, False, True]
State prediction error at timestep 844 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 845. State = [[0.0923016  0.21408421]]. Action = [[-0.12763281  0.00148878 -0.1011351  -0.6424983 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 845 is [False, False, True, False, False, True]
State prediction error at timestep 845 is tensor(4.1858e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 845 of -1
Current timestep = 846. State = [[0.09211303 0.21464916]]. Action = [[-0.14626864 -0.09427762  0.05768692  0.76195097]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 846 is [False, False, True, False, False, True]
State prediction error at timestep 846 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 847. State = [[0.09188353 0.21484548]]. Action = [[-0.09004259  0.21895492  0.14025277  0.5526309 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 847 is [False, False, True, False, False, True]
State prediction error at timestep 847 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 847 of -1
Current timestep = 848. State = [[0.0893445 0.2202944]]. Action = [[ 0.01763397 -0.22184512 -0.05733587 -0.5395057 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 848 is [False, False, True, False, False, True]
State prediction error at timestep 848 is tensor(3.2705e-05, grad_fn=<MseLossBackward0>)
Current timestep = 849. State = [[0.09001178 0.21838294]]. Action = [[ 1.1129984e-01 -1.5147030e-01 -4.3778121e-04 -9.0186286e-01]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 849 is [False, False, True, False, False, True]
State prediction error at timestep 849 is tensor(2.6584e-05, grad_fn=<MseLossBackward0>)
Current timestep = 850. State = [[0.0926059  0.21221706]]. Action = [[-0.14979748 -0.20557608 -0.14509144 -0.52536976]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 850 is [False, False, True, False, False, True]
State prediction error at timestep 850 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 850 of -1
Current timestep = 851. State = [[0.09469181 0.20599571]]. Action = [[0.17431018 0.19753218 0.2410731  0.8294159 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 851 is [False, False, True, False, False, True]
State prediction error at timestep 851 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 852. State = [[0.09475562 0.20583847]]. Action = [[-0.15808153  0.18096507  0.18158805 -0.6545962 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 852 is [False, False, True, False, False, True]
State prediction error at timestep 852 is tensor(3.4891e-05, grad_fn=<MseLossBackward0>)
Current timestep = 853. State = [[0.09234273 0.21107034]]. Action = [[-0.15218952 -0.14773984 -0.15027806 -0.00853002]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 853 is [False, False, True, False, False, True]
State prediction error at timestep 853 is tensor(3.5595e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 853 of -1
Current timestep = 854. State = [[0.09182558 0.21148825]]. Action = [[ 0.08137938 -0.05036743 -0.18443435  0.5957012 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 854 is [False, False, True, False, False, True]
State prediction error at timestep 854 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 855. State = [[0.09224932 0.21025388]]. Action = [[ 0.0440481   0.14202213 -0.09752324  0.7853739 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 855 is [False, False, True, False, False, True]
State prediction error at timestep 855 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 855 of -1
Current timestep = 856. State = [[0.09147076 0.21193586]]. Action = [[-0.03701553 -0.026134   -0.11379392 -0.8841688 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 856 is [False, False, True, False, False, True]
State prediction error at timestep 856 is tensor(1.5087e-05, grad_fn=<MseLossBackward0>)
Current timestep = 857. State = [[0.09118284 0.2125473 ]]. Action = [[ 0.16003624  0.07152247 -0.21676388 -0.38223302]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 857 is [False, False, True, False, False, True]
State prediction error at timestep 857 is tensor(3.7552e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 857 of -1
Current timestep = 858. State = [[0.09121896 0.21249321]]. Action = [[-0.08176503 -0.12835719 -0.23384985 -0.40088487]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 858 is [False, False, True, False, False, True]
State prediction error at timestep 858 is tensor(7.3061e-05, grad_fn=<MseLossBackward0>)
Current timestep = 859. State = [[0.0920559 0.2103743]]. Action = [[ 0.14404511 -0.13148552  0.04643923  0.7767072 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 859 is [False, False, True, False, False, True]
State prediction error at timestep 859 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 860. State = [[0.09457695 0.20459816]]. Action = [[ 0.18721613 -0.22393006 -0.03275216 -0.23904341]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 860 is [False, False, True, False, False, True]
State prediction error at timestep 860 is tensor(3.6865e-05, grad_fn=<MseLossBackward0>)
Current timestep = 861. State = [[0.09863685 0.19462128]]. Action = [[ 0.10495636  0.06353354 -0.04439999  0.01330686]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 861 is [False, False, True, False, False, True]
State prediction error at timestep 861 is tensor(1.7597e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 861 of -1
Current timestep = 862. State = [[0.10094005 0.18945758]]. Action = [[-0.20070817  0.22660139 -0.1188482  -0.71348715]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 862 is [False, False, True, False, False, True]
State prediction error at timestep 862 is tensor(4.4380e-05, grad_fn=<MseLossBackward0>)
Current timestep = 863. State = [[0.0994317  0.19313686]]. Action = [[ 0.18851686 -0.23899598 -0.16135333 -0.21619868]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 863 is [False, False, True, False, False, True]
State prediction error at timestep 863 is tensor(3.7891e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 863 of -1
Current timestep = 864. State = [[0.10267915 0.18597202]]. Action = [[-0.06915672  0.07850295  0.2259297  -0.3066728 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 864 is [False, False, True, False, False, True]
State prediction error at timestep 864 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 864 of -1
Current timestep = 865. State = [[0.10258403 0.18630856]]. Action = [[-0.11692068 -0.01610085  0.05038658 -0.10721833]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 865 is [False, False, True, False, False, True]
State prediction error at timestep 865 is tensor(6.1249e-05, grad_fn=<MseLossBackward0>)
Current timestep = 866. State = [[0.10205555 0.18746622]]. Action = [[-0.0896374  -0.02094659  0.0920476   0.10967946]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 866 is [False, False, True, False, False, True]
State prediction error at timestep 866 is tensor(9.3466e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 866 of -1
Current timestep = 867. State = [[0.10179616 0.18789142]]. Action = [[-0.18755093 -0.19245583  0.03889695  0.5024017 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 867 is [False, False, True, False, False, True]
State prediction error at timestep 867 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 867 of -1
Current timestep = 868. State = [[0.10217615 0.18577233]]. Action = [[-0.03242385 -0.02868366 -0.2145407  -0.19548202]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 868 is [False, False, True, False, False, True]
State prediction error at timestep 868 is tensor(7.2478e-05, grad_fn=<MseLossBackward0>)
Current timestep = 869. State = [[0.10226303 0.18493067]]. Action = [[ 0.19965231  0.2256999  -0.14151314 -0.70806307]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 869 is [False, False, True, False, False, True]
State prediction error at timestep 869 is tensor(6.4347e-05, grad_fn=<MseLossBackward0>)
Current timestep = 870. State = [[0.1011906  0.18754703]]. Action = [[0.22193018 0.16038272 0.22000831 0.98307395]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 870 is [False, False, True, False, False, True]
State prediction error at timestep 870 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 870 of -1
Current timestep = 871. State = [[0.10052909 0.19007413]]. Action = [[-0.04349996  0.18698046 -0.06824876  0.24742699]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 871 is [False, False, True, False, False, True]
State prediction error at timestep 871 is tensor(6.3503e-05, grad_fn=<MseLossBackward0>)
Current timestep = 872. State = [[0.09844942 0.19710486]]. Action = [[ 0.0871442  0.0876773 -0.17311    0.629864 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 872 is [False, False, True, False, False, True]
State prediction error at timestep 872 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 872 of -1
Current timestep = 873. State = [[0.09762956 0.20169497]]. Action = [[ 0.00290793 -0.17120478  0.18331152 -0.82604504]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 873 is [False, False, True, False, False, True]
State prediction error at timestep 873 is tensor(4.4410e-06, grad_fn=<MseLossBackward0>)
Current timestep = 874. State = [[0.09883413 0.20104223]]. Action = [[-0.01779149  0.24320108 -0.17240702  0.61191106]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 874 is [False, False, True, False, False, True]
State prediction error at timestep 874 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 875. State = [[0.09755822 0.20628557]]. Action = [[ 0.14828497 -0.13576274  0.11656055 -0.8749629 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 875 is [False, False, True, False, False, True]
State prediction error at timestep 875 is tensor(6.1077e-05, grad_fn=<MseLossBackward0>)
Current timestep = 876. State = [[0.09864162 0.20448008]]. Action = [[-0.13110115 -0.17607507 -0.22720541 -0.8649062 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 876 is [False, False, True, False, False, True]
State prediction error at timestep 876 is tensor(4.4068e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 876 of 1
Current timestep = 877. State = [[0.10027508 0.20039421]]. Action = [[ 0.22625136 -0.1732244   0.21899313  0.5656873 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 877 is [False, False, True, False, False, True]
State prediction error at timestep 877 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 878. State = [[0.10378654 0.19169416]]. Action = [[-0.07618937 -0.23560861  0.12165219  0.5717367 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 878 is [False, False, True, False, False, True]
State prediction error at timestep 878 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 879. State = [[0.10760597 0.1818516 ]]. Action = [[-0.23213333 -0.01203358  0.22652945 -0.75413334]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 879 is [False, False, True, False, False, True]
State prediction error at timestep 879 is tensor(1.3587e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 879 of -1
Current timestep = 880. State = [[0.10867289 0.1781969 ]]. Action = [[-0.0365127  -0.0459037  -0.14430597  0.96861815]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 880 is [False, False, True, False, False, True]
State prediction error at timestep 880 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 881. State = [[0.10926471 0.1756671 ]]. Action = [[ 0.11739251 -0.00276344  0.06002742 -0.6363523 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 881 is [False, False, True, False, False, True]
State prediction error at timestep 881 is tensor(2.3810e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 881 of -1
Current timestep = 882. State = [[0.11013237 0.17300773]]. Action = [[-0.03769396  0.13283402 -0.0498835   0.22079766]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 882 is [False, False, True, False, False, True]
State prediction error at timestep 882 is tensor(1.8379e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 882 of -1
Current timestep = 883. State = [[0.10936816 0.17497958]]. Action = [[ 0.17393297  0.22887531 -0.21061793  0.5727179 ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 883 is [False, False, True, False, False, True]
State prediction error at timestep 883 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 884. State = [[0.10782116 0.17965494]]. Action = [[-0.03935741  0.13008165  0.24168485 -0.7814867 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 884 is [False, False, True, False, False, True]
State prediction error at timestep 884 is tensor(6.0276e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 884 of -1
Current timestep = 885. State = [[0.10568938 0.18559171]]. Action = [[ 0.06976175  0.13040674 -0.02636985  0.50151837]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 885 is [False, False, True, False, False, True]
State prediction error at timestep 885 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 885 of -1
Current timestep = 886. State = [[0.1036266 0.1912267]]. Action = [[ 0.12334079 -0.18750012 -0.16826896  0.94949794]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 886 is [False, False, True, False, False, True]
State prediction error at timestep 886 is tensor(1.0755e-05, grad_fn=<MseLossBackward0>)
Current timestep = 887. State = [[0.10471314 0.18918298]]. Action = [[ 0.13341749  0.05174804 -0.15243673  0.4710555 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 887 is [False, False, True, False, False, True]
State prediction error at timestep 887 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 888. State = [[0.10608492 0.18824789]]. Action = [[ 0.02082282 -0.16601847 -0.19732083  0.8230083 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 888 is [False, False, True, False, False, True]
State prediction error at timestep 888 is tensor(6.0900e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 888 of -1
Current timestep = 889. State = [[0.10861237 0.18280701]]. Action = [[ 0.24367326 -0.22206323  0.20239231  0.41234744]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 889 is [False, False, True, False, False, True]
State prediction error at timestep 889 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 890. State = [[0.11375885 0.17248054]]. Action = [[-0.02357978  0.08620688  0.10132593 -0.37899017]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 890 is [False, False, True, False, False, True]
State prediction error at timestep 890 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 891. State = [[0.11544639 0.16918848]]. Action = [[-0.19187352  0.17749727  0.21848333 -0.8264769 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 891 is [False, False, True, False, False, True]
State prediction error at timestep 891 is tensor(1.1524e-05, grad_fn=<MseLossBackward0>)
Current timestep = 892. State = [[0.11406835 0.17286901]]. Action = [[-0.17858303 -0.0907003  -0.17286424  0.37150455]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 892 is [False, False, True, False, False, True]
State prediction error at timestep 892 is tensor(3.2495e-05, grad_fn=<MseLossBackward0>)
Current timestep = 893. State = [[0.11370742 0.1735595 ]]. Action = [[ 0.01768583 -0.15726829  0.16894427 -0.1999656 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 893 is [False, False, True, False, False, True]
State prediction error at timestep 893 is tensor(5.0146e-05, grad_fn=<MseLossBackward0>)
Current timestep = 894. State = [[0.11458648 0.17098074]]. Action = [[ 0.1494422   0.13758889 -0.05961427 -0.8369211 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 894 is [False, False, True, False, False, True]
State prediction error at timestep 894 is tensor(2.1122e-05, grad_fn=<MseLossBackward0>)
Current timestep = 895. State = [[0.11449469 0.17149042]]. Action = [[ 0.23411715 -0.16014023 -0.22219372  0.5530585 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 895 is [False, False, True, False, False, True]
State prediction error at timestep 895 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 895 of -1
Current timestep = 896. State = [[0.11717496 0.16553289]]. Action = [[-0.19889559 -0.06520715  0.06692746  0.3053509 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 896 is [False, False, True, False, False, True]
State prediction error at timestep 896 is tensor(3.4571e-05, grad_fn=<MseLossBackward0>)
Current timestep = 897. State = [[0.11805505 0.16296752]]. Action = [[-0.12812464  0.0482696   0.13278568  0.34395218]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 897 is [False, False, True, False, False, True]
State prediction error at timestep 897 is tensor(4.6603e-06, grad_fn=<MseLossBackward0>)
Current timestep = 898. State = [[0.11769963 0.16365965]]. Action = [[-0.19111568  0.15366703  0.18080896  0.44379056]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 898 is [False, False, True, False, False, True]
State prediction error at timestep 898 is tensor(4.1504e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 898 of -1
Current timestep = 899. State = [[0.11483943 0.17059161]]. Action = [[-0.23750338  0.22519329  0.02824605 -0.34613526]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 899 is [False, False, True, False, False, True]
State prediction error at timestep 899 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 900. State = [[0.11053243 0.18118282]]. Action = [[ 0.02824605  0.1975596  -0.18910389 -0.91354275]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 900 is [False, False, True, False, False, True]
State prediction error at timestep 900 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 901. State = [[0.10576846 0.19258219]]. Action = [[-0.1738401  -0.09480649  0.17518473  0.2104001 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 901 is [False, False, True, False, False, True]
State prediction error at timestep 901 is tensor(6.7477e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 901 of -1
Current timestep = 902. State = [[0.1028799 0.1989723]]. Action = [[-0.14578038 -0.14555171 -0.1586393   0.8723755 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 902 is [False, False, True, False, False, True]
State prediction error at timestep 902 is tensor(8.2700e-05, grad_fn=<MseLossBackward0>)
Current timestep = 903. State = [[0.1020086  0.19978106]]. Action = [[ 0.22853118  0.2126235  -0.06395763  0.14116299]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 903 is [False, False, True, False, False, True]
State prediction error at timestep 903 is tensor(3.4160e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 903 of -1
Current timestep = 904. State = [[0.1005998  0.20303407]]. Action = [[ 0.19096014 -0.00426348  0.13203946 -0.79871595]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 904 is [False, False, True, False, False, True]
State prediction error at timestep 904 is tensor(3.1811e-05, grad_fn=<MseLossBackward0>)
Current timestep = 905. State = [[0.10117453 0.20233445]]. Action = [[-0.07633427 -0.19553924  0.13027632 -0.5027113 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 905 is [False, False, True, False, False, True]
State prediction error at timestep 905 is tensor(4.1564e-05, grad_fn=<MseLossBackward0>)
Current timestep = 906. State = [[0.10264014 0.1985441 ]]. Action = [[-0.01559901 -0.22216626 -0.10462913 -0.7706285 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 906 is [False, False, True, False, False, True]
State prediction error at timestep 906 is tensor(5.7297e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 906 of -1
Current timestep = 907. State = [[0.10545905 0.19116648]]. Action = [[-0.1862333   0.17022094 -0.21018367 -0.3396014 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 907 is [False, False, True, False, False, True]
State prediction error at timestep 907 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 907 of -1
Current timestep = 908. State = [[0.10402656 0.1937843 ]]. Action = [[-0.02958985  0.2298196   0.13035214 -0.02119935]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 908 is [False, False, True, False, False, True]
State prediction error at timestep 908 is tensor(5.6383e-05, grad_fn=<MseLossBackward0>)
Current timestep = 909. State = [[0.10105086 0.20022264]]. Action = [[-0.10514365 -0.21949255  0.16973284 -0.846767  ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 909 is [False, False, True, False, False, True]
State prediction error at timestep 909 is tensor(6.5510e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 909 of -1
Current timestep = 910. State = [[0.10103502 0.1991507 ]]. Action = [[ 0.18365866 -0.17387196  0.11786133  0.7428155 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 910 is [False, False, True, False, False, True]
State prediction error at timestep 910 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 910 of -1
Current timestep = 911. State = [[0.10352518 0.19290149]]. Action = [[-0.12741424 -0.0827415  -0.227707   -0.159904  ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 911 is [False, False, True, False, False, True]
State prediction error at timestep 911 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 911 of -1
Current timestep = 912. State = [[0.1042637  0.19002438]]. Action = [[-0.23076756  0.22229642  0.0914745  -0.80776805]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 912 is [False, False, True, False, False, True]
State prediction error at timestep 912 is tensor(1.6852e-05, grad_fn=<MseLossBackward0>)
Current timestep = 913. State = [[0.10133257 0.19482742]]. Action = [[ 0.16448975 -0.15164809 -0.21228443 -0.35733104]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 913 is [False, False, True, False, False, True]
State prediction error at timestep 913 is tensor(1.1231e-05, grad_fn=<MseLossBackward0>)
Current timestep = 914. State = [[0.10219177 0.19246583]]. Action = [[0.1559928  0.17499971 0.21064728 0.6948981 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 914 is [False, False, True, False, False, True]
State prediction error at timestep 914 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 914 of -1
Current timestep = 915. State = [[0.10164207 0.19388056]]. Action = [[-0.20911513  0.08332929 -0.2311051  -0.96081084]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 915 is [False, False, True, False, False, True]
State prediction error at timestep 915 is tensor(1.0352e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 915 of -1
Current timestep = 916. State = [[0.09926458 0.1992434 ]]. Action = [[-0.13965186  0.18337506 -0.11311549  0.06128502]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 916 is [False, False, True, False, False, True]
State prediction error at timestep 916 is tensor(8.8845e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 916 of -1
Current timestep = 917. State = [[0.09510981 0.20856667]]. Action = [[ 0.23486775  0.19734195 -0.1658791   0.73604596]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 917 is [False, False, True, False, False, True]
State prediction error at timestep 917 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 918. State = [[0.09195517 0.21575788]]. Action = [[ 0.2419697   0.10106552 -0.05667564  0.9026351 ]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 918 is [False, False, True, False, False, True]
State prediction error at timestep 918 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 918 of 1
Current timestep = 919. State = [[0.09149238 0.21922942]]. Action = [[-0.08437312 -0.09525847 -0.050853    0.85312104]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 919 is [False, False, True, False, False, True]
State prediction error at timestep 919 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 920. State = [[0.09178956 0.21947268]]. Action = [[-0.02763034 -0.21340835  0.06464076  0.6186657 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 920 is [False, False, True, False, False, True]
State prediction error at timestep 920 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 920 of 1
Current timestep = 921. State = [[0.09337573 0.21597157]]. Action = [[-0.17082909  0.07865152 -0.14783236 -0.8869263 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 921 is [False, False, True, False, False, True]
State prediction error at timestep 921 is tensor(2.5350e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 921 of 1
Current timestep = 922. State = [[0.09269337 0.21735315]]. Action = [[ 0.07548067 -0.17466144 -0.23758058 -0.96630335]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 922 is [False, False, True, False, False, True]
State prediction error at timestep 922 is tensor(2.5386e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 922 of 1
Current timestep = 923. State = [[0.09412486 0.2139486 ]]. Action = [[-0.23092332  0.05797905  0.15463984 -0.256379  ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 923 is [False, False, True, False, False, True]
State prediction error at timestep 923 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 924. State = [[0.09296912 0.21605963]]. Action = [[-0.19944824  0.05358517  0.0389283  -0.62724066]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 924 is [False, False, True, False, False, True]
State prediction error at timestep 924 is tensor(6.2419e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 924 of 1
Current timestep = 925. State = [[0.08996645 0.22049503]]. Action = [[-0.18089877  0.15400368 -0.23383218  0.8468745 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 925 is [False, False, True, False, False, True]
State prediction error at timestep 925 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 926. State = [[0.08586307 0.22794189]]. Action = [[ 0.19996268 -0.20425208 -0.05219753 -0.8076987 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 926 is [False, False, True, False, False, True]
State prediction error at timestep 926 is tensor(8.3571e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 926 of -1
Current timestep = 927. State = [[0.08647012 0.22600798]]. Action = [[-0.21608558  0.07147071  0.20067996  0.43062425]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 927 is [False, False, True, False, False, True]
State prediction error at timestep 927 is tensor(5.2679e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 927 of -1
Current timestep = 928. State = [[0.08494662 0.22836629]]. Action = [[-0.08587396 -0.17423718 -0.1267193  -0.0543707 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 928 is [False, False, True, False, False, True]
State prediction error at timestep 928 is tensor(6.9706e-06, grad_fn=<MseLossBackward0>)
Current timestep = 929. State = [[0.08483185 0.22683708]]. Action = [[-0.02096379 -0.04153125 -0.06436983  0.8515917 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 929 is [False, False, True, False, False, True]
State prediction error at timestep 929 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 930. State = [[0.0849125 0.2249049]]. Action = [[ 0.01326612 -0.14527686  0.1825372  -0.79850197]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 930 is [False, False, True, False, False, True]
State prediction error at timestep 930 is tensor(3.1620e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 930 of 1
Current timestep = 931. State = [[0.08638044 0.22014374]]. Action = [[ 0.11017695 -0.0543288  -0.09115326  0.1514169 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 931 is [False, False, True, False, False, True]
State prediction error at timestep 931 is tensor(4.5715e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 931 of 1
Current timestep = 932. State = [[0.08840251 0.21466678]]. Action = [[ 1.1590332e-01  2.7990341e-04 -6.1268568e-02 -6.1289650e-01]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 932 is [False, False, True, False, False, True]
State prediction error at timestep 932 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 932 of 1
Current timestep = 933. State = [[0.08992945 0.21055657]]. Action = [[-0.09335613  0.22715753  0.15420234  0.1174053 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 933 is [False, False, True, False, False, True]
State prediction error at timestep 933 is tensor(1.5499e-05, grad_fn=<MseLossBackward0>)
Current timestep = 934. State = [[0.08781777 0.21494578]]. Action = [[-0.10281932  0.15525162 -0.21002226 -0.24367428]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 934 is [False, False, True, False, False, True]
State prediction error at timestep 934 is tensor(8.5550e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 934 of 1
Current timestep = 935. State = [[0.08489741 0.22126088]]. Action = [[-0.0298385  -0.1913845  -0.11226785  0.44781697]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 935 is [False, False, True, False, False, True]
State prediction error at timestep 935 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 935 of 1
Current timestep = 936. State = [[0.08509162 0.22029994]]. Action = [[ 0.22126275 -0.098977    0.19870132 -0.335052  ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 936 is [False, False, True, False, False, True]
State prediction error at timestep 936 is tensor(3.2268e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 936 of 1
Current timestep = 937. State = [[0.08734037 0.215257  ]]. Action = [[ 0.19310671 -0.07040289  0.1859068   0.04165578]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 937 is [False, False, True, False, False, True]
State prediction error at timestep 937 is tensor(1.2211e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 937 of 1
Current timestep = 938. State = [[0.09104146 0.20843911]]. Action = [[ 0.2362195  -0.07968472  0.08693597 -0.10973835]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 938 is [False, False, True, False, False, True]
State prediction error at timestep 938 is tensor(7.8921e-06, grad_fn=<MseLossBackward0>)
Current timestep = 939. State = [[0.09581503 0.2005427 ]]. Action = [[ 0.1314314   0.11279485 -0.1709632   0.13725626]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 939 is [False, False, True, False, False, True]
State prediction error at timestep 939 is tensor(4.7866e-06, grad_fn=<MseLossBackward0>)
Current timestep = 940. State = [[0.09915578 0.19705278]]. Action = [[ 0.1494933  -0.17462894  0.09860766 -0.3605435 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 940 is [False, False, True, False, False, True]
State prediction error at timestep 940 is tensor(4.0322e-05, grad_fn=<MseLossBackward0>)
Current timestep = 941. State = [[0.10281245 0.18972674]]. Action = [[ 0.12005383 -0.18525827 -0.0251054   0.71035063]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 941 is [False, False, True, False, False, True]
State prediction error at timestep 941 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 941 of -1
Current timestep = 942. State = [[0.10788902 0.17830816]]. Action = [[-0.07605995 -0.09254661 -0.21557638 -0.7570071 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 942 is [False, False, True, False, False, True]
State prediction error at timestep 942 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 943. State = [[0.11124519 0.17007653]]. Action = [[-0.23702149 -0.02233893 -0.15979847 -0.48994517]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 943 is [False, False, True, False, False, True]
State prediction error at timestep 943 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 943 of -1
Current timestep = 944. State = [[0.11180469 0.16833033]]. Action = [[-0.05789717  0.14977628  0.10809603  0.34906626]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 944 is [False, False, True, False, False, True]
State prediction error at timestep 944 is tensor(1.8659e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 944 of -1
Current timestep = 945. State = [[0.11064245 0.1711732 ]]. Action = [[-0.02014084  0.06827116 -0.08542435 -0.07797247]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 945 is [False, False, True, False, False, True]
State prediction error at timestep 945 is tensor(5.5896e-05, grad_fn=<MseLossBackward0>)
Current timestep = 946. State = [[0.10957339 0.17391989]]. Action = [[-0.13678399 -0.17479822  0.10226506  0.8428004 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 946 is [False, False, True, False, False, True]
State prediction error at timestep 946 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 947. State = [[0.10972635 0.1729629 ]]. Action = [[-0.24359645 -0.08148786  0.21651167 -0.47702694]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 947 is [False, False, True, False, False, True]
State prediction error at timestep 947 is tensor(2.9785e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 947 of -1
Current timestep = 948. State = [[0.10876394 0.17266808]]. Action = [[-0.19510612 -0.14764217  0.10164317 -0.12041849]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 948 is [False, False, True, False, False, True]
State prediction error at timestep 948 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 948 of -1
Current timestep = 949. State = [[0.10775764 0.17060322]]. Action = [[ 0.16230512 -0.14890113  0.23593068 -0.8409913 ]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 949 is [False, False, True, False, False, True]
State prediction error at timestep 949 is tensor(2.4599e-07, grad_fn=<MseLossBackward0>)
Current timestep = 950. State = [[0.10966432 0.16430238]]. Action = [[-0.07421114 -0.18102999 -0.17039303 -0.13570267]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 950 is [False, False, True, False, False, True]
State prediction error at timestep 950 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 950 of -1
Current timestep = 951. State = [[0.11184716 0.15627626]]. Action = [[-0.21037313  0.0771451  -0.05115165 -0.09984034]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 951 is [False, False, True, False, False, True]
State prediction error at timestep 951 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 952. State = [[0.11101741 0.15553033]]. Action = [[-0.17890775 -0.02194655  0.23115009  0.6159352 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 952 is [False, False, True, False, False, True]
State prediction error at timestep 952 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 952 of 1
Current timestep = 953. State = [[0.10940278 0.15608995]]. Action = [[-0.19101058 -0.08216321 -0.13293697  0.36227334]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 953 is [False, False, True, False, False, True]
State prediction error at timestep 953 is tensor(1.1364e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 953 of 1
Current timestep = 954. State = [[0.10758337 0.15547523]]. Action = [[ 0.03836036 -0.14937958 -0.1187036   0.56038666]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 954 is [False, False, True, False, False, True]
State prediction error at timestep 954 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 954 of 1
Current timestep = 955. State = [[0.10776059 0.15063344]]. Action = [[ 0.12049514 -0.04126495 -0.0696014   0.34157944]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 955 is [False, False, True, False, False, True]
State prediction error at timestep 955 is tensor(7.4324e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 955 of 1
Current timestep = 956. State = [[0.10861871 0.14625128]]. Action = [[-0.22585845  0.09633204  0.12054679  0.8441453 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 956 is [False, False, True, False, False, True]
State prediction error at timestep 956 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 957. State = [[0.10703117 0.1477029 ]]. Action = [[-0.18898953  0.21828097  0.03773803  0.9439955 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 957 is [False, False, True, False, False, True]
State prediction error at timestep 957 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 958. State = [[0.1034226  0.15549397]]. Action = [[ 0.16952294 -0.07597172 -0.00226763 -0.3175947 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 958 is [False, False, True, False, False, True]
State prediction error at timestep 958 is tensor(2.5634e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 958 of 1
Current timestep = 959. State = [[0.10308495 0.15623152]]. Action = [[ 0.1787619  -0.1035023  -0.01146951  0.58466053]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 959 is [False, False, True, False, False, True]
State prediction error at timestep 959 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 959 of 1
Current timestep = 960. State = [[0.1044241  0.15245472]]. Action = [[ 0.05906701 -0.20477074  0.06416014 -0.39330828]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 960 is [False, False, True, False, False, True]
State prediction error at timestep 960 is tensor(7.0615e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 960 of 1
Current timestep = 961. State = [[0.10706913 0.1450444 ]]. Action = [[-0.1137194   0.21241066 -0.11744621  0.02228379]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 961 is [False, False, True, False, False, True]
State prediction error at timestep 961 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 961 of 1
Current timestep = 962. State = [[0.10637401 0.1470799 ]]. Action = [[ 0.11715555  0.17524683 -0.13586174 -0.63637745]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 962 is [False, False, True, False, False, True]
State prediction error at timestep 962 is tensor(1.2131e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 962 of 1
Current timestep = 963. State = [[0.1051596  0.15136962]]. Action = [[ 0.21877787 -0.02353302  0.20800164 -0.9226249 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 963 is [False, False, True, False, False, True]
State prediction error at timestep 963 is tensor(1.9916e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 963 of 1
Current timestep = 964. State = [[0.10595692 0.1508707 ]]. Action = [[-0.21080185 -0.2026844  -0.06870842  0.4467982 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 964 is [False, False, True, False, False, True]
State prediction error at timestep 964 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 965. State = [[0.10678039 0.14822133]]. Action = [[ 0.11918744  0.09810075 -0.18930608  0.5941663 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 965 is [False, False, True, False, False, True]
State prediction error at timestep 965 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 966. State = [[0.10677521 0.1484797 ]]. Action = [[ 0.04015183  0.24119455  0.01789767 -0.41974413]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 966 is [False, False, True, False, False, True]
State prediction error at timestep 966 is tensor(5.8017e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 966 of 1
Current timestep = 967. State = [[0.10541321 0.15373929]]. Action = [[ 0.08293676 -0.15956053 -0.18870664  0.0360688 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 967 is [False, False, True, False, False, True]
State prediction error at timestep 967 is tensor(5.7001e-06, grad_fn=<MseLossBackward0>)
Current timestep = 968. State = [[0.10620243 0.15176974]]. Action = [[ 0.15231985  0.01974857 -0.20579183  0.47221792]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 968 is [False, False, True, False, False, True]
State prediction error at timestep 968 is tensor(9.1809e-05, grad_fn=<MseLossBackward0>)
Current timestep = 969. State = [[0.10777463 0.15005891]]. Action = [[-0.03112069 -0.18528734 -0.01893263 -0.7755296 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 969 is [False, False, True, False, False, True]
State prediction error at timestep 969 is tensor(2.5643e-05, grad_fn=<MseLossBackward0>)
Current timestep = 970. State = [[0.11014036 0.14443937]]. Action = [[ 0.01896578 -0.06862074  0.09588593 -0.19534647]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 970 is [False, False, True, False, False, True]
State prediction error at timestep 970 is tensor(5.5337e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 970 of 1
Current timestep = 971. State = [[0.11267127 0.13958427]]. Action = [[ 0.13713777  0.13833952  0.16488227 -0.37409902]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 971 is [False, False, True, False, False, True]
State prediction error at timestep 971 is tensor(1.0284e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 971 of 1
Current timestep = 972. State = [[0.11423253 0.13949618]]. Action = [[ 0.05441368  0.03330541 -0.22481133  0.11345422]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 972 is [False, False, True, False, False, True]
State prediction error at timestep 972 is tensor(1.2087e-05, grad_fn=<MseLossBackward0>)
Current timestep = 973. State = [[0.11527257 0.13944843]]. Action = [[ 0.05739087 -0.1917935   0.0623813  -0.45461452]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 973 is [False, False, True, False, False, True]
State prediction error at timestep 973 is tensor(4.7244e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 973 of 1
Current timestep = 974. State = [[0.11717369 0.13376564]]. Action = [[-0.08763793  0.19630241 -0.1673806   0.9321927 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 974 is [False, False, True, False, False, True]
State prediction error at timestep 974 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 975. State = [[0.1164902  0.13630699]]. Action = [[-0.24291475  0.18929738  0.0899055   0.6996137 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 975 is [False, False, True, False, False, True]
State prediction error at timestep 975 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 975 of -1
Current timestep = 976. State = [[0.11380858 0.14407933]]. Action = [[-0.22099191 -0.07613644 -0.17640916  0.21813488]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 976 is [False, False, True, False, False, True]
State prediction error at timestep 976 is tensor(8.1639e-05, grad_fn=<MseLossBackward0>)
Current timestep = 977. State = [[0.1114079 0.1499902]]. Action = [[ 0.04217538  0.23165369 -0.08959612 -0.83775014]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 977 is [False, False, True, False, False, True]
State prediction error at timestep 977 is tensor(7.3599e-05, grad_fn=<MseLossBackward0>)
Current timestep = 978. State = [[0.10810876 0.1590212 ]]. Action = [[ 0.2021783   0.08302167 -0.09830314 -0.5597993 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 978 is [False, False, True, False, False, True]
State prediction error at timestep 978 is tensor(7.1685e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 978 of -1
Current timestep = 979. State = [[0.10632101 0.163783  ]]. Action = [[-0.09679842 -0.20850685 -0.09729967 -0.233455  ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 979 is [False, False, True, False, False, True]
State prediction error at timestep 979 is tensor(6.8024e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 979 of -1
Current timestep = 980. State = [[0.10665701 0.16254511]]. Action = [[ 0.09924015 -0.21264632  0.237373    0.9107758 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 980 is [False, False, True, False, False, True]
State prediction error at timestep 980 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 981. State = [[0.10900523 0.15593524]]. Action = [[-0.18813172  0.13660234 -0.23844421 -0.75414276]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 981 is [False, False, True, False, False, True]
State prediction error at timestep 981 is tensor(8.6173e-05, grad_fn=<MseLossBackward0>)
Current timestep = 982. State = [[0.10828868 0.15766397]]. Action = [[-0.19286247  0.15336454 -0.21998948 -0.7239889 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 982 is [False, False, True, False, False, True]
State prediction error at timestep 982 is tensor(2.0024e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 982 of -1
Current timestep = 983. State = [[0.10577534 0.16374609]]. Action = [[-0.007993   -0.17920154  0.05517054 -0.81724465]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 983 is [False, False, True, False, False, True]
State prediction error at timestep 983 is tensor(3.9898e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 983 of -1
Current timestep = 984. State = [[0.10598847 0.16253996]]. Action = [[-0.01675764 -0.22779945 -0.1741476   0.6057997 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 984 is [False, False, True, False, False, True]
State prediction error at timestep 984 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 985. State = [[0.10787237 0.15653212]]. Action = [[ 0.20599443  0.06989372 -0.01072608 -0.7349875 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 985 is [False, False, True, False, False, True]
State prediction error at timestep 985 is tensor(7.6535e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 985 of -1
Current timestep = 986. State = [[0.10893585 0.15338875]]. Action = [[ 0.02405229  0.22002989  0.24444032 -0.44389665]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 986 is [False, False, True, False, False, True]
State prediction error at timestep 986 is tensor(9.0312e-06, grad_fn=<MseLossBackward0>)
Current timestep = 987. State = [[0.10784768 0.15671386]]. Action = [[ 0.0694299   0.21997806  0.18713889 -0.11162251]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 987 is [False, False, True, False, False, True]
State prediction error at timestep 987 is tensor(9.0836e-05, grad_fn=<MseLossBackward0>)
Current timestep = 988. State = [[0.10581526 0.16266291]]. Action = [[ 0.21675223 -0.20883386 -0.00273365  0.10859597]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 988 is [False, False, True, False, False, True]
State prediction error at timestep 988 is tensor(2.2772e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 988 of -1
Current timestep = 989. State = [[0.10729242 0.15924871]]. Action = [[-0.11578158 -0.04026853 -0.1509663   0.23138142]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 989 is [False, False, True, False, False, True]
State prediction error at timestep 989 is tensor(1.0309e-05, grad_fn=<MseLossBackward0>)
Current timestep = 990. State = [[0.10770276 0.1583167 ]]. Action = [[-0.14255674  0.05744573  0.12690401 -0.5374041 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 990 is [False, False, True, False, False, True]
State prediction error at timestep 990 is tensor(4.9223e-05, grad_fn=<MseLossBackward0>)
Current timestep = 991. State = [[0.10692896 0.16047345]]. Action = [[-0.16926028 -0.05439025 -0.07460067  0.59704936]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 991 is [False, False, True, False, False, True]
State prediction error at timestep 991 is tensor(8.9200e-05, grad_fn=<MseLossBackward0>)
Current timestep = 992. State = [[0.10643546 0.16128115]]. Action = [[-0.23320311  0.21316093  0.04174432  0.5535419 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 992 is [False, False, True, False, False, True]
State prediction error at timestep 992 is tensor(1.1550e-05, grad_fn=<MseLossBackward0>)
Current timestep = 993. State = [[0.10344587 0.1688649 ]]. Action = [[ 0.2043373  -0.07226259  0.12829685  0.7274327 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 993 is [False, False, True, False, False, True]
State prediction error at timestep 993 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 994. State = [[0.10284336 0.1703753 ]]. Action = [[0.07922134 0.15995368 0.11861947 0.4472561 ]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 994 is [False, False, True, False, False, True]
State prediction error at timestep 994 is tensor(5.2115e-06, grad_fn=<MseLossBackward0>)
Current timestep = 995. State = [[0.10129813 0.17480546]]. Action = [[-0.06190658  0.24483594  0.23300526 -0.8387801 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 995 is [False, False, True, False, False, True]
State prediction error at timestep 995 is tensor(2.4243e-05, grad_fn=<MseLossBackward0>)
Current timestep = 996. State = [[0.09768527 0.18466455]]. Action = [[-0.2012603   0.17502385  0.0362536  -0.20706218]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 996 is [False, False, True, False, False, True]
State prediction error at timestep 996 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 997. State = [[0.09208178 0.19807136]]. Action = [[ 0.08657548  0.10479701  0.07441324 -0.65127987]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 997 is [False, False, True, False, False, True]
State prediction error at timestep 997 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 998. State = [[0.08843967 0.20693882]]. Action = [[ 0.24014252 -0.23098038  0.02371684  0.5226908 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 998 is [False, False, True, False, False, True]
State prediction error at timestep 998 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 999. State = [[0.08948319 0.20443073]]. Action = [[ 0.15365043 -0.07631421 -0.13988079 -0.42294812]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 999 is [False, False, True, False, False, True]
State prediction error at timestep 999 is tensor(5.6382e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1000. State = [[0.09214044 0.19901106]]. Action = [[ 0.16479975  0.13311759 -0.01878425 -0.49212682]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 1000 is [False, False, True, False, False, True]
State prediction error at timestep 1000 is tensor(5.6836e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1000 of -1
Current timestep = 1001. State = [[0.09343393 0.19837293]]. Action = [[ 0.22040814 -0.18288884  0.04429707  0.2554456 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 1001 is [False, False, True, False, False, True]
State prediction error at timestep 1001 is tensor(8.6482e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1002. State = [[0.09748866 0.1915053 ]]. Action = [[0.01668823 0.18586269 0.20524514 0.32889068]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 1002 is [False, False, True, False, False, True]
State prediction error at timestep 1002 is tensor(5.1299e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1003. State = [[0.09941133 0.19206639]]. Action = [[ 0.05890688  0.14785588 -0.12129477  0.14938772]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 1003 is [False, False, True, False, False, True]
State prediction error at timestep 1003 is tensor(3.2227e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1003 of -1
Current timestep = 1004. State = [[0.09968451 0.1949532 ]]. Action = [[ 0.17740315  0.09776157 -0.01255181  0.79773784]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 1004 is [False, False, True, False, False, True]
State prediction error at timestep 1004 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1004 of -1
Current timestep = 1005. State = [[0.10080149 0.19603121]]. Action = [[ 0.1297046  -0.22040233  0.16973597 -0.40426648]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 1005 is [False, False, True, False, False, True]
State prediction error at timestep 1005 is tensor(1.2125e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1006. State = [[0.10456    0.19007711]]. Action = [[ 0.24220741 -0.20274392  0.05071342  0.6730039 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 1006 is [False, False, True, False, False, True]
State prediction error at timestep 1006 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1006 of -1
Current timestep = 1007. State = [[0.11033875 0.17913762]]. Action = [[ 0.21102154 -0.19391897 -0.15293007 -0.06496227]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 1007 is [False, False, True, False, False, True]
State prediction error at timestep 1007 is tensor(6.8669e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1007 of -1
Current timestep = 1008. State = [[0.1167805  0.16566463]]. Action = [[0.06484085 0.07963082 0.20643768 0.55822945]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 1008 is [False, False, True, False, False, True]
State prediction error at timestep 1008 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1008 of -1
Current timestep = 1009. State = [[0.11997358 0.15974896]]. Action = [[ 0.04912966 -0.23099582 -0.0776367   0.49702907]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 1009 is [False, False, True, False, False, True]
State prediction error at timestep 1009 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1010. State = [[0.12420218 0.1496033 ]]. Action = [[ 0.14667311  0.01588842  0.2106117  -0.11253452]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 1010 is [False, False, True, False, False, True]
State prediction error at timestep 1010 is tensor(6.7451e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1010 of -1
Current timestep = 1011. State = [[0.12741737 0.14274363]]. Action = [[-0.10178317  0.04825205 -0.23597597  0.3969754 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 1011 is [False, False, True, False, False, True]
State prediction error at timestep 1011 is tensor(1.6889e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1011 of -1
Current timestep = 1012. State = [[0.1279387  0.14177413]]. Action = [[ 0.17379957  0.05934855  0.10547858 -0.4803009 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 1012 is [False, False, True, False, False, True]
State prediction error at timestep 1012 is tensor(1.4833e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1012 of -1
Current timestep = 1013. State = [[0.12924701 0.14135142]]. Action = [[0.21177384 0.02758193 0.14656216 0.26949334]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 1013 is [False, False, True, False, False, True]
State prediction error at timestep 1013 is tensor(6.0390e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1014. State = [[0.13123158 0.14011984]]. Action = [[-0.04595417  0.01074773  0.05599332  0.7716552 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 1014 is [False, False, True, False, False, True]
State prediction error at timestep 1014 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1014 of -1
Current timestep = 1015. State = [[0.1321174  0.13996857]]. Action = [[-0.152391   -0.12586945  0.2009263  -0.71857405]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 1015 is [False, False, True, False, False, True]
State prediction error at timestep 1015 is tensor(5.1560e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1015 of -1
Current timestep = 1016. State = [[0.13278356 0.13787143]]. Action = [[ 0.02662474 -0.0430644   0.06804028 -0.28858018]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 1016 is [False, False, True, False, False, True]
State prediction error at timestep 1016 is tensor(4.2417e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1016 of -1
Current timestep = 1017. State = [[0.13357694 0.13571097]]. Action = [[ 0.11188802  0.24138337 -0.03265329 -0.6331372 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 1017 is [False, False, True, False, False, True]
State prediction error at timestep 1017 is tensor(2.1567e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1018. State = [[0.13347694 0.13956025]]. Action = [[0.0754765  0.19479325 0.22053784 0.93681574]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 1018 is [False, False, True, False, False, True]
State prediction error at timestep 1018 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1018 of -1
Current timestep = 1019. State = [[0.13299333 0.14574619]]. Action = [[ 0.04492638 -0.01158807 -0.23299876 -0.39616835]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 1019 is [False, False, True, False, False, True]
State prediction error at timestep 1019 is tensor(7.7922e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1019 of -1
Current timestep = 1020. State = [[0.13341275 0.14815459]]. Action = [[-0.11451793  0.22245222 -0.20974606 -0.8358201 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 1020 is [False, False, True, False, False, True]
State prediction error at timestep 1020 is tensor(3.6819e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1020 of -1
Current timestep = 1021. State = [[0.13098967 0.15619682]]. Action = [[-0.06073913  0.01822037 -0.00875685  0.09032106]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 1021 is [False, False, True, False, False, True]
State prediction error at timestep 1021 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1022. State = [[0.12852009 0.16296993]]. Action = [[-0.05206862  0.13973236 -0.1705267  -0.3137589 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 1022 is [False, False, True, False, False, True]
State prediction error at timestep 1022 is tensor(6.4129e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1022 of -1
Current timestep = 1023. State = [[0.12574351 0.17053239]]. Action = [[-0.21295348  0.15397209  0.02703026  0.80543065]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 1023 is [False, False, True, False, False, True]
State prediction error at timestep 1023 is tensor(6.9134e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1023 of -1
Current timestep = 1024. State = [[0.12153099 0.1813833 ]]. Action = [[-0.22855741  0.16009176 -0.20645282  0.41375458]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 1024 is [False, False, True, False, False, True]
State prediction error at timestep 1024 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1024 of -1
Current timestep = 1025. State = [[0.1162427 0.1940907]]. Action = [[ 0.0758464   0.06643674  0.13460827 -0.6826966 ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 1025 is [False, False, True, False, False, True]
State prediction error at timestep 1025 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1026. State = [[0.11301322 0.20221399]]. Action = [[-0.20110223  0.13996977  0.09272891 -0.259503  ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 1026 is [False, False, True, False, False, True]
State prediction error at timestep 1026 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1026 of -1
Current timestep = 1027. State = [[0.10879386 0.21278161]]. Action = [[-0.08687314  0.02512062  0.20091948 -0.13793558]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 1027 is [False, False, True, False, False, True]
State prediction error at timestep 1027 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1027 of -1
Current timestep = 1028. State = [[0.10582317 0.2201663 ]]. Action = [[-0.03453958 -0.11848202 -0.16727696  0.92167735]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 1028 is [False, False, True, False, False, True]
State prediction error at timestep 1028 is tensor(6.4424e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1029. State = [[0.1053134  0.22138311]]. Action = [[-0.05380008  0.10761791 -0.12294397  0.78694296]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 1029 is [False, False, True, False, False, True]
State prediction error at timestep 1029 is tensor(3.9348e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1029 of -1
Current timestep = 1030. State = [[0.10354728 0.22540928]]. Action = [[ 0.16093186  0.00477669 -0.0029555  -0.36438525]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 1030 is [False, False, True, False, False, True]
State prediction error at timestep 1030 is tensor(1.4026e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1030 of -1
Current timestep = 1031. State = [[0.10388762 0.22531632]]. Action = [[ 0.05529848 -0.18157215 -0.13001886 -0.9019622 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 1031 is [False, False, True, False, False, True]
State prediction error at timestep 1031 is tensor(8.0127e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1031 of -1
Current timestep = 1032. State = [[0.10597523 0.22089498]]. Action = [[ 0.21803784 -0.16198443 -0.0503502  -0.66551596]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 1032 is [False, False, True, False, False, True]
State prediction error at timestep 1032 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1033. State = [[0.10985631 0.21265833]]. Action = [[0.20098555 0.10318229 0.05337209 0.555195  ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 1033 is [False, False, True, False, False, True]
State prediction error at timestep 1033 is tensor(3.6977e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1034. State = [[0.11287777 0.20870355]]. Action = [[-0.09096897 -0.04893434  0.09935218 -0.05195564]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 1034 is [False, False, True, False, False, True]
State prediction error at timestep 1034 is tensor(4.1304e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1034 of -1
Current timestep = 1035. State = [[0.11417306 0.20665316]]. Action = [[-0.10771871  0.08444443  0.12763393  0.8483032 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 1035 is [False, False, True, False, False, True]
State prediction error at timestep 1035 is tensor(5.5366e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1035 of -1
Current timestep = 1036. State = [[0.11375014 0.20766479]]. Action = [[ 0.02676791 -0.17189924 -0.12249643  0.53576195]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 1036 is [False, False, True, False, False, True]
State prediction error at timestep 1036 is tensor(4.9094e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1037. State = [[0.11513786 0.20416528]]. Action = [[ 0.22001326  0.06528783 -0.06856835  0.73531723]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 1037 is [False, False, True, False, False, True]
State prediction error at timestep 1037 is tensor(8.9036e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1038. State = [[0.11648846 0.20125733]]. Action = [[-0.12635079 -0.22643128 -0.1890266  -0.28185022]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 1038 is [False, False, True, False, False, True]
State prediction error at timestep 1038 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1038 of -1
Current timestep = 1039. State = [[0.11905347 0.19508648]]. Action = [[ 0.15149033 -0.01898763  0.19921964 -0.69453067]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 1039 is [False, False, True, False, False, True]
State prediction error at timestep 1039 is tensor(2.2070e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1040. State = [[0.12143301 0.18969296]]. Action = [[-0.03724504 -0.07166885  0.15499479  0.988286  ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 1040 is [False, False, True, False, False, True]
State prediction error at timestep 1040 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1041. State = [[0.12311137 0.18553549]]. Action = [[ 0.12833446 -0.20353335 -0.17957622  0.23630214]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 1041 is [False, False, True, False, False, True]
State prediction error at timestep 1041 is tensor(4.1321e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1041 of -1
Current timestep = 1042. State = [[0.12706043 0.17560847]]. Action = [[-0.11520398  0.03313243 -0.13074389 -0.5057697 ]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 1042 is [False, False, True, False, False, True]
State prediction error at timestep 1042 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1042 of -1
Current timestep = 1043. State = [[0.12793347 0.17331676]]. Action = [[-0.24730764  0.06356877  0.03645629  0.8313534 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 1043 is [False, False, True, False, False, True]
State prediction error at timestep 1043 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1043 of -1
Current timestep = 1044. State = [[0.12709293 0.17525628]]. Action = [[-0.1428701  -0.15579273 -0.19784118 -0.01724112]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 1044 is [False, False, True, False, False, True]
State prediction error at timestep 1044 is tensor(6.6354e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1045. State = [[0.12730417 0.17372186]]. Action = [[-0.1154878  -0.21482764  0.16031313  0.55672646]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 1045 is [False, False, True, False, False, True]
State prediction error at timestep 1045 is tensor(7.7479e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1045 of -1
Current timestep = 1046. State = [[0.12816003 0.16872275]]. Action = [[ 0.21650955  0.15954018 -0.13604698  0.5396247 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 1046 is [False, False, True, False, False, True]
State prediction error at timestep 1046 is tensor(9.5552e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1046 of -1
Current timestep = 1047. State = [[0.12827335 0.1684671 ]]. Action = [[-0.06691054  0.1659894   0.09307176 -0.30222106]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 1047 is [False, False, True, False, False, True]
State prediction error at timestep 1047 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1048. State = [[0.12677664 0.17211387]]. Action = [[-0.21509825 -0.17628783 -0.18054324 -0.927037  ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 1048 is [False, False, True, False, False, True]
State prediction error at timestep 1048 is tensor(3.3054e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1048 of -1
Current timestep = 1049. State = [[0.12626164 0.17119741]]. Action = [[-0.12888426 -0.20843343  0.09225953 -0.12248176]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 1049 is [False, False, True, False, False, True]
State prediction error at timestep 1049 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1049 of -1
Current timestep = 1050. State = [[0.12642087 0.16717194]]. Action = [[-0.17732036 -0.02810439 -0.17526625 -0.03969842]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 1050 is [False, False, True, False, False, True]
State prediction error at timestep 1050 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1051. State = [[0.12529689 0.1664487 ]]. Action = [[0.02341467 0.21263069 0.05008772 0.68375266]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 1051 is [False, False, True, False, False, True]
State prediction error at timestep 1051 is tensor(6.1077e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1052. State = [[0.12354289 0.17088212]]. Action = [[0.2262156  0.18668336 0.05752403 0.7608416 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 1052 is [False, False, True, False, False, True]
State prediction error at timestep 1052 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1052 of -1
Current timestep = 1053. State = [[0.1223719  0.17442748]]. Action = [[ 0.17517966 -0.17973031  0.19245753  0.71826804]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 1053 is [False, False, True, False, False, True]
State prediction error at timestep 1053 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1053 of 1
Current timestep = 1054. State = [[0.12365287 0.17113319]]. Action = [[0.10833079 0.04564831 0.10440293 0.57988536]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 1054 is [False, False, True, False, False, True]
State prediction error at timestep 1054 is tensor(5.4856e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1055. State = [[0.12438176 0.16967346]]. Action = [[ 0.1766155  -0.18954831  0.21945286  0.6219752 ]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 1055 is [False, False, True, False, False, True]
State prediction error at timestep 1055 is tensor(8.5574e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1055 of 1
Current timestep = 1056. State = [[0.12723182 0.16213739]]. Action = [[ 0.18912077 -0.00914547 -0.15021636  0.30523098]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 1056 is [False, False, True, False, False, True]
State prediction error at timestep 1056 is tensor(4.2571e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1057. State = [[0.13009591 0.15560484]]. Action = [[ 0.09552386 -0.12211671  0.02899677 -0.4466251 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 1057 is [False, False, True, False, False, True]
State prediction error at timestep 1057 is tensor(6.7735e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1057 of -1
Current timestep = 1058. State = [[0.13344675 0.147268  ]]. Action = [[-0.12855528 -0.1329771  -0.0194595   0.00874019]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 1058 is [False, False, True, False, False, True]
State prediction error at timestep 1058 is tensor(9.4781e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1059. State = [[0.13586769 0.14024644]]. Action = [[ 0.0884535  -0.09608808 -0.03372651 -0.740406  ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 1059 is [False, False, True, False, False, True]
State prediction error at timestep 1059 is tensor(5.0424e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1059 of -1
Current timestep = 1060. State = [[0.13847546 0.1330833 ]]. Action = [[ 0.05017504  0.03693476 -0.22877948  0.21570802]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 1060 is [False, False, True, False, False, True]
State prediction error at timestep 1060 is tensor(3.9451e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1060 of -1
Current timestep = 1061. State = [[0.13959584 0.13010095]]. Action = [[-0.04694009  0.0998337  -0.24724896  0.7980447 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 1061 is [False, False, True, False, False, True]
State prediction error at timestep 1061 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1062. State = [[0.13934124 0.13131128]]. Action = [[-0.24543759  0.04588398  0.01541796 -0.0102123 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 1062 is [False, False, True, False, False, True]
State prediction error at timestep 1062 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1062 of -1
Current timestep = 1063. State = [[0.1380104  0.13502304]]. Action = [[-0.00433382  0.23563373 -0.23827279 -0.09556907]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 1063 is [False, False, True, False, False, True]
State prediction error at timestep 1063 is tensor(3.5397e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1063 of -1
Current timestep = 1064. State = [[0.13555399 0.1431013 ]]. Action = [[ 0.07339251 -0.06315872  0.11488926 -0.06235838]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 1064 is [False, False, True, False, False, True]
State prediction error at timestep 1064 is tensor(8.5814e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1064 of -1
Current timestep = 1065. State = [[0.1350555  0.14489076]]. Action = [[ 0.00235254 -0.06606144  0.10958719 -0.14103466]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 1065 is [False, False, True, False, False, True]
State prediction error at timestep 1065 is tensor(4.3621e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1066. State = [[0.13528739 0.14498745]]. Action = [[-0.18477033  0.19837797  0.06882536  0.3763156 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 1066 is [False, False, True, False, False, True]
State prediction error at timestep 1066 is tensor(2.0196e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1066 of -1
Current timestep = 1067. State = [[0.13369726 0.15190305]]. Action = [[ 0.03371367  0.03607246 -0.18226264  0.13872266]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 1067 is [False, False, True, False, False, True]
State prediction error at timestep 1067 is tensor(2.5645e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1067 of -1
Current timestep = 1068. State = [[0.1327001 0.156997 ]]. Action = [[-0.21757628  0.04865938 -0.13120498 -0.60221887]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 1068 is [False, False, True, False, False, True]
State prediction error at timestep 1068 is tensor(7.1892e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1069. State = [[0.1308973  0.16289203]]. Action = [[ 0.22011518 -0.24002273  0.18260649 -0.6602522 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 1069 is [False, False, True, False, False, True]
State prediction error at timestep 1069 is tensor(3.0014e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1070. State = [[0.13249967 0.1586765 ]]. Action = [[ 0.09489158  0.05238074 -0.14876485 -0.5093986 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 1070 is [False, False, True, False, False, True]
State prediction error at timestep 1070 is tensor(9.7596e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1070 of -1
Current timestep = 1071. State = [[0.13308863 0.15745568]]. Action = [[-0.18662937  0.20792562  0.00719982 -0.66598284]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 1071 is [False, False, True, False, False, True]
State prediction error at timestep 1071 is tensor(1.4054e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1072. State = [[0.13106224 0.16314346]]. Action = [[ 0.0494954  -0.02586704 -0.2140136   0.7375449 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 1072 is [False, False, True, False, False, True]
State prediction error at timestep 1072 is tensor(4.5735e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1073. State = [[0.1306377  0.16437235]]. Action = [[-0.19068745 -0.14578962 -0.09823206  0.19582415]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 1073 is [False, False, True, False, False, True]
State prediction error at timestep 1073 is tensor(1.0993e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1073 of -1
Current timestep = 1074. State = [[0.13087472 0.16364872]]. Action = [[-0.13406204 -0.05283524  0.16289014  0.4908855 ]]. Reward = [0.]
Curr episode timestep = 318
Human Feedback received at timestep 1074 of -1
Current timestep = 1075. State = [[0.13072085 0.16373338]]. Action = [[ 0.11242899  0.1511755  -0.10442375 -0.81564736]]. Reward = [0.]
Curr episode timestep = 319
Current timestep = 1076. State = [[0.13000335 0.16575949]]. Action = [[ 0.22175938  0.00123712 -0.13393198 -0.26581073]]. Reward = [0.]
Curr episode timestep = 320
Human Feedback received at timestep 1076 of -1
Current timestep = 1077. State = [[0.13026814 0.16493472]]. Action = [[-0.22626068 -0.07599694  0.22862422 -0.6122587 ]]. Reward = [0.]
Curr episode timestep = 321
Current timestep = 1078. State = [[0.13044399 0.1645151 ]]. Action = [[ 0.15481383 -0.23553981 -0.14120753 -0.31979185]]. Reward = [0.]
Curr episode timestep = 322
Human Feedback received at timestep 1078 of -1
Current timestep = 1079. State = [[0.1329241  0.15766798]]. Action = [[ 0.21046937 -0.17231275 -0.13970777  0.871722  ]]. Reward = [0.]
Curr episode timestep = 323
Human Feedback received at timestep 1079 of -1
Current timestep = 1080. State = [[0.13658449 0.14755696]]. Action = [[ 0.14854875  0.14703578 -0.04423422  0.22821808]]. Reward = [0.]
Curr episode timestep = 324
Current timestep = 1081. State = [[0.13831934 0.14348128]]. Action = [[ 0.22574669 -0.03461972  0.22891396 -0.6672762 ]]. Reward = [0.]
Curr episode timestep = 325
Current timestep = 1082. State = [[0.13983822 0.13912097]]. Action = [[-0.16348179  0.20148957  0.08780628  0.19362843]]. Reward = [0.]
Curr episode timestep = 326
Current timestep = 1083. State = [[0.13923344 0.14181358]]. Action = [[ 0.21304542 -0.22093782 -0.16565403  0.8729178 ]]. Reward = [0.]
Curr episode timestep = 327
Current timestep = 1084. State = [[0.14096838 0.13706014]]. Action = [[ 0.03193921  0.03124672 -0.12569733  0.5158169 ]]. Reward = [0.]
Curr episode timestep = 328
Current timestep = 1085. State = [[0.14194208 0.13487019]]. Action = [[ 0.06190756  0.13737601 -0.23802996 -0.48145676]]. Reward = [0.]
Curr episode timestep = 329
Current timestep = 1086. State = [[0.14203274 0.13600779]]. Action = [[0.09783554 0.11286041 0.1289694  0.7180997 ]]. Reward = [0.]
Curr episode timestep = 330
Current timestep = 1087. State = [[0.14211145 0.13756196]]. Action = [[ 0.15310869 -0.21581075  0.14330143  0.892259  ]]. Reward = [0.]
Curr episode timestep = 331
Current timestep = 1088. State = [[0.14420557 0.13294746]]. Action = [[ 0.15373671 -0.06955296 -0.15244024  0.8195262 ]]. Reward = [0.]
Curr episode timestep = 332
Current timestep = 1089. State = [[0.1469833  0.12770091]]. Action = [[-0.21354282  0.16078949 -0.20846096 -0.898954  ]]. Reward = [0.]
Curr episode timestep = 333
Current timestep = 1090. State = [[0.14700396 0.12991613]]. Action = [[0.19023484 0.01214454 0.19671592 0.01096165]]. Reward = [0.]
Curr episode timestep = 334
Current timestep = 1091. State = [[0.14804699 0.12938222]]. Action = [[-0.06746703 -0.16735865  0.22749192 -0.03497487]]. Reward = [0.]
Curr episode timestep = 335
Current timestep = 1092. State = [[0.14969164 0.12548244]]. Action = [[ 0.2184977  -0.06511277  0.02911747  0.46056592]]. Reward = [0.]
Curr episode timestep = 336
Current timestep = 1093. State = [[-0.25841078  0.00842566]]. Action = [[-0.04179932 -0.10885638  0.07799566 -0.8466752 ]]. Reward = [1.]
Curr episode timestep = 337
Current timestep = 1094. State = [[-0.25009376  0.01304282]]. Action = [[ 0.01142696  0.1268759   0.1518291  -0.972898  ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1095. State = [[-0.20617533  0.03767897]]. Action = [[-0.23620926  0.01418784 -0.03585127 -0.48068964]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1096. State = [[-0.13801745  0.07094558]]. Action = [[0.12078071 0.06556702 0.07184893 0.10814977]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1097. State = [[-0.07361177  0.09457602]]. Action = [[-0.1826145   0.21394724  0.2424283  -0.7352726 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1098. State = [[-0.25698456  0.00764476]]. Action = [[0.17816406 0.11220774 0.22101343 0.6053374 ]]. Reward = [1.]
Curr episode timestep = 4
Current timestep = 1099. State = [[-0.24963818  0.01220628]]. Action = [[-0.07109693  0.17377213 -0.1698799   0.43992352]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1100. State = [[-0.20961808  0.04320879]]. Action = [[ 0.01103821 -0.16527848  0.10979098  0.0929271 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1101. State = [[-0.14376962  0.08320105]]. Action = [[-0.23955138  0.0092366   0.08447468  0.41252756]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1102. State = [[-0.07607741  0.11128436]]. Action = [[ 0.00729433  0.00200987 -0.10983016 -0.6035115 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1103. State = [[-0.03354095  0.12567404]]. Action = [[-0.06935385  0.01470685 -0.18191648 -0.79745317]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1103 is [False, True, False, False, False, True]
State prediction error at timestep 1103 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 1104. State = [[-0.00401732  0.13591142]]. Action = [[ 0.24353966 -0.1195749  -0.24220957  0.66106606]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1104 is [False, True, False, False, False, True]
State prediction error at timestep 1104 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1105. State = [[0.02109882 0.13764212]]. Action = [[-0.18178865 -0.20646705  0.23753768 -0.9919216 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1105 is [False, True, False, False, False, True]
State prediction error at timestep 1105 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1106. State = [[0.04067887 0.13331787]]. Action = [[ 0.12251821  0.06813714 -0.1459     -0.8812001 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1106 is [False, True, False, False, False, True]
State prediction error at timestep 1106 is tensor(5.5213e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1107. State = [[0.06030384 0.1323485 ]]. Action = [[-0.15426254  0.03478804  0.12116921  0.8623613 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1108. State = [[0.07537593 0.13385779]]. Action = [[-0.20484684  0.20600581  0.00579271 -0.83773243]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1109. State = [[0.08516003 0.14128654]]. Action = [[-0.10016885 -0.05761307 -0.03506418  0.6260407 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1110. State = [[0.09627686 0.14547925]]. Action = [[-0.12079093  0.20329782 -0.20060799  0.6934931 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1111. State = [[0.10446256 0.15391845]]. Action = [[ 0.13618207  0.06544563 -0.14648953 -0.42293853]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1112. State = [[0.11468087 0.16065003]]. Action = [[ 0.23366803 -0.16951431  0.08329183 -0.37694848]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1113. State = [[0.12685525 0.15662053]]. Action = [[-0.04353929  0.24032283  0.00387663 -0.43115413]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1114. State = [[0.13017444 0.15916908]]. Action = [[ 0.10672504 -0.07315508  0.04724765 -0.25106376]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1115. State = [[0.13150764 0.15994439]]. Action = [[-0.06963     0.235367    0.12713349 -0.0032438 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1116. State = [[0.12874323 0.16522518]]. Action = [[-0.12024692  0.1230908  -0.10661674 -0.6233696 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1117. State = [[0.12488483 0.17280447]]. Action = [[ 0.04703534 -0.02749309  0.23419344  0.7502446 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1118. State = [[0.12315266 0.17600004]]. Action = [[-0.16685446 -0.08095674  0.17569631  0.04838037]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1119. State = [[0.12202972 0.1767775 ]]. Action = [[ 0.09285021 -0.2048104   0.23325479 -0.14068812]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1120. State = [[0.12338793 0.17248799]]. Action = [[-0.22212522  0.15784866  0.12145033 -0.60552436]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1121. State = [[0.12178236 0.17578886]]. Action = [[-0.07782507  0.07879972 -0.16014834 -0.6540564 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1122. State = [[0.11992566 0.17984857]]. Action = [[-0.03199017  0.04114178  0.22795153 -0.8213759 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1123. State = [[0.11830518 0.1836072 ]]. Action = [[-0.16628781 -0.18121392 -0.01505589  0.14931488]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1124. State = [[0.11776245 0.18263513]]. Action = [[-0.19774605 -0.058311   -0.10352629 -0.49354494]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1125. State = [[0.11676409 0.18178184]]. Action = [[ 0.10658944 -0.08964533 -0.0855841  -0.28846204]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1126. State = [[0.11742394 0.1788648 ]]. Action = [[ 0.07164487 -0.12806101 -0.13648279 -0.00812018]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1127. State = [[0.1188532  0.17390105]]. Action = [[ 2.4876308e-01 -3.6495119e-02 -6.7205727e-04 -9.4949698e-01]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1128. State = [[0.12121227 0.16759436]]. Action = [[ 0.17012644 -0.18949713  0.15170175  0.07801831]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1129. State = [[0.12507683 0.15867577]]. Action = [[ 0.08632374  0.03597572 -0.15604164  0.29783738]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1130. State = [[0.12774324 0.15269122]]. Action = [[-0.03114314 -0.18868773  0.14692277 -0.43171817]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1131. State = [[0.13047577 0.14561757]]. Action = [[-0.02077575  0.14014453 -0.21717356 -0.5705738 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1132. State = [[0.13054518 0.14483   ]]. Action = [[-0.00340119  0.06272617  0.13493812  0.33607078]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1133. State = [[0.13032088 0.14548577]]. Action = [[-0.03562409 -0.02597925  0.10145321  0.60891366]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1134. State = [[0.13047008 0.14505288]]. Action = [[ 0.2206972  -0.03681651  0.18290302 -0.93320405]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1135. State = [[0.13168159 0.14218862]]. Action = [[ 0.11049718 -0.18742447  0.12311617 -0.07068872]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1136. State = [[0.13435249 0.13493532]]. Action = [[ 0.22325021 -0.08820987  0.0802379   0.28162885]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1137. State = [[0.13704985 0.12790765]]. Action = [[-0.01628827  0.16127706  0.10419875 -0.00044012]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1138. State = [[0.13761985 0.12698507]]. Action = [[-0.07928647 -0.08121115  0.0660069  -0.7077894 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1139. State = [[0.13820831 0.12482578]]. Action = [[-0.07448566 -0.11719739 -0.09557727  0.48766935]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1139 is [False, False, True, False, True, False]
State prediction error at timestep 1139 is tensor(8.6630e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1140. State = [[-0.25801882  0.00961983]]. Action = [[-0.20626982 -0.16298978 -0.084627   -0.21377617]]. Reward = [1.]
Curr episode timestep = 41
Current timestep = 1141. State = [[-0.24896632  0.01436679]]. Action = [[ 0.01517278 -0.03981553  0.1849513  -0.41391397]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1142. State = [[-0.203795    0.03588295]]. Action = [[ 0.08536249 -0.10180709  0.12477136  0.25785828]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1143. State = [[-0.1338531   0.06131315]]. Action = [[ 0.1750288  -0.21031989  0.09921077 -0.0298174 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1144. State = [[-0.06887085  0.07250695]]. Action = [[ 0.05878687  0.19205949 -0.08170578  0.9652544 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1145. State = [[-0.25681263  0.007682  ]]. Action = [[-0.07946774 -0.04154193 -0.13169077  0.7411883 ]]. Reward = [1.]
Curr episode timestep = 4
Current timestep = 1146. State = [[-0.24895297  0.01115926]]. Action = [[-0.15676999  0.12376663 -0.00975958 -0.40277863]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1147. State = [[-0.19970384  0.03439109]]. Action = [[ 0.17123175 -0.20689209 -0.21063603 -0.42207432]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1148. State = [[-0.13456064  0.05756031]]. Action = [[ 0.20925203 -0.20487967 -0.2090272   0.20767653]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1149. State = [[-0.06744231  0.06680941]]. Action = [[ 0.09937221 -0.24798544  0.07197058  0.6205306 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1150. State = [[-0.25667784  0.00768879]]. Action = [[0.05336219 0.15857434 0.1305758  0.8452437 ]]. Reward = [1.]
Curr episode timestep = 4
Current timestep = 1151. State = [[-0.24756713  0.00994936]]. Action = [[ 0.19819763 -0.21787392 -0.0752832  -0.5780104 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1152. State = [[-0.1982765   0.02090099]]. Action = [[0.19340771 0.21260858 0.24016893 0.74024963]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1153. State = [[-0.12467282  0.03893148]]. Action = [[-0.12020272  0.09821445  0.11240736  0.8470545 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1154. State = [[-0.05767475  0.05609639]]. Action = [[ 0.24714416  0.13358715 -0.06563413 -0.7746083 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1155. State = [[-0.25684902  0.00757502]]. Action = [[ 0.19504964  0.04612803 -0.19640157 -0.51848495]]. Reward = [1.]
Curr episode timestep = 4
Current timestep = 1156. State = [[-0.24861307  0.00973771]]. Action = [[ 0.23133653 -0.1767483   0.03085965 -0.55584806]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1157. State = [[-0.19354548  0.02194816]]. Action = [[ 0.19981569  0.12381536  0.05859345 -0.41352814]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1158. State = [[-0.12466912  0.04107841]]. Action = [[ 0.07281309 -0.21823363  0.17985523  0.15045738]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1159. State = [[-0.05330357  0.05733402]]. Action = [[ 0.00833663  0.2146973  -0.01499672 -0.8015695 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1160. State = [[-0.25652835  0.0078564 ]]. Action = [[-0.15516353  0.01628023  0.18700951  0.6344719 ]]. Reward = [1.]
Curr episode timestep = 4
Current timestep = 1161. State = [[-0.24783361  0.00989362]]. Action = [[ 0.12618935  0.03282684  0.06530491 -0.08359849]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1162. State = [[-0.19246933  0.02272544]]. Action = [[ 0.1726355   0.10293758 -0.2216631  -0.06177771]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1163. State = [[-0.12358104  0.04472749]]. Action = [[ 0.16163722  0.02842364 -0.17991148 -0.574171  ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1164. State = [[-0.05322111  0.06729406]]. Action = [[-0.05280694 -0.21910489  0.02881727 -0.32898337]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1165. State = [[-0.25659305  0.00784174]]. Action = [[-0.09901205  0.02070633 -0.190089   -0.46620655]]. Reward = [1.]
Curr episode timestep = 4
Current timestep = 1166. State = [[-0.24650058  0.01019072]]. Action = [[ 0.15547848 -0.17465387  0.03922862 -0.67634237]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1167. State = [[-0.19506924  0.02088359]]. Action = [[ 0.21601701  0.16797578 -0.01871623 -0.49347186]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1168. State = [[-0.12451848  0.03766119]]. Action = [[ 0.0763869   0.22825634  0.1867671  -0.17886007]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1169. State = [[-0.04846307  0.06505384]]. Action = [[ 0.24262944 -0.06845367  0.24685985  0.63509023]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1169 is [False, True, False, False, True, False]
State prediction error at timestep 1169 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 1170. State = [[-0.25674918  0.00778738]]. Action = [[-0.14464755 -0.10146987 -0.1762334   0.17791247]]. Reward = [1.]
Curr episode timestep = 4
Current timestep = 1171. State = [[-0.24610184  0.01014944]]. Action = [[ 0.06379756 -0.16320051  0.19757605 -0.9683744 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1172. State = [[-0.19355276  0.02074385]]. Action = [[-0.20123717  0.18099773  0.18796548 -0.69243574]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1173. State = [[-0.12313582  0.03914087]]. Action = [[-0.24320766  0.03065255  0.12764761 -0.67365414]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1174. State = [[-0.04609234  0.06353779]]. Action = [[ 0.217744    0.02945438 -0.05346188  0.0859251 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1174 is [False, True, False, False, True, False]
State prediction error at timestep 1174 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 1175. State = [[-0.25653636  0.00771747]]. Action = [[ 0.0432421  -0.21441764 -0.14864834 -0.00391757]]. Reward = [1.]
Curr episode timestep = 4
Current timestep = 1176. State = [[-0.24815026  0.00972003]]. Action = [[ 0.08858949 -0.09939657  0.12459311 -0.8846143 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1177. State = [[-0.197832    0.02019931]]. Action = [[-0.10045925 -0.02041554  0.0846445  -0.32234907]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1178. State = [[-0.11947741  0.03450138]]. Action = [[ 0.02205792 -0.11363733 -0.14417978 -0.18895626]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1179. State = [[-0.03979774  0.05018429]]. Action = [[-0.23157291 -0.21196899  0.06529188 -0.3419118 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1179 is [False, True, False, False, True, False]
State prediction error at timestep 1179 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 1180. State = [[-0.25658867  0.00771745]]. Action = [[ 0.1175724  -0.01780263  0.13129944 -0.805319  ]]. Reward = [1.]
Curr episode timestep = 4
Current timestep = 1181. State = [[-0.24813008  0.00977432]]. Action = [[-0.11240403  0.16620493 -0.08171287 -0.71748817]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1182. State = [[-0.19796827  0.01991942]]. Action = [[ 0.16118637 -0.22936103  0.01811582 -0.12326133]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1183. State = [[-0.11823296  0.032664  ]]. Action = [[-0.16439779  0.02114701 -0.11814776 -0.98097396]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1184. State = [[-0.04325118  0.04120559]]. Action = [[-0.01941451 -0.15064503  0.10811657  0.19552314]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1184 is [False, True, False, False, True, False]
State prediction error at timestep 1184 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 1185. State = [[-0.25652835  0.0078564 ]]. Action = [[-0.189251    0.16931081 -0.22773369 -0.47020125]]. Reward = [1.]
Curr episode timestep = 4
Current timestep = 1186. State = [[-0.24718149  0.00995103]]. Action = [[-0.1337749   0.01485002 -0.09650376  0.43656373]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1187. State = [[-0.19649369  0.02014164]]. Action = [[0.23197055 0.15770069 0.18991753 0.95069873]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1188. State = [[-0.11784969  0.03294787]]. Action = [[-0.01756224  0.08546996 -0.16603726 -0.09876621]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1189. State = [[-0.04643659  0.04861519]]. Action = [[ 0.22200102  0.20047718 -0.10784651  0.5221795 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1189 is [False, True, False, False, True, False]
State prediction error at timestep 1189 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 1190. State = [[-0.25665578  0.00770293]]. Action = [[ 0.24151105 -0.07604304 -0.01205684 -0.72787684]]. Reward = [1.]
Curr episode timestep = 4
Current timestep = 1191. State = [[-0.24901381  0.00950055]]. Action = [[ 0.15790412  0.06992599 -0.02388713 -0.31136775]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1192. State = [[-0.19306251  0.02080764]]. Action = [[ 0.19192833 -0.07461447  0.2225464   0.55950046]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1193. State = [[-0.11949614  0.03299078]]. Action = [[-0.19319782 -0.03946464 -0.06375206 -0.39055854]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1194. State = [[-0.0396038   0.05120116]]. Action = [[ 0.17973739 -0.11368772 -0.02187482 -0.19578987]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1194 is [False, True, False, False, True, False]
State prediction error at timestep 1194 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 1195. State = [[-0.2565213   0.00777211]]. Action = [[-0.07651919  0.23155048  0.15190858  0.9229536 ]]. Reward = [1.]
Curr episode timestep = 4
Current timestep = 1196. State = [[-0.24586792  0.01027604]]. Action = [[-0.00397182  0.09679708  0.03908893 -0.01498479]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1197. State = [[-0.19251733  0.02101523]]. Action = [[ 0.24529004 -0.11484006 -0.09542    -0.8862143 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1198. State = [[-0.11255126  0.03465402]]. Action = [[ 0.09915665 -0.24748522  0.17540094  0.76391006]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1199. State = [[-0.03730084  0.04822153]]. Action = [[ 0.05827257 -0.12629542  0.0598428  -0.3399297 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1199 is [False, True, False, False, True, False]
State prediction error at timestep 1199 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 1200. State = [[-0.25679615  0.00772436]]. Action = [[ 0.19212031  0.10176194  0.04908991 -0.6756353 ]]. Reward = [1.]
Curr episode timestep = 4
Current timestep = 1201. State = [[-0.24650496  0.01009405]]. Action = [[-0.22322004  0.08223942  0.23386112 -0.08837408]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1202. State = [[-0.19346176  0.02029963]]. Action = [[-0.08351731  0.00926509  0.17092574  0.00358593]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1203. State = [[-0.11920962  0.03124431]]. Action = [[ 0.03516513 -0.16490987 -0.03396955  0.00646949]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1204. State = [[-0.0365086  0.0446308]]. Action = [[-0.09039965  0.06608415  0.22992247 -0.9147239 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1204 is [False, True, False, False, True, False]
State prediction error at timestep 1204 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 1205. State = [[-0.2566112   0.00776762]]. Action = [[-0.01129727 -0.13448639  0.18057933 -0.94965345]]. Reward = [1.]
Curr episode timestep = 4
Current timestep = 1206. State = [[-0.24927583  0.00944821]]. Action = [[-0.22374396 -0.00681935 -0.21457723 -0.6454181 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1207. State = [[-0.19339     0.02060965]]. Action = [[ 0.04625577 -0.19394094 -0.0348987   0.3594346 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1208. State = [[-0.11172754  0.03234804]]. Action = [[ 0.01475772 -0.05001147  0.03583011  0.53407264]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1209. State = [[-0.25665218  0.00762865]]. Action = [[ 0.14093381 -0.13001232  0.20021904  0.9303415 ]]. Reward = [1.]
Curr episode timestep = 3
Current timestep = 1210. State = [[-0.24712344  0.01006934]]. Action = [[ 0.12456357 -0.13756199 -0.19995928 -0.08257508]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1211. State = [[-0.19468261  0.02045091]]. Action = [[-0.09124754  0.09652084 -0.11115962 -0.16028744]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1212. State = [[-0.1129542   0.03219816]]. Action = [[-0.08341396  0.1216906  -0.21416889 -0.8525776 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1213. State = [[-0.25673193  0.00758515]]. Action = [[-0.09021968  0.03921294  0.19872695  0.93055344]]. Reward = [1.]
Curr episode timestep = 3
Current timestep = 1214. State = [[-0.24750629  0.00988682]]. Action = [[-0.07078603 -0.05387802  0.21439424 -0.63887274]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1215. State = [[-0.19568597  0.02021885]]. Action = [[ 0.06954768  0.16244319 -0.03758454 -0.6252854 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1216. State = [[-0.11467518  0.0322023 ]]. Action = [[-0.17658937 -0.23876084  0.13334692 -0.70236236]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1217. State = [[-0.03686659  0.03578981]]. Action = [[ 0.1545873   0.13236362  0.00087091 -0.25213027]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1217 is [False, True, False, False, True, False]
State prediction error at timestep 1217 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 1218. State = [[-0.25669384  0.00758499]]. Action = [[ 0.08432728 -0.13293086 -0.03951821  0.34912252]]. Reward = [1.]
Curr episode timestep = 4
Current timestep = 1219. State = [[-0.24690858  0.00994548]]. Action = [[ 0.21294463  0.00525382 -0.10546532  0.17601335]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1220. State = [[-0.19405837  0.02031123]]. Action = [[ 0.18471241 -0.1913354  -0.00937794 -0.5727414 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1221. State = [[-0.11182156  0.03074336]]. Action = [[ 0.24237043 -0.09417176 -0.00484052  0.30613708]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1222. State = [[-0.25666445  0.00753042]]. Action = [[-0.23141187  0.03046927 -0.02807404 -0.9513534 ]]. Reward = [1.]
Curr episode timestep = 3
Current timestep = 1223. State = [[-0.2490471   0.00943553]]. Action = [[-0.12797605  0.1339455  -0.19286408  0.31490004]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1224. State = [[-0.19200413  0.02041763]]. Action = [[ 0.20783702 -0.10886675  0.1425468   0.14539683]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1225. State = [[-0.11669368  0.02994316]]. Action = [[-0.00499533  0.22724369 -0.02629094  0.5974524 ]]. Reward = [0.]
Curr episode timestep = 2
