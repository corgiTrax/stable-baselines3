Current timestep = 0. State = [[-0.3052239  -0.19330133]]. Action = [[-0.06292832 -0.00783072  0.21038541 -0.03610903]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Current timestep = 1. State = [[-0.3052105  -0.19317716]]. Action = [[ 0.02804708  0.09211323 -0.17953753 -0.24135602]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, True, False, False]
Scene graph at timestep 1 is [True, False, False, True, False, False]
State prediction error at timestep 1 is tensor(0.0593, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.30506113 -0.19212872]]. Action = [[-0.15283313  0.10317436  0.08250085  0.97395396]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, True, False, False]
Current timestep = 3. State = [[-0.3057041  -0.18872137]]. Action = [[-0.15658152  0.18879378 -0.05679892 -0.21903908]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, True, False, False]
Scene graph at timestep 3 is [True, False, False, True, False, False]
State prediction error at timestep 3 is tensor(0.0588, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.30871978 -0.18304972]]. Action = [[-0.06515831 -0.21350987 -0.2270246   0.91687274]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, True, False, False]
Current timestep = 5. State = [[-0.31177667 -0.18362668]]. Action = [[-0.19404647  0.20411986 -0.19760707 -0.8448607 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, True, False, False]
Current timestep = 6. State = [[-0.3159145  -0.18009278]]. Action = [[ 0.24527463 -0.10609299 -0.19202721 -0.2252906 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, True, False, False]
Current timestep = 7. State = [[-0.31630945 -0.17852114]]. Action = [[-0.13173239  0.19834805 -0.12744653  0.7123525 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, True, False, False]
Scene graph at timestep 7 is [True, False, False, True, False, False]
State prediction error at timestep 7 is tensor(0.0729, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.3185546  -0.17041619]]. Action = [[ 0.18874025  0.10747921 -0.21657972 -0.08161724]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, True, False, False]
Current timestep = 9. State = [[-0.3179105  -0.16573258]]. Action = [[-0.17556798 -0.14458303  0.2087945   0.24431825]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, True, False, False]
Scene graph at timestep 9 is [True, False, False, True, False, False]
State prediction error at timestep 9 is tensor(0.0640, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9 of 0
Current timestep = 10. State = [[-0.3190627  -0.16529736]]. Action = [[ 0.16702765 -0.20503464 -0.198931   -0.67114556]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, True, False, False]
Scene graph at timestep 10 is [True, False, False, True, False, False]
State prediction error at timestep 10 is tensor(0.0443, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.31838137 -0.16858052]]. Action = [[-0.02317014 -0.1613437   0.12703657 -0.9684069 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, True, False, False]
Human Feedback received at timestep 11 of 0
Current timestep = 12. State = [[-0.31818113 -0.174388  ]]. Action = [[-0.02471386 -0.24597779  0.20099166 -0.614913  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, True, False, False]
Scene graph at timestep 12 is [True, False, False, True, False, False]
State prediction error at timestep 12 is tensor(0.0415, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.3181929  -0.18391694]]. Action = [[ 0.22643995 -0.13567218  0.19493079  0.24568582]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, True, False, False]
Scene graph at timestep 13 is [True, False, False, True, False, False]
State prediction error at timestep 13 is tensor(0.0677, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.31573713 -0.19347334]]. Action = [[ 0.18065628  0.03531045 -0.23489961 -0.4160719 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, True, False, False]
Human Feedback received at timestep 14 of 0
Current timestep = 15. State = [[-0.31164458 -0.19973016]]. Action = [[-0.1477273  -0.12928894 -0.13149224  0.7692232 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, True, False, False]
Current timestep = 16. State = [[-0.31071433 -0.20581831]]. Action = [[-0.22118598  0.18397707 -0.03226592  0.7894316 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, True, False, False]
Scene graph at timestep 16 is [True, False, False, True, False, False]
State prediction error at timestep 16 is tensor(0.0724, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of 0
Current timestep = 17. State = [[-0.31227225 -0.2068786 ]]. Action = [[ 0.01967832  0.2151779  -0.18030106 -0.33679414]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, True, False, False]
Current timestep = 18. State = [[-0.31221345 -0.20366931]]. Action = [[-0.08254196  0.20234978 -0.12120283 -0.731575  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, True, False, False]
Current timestep = 19. State = [[-0.31318414 -0.19821441]]. Action = [[ 0.00782257 -0.09399566  0.12785363 -0.25821555]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, True, False, False]
Scene graph at timestep 19 is [True, False, False, True, False, False]
State prediction error at timestep 19 is tensor(0.0543, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.31405422 -0.19482304]]. Action = [[ 0.15812397  0.14410222 -0.09557876  0.49787724]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, True, False, False]
Current timestep = 21. State = [[-0.31361794 -0.18992206]]. Action = [[ 0.14182323  0.01168245 -0.18392065  0.501621  ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, True, False, False]
Current timestep = 22. State = [[-0.31211597 -0.18533023]]. Action = [[ 0.1688351   0.24548686  0.09852752 -0.9514527 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, True, False, False]
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.30798313 -0.17616749]]. Action = [[-0.05702102  0.17219979  0.14208743  0.03624845]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, True, False, False]
Scene graph at timestep 23 is [True, False, False, True, False, False]
State prediction error at timestep 23 is tensor(0.0619, grad_fn=<MseLossBackward0>)
Current timestep = 24. State = [[-0.3057783  -0.16510993]]. Action = [[ 0.08372721  0.24889523  0.13376862 -0.682824  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, True, False, False]
Human Feedback received at timestep 24 of -1
Current timestep = 25. State = [[-0.3033965  -0.15301734]]. Action = [[-0.11555925 -0.20196092  0.24720162  0.63734305]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, True, False, False]
Current timestep = 26. State = [[-0.30263487 -0.14949772]]. Action = [[ 0.11335847  0.04647794 -0.01763617 -0.01341397]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, True, False, False]
Scene graph at timestep 26 is [True, False, False, True, False, False]
State prediction error at timestep 26 is tensor(0.0576, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 26 of -1
Current timestep = 27. State = [[-0.3014867  -0.14569311]]. Action = [[-0.1318131  -0.15813036  0.01346835 -0.01878482]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, True, False, False]
Scene graph at timestep 27 is [True, False, False, True, False, False]
State prediction error at timestep 27 is tensor(0.0515, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.3013786  -0.14661868]]. Action = [[-0.18267702  0.03736448 -0.0018138  -0.91120285]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, True, False, False]
Current timestep = 29. State = [[-0.30285937 -0.14686117]]. Action = [[ 0.12679619  0.13841623 -0.04986307 -0.4208697 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, True, False, False]
Current timestep = 30. State = [[-0.302755   -0.14373665]]. Action = [[-0.0342588   0.1584205  -0.23644787 -0.95380545]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, True, False, False]
Current timestep = 31. State = [[-0.3029147  -0.13852327]]. Action = [[-0.05987707  0.03214163 -0.05190513  0.84736323]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, True, False, False]
Current timestep = 32. State = [[-0.30363706 -0.13434601]]. Action = [[ 0.00109294 -0.17708966 -0.18974707 -0.4909749 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, True, False, False]
Current timestep = 33. State = [[-0.30475333 -0.13547136]]. Action = [[-0.19922154 -0.19781938  0.21593627 -0.9374181 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, True, False, False]
Current timestep = 34. State = [[-0.30772957 -0.13935739]]. Action = [[ 0.05743226  0.03883946 -0.23645777  0.09854746]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, True, False, False]
Current timestep = 35. State = [[-0.30909333 -0.14068261]]. Action = [[-0.0594888  -0.08989099  0.08439064  0.6699455 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, True, False, False]
Current timestep = 36. State = [[-0.31025985 -0.1419185 ]]. Action = [[ 0.23280835 -0.02098373  0.16651341  0.02484822]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, True, False, False]
Current timestep = 37. State = [[-0.30955663 -0.14208019]]. Action = [[ 0.16575176 -0.11865516  0.24713773  0.314659  ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, True, False, False]
Current timestep = 38. State = [[-0.30654433 -0.14537577]]. Action = [[-0.08253792 -0.17259337 -0.16319866 -0.56067777]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, True, False, False]
Current timestep = 39. State = [[-0.30570415 -0.15085836]]. Action = [[-0.0535952  -0.0970277  -0.09475388  0.5556381 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, True, False, False]
Current timestep = 40. State = [[-0.30574596 -0.15608156]]. Action = [[-0.22650304  0.23190928 -0.19892998  0.32077074]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, True, False, False]
Current timestep = 41. State = [[-0.30667844 -0.15637168]]. Action = [[ 0.10872632 -0.09703131  0.20763269 -0.17703742]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, True, False, False]
Scene graph at timestep 41 is [True, False, False, True, False, False]
State prediction error at timestep 41 is tensor(0.0500, grad_fn=<MseLossBackward0>)
Current timestep = 42. State = [[-0.3071285  -0.15680704]]. Action = [[-0.09357411  0.01792291  0.154369    0.46081853]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, True, False, False]
Scene graph at timestep 42 is [True, False, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0646, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of -1
Current timestep = 43. State = [[-0.30773136 -0.15755549]]. Action = [[ 0.15594012  0.13645342  0.0622842  -0.7390872 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, True, False, False]
Current timestep = 44. State = [[-0.30770233 -0.1564103 ]]. Action = [[-0.02267635  0.18309161  0.1395728  -0.0730992 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, True, False, False]
Scene graph at timestep 44 is [True, False, False, True, False, False]
State prediction error at timestep 44 is tensor(0.0584, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 44 of -1
Current timestep = 45. State = [[-0.3074745  -0.15332161]]. Action = [[ 0.11342704 -0.04019237  0.05631548 -0.51758724]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, True, False, False]
Scene graph at timestep 45 is [True, False, False, True, False, False]
State prediction error at timestep 45 is tensor(0.0440, grad_fn=<MseLossBackward0>)
Current timestep = 46. State = [[-0.3066277  -0.15061453]]. Action = [[0.09529731 0.10571244 0.22975743 0.02757192]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, True, False, False]
Scene graph at timestep 46 is [True, False, False, True, False, False]
State prediction error at timestep 46 is tensor(0.0573, grad_fn=<MseLossBackward0>)
Current timestep = 47. State = [[-0.3051602  -0.14870986]]. Action = [[-0.03294587 -0.06545065  0.09615448  0.755185  ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, True, False, False]
Current timestep = 48. State = [[-0.30512956 -0.14839858]]. Action = [[-0.1157887   0.05567047 -0.20120142  0.37288344]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, True, False, False]
Human Feedback received at timestep 48 of -1
Current timestep = 49. State = [[-0.30515844 -0.14779003]]. Action = [[ 0.1134553   0.07748425 -0.20218235  0.8847685 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, True, False, False]
Current timestep = 50. State = [[-0.3048838 -0.146168 ]]. Action = [[-0.07827386 -0.20051333 -0.12017211 -0.02700591]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, True, False, False]
Human Feedback received at timestep 50 of -1
Current timestep = 51. State = [[-0.30476955 -0.14724895]]. Action = [[ 0.2068848   0.22652376 -0.03586359  0.1242373 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, True, False, False]
Scene graph at timestep 51 is [True, False, False, True, False, False]
State prediction error at timestep 51 is tensor(0.0662, grad_fn=<MseLossBackward0>)
Current timestep = 52. State = [[-0.30254006 -0.14452319]]. Action = [[ 0.18189603 -0.20764373  0.18816039  0.5712166 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, True, False, False]
Current timestep = 53. State = [[-0.29881117 -0.14649844]]. Action = [[-0.11441588 -0.05674538 -0.12370075 -0.8974389 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, True, False, False]
Current timestep = 54. State = [[-0.2973199  -0.14881083]]. Action = [[-0.12237573 -0.2230823   0.20830828  0.16367233]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, True, False, False]
Current timestep = 55. State = [[-0.2972025  -0.15562043]]. Action = [[-0.08687785 -0.17729017 -0.08210889  0.7207265 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, True, False, False]
Current timestep = 56. State = [[-0.29820976 -0.16422173]]. Action = [[ 0.05953512  0.11114797 -0.14817959  0.91396666]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, True, False, False]
Scene graph at timestep 56 is [True, False, False, True, False, False]
State prediction error at timestep 56 is tensor(0.0596, grad_fn=<MseLossBackward0>)
Current timestep = 57. State = [[-0.29805583 -0.16772524]]. Action = [[ 0.23208168 -0.15677391  0.2330029  -0.27012765]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, True, False, False]
Current timestep = 58. State = [[-0.29668248 -0.1732119 ]]. Action = [[-0.01688263 -0.17942126 -0.09049878  0.02913284]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, True, False, False]
Scene graph at timestep 58 is [True, False, False, True, False, False]
State prediction error at timestep 58 is tensor(0.0567, grad_fn=<MseLossBackward0>)
Current timestep = 59. State = [[-0.29588497 -0.18109052]]. Action = [[-0.22462757  0.22966427  0.03413746 -0.01006943]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, True, False, False]
Current timestep = 60. State = [[-0.29656568 -0.18152384]]. Action = [[ 0.08144125  0.15033007 -0.04957311  0.36233544]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, True, False, False]
Current timestep = 61. State = [[-0.29622278 -0.17914715]]. Action = [[ 0.21871018 -0.00888601  0.11972737  0.6086904 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, True, False, False]
Current timestep = 62. State = [[-0.29427686 -0.17768626]]. Action = [[-0.015443   -0.1623418  -0.07818675 -0.59949154]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, True, False, False]
Current timestep = 63. State = [[-0.29299966 -0.17895438]]. Action = [[ 0.09055582  0.1572401   0.16502541 -0.9037737 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, True, False, False]
Current timestep = 64. State = [[-0.2903664  -0.17675543]]. Action = [[-0.11106697  0.02211285 -0.24506785 -0.55001134]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, True, False, False]
Human Feedback received at timestep 64 of 1
Current timestep = 65. State = [[-0.28995538 -0.17578009]]. Action = [[-0.18644704  0.01333836  0.17273095 -0.73126566]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, True, False, False]
Current timestep = 66. State = [[-0.29128313 -0.1752743 ]]. Action = [[ 0.07175013 -0.1642149  -0.24593687 -0.78596085]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, True, False, False]
Scene graph at timestep 66 is [True, False, False, True, False, False]
State prediction error at timestep 66 is tensor(0.0326, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 66 of 1
Current timestep = 67. State = [[-0.29151812 -0.1774112 ]]. Action = [[ 0.24854136 -0.08151072  0.20405278  0.7270007 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, True, False, False]
Scene graph at timestep 67 is [True, False, False, True, False, False]
State prediction error at timestep 67 is tensor(0.0598, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of 1
Current timestep = 68. State = [[-0.28852952 -0.17931204]]. Action = [[0.03160676 0.21968237 0.19586185 0.43771553]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, True, False, False]
Human Feedback received at timestep 68 of 1
Current timestep = 69. State = [[-0.2860399  -0.17727701]]. Action = [[-0.20742673 -0.13721892  0.14055955  0.3013432 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, True, False, False]
Current timestep = 70. State = [[-0.28660753 -0.17882363]]. Action = [[-0.03636354 -0.05990322 -0.18529083  0.02888215]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, True, False, False]
Scene graph at timestep 70 is [True, False, False, True, False, False]
State prediction error at timestep 70 is tensor(0.0564, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of 1
Current timestep = 71. State = [[-0.2874052  -0.18098758]]. Action = [[ 0.01221383 -0.01296236  0.090105   -0.54344183]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, True, False, False]
Human Feedback received at timestep 71 of 1
Current timestep = 72. State = [[-0.28805384 -0.18260962]]. Action = [[-0.16936138 -0.06833398 -0.01987658  0.9019809 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, True, False, False]
Current timestep = 73. State = [[-0.29043958 -0.1848227 ]]. Action = [[ 0.04703778  0.2433297   0.10549861 -0.14095664]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, True, False, False]
Human Feedback received at timestep 73 of 1
Current timestep = 74. State = [[-0.29143265 -0.18297412]]. Action = [[-0.04591274 -0.21344574  0.05896783 -0.5593137 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, True, False, False]
Current timestep = 75. State = [[-0.29306757 -0.1857712 ]]. Action = [[-0.11358589 -0.18174565  0.24147367 -0.39439988]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, True, False, False]
Human Feedback received at timestep 75 of 0
Current timestep = 76. State = [[-0.2962435 -0.1918693]]. Action = [[-9.40460265e-02  4.54038382e-04 -1.00999266e-01  7.13565111e-01]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, True, False, False]
Current timestep = 77. State = [[-0.29850653 -0.19346052]]. Action = [[-0.03371632 -0.19152136 -0.07343674 -0.311545  ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, True, False, False]
Current timestep = 78. State = [[-0.30062222 -0.19627827]]. Action = [[ 0.022044   -0.2426457   0.10017937  0.51455307]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, True, False, False]
Current timestep = 79. State = [[-0.30314383 -0.20035979]]. Action = [[-0.2102649  -0.13816953 -0.21217577 -0.64967614]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, True, False, False]
Scene graph at timestep 79 is [True, False, False, True, False, False]
State prediction error at timestep 79 is tensor(0.0418, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.30703357 -0.20623666]]. Action = [[-0.0068455  -0.16103208 -0.06041899 -0.23225856]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, True, False, False]
Current timestep = 81. State = [[-0.31033912 -0.21169284]]. Action = [[ 0.13097224 -0.0734749  -0.16514692 -0.47577   ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, True, False, False]
Current timestep = 82. State = [[-0.3107833  -0.21612495]]. Action = [[-0.202117    0.23224825 -0.12801798 -0.83660877]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, True, False, False]
Current timestep = 83. State = [[-0.31244737 -0.21774802]]. Action = [[ 0.2389484   0.02112851  0.07512534 -0.7967107 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, True, False, False]
Current timestep = 84. State = [[-0.31189254 -0.21805924]]. Action = [[ 0.02619484 -0.00192858 -0.2153943  -0.9035846 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, True, False, False]
Human Feedback received at timestep 84 of -1
Current timestep = 85. State = [[-0.3113964 -0.2184165]]. Action = [[ 0.09412518  0.08068794 -0.08480367 -0.30874026]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, True, False, False]
Scene graph at timestep 85 is [True, False, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0621, grad_fn=<MseLossBackward0>)
Current timestep = 86. State = [[-0.310319  -0.2186749]]. Action = [[-0.04189269 -0.13539053  0.167485   -0.28540534]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, True, False, False]
Current timestep = 87. State = [[-0.30983052 -0.21918789]]. Action = [[-0.13219015 -0.04103981  0.03815037 -0.12867564]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, True, False, False]
Current timestep = 88. State = [[-0.31057236 -0.22040902]]. Action = [[-0.00802706 -0.01777503 -0.12589616  0.3568728 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, True, False, False]
Scene graph at timestep 88 is [True, False, False, True, False, False]
State prediction error at timestep 88 is tensor(0.0781, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of -1
Current timestep = 89. State = [[-0.31179968 -0.22193639]]. Action = [[-1.2616846e-01 -2.2613351e-01  7.9822540e-04  9.8299289e-01]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, True, False, False]
Scene graph at timestep 89 is [True, False, False, True, False, False]
State prediction error at timestep 89 is tensor(0.0736, grad_fn=<MseLossBackward0>)
Current timestep = 90. State = [[-0.31435084 -0.22539148]]. Action = [[-0.17308962  0.20423535 -0.04385433  0.30456305]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, True, False, False]
Scene graph at timestep 90 is [True, False, False, True, False, False]
State prediction error at timestep 90 is tensor(0.0774, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of -1
Current timestep = 91. State = [[-0.31777084 -0.22708575]]. Action = [[-0.24324673  0.13268635 -0.24387646 -0.79303753]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, True, False, False]
Current timestep = 92. State = [[-0.32300866 -0.22764374]]. Action = [[ 0.05326334 -0.16724561  0.03834906 -0.19529057]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, True, False, False]
Current timestep = 93. State = [[-0.3255636  -0.23003535]]. Action = [[ 0.00920275  0.15086064 -0.07823598  0.37502968]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, True, False, False]
Current timestep = 94. State = [[-0.32708186 -0.2303384 ]]. Action = [[0.07915059 0.01438561 0.04716134 0.10213578]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, True, False, False]
Scene graph at timestep 94 is [True, False, False, True, False, False]
State prediction error at timestep 94 is tensor(0.0779, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of -1
Current timestep = 95. State = [[-0.3273762  -0.23013997]]. Action = [[-0.07474667  0.10044095 -0.15577957 -0.81921107]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, True, False, False]
Current timestep = 96. State = [[-0.32826734 -0.22938544]]. Action = [[-0.02428597  0.07535601 -0.1852176  -0.04370892]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, True, False, False]
Current timestep = 97. State = [[-0.3291448 -0.2288459]]. Action = [[ 0.20194349  0.10286158  0.20833257 -0.6593675 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, True, False, False]
Current timestep = 98. State = [[-0.3290006  -0.22709036]]. Action = [[-0.09913734  0.04422888  0.02979702  0.92147255]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, True, False, False]
Current timestep = 99. State = [[-0.32951   -0.2261211]]. Action = [[-0.09856635  0.0674566  -0.21167783 -0.16999316]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, True, False, False]
Current timestep = 100. State = [[-0.33064964 -0.22521311]]. Action = [[-0.15079565 -0.21940632 -0.02369845  0.11940181]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, True, False, False]
Scene graph at timestep 100 is [True, False, False, True, False, False]
State prediction error at timestep 100 is tensor(0.0761, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 100 of -1
Current timestep = 101. State = [[-0.33325464 -0.2266164 ]]. Action = [[-0.22904678 -0.02748613  0.116041    0.28773856]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, True, False, False]
Current timestep = 102. State = [[-0.33796415 -0.22807543]]. Action = [[0.09275281 0.10883743 0.06741947 0.2768674 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, True, False, False]
Current timestep = 103. State = [[-0.33993882 -0.22795393]]. Action = [[ 0.22295952 -0.02079807 -0.20881678 -0.28873467]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, True, False, False]
Current timestep = 104. State = [[-0.33995336 -0.22792947]]. Action = [[-0.10683371 -0.1037007   0.21886036 -0.7239744 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, True, False, False]
Current timestep = 105. State = [[-0.34018505 -0.22830167]]. Action = [[-0.0370544  -0.08820483 -0.06997551 -0.9768368 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, True, False, False]
Current timestep = 106. State = [[-0.34080458 -0.22922437]]. Action = [[ 0.20893937 -0.03371015  0.20521957 -0.11461991]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, True, False, False]
Scene graph at timestep 106 is [True, False, False, True, False, False]
State prediction error at timestep 106 is tensor(0.0735, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 106 of -1
Current timestep = 107. State = [[-0.33928597 -0.23010786]]. Action = [[-0.02727693 -0.0242108   0.22461662 -0.08456141]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, True, False, False]
Current timestep = 108. State = [[-0.33871865 -0.23037162]]. Action = [[-0.03981924 -0.0557791  -0.02126198 -0.52216256]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, True, False, False]
Human Feedback received at timestep 108 of -1
Current timestep = 109. State = [[-0.339048   -0.23090626]]. Action = [[ 0.14718503  0.10361809 -0.11176175 -0.8177222 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, True, False, False]
Current timestep = 110. State = [[-0.33766943 -0.23147452]]. Action = [[ 0.20218009  0.0075199   0.15887168 -0.9461646 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, True, False, False]
Current timestep = 111. State = [[-0.33401757 -0.23163301]]. Action = [[ 0.20505464  0.19873732 -0.13456756  0.9906614 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, True, False, False]
Current timestep = 112. State = [[-0.32935116 -0.22980234]]. Action = [[ 0.2132181  -0.03087531  0.17487937 -0.96350336]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, True, False, False]
Human Feedback received at timestep 112 of -1
Current timestep = 113. State = [[-0.3230568  -0.22788666]]. Action = [[-0.1854473   0.08423623  0.15571609  0.84478045]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, True, False, False]
Scene graph at timestep 113 is [True, False, False, True, False, False]
State prediction error at timestep 113 is tensor(0.0781, grad_fn=<MseLossBackward0>)
Current timestep = 114. State = [[-0.32159907 -0.22583431]]. Action = [[ 0.17291403  0.24345651 -0.08439955  0.2454791 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, True, False, False]
Human Feedback received at timestep 114 of -1
Current timestep = 115. State = [[-0.31834418 -0.22060202]]. Action = [[-0.02897206  0.20259047  0.20263237 -0.38037252]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, True, False, False]
Current timestep = 116. State = [[-0.3156204  -0.21415135]]. Action = [[ 0.08030078 -0.07574013 -0.17948721  0.67524683]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, True, False, False]
Human Feedback received at timestep 116 of -1
Current timestep = 117. State = [[-0.3133248  -0.20979898]]. Action = [[0.14925262 0.21274069 0.03794622 0.4118315 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, True, False, False]
Current timestep = 118. State = [[-0.31023386 -0.20147456]]. Action = [[-0.0137002   0.07842419  0.22548386  0.8506783 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, True, False, False]
Current timestep = 119. State = [[-0.30825597 -0.19457226]]. Action = [[ 0.18740013  0.12822524  0.15111178 -0.5467224 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, True, False, False]
Scene graph at timestep 119 is [True, False, False, True, False, False]
State prediction error at timestep 119 is tensor(0.0475, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.3029784  -0.18504864]]. Action = [[ 0.03344995 -0.03253202 -0.18300799 -0.27900124]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, True, False, False]
Current timestep = 121. State = [[-0.2982381  -0.17808774]]. Action = [[-0.1797661   0.15656316  0.21257979 -0.14114434]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, True, False, False]
Current timestep = 122. State = [[-0.29670897 -0.1723102 ]]. Action = [[ 0.21754336 -0.03369781 -0.02604298  0.5711534 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, True, False, False]
Current timestep = 123. State = [[-0.29391873 -0.16789095]]. Action = [[-0.04699215 -0.08934003  0.03357944  0.10692334]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, True, False, False]
Human Feedback received at timestep 123 of 1
Current timestep = 124. State = [[-0.29271406 -0.16670544]]. Action = [[ 0.04003665 -0.05808204  0.1481753   0.21763086]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, True, False, False]
Scene graph at timestep 124 is [True, False, False, True, False, False]
State prediction error at timestep 124 is tensor(0.0581, grad_fn=<MseLossBackward0>)
Current timestep = 125. State = [[-0.29115131 -0.1677698 ]]. Action = [[-0.1978653   0.02526319 -0.0407638  -0.05132365]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, True, False, False]
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.29111993 -0.16708235]]. Action = [[ 0.17063957 -0.01758155  0.12360758 -0.04624939]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, True, False, False]
Scene graph at timestep 126 is [True, False, False, True, False, False]
State prediction error at timestep 126 is tensor(0.0526, grad_fn=<MseLossBackward0>)
Current timestep = 127. State = [[-0.2902937  -0.16720729]]. Action = [[-0.11885005  0.01701701 -0.03999653  0.01879144]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, True, False, False]
Human Feedback received at timestep 127 of 1
Current timestep = 128. State = [[-0.29057628 -0.1664197 ]]. Action = [[-0.24058092  0.05086353 -0.04525606  0.18736851]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, True, False, False]
Current timestep = 129. State = [[-0.29366422 -0.16588679]]. Action = [[-0.24276508  0.19436371 -0.2091362  -0.909334  ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, True, False, False]
Human Feedback received at timestep 129 of 1
Current timestep = 130. State = [[-0.30088404 -0.16341868]]. Action = [[-0.212147   -0.20059066  0.02537453  0.97973585]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, True, False, False]
Current timestep = 131. State = [[-0.30694947 -0.16361094]]. Action = [[ 0.06131846  0.21765405  0.14906627 -0.33817333]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, True, False, False]
Scene graph at timestep 131 is [True, False, False, True, False, False]
State prediction error at timestep 131 is tensor(0.0525, grad_fn=<MseLossBackward0>)
Current timestep = 132. State = [[-0.30891633 -0.1608157 ]]. Action = [[-3.9282441e-04 -1.8119034e-01 -6.9955885e-02 -9.3395925e-01]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, True, False, False]
Current timestep = 133. State = [[-0.31039953 -0.16072756]]. Action = [[ 0.04398781 -0.1731964   0.03767967  0.3322369 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, True, False, False]
Human Feedback received at timestep 133 of -1
Current timestep = 134. State = [[-0.31176698 -0.1625739 ]]. Action = [[ 0.20921946 -0.22330736 -0.07237035 -0.1830532 ]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, True, False, False]
Scene graph at timestep 134 is [True, False, False, True, False, False]
State prediction error at timestep 134 is tensor(0.0555, grad_fn=<MseLossBackward0>)
Current timestep = 135. State = [[-0.30999595 -0.1656781 ]]. Action = [[-0.10398477  0.06595671 -0.08796173 -0.263579  ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, True, False, False]
Human Feedback received at timestep 135 of -1
Current timestep = 136. State = [[-0.3105822  -0.16689718]]. Action = [[ 0.10536161 -0.0464419   0.116898   -0.5455249 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, True, False, False]
Scene graph at timestep 136 is [True, False, False, True, False, False]
State prediction error at timestep 136 is tensor(0.0444, grad_fn=<MseLossBackward0>)
Current timestep = 137. State = [[-0.3094221  -0.16758424]]. Action = [[ 0.11343995  0.10054204 -0.23627533 -0.8251441 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, True, False, False]
Current timestep = 138. State = [[-0.30783272 -0.16788428]]. Action = [[-0.02240205 -0.21621992  0.01480576 -0.6011182 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, True, False, False]
Current timestep = 139. State = [[-0.3066569  -0.16918531]]. Action = [[ 0.00360641 -0.24350345 -0.21222901  0.4732709 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, True, False, False]
Scene graph at timestep 139 is [True, False, False, True, False, False]
State prediction error at timestep 139 is tensor(0.0689, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 139 of -1
Current timestep = 140. State = [[-0.3055222  -0.17308891]]. Action = [[ 0.21982399  0.1689884  -0.12315628  0.75975275]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, True, False, False]
Current timestep = 141. State = [[-0.30222318 -0.17404607]]. Action = [[ 0.1033833   0.22558865 -0.03103516 -0.06988275]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, True, False, False]
Current timestep = 142. State = [[-0.29911858 -0.1735493 ]]. Action = [[-0.0604167   0.17406571  0.14437044  0.17399645]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, True, False, False]
Human Feedback received at timestep 142 of -1
Current timestep = 143. State = [[-0.29834443 -0.17159005]]. Action = [[ 0.10276279  0.22698045 -0.21996301 -0.52428895]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, True, False, False]
Current timestep = 144. State = [[-0.29605076 -0.16676791]]. Action = [[ 0.15435696 -0.02193694 -0.171608   -0.70893675]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, True, False, False]
Human Feedback received at timestep 144 of -1
Current timestep = 145. State = [[-0.2911196  -0.16388112]]. Action = [[ 0.21616817 -0.06358007 -0.03994004  0.4767716 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, True, False, False]
Scene graph at timestep 145 is [True, False, False, True, False, False]
State prediction error at timestep 145 is tensor(0.0625, grad_fn=<MseLossBackward0>)
Current timestep = 146. State = [[-0.2836612  -0.16390654]]. Action = [[ 0.00769588 -0.09368058 -0.1350201   0.24625003]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, True, False, False]
Current timestep = 147. State = [[-0.27754733 -0.16565153]]. Action = [[-0.18548284  0.04960519  0.105371   -0.9420088 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, True, False, False]
Current timestep = 148. State = [[-0.27636    -0.16634786]]. Action = [[-0.05987763  0.03108501 -0.16109483 -0.21106172]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, True, False, False]
Scene graph at timestep 148 is [True, False, False, True, False, False]
State prediction error at timestep 148 is tensor(0.0452, grad_fn=<MseLossBackward0>)
Current timestep = 149. State = [[-0.2759777  -0.16610779]]. Action = [[-0.04991546  0.1909563   0.15661633 -0.31137848]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, True, False, False]
Current timestep = 150. State = [[-0.27582595 -0.16283873]]. Action = [[ 0.01599884 -0.11815986 -0.0712131  -0.23423773]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, True, False, False]
Human Feedback received at timestep 150 of 1
Current timestep = 151. State = [[-0.27598372 -0.16239345]]. Action = [[-0.08839943  0.19495249  0.03480902 -0.5698542 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, True, False, False]
Current timestep = 152. State = [[-0.27637276 -0.15918687]]. Action = [[-0.21367085 -0.1041984   0.20831594 -0.30824268]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, True, False, False]
Current timestep = 153. State = [[-0.28126737 -0.15874141]]. Action = [[-0.04773346 -0.14984342 -0.06717011 -0.49667573]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, True, False, False]
Current timestep = 154. State = [[-0.2861514 -0.1605192]]. Action = [[-0.04414341 -0.2276486   0.2119298   0.677847  ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, True, False, False]
Scene graph at timestep 154 is [True, False, False, True, False, False]
State prediction error at timestep 154 is tensor(0.0577, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of 1
Current timestep = 155. State = [[-0.2900485  -0.16454442]]. Action = [[-0.16135721 -0.07142517 -0.20095442 -0.58711743]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, True, False, False]
Scene graph at timestep 155 is [True, False, False, True, False, False]
State prediction error at timestep 155 is tensor(0.0376, grad_fn=<MseLossBackward0>)
Current timestep = 156. State = [[-0.29579875 -0.1684055 ]]. Action = [[ 0.24078846 -0.1651571   0.21897483  0.2608112 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, True, False, False]
Human Feedback received at timestep 156 of -1
Current timestep = 157. State = [[-0.29697672 -0.17176405]]. Action = [[-0.14201003 -0.1747597  -0.1260021   0.16281736]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, True, False, False]
Current timestep = 158. State = [[-0.29865173 -0.17615762]]. Action = [[-0.04772323 -0.1936308   0.18407002 -0.32034814]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, True, False, False]
Scene graph at timestep 158 is [True, False, False, True, False, False]
State prediction error at timestep 158 is tensor(0.0438, grad_fn=<MseLossBackward0>)
Current timestep = 159. State = [[-0.30124158 -0.18132582]]. Action = [[ 0.20126098 -0.18334974 -0.09023497  0.94406533]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, True, False, False]
Scene graph at timestep 159 is [True, False, False, True, False, False]
State prediction error at timestep 159 is tensor(0.0581, grad_fn=<MseLossBackward0>)
Current timestep = 160. State = [[-0.29999053 -0.1860025 ]]. Action = [[ 0.06107664 -0.01960391  0.06645605 -0.8740796 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, True, False, False]
Scene graph at timestep 160 is [True, False, False, True, False, False]
State prediction error at timestep 160 is tensor(0.0322, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of -1
Current timestep = 161. State = [[-0.29794025 -0.1895431 ]]. Action = [[0.19856533 0.08630535 0.1669032  0.36968005]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, True, False, False]
Current timestep = 162. State = [[-0.29434055 -0.19473831]]. Action = [[-0.02202916 -0.0201029   0.1141777  -0.33206046]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, True, False, False]
Scene graph at timestep 162 is [True, False, False, True, False, False]
State prediction error at timestep 162 is tensor(0.0475, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.29203653 -0.19814755]]. Action = [[-0.08309558  0.14805996 -0.175633    0.6142738 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, True, False, False]
Current timestep = 164. State = [[-0.29167837 -0.20219447]]. Action = [[ 0.08665794  0.09695548 -0.22014058 -0.14046508]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, True, False, False]
Current timestep = 165. State = [[-0.29039213 -0.20404951]]. Action = [[ 0.2477481   0.14907333 -0.16502199 -0.28255308]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, True, False, False]
Scene graph at timestep 165 is [True, False, False, True, False, False]
State prediction error at timestep 165 is tensor(0.0554, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.2843469  -0.20388019]]. Action = [[ 0.08860108  0.03889459  0.03688529 -0.71008694]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, True, False, False]
Scene graph at timestep 166 is [True, False, False, True, False, False]
State prediction error at timestep 166 is tensor(0.0356, grad_fn=<MseLossBackward0>)
Current timestep = 167. State = [[-0.27782124 -0.20315574]]. Action = [[ 0.15250719 -0.04388523 -0.20775358 -0.9477239 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, True, False, False]
Human Feedback received at timestep 167 of -1
Current timestep = 168. State = [[-0.2708137  -0.20297554]]. Action = [[ 0.13893068  0.17110065 -0.16432643  0.3619163 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, True, False, False]
Scene graph at timestep 168 is [True, False, False, True, False, False]
State prediction error at timestep 168 is tensor(0.0645, grad_fn=<MseLossBackward0>)
Current timestep = 169. State = [[-0.2610075  -0.19986355]]. Action = [[ 0.21873432 -0.2376918   0.13337493 -0.6846394 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, True, False, False]
Scene graph at timestep 169 is [True, False, False, True, False, False]
State prediction error at timestep 169 is tensor(0.0303, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of 1
Current timestep = 170. State = [[-0.2508194  -0.20189422]]. Action = [[ 0.00389519  0.1709145  -0.05306125 -0.57995296]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, True, False, False]
Human Feedback received at timestep 170 of 1
Current timestep = 171. State = [[-0.24224627 -0.20014001]]. Action = [[ 0.12400207  0.0537281   0.10981166 -0.50004685]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, True, False, False]
Current timestep = 172. State = [[-0.2349384  -0.19747509]]. Action = [[-0.1143738   0.15419835 -0.00956962 -0.32204366]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, True, False, False]
Human Feedback received at timestep 172 of 1
Current timestep = 173. State = [[-0.23143354 -0.19357537]]. Action = [[-0.10765925 -0.24071933  0.11024138  0.6125511 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, True, False, False]
Current timestep = 174. State = [[-0.23150174 -0.19588645]]. Action = [[-0.17602418  0.03908932  0.22933602  0.65946627]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, True, False, False]
Current timestep = 175. State = [[-0.23261581 -0.19713327]]. Action = [[-0.00339782  0.09062034  0.18818784 -0.50893474]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, True, False, False]
Scene graph at timestep 175 is [True, False, False, True, False, False]
State prediction error at timestep 175 is tensor(0.0295, grad_fn=<MseLossBackward0>)
Current timestep = 176. State = [[-0.2332188  -0.19612499]]. Action = [[ 0.17917651 -0.03283335  0.23054433 -0.30285203]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, True, False, False]
Scene graph at timestep 176 is [True, False, False, True, False, False]
State prediction error at timestep 176 is tensor(0.0333, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of 1
Current timestep = 177. State = [[-0.23285253 -0.1949762 ]]. Action = [[ 0.00195935  0.18051466 -0.01837017 -0.06658101]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, True, False, False]
Human Feedback received at timestep 177 of 1
Current timestep = 178. State = [[-0.23190865 -0.1905811 ]]. Action = [[ 0.08475149  0.16424847  0.05031881 -0.03266454]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, True, False, False]
Current timestep = 179. State = [[-0.22981845 -0.18402866]]. Action = [[ 0.07929349 -0.21524003  0.06381196 -0.6378873 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, True, False, False]
Human Feedback received at timestep 179 of 1
Current timestep = 180. State = [[-0.22794735 -0.18404631]]. Action = [[-0.05665383 -0.11533035 -0.13117161  0.69155   ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, True, False, False]
Current timestep = 181. State = [[-0.22743781 -0.18649128]]. Action = [[-0.12755789  0.08888021  0.14275604  0.8183851 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, True, False, False]
Current timestep = 182. State = [[-0.2272826  -0.18711285]]. Action = [[ 0.10639477 -0.22023146  0.18306822 -0.8239961 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, True, False, False]
Human Feedback received at timestep 182 of 1
Current timestep = 183. State = [[-0.2273099  -0.19054273]]. Action = [[-0.17693232  0.21541941  0.19861531 -0.18445301]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, True, False, False]
Scene graph at timestep 183 is [True, False, False, True, False, False]
State prediction error at timestep 183 is tensor(0.0340, grad_fn=<MseLossBackward0>)
Current timestep = 184. State = [[-0.22807834 -0.18931994]]. Action = [[0.06473324 0.06303507 0.11400709 0.8529171 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, True, False, False]
Current timestep = 185. State = [[-0.2276882  -0.18734774]]. Action = [[ 0.16424423  0.04459471 -0.20879027 -0.38379776]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, True, False, False]
Current timestep = 186. State = [[-0.22660658 -0.1850019 ]]. Action = [[-0.14548479 -0.16777208  0.02824131 -0.94576174]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, True, False, False]
Human Feedback received at timestep 186 of 1
Current timestep = 187. State = [[-0.22664368 -0.18613102]]. Action = [[0.18637186 0.2318497  0.13847488 0.47669947]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, True, False, False]
Scene graph at timestep 187 is [True, False, False, True, False, False]
State prediction error at timestep 187 is tensor(0.0438, grad_fn=<MseLossBackward0>)
Current timestep = 188. State = [[-0.22467549 -0.18241423]]. Action = [[ 0.13150212  0.00635302  0.02051318 -0.57161003]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, True, False, False]
Scene graph at timestep 188 is [True, False, False, True, False, False]
State prediction error at timestep 188 is tensor(0.0241, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of 1
Current timestep = 189. State = [[-0.22105686 -0.17962357]]. Action = [[0.21966851 0.23917645 0.16514823 0.1119225 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, True, False, False]
Human Feedback received at timestep 189 of 1
Current timestep = 190. State = [[-0.21363936 -0.17246756]]. Action = [[-0.13592981 -0.21215163 -0.08386201  0.78376555]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, True, False, False]
Current timestep = 191. State = [[-0.21094197 -0.17232338]]. Action = [[-0.01555954 -0.02376109  0.07114136 -0.7951282 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, True, False, False]
Current timestep = 192. State = [[-0.20916331 -0.17299043]]. Action = [[0.19617379 0.09813368 0.16204616 0.23868382]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, True, False, False]
Scene graph at timestep 192 is [True, False, False, True, False, False]
State prediction error at timestep 192 is tensor(0.0354, grad_fn=<MseLossBackward0>)
Current timestep = 193. State = [[-0.20443423 -0.17128946]]. Action = [[ 0.21188438 -0.24325876  0.18500295 -0.85029477]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, True, False, False]
Current timestep = 194. State = [[-0.19705763 -0.17396195]]. Action = [[ 0.0614987   0.23780507 -0.0826121  -0.0240348 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, True, False, False]
Current timestep = 195. State = [[-0.18932967 -0.17100799]]. Action = [[-0.06690162 -0.17114142  0.12228313  0.64145064]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, True, False, False]
Human Feedback received at timestep 195 of 1
Current timestep = 196. State = [[-0.18523231 -0.17326401]]. Action = [[-0.04993281 -0.15844724  0.22818685 -0.46609902]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, True, False, False]
Current timestep = 197. State = [[-0.18366821 -0.17780596]]. Action = [[-0.07283235  0.24509746  0.24752    -0.04217392]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, True, False, False]
Scene graph at timestep 197 is [True, False, False, True, False, False]
State prediction error at timestep 197 is tensor(0.0245, grad_fn=<MseLossBackward0>)
Current timestep = 198. State = [[-0.18344675 -0.17544064]]. Action = [[-0.01295364  0.05526581 -0.23472893 -0.13174242]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, True, False, False]
Human Feedback received at timestep 198 of 1
Current timestep = 199. State = [[-0.18347172 -0.17319596]]. Action = [[0.03442681 0.07938659 0.20709142 0.46658075]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, True, False, False]
Current timestep = 200. State = [[-0.18305238 -0.17007993]]. Action = [[ 0.22167093 -0.15932208 -0.12230529  0.8047106 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, True, False, False]
Current timestep = 201. State = [[-0.18007348 -0.17055626]]. Action = [[-0.24909319  0.11405769 -0.18466775 -0.7303585 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, True, False, False]
Current timestep = 202. State = [[-0.18012999 -0.16919173]]. Action = [[ 0.03432423  0.1850369   0.06134829 -0.37497103]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, True, False, False]
Current timestep = 203. State = [[-0.18015954 -0.16424042]]. Action = [[0.07699198 0.07341385 0.20333654 0.88116693]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, True, False, False]
Scene graph at timestep 203 is [True, False, False, True, False, False]
State prediction error at timestep 203 is tensor(0.0271, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of 1
Current timestep = 204. State = [[-0.17981012 -0.15817252]]. Action = [[-0.23884155  0.01859024  0.18437639  0.56615937]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, True, False, False]
Current timestep = 205. State = [[-0.18067747 -0.15502475]]. Action = [[ 0.19656172 -0.03054556  0.1307362  -0.00350201]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, True, False, False]
Scene graph at timestep 205 is [True, False, False, True, False, False]
State prediction error at timestep 205 is tensor(0.0241, grad_fn=<MseLossBackward0>)
Current timestep = 206. State = [[-0.18040903 -0.1530978 ]]. Action = [[-0.06895259 -0.07097568 -0.12628926  0.43885493]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, True, False, False]
Current timestep = 207. State = [[-0.18057725 -0.15385975]]. Action = [[-0.16641675 -0.11855742 -0.23823376  0.9427171 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, True, False, False]
Scene graph at timestep 207 is [True, False, False, True, False, False]
State prediction error at timestep 207 is tensor(0.0275, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 207 of 1
Current timestep = 208. State = [[-0.18229797 -0.15772481]]. Action = [[ 0.2343004  -0.13623904 -0.21723409  0.99166226]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, True, False, False]
Current timestep = 209. State = [[-0.18178773 -0.16151714]]. Action = [[ 0.04622212 -0.10608259 -0.02175751  0.32107306]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, True, False, False]
Human Feedback received at timestep 209 of 1
Current timestep = 210. State = [[-0.18066125 -0.16576393]]. Action = [[ 0.1895284   0.19233215 -0.06150877  0.27240837]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, True, False, False]
Current timestep = 211. State = [[-0.17615917 -0.16512719]]. Action = [[ 0.01404774 -0.03061989 -0.14222047 -0.4178825 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, True, False, False]
Current timestep = 212. State = [[-0.17294067 -0.16404316]]. Action = [[-0.24554722  0.21270025  0.02171019  0.60448813]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, True, False, False]
Current timestep = 213. State = [[-0.17313384 -0.16031088]]. Action = [[-0.09468235  0.13363963 -0.13187072 -0.6106347 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, True, False, False]
Current timestep = 214. State = [[-0.17350793 -0.15475938]]. Action = [[ 0.20487124  0.16343334 -0.24176301 -0.37116373]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, True, False, False]
Human Feedback received at timestep 214 of 1
Current timestep = 215. State = [[-0.1729419 -0.1466088]]. Action = [[-0.02497853  0.05948555 -0.2479739  -0.535717  ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, True, False, False]
Current timestep = 216. State = [[-0.1729013 -0.1397424]]. Action = [[ 0.13009328  0.15356517 -0.04910804 -0.7833507 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, True, False, False]
Current timestep = 217. State = [[-0.17058279 -0.1317097 ]]. Action = [[-0.23795587 -0.03682694  0.21681976  0.03574967]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, True, False, False]
Current timestep = 218. State = [[-0.17102252 -0.12790205]]. Action = [[ 0.16888863 -0.10855861  0.12292001 -0.55490094]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, True, False, False]
Scene graph at timestep 218 is [True, False, False, True, False, False]
State prediction error at timestep 218 is tensor(0.0101, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of 1
Current timestep = 219. State = [[-0.17068626 -0.12756678]]. Action = [[-0.05478552  0.04259902  0.08141324  0.1606834 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, True, False, False]
Current timestep = 220. State = [[-0.17052814 -0.12692869]]. Action = [[ 0.03532082 -0.02958338  0.19590226 -0.653043  ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, True, False, False]
Human Feedback received at timestep 220 of 1
Current timestep = 221. State = [[-0.17036447 -0.1260695 ]]. Action = [[-0.11177251  0.20285508  0.24604356  0.81247103]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, True, False, False]
Current timestep = 222. State = [[-0.17075144 -0.12127634]]. Action = [[-0.21468942  0.20655793  0.19606614  0.8246758 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, True, False, False]
Current timestep = 223. State = [[-0.17295611 -0.11348709]]. Action = [[-0.21628301  0.19669035  0.238145    0.93394005]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
Human Feedback received at timestep 223 of 1
Current timestep = 224. State = [[-0.17885701 -0.10247406]]. Action = [[-0.0909445   0.23682892  0.14686653 -0.4123146 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0158, grad_fn=<MseLossBackward0>)
Current timestep = 225. State = [[-0.18614325 -0.08812429]]. Action = [[ 0.24408162 -0.00055966 -0.07960401  0.51452816]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
Current timestep = 226. State = [[-0.18724857 -0.07962754]]. Action = [[-0.14413036  0.08864668 -0.1309307   0.5289302 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
Current timestep = 227. State = [[-0.18902932 -0.0720408 ]]. Action = [[ 0.19259417 -0.21434656 -0.21341625  0.08602011]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
Current timestep = 228. State = [[-0.18843547 -0.07238646]]. Action = [[ 0.07690215 -0.1414266  -0.227726   -0.70641214]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0110, grad_fn=<MseLossBackward0>)
Current timestep = 229. State = [[-0.18737279 -0.07509904]]. Action = [[-0.2028042   0.20139301  0.13227218 -0.54819727]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
Current timestep = 230. State = [[-0.1882027  -0.07311291]]. Action = [[-0.05069827 -0.12104085  0.21157914  0.9498532 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(0.0228, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 230 of 1
Current timestep = 231. State = [[-0.19011714 -0.07430035]]. Action = [[ 0.22333002  0.08914188 -0.07450783 -0.5562528 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0135, grad_fn=<MseLossBackward0>)
Current timestep = 232. State = [[-0.18981624 -0.0738651 ]]. Action = [[ 0.20445591 -0.21000135 -0.06569147  0.75736856]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
Human Feedback received at timestep 232 of 1
Current timestep = 233. State = [[-0.18724659 -0.07694419]]. Action = [[-0.20776093  0.24428481 -0.00255047  0.25492954]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, True, False]
Current timestep = 234. State = [[-0.1872468  -0.07384073]]. Action = [[ 0.23407203 -0.12511943 -0.17534637  0.01931775]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, True, False]
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(0.0229, grad_fn=<MseLossBackward0>)
Current timestep = 235. State = [[-0.18465027 -0.07419627]]. Action = [[-0.13189304  0.09068683 -0.10971878 -0.12875408]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, True, False]
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(0.0204, grad_fn=<MseLossBackward0>)
Current timestep = 236. State = [[-0.1846497 -0.0736379]]. Action = [[ 0.12616518 -0.24618413  0.21612778 -0.9721306 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, True, False]
Human Feedback received at timestep 236 of 1
Current timestep = 237. State = [[-0.18313941 -0.07747353]]. Action = [[ 0.1558631   0.15834898 -0.16642994  0.4381032 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, True, False]
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(0.0280, grad_fn=<MseLossBackward0>)
Current timestep = 238. State = [[-0.17789654 -0.07624876]]. Action = [[ 0.13382673  0.16866386 -0.21316151  0.43462968]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, True, False]
Current timestep = 239. State = [[-0.17079888 -0.07145103]]. Action = [[ 0.10598162  0.14920929 -0.17376275 -0.61793566]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
Current timestep = 240. State = [[-0.16359602 -0.06494865]]. Action = [[ 0.08344057 -0.11720499 -0.19889396  0.7587408 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
Human Feedback received at timestep 240 of 1
Current timestep = 241. State = [[-0.15723348 -0.06321394]]. Action = [[ 0.01065865 -0.02331588  0.11452988  0.20876658]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
Current timestep = 242. State = [[-0.15228438 -0.06381848]]. Action = [[-0.09266159 -0.16566388  0.22805709  0.36524403]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
Current timestep = 243. State = [[-0.14980452 -0.06711698]]. Action = [[-0.10562342  0.14856115 -0.17018382 -0.5759227 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, False, True, False]
Current timestep = 244. State = [[-0.14963435 -0.06540209]]. Action = [[-0.06522837  0.10756516 -0.09153476  0.02485895]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, False, True, False]
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(0.0166, grad_fn=<MseLossBackward0>)
Current timestep = 245. State = [[-0.15056178 -0.06153731]]. Action = [[-0.0735945   0.20464015 -0.06401554  0.70086753]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, False, True, False]
Human Feedback received at timestep 245 of 1
Current timestep = 246. State = [[-0.1519659  -0.05478622]]. Action = [[ 0.16232067  0.0087887  -0.17061064  0.01838219]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, False, True, False]
Scene graph at timestep 246 is [True, False, False, False, True, False]
State prediction error at timestep 246 is tensor(0.0171, grad_fn=<MseLossBackward0>)
Current timestep = 247. State = [[-0.15230668 -0.04887808]]. Action = [[-0.24501383  0.20665208  0.13832492  0.6473994 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
Current timestep = 248. State = [[-0.15448575 -0.04053783]]. Action = [[-0.16684389  0.10406646 -0.14902791  0.14702559]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
Current timestep = 249. State = [[-0.15801722 -0.03379964]]. Action = [[-0.0837819  -0.24446994 -0.16581596 -0.7860285 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
Current timestep = 250. State = [[-0.16272205 -0.03544814]]. Action = [[-0.16087376 -0.22170264  0.00859144  0.39860976]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, True, False]
Human Feedback received at timestep 250 of 1
Current timestep = 251. State = [[-0.16944513 -0.04158541]]. Action = [[ 0.21816403  0.11784059 -0.01532617 -0.43543458]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, True, False]
Current timestep = 252. State = [[-0.17041798 -0.04255931]]. Action = [[-0.16015641 -0.03660324 -0.04082091  0.7022412 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, True, False]
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(0.0221, grad_fn=<MseLossBackward0>)
Current timestep = 253. State = [[-0.17964813 -0.04615808]]. Action = [[-0.24579757 -0.03500755  0.05094171  0.02466536]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, True, False]
Current timestep = 254. State = [[-0.18557757 -0.04744912]]. Action = [[ 0.13049084 -0.05805054  0.11064038 -0.7885103 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
Current timestep = 255. State = [[-0.187052   -0.04897263]]. Action = [[-0.06627987  0.22305524  0.23126104 -0.8168889 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, True, False]
Human Feedback received at timestep 255 of -1
Current timestep = 256. State = [[-0.1883929  -0.04479199]]. Action = [[-0.05675259  0.09552652  0.04346678 -0.8023792 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(0.0103, grad_fn=<MseLossBackward0>)
Current timestep = 257. State = [[-0.19050105 -0.03936807]]. Action = [[ 0.09580943  0.22550112  0.2285018  -0.8241176 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0112, grad_fn=<MseLossBackward0>)
Current timestep = 258. State = [[-0.19240214 -0.0298702 ]]. Action = [[-0.00741103  0.06764382 -0.0429339  -0.7659553 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
Current timestep = 259. State = [[-0.19434138 -0.02182868]]. Action = [[-0.1414631   0.14259434  0.17767245  0.68668246]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
Human Feedback received at timestep 259 of -1
Current timestep = 260. State = [[-0.19683488 -0.01246548]]. Action = [[ 0.1674281   0.24226135 -0.01459828  0.94628024]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
Current timestep = 261. State = [[-0.19793731 -0.0020368 ]]. Action = [[ 0.14681676 -0.2141407   0.14839897  0.53379023]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, True, False]
Current timestep = 262. State = [[-1.9676740e-01 -1.8727928e-04]]. Action = [[-0.13455038 -0.05033401  0.15135208  0.30620217]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, True, False]
Current timestep = 263. State = [[-0.1963162   0.00038924]]. Action = [[-0.2331721   0.23358351 -0.22470807 -0.0293349 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, True, False]
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(0.0304, grad_fn=<MseLossBackward0>)
Current timestep = 264. State = [[-0.19921589  0.00696063]]. Action = [[-0.14683115  0.14240545  0.05516675 -0.84896934]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, True, False]
Current timestep = 265. State = [[-0.20384482  0.01402992]]. Action = [[-0.11730245 -0.19366647  0.16625956  0.10845613]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
Human Feedback received at timestep 265 of -1
Current timestep = 266. State = [[-0.21103434  0.01361298]]. Action = [[-0.03388691 -0.22867149 -0.15199074  0.563499  ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
Current timestep = 267. State = [[-0.21686783  0.00780341]]. Action = [[ 0.14198232 -0.22215258  0.16073337 -0.20960104]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
Current timestep = 268. State = [[-0.21981347 -0.00073658]]. Action = [[-0.19415663  0.04800582 -0.18125214  0.76058364]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
Current timestep = 269. State = [[-0.2232943  -0.00494043]]. Action = [[0.00544444 0.05199674 0.24074498 0.89136255]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0305, grad_fn=<MseLossBackward0>)
Current timestep = 270. State = [[-0.22605653 -0.00671022]]. Action = [[-0.04561377 -0.01057202 -0.0559461   0.37617683]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
Current timestep = 271. State = [[-0.22783999 -0.00814908]]. Action = [[ 0.09351668 -0.02025799 -0.24399182  0.11308277]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
Human Feedback received at timestep 271 of -1
Current timestep = 272. State = [[-0.22819431 -0.0086653 ]]. Action = [[-0.10475013  0.19848222  0.14973074  0.3330425 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
Current timestep = 273. State = [[-0.22993322 -0.00551785]]. Action = [[ 0.03401643 -0.11779597 -0.1482825   0.6510427 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
Current timestep = 274. State = [[-0.23099476 -0.00595087]]. Action = [[-0.0346882   0.22812301 -0.23656636 -0.71032465]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, True, False]
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0256, grad_fn=<MseLossBackward0>)
Current timestep = 275. State = [[-0.23280177 -0.00024295]]. Action = [[ 0.14904675  0.2025336   0.13415867 -0.0348435 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, True, False]
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(0.0353, grad_fn=<MseLossBackward0>)
Current timestep = 276. State = [[-0.23379734  0.00779662]]. Action = [[-0.19725594 -0.12320578 -0.01666395 -0.3679859 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, True, False]
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(0.0253, grad_fn=<MseLossBackward0>)
Current timestep = 277. State = [[-0.23615122  0.01021561]]. Action = [[ 0.051844   -0.23044468  0.23371124  0.75566316]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, True, False]
Current timestep = 278. State = [[-0.23726867  0.00712996]]. Action = [[-0.13019408  0.20826912 -0.05643457  0.65749335]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, True, False]
Current timestep = 279. State = [[-0.23961511  0.00906376]]. Action = [[-0.13500734 -0.04399964  0.05741155 -0.36770654]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, True, False]
Current timestep = 280. State = [[-0.24411178  0.008798  ]]. Action = [[-0.11497733 -0.14535372 -0.06744662 -0.09917009]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, True, False]
Current timestep = 281. State = [[-0.2503533   0.00481914]]. Action = [[ 0.12718004 -0.04739977  0.00409278  0.49590337]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, True, False]
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0421, grad_fn=<MseLossBackward0>)
Current timestep = 282. State = [[-0.25225422  0.00166209]]. Action = [[ 0.17700568 -0.21997114 -0.11565176  0.41281605]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, True, False]
Current timestep = 283. State = [[-0.25017568 -0.00483328]]. Action = [[-0.07046781  0.0016174  -0.15857404  0.8113942 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
Current timestep = 284. State = [[-0.2493394  -0.00897467]]. Action = [[-0.24727397 -0.00294057 -0.02004679 -0.0380137 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
Current timestep = 285. State = [[-0.25285527 -0.01237706]]. Action = [[ 0.12173414  0.00119254 -0.0334174  -0.27904463]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, True, False]
Human Feedback received at timestep 285 of -1
Current timestep = 286. State = [[-0.25431225 -0.01454479]]. Action = [[-0.18390064 -0.19500923  0.0542081  -0.07798249]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, True, False]
Current timestep = 287. State = [[-0.2593797  -0.02002006]]. Action = [[-0.07817903  0.24974644 -0.05386518 -0.5866436 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0323, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 287 of -1
Current timestep = 288. State = [[-0.2648688  -0.01790182]]. Action = [[ 0.01028073  0.22316831  0.10373178 -0.09009129]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
Current timestep = 289. State = [[-0.26752573 -0.01211432]]. Action = [[-0.1008929  -0.08855754 -0.20721525 -0.2635529 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
Human Feedback received at timestep 289 of -1
Current timestep = 290. State = [[-0.2718292  -0.01051015]]. Action = [[ 0.24335366 -0.17864884 -0.16622287 -0.02795124]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0435, grad_fn=<MseLossBackward0>)
Current timestep = 291. State = [[-0.27091414 -0.01280794]]. Action = [[-0.14284828  0.02666262  0.14307636 -0.7663967 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
Current timestep = 292. State = [[-0.27145702 -0.0142473 ]]. Action = [[-0.15636575 -0.1805887  -0.21037896  0.49005258]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, True, False]
Current timestep = 293. State = [[-0.27514833 -0.01977785]]. Action = [[ 0.11829767 -0.18967219  0.0033237   0.20205629]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, True, False]
Human Feedback received at timestep 293 of -1
Current timestep = 294. State = [[-0.27595708 -0.0273399 ]]. Action = [[0.11746955 0.11282527 0.12952775 0.29335082]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, True, False]
Current timestep = 295. State = [[-0.2743307  -0.02933185]]. Action = [[-0.1979075   0.00405869  0.04390317  0.7200632 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, True, False]
Current timestep = 296. State = [[-0.2759083  -0.02659669]]. Action = [[-0.22503477  0.02290606  0.04141948  0.8843081 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, True, False]
Current timestep = 297. State = [[-0.28089553 -0.02280826]]. Action = [[ 0.22907895  0.21820211  0.0883787  -0.98925376]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, True, False]
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0259, grad_fn=<MseLossBackward0>)
Current timestep = 298. State = [[-0.28307542 -0.01578376]]. Action = [[-0.16847706  0.2031959   0.20241317  0.87305737]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, True, False]
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0492, grad_fn=<MseLossBackward0>)
Current timestep = 299. State = [[-0.28663442 -0.00744233]]. Action = [[ 0.14989555 -0.05269396  0.04854783 -0.16872978]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, True, False]
Human Feedback received at timestep 299 of -1
Current timestep = 300. State = [[-0.2886149  -0.00149896]]. Action = [[ 0.06016815 -0.06185862 -0.23609644  0.8243147 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, True, False]
Current timestep = 301. State = [[-0.28977424  0.00223136]]. Action = [[-0.12334317 -0.08496915 -0.04075986 -0.41738147]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, True, False]
Human Feedback received at timestep 301 of -1
Current timestep = 302. State = [[-0.29099587  0.00333964]]. Action = [[-0.04655159 -0.23836389  0.11342287 -0.6467637 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, True, False]
Current timestep = 303. State = [[-0.29276675  0.0011479 ]]. Action = [[0.00282103 0.23250079 0.20972288 0.01337445]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, True, False]
Current timestep = 304. State = [[-0.29452276  0.0029583 ]]. Action = [[ 0.22818995  0.09646788 -0.03094155 -0.5939901 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, True, False]
Scene graph at timestep 304 is [True, False, False, False, True, False]
State prediction error at timestep 304 is tensor(0.0403, grad_fn=<MseLossBackward0>)
Current timestep = 305. State = [[-0.29325953  0.00492045]]. Action = [[ 0.03143471 -0.02668905 -0.14678264 -0.13458979]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, True, False]
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0496, grad_fn=<MseLossBackward0>)
Current timestep = 306. State = [[-0.29201236  0.00557871]]. Action = [[ 0.03555176 -0.04896116  0.07185972  0.42453074]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, True, False]
Human Feedback received at timestep 306 of -1
Current timestep = 307. State = [[-0.29076856  0.00440103]]. Action = [[-0.05673547  0.1568076   0.15336204 -0.7118628 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, True, False]
Current timestep = 308. State = [[-0.2893638   0.00534636]]. Action = [[-0.19176978 -0.03002229 -0.09161752 -0.57948595]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, True, False]
Human Feedback received at timestep 308 of -1
Current timestep = 309. State = [[-0.28998685  0.00528908]]. Action = [[-0.04748781 -0.05484791 -0.20752318 -0.24825007]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, True, False]
Current timestep = 310. State = [[-0.2910561   0.00693388]]. Action = [[-0.0192827   0.22745982  0.01754797  0.89527404]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, True, False]
Current timestep = 311. State = [[-0.29249159  0.010864  ]]. Action = [[ 0.24598312  0.23363417  0.19990122 -0.2632689 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, True, False]
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0465, grad_fn=<MseLossBackward0>)
Current timestep = 312. State = [[-0.29091945  0.01739376]]. Action = [[ 0.0868603   0.14239407 -0.12051478 -0.28462952]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, True, False]
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(0.0496, grad_fn=<MseLossBackward0>)
Current timestep = 313. State = [[-0.28815174  0.02126752]]. Action = [[-0.09736218  0.10388878  0.1806775  -0.3176986 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, True, False]
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0442, grad_fn=<MseLossBackward0>)
Current timestep = 314. State = [[-0.2875818   0.02653405]]. Action = [[0.15163898 0.22656208 0.1745058  0.03274632]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
Human Feedback received at timestep 314 of -1
Current timestep = 315. State = [[-0.28501785  0.03502479]]. Action = [[ 0.01717269  0.12485924 -0.08647761 -0.52691865]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
Current timestep = 316. State = [[-0.28313047  0.04347304]]. Action = [[ 0.23792255 -0.19138168  0.05486271  0.9859054 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
Human Feedback received at timestep 316 of -1
Current timestep = 317. State = [[-0.27804536  0.04600638]]. Action = [[-0.08270381  0.16747987 -0.05599672 -0.44100857]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, True, False]
Current timestep = 318. State = [[-0.27413398  0.05020219]]. Action = [[-0.00962296 -0.21153666  0.23658735  0.538484  ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, True, False]
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0478, grad_fn=<MseLossBackward0>)
Current timestep = 319. State = [[-0.27207023  0.04969281]]. Action = [[ 0.1986165   0.07936335  0.09900945 -0.489398  ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, True, False]
Current timestep = 320. State = [[-0.26703423  0.05040511]]. Action = [[-0.02845798 -0.10386884 -0.10968763 -0.07388496]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, True, False]
Current timestep = 321. State = [[-0.26427445  0.04898766]]. Action = [[-0.0990124  -0.06469022  0.11580673  0.7758266 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, True, False]
Current timestep = 322. State = [[-0.26262814  0.04764207]]. Action = [[0.2382291  0.06340122 0.09194005 0.23380637]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, True, False]
Current timestep = 323. State = [[-0.2589806   0.04799059]]. Action = [[-0.09507224 -0.08900395  0.19966567  0.73209715]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, True, False]
Current timestep = 324. State = [[-0.25822672  0.04742434]]. Action = [[-0.21417637  0.10341573 -0.2203437  -0.65301824]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, True, False]
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0362, grad_fn=<MseLossBackward0>)
Current timestep = 325. State = [[-0.25934327  0.04786273]]. Action = [[ 0.01872355 -0.16982254 -0.15279731 -0.74373543]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, True, False]
Human Feedback received at timestep 325 of -1
Current timestep = 326. State = [[-0.25975704  0.04512263]]. Action = [[ 0.02318859  0.10557556 -0.21522062 -0.96729946]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, True, False]
Current timestep = 327. State = [[-0.25930914  0.0445862 ]]. Action = [[-0.0694325  -0.22754604  0.21761382  0.63733697]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, True, False]
Human Feedback received at timestep 327 of -1
Current timestep = 328. State = [[-0.260168    0.04019095]]. Action = [[-0.24432375 -0.1452278   0.18519777 -0.22927636]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, True, False]
Current timestep = 329. State = [[-0.26560804  0.03460489]]. Action = [[-0.2005955   0.16930556  0.18864614  0.21889639]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, True, False]
Human Feedback received at timestep 329 of -1
Current timestep = 330. State = [[-0.27358758  0.03448471]]. Action = [[ 0.20111847 -0.16314597  0.07994545  0.34611845]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, True, False]
Current timestep = 331. State = [[-0.27579445  0.03424564]]. Action = [[-0.23933971  0.17342329 -0.09801787 -0.01418412]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, True, False]
Current timestep = 332. State = [[-0.28149757  0.03511982]]. Action = [[-0.00810161  0.03671473  0.18093175 -0.9650372 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, True, False]
Current timestep = 333. State = [[-0.2853285   0.03724958]]. Action = [[ 0.12924582  0.2283318   0.12670863 -0.9666131 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, True, False]
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0338, grad_fn=<MseLossBackward0>)
Current timestep = 334. State = [[-0.28706577  0.04224972]]. Action = [[-0.03061958 -0.19909368 -0.17821075 -0.23841894]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, True, False]
Current timestep = 335. State = [[-0.28830373  0.04424487]]. Action = [[-0.05106708 -0.00397305 -0.14977463 -0.24857134]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, True, False]
Current timestep = 336. State = [[-0.28946248  0.04576083]]. Action = [[-0.09272014  0.08565399  0.12230873  0.544919  ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, True, False]
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0573, grad_fn=<MseLossBackward0>)
Current timestep = 337. State = [[-0.2917895   0.04803358]]. Action = [[-0.04950137 -0.21603273 -0.24416505 -0.21847332]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, True, False]
Current timestep = 338. State = [[-0.2943703   0.04748784]]. Action = [[-0.16546598  0.23410219  0.11023152 -0.73228985]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, True, False]
Current timestep = 339. State = [[-0.29912832  0.0496097 ]]. Action = [[-0.13661401 -0.21665384  0.10210997 -0.43282878]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, True, False]
Current timestep = 340. State = [[-0.3041429   0.04949733]]. Action = [[ 0.14101002  0.08166957  0.11080635 -0.7109239 ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, True, False]
Human Feedback received at timestep 340 of -1
Current timestep = 341. State = [[-0.3074322   0.05124285]]. Action = [[ 0.18061775  0.04637587  0.22132954 -0.53498703]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, True, False]
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0434, grad_fn=<MseLossBackward0>)
Current timestep = 342. State = [[-0.30875054  0.05354003]]. Action = [[0.0029926  0.01291969 0.2294766  0.68729067]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, True, False]
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0601, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 342 of -1
Current timestep = 343. State = [[-0.30874297  0.05391904]]. Action = [[ 0.12324908 -0.08229987 -0.13664523 -0.6164951 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, True, False]
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0483, grad_fn=<MseLossBackward0>)
Current timestep = 344. State = [[-0.307668    0.05456972]]. Action = [[ 0.11365283 -0.0664736   0.04917908  0.604738  ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, True, False]
Current timestep = 345. State = [[-0.30523938  0.05563188]]. Action = [[ 0.05304357 -0.21711743  0.01088193 -0.15997213]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, True, False]
Current timestep = 346. State = [[-0.3033456   0.05434709]]. Action = [[-0.06937063  0.21108481 -0.15869917 -0.35764176]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, True, False]
Human Feedback received at timestep 346 of -1
Current timestep = 347. State = [[-0.30281225  0.05523252]]. Action = [[0.01284745 0.01944196 0.11819389 0.38699365]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, True, False]
Current timestep = 348. State = [[-0.30225033  0.05564348]]. Action = [[ 0.13355124 -0.03734936  0.19594842 -0.44285524]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, True, False]
Human Feedback received at timestep 348 of -1
Current timestep = 349. State = [[-0.29997882  0.05645064]]. Action = [[ 0.02317917 -0.15682155  0.20274532 -0.64070034]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, True, False]
Current timestep = 350. State = [[-0.29850534  0.05553388]]. Action = [[-0.0895699  -0.00805157 -0.13619357 -0.32054746]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, True, False]
Human Feedback received at timestep 350 of -1
Current timestep = 351. State = [[-0.29870465  0.05383767]]. Action = [[-3.59720737e-02 -3.32266092e-04  1.02556944e-01  5.37557602e-01]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, True, False]
Current timestep = 352. State = [[-0.29888406  0.05292967]]. Action = [[-0.072612    0.07750195  0.04819214 -0.9101254 ]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, True, False]
Current timestep = 353. State = [[-0.29921517  0.05218468]]. Action = [[-0.09469867 -0.24609025 -0.00074832 -0.48478913]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, True, False]
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0403, grad_fn=<MseLossBackward0>)
Current timestep = 354. State = [[-0.30117685  0.04812781]]. Action = [[-0.22265866 -0.00817369 -0.23016062  0.7360873 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, True, False]
Current timestep = 355. State = [[-0.30548957  0.04437749]]. Action = [[-0.08099765  0.03628013 -0.06126581 -0.24832118]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, True, False]
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(0.0526, grad_fn=<MseLossBackward0>)
Current timestep = 356. State = [[-0.3095581   0.04234735]]. Action = [[0.18267804 0.15306312 0.11147755 0.0860337 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, True, False]
Human Feedback received at timestep 356 of -1
Current timestep = 357. State = [[-0.3095304   0.04287962]]. Action = [[ 0.14349246 -0.21327515  0.12247166  0.9602027 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, True, False]
Current timestep = 358. State = [[-0.30919027  0.04244436]]. Action = [[ 0.1433684 -0.1500123  0.1905356  0.6758517]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, True, False]
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0567, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 358 of -1
Current timestep = 359. State = [[-0.30773675  0.04116004]]. Action = [[0.05090523 0.10076836 0.03580984 0.12570786]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, True, False]
Current timestep = 360. State = [[-0.30543733  0.04210316]]. Action = [[ 0.13462365  0.23287162  0.00525573 -0.61892647]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, True, False]
Human Feedback received at timestep 360 of -1
Current timestep = 361. State = [[-0.3011882   0.04405295]]. Action = [[-0.2031908   0.17001104 -0.164652    0.8156121 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, True, False]
Current timestep = 362. State = [[-0.30089694  0.04451694]]. Action = [[-0.03897995  0.03425282 -0.1394416  -0.35039037]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, True, False]
Current timestep = 363. State = [[-0.30106398  0.04556568]]. Action = [[ 0.18064845 -0.12550405 -0.05125922  0.3761872 ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, True, False]
Current timestep = 364. State = [[-0.29861686  0.04347671]]. Action = [[ 0.10928142 -0.08886972  0.11869407 -0.97263044]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, True, False]
Current timestep = 365. State = [[-0.29482377  0.0426842 ]]. Action = [[-0.07885459 -0.16705854 -0.22025645 -0.27600598]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, True, False]
Human Feedback received at timestep 365 of -1
Current timestep = 366. State = [[-0.2943777   0.04082765]]. Action = [[ 0.11501312  0.17025846 -0.16781598  0.84440506]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, True, False]
Current timestep = 367. State = [[-0.29108787  0.03893963]]. Action = [[-0.07545272 -0.19695397 -0.16551189  0.6478708 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, True, False]
Current timestep = 368. State = [[-0.29094663  0.03863832]]. Action = [[ 0.23848069  0.20635632 -0.16398728 -0.5728567 ]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, True, False]
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0484, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of -1
Current timestep = 369. State = [[-0.2844561   0.03618795]]. Action = [[ 0.1024386  -0.17311867 -0.14773993  0.02762902]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, True, False]
Current timestep = 370. State = [[-0.27974582  0.03746465]]. Action = [[ 0.12162054  0.15833396  0.1017946  -0.6584465 ]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, True, False]
Current timestep = 371. State = [[-0.2728771   0.03364923]]. Action = [[-0.2306885   0.02579218  0.24652606  0.10843635]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, True, False]
Current timestep = 372. State = [[-0.27118465  0.03434905]]. Action = [[-0.10513338 -0.1524755  -0.1738475  -0.32867497]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, True, False]
Current timestep = 373. State = [[-0.2713506   0.03476305]]. Action = [[ 0.06044602  0.04359072 -0.12441617  0.37088263]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, False, True, False]
Current timestep = 374. State = [[-0.27149028  0.03502919]]. Action = [[ 0.07123291  0.12582636 -0.02907166  0.45122695]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, False, True, False]
Current timestep = 375. State = [[-0.26989666  0.03461743]]. Action = [[-0.03237647  0.06646734 -0.19660771 -0.46888477]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, False, True, False]
Scene graph at timestep 375 is [True, False, False, False, True, False]
State prediction error at timestep 375 is tensor(0.0414, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of -1
Current timestep = 376. State = [[-0.26809454  0.03489093]]. Action = [[ 0.1948365  -0.17575721  0.1984961  -0.7294052 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, False, True, False]
Current timestep = 377. State = [[-0.2644771   0.03415147]]. Action = [[ 0.04070619  0.14198223  0.00413352 -0.41296673]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, False, True, False]
Current timestep = 378. State = [[-0.2616123   0.03551676]]. Action = [[ 0.06249788 -0.21649724 -0.2370868  -0.3394674 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, False, True, False]
Current timestep = 379. State = [[-0.25893483  0.03357874]]. Action = [[-0.24891637  0.08087739  0.12442592 -0.5708886 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, False, True, False]
Human Feedback received at timestep 379 of 1
Current timestep = 380. State = [[-0.25972596  0.03246522]]. Action = [[ 0.22872257 -0.16502248 -0.17737074 -0.7401458 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, False, True, False]
Current timestep = 381. State = [[-0.25780413  0.02870244]]. Action = [[ 0.01084745  0.16277838  0.22258627 -0.710396  ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, False, True, False]
Current timestep = 382. State = [[-0.2547308   0.02692301]]. Action = [[-0.08200383 -0.16086976  0.06939167  0.7218077 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, False, True, False]
Current timestep = 383. State = [[-0.25447828  0.02538233]]. Action = [[ 0.07652029 -0.24158217 -0.15613778 -0.47275913]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, False, True, False]
Current timestep = 384. State = [[-0.2540841   0.02216868]]. Action = [[ 0.22622702 -0.21811932 -0.08395199  0.09765232]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, False, True, False]
Human Feedback received at timestep 384 of 1
Current timestep = 385. State = [[-0.25139743  0.01621209]]. Action = [[-0.17512873 -0.24138223 -0.07343137  0.06793427]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, False, True, False]
Current timestep = 386. State = [[-0.24961133  0.0066573 ]]. Action = [[ 0.02392405  0.17742503 -0.18236668 -0.30797338]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, False, True, False]
Scene graph at timestep 386 is [True, False, False, False, True, False]
State prediction error at timestep 386 is tensor(0.0378, grad_fn=<MseLossBackward0>)
Current timestep = 387. State = [[-0.2481806  -0.00051181]]. Action = [[-0.22826855  0.17682889 -0.10021573 -0.82999414]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, False, True, False]
Human Feedback received at timestep 387 of 1
Current timestep = 388. State = [[-0.24914256 -0.00595287]]. Action = [[-0.14683391 -0.13267958 -0.11738974  0.54345584]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, False, True, False]
Current timestep = 389. State = [[-0.2511581  -0.00559448]]. Action = [[-0.14410399  0.14523447  0.230968    0.0535357 ]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, False, True, False]
Human Feedback received at timestep 389 of 1
Current timestep = 390. State = [[-0.25451255 -0.00777508]]. Action = [[-0.00436376  0.00788525 -0.02925782  0.12712479]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, False, True, False]
Current timestep = 391. State = [[-0.2567198  -0.00545456]]. Action = [[ 0.00680155 -0.07619743  0.13877463 -0.9681096 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, False, True, False]
Current timestep = 392. State = [[-0.25958347 -0.00697385]]. Action = [[-0.13603221 -0.04600768 -0.16513947 -0.70569754]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, False, True, False]
Current timestep = 393. State = [[-0.2639514  -0.00620771]]. Action = [[ 0.19411168 -0.07861337 -0.06811731 -0.9625329 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, False, True, False]
Current timestep = 394. State = [[-0.26526684 -0.00572213]]. Action = [[-0.2149664  -0.10418855  0.08010542  0.9566463 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, False, True, False]
Scene graph at timestep 394 is [True, False, False, False, True, False]
State prediction error at timestep 394 is tensor(0.0440, grad_fn=<MseLossBackward0>)
Current timestep = 395. State = [[-0.26772153 -0.0114275 ]]. Action = [[-0.0307807  -0.06188348 -0.09164914  0.16604269]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, False, True, False]
Human Feedback received at timestep 395 of 1
Current timestep = 396. State = [[-0.27082738 -0.01240883]]. Action = [[-0.10154295 -0.07955533  0.05182603  0.74991536]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, False, True, False]
Current timestep = 397. State = [[-0.2743938  -0.01201923]]. Action = [[-0.13271114 -0.15138914  0.00269243  0.01375067]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, False, True, False]
Human Feedback received at timestep 397 of 1
Current timestep = 398. State = [[-0.27970716 -0.01491655]]. Action = [[-0.02145794  0.07253754  0.12534273  0.7597914 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, False, True, False]
Current timestep = 399. State = [[-0.2856239  -0.01499332]]. Action = [[ 0.03946671 -0.0701433   0.16483977  0.5673311 ]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, False, True, False]
Current timestep = 400. State = [[-0.2883514 -0.0137097]]. Action = [[-0.0214601  -0.17681594 -0.01717755  0.90279615]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, False, True, False]
Human Feedback received at timestep 400 of 1
Current timestep = 401. State = [[-0.29069158 -0.01984465]]. Action = [[-0.03051876  0.00969654 -0.17877291 -0.4020986 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, False, True, False]
Scene graph at timestep 401 is [True, False, False, False, True, False]
State prediction error at timestep 401 is tensor(0.0431, grad_fn=<MseLossBackward0>)
Current timestep = 402. State = [[-0.29269528 -0.022143  ]]. Action = [[-0.1797141  -0.06812574  0.2368921  -0.3986498 ]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, False, True, False]
Current timestep = 403. State = [[-0.29886082 -0.02169859]]. Action = [[ 0.12693655 -0.02220267 -0.13935295 -0.8169536 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, False, True, False]
Current timestep = 404. State = [[-0.30129778 -0.02111958]]. Action = [[ 0.15387756 -0.22228073 -0.02099338  0.85012555]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, False, True, False]
Current timestep = 405. State = [[-0.30265075 -0.02400052]]. Action = [[ 0.16062409  0.11864492 -0.14652179 -0.4720571 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, False, True, False]
Scene graph at timestep 405 is [True, False, False, False, True, False]
State prediction error at timestep 405 is tensor(0.0472, grad_fn=<MseLossBackward0>)
Current timestep = 406. State = [[-0.300754  -0.0277766]]. Action = [[ 0.04444024 -0.03668763  0.16816202  0.31221056]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, False, True, False]
Scene graph at timestep 406 is [True, False, False, False, True, False]
State prediction error at timestep 406 is tensor(0.0553, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 406 of 1
Current timestep = 407. State = [[-0.2983636  -0.03101345]]. Action = [[-0.1751818   0.03859329  0.18841594 -0.5621452 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, False, True, False]
Current timestep = 408. State = [[-0.30021548 -0.02940727]]. Action = [[-0.03189123 -0.14287917 -0.03860381  0.22244465]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, False, True, False]
Current timestep = 409. State = [[-0.30149722 -0.03231912]]. Action = [[ 0.0998385  -0.22945124  0.00485605 -0.21691102]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, False, True, False]
Human Feedback received at timestep 409 of -1
Current timestep = 410. State = [[-0.30097842 -0.03636176]]. Action = [[-0.16516352 -0.24781941 -0.03480011  0.8801396 ]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, False, True, False]
Current timestep = 411. State = [[-0.3042911  -0.03811736]]. Action = [[-0.10200152 -0.1742281   0.24381262  0.9302002 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, False, True, False]
Current timestep = 412. State = [[-0.30908242 -0.04111471]]. Action = [[ 0.04059905  0.23550892  0.22864729 -0.9161777 ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, False, True, False]
Current timestep = 413. State = [[-0.31040728 -0.04612679]]. Action = [[ 0.06609264 -0.10250154  0.16600943 -0.70916575]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, False, True, False]
Scene graph at timestep 413 is [True, False, False, False, True, False]
State prediction error at timestep 413 is tensor(0.0333, grad_fn=<MseLossBackward0>)
Current timestep = 414. State = [[-0.3108828  -0.04884853]]. Action = [[-0.14116438  0.06443331  0.07274228  0.9251492 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, False, True, False]
Scene graph at timestep 414 is [True, False, False, False, True, False]
State prediction error at timestep 414 is tensor(0.0583, grad_fn=<MseLossBackward0>)
Current timestep = 415. State = [[-0.3135123  -0.04710309]]. Action = [[ 0.11702269 -0.08997047  0.12269658  0.98881316]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, False, True, False]
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(0.0532, grad_fn=<MseLossBackward0>)
Current timestep = 416. State = [[-0.3145446  -0.05213461]]. Action = [[-0.1556667  -0.00815974  0.18672442 -0.4628172 ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, False, True, False]
Scene graph at timestep 416 is [True, False, False, False, True, False]
State prediction error at timestep 416 is tensor(0.0425, grad_fn=<MseLossBackward0>)
Current timestep = 417. State = [[-0.31660014 -0.05128152]]. Action = [[ 0.20032644  0.0009011   0.03591508 -0.21147603]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, False, True, False]
Current timestep = 418. State = [[-0.31661528 -0.05326807]]. Action = [[-0.20415716 -0.09453255 -0.01780581 -0.70637006]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, False, True, False]
Current timestep = 419. State = [[-0.3188444  -0.05723747]]. Action = [[ 0.01213333  0.245601   -0.07274039  0.3252722 ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, False, True, False]
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(0.0697, grad_fn=<MseLossBackward0>)
Current timestep = 420. State = [[-0.31910172 -0.05758645]]. Action = [[ 0.10508773  0.15494996  0.16751337 -0.16977727]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, False, True, False]
Current timestep = 421. State = [[-0.3163588  -0.05382938]]. Action = [[-0.12738976 -0.05455816 -0.07382244  0.7509217 ]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, False, True, False]
