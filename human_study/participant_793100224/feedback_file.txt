Current timestep = 0. State = [[-0.25686243  0.0091581 ]]. Action = [[ 0.05927089 -0.07895648  0.10004079  0.69159627]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0415, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-0.2585536   0.00831022]]. Action = [[-0.2317612   0.1615954  -0.17456411 -0.9083504 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0208, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.2641799   0.01100163]]. Action = [[ 0.10777855 -0.17334911  0.23817489 -0.8265799 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0107, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.2656914   0.00308229]]. Action = [[-0.15496977 -0.21612413 -0.09071642 -0.94266754]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0072, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.2747572  -0.00840202]]. Action = [[-0.22333133 -0.0796811  -0.01630859 -0.9672659 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-0.28849867 -0.01203988]]. Action = [[-0.15644245  0.22376871 -0.17907809 -0.8182897 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.3008205  -0.00930598]]. Action = [[-0.22823544 -0.23522224 -0.01764438 -0.94186074]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.30974582 -0.01788659]]. Action = [[ 0.24833125 -0.15870309  0.10247529 -0.44554484]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.30705512 -0.02711796]]. Action = [[-0.12174657 -0.00828119  0.22890708  0.68679595]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0139, grad_fn=<MseLossBackward0>)
Current timestep = 9. State = [[-0.30788696 -0.0272812 ]]. Action = [[0.11027759 0.22038811 0.1769619  0.395015  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0133, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.3055581 -0.0215893]]. Action = [[-0.11453752 -0.1144819  -0.12563875 -0.86217356]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.3075447  -0.02276708]]. Action = [[ 0.09019065  0.01001364 -0.18699352 -0.26389027]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Current timestep = 12. State = [[-0.30349338 -0.0239691 ]]. Action = [[ 0.12452674 -0.05153893  0.24320906 -0.14901447]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.30005538 -0.02733498]]. Action = [[-0.15809594 -0.07603636 -0.11185345 -0.10260904]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.30677274 -0.03233805]]. Action = [[-0.19226804 -0.0881267   0.19590172 -0.14845425]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-0.31931224 -0.03924376]]. Action = [[-0.22864854 -0.14769033 -0.2279048  -0.0899508 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0071, grad_fn=<MseLossBackward0>)
Current timestep = 16. State = [[-0.33174574 -0.04571804]]. Action = [[ 0.02577683  0.04822856 -0.15877107 -0.6876927 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 17. State = [[-0.33287925 -0.04438907]]. Action = [[ 0.17022753  0.15214238  0.24375924 -0.9489951 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Current timestep = 18. State = [[-0.32525212 -0.0399202 ]]. Action = [[ 0.09043723 -0.07518755  0.18288785  0.5620893 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Current timestep = 19. State = [[-0.31661028 -0.04119852]]. Action = [[ 0.19130236 -0.04332943 -0.21260351  0.78845   ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.30437845 -0.04363887]]. Action = [[ 0.1928044  -0.02055864 -0.20088436 -0.0035491 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Current timestep = 21. State = [[-0.2910954  -0.04418418]]. Action = [[0.16824883 0.06848818 0.23920771 0.69561934]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 22. State = [[-0.28148645 -0.04531638]]. Action = [[-0.10477269 -0.20499471  0.09255651  0.8422077 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 23. State = [[-0.28215677 -0.05150297]]. Action = [[ 0.02104673  0.04931909 -0.03004396 -0.9009538 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 24. State = [[-0.28362593 -0.05464021]]. Action = [[-0.14759757 -0.13892211 -0.01724569  0.38856328]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Current timestep = 25. State = [[-0.2908419 -0.0627676]]. Action = [[-0.11424653 -0.1790047   0.15915823  0.66795874]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 26. State = [[-0.30037507 -0.07437284]]. Action = [[-0.18287833 -0.17395765 -0.03452304 -0.2788962 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.31170213 -0.08631531]]. Action = [[-0.11252427 -0.13395067 -0.1721649   0.6883557 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.32290822 -0.09370656]]. Action = [[-0.19168207  0.08249804  0.0974353   0.9646219 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 29. State = [[-0.33272916 -0.0947777 ]]. Action = [[-0.0186884  -0.06677613 -0.19867498 -0.17760992]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0119, grad_fn=<MseLossBackward0>)
Current timestep = 30. State = [[-0.33935264 -0.09660511]]. Action = [[-0.13254254  0.02673838  0.20145789  0.45365512]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0080, grad_fn=<MseLossBackward0>)
Current timestep = 31. State = [[-0.3452554 -0.0991325]]. Action = [[ 0.00428194 -0.16367623 -0.10569179 -0.9655137 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 32. State = [[-0.34632817 -0.10173209]]. Action = [[0.14483482 0.20194378 0.20257717 0.6672895 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Current timestep = 33. State = [[-0.34129295 -0.0996257 ]]. Action = [[-0.06251675 -0.16926861 -0.07541215 -0.5021948 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Current timestep = 34. State = [[-0.34243277 -0.10664737]]. Action = [[-0.051792   -0.18623511  0.01221448 -0.16317332]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0103, grad_fn=<MseLossBackward0>)
Current timestep = 35. State = [[-0.34482813 -0.11186726]]. Action = [[0.06965896 0.20597255 0.18307912 0.6379988 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0068, grad_fn=<MseLossBackward0>)
Current timestep = 36. State = [[-0.3392519  -0.11036913]]. Action = [[ 0.18380529 -0.16448149  0.02466071 -0.78280514]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0043, grad_fn=<MseLossBackward0>)
