Current timestep = 0. State = [[-0.32604954 -0.08628346]]. Action = [[0.00435984 0.09830839 0.         0.7271644 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0668, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.33171427 -0.08338373]]. Action = [[-0.08950701 -0.00990611  0.          0.9261981 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0582, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.33283383 -0.08530019]]. Action = [[ 0.05919518 -0.05022885  0.          0.01753342]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0498, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of 0
Current timestep = 3. State = [[-0.32827216 -0.08361274]]. Action = [[ 0.0794167   0.04838897  0.         -0.5082193 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0282, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of 0
Current timestep = 4. State = [[-0.32541105 -0.08398371]]. Action = [[ 0.02240814 -0.05188417  0.          0.06055915]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0375, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of 1
Current timestep = 5. State = [[-0.32796517 -0.08313134]]. Action = [[-0.0650128   0.04300299  0.         -0.7177578 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of 1
Current timestep = 6. State = [[-0.33477607 -0.08682027]]. Action = [[-0.09399814 -0.09525654  0.         -0.5532159 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of 1
Current timestep = 7. State = [[-0.34229532 -0.08952611]]. Action = [[-0.08570417  0.01555561  0.          0.74996316]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0277, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of 1
Current timestep = 8. State = [[-0.34298766 -0.09308955]]. Action = [[ 0.06543507 -0.06989755  0.          0.3585831 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0267, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of 1
Current timestep = 9. State = [[-0.3389524  -0.09814583]]. Action = [[ 0.05124535 -0.04889329  0.          0.9660307 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0148, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of 1
Current timestep = 10. State = [[-0.3412397  -0.09597597]]. Action = [[-0.09367169  0.09745923  0.          0.9561676 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0123, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of 1
Current timestep = 11. State = [[-0.3479158  -0.08937293]]. Action = [[-0.06628231  0.08782079  0.         -0.77547246]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of 1
Current timestep = 12. State = [[-0.35242483 -0.09062549]]. Action = [[-0.02037992 -0.08015904  0.         -0.95404047]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of 1
Current timestep = 13. State = [[-0.3513746  -0.09702729]]. Action = [[ 0.06875444 -0.08373401  0.          0.79827213]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of 1
Current timestep = 14. State = [[-0.34897906 -0.10328852]]. Action = [[ 0.03644241 -0.0735485   0.          0.991416  ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of 0
Current timestep = 15. State = [[-0.34903654 -0.10770089]]. Action = [[-0.00871034 -0.03376515  0.         -0.21796441]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of 0
Current timestep = 16. State = [[-0.3514281  -0.11450402]]. Action = [[-0.03313863 -0.09520341  0.         -0.47117627]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of 0
Current timestep = 17. State = [[-0.3501889  -0.11516192]]. Action = [[ 0.06364005  0.07837988  0.         -0.05265433]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of 0
Current timestep = 18. State = [[-0.35221055 -0.11067695]]. Action = [[-0.07403073  0.0627125   0.         -0.80233306]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 18 of 0
Current timestep = 19. State = [[-0.35543588 -0.10862422]]. Action = [[-0.01382591  0.0140862   0.         -0.61908865]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(8.8911e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 19 of 0
Current timestep = 20. State = [[-0.3523969  -0.10687789]]. Action = [[0.08633859 0.02253143 0.         0.29924166]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 20 of 0
Current timestep = 21. State = [[-0.35466772 -0.10571937]]. Action = [[-0.09331238  0.00683825  0.          0.02068138]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 21 of 0
Current timestep = 22. State = [[-0.35533795 -0.10973286]]. Action = [[ 0.06865007 -0.0930542   0.         -0.5477738 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 22 of 0
Current timestep = 23. State = [[-0.34960932 -0.11684751]]. Action = [[ 0.08835829 -0.09061819  0.          0.18854868]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 23 of 0
Current timestep = 24. State = [[-0.34304675 -0.12389998]]. Action = [[ 0.07361896 -0.08423173  0.         -0.63579774]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 24 of 0
Current timestep = 25. State = [[-0.3369773  -0.13042858]]. Action = [[ 0.05950814 -0.0683358   0.         -0.7147594 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, True, False, False]
State prediction error at timestep 25 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 25 of 0
Current timestep = 26. State = [[-0.33301538 -0.13643613]]. Action = [[ 0.01903774 -0.05578966  0.         -0.5061731 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, True, False, False]
State prediction error at timestep 26 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 26 of 0
Current timestep = 27. State = [[-0.327333   -0.14237295]]. Action = [[ 0.07516063 -0.05282439  0.         -0.8422386 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, True, False, False]
State prediction error at timestep 27 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 27 of 0
Current timestep = 28. State = [[-0.3271185  -0.14294182]]. Action = [[-0.07931294  0.06938038  0.          0.7019266 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, True, False, False]
State prediction error at timestep 28 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 28 of 0
Current timestep = 29. State = [[-0.32506606 -0.13984954]]. Action = [[ 0.07772846  0.05336227  0.         -0.22013861]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, True, False, False]
State prediction error at timestep 29 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 29 of 1
Current timestep = 30. State = [[-0.32055527 -0.1369205 ]]. Action = [[0.03898046 0.03698852 0.         0.9086926 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, True, False, False]
State prediction error at timestep 30 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 30 of 1
Current timestep = 31. State = [[-0.3191937  -0.13354903]]. Action = [[-0.00589917  0.0504447   0.         -0.69993025]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, True, False, False]
State prediction error at timestep 31 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.31468433 -0.13561562]]. Action = [[ 0.09184494 -0.07538123  0.         -0.03840941]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, True, False, False]
State prediction error at timestep 32 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 32 of 1
Current timestep = 33. State = [[-0.3123784  -0.13513106]]. Action = [[-0.02925529  0.06024864  0.         -0.45533967]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, True, False, False]
State prediction error at timestep 33 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
