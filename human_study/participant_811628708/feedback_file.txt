Current timestep = 0. State = [[-0.3270049  -0.09084156]]. Action = [[-0.02517132 -0.00313229  0.         -0.03610903]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is 0.012
Current timestep = 1. State = [[-0.3284511  -0.08900083]]. Action = [[ 0.01121883  0.03684529  0.         -0.24135602]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is 0.012
Current timestep = 2. State = [[-0.3323186  -0.08544317]]. Action = [[-0.06113325  0.04126974  0.          0.97395396]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is 0.012
Current timestep = 3. State = [[-0.3383631  -0.08004148]]. Action = [[-0.06263261  0.07551753  0.         -0.21903908]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is 0.012
Current timestep = 4. State = [[-0.3429387  -0.08150396]]. Action = [[-0.02606332 -0.08540395  0.          0.91687274]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is 0.012
Current timestep = 5. State = [[-0.3490021  -0.07965203]]. Action = [[-0.07761858  0.08164795  0.         -0.8448607 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is 0.012
Current timestep = 6. State = [[-0.34893882 -0.07850175]]. Action = [[ 0.09810986 -0.04243719  0.         -0.2252906 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is 0.012
Current timestep = 7. State = [[-0.3492724  -0.07514054]]. Action = [[-0.05269296  0.07933923  0.          0.7123525 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is 0.012
Current timestep = 8. State = [[-0.34811005 -0.06969579]]. Action = [[ 0.0754961   0.04299169  0.         -0.08161724]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is 0.012
Current timestep = 9. State = [[-0.35013327 -0.07000023]]. Action = [[-0.07022719 -0.05783322  0.          0.24431825]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is 0.012
Current timestep = 10. State = [[-0.34971988 -0.0753062 ]]. Action = [[ 0.06681105 -0.08201386  0.         -0.67114556]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is 0.012
Current timestep = 11. State = [[-0.34873503 -0.08106681]]. Action = [[-0.00926805 -0.06453748  0.         -0.9684069 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is 0.012
Current timestep = 12. State = [[-0.34941414 -0.08864632]]. Action = [[-0.00988554 -0.09839112  0.         -0.614913  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is 0.012
Current timestep = 13. State = [[-0.3451792  -0.09532307]]. Action = [[ 0.09057599 -0.05426887  0.          0.24568582]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is 0.012
Current timestep = 14. State = [[-0.33860153 -0.09723782]]. Action = [[ 0.07226252  0.01412418  0.         -0.4160719 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is 0.012
Current timestep = 15. State = [[-0.33855936 -0.10063896]]. Action = [[-0.05909092 -0.05171558  0.          0.7692232 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is 0.012
Current timestep = 16. State = [[-0.3438158 -0.1003454]]. Action = [[-0.08847439  0.07359082  0.          0.7894316 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is 0.012
Human Feedback received at timestep 16 of -1
Current timestep = 17. State = [[-0.3458716  -0.09542044]]. Action = [[ 0.00787131  0.08607116  0.         -0.33679414]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is 0.012
Current timestep = 18. State = [[-0.34760857 -0.08906771]]. Action = [[-0.03301679  0.08093991  0.         -0.731575  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is 0.012
Current timestep = 19. State = [[-0.3488182  -0.08823722]]. Action = [[ 0.00312903 -0.03759826  0.         -0.25821555]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is 0.012
Current timestep = 20. State = [[-0.34626248 -0.08579159]]. Action = [[0.0632496  0.05764089 0.         0.49787724]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is 0.012
Human Feedback received at timestep 20 of -1
Current timestep = 21. State = [[-0.342236   -0.08267199]]. Action = [[0.05672931 0.00467298 0.         0.501621  ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is 0.012
Current timestep = 22. State = [[-0.33771208 -0.07604022]]. Action = [[ 0.06753404  0.09819474  0.         -0.9514527 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is 0.012
Current timestep = 23. State = [[-0.33737254 -0.06802481]]. Action = [[-0.02280841  0.06887992  0.          0.03624845]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is 0.012
Human Feedback received at timestep 23 of -1
Current timestep = 24. State = [[-0.33695143 -0.05916549]]. Action = [[ 0.03349089  0.09955809  0.         -0.682824  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is 0.012
Current timestep = 25. State = [[-0.33869988 -0.05770453]]. Action = [[-0.0462237  -0.08078437  0.          0.63734305]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is 0.012
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.3381048  -0.05767744]]. Action = [[ 0.04534339  0.01859118  0.         -0.01341397]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is 0.012
Human Feedback received at timestep 26 of 1
Current timestep = 27. State = [[-0.33955047 -0.05927359]]. Action = [[-0.05272523 -0.06325215  0.         -0.01878482]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is 0.012
Current timestep = 28. State = [[-0.3442493  -0.05989646]]. Action = [[-0.07307081  0.01494579  0.         -0.91120285]]. Reward = [0.]
Curr episode timestep = 28
