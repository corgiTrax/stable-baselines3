Current timestep = 0. State = [[-0.24569419  0.22995064]]. Action = [[-0.06292832 -0.00783072  0.21038541 -0.03610903]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Current timestep = 1. State = [[-0.24770664  0.23223567]]. Action = [[ 0.02804708  0.09211323 -0.17953753 -0.24135602]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, False, True]
Scene graph at timestep 1 is [True, False, False, False, False, True]
State prediction error at timestep 1 is tensor(0.0771, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.2498676   0.23756686]]. Action = [[-0.15283313  0.10317436  0.08250085  0.97395396]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, False, True]
Current timestep = 3. State = [[-0.25799724  0.24574114]]. Action = [[-0.15658152  0.18879378 -0.05679892 -0.21903908]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, False, True]
Scene graph at timestep 3 is [True, False, False, False, False, True]
State prediction error at timestep 3 is tensor(0.0837, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.26702735  0.2523924 ]]. Action = [[-0.06515831 -0.21350987 -0.2270246   0.91687274]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, False, True]
Current timestep = 5. State = [[-0.27577662  0.24993737]]. Action = [[-0.19404647  0.20411986 -0.19760707 -0.8448607 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, False, True]
Current timestep = 6. State = [[-0.2812654   0.25451604]]. Action = [[ 0.24527463 -0.10609299 -0.19202721 -0.2252906 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, False, True]
Current timestep = 7. State = [[-0.27746245  0.2552947 ]]. Action = [[-0.13173239  0.19834805 -0.12744653  0.7123525 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, False, True]
Scene graph at timestep 7 is [True, False, False, False, False, True]
State prediction error at timestep 7 is tensor(0.0968, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.27696112  0.26400167]]. Action = [[ 0.18874025  0.10747921 -0.21657972 -0.08161724]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, False, True]
Current timestep = 9. State = [[-0.27336007  0.2690263 ]]. Action = [[-0.17556798 -0.14458303  0.2087945   0.24431825]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, False, True]
Scene graph at timestep 9 is [True, False, False, False, False, True]
State prediction error at timestep 9 is tensor(0.0850, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.2750488  0.2629357]]. Action = [[ 0.16702765 -0.20503464 -0.198931   -0.67114556]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, False, True]
Scene graph at timestep 10 is [True, False, False, False, False, True]
State prediction error at timestep 10 is tensor(0.0796, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.27100578  0.25083777]]. Action = [[-0.02317014 -0.1613437   0.12703657 -0.9684069 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, False, True]
Current timestep = 12. State = [[-0.27030936  0.23748428]]. Action = [[-0.02471386 -0.24597779  0.20099166 -0.614913  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, False, True]
Scene graph at timestep 12 is [True, False, False, False, False, True]
State prediction error at timestep 12 is tensor(0.0613, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.2677434  0.2222952]]. Action = [[ 0.22643995 -0.13567218  0.19493079  0.24568582]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, False, True]
Scene graph at timestep 13 is [True, False, False, False, False, True]
State prediction error at timestep 13 is tensor(0.0697, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.25678864  0.21244192]]. Action = [[ 0.18065628  0.03531045 -0.23489961 -0.4160719 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, False, True]
Current timestep = 15. State = [[-0.24843436  0.20868917]]. Action = [[-0.1477273  -0.12928894 -0.13149224  0.7692232 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, False, True]
Current timestep = 16. State = [[-0.2539011   0.20571578]]. Action = [[-0.22118598  0.18397707 -0.03226592  0.7894316 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, False, True]
Scene graph at timestep 16 is [True, False, False, False, False, True]
State prediction error at timestep 16 is tensor(0.0750, grad_fn=<MseLossBackward0>)
Current timestep = 17. State = [[-0.26329637  0.2139885 ]]. Action = [[ 0.01967832  0.2151779  -0.18030106 -0.33679414]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, False, True]
Current timestep = 18. State = [[-0.26749596  0.22759879]]. Action = [[-0.08254196  0.20234978 -0.12120283 -0.731575  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, False, True]
Current timestep = 19. State = [[-0.27110973  0.23842803]]. Action = [[ 0.00782257 -0.09399566  0.12785363 -0.25821555]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, False, True]
Scene graph at timestep 19 is [True, False, False, False, False, True]
State prediction error at timestep 19 is tensor(0.0722, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.27004525  0.24078973]]. Action = [[ 0.15812397  0.14410222 -0.09557876  0.49787724]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, False, True]
Current timestep = 21. State = [[-0.26207346  0.2464603 ]]. Action = [[ 0.14182323  0.01168245 -0.18392065  0.501621  ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, False, True]
Current timestep = 22. State = [[-0.2519217  0.2523749]]. Action = [[ 0.1688351   0.24548686  0.09852752 -0.9514527 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, False, True]
Current timestep = 23. State = [[-0.24340004  0.26538867]]. Action = [[-0.05702102  0.17219979  0.14208743  0.03624845]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, False, True]
Scene graph at timestep 23 is [True, False, False, False, False, True]
State prediction error at timestep 23 is tensor(0.0846, grad_fn=<MseLossBackward0>)
Current timestep = 24. State = [[-0.2409719   0.27975875]]. Action = [[ 0.08372721  0.24889523  0.13376862 -0.682824  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, False, True]
Current timestep = 25. State = [[-0.23902957  0.29070255]]. Action = [[-0.11555925 -0.20196092  0.24720162  0.63734305]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, False, True]
Current timestep = 26. State = [[-0.24062873  0.2880705 ]]. Action = [[ 0.11335847  0.04647794 -0.01763617 -0.01341397]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, False, True]
Scene graph at timestep 26 is [True, False, False, False, False, True]
State prediction error at timestep 26 is tensor(0.0885, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.23949666  0.28598827]]. Action = [[-0.1318131  -0.15813036  0.01346835 -0.01878482]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, False, True]
Scene graph at timestep 27 is [True, False, False, False, False, True]
State prediction error at timestep 27 is tensor(0.0812, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.24615282  0.28035274]]. Action = [[-0.18267702  0.03736448 -0.0018138  -0.91120285]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, False, True]
Current timestep = 29. State = [[-0.2530126   0.28143325]]. Action = [[ 0.12679619  0.13841623 -0.04986307 -0.4208697 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, False, True]
Current timestep = 30. State = [[-0.25190052  0.28901696]]. Action = [[-0.0342588   0.1584205  -0.23644787 -0.95380545]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, False, True]
Current timestep = 31. State = [[-0.25302416  0.2979647 ]]. Action = [[-0.05987707  0.03214163 -0.05190513  0.84736323]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, False, True]
Current timestep = 32. State = [[-0.2553603   0.29985726]]. Action = [[ 0.00109294 -0.17708966 -0.18974707 -0.4909749 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, False, True]
Current timestep = 33. State = [[-0.2593065   0.29129684]]. Action = [[-0.19922154 -0.19781938  0.21593627 -0.9374181 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, False, True]
Current timestep = 34. State = [[-0.26666525  0.28159216]]. Action = [[ 0.05743226  0.03883946 -0.23645777  0.09854746]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, False, True]
Current timestep = 35. State = [[-0.26834157  0.2782038 ]]. Action = [[-0.0594888  -0.08989099  0.08439064  0.6699455 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, False, True]
Current timestep = 36. State = [[-0.2671801   0.27393514]]. Action = [[ 0.23280835 -0.02098373  0.16651341  0.02484822]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, False, True]
Current timestep = 37. State = [[-0.25654215  0.26984575]]. Action = [[ 0.16575176 -0.11865516  0.24713773  0.314659  ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, False, True]
Current timestep = 38. State = [[-0.24804617  0.26191473]]. Action = [[-0.08253792 -0.17259337 -0.16319866 -0.56067777]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, False, True]
Current timestep = 39. State = [[-0.24863668  0.25177217]]. Action = [[-0.0535952  -0.0970277  -0.09475388  0.5556381 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, False, True]
Current timestep = 40. State = [[-0.25466585  0.24781474]]. Action = [[-0.22650304  0.23190928 -0.19892998  0.32077074]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, False, True]
Current timestep = 41. State = [[-0.26228896  0.25352916]]. Action = [[ 0.10872632 -0.09703131  0.20763269 -0.17703742]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, False, True]
Scene graph at timestep 41 is [True, False, False, False, False, True]
State prediction error at timestep 41 is tensor(0.0745, grad_fn=<MseLossBackward0>)
Current timestep = 42. State = [[-0.26330033  0.25263363]]. Action = [[-0.09357411  0.01792291  0.154369    0.46081853]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, False, True]
Scene graph at timestep 42 is [True, False, False, False, False, True]
State prediction error at timestep 42 is tensor(0.0827, grad_fn=<MseLossBackward0>)
Current timestep = 43. State = [[-0.26413816  0.2547004 ]]. Action = [[ 0.15594012  0.13645342  0.0622842  -0.7390872 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, False, True]
Current timestep = 44. State = [[-0.25976625  0.2626915 ]]. Action = [[-0.02267635  0.18309161  0.1395728  -0.0730992 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, False, True]
Scene graph at timestep 44 is [True, False, False, False, False, True]
State prediction error at timestep 44 is tensor(0.0869, grad_fn=<MseLossBackward0>)
Current timestep = 45. State = [[-0.256683    0.27168614]]. Action = [[ 0.11342704 -0.04019237  0.05631548 -0.51758724]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, False, True]
Scene graph at timestep 45 is [True, False, False, False, False, True]
State prediction error at timestep 45 is tensor(0.0763, grad_fn=<MseLossBackward0>)
Current timestep = 46. State = [[-0.25075206  0.27484924]]. Action = [[0.09529731 0.10571244 0.22975743 0.02757192]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, False, True]
Scene graph at timestep 46 is [True, False, False, False, False, True]
State prediction error at timestep 46 is tensor(0.0844, grad_fn=<MseLossBackward0>)
Current timestep = 47. State = [[-0.24562709  0.27851212]]. Action = [[-0.03294587 -0.06545065  0.09615448  0.755185  ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, False, True]
Current timestep = 48. State = [[-0.24674164  0.27834734]]. Action = [[-0.1157887   0.05567047 -0.20120142  0.37288344]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, False, True]
Current timestep = 49. State = [[-0.24948633  0.2812127 ]]. Action = [[ 0.1134553   0.07748425 -0.20218235  0.8847685 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, False, True]
Current timestep = 50. State = [[-0.24776015  0.2821573 ]]. Action = [[-0.07827386 -0.20051333 -0.12017211 -0.02700591]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, False, True]
Current timestep = 51. State = [[-0.24693558  0.27870122]]. Action = [[ 0.2068848   0.22652376 -0.03586359  0.1242373 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, False, True]
Scene graph at timestep 51 is [True, False, False, False, False, True]
State prediction error at timestep 51 is tensor(0.0936, grad_fn=<MseLossBackward0>)
Current timestep = 52. State = [[-0.23664112  0.2821854 ]]. Action = [[ 0.18189603 -0.20764373  0.18816039  0.5712166 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, False, True]
Current timestep = 53. State = [[-0.22852403  0.27563712]]. Action = [[-0.11441588 -0.05674538 -0.12370075 -0.8974389 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, False, True]
Current timestep = 54. State = [[-0.23097181  0.2680084 ]]. Action = [[-0.12237573 -0.2230823   0.20830828  0.16367233]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, False, True]
Current timestep = 55. State = [[-0.23783591  0.25509548]]. Action = [[-0.08687785 -0.17729017 -0.08210889  0.7207265 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, False, True]
Current timestep = 56. State = [[-0.2428318   0.24574931]]. Action = [[ 0.05953512  0.11114797 -0.14817959  0.91396666]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, False, True]
Scene graph at timestep 56 is [True, False, False, False, False, True]
State prediction error at timestep 56 is tensor(0.0802, grad_fn=<MseLossBackward0>)
Current timestep = 57. State = [[-0.23853517  0.24425028]]. Action = [[ 0.23208168 -0.15677391  0.2330029  -0.27012765]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, False, True]
Current timestep = 58. State = [[-0.22940995  0.2359187 ]]. Action = [[-0.01688263 -0.17942126 -0.09049878  0.02913284]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, False, True]
Scene graph at timestep 58 is [True, False, False, False, False, True]
State prediction error at timestep 58 is tensor(0.0670, grad_fn=<MseLossBackward0>)
Current timestep = 59. State = [[-0.23036666  0.22975153]]. Action = [[-0.22462757  0.22966427  0.03413746 -0.01006943]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, False, True]
Current timestep = 60. State = [[-0.23676771  0.23787373]]. Action = [[ 0.08144125  0.15033007 -0.04957311  0.36233544]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, False, True]
Current timestep = 61. State = [[-0.233321    0.24631464]]. Action = [[ 0.21871018 -0.00888601  0.11972737  0.6086904 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, False, True]
Current timestep = 62. State = [[-0.2246269   0.24657486]]. Action = [[-0.015443   -0.1623418  -0.07818675 -0.59949154]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, False, True]
Current timestep = 63. State = [[-0.2209138   0.24310595]]. Action = [[ 0.09055582  0.1572401   0.16502541 -0.9037737 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, False, True]
Current timestep = 64. State = [[-0.21811597  0.24736343]]. Action = [[-0.11106697  0.02211285 -0.24506785 -0.55001134]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, False, True]
Current timestep = 65. State = [[-0.2234984   0.25016755]]. Action = [[-0.18644704  0.01333836  0.17273095 -0.73126566]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, False, True]
Current timestep = 66. State = [[-0.23058818  0.24934952]]. Action = [[ 0.07175013 -0.1642149  -0.24593687 -0.78596085]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, False, True]
Scene graph at timestep 66 is [True, False, False, False, False, True]
State prediction error at timestep 66 is tensor(0.0688, grad_fn=<MseLossBackward0>)
Current timestep = 67. State = [[-0.22753493  0.24207193]]. Action = [[ 0.24854136 -0.08151072  0.20405278  0.7270007 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, False, True]
Scene graph at timestep 67 is [True, False, False, False, False, True]
State prediction error at timestep 67 is tensor(0.0667, grad_fn=<MseLossBackward0>)
Current timestep = 68. State = [[-0.21791746  0.2394017 ]]. Action = [[0.03160676 0.21968237 0.19586185 0.43771553]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, False, True]
Current timestep = 69. State = [[-0.21523045  0.24431138]]. Action = [[-0.20742673 -0.13721892  0.14055955  0.3013432 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, False, True]
Current timestep = 70. State = [[-0.22220933  0.24092823]]. Action = [[-0.03636354 -0.05990322 -0.18529083  0.02888215]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, False, True]
Scene graph at timestep 70 is [True, False, False, False, False, True]
State prediction error at timestep 70 is tensor(0.0712, grad_fn=<MseLossBackward0>)
Current timestep = 71. State = [[-0.22633292  0.23706406]]. Action = [[ 0.01221383 -0.01296236  0.090105   -0.54344183]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, False, True]
Current timestep = 72. State = [[-0.22968397  0.23419707]]. Action = [[-0.16936138 -0.06833398 -0.01987658  0.9019809 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, False, True]
Current timestep = 73. State = [[-0.23619144  0.23446508]]. Action = [[ 0.04703778  0.2433297   0.10549861 -0.14095664]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, False, True]
Current timestep = 74. State = [[-0.23689444  0.23985492]]. Action = [[-0.04591274 -0.21344574  0.05896783 -0.5593137 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, False, True]
Current timestep = 75. State = [[-0.24047697  0.23213367]]. Action = [[-0.11358589 -0.18174565  0.24147367 -0.39439988]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, False, True]
Current timestep = 76. State = [[-0.2474502   0.22270091]]. Action = [[-9.40460265e-02  4.54038382e-04 -1.00999266e-01  7.13565111e-01]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, False, True]
Current timestep = 77. State = [[-0.25313562  0.21659136]]. Action = [[-0.03371632 -0.19152136 -0.07343674 -0.311545  ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, False, True]
Current timestep = 78. State = [[-0.25601453  0.20454742]]. Action = [[ 0.022044   -0.2426457   0.10017937  0.51455307]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, False, True]
Current timestep = 79. State = [[-0.25979868  0.18970145]]. Action = [[-0.2102649  -0.13816953 -0.21217577 -0.64967614]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, False, True]
Scene graph at timestep 79 is [True, False, False, False, False, True]
State prediction error at timestep 79 is tensor(0.0580, grad_fn=<MseLossBackward0>)
Current timestep = 80. State = [[-0.2678551   0.17750517]]. Action = [[-0.0068455  -0.16103208 -0.06041899 -0.23225856]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, False, True]
Current timestep = 81. State = [[-0.2695113   0.16669837]]. Action = [[ 0.13097224 -0.0734749  -0.16514692 -0.47577   ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, False, True]
Current timestep = 82. State = [[-0.26922938  0.16357152]]. Action = [[-0.202117    0.23224825 -0.12801798 -0.83660877]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, False, True]
Current timestep = 83. State = [[-0.27084497  0.17113811]]. Action = [[ 0.2389484   0.02112851  0.07512534 -0.7967107 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, False, True]
Current timestep = 84. State = [[-0.26388192  0.17477822]]. Action = [[ 0.02619484 -0.00192858 -0.2153943  -0.9035846 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, False, True]
Current timestep = 85. State = [[-0.25874126  0.17712928]]. Action = [[ 0.09412518  0.08068794 -0.08480367 -0.30874026]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, False, True]
Scene graph at timestep 85 is [True, False, False, False, False, True]
State prediction error at timestep 85 is tensor(0.0615, grad_fn=<MseLossBackward0>)
Current timestep = 86. State = [[-0.25402158  0.17857906]]. Action = [[-0.04189269 -0.13539053  0.167485   -0.28540534]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, False, True]
Current timestep = 87. State = [[-0.25606465  0.17396705]]. Action = [[-0.13219015 -0.04103981  0.03815037 -0.12867564]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, False, True]
Current timestep = 88. State = [[-0.2614744   0.17048538]]. Action = [[-0.00802706 -0.01777503 -0.12589616  0.3568728 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, False, True]
Scene graph at timestep 88 is [True, False, False, False, False, True]
State prediction error at timestep 88 is tensor(0.0663, grad_fn=<MseLossBackward0>)
Current timestep = 89. State = [[-0.26531482  0.16525464]]. Action = [[-1.2616846e-01 -2.2613351e-01  7.9822540e-04  9.8299289e-01]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, False, True]
Scene graph at timestep 89 is [True, False, False, False, False, True]
State prediction error at timestep 89 is tensor(0.0626, grad_fn=<MseLossBackward0>)
Current timestep = 90. State = [[-0.27458912  0.15883933]]. Action = [[-0.17308962  0.20423535 -0.04385433  0.30456305]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, False, True]
Scene graph at timestep 90 is [True, False, False, False, False, True]
State prediction error at timestep 90 is tensor(0.0714, grad_fn=<MseLossBackward0>)
Current timestep = 91. State = [[-0.28675583  0.16508064]]. Action = [[-0.24324673  0.13268635 -0.24387646 -0.79303753]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, False, True]
Current timestep = 92. State = [[-0.29766184  0.16996148]]. Action = [[ 0.05326334 -0.16724561  0.03834906 -0.19529057]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, False, True]
Current timestep = 93. State = [[-0.30107436  0.16824235]]. Action = [[ 0.00920275  0.15086064 -0.07823598  0.37502968]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, False, True]
Current timestep = 94. State = [[-0.29971877  0.17262305]]. Action = [[0.07915059 0.01438561 0.04716134 0.10213578]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, False, True]
Scene graph at timestep 94 is [True, False, False, False, False, True]
State prediction error at timestep 94 is tensor(0.0742, grad_fn=<MseLossBackward0>)
Current timestep = 95. State = [[-0.29797763  0.17638019]]. Action = [[-0.07474667  0.10044095 -0.15577957 -0.81921107]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, False, True]
Current timestep = 96. State = [[-0.29986435  0.18219952]]. Action = [[-0.02428597  0.07535601 -0.1852176  -0.04370892]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, False, True]
Current timestep = 97. State = [[-0.29853627  0.18847218]]. Action = [[ 0.20194349  0.10286158  0.20833257 -0.6593675 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, False, True]
Current timestep = 98. State = [[-0.2928982   0.19465289]]. Action = [[-0.09913734  0.04422888  0.02979702  0.92147255]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, False, True]
Current timestep = 99. State = [[-0.29524347  0.19943762]]. Action = [[-0.09856635  0.0674566  -0.21167783 -0.16999316]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, False, True]
Current timestep = 100. State = [[-0.3013333  0.2001096]]. Action = [[-0.15079565 -0.21940632 -0.02369845  0.11940181]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, False, True]
Scene graph at timestep 100 is [True, False, False, False, False, True]
State prediction error at timestep 100 is tensor(0.0760, grad_fn=<MseLossBackward0>)
Current timestep = 101. State = [[-0.31292307  0.19248389]]. Action = [[-0.22904678 -0.02748613  0.116041    0.28773856]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, False, True]
Current timestep = 102. State = [[-0.32339072  0.19019057]]. Action = [[0.09275281 0.10883743 0.06741947 0.2768674 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, False, True]
Current timestep = 103. State = [[-0.32040492  0.19281895]]. Action = [[ 0.22295952 -0.02079807 -0.20881678 -0.28873467]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, False, True]
Current timestep = 104. State = [[-0.31320825  0.19162776]]. Action = [[-0.10683371 -0.1037007   0.21886036 -0.7239744 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, False, True]
Current timestep = 105. State = [[-0.31435308  0.18645504]]. Action = [[-0.0370544  -0.08820483 -0.06997551 -0.9768368 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, False, True]
Current timestep = 106. State = [[-0.3133598   0.18102615]]. Action = [[ 0.20893937 -0.03371015  0.20521957 -0.11461991]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, False, True]
Scene graph at timestep 106 is [True, False, False, False, False, True]
State prediction error at timestep 106 is tensor(0.0724, grad_fn=<MseLossBackward0>)
Current timestep = 107. State = [[-0.30666062  0.17744148]]. Action = [[-0.02727693 -0.0242108   0.22461662 -0.08456141]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, False, True]
Current timestep = 108. State = [[-0.30515617  0.17468429]]. Action = [[-0.03981924 -0.0557791  -0.02126198 -0.52216256]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, False, True]
Current timestep = 109. State = [[-0.30435452  0.17347908]]. Action = [[ 0.14718503  0.10361809 -0.11176175 -0.8177222 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, False, True]
Current timestep = 110. State = [[-0.2959203   0.17655548]]. Action = [[ 0.20218009  0.0075199   0.15887168 -0.9461646 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, False, True]
Current timestep = 111. State = [[-0.28343433  0.18111199]]. Action = [[ 0.20505464  0.19873732 -0.13456756  0.9906614 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, False, True]
Current timestep = 112. State = [[-0.2680625   0.18887803]]. Action = [[ 0.2132181  -0.03087531  0.17487937 -0.96350336]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, False, True]
Current timestep = 113. State = [[-0.25860447  0.19204867]]. Action = [[-0.1854473   0.08423623  0.15571609  0.84478045]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, False, True]
Scene graph at timestep 113 is [True, False, False, False, False, True]
State prediction error at timestep 113 is tensor(0.0689, grad_fn=<MseLossBackward0>)
Current timestep = 114. State = [[-0.2590005   0.19949795]]. Action = [[ 0.17291403  0.24345651 -0.08439955  0.2454791 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, False, True]
Current timestep = 115. State = [[-0.25423843  0.21320835]]. Action = [[-0.02897206  0.20259047  0.20263237 -0.38037252]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, False, True]
Current timestep = 116. State = [[-0.2512654   0.22405578]]. Action = [[ 0.08030078 -0.07574013 -0.17948721  0.67524683]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, False, True]
Current timestep = 117. State = [[-0.24648811  0.2285735 ]]. Action = [[0.14925262 0.21274069 0.03794622 0.4118315 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, False, True]
Current timestep = 118. State = [[-0.23939215  0.23806554]]. Action = [[-0.0137002   0.07842419  0.22548386  0.8506783 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, False, True]
Current timestep = 119. State = [[-0.23451953  0.24631861]]. Action = [[ 0.18740013  0.12822524  0.15111178 -0.5467224 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, False, True]
Scene graph at timestep 119 is [True, False, False, False, False, True]
State prediction error at timestep 119 is tensor(0.0676, grad_fn=<MseLossBackward0>)
Current timestep = 120. State = [[-0.22591974  0.25290552]]. Action = [[ 0.03344995 -0.03253202 -0.18300799 -0.27900124]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, False, True]
Current timestep = 121. State = [[-0.224611    0.25651628]]. Action = [[-0.1797661   0.15656316  0.21257979 -0.14114434]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, False, True]
Current timestep = 122. State = [[-0.22655481  0.26239085]]. Action = [[ 0.21754336 -0.03369781 -0.02604298  0.5711534 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, False, True]
Current timestep = 123. State = [[-0.22114113  0.2620609 ]]. Action = [[-0.04699215 -0.08934003  0.03357944  0.10692334]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, False, True]
Current timestep = 124. State = [[-0.21983671  0.257985  ]]. Action = [[ 0.04003665 -0.05808204  0.1481753   0.21763086]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, False, True]
Scene graph at timestep 124 is [True, False, False, False, False, True]
State prediction error at timestep 124 is tensor(0.0707, grad_fn=<MseLossBackward0>)
Current timestep = 125. State = [[-0.22132128  0.2547828 ]]. Action = [[-0.1978653   0.02526319 -0.0407638  -0.05132365]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, False, True]
Current timestep = 126. State = [[-0.22579719  0.25428286]]. Action = [[ 0.17063957 -0.01758155  0.12360758 -0.04624939]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, False, True]
Scene graph at timestep 126 is [True, False, False, False, False, True]
State prediction error at timestep 126 is tensor(0.0704, grad_fn=<MseLossBackward0>)
Current timestep = 127. State = [[-0.2238429   0.25378236]]. Action = [[-0.11885005  0.01701701 -0.03999653  0.01879144]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, False, True]
Current timestep = 128. State = [[-0.23018767  0.25497866]]. Action = [[-0.24058092  0.05086353 -0.04525606  0.18736851]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, False, True]
Current timestep = 129. State = [[-0.24470188  0.26002967]]. Action = [[-0.24276508  0.19436371 -0.2091362  -0.909334  ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, False, True]
Current timestep = 130. State = [[-0.26050052  0.26524466]]. Action = [[-0.212147   -0.20059066  0.02537453  0.97973585]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, False, True]
Current timestep = 131. State = [[-0.27374893  0.2639297 ]]. Action = [[ 0.06131846  0.21765405  0.14906627 -0.33817333]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, False, True]
Scene graph at timestep 131 is [True, False, False, False, False, True]
State prediction error at timestep 131 is tensor(0.0871, grad_fn=<MseLossBackward0>)
Current timestep = 132. State = [[-0.275172    0.26749766]]. Action = [[-3.9282441e-04 -1.8119034e-01 -6.9955885e-02 -9.3395925e-01]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, False, True]
Current timestep = 133. State = [[-0.2749065   0.26058537]]. Action = [[ 0.04398781 -0.1731964   0.03767967  0.3322369 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, False, True]
Current timestep = 134. State = [[-0.2703118   0.24862714]]. Action = [[ 0.20921946 -0.22330736 -0.07237035 -0.1830532 ]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, False, True]
Scene graph at timestep 134 is [True, False, False, False, False, True]
State prediction error at timestep 134 is tensor(0.0767, grad_fn=<MseLossBackward0>)
Current timestep = 135. State = [[-0.26372725  0.23789856]]. Action = [[-0.10398477  0.06595671 -0.08796173 -0.263579  ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, False, True]
Current timestep = 136. State = [[-0.2624787   0.23551925]]. Action = [[ 0.10536161 -0.0464419   0.116898   -0.5455249 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, False, True]
Scene graph at timestep 136 is [True, False, False, False, False, True]
State prediction error at timestep 136 is tensor(0.0653, grad_fn=<MseLossBackward0>)
Current timestep = 137. State = [[-0.2575782   0.23497242]]. Action = [[ 0.11343995  0.10054204 -0.23627533 -0.8251441 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, False, True]
Current timestep = 138. State = [[-0.2513595   0.23468983]]. Action = [[-0.02240205 -0.21621992  0.01480576 -0.6011182 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, False, True]
Current timestep = 139. State = [[-0.2500187   0.22393951]]. Action = [[ 0.00360641 -0.24350345 -0.21222901  0.4732709 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, False, True]
Scene graph at timestep 139 is [True, False, False, False, False, True]
State prediction error at timestep 139 is tensor(0.0711, grad_fn=<MseLossBackward0>)
Current timestep = 140. State = [[-0.24744081  0.21447013]]. Action = [[ 0.21982399  0.1689884  -0.12315628  0.75975275]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, False, True]
Current timestep = 141. State = [[-0.23723084  0.21985084]]. Action = [[ 0.1033833   0.22558865 -0.03103516 -0.06988275]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, False, True]
Current timestep = 142. State = [[-0.23030724  0.23234044]]. Action = [[-0.0604167   0.17406571  0.14437044  0.17399645]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, False, True]
Current timestep = 143. State = [[-0.22866894  0.2462951 ]]. Action = [[ 0.10276279  0.22698045 -0.21996301 -0.52428895]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, False, True]
Current timestep = 144. State = [[-0.22226147  0.25831893]]. Action = [[ 0.15435696 -0.02193694 -0.171608   -0.70893675]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, False, True]
Current timestep = 145. State = [[-0.2115742   0.26103035]]. Action = [[ 0.21616817 -0.06358007 -0.03994004  0.4767716 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, False, True]
Scene graph at timestep 145 is [True, False, False, False, False, True]
State prediction error at timestep 145 is tensor(0.0732, grad_fn=<MseLossBackward0>)
Current timestep = 146. State = [[-0.20018382  0.25819534]]. Action = [[ 0.00769588 -0.09368058 -0.1350201   0.24625003]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, False, True]
Current timestep = 147. State = [[-0.19855028  0.25476813]]. Action = [[-0.18548284  0.04960519  0.105371   -0.9420088 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, False, True]
Current timestep = 148. State = [[-0.20535351  0.25550982]]. Action = [[-0.05987763  0.03108501 -0.16109483 -0.21106172]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, False, True]
Scene graph at timestep 148 is [True, False, False, False, False, True]
State prediction error at timestep 148 is tensor(0.0722, grad_fn=<MseLossBackward0>)
Current timestep = 149. State = [[-0.21115367  0.2598411 ]]. Action = [[-0.04991546  0.1909563   0.15661633 -0.31137848]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, False, True]
Current timestep = 150. State = [[-0.21409892  0.26579922]]. Action = [[ 0.01599884 -0.11815986 -0.0712131  -0.23423773]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, False, True]
Current timestep = 151. State = [[-0.2163673   0.26735446]]. Action = [[-0.08839943  0.19495249  0.03480902 -0.5698542 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, False, True]
Current timestep = 152. State = [[-0.22258984  0.2723097 ]]. Action = [[-0.21367085 -0.1041984   0.20831594 -0.30824268]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, False, True]
Current timestep = 153. State = [[-0.23298623  0.2686048 ]]. Action = [[-0.04773346 -0.14984342 -0.06717011 -0.49667573]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, False, True]
Current timestep = 154. State = [[-0.2392222   0.25857624]]. Action = [[-0.04414341 -0.2276486   0.2119298   0.677847  ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, False, True]
Scene graph at timestep 154 is [True, False, False, False, False, True]
State prediction error at timestep 154 is tensor(0.0726, grad_fn=<MseLossBackward0>)
Current timestep = 155. State = [[-0.24533181  0.24636582]]. Action = [[-0.16135721 -0.07142517 -0.20095442 -0.58711743]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, False, True]
Scene graph at timestep 155 is [True, False, False, False, False, True]
State prediction error at timestep 155 is tensor(0.0722, grad_fn=<MseLossBackward0>)
Current timestep = 156. State = [[-0.24931976  0.23694888]]. Action = [[ 0.24078846 -0.1651571   0.21897483  0.2608112 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, False, True]
Current timestep = 157. State = [[-0.24471766  0.22557208]]. Action = [[-0.14201003 -0.1747597  -0.1260021   0.16281736]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, False, True]
Current timestep = 158. State = [[-0.24801275  0.21283881]]. Action = [[-0.04772323 -0.1936308   0.18407002 -0.32034814]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, False, True]
Scene graph at timestep 158 is [True, False, False, False, False, True]
State prediction error at timestep 158 is tensor(0.0561, grad_fn=<MseLossBackward0>)
Current timestep = 159. State = [[-0.24844123  0.19905895]]. Action = [[ 0.20126098 -0.18334974 -0.09023497  0.94406533]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, False, True]
Scene graph at timestep 159 is [True, False, False, False, False, True]
State prediction error at timestep 159 is tensor(0.0622, grad_fn=<MseLossBackward0>)
Current timestep = 160. State = [[-0.24099192  0.18779005]]. Action = [[ 0.06107664 -0.01960391  0.06645605 -0.8740796 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, False, True]
Scene graph at timestep 160 is [True, False, False, False, False, True]
State prediction error at timestep 160 is tensor(0.0501, grad_fn=<MseLossBackward0>)
Current timestep = 161. State = [[-0.23293602  0.18449433]]. Action = [[0.19856533 0.08630535 0.1669032  0.36968005]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, False, True]
Current timestep = 162. State = [[-0.22352718  0.185923  ]]. Action = [[-0.02202916 -0.0201029   0.1141777  -0.33206046]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, False, True]
Scene graph at timestep 162 is [True, False, False, False, False, True]
State prediction error at timestep 162 is tensor(0.0497, grad_fn=<MseLossBackward0>)
Current timestep = 163. State = [[-0.22207445  0.18839791]]. Action = [[-0.08309558  0.14805996 -0.175633    0.6142738 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, False, True]
Current timestep = 164. State = [[-0.22339077  0.19550632]]. Action = [[ 0.08665794  0.09695548 -0.22014058 -0.14046508]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, False, True]
Current timestep = 165. State = [[-0.21762449  0.20369168]]. Action = [[ 0.2477481   0.14907333 -0.16502199 -0.28255308]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, False, True]
Scene graph at timestep 165 is [True, False, False, False, False, True]
State prediction error at timestep 165 is tensor(0.0628, grad_fn=<MseLossBackward0>)
Current timestep = 166. State = [[-0.20560902  0.21203588]]. Action = [[ 0.08860108  0.03889459  0.03688529 -0.71008694]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, False, True]
Scene graph at timestep 166 is [True, False, False, False, False, True]
State prediction error at timestep 166 is tensor(0.0522, grad_fn=<MseLossBackward0>)
Current timestep = 167. State = [[-0.19572738  0.21569997]]. Action = [[ 0.15250719 -0.04388523 -0.20775358 -0.9477239 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, False, True]
Current timestep = 168. State = [[-0.18533865  0.21819197]]. Action = [[ 0.13893068  0.17110065 -0.16432643  0.3619163 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, False, True]
Scene graph at timestep 168 is [True, False, False, False, False, True]
State prediction error at timestep 168 is tensor(0.0641, grad_fn=<MseLossBackward0>)
Current timestep = 169. State = [[-0.17349538  0.22068517]]. Action = [[ 0.21873432 -0.2376918   0.13337493 -0.6846394 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, False, True]
Scene graph at timestep 169 is [True, False, False, False, False, True]
State prediction error at timestep 169 is tensor(0.0444, grad_fn=<MseLossBackward0>)
Current timestep = 170. State = [[-0.16209319  0.21684095]]. Action = [[ 0.00389519  0.1709145  -0.05306125 -0.57995296]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, False, True]
Current timestep = 171. State = [[-0.1558294   0.22144017]]. Action = [[ 0.12400207  0.0537281   0.10981166 -0.50004685]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, False, True]
Current timestep = 172. State = [[-0.15144552  0.22759663]]. Action = [[-0.1143738   0.15419835 -0.00956962 -0.32204366]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, False, True]
Current timestep = 173. State = [[-0.15507284  0.23095056]]. Action = [[-0.10765925 -0.24071933  0.11024138  0.6125511 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, False, True]
Current timestep = 174. State = [[-0.16305742  0.2251835 ]]. Action = [[-0.17602418  0.03908932  0.22933602  0.65946627]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, False, True]
Current timestep = 175. State = [[-0.17194642  0.2253227 ]]. Action = [[-0.00339782  0.09062034  0.18818784 -0.50893474]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, False, True]
Scene graph at timestep 175 is [True, False, False, False, False, True]
State prediction error at timestep 175 is tensor(0.0509, grad_fn=<MseLossBackward0>)
Current timestep = 176. State = [[-0.1726929   0.22790302]]. Action = [[ 0.17917651 -0.03283335  0.23054433 -0.30285203]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, False, True]
Scene graph at timestep 176 is [True, False, False, False, False, True]
State prediction error at timestep 176 is tensor(0.0493, grad_fn=<MseLossBackward0>)
Current timestep = 177. State = [[-0.1667895   0.23081319]]. Action = [[ 0.00195935  0.18051466 -0.01837017 -0.06658101]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, False, True]
Current timestep = 178. State = [[-0.16295342  0.24016294]]. Action = [[ 0.08475149  0.16424847  0.05031881 -0.03266454]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, False, True]
Current timestep = 179. State = [[-0.1577119  0.2455578]]. Action = [[ 0.07929349 -0.21524003  0.06381196 -0.6378873 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, False, True]
Current timestep = 180. State = [[-0.15398328  0.23887655]]. Action = [[-0.05665383 -0.11533035 -0.13117161  0.69155   ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, False, True]
Current timestep = 181. State = [[-0.15641148  0.23380105]]. Action = [[-0.12755789  0.08888021  0.14275604  0.8183851 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, False, True]
Current timestep = 182. State = [[-0.16008088  0.23117912]]. Action = [[ 0.10639477 -0.22023146  0.18306822 -0.8239961 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, False, True]
Current timestep = 183. State = [[-0.16078857  0.22696309]]. Action = [[-0.17693232  0.21541941  0.19861531 -0.18445301]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, False, True]
Scene graph at timestep 183 is [True, False, False, False, False, True]
State prediction error at timestep 183 is tensor(0.0569, grad_fn=<MseLossBackward0>)
Current timestep = 184. State = [[-0.16554224  0.23296036]]. Action = [[0.06473324 0.06303507 0.11400709 0.8529171 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, False, True]
Current timestep = 185. State = [[-0.16300374  0.23823807]]. Action = [[ 0.16424423  0.04459471 -0.20879027 -0.38379776]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, False, True]
Current timestep = 186. State = [[-0.15869814  0.23889169]]. Action = [[-0.14548479 -0.16777208  0.02824131 -0.94576174]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, False, True]
Current timestep = 187. State = [[-0.1586853   0.23747465]]. Action = [[0.18637186 0.2318497  0.13847488 0.47669947]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, False, True]
Scene graph at timestep 187 is [True, False, False, False, False, True]
State prediction error at timestep 187 is tensor(0.0595, grad_fn=<MseLossBackward0>)
Current timestep = 188. State = [[-0.15110375  0.2441174 ]]. Action = [[ 0.13150212  0.00635302  0.02051318 -0.57161003]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, False, True]
Scene graph at timestep 188 is [True, False, False, False, False, True]
State prediction error at timestep 188 is tensor(0.0534, grad_fn=<MseLossBackward0>)
Current timestep = 189. State = [[-0.14028879  0.25093973]]. Action = [[0.21966851 0.23917645 0.16514823 0.1119225 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, False, True]
Current timestep = 190. State = [[-0.1314367   0.25747544]]. Action = [[-0.13592981 -0.21215163 -0.08386201  0.78376555]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, False, True]
Current timestep = 191. State = [[-0.13253513  0.25302163]]. Action = [[-0.01555954 -0.02376109  0.07114136 -0.7951282 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, False, True]
Current timestep = 192. State = [[-0.13088104  0.25175872]]. Action = [[0.19617379 0.09813368 0.16204616 0.23868382]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, False, True]
Scene graph at timestep 192 is [True, False, False, False, False, True]
State prediction error at timestep 192 is tensor(0.0550, grad_fn=<MseLossBackward0>)
Current timestep = 193. State = [[-0.12081113  0.25043517]]. Action = [[ 0.21188438 -0.24325876  0.18500295 -0.85029477]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, False, True]
Current timestep = 194. State = [[-0.10885444  0.24633381]]. Action = [[ 0.0614987   0.23780507 -0.0826121  -0.0240348 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, False, True]
Current timestep = 195. State = [[-0.10359793  0.24925533]]. Action = [[-0.06690162 -0.17114142  0.12228313  0.64145064]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, False, True]
Current timestep = 196. State = [[-0.10473488  0.24284524]]. Action = [[-0.04993281 -0.15844724  0.22818685 -0.46609902]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, False, True]
Current timestep = 197. State = [[-0.10801539  0.23925383]]. Action = [[-0.07283235  0.24509746  0.24752    -0.04217392]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, False, True]
Scene graph at timestep 197 is [True, False, False, False, False, True]
State prediction error at timestep 197 is tensor(0.0547, grad_fn=<MseLossBackward0>)
Current timestep = 198. State = [[-0.11197363  0.24639066]]. Action = [[-0.01295364  0.05526581 -0.23472893 -0.13174242]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, False, True]
Current timestep = 199. State = [[-0.11332239  0.2524366 ]]. Action = [[0.03442681 0.07938659 0.20709142 0.46658075]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, False, True]
Current timestep = 200. State = [[-0.10920139  0.2543823 ]]. Action = [[ 0.22167093 -0.15932208 -0.12230529  0.8047106 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, False, True]
Current timestep = 201. State = [[-0.10435638  0.25207728]]. Action = [[-0.24909319  0.11405769 -0.18466775 -0.7303585 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, False, True]
Current timestep = 202. State = [[-0.10959981  0.2575665 ]]. Action = [[ 0.03432423  0.1850369   0.06134829 -0.37497103]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, False, True]
Current timestep = 203. State = [[-0.10994019  0.26670244]]. Action = [[0.07699198 0.07341385 0.20333654 0.88116693]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, False, True]
Scene graph at timestep 203 is [True, False, False, False, False, True]
State prediction error at timestep 203 is tensor(0.0598, grad_fn=<MseLossBackward0>)
Current timestep = 204. State = [[-0.11128294  0.27285898]]. Action = [[-0.23884155  0.01859024  0.18437639  0.56615937]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, False, True]
Current timestep = 205. State = [[-0.11590607  0.27505487]]. Action = [[ 0.19656172 -0.03054556  0.1307362  -0.00350201]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, False, True]
Scene graph at timestep 205 is [True, False, False, False, False, True]
State prediction error at timestep 205 is tensor(0.0589, grad_fn=<MseLossBackward0>)
Current timestep = 206. State = [[-0.11297888  0.2736445 ]]. Action = [[-0.06895259 -0.07097568 -0.12628926  0.43885493]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, False, True]
Current timestep = 207. State = [[-0.11627462  0.26894262]]. Action = [[-0.16641675 -0.11855742 -0.23823376  0.9427171 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, False, True]
Scene graph at timestep 207 is [True, False, False, False, False, True]
State prediction error at timestep 207 is tensor(0.0635, grad_fn=<MseLossBackward0>)
Current timestep = 208. State = [[-0.11909     0.26113364]]. Action = [[ 0.2343004  -0.13623904 -0.21723409  0.99166226]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, False, True]
Current timestep = 209. State = [[-0.1122895   0.25219932]]. Action = [[ 0.04622212 -0.10608259 -0.02175751  0.32107306]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, False, True]
Current timestep = 210. State = [[-0.10459273  0.24879177]]. Action = [[ 0.1895284   0.19233215 -0.06150877  0.27240837]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, False, True]
Current timestep = 211. State = [[-0.09593209  0.2529852 ]]. Action = [[ 0.01404774 -0.03061989 -0.14222047 -0.4178825 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, False, True]
Current timestep = 212. State = [[-0.09593724  0.25757587]]. Action = [[-0.24554722  0.21270025  0.02171019  0.60448813]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, False, True]
Current timestep = 213. State = [[-0.10546855  0.26793712]]. Action = [[-0.09468235  0.13363963 -0.13187072 -0.6106347 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, False, True]
Current timestep = 214. State = [[-0.10901198  0.27867845]]. Action = [[ 0.20487124  0.16343334 -0.24176301 -0.37116373]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, False, True]
Current timestep = 215. State = [[-0.10439973  0.28860664]]. Action = [[-0.02497853  0.05948555 -0.2479739  -0.535717  ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, False, True]
Current timestep = 216. State = [[-0.1007185   0.29670566]]. Action = [[ 0.13009328  0.15356517 -0.04910804 -0.7833507 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, False, True]
Current timestep = 217. State = [[-0.09958889  0.3037073 ]]. Action = [[-0.23795587 -0.03682694  0.21681976  0.03574967]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, False, True]
Current timestep = 218. State = [[-0.10347108  0.30338797]]. Action = [[ 0.16888863 -0.10855861  0.12292001 -0.55490094]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, False, True]
Scene graph at timestep 218 is [True, False, False, False, False, True]
State prediction error at timestep 218 is tensor(0.0670, grad_fn=<MseLossBackward0>)
Current timestep = 219. State = [[-0.10096002  0.3004917 ]]. Action = [[-0.05478552  0.04259902  0.08141324  0.1606834 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, False, True]
Current timestep = 220. State = [[-0.10084004  0.30007198]]. Action = [[ 0.03532082 -0.02958338  0.19590226 -0.653043  ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, False, True]
Current timestep = 221. State = [[-0.10156815  0.30260938]]. Action = [[-0.11177251  0.20285508  0.24604356  0.81247103]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, False, True]
Current timestep = 222. State = [[-0.10544303  0.30962288]]. Action = [[-0.21468942  0.20655793  0.19606614  0.8246758 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, False, True]
Current timestep = 223. State = [[-0.10695502  0.31243727]]. Action = [[-0.21628301  0.19669035  0.238145    0.93394005]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, False, True]
Current timestep = 224. State = [[-0.10740344  0.31336042]]. Action = [[-0.0909445   0.23682892  0.14686653 -0.4123146 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, False, True]
Scene graph at timestep 224 is [True, False, False, False, False, True]
State prediction error at timestep 224 is tensor(0.0778, grad_fn=<MseLossBackward0>)
Current timestep = 225. State = [[-0.10345276  0.31357685]]. Action = [[ 0.24408162 -0.00055966 -0.07960401  0.51452816]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, False, True]
Current timestep = 226. State = [[-0.0945792   0.31365436]]. Action = [[-0.14413036  0.08864668 -0.1309307   0.5289302 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, False, True]
Current timestep = 227. State = [[-0.08782367  0.3100051 ]]. Action = [[ 0.19259417 -0.21434656 -0.21341625  0.08602011]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, False, True]
Current timestep = 228. State = [[-0.07826934  0.29979494]]. Action = [[ 0.07690215 -0.1414266  -0.227726   -0.70641214]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, False, True]
Scene graph at timestep 228 is [True, False, False, False, False, True]
State prediction error at timestep 228 is tensor(0.0698, grad_fn=<MseLossBackward0>)
Current timestep = 229. State = [[-0.07540564  0.29505783]]. Action = [[-0.2028042   0.20139301  0.13227218 -0.54819727]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, False, True]
Current timestep = 230. State = [[-0.08178534  0.29728344]]. Action = [[-0.05069827 -0.12104085  0.21157914  0.9498532 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, False, True]
Scene graph at timestep 230 is [True, False, False, False, False, True]
State prediction error at timestep 230 is tensor(0.0656, grad_fn=<MseLossBackward0>)
Current timestep = 231. State = [[-0.08211279  0.29627067]]. Action = [[ 0.22333002  0.08914188 -0.07450783 -0.5562528 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, False, True]
Scene graph at timestep 231 is [True, False, False, False, False, True]
State prediction error at timestep 231 is tensor(0.0709, grad_fn=<MseLossBackward0>)
Current timestep = 232. State = [[-0.07226433  0.2946984 ]]. Action = [[ 0.20445591 -0.21000135 -0.06569147  0.75736856]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, False, True]
Current timestep = 233. State = [[-0.06510706  0.29227772]]. Action = [[-0.20776093  0.24428481 -0.00255047  0.25492954]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, False, True]
Current timestep = 234. State = [[-0.06502438  0.29625845]]. Action = [[ 0.23407203 -0.12511943 -0.17534637  0.01931775]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, False, True]
Scene graph at timestep 234 is [True, False, False, False, False, True]
State prediction error at timestep 234 is tensor(0.0641, grad_fn=<MseLossBackward0>)
Current timestep = 235. State = [[-0.06039006  0.29591042]]. Action = [[-0.13189304  0.09068683 -0.10971878 -0.12875408]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, False, True]
Scene graph at timestep 235 is [True, False, False, False, False, True]
State prediction error at timestep 235 is tensor(0.0682, grad_fn=<MseLossBackward0>)
Current timestep = 236. State = [[-0.06040631  0.29404068]]. Action = [[ 0.12616518 -0.24618413  0.21612778 -0.9721306 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, False, True]
Current timestep = 237. State = [[-0.053923   0.2887756]]. Action = [[ 0.1558631   0.15834898 -0.16642994  0.4381032 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, False, True]
Scene graph at timestep 237 is [True, False, False, False, False, True]
State prediction error at timestep 237 is tensor(0.0655, grad_fn=<MseLossBackward0>)
Current timestep = 238. State = [[-0.04453143  0.29415846]]. Action = [[ 0.13382673  0.16866386 -0.21316151  0.43462968]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, False, True]
Current timestep = 239. State = [[-0.03515706  0.30408025]]. Action = [[ 0.10598162  0.14920929 -0.17376275 -0.61793566]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [False, True, False, False, False, True]
Current timestep = 240. State = [[-0.02716145  0.31035817]]. Action = [[ 0.08344057 -0.11720499 -0.19889396  0.7587408 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [False, True, False, False, False, True]
Current timestep = 241. State = [[-0.02128759  0.30885017]]. Action = [[ 0.01065865 -0.02331588  0.11452988  0.20876658]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [False, True, False, False, False, True]
Current timestep = 242. State = [[-0.02049892  0.30449656]]. Action = [[-0.09266159 -0.16566388  0.22805709  0.36524403]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [False, True, False, False, False, True]
Current timestep = 243. State = [[-0.02466634  0.30051348]]. Action = [[-0.10562342  0.14856115 -0.17018382 -0.5759227 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [False, True, False, False, False, True]
Current timestep = 244. State = [[-0.03087206  0.30516684]]. Action = [[-0.06522837  0.10756516 -0.09153476  0.02485895]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [False, True, False, False, False, True]
Scene graph at timestep 244 is [False, True, False, False, False, True]
State prediction error at timestep 244 is tensor(0.0706, grad_fn=<MseLossBackward0>)
Current timestep = 245. State = [[-0.03529834  0.31031668]]. Action = [[-0.0735945   0.20464015 -0.06401554  0.70086753]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [False, True, False, False, False, True]
Current timestep = 246. State = [[-0.03405378  0.3124039 ]]. Action = [[ 0.16232067  0.0087887  -0.17061064  0.01838219]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [False, True, False, False, False, True]
Scene graph at timestep 246 is [False, True, False, False, False, True]
State prediction error at timestep 246 is tensor(0.0716, grad_fn=<MseLossBackward0>)
Current timestep = 247. State = [[-0.02864769  0.31336567]]. Action = [[-0.24501383  0.20665208  0.13832492  0.6473994 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [False, True, False, False, False, True]
Current timestep = 248. State = [[-0.02635987  0.3137481 ]]. Action = [[-0.16684389  0.10406646 -0.14902791  0.14702559]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [False, True, False, False, False, True]
Current timestep = 249. State = [[-0.02719023  0.30964154]]. Action = [[-0.0837819  -0.24446994 -0.16581596 -0.7860285 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [False, True, False, False, False, True]
Current timestep = 250. State = [[-0.0327233   0.29703388]]. Action = [[-0.16087376 -0.22170264  0.00859144  0.39860976]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [False, True, False, False, False, True]
Current timestep = 251. State = [[-0.03584146  0.28750506]]. Action = [[ 0.21816403  0.11784059 -0.01532617 -0.43543458]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [False, True, False, False, False, True]
Current timestep = 252. State = [[-0.03357779  0.28668404]]. Action = [[-0.16015641 -0.03660324 -0.04082091  0.7022412 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [False, True, False, False, False, True]
Scene graph at timestep 252 is [False, True, False, False, False, True]
State prediction error at timestep 252 is tensor(0.0592, grad_fn=<MseLossBackward0>)
Current timestep = 253. State = [[-0.04120163  0.28515968]]. Action = [[-0.24579757 -0.03500755  0.05094171  0.02466536]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [False, True, False, False, False, True]
Current timestep = 254. State = [[-0.0495339   0.28247043]]. Action = [[ 0.13049084 -0.05805054  0.11064038 -0.7885103 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [False, True, False, False, False, True]
Current timestep = 255. State = [[-0.04990116  0.2835028 ]]. Action = [[-0.06627987  0.22305524  0.23126104 -0.8168889 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [False, True, False, False, False, True]
Current timestep = 256. State = [[-0.05283086  0.29215086]]. Action = [[-0.05675259  0.09552652  0.04346678 -0.8023792 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [False, True, False, False, False, True]
Scene graph at timestep 256 is [True, False, False, False, False, True]
State prediction error at timestep 256 is tensor(0.0701, grad_fn=<MseLossBackward0>)
Current timestep = 257. State = [[-0.05394618  0.30237752]]. Action = [[ 0.09580943  0.22550112  0.2285018  -0.8241176 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, False, True]
Scene graph at timestep 257 is [True, False, False, False, False, True]
State prediction error at timestep 257 is tensor(0.0750, grad_fn=<MseLossBackward0>)
Current timestep = 258. State = [[-0.05181744  0.31397843]]. Action = [[-0.00741103  0.06764382 -0.0429339  -0.7659553 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, False, True]
Current timestep = 259. State = [[-0.05097749  0.3204828 ]]. Action = [[-0.1414631   0.14259434  0.17767245  0.68668246]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, False, True]
Current timestep = 260. State = [[-0.0506478   0.32282373]]. Action = [[ 0.1674281   0.24226135 -0.01459828  0.94628024]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, False, True]
Current timestep = 261. State = [[-0.04814834  0.31977034]]. Action = [[ 0.14681676 -0.2141407   0.14839897  0.53379023]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, False, True]
Current timestep = 262. State = [[-0.04500344  0.31147644]]. Action = [[-0.13455038 -0.05033401  0.15135208  0.30620217]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [False, True, False, False, False, True]
Current timestep = 263. State = [[-0.0476949   0.30671924]]. Action = [[-0.2331721   0.23358351 -0.22470807 -0.0293349 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [False, True, False, False, False, True]
Scene graph at timestep 263 is [False, True, False, False, False, True]
State prediction error at timestep 263 is tensor(0.0788, grad_fn=<MseLossBackward0>)
Current timestep = 264. State = [[-0.0490131   0.30494556]]. Action = [[-0.14683115  0.14240545  0.05516675 -0.84896934]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [False, True, False, False, False, True]
Current timestep = 265. State = [[-0.0515699  0.3009867]]. Action = [[-0.11730245 -0.19366647  0.16625956  0.10845613]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [False, True, False, False, False, True]
Current timestep = 266. State = [[-0.05652251  0.28985193]]. Action = [[-0.03388691 -0.22867149 -0.15199074  0.563499  ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, False, True]
Current timestep = 267. State = [[-0.05713357  0.27479297]]. Action = [[ 0.14198232 -0.22215258  0.16073337 -0.20960104]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, False, True]
Current timestep = 268. State = [[-0.05619958  0.26341406]]. Action = [[-0.19415663  0.04800582 -0.18125214  0.76058364]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, False, True]
Current timestep = 269. State = [[-0.06142494  0.26152503]]. Action = [[0.00544444 0.05199674 0.24074498 0.89136255]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, False, True]
Scene graph at timestep 269 is [True, False, False, False, False, True]
State prediction error at timestep 269 is tensor(0.0545, grad_fn=<MseLossBackward0>)
Current timestep = 270. State = [[-0.06435509  0.26234007]]. Action = [[-0.04561377 -0.01057202 -0.0559461   0.37617683]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, False, True]
Current timestep = 271. State = [[-0.06507354  0.26207504]]. Action = [[ 0.09351668 -0.02025799 -0.24399182  0.11308277]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, False, True]
Current timestep = 272. State = [[-0.06417535  0.2648771 ]]. Action = [[-0.10475013  0.19848222  0.14973074  0.3330425 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, False, True]
Current timestep = 273. State = [[-0.06654957  0.26961654]]. Action = [[ 0.03401643 -0.11779597 -0.1482825   0.6510427 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, False, True]
Current timestep = 274. State = [[-0.06680752  0.27221012]]. Action = [[-0.0346882   0.22812301 -0.23656636 -0.71032465]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, False, True]
Scene graph at timestep 274 is [True, False, False, False, False, True]
State prediction error at timestep 274 is tensor(0.0722, grad_fn=<MseLossBackward0>)
Current timestep = 275. State = [[-0.06555203  0.28311333]]. Action = [[ 0.14904675  0.2025336   0.13415867 -0.0348435 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, False, True]
Scene graph at timestep 275 is [True, False, False, False, False, True]
State prediction error at timestep 275 is tensor(0.0628, grad_fn=<MseLossBackward0>)
Current timestep = 276. State = [[-0.06419966  0.29124394]]. Action = [[-0.19725594 -0.12320578 -0.01666395 -0.3679859 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, False, True]
Scene graph at timestep 276 is [True, False, False, False, False, True]
State prediction error at timestep 276 is tensor(0.0613, grad_fn=<MseLossBackward0>)
Current timestep = 277. State = [[-0.06830212  0.28674626]]. Action = [[ 0.051844   -0.23044468  0.23371124  0.75566316]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, False, True]
Current timestep = 278. State = [[-0.07059321  0.28174886]]. Action = [[-0.13019408  0.20826912 -0.05643457  0.65749335]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, False, True]
Current timestep = 279. State = [[-0.07789069  0.2849155 ]]. Action = [[-0.13500734 -0.04399964  0.05741155 -0.36770654]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, False, True]
Current timestep = 280. State = [[-0.0866827   0.28259537]]. Action = [[-0.11497733 -0.14535372 -0.06744662 -0.09917009]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, False, True]
Current timestep = 281. State = [[-0.09121884  0.27649873]]. Action = [[ 0.12718004 -0.04739977  0.00409278  0.49590337]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, False, True]
Scene graph at timestep 281 is [True, False, False, False, False, True]
State prediction error at timestep 281 is tensor(0.0570, grad_fn=<MseLossBackward0>)
Current timestep = 282. State = [[-0.08624113  0.2686467 ]]. Action = [[ 0.17700568 -0.21997114 -0.11565176  0.41281605]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, False, True]
Current timestep = 283. State = [[-0.07977273  0.25951108]]. Action = [[-0.07046781  0.0016174  -0.15857404  0.8113942 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, False, True]
Current timestep = 284. State = [[-0.08352704  0.2560567 ]]. Action = [[-0.24727397 -0.00294057 -0.02004679 -0.0380137 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, False, True]
Current timestep = 285. State = [[-0.09058788  0.254785  ]]. Action = [[ 0.12173414  0.00119254 -0.0334174  -0.27904463]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, False, True]
Current timestep = 286. State = [[-0.09308289  0.2508381 ]]. Action = [[-0.18390064 -0.19500923  0.0542081  -0.07798249]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, False, True]
Current timestep = 287. State = [[-0.10019638  0.24833415]]. Action = [[-0.07817903  0.24974644 -0.05386518 -0.5866436 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, False, True]
Scene graph at timestep 287 is [True, False, False, False, False, True]
State prediction error at timestep 287 is tensor(0.0585, grad_fn=<MseLossBackward0>)
Current timestep = 288. State = [[-0.10558101  0.2582376 ]]. Action = [[ 0.01028073  0.22316831  0.10373178 -0.09009129]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, False, True]
Current timestep = 289. State = [[-0.10907957  0.2671707 ]]. Action = [[-0.1008929  -0.08855754 -0.20721525 -0.2635529 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, False, True]
Current timestep = 290. State = [[-0.10895954  0.2649052 ]]. Action = [[ 0.24335366 -0.17864884 -0.16622287 -0.02795124]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, False, True]
Scene graph at timestep 290 is [True, False, False, False, False, True]
State prediction error at timestep 290 is tensor(0.0563, grad_fn=<MseLossBackward0>)
Current timestep = 291. State = [[-0.1039834   0.25919062]]. Action = [[-0.14284828  0.02666262  0.14307636 -0.7663967 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, False, True]
Current timestep = 292. State = [[-0.10887802  0.2544774 ]]. Action = [[-0.15636575 -0.1805887  -0.21037896  0.49005258]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, False, True]
Current timestep = 293. State = [[-0.11370669  0.24416271]]. Action = [[ 0.11829767 -0.18967219  0.0033237   0.20205629]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, False, True]
Current timestep = 294. State = [[-0.10998453  0.2365991 ]]. Action = [[0.11746955 0.11282527 0.12952775 0.29335082]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, False, True]
Current timestep = 295. State = [[-0.108258    0.23705484]]. Action = [[-0.1979075   0.00405869  0.04390317  0.7200632 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, False, True]
Current timestep = 296. State = [[-0.11715376  0.23814638]]. Action = [[-0.22503477  0.02290606  0.04141948  0.8843081 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, False, True]
Current timestep = 297. State = [[-0.12375003  0.24304394]]. Action = [[ 0.22907895  0.21820211  0.0883787  -0.98925376]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, False, True]
Scene graph at timestep 297 is [True, False, False, False, False, True]
State prediction error at timestep 297 is tensor(0.0589, grad_fn=<MseLossBackward0>)
Current timestep = 298. State = [[-0.12245288  0.25471303]]. Action = [[-0.16847706  0.2031959   0.20241317  0.87305737]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, False, True]
Scene graph at timestep 298 is [True, False, False, False, False, True]
State prediction error at timestep 298 is tensor(0.0629, grad_fn=<MseLossBackward0>)
Current timestep = 299. State = [[-0.12406021  0.26421672]]. Action = [[ 0.14989555 -0.05269396  0.04854783 -0.16872978]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, False, True]
Current timestep = 300. State = [[-0.11921538  0.26526496]]. Action = [[ 0.06016815 -0.06185862 -0.23609644  0.8243147 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, False, True]
Current timestep = 301. State = [[-0.11742626  0.2622352 ]]. Action = [[-0.12334317 -0.08496915 -0.04075986 -0.41738147]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, False, True]
Current timestep = 302. State = [[-0.12134041  0.25425577]]. Action = [[-0.04655159 -0.23836389  0.11342287 -0.6467637 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, False, True]
Current timestep = 303. State = [[-0.12417588  0.24854825]]. Action = [[0.00282103 0.23250079 0.20972288 0.01337445]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, False, True]
Current timestep = 304. State = [[-0.12161366  0.25444752]]. Action = [[ 0.22818995  0.09646788 -0.03094155 -0.5939901 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, False, True]
Scene graph at timestep 304 is [True, False, False, False, False, True]
State prediction error at timestep 304 is tensor(0.0579, grad_fn=<MseLossBackward0>)
Current timestep = 305. State = [[-0.11314087  0.25944132]]. Action = [[ 0.03143471 -0.02668905 -0.14678264 -0.13458979]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, False, True]
Scene graph at timestep 305 is [True, False, False, False, False, True]
State prediction error at timestep 305 is tensor(0.0576, grad_fn=<MseLossBackward0>)
Current timestep = 306. State = [[-0.10798556  0.25979367]]. Action = [[ 0.03555176 -0.04896116  0.07185972  0.42453074]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, False, True]
Current timestep = 307. State = [[-0.10597583  0.2612303 ]]. Action = [[-0.05673547  0.1568076   0.15336204 -0.7118628 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, False, True]
Current timestep = 308. State = [[-0.11044724  0.26566252]]. Action = [[-0.19176978 -0.03002229 -0.09161752 -0.57948595]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, False, True]
Current timestep = 309. State = [[-0.11873698  0.26563933]]. Action = [[-0.04748781 -0.05484791 -0.20752318 -0.24825007]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, False, True]
Current timestep = 310. State = [[-0.12370863  0.2680011 ]]. Action = [[-0.0192827   0.22745982  0.01754797  0.89527404]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, False, True]
Current timestep = 311. State = [[-0.12207025  0.27938306]]. Action = [[ 0.24598312  0.23363417  0.19990122 -0.2632689 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, False, True]
Scene graph at timestep 311 is [True, False, False, False, False, True]
State prediction error at timestep 311 is tensor(0.0658, grad_fn=<MseLossBackward0>)
Current timestep = 312. State = [[-0.11248315  0.29281196]]. Action = [[ 0.0868603   0.14239407 -0.12051478 -0.28462952]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, False, True]
Scene graph at timestep 312 is [True, False, False, False, False, True]
State prediction error at timestep 312 is tensor(0.0718, grad_fn=<MseLossBackward0>)
Current timestep = 313. State = [[-0.1075142   0.30452988]]. Action = [[-0.09736218  0.10388878  0.1806775  -0.3176986 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, False, True]
Scene graph at timestep 313 is [True, False, False, False, False, True]
State prediction error at timestep 313 is tensor(0.0709, grad_fn=<MseLossBackward0>)
Current timestep = 314. State = [[-0.10860471  0.31193733]]. Action = [[0.15163898 0.22656208 0.1745058  0.03274632]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, False, True]
Current timestep = 315. State = [[-0.10925502  0.31457067]]. Action = [[ 0.01717269  0.12485924 -0.08647761 -0.52691865]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, False, True]
Current timestep = 316. State = [[-0.10552253  0.31185284]]. Action = [[ 0.23792255 -0.19138168  0.05486271  0.9859054 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, False, True]
Current timestep = 317. State = [[-0.09695566  0.3053617 ]]. Action = [[-0.08270381  0.16747987 -0.05599672 -0.44100857]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, False, True]
Current timestep = 318. State = [[-0.09376785  0.29901454]]. Action = [[-0.00962296 -0.21153666  0.23658735  0.538484  ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, False, True]
Scene graph at timestep 318 is [True, False, False, False, False, True]
State prediction error at timestep 318 is tensor(0.0590, grad_fn=<MseLossBackward0>)
Current timestep = 319. State = [[-0.08942267  0.29195347]]. Action = [[ 0.1986165   0.07936335  0.09900945 -0.489398  ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, False, True]
Current timestep = 320. State = [[-0.0826848  0.289646 ]]. Action = [[-0.02845798 -0.10386884 -0.10968763 -0.07388496]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, False, True]
Current timestep = 321. State = [[-0.08230788  0.28490087]]. Action = [[-0.0990124  -0.06469022  0.11580673  0.7758266 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, False, True]
Current timestep = 322. State = [[-0.08120937  0.28217298]]. Action = [[0.2382291  0.06340122 0.09194005 0.23380637]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, False, True]
Current timestep = 323. State = [[-0.07565357  0.28146082]]. Action = [[-0.09507224 -0.08900395  0.19966567  0.73209715]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, False, True]
Current timestep = 324. State = [[-0.07943238  0.28064156]]. Action = [[-0.21417637  0.10341573 -0.2203437  -0.65301824]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, False, True]
Scene graph at timestep 324 is [True, False, False, False, False, True]
State prediction error at timestep 324 is tensor(0.0696, grad_fn=<MseLossBackward0>)
Current timestep = 325. State = [[-0.0874814  0.2801625]]. Action = [[ 0.01872355 -0.16982254 -0.15279731 -0.74373543]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, False, True]
Current timestep = 326. State = [[-0.08953737  0.27696553]]. Action = [[ 0.02318859  0.10557556 -0.21522062 -0.96729946]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, False, True]
Current timestep = 327. State = [[-0.090923    0.27467212]]. Action = [[-0.0694325  -0.22754604  0.21761382  0.63733697]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, False, True]
Current timestep = 328. State = [[-0.09734597  0.2648394 ]]. Action = [[-0.24432375 -0.1452278   0.18519777 -0.22927636]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, False, True]
Current timestep = 329. State = [[-0.11041434  0.25965804]]. Action = [[-0.2005955   0.16930556  0.18864614  0.21889639]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, False, True]
Current timestep = 330. State = [[-0.11867809  0.2596449 ]]. Action = [[ 0.20111847 -0.16314597  0.07994545  0.34611845]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, False, True]
Current timestep = 331. State = [[-0.11931334  0.25825447]]. Action = [[-0.23933971  0.17342329 -0.09801787 -0.01418412]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, False, True]
Current timestep = 332. State = [[-0.1266172   0.26331243]]. Action = [[-0.00810161  0.03671473  0.18093175 -0.9650372 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, False, True]
Current timestep = 333. State = [[-0.12764052  0.2704792 ]]. Action = [[ 0.12924582  0.2283318   0.12670863 -0.9666131 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, False, True]
Scene graph at timestep 333 is [True, False, False, False, False, True]
State prediction error at timestep 333 is tensor(0.0672, grad_fn=<MseLossBackward0>)
Current timestep = 334. State = [[-0.12488133  0.2763838 ]]. Action = [[-0.03061958 -0.19909368 -0.17821075 -0.23841894]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, False, True]
Current timestep = 335. State = [[-0.12497483  0.27290168]]. Action = [[-0.05106708 -0.00397305 -0.14977463 -0.24857134]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, False, True]
Current timestep = 336. State = [[-0.12823765  0.27258882]]. Action = [[-0.09272014  0.08565399  0.12230873  0.544919  ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, False, True]
Scene graph at timestep 336 is [True, False, False, False, False, True]
State prediction error at timestep 336 is tensor(0.0625, grad_fn=<MseLossBackward0>)
Current timestep = 337. State = [[-0.13345988  0.2712263 ]]. Action = [[-0.04950137 -0.21603273 -0.24416505 -0.21847332]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, False, True]
Current timestep = 338. State = [[-0.13930729  0.26860008]]. Action = [[-0.16546598  0.23410219  0.11023152 -0.73228985]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, False, True]
Current timestep = 339. State = [[-0.1492963  0.270434 ]]. Action = [[-0.13661401 -0.21665384  0.10210997 -0.43282878]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, False, True]
Current timestep = 340. State = [[-0.15455668  0.26650533]]. Action = [[ 0.14101002  0.08166957  0.11080635 -0.7109239 ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, False, True]
Current timestep = 341. State = [[-0.1494131   0.26789123]]. Action = [[ 0.18061775  0.04637587  0.22132954 -0.53498703]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, False, True]
Scene graph at timestep 341 is [True, False, False, False, False, True]
State prediction error at timestep 341 is tensor(0.0604, grad_fn=<MseLossBackward0>)
Current timestep = 342. State = [[-0.14162095  0.2703122 ]]. Action = [[0.0029926  0.01291969 0.2294766  0.68729067]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, False, True]
Scene graph at timestep 342 is [True, False, False, False, False, True]
State prediction error at timestep 342 is tensor(0.0610, grad_fn=<MseLossBackward0>)
Current timestep = 343. State = [[-0.13636926  0.27020147]]. Action = [[ 0.12324908 -0.08229987 -0.13664523 -0.6164951 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, False, True]
Scene graph at timestep 343 is [True, False, False, False, False, True]
State prediction error at timestep 343 is tensor(0.0609, grad_fn=<MseLossBackward0>)
Current timestep = 344. State = [[-0.12890811  0.26652393]]. Action = [[ 0.11365283 -0.0664736   0.04917908  0.604738  ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, False, True]
Current timestep = 345. State = [[-0.12183519  0.25932622]]. Action = [[ 0.05304357 -0.21711743  0.01088193 -0.15997213]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, False, True]
Current timestep = 346. State = [[-0.11832389  0.25379458]]. Action = [[-0.06937063  0.21108481 -0.15869917 -0.35764176]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, False, True]
Current timestep = 347. State = [[-0.11927246  0.25816792]]. Action = [[0.01284745 0.01944196 0.11819389 0.38699365]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, False, True]
Current timestep = 348. State = [[-0.1172158   0.26009524]]. Action = [[ 0.13355124 -0.03734936  0.19594842 -0.44285524]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, False, True]
Current timestep = 349. State = [[-0.11201803  0.25700992]]. Action = [[ 0.02317917 -0.15682155  0.20274532 -0.64070034]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, False, True]
Current timestep = 350. State = [[-0.1104963   0.25095117]]. Action = [[-0.0895699  -0.00805157 -0.13619357 -0.32054746]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, False, True]
Current timestep = 351. State = [[-0.11335871  0.24824563]]. Action = [[-3.59720737e-02 -3.32266092e-04  1.02556944e-01  5.37557602e-01]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, False, True]
Current timestep = 352. State = [[-0.11688574  0.24863812]]. Action = [[-0.072612    0.07750195  0.04819214 -0.9101254 ]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, False, True]
Current timestep = 353. State = [[-0.12224479  0.2469761 ]]. Action = [[-0.09469867 -0.24609025 -0.00074832 -0.48478913]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, False, True]
Scene graph at timestep 353 is [True, False, False, False, False, True]
State prediction error at timestep 353 is tensor(0.0476, grad_fn=<MseLossBackward0>)
Current timestep = 354. State = [[-0.13056569  0.23892513]]. Action = [[-0.22265866 -0.00817369 -0.23016062  0.7360873 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, False, True]
Current timestep = 355. State = [[-0.14191404  0.235958  ]]. Action = [[-0.08099765  0.03628013 -0.06126581 -0.24832118]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, False, True]
Scene graph at timestep 355 is [True, False, False, False, False, True]
State prediction error at timestep 355 is tensor(0.0534, grad_fn=<MseLossBackward0>)
Current timestep = 356. State = [[-0.14580268  0.23861198]]. Action = [[0.18267804 0.15306312 0.11147755 0.0860337 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, False, True]
Current timestep = 357. State = [[-0.13947682  0.24078871]]. Action = [[ 0.14349246 -0.21327515  0.12247166  0.9602027 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, False, True]
Current timestep = 358. State = [[-0.12976384  0.23285012]]. Action = [[ 0.1433684 -0.1500123  0.1905356  0.6758517]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, False, True]
Scene graph at timestep 358 is [True, False, False, False, False, True]
State prediction error at timestep 358 is tensor(0.0457, grad_fn=<MseLossBackward0>)
Current timestep = 359. State = [[-0.12061916  0.22675985]]. Action = [[0.05090523 0.10076836 0.03580984 0.12570786]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, False, True]
Current timestep = 360. State = [[-0.11341318  0.23138912]]. Action = [[ 0.13462365  0.23287162  0.00525573 -0.61892647]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, False, True]
Current timestep = 361. State = [[-0.11016835  0.24338433]]. Action = [[-0.2031908   0.17001104 -0.164652    0.8156121 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, False, True]
Current timestep = 362. State = [[-0.11585436  0.2536968 ]]. Action = [[-0.03897995  0.03425282 -0.1394416  -0.35039037]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, False, True]
Current timestep = 363. State = [[-0.11669794  0.25639135]]. Action = [[ 0.18064845 -0.12550405 -0.05125922  0.3761872 ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, False, True]
Current timestep = 364. State = [[-0.10953637  0.25201285]]. Action = [[ 0.10928142 -0.08886972  0.11869407 -0.97263044]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, False, True]
Current timestep = 365. State = [[-0.10447253  0.24472328]]. Action = [[-0.07885459 -0.16705854 -0.22025645 -0.27600598]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, False, True]
Current timestep = 366. State = [[-0.10291963  0.2396029 ]]. Action = [[ 0.11501312  0.17025846 -0.16781598  0.84440506]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, False, True]
Current timestep = 367. State = [[-0.10053188  0.2396374 ]]. Action = [[-0.07545272 -0.19695397 -0.16551189  0.6478708 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, False, True]
Current timestep = 368. State = [[-0.09771605  0.23726617]]. Action = [[ 0.23848069  0.20635632 -0.16398728 -0.5728567 ]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, False, True]
Scene graph at timestep 368 is [True, False, False, False, False, True]
State prediction error at timestep 368 is tensor(0.0575, grad_fn=<MseLossBackward0>)
Current timestep = 369. State = [[-0.08808012  0.23973346]]. Action = [[ 0.1024386  -0.17311867 -0.14773993  0.02762902]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, False, True]
Current timestep = 370. State = [[-0.07877384  0.23824432]]. Action = [[ 0.12162054  0.15833396  0.1017946  -0.6584465 ]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, False, True]
Current timestep = 371. State = [[-0.07534509  0.2427601 ]]. Action = [[-0.2306885   0.02579218  0.24652606  0.10843635]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, False, True]
Current timestep = 372. State = [[-0.08305359  0.24283028]]. Action = [[-0.10513338 -0.1524755  -0.1738475  -0.32867497]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, False, True]
Current timestep = 373. State = [[-0.08865433  0.2387483 ]]. Action = [[ 0.06044602  0.04359072 -0.12441617  0.37088263]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, False, False, True]
Current timestep = 374. State = [[-0.08767395  0.24049927]]. Action = [[ 0.07123291  0.12582636 -0.02907166  0.45122695]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, False, False, True]
Current timestep = 375. State = [[-0.08537652  0.24633807]]. Action = [[-0.03237647  0.06646734 -0.19660771 -0.46888477]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, False, False, True]
Scene graph at timestep 375 is [True, False, False, False, False, True]
State prediction error at timestep 375 is tensor(0.0543, grad_fn=<MseLossBackward0>)
Current timestep = 376. State = [[-0.08245341  0.2478036 ]]. Action = [[ 0.1948365  -0.17575721  0.1984961  -0.7294052 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, False, False, True]
Current timestep = 377. State = [[-0.07454129  0.24520099]]. Action = [[ 0.04070619  0.14198223  0.00413352 -0.41296673]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, False, False, True]
Current timestep = 378. State = [[-0.06920341  0.24488132]]. Action = [[ 0.06249788 -0.21649724 -0.2370868  -0.3394674 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, False, False, True]
Current timestep = 379. State = [[-0.06914744  0.23952448]]. Action = [[-0.24891637  0.08087739  0.12442592 -0.5708886 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, False, False, True]
Current timestep = 380. State = [[-0.07361497  0.23703848]]. Action = [[ 0.22872257 -0.16502248 -0.17737074 -0.7401458 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, False, False, True]
Current timestep = 381. State = [[-0.0681475   0.23388566]]. Action = [[ 0.01084745  0.16277838  0.22258627 -0.710396  ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, False, False, True]
Current timestep = 382. State = [[-0.0665914   0.23506658]]. Action = [[-0.08200383 -0.16086976  0.06939167  0.7218077 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, False, False, True]
Current timestep = 383. State = [[-0.06764187  0.22682492]]. Action = [[ 0.07652029 -0.24158217 -0.15613778 -0.47275913]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, False, False, True]
Current timestep = 384. State = [[-0.06231387  0.21244252]]. Action = [[ 0.22622702 -0.21811932 -0.08395199  0.09765232]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, False, False, True]
Current timestep = 385. State = [[-0.05553313  0.19620484]]. Action = [[-0.17512873 -0.24138223 -0.07343137  0.06793427]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, False, False, True]
Current timestep = 386. State = [[-0.0580385   0.18542685]]. Action = [[ 0.02392405  0.17742503 -0.18236668 -0.30797338]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, False, False, True]
Scene graph at timestep 386 is [True, False, False, False, False, True]
State prediction error at timestep 386 is tensor(0.0373, grad_fn=<MseLossBackward0>)
Current timestep = 387. State = [[-0.06204497  0.18989027]]. Action = [[-0.22826855  0.17682889 -0.10021573 -0.82999414]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, False, False, True]
Current timestep = 388. State = [[-0.07316168  0.19548395]]. Action = [[-0.14683391 -0.13267958 -0.11738974  0.54345584]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, False, False, True]
Current timestep = 389. State = [[-0.08453428  0.19579872]]. Action = [[-0.14410399  0.14523447  0.230968    0.0535357 ]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, False, False, True]
Current timestep = 390. State = [[-0.09341539  0.20034999]]. Action = [[-0.00436376  0.00788525 -0.02925782  0.12712479]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, False, False, True]
Current timestep = 391. State = [[-0.09663361  0.20129767]]. Action = [[ 0.00680155 -0.07619743  0.13877463 -0.9681096 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, False, False, True]
Current timestep = 392. State = [[-0.09936948  0.19839866]]. Action = [[-0.13603221 -0.04600768 -0.16513947 -0.70569754]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, False, False, True]
Current timestep = 393. State = [[-0.10188217  0.1945055 ]]. Action = [[ 0.19411168 -0.07861337 -0.06811731 -0.9625329 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, False, False, True]
Current timestep = 394. State = [[-0.09971062  0.18887922]]. Action = [[-0.2149664  -0.10418855  0.08010542  0.9566463 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, False, False, True]
Scene graph at timestep 394 is [True, False, False, False, False, True]
State prediction error at timestep 394 is tensor(0.0359, grad_fn=<MseLossBackward0>)
Current timestep = 395. State = [[-0.10611248  0.18252216]]. Action = [[-0.0307807  -0.06188348 -0.09164914  0.16604269]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, False, False, True]
Current timestep = 396. State = [[-0.11141779  0.17692073]]. Action = [[-0.10154295 -0.07955533  0.05182603  0.74991536]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, False, False, True]
Current timestep = 397. State = [[-0.11854681  0.16998406]]. Action = [[-0.13271114 -0.15138914  0.00269243  0.01375067]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, False, False, True]
Current timestep = 398. State = [[-0.12598966  0.16377649]]. Action = [[-0.02145794  0.07253754  0.12534273  0.7597914 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, False, False, True]
Current timestep = 399. State = [[-0.12875232  0.16263473]]. Action = [[ 0.03946671 -0.0701433   0.16483977  0.5673311 ]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, False, False, True]
Current timestep = 400. State = [[-0.12855966  0.15741915]]. Action = [[-0.0214601  -0.17681594 -0.01717755  0.90279615]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, False, False, True]
Current timestep = 401. State = [[-0.1296005   0.14994335]]. Action = [[-0.03051876  0.00969654 -0.17877291 -0.4020986 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, False, False, True]
Scene graph at timestep 401 is [True, False, False, False, False, True]
State prediction error at timestep 401 is tensor(0.0316, grad_fn=<MseLossBackward0>)
Current timestep = 402. State = [[-0.13358273  0.1462779 ]]. Action = [[-0.1797141  -0.06812574  0.2368921  -0.3986498 ]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, False, False, True]
Current timestep = 403. State = [[-0.13930207  0.14263709]]. Action = [[ 0.12693655 -0.02220267 -0.13935295 -0.8169536 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, False, False, True]
Current timestep = 404. State = [[-0.13499579  0.13702434]]. Action = [[ 0.15387756 -0.22228073 -0.02099338  0.85012555]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, False, False, True]
Current timestep = 405. State = [[-0.12567997  0.129736  ]]. Action = [[ 0.16062409  0.11864492 -0.14652179 -0.4720571 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, False, False, True]
Scene graph at timestep 405 is [True, False, False, False, False, True]
State prediction error at timestep 405 is tensor(0.0281, grad_fn=<MseLossBackward0>)
Current timestep = 406. State = [[-0.11592384  0.1300855 ]]. Action = [[ 0.04444024 -0.03668763  0.16816202  0.31221056]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, False, False, True]
Scene graph at timestep 406 is [True, False, False, False, False, True]
State prediction error at timestep 406 is tensor(0.0241, grad_fn=<MseLossBackward0>)
Current timestep = 407. State = [[-0.11344238  0.12999378]]. Action = [[-0.1751818   0.03859329  0.18841594 -0.5621452 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, False, False, True]
Current timestep = 408. State = [[-0.1190984   0.12897238]]. Action = [[-0.03189123 -0.14287917 -0.03860381  0.22244465]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, False, False, True]
Current timestep = 409. State = [[-0.12122534  0.12047221]]. Action = [[ 0.0998385  -0.22945124  0.00485605 -0.21691102]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, False, False, True]
Current timestep = 410. State = [[-0.12092046  0.10581605]]. Action = [[-0.16516352 -0.24781941 -0.03480011  0.8801396 ]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, False, True, False]
Current timestep = 411. State = [[-0.12773286  0.08973807]]. Action = [[-0.10200152 -0.1742281   0.24381262  0.9302002 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, False, True, False]
Current timestep = 412. State = [[-0.13349497  0.08195242]]. Action = [[ 0.04059905  0.23550892  0.22864729 -0.9161777 ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, False, True, False]
Current timestep = 413. State = [[-0.13285685  0.08507894]]. Action = [[ 0.06609264 -0.10250154  0.16600943 -0.70916575]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, False, True, False]
Scene graph at timestep 413 is [True, False, False, False, True, False]
State prediction error at timestep 413 is tensor(0.0150, grad_fn=<MseLossBackward0>)
Current timestep = 414. State = [[-0.13229223  0.08442361]]. Action = [[-0.14116438  0.06443331  0.07274228  0.9251492 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, False, True, False]
Scene graph at timestep 414 is [True, False, False, False, True, False]
State prediction error at timestep 414 is tensor(0.0225, grad_fn=<MseLossBackward0>)
Current timestep = 415. State = [[-0.13506432  0.08474627]]. Action = [[ 0.11702269 -0.08997047  0.12269658  0.98881316]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, False, True, False]
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(0.0193, grad_fn=<MseLossBackward0>)
Current timestep = 416. State = [[-0.13462432  0.08180431]]. Action = [[-0.1556667  -0.00815974  0.18672442 -0.4628172 ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, False, True, False]
Scene graph at timestep 416 is [True, False, False, False, True, False]
State prediction error at timestep 416 is tensor(0.0166, grad_fn=<MseLossBackward0>)
Current timestep = 417. State = [[-0.13664302  0.08052558]]. Action = [[ 0.20032644  0.0009011   0.03591508 -0.21147603]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, False, True, False]
Current timestep = 418. State = [[-0.133629    0.07840578]]. Action = [[-0.20415716 -0.09453255 -0.01780581 -0.70637006]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, False, True, False]
Current timestep = 419. State = [[-0.13918063  0.07870138]]. Action = [[ 0.01213333  0.245601   -0.07274039  0.3252722 ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, False, True, False]
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(0.0274, grad_fn=<MseLossBackward0>)
Current timestep = 420. State = [[-0.13959488  0.08900112]]. Action = [[ 0.10508773  0.15494996  0.16751337 -0.16977727]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, False, True, False]
Current timestep = 421. State = [[-0.13759755  0.09714984]]. Action = [[-0.12738976 -0.05455816 -0.07382244  0.7509217 ]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, False, True, False]
Current timestep = 422. State = [[-0.1389965   0.09878518]]. Action = [[ 0.15636319  0.01558101 -0.09015742 -0.98159015]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 422 is [True, False, False, False, True, False]
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(0.0238, grad_fn=<MseLossBackward0>)
Current timestep = 423. State = [[-0.13453937  0.10052146]]. Action = [[-0.00808617  0.06896672  0.07461068  0.7456614 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 423 is [True, False, False, False, True, False]
Current timestep = 424. State = [[-0.12969199  0.10642026]]. Action = [[0.23140824 0.18644017 0.08775014 0.90157545]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 424 is [True, False, False, False, True, False]
Current timestep = 425. State = [[-0.1198285   0.11124121]]. Action = [[-0.00242688 -0.2050744   0.09519666 -0.8076555 ]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 425 is [True, False, False, False, True, False]
Scene graph at timestep 425 is [True, False, False, False, True, False]
State prediction error at timestep 425 is tensor(0.0179, grad_fn=<MseLossBackward0>)
Current timestep = 426. State = [[-0.11398021  0.10682246]]. Action = [[ 0.17395669  0.00738421 -0.14558955  0.488878  ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 426 is [True, False, False, False, True, False]
Current timestep = 427. State = [[-0.1070236   0.10694727]]. Action = [[-0.05712351  0.13394368 -0.22933695  0.5688182 ]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 427 is [True, False, False, False, True, False]
Scene graph at timestep 427 is [True, False, False, False, True, False]
State prediction error at timestep 427 is tensor(0.0260, grad_fn=<MseLossBackward0>)
Current timestep = 428. State = [[-0.10820252  0.10947058]]. Action = [[-0.15509167 -0.12963893  0.0948441  -0.68762636]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 428 is [True, False, False, False, True, False]
Current timestep = 429. State = [[-0.11325348  0.11023119]]. Action = [[ 0.10866195  0.23302346  0.10088041 -0.13253349]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 429 is [True, False, False, False, True, False]
Current timestep = 430. State = [[-0.11147262  0.11828437]]. Action = [[-0.01928495  0.04566443  0.18790436 -0.68535745]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 430 is [True, False, False, False, True, False]
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(0.0214, grad_fn=<MseLossBackward0>)
Current timestep = 431. State = [[-0.10911118  0.12234306]]. Action = [[ 0.14192837 -0.05185299  0.18181825 -0.34598404]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 431 is [True, False, False, False, True, False]
Current timestep = 432. State = [[-0.10206933  0.12098906]]. Action = [[ 0.10484713 -0.06039947  0.12234759 -0.7631167 ]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 432 is [True, False, False, False, True, False]
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(0.0205, grad_fn=<MseLossBackward0>)
Current timestep = 433. State = [[-0.09651911  0.11458669]]. Action = [[-0.06716004 -0.23789637  0.04336601  0.866186  ]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 433 is [True, False, False, False, True, False]
Current timestep = 434. State = [[-0.10018049  0.1040785 ]]. Action = [[-0.21122716 -0.03249183 -0.04530647  0.6640625 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 434 is [True, False, False, False, True, False]
Current timestep = 435. State = [[-0.11139952  0.09573147]]. Action = [[-0.18218133 -0.21247989 -0.02945906 -0.01479512]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 435 is [True, False, False, False, True, False]
Scene graph at timestep 435 is [True, False, False, False, True, False]
State prediction error at timestep 435 is tensor(0.0172, grad_fn=<MseLossBackward0>)
Current timestep = 436. State = [[-0.12450904  0.08830646]]. Action = [[-0.14760782  0.1435127   0.07144693 -0.5950826 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 436 is [True, False, False, False, True, False]
Scene graph at timestep 436 is [True, False, False, False, True, False]
State prediction error at timestep 436 is tensor(0.0190, grad_fn=<MseLossBackward0>)
Current timestep = 437. State = [[-0.13727324  0.09007285]]. Action = [[-0.24135432  0.00660768  0.11792356  0.5140873 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 437 is [True, False, False, False, True, False]
Current timestep = 438. State = [[-0.15357168  0.09210431]]. Action = [[-0.24296084  0.04713529  0.13506544  0.5001869 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 438 is [True, False, False, False, True, False]
Current timestep = 439. State = [[-0.1653282   0.09193078]]. Action = [[ 0.16672134 -0.162823    0.16041815  0.8095763 ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 439 is [True, False, False, False, True, False]
Current timestep = 440. State = [[-0.1628478   0.08718039]]. Action = [[ 0.08356905  0.04535866 -0.04137643 -0.2590834 ]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 440 is [True, False, False, False, True, False]
Current timestep = 441. State = [[-0.16174114  0.09036599]]. Action = [[-0.23432253  0.24298966 -0.07279786  0.4045037 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 441 is [True, False, False, False, True, False]
Current timestep = 442. State = [[-0.17090034  0.10009085]]. Action = [[-0.13331445  0.02270925 -0.14265238 -0.8108663 ]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 442 is [True, False, False, False, True, False]
Current timestep = 443. State = [[-0.18195945  0.10725759]]. Action = [[-0.17030028  0.16813299  0.20133796 -0.4437071 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 443 is [True, False, False, False, True, False]
Current timestep = 444. State = [[-0.19208913  0.11902189]]. Action = [[-0.01781528  0.24882448 -0.16059689 -0.62653923]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 444 is [True, False, False, False, True, False]
Current timestep = 445. State = [[-0.19268017  0.13355279]]. Action = [[ 0.2425853   0.13633883  0.21823502 -0.4946915 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 445 is [True, False, False, False, True, False]
Current timestep = 446. State = [[-0.18498309  0.1460668 ]]. Action = [[-0.04554483  0.1853447   0.00446767  0.66037965]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 446 is [True, False, False, False, False, True]
Current timestep = 447. State = [[-0.18315983  0.15885708]]. Action = [[ 0.00617546  0.14103985 -0.04783675 -0.25697613]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 447 is [True, False, False, False, False, True]
Current timestep = 448. State = [[-0.18327233  0.16485728]]. Action = [[-0.08574274 -0.20266843  0.17045274 -0.5842132 ]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 448 is [True, False, False, False, False, True]
Current timestep = 449. State = [[-0.1846176   0.16259365]]. Action = [[ 0.1843074   0.13383171 -0.19538096  0.09421408]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 449 is [True, False, False, False, False, True]
Current timestep = 450. State = [[-0.17742777  0.16263242]]. Action = [[ 0.04803711 -0.1806341   0.16375673 -0.8079358 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 450 is [True, False, False, False, False, True]
Current timestep = 451. State = [[-0.17694297  0.15923157]]. Action = [[-0.24195147  0.13638946 -0.0764208  -0.043814  ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 451 is [True, False, False, False, False, True]
Current timestep = 452. State = [[-0.1864022   0.16150096]]. Action = [[-0.11817312 -0.04910173 -0.09910917  0.09428   ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 452 is [True, False, False, False, False, True]
Current timestep = 453. State = [[-0.19236557  0.16204154]]. Action = [[ 0.16199023  0.06500524 -0.03648323  0.7845838 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 453 is [True, False, False, False, False, True]
Current timestep = 454. State = [[-0.18755607  0.16238172]]. Action = [[ 0.0627746  -0.10686931 -0.23287979 -0.248559  ]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 454 is [True, False, False, False, False, True]
Scene graph at timestep 454 is [True, False, False, False, False, True]
State prediction error at timestep 454 is tensor(0.0433, grad_fn=<MseLossBackward0>)
Current timestep = 455. State = [[-0.1810732   0.15763427]]. Action = [[ 0.16276848 -0.10310346  0.0623056  -0.12591517]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 455 is [True, False, False, False, False, True]
Current timestep = 456. State = [[-0.1766741   0.15093397]]. Action = [[-0.24391    -0.08484645  0.01959884 -0.76892203]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 456 is [True, False, False, False, False, True]
Current timestep = 457. State = [[-0.18412416  0.14624734]]. Action = [[-0.03895494  0.01946715 -0.20645802  0.6488712 ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 457 is [True, False, False, False, False, True]
Current timestep = 458. State = [[-0.1902554   0.14374393]]. Action = [[-0.13026607 -0.08395931  0.20529595 -0.6983514 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 458 is [True, False, False, False, False, True]
Current timestep = 459. State = [[-0.20022099  0.14076167]]. Action = [[-0.22454587  0.03126729  0.21581826 -0.62494725]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 459 is [True, False, False, False, False, True]
Current timestep = 460. State = [[-0.2086818   0.13789345]]. Action = [[ 0.1756208  -0.18159893 -0.20362978  0.6815803 ]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 460 is [True, False, False, False, False, True]
Scene graph at timestep 460 is [True, False, False, False, False, True]
State prediction error at timestep 460 is tensor(0.0436, grad_fn=<MseLossBackward0>)
Current timestep = 461. State = [[-0.20377268  0.13389812]]. Action = [[0.20156771 0.18728667 0.10493171 0.7849951 ]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 461 is [True, False, False, False, False, True]
Current timestep = 462. State = [[-0.19233169  0.13587935]]. Action = [[ 0.0872153 -0.1474008  0.218436  -0.9389544]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 462 is [True, False, False, False, False, True]
Current timestep = 463. State = [[-0.18448074  0.13392632]]. Action = [[ 0.08508813  0.09982932 -0.10306492  0.9115062 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 463 is [True, False, False, False, False, True]
Current timestep = 464. State = [[-0.17929573  0.13408896]]. Action = [[-0.06755148 -0.12809858 -0.20226677 -0.50400436]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 464 is [True, False, False, False, False, True]
Current timestep = 465. State = [[-0.17880122  0.13197955]]. Action = [[ 0.11539328  0.10330862 -0.24787587  0.290493  ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 465 is [True, False, False, False, False, True]
Scene graph at timestep 465 is [True, False, False, False, False, True]
State prediction error at timestep 465 is tensor(0.0433, grad_fn=<MseLossBackward0>)
Current timestep = 466. State = [[-0.17396604  0.13401018]]. Action = [[ 0.04056418 -0.01214768 -0.21841021 -0.45286238]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 466 is [True, False, False, False, False, True]
Scene graph at timestep 466 is [True, False, False, False, False, True]
State prediction error at timestep 466 is tensor(0.0345, grad_fn=<MseLossBackward0>)
Current timestep = 467. State = [[-0.16843364  0.1324859 ]]. Action = [[ 0.14638942 -0.13852952 -0.1340348   0.9220178 ]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 467 is [True, False, False, False, False, True]
Current timestep = 468. State = [[-0.16499652  0.12867504]]. Action = [[-2.1429318e-01  9.1437519e-02 -2.4491310e-02  3.2782555e-05]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 468 is [True, False, False, False, False, True]
Current timestep = 469. State = [[-0.1742116   0.13034579]]. Action = [[-0.24736682  0.00972143 -0.07826209 -0.03090715]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 469 is [True, False, False, False, False, True]
Scene graph at timestep 469 is [True, False, False, False, False, True]
State prediction error at timestep 469 is tensor(0.0357, grad_fn=<MseLossBackward0>)
Current timestep = 470. State = [[-0.18438981  0.13023399]]. Action = [[ 0.14793366 -0.08892524 -0.15738969  0.50279486]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 470 is [True, False, False, False, False, True]
Current timestep = 471. State = [[-0.18107489  0.12825198]]. Action = [[ 0.16928887  0.07225603 -0.08331299  0.06915569]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 471 is [True, False, False, False, False, True]
Current timestep = 472. State = [[-0.17029876  0.12802087]]. Action = [[ 0.19418341 -0.10820723 -0.10971439  0.86710835]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 472 is [True, False, False, False, False, True]
Current timestep = 473. State = [[-0.1623425   0.12146631]]. Action = [[-0.20004244 -0.1937932  -0.01084034 -0.4200771 ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 473 is [True, False, False, False, False, True]
Scene graph at timestep 473 is [True, False, False, False, True, False]
State prediction error at timestep 473 is tensor(0.0242, grad_fn=<MseLossBackward0>)
Current timestep = 474. State = [[-0.16752245  0.11274677]]. Action = [[-0.05063462 -0.01138559 -0.23105957  0.59956586]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 474 is [True, False, False, False, True, False]
Current timestep = 475. State = [[-0.17263459  0.10591965]]. Action = [[-0.09686744 -0.19204725  0.17605042  0.22140479]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 475 is [True, False, False, False, True, False]
Current timestep = 476. State = [[-0.17918849  0.10085853]]. Action = [[-0.0464218   0.21445626  0.10505283  0.17878878]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 476 is [True, False, False, False, True, False]
Current timestep = 477. State = [[-0.18459602  0.10710174]]. Action = [[-0.14448692  0.09155661  0.01225483  0.28349924]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 477 is [True, False, False, False, True, False]
Current timestep = 478. State = [[-0.19015086  0.10914463]]. Action = [[ 0.05441555 -0.2397646   0.16713846 -0.93396616]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 478 is [True, False, False, False, True, False]
Current timestep = 479. State = [[-0.18882893  0.10368513]]. Action = [[ 0.17676306  0.09630769  0.09337574 -0.90368253]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 479 is [True, False, False, False, True, False]
Current timestep = 480. State = [[-0.1803107   0.10499851]]. Action = [[ 0.09238619  0.04907864 -0.0660819  -0.05145156]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 480 is [True, False, False, False, True, False]
Current timestep = 481. State = [[-0.17468551  0.10717863]]. Action = [[-0.07462344 -0.01106577 -0.16599704 -0.39157444]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 481 is [True, False, False, False, True, False]
Scene graph at timestep 481 is [True, False, False, False, True, False]
State prediction error at timestep 481 is tensor(0.0298, grad_fn=<MseLossBackward0>)
Current timestep = 482. State = [[-0.17808883  0.1049778 ]]. Action = [[-0.2100802  -0.17320034  0.17348045 -0.65935105]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 482 is [True, False, False, False, True, False]
Current timestep = 483. State = [[-0.18434201  0.10179334]]. Action = [[ 0.24261522  0.19293755 -0.12320736 -0.646887  ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 483 is [True, False, False, False, True, False]
Current timestep = 484. State = [[-0.17941985  0.10893054]]. Action = [[-0.09868596  0.1732691  -0.02579735 -0.07366562]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 484 is [True, False, False, False, True, False]
Current timestep = 485. State = [[-0.18322018  0.12002958]]. Action = [[-0.20529999  0.1505124  -0.05472447 -0.70521474]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 485 is [True, False, False, False, True, False]
Current timestep = 486. State = [[-0.19260252  0.12834042]]. Action = [[-0.06315516 -0.05081935 -0.21406282 -0.6612668 ]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 486 is [True, False, False, False, True, False]
Current timestep = 487. State = [[-0.19600527  0.13197617]]. Action = [[ 0.20945865  0.14631575 -0.180806    0.6093693 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 487 is [True, False, False, False, False, True]
Current timestep = 488. State = [[-0.18922919  0.14061964]]. Action = [[ 0.03802082  0.19752127  0.23060888 -0.1810801 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 488 is [True, False, False, False, False, True]
Current timestep = 489. State = [[-0.1815779   0.14871112]]. Action = [[ 0.1951774  -0.11257923 -0.24902055 -0.40708613]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 489 is [True, False, False, False, False, True]
Current timestep = 490. State = [[-0.17093651  0.1498478 ]]. Action = [[ 0.14756972  0.11423475  0.01739287 -0.23237932]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 490 is [True, False, False, False, False, True]
Scene graph at timestep 490 is [True, False, False, False, False, True]
State prediction error at timestep 490 is tensor(0.0366, grad_fn=<MseLossBackward0>)
Current timestep = 491. State = [[-0.15908068  0.1531156 ]]. Action = [[ 0.17007661 -0.03119515 -0.15369466 -0.24252224]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 491 is [True, False, False, False, False, True]
Scene graph at timestep 491 is [True, False, False, False, False, True]
State prediction error at timestep 491 is tensor(0.0355, grad_fn=<MseLossBackward0>)
Current timestep = 492. State = [[-0.14915584  0.15533979]]. Action = [[ 0.016774    0.12405986  0.12829551 -0.35204238]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 492 is [True, False, False, False, False, True]
Current timestep = 493. State = [[-0.14469582  0.16247271]]. Action = [[ 0.01919547  0.15387732 -0.14717856 -0.71714187]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 493 is [True, False, False, False, False, True]
Scene graph at timestep 493 is [True, False, False, False, False, True]
State prediction error at timestep 493 is tensor(0.0396, grad_fn=<MseLossBackward0>)
Current timestep = 494. State = [[-0.14226645  0.16718617]]. Action = [[-0.01882751 -0.1945853   0.16711682 -0.20175159]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 494 is [True, False, False, False, False, True]
Current timestep = 495. State = [[-0.1421684   0.16343398]]. Action = [[0.054216   0.04818913 0.20721889 0.06876302]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 495 is [True, False, False, False, False, True]
Current timestep = 496. State = [[-0.13764064  0.16041774]]. Action = [[ 0.1532036  -0.17948508 -0.11086451 -0.14420271]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 496 is [True, False, False, False, False, True]
Current timestep = 497. State = [[-0.12801528  0.15294667]]. Action = [[ 0.21972027 -0.04684529  0.00900519 -0.15000159]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 497 is [True, False, False, False, False, True]
Scene graph at timestep 497 is [True, False, False, False, False, True]
State prediction error at timestep 497 is tensor(0.0292, grad_fn=<MseLossBackward0>)
Current timestep = 498. State = [[-0.11316813  0.14542072]]. Action = [[ 0.24150032 -0.19868612 -0.01153383 -0.6509905 ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 498 is [True, False, False, False, False, True]
Current timestep = 499. State = [[-0.09954018  0.1395517 ]]. Action = [[ 0.04589212  0.20889896  0.16493988 -0.0060041 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 499 is [True, False, False, False, False, True]
Scene graph at timestep 499 is [True, False, False, False, False, True]
State prediction error at timestep 499 is tensor(0.0278, grad_fn=<MseLossBackward0>)
Current timestep = 500. State = [[-0.09195856  0.14347164]]. Action = [[ 0.02223107 -0.03900665  0.10535368 -0.44918495]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 500 is [True, False, False, False, False, True]
Current timestep = 501. State = [[-0.08793182  0.14522278]]. Action = [[ 0.08185288  0.06973344 -0.09892207  0.17022693]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 501 is [True, False, False, False, False, True]
Scene graph at timestep 501 is [True, False, False, False, False, True]
State prediction error at timestep 501 is tensor(0.0274, grad_fn=<MseLossBackward0>)
Current timestep = 502. State = [[-0.08010056  0.14569588]]. Action = [[ 0.24194854 -0.14707738  0.23148847  0.52146196]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 502 is [True, False, False, False, False, True]
Current timestep = 503. State = [[-0.06784719  0.14373684]]. Action = [[ 0.12516835  0.17008898  0.08257702 -0.11701512]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 503 is [True, False, False, False, False, True]
Current timestep = 504. State = [[-0.06063767  0.1506827 ]]. Action = [[-0.14216511  0.15857524  0.12417001  0.72881794]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 504 is [True, False, False, False, False, True]
Scene graph at timestep 504 is [True, False, False, False, False, True]
State prediction error at timestep 504 is tensor(0.0267, grad_fn=<MseLossBackward0>)
Current timestep = 505. State = [[-0.064473    0.15745918]]. Action = [[-0.13052323 -0.08570004 -0.24482742  0.7778163 ]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 505 is [True, False, False, False, False, True]
Current timestep = 506. State = [[-0.07199834  0.1564134 ]]. Action = [[-0.06290603 -0.0523534  -0.02305762  0.774933  ]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 506 is [True, False, False, False, False, True]
Scene graph at timestep 506 is [True, False, False, False, False, True]
State prediction error at timestep 506 is tensor(0.0260, grad_fn=<MseLossBackward0>)
Current timestep = 507. State = [[-0.07801508  0.15690055]]. Action = [[-0.04581495  0.18805265 -0.21451959  0.9059336 ]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 507 is [True, False, False, False, False, True]
Current timestep = 508. State = [[-0.08175368  0.16102321]]. Action = [[-0.06144807 -0.13408285  0.1531328  -0.37792307]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 508 is [True, False, False, False, False, True]
Scene graph at timestep 508 is [True, False, False, False, False, True]
State prediction error at timestep 508 is tensor(0.0256, grad_fn=<MseLossBackward0>)
Current timestep = 509. State = [[-0.08260671  0.16033158]]. Action = [[ 0.22552568  0.11342824 -0.07478799 -0.7929649 ]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 509 is [True, False, False, False, False, True]
Current timestep = 510. State = [[-0.07116304  0.15990306]]. Action = [[ 0.2378596  -0.22268087 -0.0039973   0.21174324]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 510 is [True, False, False, False, False, True]
Scene graph at timestep 510 is [True, False, False, False, False, True]
State prediction error at timestep 510 is tensor(0.0236, grad_fn=<MseLossBackward0>)
Current timestep = 511. State = [[-0.05645338  0.15193179]]. Action = [[ 0.1785301  -0.05539821 -0.0166025  -0.3633207 ]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 511 is [True, False, False, False, False, True]
Current timestep = 512. State = [[-0.04221547  0.14491904]]. Action = [[ 0.18280542 -0.13161212 -0.01811567  0.7472129 ]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 512 is [True, False, False, False, False, True]
Current timestep = 513. State = [[-0.02988314  0.1355465 ]]. Action = [[ 0.07058221 -0.16850795  0.19575632  0.34089684]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 513 is [False, True, False, False, False, True]
Current timestep = 514. State = [[-0.02612954  0.12707467]]. Action = [[-0.19868205  0.03488263 -0.05384767 -0.17455852]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 514 is [False, True, False, False, False, True]
Current timestep = 515. State = [[-0.03359625  0.12662773]]. Action = [[-0.12610453  0.09480616 -0.17506039 -0.8972438 ]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 515 is [False, True, False, False, False, True]
Current timestep = 516. State = [[-0.04217575  0.13026486]]. Action = [[-0.08797374  0.03380442  0.18540049 -0.97962   ]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 516 is [False, True, False, False, False, True]
Current timestep = 517. State = [[-0.04680521  0.13495755]]. Action = [[ 0.11669856  0.1320824  -0.04591486 -0.28167975]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 517 is [False, True, False, False, False, True]
Scene graph at timestep 517 is [False, True, False, False, False, True]
State prediction error at timestep 517 is tensor(0.0226, grad_fn=<MseLossBackward0>)
Current timestep = 518. State = [[-0.04678619  0.13801548]]. Action = [[-0.21001357 -0.17042549 -0.18052405  0.69756424]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 518 is [False, True, False, False, False, True]
Scene graph at timestep 518 is [False, True, False, False, False, True]
State prediction error at timestep 518 is tensor(0.0212, grad_fn=<MseLossBackward0>)
Current timestep = 519. State = [[-0.05723648  0.1356466 ]]. Action = [[-0.21026182  0.11435351  0.10572684 -0.5958358 ]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 519 is [False, True, False, False, False, True]
Current timestep = 520. State = [[-0.06614155  0.14161189]]. Action = [[ 0.18198892  0.21296823 -0.21728365  0.5744257 ]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 520 is [True, False, False, False, False, True]
Current timestep = 521. State = [[-0.06010498  0.14922237]]. Action = [[ 0.17249465 -0.0972231   0.06593147 -0.8461533 ]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 521 is [True, False, False, False, False, True]
Current timestep = 522. State = [[-0.0490459   0.15024662]]. Action = [[ 0.1980896   0.07211959 -0.12764546  0.8008146 ]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 522 is [True, False, False, False, False, True]
Current timestep = 523. State = [[-0.04131119  0.1527046 ]]. Action = [[-0.23837489  0.01616195  0.08610138 -0.17469162]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 523 is [False, True, False, False, False, True]
Current timestep = 524. State = [[-0.04521494  0.15188654]]. Action = [[ 0.09139162 -0.16622569 -0.02575503 -0.7645536 ]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 524 is [False, True, False, False, False, True]
Current timestep = 525. State = [[-0.04097285  0.1435832 ]]. Action = [[ 0.23805493 -0.16336086  0.14325732  0.04693067]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 525 is [False, True, False, False, False, True]
Current timestep = 526. State = [[-0.03243421  0.13353933]]. Action = [[-0.09763256 -0.0859997  -0.1786597  -0.47537816]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 526 is [False, True, False, False, False, True]
Current timestep = 527. State = [[-0.03457033  0.12902164]]. Action = [[-0.14333826  0.12155938  0.14068815  0.57581973]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 527 is [False, True, False, False, False, True]
Scene graph at timestep 527 is [False, True, False, False, False, True]
State prediction error at timestep 527 is tensor(0.0192, grad_fn=<MseLossBackward0>)
Current timestep = 528. State = [[-0.03982701  0.13040736]]. Action = [[ 0.04105204 -0.06818169  0.17867416  0.18196094]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 528 is [False, True, False, False, False, True]
Scene graph at timestep 528 is [False, True, False, False, False, True]
State prediction error at timestep 528 is tensor(0.0174, grad_fn=<MseLossBackward0>)
Current timestep = 529. State = [[-0.03759529  0.13157822]]. Action = [[0.22933465 0.16353047 0.06065506 0.7012117 ]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 529 is [False, True, False, False, False, True]
Scene graph at timestep 529 is [False, True, False, False, False, True]
State prediction error at timestep 529 is tensor(0.0202, grad_fn=<MseLossBackward0>)
Current timestep = 530. State = [[-0.03126177  0.14074607]]. Action = [[-0.18652782  0.24641186  0.15717071 -0.6181594 ]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 530 is [False, True, False, False, False, True]
Scene graph at timestep 530 is [False, True, False, False, False, True]
State prediction error at timestep 530 is tensor(0.0272, grad_fn=<MseLossBackward0>)
Current timestep = 531. State = [[-0.03738936  0.15005653]]. Action = [[-0.2017196  -0.14954865  0.13919604 -0.479398  ]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 531 is [False, True, False, False, False, True]
Current timestep = 532. State = [[-0.04600244  0.15049723]]. Action = [[0.11464921 0.1015709  0.03831959 0.1331358 ]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 532 is [False, True, False, False, False, True]
Current timestep = 533. State = [[-0.04526788  0.15164322]]. Action = [[-0.02400817 -0.10724506 -0.06793514  0.34462285]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 533 is [False, True, False, False, False, True]
Scene graph at timestep 533 is [False, True, False, False, False, True]
State prediction error at timestep 533 is tensor(0.0221, grad_fn=<MseLossBackward0>)
Current timestep = 534. State = [[-0.04861413  0.14722687]]. Action = [[-0.22576392 -0.09523493  0.11853978 -0.5598222 ]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 534 is [False, True, False, False, False, True]
Current timestep = 535. State = [[-0.05662588  0.1391093 ]]. Action = [[ 0.07431227 -0.22029541 -0.04019919  0.572804  ]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 535 is [False, True, False, False, False, True]
Current timestep = 536. State = [[-0.05460383  0.12870698]]. Action = [[ 0.20732218 -0.01823473  0.14406353  0.77230334]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 536 is [True, False, False, False, False, True]
Scene graph at timestep 536 is [True, False, False, False, False, True]
State prediction error at timestep 536 is tensor(0.0182, grad_fn=<MseLossBackward0>)
Current timestep = 537. State = [[-0.04391153  0.12656723]]. Action = [[ 0.17735213  0.15619886 -0.12443236 -0.9806415 ]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 537 is [True, False, False, False, False, True]
Current timestep = 538. State = [[-0.0320325   0.12713166]]. Action = [[ 0.08237797 -0.24776885  0.2297352  -0.5560324 ]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 538 is [False, True, False, False, False, True]
Current timestep = 539. State = [[-0.16258673  0.05423515]]. Action = [[-0.02100278 -0.18018982 -0.1365863  -0.2840904 ]]. Reward = [1.]
Curr episode timestep = 539
Scene graph at timestep 539 is [False, True, False, False, False, True]
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(0.0167, grad_fn=<MseLossBackward0>)
Current timestep = 540. State = [[-0.1587985   0.05807319]]. Action = [[ 0.1786348   0.18855733 -0.10191348 -0.42179275]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 540 is [True, False, False, False, True, False]
Current timestep = 541. State = [[-0.15356144  0.06737834]]. Action = [[-0.15800156  0.12579215 -0.1145232  -0.83937025]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 541 is [True, False, False, False, True, False]
Current timestep = 542. State = [[-0.15611354  0.07177728]]. Action = [[ 0.04239613 -0.2118715  -0.24594831  0.9404931 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 542 is [True, False, False, False, True, False]
Scene graph at timestep 542 is [True, False, False, False, True, False]
State prediction error at timestep 542 is tensor(0.0232, grad_fn=<MseLossBackward0>)
Current timestep = 543. State = [[-0.1535912   0.06587689]]. Action = [[ 0.18419802 -0.04475266  0.18721068 -0.4721111 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 543 is [True, False, False, False, True, False]
Scene graph at timestep 543 is [True, False, False, False, True, False]
State prediction error at timestep 543 is tensor(0.0152, grad_fn=<MseLossBackward0>)
Current timestep = 544. State = [[-0.14753611  0.06466774]]. Action = [[-0.08926003  0.18714717 -0.13211553  0.67991495]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 544 is [True, False, False, False, True, False]
Current timestep = 545. State = [[-0.14829643  0.06746246]]. Action = [[-0.02751818 -0.18567309 -0.22950725  0.47194982]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 545 is [True, False, False, False, True, False]
Current timestep = 546. State = [[-0.15163156  0.0655635 ]]. Action = [[-0.10688855  0.17343944 -0.16105685 -0.33416986]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 546 is [True, False, False, False, True, False]
Current timestep = 547. State = [[-0.1579929   0.06636953]]. Action = [[-0.12083858 -0.23448345  0.18883306  0.6217084 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 547 is [True, False, False, False, True, False]
Current timestep = 548. State = [[-0.16179787  0.06251112]]. Action = [[ 0.20880455  0.18555042 -0.05337209  0.40902376]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 548 is [True, False, False, False, True, False]
Current timestep = 549. State = [[-0.15695086  0.06899962]]. Action = [[-0.06999183  0.15804946 -0.05642207 -0.9365571 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 549 is [True, False, False, False, True, False]
Scene graph at timestep 549 is [True, False, False, False, True, False]
State prediction error at timestep 549 is tensor(0.0207, grad_fn=<MseLossBackward0>)
Current timestep = 550. State = [[-0.15900157  0.07749504]]. Action = [[-0.13782454  0.04142034 -0.09000301 -0.13116324]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 550 is [True, False, False, False, True, False]
Current timestep = 551. State = [[-0.1659359   0.08430396]]. Action = [[-0.07865392  0.14416191  0.04802203 -0.41768026]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 551 is [True, False, False, False, True, False]
Current timestep = 552. State = [[-0.17239505  0.09271292]]. Action = [[-0.07892999  0.09243715  0.21339324 -0.23048872]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 552 is [True, False, False, False, True, False]
Scene graph at timestep 552 is [True, False, False, False, True, False]
State prediction error at timestep 552 is tensor(0.0268, grad_fn=<MseLossBackward0>)
Current timestep = 553. State = [[-0.17711085  0.09721273]]. Action = [[ 0.01053554 -0.10245159  0.16419423 -0.13289559]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 553 is [True, False, False, False, True, False]
Current timestep = 554. State = [[-0.17603531  0.09931802]]. Action = [[ 0.18084103  0.23736876 -0.08786491 -0.6156328 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 554 is [True, False, False, False, True, False]
Current timestep = 555. State = [[-0.16947225  0.10791095]]. Action = [[-0.02257316  0.03447723  0.09040177  0.25241554]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 555 is [True, False, False, False, True, False]
Current timestep = 556. State = [[-0.16978133  0.10899722]]. Action = [[-0.16129948 -0.22028895 -0.01457649 -0.21468842]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 556 is [True, False, False, False, True, False]
Scene graph at timestep 556 is [True, False, False, False, True, False]
State prediction error at timestep 556 is tensor(0.0248, grad_fn=<MseLossBackward0>)
Current timestep = 557. State = [[-0.17479528  0.10466728]]. Action = [[ 0.07921985  0.15136033  0.15375474 -0.7844381 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 557 is [True, False, False, False, True, False]
Current timestep = 558. State = [[-0.17192297  0.10539646]]. Action = [[ 0.1400348  -0.1341649  -0.11740103 -0.17573798]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 558 is [True, False, False, False, True, False]
Current timestep = 559. State = [[-0.16400903  0.09809035]]. Action = [[ 0.12940493 -0.22598855  0.2238755  -0.8369373 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 559 is [True, False, False, False, True, False]
Current timestep = 560. State = [[-0.15816197  0.08970411]]. Action = [[-0.09594466  0.1131652   0.01615497  0.9375632 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 560 is [True, False, False, False, True, False]
Current timestep = 561. State = [[-0.15963249  0.09063143]]. Action = [[-0.03549595  0.02889591 -0.17950073 -0.05836856]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 561 is [True, False, False, False, True, False]
Current timestep = 562. State = [[-0.1626392   0.09208387]]. Action = [[-0.06373809 -0.01006353 -0.18389715 -0.59274405]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 562 is [True, False, False, False, True, False]
Current timestep = 563. State = [[-0.1626334   0.09245197]]. Action = [[ 0.23186335  0.00451005 -0.05632952 -0.06339782]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 563 is [True, False, False, False, True, False]
Current timestep = 564. State = [[-0.15215929  0.09505288]]. Action = [[ 0.18733478  0.15535033 -0.12618428  0.3664478 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 564 is [True, False, False, False, True, False]
Current timestep = 565. State = [[-0.14070809  0.10343825]]. Action = [[ 0.05882326  0.15602076  0.19833595 -0.92032325]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 565 is [True, False, False, False, True, False]
Current timestep = 566. State = [[-0.13448898  0.11139669]]. Action = [[-0.00132343 -0.02032606  0.13755494 -0.08056515]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 566 is [True, False, False, False, True, False]
Scene graph at timestep 566 is [True, False, False, False, True, False]
State prediction error at timestep 566 is tensor(0.0240, grad_fn=<MseLossBackward0>)
Current timestep = 567. State = [[-0.13084628  0.11486049]]. Action = [[ 0.12188232  0.07244617 -0.00852616  0.5677208 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 567 is [True, False, False, False, True, False]
Current timestep = 568. State = [[-0.12617794  0.11940014]]. Action = [[-0.04343671  0.07826638  0.23010781 -0.34987712]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 568 is [True, False, False, False, True, False]
Current timestep = 569. State = [[-0.12288966  0.1268891 ]]. Action = [[ 0.21018043  0.20339939 -0.24825466  0.7516494 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 569 is [True, False, False, False, True, False]
Current timestep = 570. State = [[-0.1141691  0.1349589]]. Action = [[ 0.03463662 -0.07182667  0.24192262 -0.23393238]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 570 is [True, False, False, False, False, True]
Current timestep = 571. State = [[-0.10883133  0.13532591]]. Action = [[ 0.05305985 -0.02714472  0.24651307  0.7352762 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 571 is [True, False, False, False, False, True]
Current timestep = 572. State = [[-0.10394061  0.13802537]]. Action = [[0.10364476 0.23666209 0.22456706 0.7069384 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 572 is [True, False, False, False, False, True]
Current timestep = 573. State = [[-0.1013632   0.14586954]]. Action = [[-0.19775985 -0.03949223 -0.07997288  0.6663703 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 573 is [True, False, False, False, False, True]
Current timestep = 574. State = [[-0.10691182  0.14545827]]. Action = [[ 0.01297385 -0.1521466   0.01692721  0.27689946]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 574 is [True, False, False, False, False, True]
Current timestep = 575. State = [[-0.10944897  0.14110751]]. Action = [[-0.02547908  0.0609023  -0.19900103  0.74184394]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 575 is [True, False, False, False, False, True]
Current timestep = 576. State = [[-0.11320943  0.1441103 ]]. Action = [[-0.1538654   0.18401375  0.21903223 -0.4359668 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 576 is [True, False, False, False, False, True]
Current timestep = 577. State = [[-0.12181829  0.14956485]]. Action = [[-0.15473592 -0.11074394 -0.08022904  0.48023248]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 577 is [True, False, False, False, False, True]
Current timestep = 578. State = [[-0.13094021  0.1497804 ]]. Action = [[-0.02706447  0.0979614  -0.18020105 -0.76718426]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 578 is [True, False, False, False, False, True]
Current timestep = 579. State = [[-0.13369697  0.15546569]]. Action = [[ 0.10244116  0.17377275 -0.1314235  -0.5215869 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 579 is [True, False, False, False, False, True]
Current timestep = 580. State = [[-0.13288416  0.15928486]]. Action = [[-0.15026227 -0.24889606  0.09054112 -0.52445275]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 580 is [True, False, False, False, False, True]
Current timestep = 581. State = [[-0.13602184  0.15505786]]. Action = [[ 0.1219165   0.13577196  0.18960276 -0.16657639]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 581 is [True, False, False, False, False, True]
Current timestep = 582. State = [[-0.13214645  0.15489176]]. Action = [[ 0.06359801 -0.15612105  0.09561139  0.7543514 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 582 is [True, False, False, False, False, True]
Current timestep = 583. State = [[-0.13130388  0.15367568]]. Action = [[-0.19551086  0.22903675  0.12966108 -0.16540015]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 583 is [True, False, False, False, False, True]
Scene graph at timestep 583 is [True, False, False, False, False, True]
State prediction error at timestep 583 is tensor(0.0360, grad_fn=<MseLossBackward0>)
Current timestep = 584. State = [[-0.1358724  0.1621713]]. Action = [[0.11450946 0.1088964  0.09708428 0.53009677]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 584 is [True, False, False, False, False, True]
Current timestep = 585. State = [[-0.13383895  0.16683507]]. Action = [[ 0.00389311 -0.1423532   0.06656387 -0.6675591 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 585 is [True, False, False, False, False, True]
Scene graph at timestep 585 is [True, False, False, False, False, True]
State prediction error at timestep 585 is tensor(0.0296, grad_fn=<MseLossBackward0>)
Current timestep = 586. State = [[-0.13257413  0.16383345]]. Action = [[ 0.01339215 -0.00781775  0.10383835 -0.9825558 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 586 is [True, False, False, False, False, True]
Current timestep = 587. State = [[-0.13372748  0.16136983]]. Action = [[-0.14343518 -0.04909077  0.21350414  0.19064772]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 587 is [True, False, False, False, False, True]
Current timestep = 588. State = [[-0.13873377  0.15604408]]. Action = [[ 0.01694098 -0.18817452  0.12727135 -0.74670553]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 588 is [True, False, False, False, False, True]
Current timestep = 589. State = [[-0.1387954   0.14901665]]. Action = [[0.11171588 0.06201649 0.13590872 0.48469043]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 589 is [True, False, False, False, False, True]
Scene graph at timestep 589 is [True, False, False, False, False, True]
State prediction error at timestep 589 is tensor(0.0321, grad_fn=<MseLossBackward0>)
Current timestep = 590. State = [[-0.13804872  0.1519483 ]]. Action = [[-0.22630368  0.23911506  0.16190666  0.5644152 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 590 is [True, False, False, False, False, True]
Current timestep = 591. State = [[-0.14745407  0.16158298]]. Action = [[-0.16309208  0.03782687 -0.0087077   0.4672519 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 591 is [True, False, False, False, False, True]
Scene graph at timestep 591 is [True, False, False, False, False, True]
State prediction error at timestep 591 is tensor(0.0380, grad_fn=<MseLossBackward0>)
Current timestep = 592. State = [[-0.15762684  0.16540782]]. Action = [[-0.05490589 -0.07524747 -0.14057654 -0.02628273]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 592 is [True, False, False, False, False, True]
Scene graph at timestep 592 is [True, False, False, False, False, True]
State prediction error at timestep 592 is tensor(0.0388, grad_fn=<MseLossBackward0>)
Current timestep = 593. State = [[-0.16390596  0.16320893]]. Action = [[-0.0475446  -0.06569022  0.20779032  0.9892664 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 593 is [True, False, False, False, False, True]
Current timestep = 594. State = [[-0.16984224  0.15704851]]. Action = [[-0.16194421 -0.19369729 -0.22107413  0.07841158]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 594 is [True, False, False, False, False, True]
Current timestep = 595. State = [[-0.17667459  0.15060441]]. Action = [[ 0.06801319  0.12666547 -0.09735    -0.49601126]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 595 is [True, False, False, False, False, True]
Current timestep = 596. State = [[-0.17784725  0.15562682]]. Action = [[-0.06951576  0.22386187  0.05112332  0.9023584 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 596 is [True, False, False, False, False, True]
Current timestep = 597. State = [[-0.18358047  0.16657531]]. Action = [[-0.23644106  0.10237205 -0.16851139 -0.36075795]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 597 is [True, False, False, False, False, True]
Current timestep = 598. State = [[-0.19408663  0.17400104]]. Action = [[-0.02142888 -0.00450788 -0.07033056 -0.37717366]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 598 is [True, False, False, False, False, True]
Scene graph at timestep 598 is [True, False, False, False, False, True]
State prediction error at timestep 598 is tensor(0.0440, grad_fn=<MseLossBackward0>)
Current timestep = 599. State = [[-0.1996966   0.17765059]]. Action = [[-0.05240636  0.07503346 -0.20851307 -0.4948995 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 599 is [True, False, False, False, False, True]
Current timestep = 600. State = [[-0.20437905  0.18388475]]. Action = [[-0.0783914   0.16786003 -0.23579869  0.8964596 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 600 is [True, False, False, False, False, True]
Scene graph at timestep 600 is [True, False, False, False, False, True]
State prediction error at timestep 600 is tensor(0.0589, grad_fn=<MseLossBackward0>)
Current timestep = 601. State = [[-0.20943962  0.19081874]]. Action = [[-0.06627104 -0.04771453 -0.08753651 -0.8332626 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 601 is [True, False, False, False, False, True]
Current timestep = 602. State = [[-0.2169884   0.18881527]]. Action = [[-0.24460356 -0.1961294   0.0555099  -0.53988665]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 602 is [True, False, False, False, False, True]
Current timestep = 603. State = [[-0.22606912  0.1807708 ]]. Action = [[ 0.15143484 -0.05320913  0.00804147  0.02834129]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 603 is [True, False, False, False, False, True]
Current timestep = 604. State = [[-0.22108851  0.176722  ]]. Action = [[ 0.23332101  0.05629408 -0.16058815  0.6058736 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 604 is [True, False, False, False, False, True]
Current timestep = 605. State = [[-0.21222587  0.18001197]]. Action = [[-0.1010344   0.18280694 -0.10029194  0.3625015 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 605 is [True, False, False, False, False, True]
Current timestep = 606. State = [[-0.21226127  0.18692888]]. Action = [[-0.01843186 -0.02690311  0.20265156  0.25468063]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 606 is [True, False, False, False, False, True]
Current timestep = 607. State = [[-0.21203613  0.19000286]]. Action = [[0.10294506 0.07902786 0.05448288 0.14701807]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 607 is [True, False, False, False, False, True]
Current timestep = 608. State = [[-0.21195634  0.19737357]]. Action = [[-0.23599887  0.24973676 -0.0660747   0.16068304]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 608 is [True, False, False, False, False, True]
Scene graph at timestep 608 is [True, False, False, False, False, True]
State prediction error at timestep 608 is tensor(0.0647, grad_fn=<MseLossBackward0>)
Current timestep = 609. State = [[-0.21719103  0.20668824]]. Action = [[ 0.15990245 -0.09295401 -0.12623551  0.05816877]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 609 is [True, False, False, False, False, True]
Scene graph at timestep 609 is [True, False, False, False, False, True]
State prediction error at timestep 609 is tensor(0.0599, grad_fn=<MseLossBackward0>)
Current timestep = 610. State = [[-0.21703945  0.21090873]]. Action = [[-0.18366855  0.23217547 -0.14259335 -0.72642225]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 610 is [True, False, False, False, False, True]
Current timestep = 611. State = [[-0.22231871  0.21620026]]. Action = [[ 0.00210562 -0.2164334   0.2460886   0.8458307 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 611 is [True, False, False, False, False, True]
Current timestep = 612. State = [[-0.22557351  0.21496026]]. Action = [[-0.04084554  0.22595298  0.11179438 -0.06595856]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 612 is [True, False, False, False, False, True]
Current timestep = 613. State = [[-0.23080416  0.22430249]]. Action = [[-0.20890483  0.18773663 -0.05163728 -0.32690245]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 613 is [True, False, False, False, False, True]
Current timestep = 614. State = [[-0.23890914  0.23369835]]. Action = [[ 0.04758346 -0.02343817 -0.13978577  0.22672677]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 614 is [True, False, False, False, False, True]
Current timestep = 615. State = [[-0.23869967  0.23419298]]. Action = [[ 0.1194959  -0.14071475  0.17298272  0.3957746 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 615 is [True, False, False, False, False, True]
Current timestep = 616. State = [[-0.23247798  0.22703393]]. Action = [[ 0.12776822 -0.16404466 -0.23594528  0.7569829 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 616 is [True, False, False, False, False, True]
Current timestep = 617. State = [[-0.22298108  0.22285734]]. Action = [[ 0.21410573  0.24846673 -0.04731901 -0.47485673]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 617 is [True, False, False, False, False, True]
Current timestep = 618. State = [[-0.20855366  0.23039964]]. Action = [[ 0.23376334  0.07527491 -0.05598892 -0.97987205]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 618 is [True, False, False, False, False, True]
Current timestep = 619. State = [[-0.1940112   0.23751715]]. Action = [[ 0.09210536  0.0951165  -0.05133903  0.23448598]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 619 is [True, False, False, False, False, True]
Current timestep = 620. State = [[-0.1820786   0.24427575]]. Action = [[0.24741578 0.07058072 0.15193832 0.98905706]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 620 is [True, False, False, False, False, True]
Current timestep = 621. State = [[-0.16693397  0.24915305]]. Action = [[ 0.18668681  0.01454845 -0.11912122 -0.9938354 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 621 is [True, False, False, False, False, True]
Scene graph at timestep 621 is [True, False, False, False, False, True]
State prediction error at timestep 621 is tensor(0.0665, grad_fn=<MseLossBackward0>)
Current timestep = 622. State = [[-0.15515678  0.25516376]]. Action = [[ 0.00247243  0.24167424 -0.1394255   0.7894455 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 622 is [True, False, False, False, False, True]
Scene graph at timestep 622 is [True, False, False, False, False, True]
State prediction error at timestep 622 is tensor(0.0682, grad_fn=<MseLossBackward0>)
Current timestep = 623. State = [[-0.15185457  0.26277933]]. Action = [[-0.08956347 -0.1487249  -0.16533308  0.44535136]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 623 is [True, False, False, False, False, True]
Scene graph at timestep 623 is [True, False, False, False, False, True]
State prediction error at timestep 623 is tensor(0.0628, grad_fn=<MseLossBackward0>)
Current timestep = 624. State = [[-0.15457365  0.26359847]]. Action = [[-0.01416804  0.16606814  0.20132905  0.6773325 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 624 is [True, False, False, False, False, True]
Current timestep = 625. State = [[-0.15756665  0.26710767]]. Action = [[-0.10749906 -0.10964328  0.23875105 -0.15665197]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 625 is [True, False, False, False, False, True]
Current timestep = 626. State = [[-0.15960327  0.2658856 ]]. Action = [[0.19697303 0.04981488 0.04912344 0.95648146]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 626 is [True, False, False, False, False, True]
Scene graph at timestep 626 is [True, False, False, False, False, True]
State prediction error at timestep 626 is tensor(0.0658, grad_fn=<MseLossBackward0>)
Current timestep = 627. State = [[-0.15060109  0.268197  ]]. Action = [[ 0.22110456  0.09243843  0.06665534 -0.3434568 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 627 is [True, False, False, False, False, True]
Scene graph at timestep 627 is [True, False, False, False, False, True]
State prediction error at timestep 627 is tensor(0.0634, grad_fn=<MseLossBackward0>)
Current timestep = 628. State = [[-0.1412143   0.26903892]]. Action = [[-0.14245589 -0.19319794  0.12756926 -0.55647093]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 628 is [True, False, False, False, False, True]
Current timestep = 629. State = [[-0.14100279  0.26478648]]. Action = [[ 0.12736744  0.09176791  0.21899831 -0.05748993]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 629 is [True, False, False, False, False, True]
Current timestep = 630. State = [[-0.13907799  0.26924267]]. Action = [[-0.13340378  0.21842596 -0.014736   -0.03152585]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 630 is [True, False, False, False, False, True]
Current timestep = 631. State = [[-0.14149061  0.27860028]]. Action = [[ 0.06807792  0.02590865 -0.09467317  0.4979713 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 631 is [True, False, False, False, False, True]
Current timestep = 632. State = [[-0.14000995  0.28689578]]. Action = [[ 0.03987572  0.24741483 -0.19949023  0.42594278]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 632 is [True, False, False, False, False, True]
Current timestep = 633. State = [[-0.13958591  0.29611343]]. Action = [[-0.13807969 -0.10167028  0.0350396  -0.12818384]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 633 is [True, False, False, False, False, True]
Scene graph at timestep 633 is [True, False, False, False, False, True]
State prediction error at timestep 633 is tensor(0.0667, grad_fn=<MseLossBackward0>)
Current timestep = 634. State = [[-0.14450103  0.29463193]]. Action = [[-0.02362852 -0.11273018 -0.11125937 -0.60036016]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 634 is [True, False, False, False, False, True]
Scene graph at timestep 634 is [True, False, False, False, False, True]
State prediction error at timestep 634 is tensor(0.0674, grad_fn=<MseLossBackward0>)
Current timestep = 635. State = [[-0.14491366  0.28703412]]. Action = [[ 0.17106158 -0.1954109   0.20208788 -0.2783476 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 635 is [True, False, False, False, False, True]
Scene graph at timestep 635 is [True, False, False, False, False, True]
State prediction error at timestep 635 is tensor(0.0607, grad_fn=<MseLossBackward0>)
Current timestep = 636. State = [[-0.13782886  0.27445695]]. Action = [[ 0.09556997 -0.21258259  0.1376695  -0.7730674 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 636 is [True, False, False, False, False, True]
Scene graph at timestep 636 is [True, False, False, False, False, True]
State prediction error at timestep 636 is tensor(0.0580, grad_fn=<MseLossBackward0>)
Current timestep = 637. State = [[-0.12949224  0.2667742 ]]. Action = [[ 0.17885911  0.24346274 -0.117429    0.05149865]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 637 is [True, False, False, False, False, True]
Scene graph at timestep 637 is [True, False, False, False, False, True]
State prediction error at timestep 637 is tensor(0.0684, grad_fn=<MseLossBackward0>)
Current timestep = 638. State = [[-0.12283891  0.26839268]]. Action = [[-0.20491713 -0.21175018  0.14208704 -0.17770249]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 638 is [True, False, False, False, False, True]
Current timestep = 639. State = [[-0.12775418  0.263749  ]]. Action = [[-0.00979988  0.06006718  0.00948066  0.28788435]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 639 is [True, False, False, False, False, True]
Current timestep = 640. State = [[-0.12834601  0.26084265]]. Action = [[ 0.13135302 -0.17710559 -0.06823993 -0.51357603]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 640 is [True, False, False, False, False, True]
Current timestep = 641. State = [[-0.12267363  0.25628182]]. Action = [[ 0.10871106  0.13041186  0.20070702 -0.21810913]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 641 is [True, False, False, False, False, True]
Current timestep = 642. State = [[-0.1188707  0.2615322]]. Action = [[-0.15748082  0.18636617  0.21041018 -0.2744394 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 642 is [True, False, False, False, False, True]
Current timestep = 643. State = [[-0.12436024  0.26724568]]. Action = [[-0.1280213  -0.16203566  0.2315675   0.7658851 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 643 is [True, False, False, False, False, True]
Scene graph at timestep 643 is [True, False, False, False, False, True]
State prediction error at timestep 643 is tensor(0.0558, grad_fn=<MseLossBackward0>)
Current timestep = 644. State = [[-0.12973315  0.26597956]]. Action = [[ 0.11856163  0.1088008  -0.06358472 -0.6083065 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 644 is [True, False, False, False, False, True]
Current timestep = 645. State = [[-0.12393273  0.26475003]]. Action = [[ 0.24048033 -0.2470339   0.21527588 -0.95769733]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 645 is [True, False, False, False, False, True]
Current timestep = 646. State = [[-0.11187485  0.26012537]]. Action = [[ 0.13097757  0.23264799 -0.02653024 -0.35257006]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 646 is [True, False, False, False, False, True]
Current timestep = 647. State = [[-0.09948086  0.26796234]]. Action = [[0.22051093 0.14653808 0.09829792 0.40050292]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 647 is [True, False, False, False, False, True]
Current timestep = 648. State = [[-0.08887099  0.2729912 ]]. Action = [[-0.1028821  -0.18432993 -0.17289497  0.2803011 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 648 is [True, False, False, False, False, True]
Current timestep = 649. State = [[-0.08901434  0.26830325]]. Action = [[-0.02633889 -0.04734035 -0.09582409 -0.72565717]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 649 is [True, False, False, False, False, True]
Current timestep = 650. State = [[-0.09292393  0.26641652]]. Action = [[-0.16314486  0.11210591 -0.08270523 -0.3789481 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 650 is [True, False, False, False, False, True]
Scene graph at timestep 650 is [True, False, False, False, False, True]
State prediction error at timestep 650 is tensor(0.0595, grad_fn=<MseLossBackward0>)
Current timestep = 651. State = [[-0.09714629  0.2718892 ]]. Action = [[0.1904572  0.15517706 0.20952737 0.34899652]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 651 is [True, False, False, False, False, True]
Scene graph at timestep 651 is [True, False, False, False, False, True]
State prediction error at timestep 651 is tensor(0.0590, grad_fn=<MseLossBackward0>)
Current timestep = 652. State = [[-0.09464265  0.28164858]]. Action = [[-0.15856707  0.17195868  0.20495015 -0.76596206]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 652 is [True, False, False, False, False, True]
Current timestep = 653. State = [[-0.09836922  0.29380304]]. Action = [[ 0.01973653  0.1812458  -0.03709273 -0.37702894]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 653 is [True, False, False, False, False, True]
Current timestep = 654. State = [[-0.09766735  0.30552772]]. Action = [[0.11861724 0.09259516 0.19951886 0.26188624]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 654 is [True, False, False, False, False, True]
Current timestep = 655. State = [[-0.09192003  0.30925742]]. Action = [[ 0.07440853 -0.22078376 -0.15641183  0.3069737 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 655 is [True, False, False, False, False, True]
Current timestep = 656. State = [[-0.08740491  0.30338582]]. Action = [[-0.10027209  0.15523922  0.07164839 -0.20109618]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 656 is [True, False, False, False, False, True]
Scene graph at timestep 656 is [True, False, False, False, False, True]
State prediction error at timestep 656 is tensor(0.0708, grad_fn=<MseLossBackward0>)
Current timestep = 657. State = [[-0.0843758   0.29725784]]. Action = [[ 0.08588445 -0.23154326 -0.23711471  0.18249917]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 657 is [True, False, False, False, False, True]
Current timestep = 658. State = [[-0.08187608  0.28726038]]. Action = [[-0.07948305 -0.02448566 -0.1684084  -0.7875047 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 658 is [True, False, False, False, False, True]
Scene graph at timestep 658 is [True, False, False, False, False, True]
State prediction error at timestep 658 is tensor(0.0693, grad_fn=<MseLossBackward0>)
Current timestep = 659. State = [[-0.08190521  0.28033635]]. Action = [[ 0.10827249 -0.15150808 -0.09623295  0.5375166 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 659 is [True, False, False, False, False, True]
Scene graph at timestep 659 is [True, False, False, False, False, True]
State prediction error at timestep 659 is tensor(0.0584, grad_fn=<MseLossBackward0>)
Current timestep = 660. State = [[-0.07516005  0.2724302 ]]. Action = [[ 0.23728037 -0.04260106 -0.18446212  0.4644935 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 660 is [True, False, False, False, False, True]
Current timestep = 661. State = [[-0.0652619  0.2655585]]. Action = [[-0.06199308 -0.15457131 -0.22053213  0.82555366]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 661 is [True, False, False, False, False, True]
Current timestep = 662. State = [[-0.0629741   0.25760305]]. Action = [[ 0.04562125 -0.04574406  0.14351577 -0.8011281 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 662 is [True, False, False, False, False, True]
Current timestep = 663. State = [[-0.06081605  0.25498363]]. Action = [[0.01849285 0.11712441 0.0042415  0.51387954]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 663 is [True, False, False, False, False, True]
Scene graph at timestep 663 is [True, False, False, False, False, True]
State prediction error at timestep 663 is tensor(0.0511, grad_fn=<MseLossBackward0>)
Current timestep = 664. State = [[-0.05577486  0.25839797]]. Action = [[0.24324054 0.02767491 0.02958417 0.60273874]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 664 is [True, False, False, False, False, True]
Current timestep = 665. State = [[-0.04413042  0.2617361 ]]. Action = [[ 0.11746392  0.06906191  0.13832495 -0.0470311 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 665 is [True, False, False, False, False, True]
Current timestep = 666. State = [[-0.03238945  0.2642849 ]]. Action = [[ 0.2210032  -0.05768766 -0.05330688 -0.3390838 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 666 is [False, True, False, False, False, True]
Current timestep = 667. State = [[-0.01792797  0.26714635]]. Action = [[ 0.21619236  0.24062234 -0.04799493 -0.69255114]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 667 is [False, True, False, False, False, True]
Scene graph at timestep 667 is [False, True, False, False, False, True]
State prediction error at timestep 667 is tensor(0.0676, grad_fn=<MseLossBackward0>)
Current timestep = 668. State = [[-0.00176689  0.27829432]]. Action = [[0.24980634 0.15913135 0.13131034 0.5988679 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 668 is [False, True, False, False, False, True]
Current timestep = 669. State = [[0.01380662 0.28811848]]. Action = [[ 0.08981201  0.03010434 -0.15868874  0.7943357 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 669 is [False, True, False, False, False, True]
Current timestep = 670. State = [[0.02126923 0.2954084 ]]. Action = [[-0.08673131  0.17565519  0.15842956 -0.45133263]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 670 is [False, True, False, False, False, True]
Current timestep = 671. State = [[0.02186205 0.30677953]]. Action = [[ 0.07011095  0.19317007  0.08042333 -0.16600358]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 671 is [False, True, False, False, False, True]
Current timestep = 672. State = [[0.02172543 0.3187528 ]]. Action = [[-0.16273072  0.11026764 -0.00346802 -0.2180605 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 672 is [False, True, False, False, False, True]
Current timestep = 673. State = [[0.01642754 0.32687905]]. Action = [[ 0.24689412  0.22710586 -0.17670733 -0.3289113 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 673 is [False, True, False, False, False, True]
Current timestep = 674. State = [[0.01422682 0.3296861 ]]. Action = [[-0.24438646 -0.10880038 -0.01388672  0.19812512]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 674 is [False, True, False, False, False, True]
Current timestep = 675. State = [[0.01691859 0.32683188]]. Action = [[ 0.21729475 -0.23939815 -0.10889874 -0.87877107]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 675 is [False, True, False, False, False, True]
Current timestep = 676. State = [[0.02483708 0.31537372]]. Action = [[-0.0157513 -0.1563954  0.0662885  0.4445219]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 676 is [False, True, False, False, False, True]
Current timestep = 677. State = [[0.02730583 0.30215114]]. Action = [[-0.01128617 -0.24264643  0.10156944 -0.04356968]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 677 is [False, True, False, False, False, True]
Current timestep = 678. State = [[0.02496668 0.28824934]]. Action = [[-0.16507025 -0.08093208 -0.07873289  0.23532486]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 678 is [False, True, False, False, False, True]
Scene graph at timestep 678 is [False, True, False, False, False, True]
State prediction error at timestep 678 is tensor(0.0563, grad_fn=<MseLossBackward0>)
Current timestep = 679. State = [[0.01840988 0.28415602]]. Action = [[ 0.00464347  0.21787134 -0.11605191  0.56651926]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 679 is [False, True, False, False, False, True]
Scene graph at timestep 679 is [False, True, False, False, False, True]
State prediction error at timestep 679 is tensor(0.0619, grad_fn=<MseLossBackward0>)
Current timestep = 680. State = [[0.0140025  0.28841394]]. Action = [[-0.16466518 -0.07836971  0.15014863 -0.2472384 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 680 is [False, True, False, False, False, True]
Current timestep = 681. State = [[0.00698054 0.2907927 ]]. Action = [[ 0.01281831  0.17539448  0.22017998 -0.14791906]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 681 is [False, True, False, False, False, True]
Scene graph at timestep 681 is [False, True, False, False, False, True]
State prediction error at timestep 681 is tensor(0.0670, grad_fn=<MseLossBackward0>)
Current timestep = 682. State = [[0.00409237 0.2982306 ]]. Action = [[-0.0527406   0.0794535  -0.03328604  0.44601274]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 682 is [False, True, False, False, False, True]
Scene graph at timestep 682 is [False, True, False, False, False, True]
State prediction error at timestep 682 is tensor(0.0628, grad_fn=<MseLossBackward0>)
Current timestep = 683. State = [[0.00422359 0.3029565 ]]. Action = [[ 0.18178815 -0.05398175 -0.24323778  0.024351  ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 683 is [False, True, False, False, False, True]
Scene graph at timestep 683 is [False, True, False, False, False, True]
State prediction error at timestep 683 is tensor(0.0680, grad_fn=<MseLossBackward0>)
Current timestep = 684. State = [[0.01263866 0.301349  ]]. Action = [[ 0.15806061 -0.09221403 -0.03596419 -0.15575051]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 684 is [False, True, False, False, False, True]
Scene graph at timestep 684 is [False, True, False, False, False, True]
State prediction error at timestep 684 is tensor(0.0653, grad_fn=<MseLossBackward0>)
Current timestep = 685. State = [[0.01913122 0.29455185]]. Action = [[-0.15209395 -0.18336506  0.12977165  0.07509732]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 685 is [False, True, False, False, False, True]
Current timestep = 686. State = [[0.01783492 0.28426093]]. Action = [[ 0.10684466 -0.13364784  0.18746209 -0.43095022]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 686 is [False, True, False, False, False, True]
Current timestep = 687. State = [[0.02403004 0.27731478]]. Action = [[ 0.24955028  0.075221   -0.07978448 -0.7360014 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 687 is [False, True, False, False, False, True]
Current timestep = 688. State = [[0.03453973 0.27879018]]. Action = [[-0.02487865  0.1128687   0.1672585  -0.42158002]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 688 is [False, True, False, False, False, True]
Scene graph at timestep 688 is [False, True, False, False, False, True]
State prediction error at timestep 688 is tensor(0.0658, grad_fn=<MseLossBackward0>)
Current timestep = 689. State = [[0.03887083 0.2870652 ]]. Action = [[ 0.07421616  0.23484373 -0.23465884 -0.4237852 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 689 is [False, True, False, False, False, True]
Current timestep = 690. State = [[0.0427286  0.29493886]]. Action = [[-0.00612791 -0.16856296 -0.20805486 -0.7228733 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 690 is [False, True, False, False, False, True]
Current timestep = 691. State = [[0.04029702 0.29597363]]. Action = [[-0.23147222  0.21899182 -0.14442816 -0.78427863]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 691 is [False, True, False, False, False, True]
Current timestep = 692. State = [[0.03183767 0.30126873]]. Action = [[ 0.0057984  -0.12313953 -0.13740398 -0.5128294 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 692 is [False, True, False, False, False, True]
Current timestep = 693. State = [[0.02926272 0.30075985]]. Action = [[ 0.0510141   0.08111447 -0.1034281  -0.6444013 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 693 is [False, True, False, False, False, True]
Current timestep = 694. State = [[0.03085679 0.29956862]]. Action = [[ 0.02270657 -0.20372215 -0.0103014  -0.7098818 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 694 is [False, True, False, False, False, True]
Current timestep = 695. State = [[0.03530321 0.2950424 ]]. Action = [[0.22445577 0.13397762 0.11681056 0.5158378 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 695 is [False, True, False, False, False, True]
Current timestep = 696. State = [[0.04433401 0.29924977]]. Action = [[-0.0042533   0.12886083 -0.08205609 -0.12882173]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 696 is [False, True, False, False, False, True]
Current timestep = 697. State = [[0.04723888 0.3059725 ]]. Action = [[-0.04383203  0.04907629  0.15509224  0.9793967 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 697 is [False, True, False, False, False, True]
Current timestep = 698. State = [[0.04434171 0.3106319 ]]. Action = [[-0.15301853  0.03667089  0.15976596  0.10217381]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 698 is [False, True, False, False, False, True]
Current timestep = 699. State = [[0.03504905 0.31308663]]. Action = [[-0.19851218 -0.02677473  0.10524881  0.60022473]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 699 is [False, True, False, False, False, True]
Current timestep = 700. State = [[0.02109556 0.31294915]]. Action = [[-0.24464552 -0.01249008  0.04427364 -0.9246324 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 700 is [False, True, False, False, False, True]
Current timestep = 701. State = [[0.0044014  0.30961776]]. Action = [[-0.23024112 -0.17841642  0.21196038 -0.1754458 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 701 is [False, True, False, False, False, True]
Current timestep = 702. State = [[-0.00967856  0.30270994]]. Action = [[ 0.20863679  0.18663678 -0.10527799  0.30291736]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 702 is [False, True, False, False, False, True]
Current timestep = 703. State = [[-0.0129337  0.2991984]]. Action = [[ 0.09309798 -0.04412025  0.03187272 -0.7877661 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 703 is [False, True, False, False, False, True]
Scene graph at timestep 703 is [False, True, False, False, False, True]
State prediction error at timestep 703 is tensor(0.0739, grad_fn=<MseLossBackward0>)
Current timestep = 704. State = [[-0.01284087  0.29508665]]. Action = [[-0.16742733 -0.09123483 -0.22752582 -0.1631676 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 704 is [False, True, False, False, False, True]
Scene graph at timestep 704 is [False, True, False, False, False, True]
State prediction error at timestep 704 is tensor(0.0634, grad_fn=<MseLossBackward0>)
Current timestep = 705. State = [[-0.01672811  0.28843027]]. Action = [[ 0.10941783 -0.15614775  0.03443623  0.67180943]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 705 is [False, True, False, False, False, True]
Current timestep = 706. State = [[-0.01230357  0.27811804]]. Action = [[ 0.16800433 -0.17492715  0.22999826  0.08968353]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 706 is [False, True, False, False, False, True]
Current timestep = 707. State = [[-0.0080587   0.26651827]]. Action = [[-0.23069966 -0.12444732  0.1328507   0.1369803 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 707 is [False, True, False, False, False, True]
Current timestep = 708. State = [[-0.01632583  0.2597938 ]]. Action = [[-0.13344541  0.08565611  0.1334149  -0.5549263 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 708 is [False, True, False, False, False, True]
Current timestep = 709. State = [[-0.02389659  0.25652635]]. Action = [[ 0.00769788 -0.23312892  0.03734273  0.81189466]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 709 is [False, True, False, False, False, True]
Scene graph at timestep 709 is [False, True, False, False, False, True]
State prediction error at timestep 709 is tensor(0.0469, grad_fn=<MseLossBackward0>)
Current timestep = 710. State = [[-0.02805111  0.24510905]]. Action = [[-0.10919952 -0.18442012 -0.16934286 -0.3440084 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 710 is [False, True, False, False, False, True]
Current timestep = 711. State = [[-0.0325814   0.23167436]]. Action = [[ 0.03655696 -0.20208588  0.04096729 -0.04511029]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 711 is [False, True, False, False, False, True]
Current timestep = 712. State = [[-0.03054662  0.22321622]]. Action = [[ 0.1916601   0.18669808 -0.04312854  0.33489728]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 712 is [False, True, False, False, False, True]
Current timestep = 713. State = [[-0.02541858  0.2268759 ]]. Action = [[-0.18474014  0.05879143 -0.21523991 -0.47716224]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 713 is [False, True, False, False, False, True]
Current timestep = 714. State = [[-0.02866129  0.22749586]]. Action = [[ 0.04891935 -0.20801209 -0.23005041 -0.07414454]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 714 is [False, True, False, False, False, True]
Current timestep = 715. State = [[-0.03054486  0.22022732]]. Action = [[-0.11364159 -0.04373091  0.19317985  0.02751064]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 715 is [False, True, False, False, False, True]
Scene graph at timestep 715 is [False, True, False, False, False, True]
State prediction error at timestep 715 is tensor(0.0376, grad_fn=<MseLossBackward0>)
Current timestep = 716. State = [[-0.03600239  0.21859191]]. Action = [[-0.06834874  0.17092255 -0.12566595  0.3334744 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 716 is [False, True, False, False, False, True]
Scene graph at timestep 716 is [False, True, False, False, False, True]
State prediction error at timestep 716 is tensor(0.0422, grad_fn=<MseLossBackward0>)
Current timestep = 717. State = [[-0.04155035  0.22094597]]. Action = [[-0.10210627 -0.16717601 -0.06563997  0.08878958]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 717 is [False, True, False, False, False, True]
Current timestep = 718. State = [[-0.04596204  0.21551867]]. Action = [[ 0.08557743 -0.08039938 -0.16359204  0.04506803]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 718 is [False, True, False, False, False, True]
Current timestep = 719. State = [[-0.04349924  0.21245362]]. Action = [[0.08999345 0.11754483 0.02205402 0.84846234]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 719 is [False, True, False, False, False, True]
Current timestep = 720. State = [[-0.0375248   0.21641614]]. Action = [[ 0.11322334  0.08206767  0.1686604  -0.18432838]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 720 is [False, True, False, False, False, True]
Scene graph at timestep 720 is [False, True, False, False, False, True]
State prediction error at timestep 720 is tensor(0.0395, grad_fn=<MseLossBackward0>)
Current timestep = 721. State = [[-0.02793527  0.21816908]]. Action = [[ 0.23360693 -0.1660935  -0.12117746 -0.3738433 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 721 is [False, True, False, False, False, True]
Current timestep = 722. State = [[-0.01470419  0.21088357]]. Action = [[ 0.13436812 -0.16057388  0.05842999 -0.09684998]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 722 is [False, True, False, False, False, True]
Current timestep = 723. State = [[-0.00187177  0.20158178]]. Action = [[ 0.23973328 -0.08119285 -0.11897516 -0.9610952 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 723 is [False, True, False, False, False, True]
Current timestep = 724. State = [[0.01161386 0.19795702]]. Action = [[0.06880879 0.16226733 0.04843533 0.9721887 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 724 is [False, True, False, False, False, True]
Current timestep = 725. State = [[0.01704575 0.20051803]]. Action = [[-0.1466868  -0.08775264  0.2339052   0.3132459 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 725 is [False, True, False, False, False, True]
Current timestep = 726. State = [[0.0160162  0.20219968]]. Action = [[ 0.16844094  0.19172043  0.16449723 -0.7576656 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 726 is [False, True, False, False, False, True]
Current timestep = 727. State = [[0.0235722  0.21045955]]. Action = [[0.18131995 0.11057684 0.20161936 0.20903766]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 727 is [False, True, False, False, False, True]
Current timestep = 728. State = [[0.03107545 0.21450289]]. Action = [[-0.12760387 -0.16288365  0.04745474 -0.22109938]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 728 is [False, True, False, False, False, True]
Scene graph at timestep 728 is [False, True, False, False, False, True]
State prediction error at timestep 728 is tensor(0.0373, grad_fn=<MseLossBackward0>)
Current timestep = 729. State = [[0.02594372 0.21046387]]. Action = [[-0.24419774 -0.01928043  0.09853455  0.26353765]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 729 is [False, True, False, False, False, True]
Current timestep = 730. State = [[0.01340858 0.2107928 ]]. Action = [[-0.12332939  0.166785    0.20782089  0.07376039]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 730 is [False, True, False, False, False, True]
Current timestep = 731. State = [[0.00257513 0.21913621]]. Action = [[-0.13686977  0.18140236  0.23056877 -0.6693413 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 731 is [False, True, False, False, False, True]
Current timestep = 732. State = [[-0.00740633  0.226403  ]]. Action = [[-0.1250225  -0.1172938  -0.06925973 -0.23385882]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 732 is [False, True, False, False, False, True]
Current timestep = 733. State = [[-0.01199005  0.22608694]]. Action = [[0.2373994  0.03674316 0.23376173 0.0703851 ]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 733 is [False, True, False, False, False, True]
Scene graph at timestep 733 is [False, True, False, False, False, True]
State prediction error at timestep 733 is tensor(0.0396, grad_fn=<MseLossBackward0>)
Current timestep = 734. State = [[-0.00201001  0.22804135]]. Action = [[0.24542138 0.08361459 0.1760467  0.63652277]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 734 is [False, True, False, False, False, True]
Current timestep = 735. State = [[0.01013112 0.22868872]]. Action = [[-0.03461727 -0.16929424  0.02693757 -0.25449574]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 735 is [False, True, False, False, False, True]
Current timestep = 736. State = [[0.0102427  0.22378632]]. Action = [[-0.2196649   0.01351902  0.18974176  0.66461205]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 736 is [False, True, False, False, False, True]
Current timestep = 737. State = [[0.00407014 0.22206493]]. Action = [[ 0.11528757 -0.02169792 -0.02243543 -0.8090815 ]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 737 is [False, True, False, False, False, True]
Scene graph at timestep 737 is [False, True, False, False, False, True]
State prediction error at timestep 737 is tensor(0.0517, grad_fn=<MseLossBackward0>)
Current timestep = 738. State = [[0.00435822 0.21721776]]. Action = [[-0.08197361 -0.22005728 -0.10242213 -0.8233887 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 738 is [False, True, False, False, False, True]
Current timestep = 739. State = [[0.0032415  0.21019636]]. Action = [[ 0.10136387  0.10676363 -0.17265831 -0.4175024 ]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 739 is [False, True, False, False, False, True]
Current timestep = 740. State = [[0.00978371 0.2126214 ]]. Action = [[0.24941373 0.11082608 0.20208919 0.7284478 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 740 is [False, True, False, False, False, True]
Current timestep = 741. State = [[0.01964484 0.214521  ]]. Action = [[-0.09068599 -0.16689725  0.01641995 -0.42877495]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 741 is [False, True, False, False, False, True]
Current timestep = 742. State = [[0.02087794 0.21270172]]. Action = [[ 0.04471397  0.16317901 -0.14173123 -0.30008316]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 742 is [False, True, False, False, False, True]
Current timestep = 743. State = [[0.01912649 0.21565057]]. Action = [[-0.23763363 -0.07534486 -0.08664505 -0.3036222 ]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 743 is [False, True, False, False, False, True]
Current timestep = 744. State = [[0.00824875 0.2137565 ]]. Action = [[-0.14863728 -0.06021032  0.10218012  0.7404125 ]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 744 is [False, True, False, False, False, True]
Current timestep = 745. State = [[-0.00227324  0.20856409]]. Action = [[-0.0816271  -0.15300035 -0.1973653   0.08400738]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 745 is [False, True, False, False, False, True]
Current timestep = 746. State = [[-0.00975104  0.19800442]]. Action = [[-0.06858607 -0.23489936  0.0266777   0.58146596]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 746 is [False, True, False, False, False, True]
Current timestep = 747. State = [[-0.01134335  0.18646067]]. Action = [[ 0.23944002 -0.01034637  0.06326604 -0.2073139 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 747 is [False, True, False, False, False, True]
Current timestep = 748. State = [[-0.00223762  0.18444896]]. Action = [[0.09887415 0.1784724  0.22213429 0.8553473 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 748 is [False, True, False, False, False, True]
Current timestep = 749. State = [[0.00356066 0.18760902]]. Action = [[-0.10905883 -0.12918726 -0.1898415  -0.15699548]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 749 is [False, True, False, False, False, True]
Current timestep = 750. State = [[0.00116472 0.1832111 ]]. Action = [[-0.04441613 -0.11529791  0.14051569 -0.72059715]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 750 is [False, True, False, False, False, True]
Current timestep = 751. State = [[-0.00088529  0.17817128]]. Action = [[ 0.05649081  0.03133926 -0.17047311 -0.27660632]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 751 is [False, True, False, False, False, True]
Current timestep = 752. State = [[2.3500732e-05 1.7722602e-01]]. Action = [[-0.0248559   0.00183821 -0.03341283  0.43566608]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 752 is [False, True, False, False, False, True]
Current timestep = 753. State = [[-0.00250004  0.17523728]]. Action = [[-0.15525256 -0.11615881  0.19448486 -0.01682848]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 753 is [False, True, False, False, False, True]
Current timestep = 754. State = [[-0.00571125  0.1718186 ]]. Action = [[ 0.22291467  0.0520727   0.08855551 -0.4560727 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 754 is [False, True, False, False, False, True]
Current timestep = 755. State = [[0.00163094 0.17507222]]. Action = [[0.08112898 0.19858852 0.01995742 0.46081185]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 755 is [False, True, False, False, False, True]
Current timestep = 756. State = [[0.00614828 0.18330069]]. Action = [[-0.10525197  0.03525501 -0.13347949 -0.8798099 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 756 is [False, True, False, False, False, True]
Current timestep = 757. State = [[0.00519772 0.18746746]]. Action = [[ 0.07167816 -0.0211546  -0.1380668  -0.57500964]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 757 is [False, True, False, False, False, True]
Current timestep = 758. State = [[0.00730521 0.19054013]]. Action = [[ 0.03253973  0.15462095 -0.15445897 -0.13088995]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 758 is [False, True, False, False, False, True]
Scene graph at timestep 758 is [False, True, False, False, False, True]
State prediction error at timestep 758 is tensor(0.0373, grad_fn=<MseLossBackward0>)
Current timestep = 759. State = [[0.00857275 0.1940389 ]]. Action = [[-0.06723514 -0.15429452  0.23027688 -0.41441768]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 759 is [False, True, False, False, False, True]
Current timestep = 760. State = [[0.00366043 0.18714921]]. Action = [[-0.20934694 -0.21207319  0.16085437 -0.47703707]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 760 is [False, True, False, False, False, True]
Current timestep = 761. State = [[-0.00481418  0.17783186]]. Action = [[ 0.05799845  0.00945646 -0.12979665  0.40574253]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 761 is [False, True, False, False, False, True]
Current timestep = 762. State = [[-0.00284752  0.17412587]]. Action = [[ 0.21534654 -0.02794601 -0.12229593  0.0889492 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 762 is [False, True, False, False, False, True]
Current timestep = 763. State = [[0.00869162 0.17528284]]. Action = [[ 0.22939539  0.2072607  -0.16978075  0.16887808]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 763 is [False, True, False, False, False, True]
Scene graph at timestep 763 is [False, True, False, False, False, True]
State prediction error at timestep 763 is tensor(0.0314, grad_fn=<MseLossBackward0>)
Current timestep = 764. State = [[0.02412517 0.18032002]]. Action = [[ 0.21104458 -0.13194153 -0.12348215  0.35092437]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 764 is [False, True, False, False, False, True]
Current timestep = 765. State = [[0.04048586 0.17462051]]. Action = [[ 0.24723703 -0.22774833  0.12995231 -0.9062925 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 765 is [False, True, False, False, False, True]
Current timestep = 766. State = [[0.053443  0.1644221]]. Action = [[-0.07579112 -0.01893899 -0.04533091 -0.3386684 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 766 is [False, True, False, False, False, True]
Current timestep = 767. State = [[0.05885974 0.1607181 ]]. Action = [[0.21856976 0.03323016 0.24534905 0.3423338 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 767 is [False, False, True, False, False, True]
Current timestep = 768. State = [[0.06457487 0.1626893 ]]. Action = [[-0.20873857  0.15127134  0.13981682 -0.71676725]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 768 is [False, False, True, False, False, True]
Scene graph at timestep 768 is [False, False, True, False, False, True]
State prediction error at timestep 768 is tensor(0.0388, grad_fn=<MseLossBackward0>)
Current timestep = 769. State = [[0.05646572 0.1703192 ]]. Action = [[-0.23713462  0.1141454   0.1059621   0.00723088]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 769 is [False, False, True, False, False, True]
Current timestep = 770. State = [[0.04153001 0.17639783]]. Action = [[-0.23661429 -0.03899311  0.09401107  0.98291266]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 770 is [False, False, True, False, False, True]
Current timestep = 771. State = [[0.03087105 0.17667036]]. Action = [[ 0.20522922 -0.05826716 -0.03171112 -0.04953921]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 771 is [False, True, False, False, False, True]
Scene graph at timestep 771 is [False, True, False, False, False, True]
State prediction error at timestep 771 is tensor(0.0291, grad_fn=<MseLossBackward0>)
Current timestep = 772. State = [[0.03615353 0.17734194]]. Action = [[ 0.15098694  0.17937082  0.13345394 -0.8955244 ]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 772 is [False, True, False, False, False, True]
Scene graph at timestep 772 is [False, True, False, False, False, True]
State prediction error at timestep 772 is tensor(0.0446, grad_fn=<MseLossBackward0>)
Current timestep = 773. State = [[0.04294667 0.18011087]]. Action = [[-0.07684277 -0.19175513  0.21294141 -0.0803296 ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 773 is [False, True, False, False, False, True]
Current timestep = 774. State = [[0.04555001 0.17750949]]. Action = [[ 0.1958322   0.13118374 -0.03345583  0.6510874 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 774 is [False, True, False, False, False, True]
Current timestep = 775. State = [[0.05254757 0.18224148]]. Action = [[-0.01607351  0.12739474  0.19898343  0.5468526 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 775 is [False, True, False, False, False, True]
Scene graph at timestep 775 is [False, False, True, False, False, True]
State prediction error at timestep 775 is tensor(0.0300, grad_fn=<MseLossBackward0>)
Current timestep = 776. State = [[0.05656175 0.19134596]]. Action = [[ 0.11952055  0.18410924  0.2209177  -0.09134418]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 776 is [False, False, True, False, False, True]
Current timestep = 777. State = [[0.05992206 0.20175025]]. Action = [[-0.13178778  0.08138368 -0.22114272  0.31260288]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 777 is [False, False, True, False, False, True]
Current timestep = 778. State = [[0.06027249 0.20625538]]. Action = [[ 0.22746685 -0.14466788  0.17701459  0.64170885]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 778 is [False, False, True, False, False, True]
Current timestep = 779. State = [[0.06614929 0.2012955 ]]. Action = [[-0.10112876 -0.09549549 -0.05488679 -0.8567645 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 779 is [False, False, True, False, False, True]
Scene graph at timestep 779 is [False, False, True, False, False, True]
State prediction error at timestep 779 is tensor(0.0538, grad_fn=<MseLossBackward0>)
Current timestep = 780. State = [[0.06634074 0.19318627]]. Action = [[ 0.07066715 -0.1994737  -0.06635685  0.29480922]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 780 is [False, False, True, False, False, True]
Current timestep = 781. State = [[0.06612743 0.18412285]]. Action = [[-0.14772804  0.03176078 -0.00267649 -0.4156394 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 781 is [False, False, True, False, False, True]
Current timestep = 782. State = [[0.05961888 0.1825444 ]]. Action = [[-0.11096108  0.0452641  -0.12767416  0.1762631 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 782 is [False, False, True, False, False, True]
Current timestep = 783. State = [[0.05172963 0.18751195]]. Action = [[-0.1010676   0.24330002 -0.11607048 -0.01336288]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 783 is [False, False, True, False, False, True]
Current timestep = 784. State = [[0.04269535 0.20086414]]. Action = [[-0.19395904  0.23385462 -0.22275564 -0.8942189 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 784 is [False, False, True, False, False, True]
Current timestep = 785. State = [[0.03177088 0.21690983]]. Action = [[-0.07778469  0.2104226   0.0745692  -0.22903812]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 785 is [False, True, False, False, False, True]
Current timestep = 786. State = [[0.02565067 0.23208179]]. Action = [[ 0.03439915  0.16002601 -0.06646325 -0.55082744]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 786 is [False, True, False, False, False, True]
Current timestep = 787. State = [[0.02505624 0.24109021]]. Action = [[ 0.00318256 -0.09730253 -0.11753304  0.04489005]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 787 is [False, True, False, False, False, True]
Scene graph at timestep 787 is [False, True, False, False, False, True]
State prediction error at timestep 787 is tensor(0.0444, grad_fn=<MseLossBackward0>)
Current timestep = 788. State = [[0.02670392 0.23872407]]. Action = [[ 0.10722053 -0.15796152 -0.12379405  0.23667431]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 788 is [False, True, False, False, False, True]
Current timestep = 789. State = [[0.03198436 0.23206039]]. Action = [[ 0.08479118 -0.02306654 -0.2389254  -0.35315073]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 789 is [False, True, False, False, False, True]
Current timestep = 790. State = [[0.03954337 0.22712661]]. Action = [[ 0.17732245 -0.10528974 -0.14644177 -0.5083091 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 790 is [False, True, False, False, False, True]
Current timestep = 791. State = [[0.04529603 0.22059566]]. Action = [[-0.20258613 -0.07698721  0.01837048 -0.17037821]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 791 is [False, True, False, False, False, True]
Current timestep = 792. State = [[0.03793371 0.2171571 ]]. Action = [[-0.1781875   0.08450991  0.04386595 -0.2509128 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 792 is [False, True, False, False, False, True]
Current timestep = 793. State = [[0.02606387 0.21572247]]. Action = [[-0.19547072 -0.1854766  -0.23382545 -0.45124555]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 793 is [False, True, False, False, False, True]
Current timestep = 794. State = [[0.01286791 0.21151438]]. Action = [[-0.12892842  0.12975347  0.04684162  0.83721507]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 794 is [False, True, False, False, False, True]
Current timestep = 795. State = [[0.0034561 0.2125193]]. Action = [[-0.04137313 -0.0894403   0.1374608   0.6436479 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 795 is [False, True, False, False, False, True]
Current timestep = 796. State = [[-0.00118888  0.21288218]]. Action = [[ 0.01089039  0.16449451 -0.099931    0.34108114]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 796 is [False, True, False, False, False, True]
Current timestep = 797. State = [[-0.00274495  0.216522  ]]. Action = [[-0.04904716 -0.10134652  0.2495512   0.4180622 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 797 is [False, True, False, False, False, True]
Current timestep = 798. State = [[-0.00485669  0.21563572]]. Action = [[ 0.00871336  0.04898271 -0.0521917   0.9446156 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 798 is [False, True, False, False, False, True]
Current timestep = 799. State = [[-0.0087212   0.21889628]]. Action = [[-0.23093823  0.14974171  0.11066303 -0.86301434]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 799 is [False, True, False, False, False, True]
Current timestep = 800. State = [[-0.01879407  0.2227726 ]]. Action = [[-0.07751425 -0.14889406 -0.08264202  0.42639923]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 800 is [False, True, False, False, False, True]
Current timestep = 801. State = [[-0.02451277  0.21662846]]. Action = [[ 0.06578648 -0.1864992   0.10199988  0.64242697]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 801 is [False, True, False, False, False, True]
Current timestep = 802. State = [[-0.02398909  0.21108323]]. Action = [[ 0.03267607  0.19381064 -0.11265546 -0.45114148]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 802 is [False, True, False, False, False, True]
Current timestep = 803. State = [[-0.02276729  0.21390948]]. Action = [[-0.05006772 -0.07850619 -0.23095971 -0.79141146]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 803 is [False, True, False, False, False, True]
Current timestep = 804. State = [[-0.02115489  0.2164235 ]]. Action = [[ 0.21683434  0.2213679   0.02797493 -0.23511785]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 804 is [False, True, False, False, False, True]
Current timestep = 805. State = [[-0.01102019  0.22742806]]. Action = [[ 0.16310036  0.20589542  0.15120247 -0.8650296 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 805 is [False, True, False, False, False, True]
Scene graph at timestep 805 is [False, True, False, False, False, True]
State prediction error at timestep 805 is tensor(0.0533, grad_fn=<MseLossBackward0>)
Current timestep = 806. State = [[-0.00274029  0.24057247]]. Action = [[-0.09112519  0.14926207  0.03752792 -0.29706693]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 806 is [False, True, False, False, False, True]
Current timestep = 807. State = [[-0.0026955   0.25251457]]. Action = [[ 0.00478011  0.13801098 -0.22894435  0.85993576]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 807 is [False, True, False, False, False, True]
Current timestep = 808. State = [[-0.00303567  0.25960967]]. Action = [[-0.02095254 -0.10026231 -0.16266009 -0.8830146 ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 808 is [False, True, False, False, False, True]
Current timestep = 809. State = [[-0.00232454  0.2612232 ]]. Action = [[0.12412441 0.14276046 0.18254727 0.22307944]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 809 is [False, True, False, False, False, True]
Current timestep = 810. State = [[-0.00106955  0.2642078 ]]. Action = [[-0.23779951 -0.10670486  0.20464426  0.1658225 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 810 is [False, True, False, False, False, True]
Current timestep = 811. State = [[-0.01221005  0.26181233]]. Action = [[-0.24619709 -0.02183965 -0.15547667 -0.58889747]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 811 is [False, True, False, False, False, True]
Current timestep = 812. State = [[-0.02301338  0.2584828 ]]. Action = [[ 0.1444557  -0.10731182  0.24247733  0.5534167 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 812 is [False, True, False, False, False, True]
Scene graph at timestep 812 is [False, True, False, False, False, True]
State prediction error at timestep 812 is tensor(0.0448, grad_fn=<MseLossBackward0>)
Current timestep = 813. State = [[-0.02428385  0.25420097]]. Action = [[-0.15423948  0.03803131  0.1776067   0.14616466]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 813 is [False, True, False, False, False, True]
Current timestep = 814. State = [[-0.0272633   0.25157318]]. Action = [[ 0.13812968 -0.15344799  0.07691148  0.6213716 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 814 is [False, True, False, False, False, True]
Current timestep = 815. State = [[-0.02692048  0.24871247]]. Action = [[-0.19120649  0.19587564 -0.19123287 -0.9107446 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 815 is [False, True, False, False, False, True]
Scene graph at timestep 815 is [False, True, False, False, False, True]
State prediction error at timestep 815 is tensor(0.0676, grad_fn=<MseLossBackward0>)
Current timestep = 816. State = [[-0.0316968   0.25695223]]. Action = [[0.08282626 0.18790102 0.18958151 0.3436234 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 816 is [False, True, False, False, False, True]
Current timestep = 817. State = [[-0.03122143  0.26651216]]. Action = [[-0.03061564  0.01792908 -0.15995961  0.6208749 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 817 is [False, True, False, False, False, True]
Scene graph at timestep 817 is [False, True, False, False, False, True]
State prediction error at timestep 817 is tensor(0.0543, grad_fn=<MseLossBackward0>)
Current timestep = 818. State = [[-0.03414905  0.27307826]]. Action = [[-0.15452561  0.1546349  -0.22909272 -0.02620709]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 818 is [False, True, False, False, False, True]
Current timestep = 819. State = [[-0.04278376  0.28279713]]. Action = [[-0.1557588   0.15877903 -0.01594977 -0.5510592 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 819 is [False, True, False, False, False, True]
Current timestep = 820. State = [[-0.04966961  0.2900103 ]]. Action = [[ 0.10383651 -0.1008717  -0.08971922  0.20208788]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 820 is [False, True, False, False, False, True]
Current timestep = 821. State = [[-0.0479143  0.2918509]]. Action = [[ 0.07190174  0.16149306  0.2457332  -0.1201269 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 821 is [False, True, False, False, False, True]
Current timestep = 822. State = [[-0.04430391  0.29686844]]. Action = [[-0.00689781 -0.03481403 -0.20147029  0.35883915]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 822 is [False, True, False, False, False, True]
Current timestep = 823. State = [[-0.04303277  0.2984421 ]]. Action = [[0.0152683  0.04053921 0.03242469 0.2921095 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 823 is [False, True, False, False, False, True]
Current timestep = 824. State = [[-0.03897123  0.30354875]]. Action = [[ 0.23889863  0.21136016  0.17208403 -0.63137627]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 824 is [False, True, False, False, False, True]
Current timestep = 825. State = [[-0.03241416  0.31307417]]. Action = [[-0.22125824  0.06836566  0.05920208 -0.5202895 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 825 is [False, True, False, False, False, True]
Current timestep = 826. State = [[-0.03711261  0.31915963]]. Action = [[ 0.08298686  0.09173298  0.11985281 -0.20287979]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 826 is [False, True, False, False, False, True]
Scene graph at timestep 826 is [False, True, False, False, False, True]
State prediction error at timestep 826 is tensor(0.0742, grad_fn=<MseLossBackward0>)
Current timestep = 827. State = [[-0.03951711  0.31978956]]. Action = [[-0.01084651 -0.08867992 -0.24276026  0.21912348]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 827 is [False, True, False, False, False, True]
Current timestep = 828. State = [[-0.04074808  0.31703523]]. Action = [[-0.03357676  0.17309323 -0.18931717  0.42798185]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 828 is [False, True, False, False, False, True]
Current timestep = 829. State = [[-0.03745453  0.31297582]]. Action = [[ 0.24473366 -0.19068101 -0.10698374 -0.56486595]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 829 is [False, True, False, False, False, True]
Current timestep = 830. State = [[-0.02597558  0.30591604]]. Action = [[0.15775043 0.03598836 0.06388235 0.6298704 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 830 is [False, True, False, False, False, True]
Current timestep = 831. State = [[-0.01632965  0.30554315]]. Action = [[-0.01490763  0.07929119 -0.06180395 -0.93713677]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 831 is [False, True, False, False, False, True]
Current timestep = 832. State = [[-0.01086282  0.30503157]]. Action = [[ 0.15324986 -0.20470272 -0.22636856  0.6372268 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 832 is [False, True, False, False, False, True]
Current timestep = 833. State = [[-0.0028754   0.29704034]]. Action = [[ 0.09258634 -0.08173594 -0.05256239 -0.84256804]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 833 is [False, True, False, False, False, True]
Current timestep = 834. State = [[0.00184605 0.2920792 ]]. Action = [[-0.09158349  0.05811948  0.15416172  0.645414  ]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 834 is [False, True, False, False, False, True]
Current timestep = 835. State = [[0.00207372 0.29454583]]. Action = [[ 0.11461872  0.13953042 -0.1555098   0.7376232 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 835 is [False, True, False, False, False, True]
Current timestep = 836. State = [[0.00579698 0.30079436]]. Action = [[-3.0866265e-04  5.0708860e-02 -5.1482558e-02  5.3654909e-01]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 836 is [False, True, False, False, False, True]
Current timestep = 837. State = [[0.00966306 0.30684254]]. Action = [[ 0.16416329  0.11927867  0.23129246 -0.13291478]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 837 is [False, True, False, False, False, True]
Scene graph at timestep 837 is [False, True, False, False, False, True]
State prediction error at timestep 837 is tensor(0.0710, grad_fn=<MseLossBackward0>)
Current timestep = 838. State = [[0.0192327  0.31279868]]. Action = [[ 0.19058213  0.00078651 -0.08712961  0.18038046]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 838 is [False, True, False, False, False, True]
Current timestep = 839. State = [[0.02924667 0.31490058]]. Action = [[ 0.22407082  0.17436248 -0.07456365  0.0684495 ]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 839 is [False, True, False, False, False, True]
Current timestep = 840. State = [[0.03069327 0.3133424 ]]. Action = [[-0.16409512 -0.13239533 -0.01702628 -0.08991122]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 840 is [False, True, False, False, False, True]
Current timestep = 841. State = [[0.02381797 0.30780712]]. Action = [[-0.10239348 -0.05146241 -0.03433369  0.6408386 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 841 is [False, True, False, False, False, True]
Current timestep = 842. State = [[0.01648249 0.30151248]]. Action = [[-0.07680887 -0.15635869  0.24288869  0.15673113]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 842 is [False, True, False, False, False, True]
Current timestep = 843. State = [[0.01324931 0.29117614]]. Action = [[ 0.13545746 -0.20169388 -0.08325359 -0.43133378]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 843 is [False, True, False, False, False, True]
Current timestep = 844. State = [[0.02028085 0.27695632]]. Action = [[ 0.24359012 -0.24759462  0.22058704 -0.84396744]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 844 is [False, True, False, False, False, True]
Current timestep = 845. State = [[0.03245011 0.26434162]]. Action = [[ 0.08743927  0.04292876  0.21314299 -0.6419293 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 845 is [False, True, False, False, False, True]
Current timestep = 846. State = [[0.04320861 0.26087546]]. Action = [[ 0.20922047 -0.01868719  0.22163692  0.9522784 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 846 is [False, True, False, False, False, True]
Current timestep = 847. State = [[0.05479257 0.26127645]]. Action = [[0.06566206 0.12879401 0.09708595 0.37349308]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 847 is [False, True, False, False, False, True]
Current timestep = 848. State = [[0.05996598 0.26503688]]. Action = [[-0.09247687 -0.02780247  0.03755426 -0.90853995]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 848 is [False, False, True, False, False, True]
Current timestep = 849. State = [[0.05563966 0.2637193 ]]. Action = [[-0.20146811 -0.12238155  0.23180306  0.23734021]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 849 is [False, False, True, False, False, True]
Current timestep = 850. State = [[0.04460501 0.26318935]]. Action = [[-0.1546876   0.2435902   0.17041636 -0.9838493 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 850 is [False, False, True, False, False, True]
Current timestep = 851. State = [[0.03285322 0.27169374]]. Action = [[-0.16754818  0.06823581 -0.07945058  0.50647426]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 851 is [False, True, False, False, False, True]
Scene graph at timestep 851 is [False, True, False, False, False, True]
State prediction error at timestep 851 is tensor(0.0550, grad_fn=<MseLossBackward0>)
Current timestep = 852. State = [[0.02440317 0.2777975 ]]. Action = [[ 0.08985537  0.01896843 -0.19115043 -0.568202  ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 852 is [False, True, False, False, False, True]
Scene graph at timestep 852 is [False, True, False, False, False, True]
State prediction error at timestep 852 is tensor(0.0683, grad_fn=<MseLossBackward0>)
Current timestep = 853. State = [[0.02372039 0.28187335]]. Action = [[-0.04286188  0.08949262 -0.20575699  0.87571764]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 853 is [False, True, False, False, False, True]
Current timestep = 854. State = [[0.02347366 0.2874271 ]]. Action = [[ 0.0686861   0.07841343 -0.17552608 -0.13461995]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 854 is [False, True, False, False, False, True]
Current timestep = 855. State = [[0.02653491 0.29259074]]. Action = [[ 0.06320733  0.03869879 -0.09009346  0.35942578]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 855 is [False, True, False, False, False, True]
Current timestep = 856. State = [[0.03112085 0.29761186]]. Action = [[0.08568764 0.11974409 0.05659014 0.5365486 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 856 is [False, True, False, False, False, True]
Current timestep = 857. State = [[0.03827765 0.30558452]]. Action = [[0.18025881 0.14585578 0.15588993 0.21490633]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 857 is [False, True, False, False, False, True]
Current timestep = 858. State = [[0.04491964 0.31173405]]. Action = [[-0.12843645 -0.07195368  0.23672271  0.82392335]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 858 is [False, True, False, False, False, True]
Scene graph at timestep 858 is [False, True, False, False, False, True]
State prediction error at timestep 858 is tensor(0.0699, grad_fn=<MseLossBackward0>)
Current timestep = 859. State = [[0.04338495 0.31193864]]. Action = [[-0.09839189  0.08696604  0.06672895  0.81637096]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 859 is [False, True, False, False, False, True]
Current timestep = 860. State = [[0.04192236 0.31142083]]. Action = [[-0.03177305 -0.02314375 -0.12590961  0.23814678]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 860 is [False, True, False, False, False, True]
Current timestep = 861. State = [[0.03789156 0.31052804]]. Action = [[-0.16483873  0.00908101  0.1436879  -0.3307966 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 861 is [False, True, False, False, False, True]
Current timestep = 862. State = [[0.03095333 0.3106203 ]]. Action = [[-0.10820147  0.20772907 -0.11341853  0.8063748 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 862 is [False, True, False, False, False, True]
Current timestep = 863. State = [[0.02827173 0.31068143]]. Action = [[-0.24131466  0.13929176 -0.19415449 -0.71435916]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 863 is [False, True, False, False, False, True]
Current timestep = 864. State = [[0.02946386 0.30863675]]. Action = [[ 0.13057083 -0.13324802  0.04164219  0.85693955]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 864 is [False, True, False, False, False, True]
Scene graph at timestep 864 is [False, True, False, False, False, True]
State prediction error at timestep 864 is tensor(0.0678, grad_fn=<MseLossBackward0>)
Current timestep = 865. State = [[0.03164423 0.305159  ]]. Action = [[-0.17294313  0.1091907   0.04533392 -0.52012765]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 865 is [False, True, False, False, False, True]
Current timestep = 866. State = [[0.02482347 0.3086264 ]]. Action = [[-0.14241667  0.08449888  0.02742952  0.5667535 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 866 is [False, True, False, False, False, True]
Current timestep = 867. State = [[0.02073705 0.3107432 ]]. Action = [[ 0.23047835 -0.14513981 -0.0817778   0.3958149 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 867 is [False, True, False, False, False, True]
Current timestep = 868. State = [[0.02557503 0.3078361 ]]. Action = [[-0.07810727  0.07099301 -0.18804981 -0.32739675]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 868 is [False, True, False, False, False, True]
Scene graph at timestep 868 is [False, True, False, False, False, True]
State prediction error at timestep 868 is tensor(0.0754, grad_fn=<MseLossBackward0>)
Current timestep = 869. State = [[0.02533384 0.30884486]]. Action = [[0.16181505 0.22994596 0.16031301 0.41242588]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 869 is [False, True, False, False, False, True]
Current timestep = 870. State = [[0.02416728 0.30561662]]. Action = [[-0.05349211 -0.23328139  0.22877002  0.21802056]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 870 is [False, True, False, False, False, True]
Current timestep = 871. State = [[0.02192481 0.29692826]]. Action = [[ 4.5910478e-04 -7.9987794e-03 -9.6119240e-02  6.5077376e-01]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 871 is [False, True, False, False, False, True]
Current timestep = 872. State = [[0.02168574 0.29441753]]. Action = [[0.03912026 0.07221895 0.06121877 0.6373844 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 872 is [False, True, False, False, False, True]
Scene graph at timestep 872 is [False, True, False, False, False, True]
State prediction error at timestep 872 is tensor(0.0617, grad_fn=<MseLossBackward0>)
Current timestep = 873. State = [[0.02386249 0.29879156]]. Action = [[ 0.0602963   0.17685518 -0.00980198 -0.96167463]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 873 is [False, True, False, False, False, True]
Scene graph at timestep 873 is [False, True, False, False, False, True]
State prediction error at timestep 873 is tensor(0.0857, grad_fn=<MseLossBackward0>)
Current timestep = 874. State = [[0.03006116 0.3080739 ]]. Action = [[ 0.22701216  0.11456141 -0.21930298  0.30074644]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 874 is [False, True, False, False, False, True]
Current timestep = 875. State = [[0.03881873 0.3120749 ]]. Action = [[-0.06192888 -0.179818   -0.00268634  0.6413734 ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 875 is [False, True, False, False, False, True]
Current timestep = 876. State = [[0.04368312 0.30823165]]. Action = [[ 0.21739414  0.00851762 -0.14778836 -0.9982169 ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 876 is [False, True, False, False, False, True]
Scene graph at timestep 876 is [False, True, False, False, False, True]
State prediction error at timestep 876 is tensor(0.0941, grad_fn=<MseLossBackward0>)
Current timestep = 877. State = [[0.04906971 0.30802128]]. Action = [[-0.20390332  0.1007548   0.10359704  0.81873894]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 877 is [False, True, False, False, False, True]
Current timestep = 878. State = [[0.04322793 0.3107062 ]]. Action = [[-0.095375   -0.04209779  0.12006426 -0.23263693]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 878 is [False, True, False, False, False, True]
Scene graph at timestep 878 is [False, True, False, False, False, True]
State prediction error at timestep 878 is tensor(0.0725, grad_fn=<MseLossBackward0>)
Current timestep = 879. State = [[0.03846898 0.30996805]]. Action = [[ 0.0676105  -0.04468462  0.1745879   0.17300022]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 879 is [False, True, False, False, False, True]
Current timestep = 880. State = [[0.04107865 0.30738932]]. Action = [[ 0.14130157 -0.05035625  0.18141729 -0.38552034]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 880 is [False, True, False, False, False, True]
Current timestep = 881. State = [[0.04363006 0.30494028]]. Action = [[-0.21479218  0.02929813  0.15331069 -0.30482107]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 881 is [False, True, False, False, False, True]
Current timestep = 882. State = [[0.03924371 0.3060099 ]]. Action = [[ 0.09785104  0.04741514 -0.17880939  0.07608569]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 882 is [False, True, False, False, False, True]
Scene graph at timestep 882 is [False, True, False, False, False, True]
State prediction error at timestep 882 is tensor(0.0709, grad_fn=<MseLossBackward0>)
Current timestep = 883. State = [[0.04036734 0.30793592]]. Action = [[-0.22916327  0.1760959  -0.20696525 -0.03199273]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 883 is [False, True, False, False, False, True]
Current timestep = 884. State = [[0.04459431 0.30550882]]. Action = [[ 0.23220068 -0.20583291  0.06277823  0.5820422 ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 884 is [False, True, False, False, False, True]
Current timestep = 885. State = [[0.05700247 0.29917285]]. Action = [[ 0.21972126  0.07458806  0.06660196 -0.7006631 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 885 is [False, True, False, False, False, True]
Current timestep = 886. State = [[0.06874494 0.29756626]]. Action = [[-0.01029046 -0.06897324  0.00338122 -0.12688214]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 886 is [False, False, True, False, False, True]
Scene graph at timestep 886 is [False, False, True, False, False, True]
State prediction error at timestep 886 is tensor(0.0679, grad_fn=<MseLossBackward0>)
Current timestep = 887. State = [[0.07321549 0.29330388]]. Action = [[ 0.02118602 -0.11584955 -0.1837655   0.7721385 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 887 is [False, False, True, False, False, True]
Current timestep = 888. State = [[0.07626337 0.2878614 ]]. Action = [[ 0.06717372 -0.01841904  0.08630487  0.246279  ]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 888 is [False, False, True, False, False, True]
Current timestep = 889. State = [[0.08247324 0.2852404 ]]. Action = [[ 0.1961869  -0.00580126 -0.13929293  0.06501865]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 889 is [False, False, True, False, False, True]
Current timestep = 890. State = [[0.08857771 0.28465882]]. Action = [[-0.16102211  0.05840707  0.24849278  0.20854795]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 890 is [False, False, True, False, False, True]
Current timestep = 891. State = [[0.0886571 0.2889286]]. Action = [[ 0.17786488  0.1302864  -0.14387167  0.72177327]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 891 is [False, False, True, False, False, True]
Current timestep = 892. State = [[0.09474512 0.29522148]]. Action = [[ 0.05866352  0.04312021 -0.2426775  -0.26254362]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 892 is [False, False, True, False, False, True]
Current timestep = 893. State = [[0.09997724 0.29786354]]. Action = [[ 0.05182296 -0.06393413  0.07139316  0.60408294]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 893 is [False, False, True, False, False, True]
Current timestep = 894. State = [[0.1032119  0.29889792]]. Action = [[-0.0270061   0.13508373 -0.02604957  0.23601127]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 894 is [False, False, True, False, False, True]
Scene graph at timestep 894 is [False, False, True, False, False, True]
State prediction error at timestep 894 is tensor(0.0712, grad_fn=<MseLossBackward0>)
Current timestep = 895. State = [[0.10254902 0.30394283]]. Action = [[-0.05664238  0.03479081  0.17798156 -0.7242096 ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 895 is [False, False, True, False, False, True]
Current timestep = 896. State = [[0.09775203 0.30540332]]. Action = [[-0.16287294 -0.09264097 -0.21911818 -0.66519666]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 896 is [False, False, True, False, False, True]
Current timestep = 897. State = [[0.08765732 0.30527815]]. Action = [[-0.19602224  0.139503    0.07635754 -0.76408756]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 897 is [False, False, True, False, False, True]
Scene graph at timestep 897 is [False, False, True, False, False, True]
State prediction error at timestep 897 is tensor(0.0887, grad_fn=<MseLossBackward0>)
Current timestep = 898. State = [[0.076203   0.30647278]]. Action = [[-0.07724032 -0.19515918  0.07257748  0.647159  ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 898 is [False, False, True, False, False, True]
Current timestep = 899. State = [[0.06576255 0.2972728 ]]. Action = [[-0.23254347 -0.21724492 -0.12604873 -0.5352886 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 899 is [False, False, True, False, False, True]
Current timestep = 900. State = [[0.0511915  0.28317416]]. Action = [[-0.20854913 -0.22390439 -0.10566157  0.42098212]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 900 is [False, False, True, False, False, True]
Current timestep = 901. State = [[0.04018061 0.26721966]]. Action = [[ 0.07899529 -0.2403522  -0.18614647  0.41597986]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 901 is [False, False, True, False, False, True]
Current timestep = 902. State = [[0.04260067 0.25638226]]. Action = [[ 0.23463523  0.15865663  0.00688726 -0.73376685]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 902 is [False, True, False, False, False, True]
Current timestep = 903. State = [[0.04862599 0.25776637]]. Action = [[-0.19839007  0.04185051  0.07277259  0.57125115]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 903 is [False, True, False, False, False, True]
Current timestep = 904. State = [[0.04762008 0.25797918]]. Action = [[ 0.18844742 -0.14833659 -0.07522601 -0.34267306]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 904 is [False, True, False, False, False, True]
Current timestep = 905. State = [[0.05076798 0.25608823]]. Action = [[-0.13914958  0.1822479   0.15513757  0.40900445]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 905 is [False, True, False, False, False, True]
Current timestep = 906. State = [[0.04522556 0.26138598]]. Action = [[-0.16551702  0.02674946 -0.04615697 -0.8688945 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 906 is [False, False, True, False, False, True]
Current timestep = 907. State = [[0.03434222 0.26283786]]. Action = [[-0.20149899 -0.10270865  0.14971578  0.7860532 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 907 is [False, True, False, False, False, True]
Current timestep = 908. State = [[0.02427108 0.262002  ]]. Action = [[ 0.04517156  0.11420849 -0.23973352  0.87451863]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 908 is [False, True, False, False, False, True]
Scene graph at timestep 908 is [False, True, False, False, False, True]
State prediction error at timestep 908 is tensor(0.0557, grad_fn=<MseLossBackward0>)
Current timestep = 909. State = [[0.02581633 0.26303267]]. Action = [[ 0.24747854 -0.13974017 -0.20337293  0.2814293 ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 909 is [False, True, False, False, False, True]
Current timestep = 910. State = [[0.03246451 0.25981784]]. Action = [[-0.15821259  0.06108397  0.15738568  0.6967299 ]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 910 is [False, True, False, False, False, True]
Current timestep = 911. State = [[0.03378993 0.2596169 ]]. Action = [[ 0.24133879 -0.06331581  0.23625469  0.25446916]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 911 is [False, True, False, False, False, True]
Current timestep = 912. State = [[0.0450439  0.25784168]]. Action = [[ 0.22432053  0.01319566 -0.00951742  0.0655055 ]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 912 is [False, True, False, False, False, True]
Current timestep = 913. State = [[0.05939107 0.25986886]]. Action = [[ 0.16949677  0.15459746 -0.03751822 -0.93427217]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 913 is [False, True, False, False, False, True]
Current timestep = 914. State = [[0.07057644 0.2622386 ]]. Action = [[ 0.02258164 -0.20184617 -0.17720371 -0.5293332 ]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 914 is [False, False, True, False, False, True]
Current timestep = 915. State = [[0.07853812 0.2597266 ]]. Action = [[0.20295542 0.16301167 0.06576949 0.6361582 ]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 915 is [False, False, True, False, False, True]
Current timestep = 916. State = [[0.08904412 0.26319766]]. Action = [[ 0.08872113 -0.01945874 -0.18577269  0.35799026]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 916 is [False, False, True, False, False, True]
Current timestep = 917. State = [[0.09220002 0.26171964]]. Action = [[-0.24475455 -0.12842096  0.04122373  0.50291705]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 917 is [False, False, True, False, False, True]
Current timestep = 918. State = [[0.08426567 0.26105013]]. Action = [[-0.06466556  0.22468126 -0.12681732  0.2557869 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 918 is [False, False, True, False, False, True]
Current timestep = 919. State = [[0.07743164 0.2672521 ]]. Action = [[-0.09233838 -0.03333969  0.04118294 -0.7058575 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 919 is [False, False, True, False, False, True]
Current timestep = 920. State = [[0.06990634 0.2697119 ]]. Action = [[-0.13672659  0.06046119 -0.20318237  0.5819497 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 920 is [False, False, True, False, False, True]
Current timestep = 921. State = [[0.06488326 0.27653202]]. Action = [[ 0.14946699  0.24472284  0.17324525 -0.27533615]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 921 is [False, False, True, False, False, True]
Current timestep = 922. State = [[0.06984519 0.28279465]]. Action = [[ 0.11876088 -0.24160852 -0.10641053 -0.7038536 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 922 is [False, False, True, False, False, True]
Current timestep = 923. State = [[0.07824512 0.2744338 ]]. Action = [[ 0.15164065 -0.19655567 -0.1415833  -0.28851628]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 923 is [False, False, True, False, False, True]
Current timestep = 924. State = [[0.08739381 0.26682085]]. Action = [[ 0.07511598  0.13965034 -0.16401777 -0.7354646 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 924 is [False, False, True, False, False, True]
Current timestep = 925. State = [[0.09631795 0.27227908]]. Action = [[ 0.2079007   0.23289442  0.02589852 -0.88165176]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 925 is [False, False, True, False, False, True]
Current timestep = 926. State = [[0.10350309 0.28368273]]. Action = [[-0.16343679  0.13974637  0.02130932  0.9782543 ]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 926 is [False, False, True, False, False, True]
Current timestep = 927. State = [[0.09807532 0.29247433]]. Action = [[-0.19877687 -0.01341039  0.0447197   0.484581  ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 927 is [False, False, True, False, False, True]
Current timestep = 928. State = [[0.08792765 0.2961081 ]]. Action = [[-0.08160713  0.02422377  0.17667443 -0.89557725]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 928 is [False, False, True, False, False, True]
Current timestep = 929. State = [[0.07805133 0.29609942]]. Action = [[-0.22011717 -0.10315856 -0.04542917  0.3785901 ]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 929 is [False, False, True, False, False, True]
Scene graph at timestep 929 is [False, False, True, False, False, True]
State prediction error at timestep 929 is tensor(0.0607, grad_fn=<MseLossBackward0>)
Current timestep = 930. State = [[0.06434175 0.29406962]]. Action = [[-0.17971937  0.06419936  0.22911704 -0.9348801 ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 930 is [False, False, True, False, False, True]
Current timestep = 931. State = [[0.05251644 0.29694653]]. Action = [[-0.06557979  0.10946566  0.11554968  0.8469348 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 931 is [False, False, True, False, False, True]
Current timestep = 932. State = [[0.04911664 0.29863212]]. Action = [[ 0.17210513 -0.18408637 -0.1864234  -0.48603934]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 932 is [False, False, True, False, False, True]
Scene graph at timestep 932 is [False, True, False, False, False, True]
State prediction error at timestep 932 is tensor(0.0731, grad_fn=<MseLossBackward0>)
Current timestep = 933. State = [[0.05512457 0.2929583 ]]. Action = [[ 0.10062742 -0.02086207  0.054335   -0.39977723]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 933 is [False, True, False, False, False, True]
Scene graph at timestep 933 is [False, False, True, False, False, True]
State prediction error at timestep 933 is tensor(0.0716, grad_fn=<MseLossBackward0>)
Current timestep = 934. State = [[0.05948767 0.2882224 ]]. Action = [[-0.10289562 -0.0994454   0.00333804 -0.7556489 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 934 is [False, False, True, False, False, True]
Current timestep = 935. State = [[0.05436083 0.2830828 ]]. Action = [[-0.21375854 -0.02325247 -0.13222483  0.51018786]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 935 is [False, False, True, False, False, True]
Current timestep = 936. State = [[0.04322349 0.28329733]]. Action = [[-0.1105749   0.16714293  0.14169556  0.68021655]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 936 is [False, False, True, False, False, True]
Current timestep = 937. State = [[0.035286   0.28828135]]. Action = [[-0.02501278 -0.02820922  0.10617253  0.46054518]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 937 is [False, True, False, False, False, True]
Current timestep = 938. State = [[0.03410742 0.28999558]]. Action = [[0.16817862 0.03292024 0.05000344 0.906888  ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 938 is [False, True, False, False, False, True]
Current timestep = 939. State = [[0.03908253 0.2904427 ]]. Action = [[-0.03332239 -0.05551463 -0.18841499 -0.11340064]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 939 is [False, True, False, False, False, True]
Current timestep = 940. State = [[0.03937199 0.2897382 ]]. Action = [[-0.04200074  0.05169469  0.11237186  0.80609727]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 940 is [False, True, False, False, False, True]
Current timestep = 941. State = [[0.03597641 0.28748447]]. Action = [[-0.14348629 -0.22083147  0.10088518 -0.46496332]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 941 is [False, True, False, False, False, True]
Current timestep = 942. State = [[0.0283699  0.28316453]]. Action = [[-0.0627044   0.2113001   0.15425164 -0.29688525]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 942 is [False, True, False, False, False, True]
Current timestep = 943. State = [[0.0262741  0.29149875]]. Action = [[ 0.18657482  0.21069616 -0.17159872  0.7790005 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 943 is [False, True, False, False, False, True]
Current timestep = 944. State = [[0.03423223 0.30202985]]. Action = [[0.15139627 0.02947879 0.17005312 0.97877383]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 944 is [False, True, False, False, False, True]
Current timestep = 945. State = [[0.04215362 0.30586922]]. Action = [[-0.03089152 -0.06099382  0.1202085  -0.45056152]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 945 is [False, True, False, False, False, True]
Scene graph at timestep 945 is [False, True, False, False, False, True]
State prediction error at timestep 945 is tensor(0.0738, grad_fn=<MseLossBackward0>)
Current timestep = 946. State = [[0.04467477 0.30441022]]. Action = [[ 0.03903702 -0.05241822 -0.09905931  0.8735049 ]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 946 is [False, True, False, False, False, True]
Current timestep = 947. State = [[0.04745915 0.29829818]]. Action = [[ 0.04796076 -0.23297182 -0.14455545 -0.3469535 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 947 is [False, True, False, False, False, True]
Scene graph at timestep 947 is [False, True, False, False, False, True]
State prediction error at timestep 947 is tensor(0.0680, grad_fn=<MseLossBackward0>)
Current timestep = 948. State = [[0.05141976 0.28942978]]. Action = [[ 0.11245215  0.04885823  0.24496692 -0.02904272]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 948 is [False, True, False, False, False, True]
Current timestep = 949. State = [[0.05486742 0.28862947]]. Action = [[-0.12173487  0.07942715 -0.18169376 -0.28634965]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 949 is [False, False, True, False, False, True]
Current timestep = 950. State = [[0.0535699  0.29180261]]. Action = [[ 0.08946708  0.02678379  0.06734818 -0.7555905 ]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 950 is [False, False, True, False, False, True]
Current timestep = 951. State = [[0.05870898 0.29614332]]. Action = [[ 0.20500782  0.13607997 -0.04448339  0.6757114 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 951 is [False, False, True, False, False, True]
Current timestep = 952. State = [[0.06486416 0.29796255]]. Action = [[-0.18560931 -0.22896779 -0.19237332  0.6195617 ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 952 is [False, False, True, False, False, True]
Current timestep = 953. State = [[0.05839717 0.29182148]]. Action = [[-0.17342749  0.00954446  0.16549075 -0.21495974]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 953 is [False, False, True, False, False, True]
Current timestep = 954. State = [[0.05010016 0.29319432]]. Action = [[ 0.02727294  0.21648633 -0.18629898  0.38285136]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 954 is [False, False, True, False, False, True]
Current timestep = 955. State = [[0.04840949 0.3003257 ]]. Action = [[ 0.00635216 -0.01267996 -0.12882929 -0.7048568 ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 955 is [False, False, True, False, False, True]
Current timestep = 956. State = [[0.04940981 0.30278662]]. Action = [[ 0.0791567  -0.00494607 -0.17875902  0.51603174]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 956 is [False, True, False, False, False, True]
Scene graph at timestep 956 is [False, True, False, False, False, True]
State prediction error at timestep 956 is tensor(0.0659, grad_fn=<MseLossBackward0>)
Current timestep = 957. State = [[0.05241343 0.30327445]]. Action = [[0.1861026  0.24457377 0.22512609 0.6951506 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 957 is [False, True, False, False, False, True]
Current timestep = 958. State = [[0.05671464 0.3036508 ]]. Action = [[0.21013573 0.00240952 0.15890634 0.50041604]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 958 is [False, False, True, False, False, True]
Current timestep = 959. State = [[0.06742498 0.30288833]]. Action = [[ 0.15603447 -0.05683945  0.17393774  0.04101753]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 959 is [False, False, True, False, False, True]
Current timestep = 960. State = [[0.07662863 0.30066383]]. Action = [[ 0.21102893  0.20381448 -0.16745748 -0.523966  ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 960 is [False, False, True, False, False, True]
Current timestep = 961. State = [[0.08076158 0.30306152]]. Action = [[ 0.0515385   0.19801748  0.09321088 -0.8064507 ]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 961 is [False, False, True, False, False, True]
Current timestep = 962. State = [[0.08218895 0.3067924 ]]. Action = [[-0.10659687 -0.1979469   0.15115869 -0.21784282]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 962 is [False, False, True, False, False, True]
Current timestep = 963. State = [[0.07912487 0.29958022]]. Action = [[-0.0049369  -0.17207031  0.23353115 -0.13169128]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 963 is [False, False, True, False, False, True]
Current timestep = 964. State = [[0.07695243 0.29326445]]. Action = [[-0.03755513  0.13486218  0.20630246  0.84701204]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 964 is [False, False, True, False, False, True]
Scene graph at timestep 964 is [False, False, True, False, False, True]
State prediction error at timestep 964 is tensor(0.0712, grad_fn=<MseLossBackward0>)
Current timestep = 965. State = [[0.07886676 0.29679537]]. Action = [[0.24748194 0.08370766 0.19144687 0.96536326]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 965 is [False, False, True, False, False, True]
Scene graph at timestep 965 is [False, False, True, False, False, True]
State prediction error at timestep 965 is tensor(0.0746, grad_fn=<MseLossBackward0>)
Current timestep = 966. State = [[0.09016252 0.30244684]]. Action = [[0.17791143 0.09366718 0.23576272 0.1969577 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 966 is [False, False, True, False, False, True]
Scene graph at timestep 966 is [False, False, True, False, False, True]
State prediction error at timestep 966 is tensor(0.0722, grad_fn=<MseLossBackward0>)
Current timestep = 967. State = [[0.10223882 0.3050553 ]]. Action = [[ 0.12475586 -0.14568742  0.03641868  0.75828576]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 967 is [False, False, True, False, False, True]
Current timestep = 968. State = [[0.1108837  0.30124193]]. Action = [[-0.12029368  0.17186922 -0.2441596   0.5653417 ]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 968 is [False, False, True, False, False, True]
Current timestep = 969. State = [[0.11792988 0.29825974]]. Action = [[ 0.24314448 -0.11437982  0.23540515 -0.12988919]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 969 is [False, False, True, False, False, True]
Current timestep = 970. State = [[0.12547228 0.29347715]]. Action = [[-0.15224019  0.04082474  0.00165352 -0.80751985]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 970 is [False, False, True, False, False, True]
Scene graph at timestep 970 is [False, False, True, False, False, True]
State prediction error at timestep 970 is tensor(0.0933, grad_fn=<MseLossBackward0>)
Current timestep = 971. State = [[0.12324631 0.29553217]]. Action = [[-0.03619979  0.13804504 -0.03965405  0.8585055 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 971 is [False, False, True, False, False, True]
Scene graph at timestep 971 is [False, False, True, False, False, True]
State prediction error at timestep 971 is tensor(0.0762, grad_fn=<MseLossBackward0>)
Current timestep = 972. State = [[0.12161219 0.3046793 ]]. Action = [[ 0.04297778  0.22351637  0.09879351 -0.01785398]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 972 is [False, False, True, False, False, True]
Current timestep = 973. State = [[0.12455798 0.31598198]]. Action = [[ 0.1587115   0.04740503  0.23632091 -0.5259989 ]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 973 is [False, False, True, False, False, True]
Current timestep = 974. State = [[0.13127267 0.3215006 ]]. Action = [[ 0.03398871  0.09020093 -0.14074798  0.20189917]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 974 is [False, False, True, False, False, True]
Current timestep = 975. State = [[0.13789931 0.32365835]]. Action = [[-0.06943879 -0.07347786 -0.15899692 -0.28144813]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 975 is [False, False, True, False, False, True]
Current timestep = 976. State = [[0.14420445 0.32403272]]. Action = [[-0.24226841 -0.0075376   0.11905679  0.55649614]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 976 is [False, False, True, False, False, True]
Scene graph at timestep 976 is [False, False, True, False, False, True]
State prediction error at timestep 976 is tensor(0.0841, grad_fn=<MseLossBackward0>)
Current timestep = 977. State = [[0.14989439 0.32181853]]. Action = [[-0.02528073 -0.23105972  0.23494285 -0.50125897]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 977 is [False, False, True, False, False, True]
Current timestep = 978. State = [[0.15557635 0.31577638]]. Action = [[ 0.23945421  0.05897823  0.0269928  -0.3578788 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 978 is [False, False, True, False, False, True]
Current timestep = 979. State = [[0.15840605 0.31408986]]. Action = [[ 0.2419082   0.10520801 -0.00994778 -0.45464778]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 979 is [False, False, True, False, False, True]
Current timestep = 980. State = [[0.15887526 0.31177828]]. Action = [[-0.02981311 -0.16319676  0.21387708  0.31549096]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 980 is [False, False, True, False, False, True]
Scene graph at timestep 980 is [False, False, True, False, False, True]
State prediction error at timestep 980 is tensor(0.0777, grad_fn=<MseLossBackward0>)
Current timestep = 981. State = [[0.15934452 0.30243936]]. Action = [[ 0.19496489 -0.09659098 -0.09621924 -0.6364405 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 981 is [False, False, True, False, False, True]
Current timestep = 982. State = [[0.15837878 0.2934236 ]]. Action = [[-0.10695472 -0.04282087  0.12601244  0.89997673]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 982 is [False, False, True, False, False, True]
Scene graph at timestep 982 is [False, False, True, False, False, True]
State prediction error at timestep 982 is tensor(0.0801, grad_fn=<MseLossBackward0>)
Current timestep = 983. State = [[0.14393698 0.28797713]]. Action = [[ 0.04304075  0.19157127  0.18129122 -0.6600516 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 983 is [False, False, True, False, False, True]
Current timestep = 984. State = [[0.14160031 0.2902586 ]]. Action = [[ 0.18664187 -0.00988048  0.15726012 -0.83267987]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 984 is [False, False, True, False, False, True]
Current timestep = 985. State = [[0.14437674 0.2906792 ]]. Action = [[-0.08399031 -0.04492794  0.18506843  0.8324299 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 985 is [False, False, True, False, False, True]
Current timestep = 986. State = [[0.14375141 0.28880247]]. Action = [[ 0.01090425 -0.04267991 -0.2378129   0.7571852 ]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 986 is [False, False, True, False, False, True]
Current timestep = 987. State = [[0.14645562 0.29058608]]. Action = [[0.13484961 0.21129471 0.15087724 0.91074586]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 987 is [False, False, True, False, False, True]
Current timestep = 988. State = [[0.14614072 0.2939005 ]]. Action = [[-0.23783794 -0.18937573 -0.24440108  0.5027149 ]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 988 is [False, False, True, False, False, True]
Current timestep = 989. State = [[0.14187683 0.2880567 ]]. Action = [[ 0.09979767 -0.10339439  0.15369582  0.14787495]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 989 is [False, False, True, False, False, True]
Scene graph at timestep 989 is [False, False, True, False, False, True]
State prediction error at timestep 989 is tensor(0.0709, grad_fn=<MseLossBackward0>)
Current timestep = 990. State = [[0.14328171 0.28238088]]. Action = [[ 0.02622879 -0.00879367  0.19000661 -0.08527881]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 990 is [False, False, True, False, False, True]
Current timestep = 991. State = [[0.14322771 0.2772182 ]]. Action = [[-0.06911469 -0.15781231 -0.02252246  0.650923  ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 991 is [False, False, True, False, False, True]
Scene graph at timestep 991 is [False, False, True, False, False, True]
State prediction error at timestep 991 is tensor(0.0640, grad_fn=<MseLossBackward0>)
Current timestep = 992. State = [[0.14014821 0.27171645]]. Action = [[-0.07842283  0.06074503  0.05098653  0.6611495 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 992 is [False, False, True, False, False, True]
Current timestep = 993. State = [[0.14068167 0.27050894]]. Action = [[ 0.23024642 -0.06301259 -0.11175896 -0.692278  ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 993 is [False, False, True, False, False, True]
Current timestep = 994. State = [[0.14467838 0.27096096]]. Action = [[-0.15011315  0.15958983  0.00884306  0.4478165 ]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 994 is [False, False, True, False, False, True]
Current timestep = 995. State = [[0.13814704 0.27428216]]. Action = [[-0.21540794 -0.08630547 -0.1887196  -0.5358048 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 995 is [False, False, True, False, False, True]
Scene graph at timestep 995 is [False, False, True, False, False, True]
State prediction error at timestep 995 is tensor(0.0812, grad_fn=<MseLossBackward0>)
Current timestep = 996. State = [[0.12980522 0.27332807]]. Action = [[ 0.01750785  0.00547904 -0.06029391 -0.5991376 ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 996 is [False, False, True, False, False, True]
Current timestep = 997. State = [[0.1229495 0.2701642]]. Action = [[-0.2122466  -0.15422596 -0.01901317  0.7437209 ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 997 is [False, False, True, False, False, True]
Current timestep = 998. State = [[0.11013123 0.2603967 ]]. Action = [[-0.23802093 -0.228882   -0.18587928  0.5565076 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 998 is [False, False, True, False, False, True]
Current timestep = 999. State = [[0.10077523 0.24756502]]. Action = [[ 0.14089906 -0.13142313 -0.04008768  0.8191378 ]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 999 is [False, False, True, False, False, True]
Current timestep = 1000. State = [[0.10033394 0.23596881]]. Action = [[-0.05210929 -0.1621607  -0.13272426  0.36910772]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 1000 is [False, False, True, False, False, True]
Scene graph at timestep 1000 is [False, False, True, False, False, True]
State prediction error at timestep 1000 is tensor(0.0438, grad_fn=<MseLossBackward0>)
Current timestep = 1001. State = [[0.09737924 0.22341958]]. Action = [[-0.09653629 -0.20266289  0.13992304 -0.37766755]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 1001 is [False, False, True, False, False, True]
Current timestep = 1002. State = [[0.09479333 0.21185346]]. Action = [[ 0.07413048 -0.04800558 -0.19679922  0.7019793 ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 1002 is [False, False, True, False, False, True]
Current timestep = 1003. State = [[0.0954062  0.20195188]]. Action = [[-0.01941752 -0.24531704  0.19900268  0.9466896 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 1003 is [False, False, True, False, False, True]
Current timestep = 1004. State = [[0.09350502 0.18999216]]. Action = [[-0.1104655  -0.05718595  0.05351287  0.22634363]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 1004 is [False, False, True, False, False, True]
Current timestep = 1005. State = [[0.08773652 0.18054867]]. Action = [[-0.09544334 -0.18650396 -0.10028607 -0.77004814]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 1005 is [False, False, True, False, False, True]
Current timestep = 1006. State = [[0.08429763 0.1698892 ]]. Action = [[ 0.09799284 -0.09765461 -0.09791805  0.87841535]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 1006 is [False, False, True, False, False, True]
Scene graph at timestep 1006 is [False, False, True, False, False, True]
State prediction error at timestep 1006 is tensor(0.0293, grad_fn=<MseLossBackward0>)
Current timestep = 1007. State = [[0.08504883 0.16158958]]. Action = [[-0.0513546  -0.07820642  0.15981203  0.90425086]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 1007 is [False, False, True, False, False, True]
Scene graph at timestep 1007 is [False, False, True, False, False, True]
State prediction error at timestep 1007 is tensor(0.0283, grad_fn=<MseLossBackward0>)
Current timestep = 1008. State = [[0.08532613 0.15829903]]. Action = [[ 0.06813496  0.1249063  -0.16486663 -0.6225882 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 1008 is [False, False, True, False, False, True]
Current timestep = 1009. State = [[0.0913077 0.163275 ]]. Action = [[ 0.2372894   0.13616425 -0.00514579  0.76703167]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 1009 is [False, False, True, False, False, True]
Current timestep = 1010. State = [[0.1009294  0.16719228]]. Action = [[ 0.02547789 -0.1399797   0.02649245 -0.68790007]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 1010 is [False, False, True, False, False, True]
Current timestep = 1011. State = [[0.1089927  0.16566454]]. Action = [[ 0.18578947  0.07096726 -0.04145974 -0.03772992]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 1011 is [False, False, True, False, False, True]
Current timestep = 1012. State = [[0.11846492 0.16745579]]. Action = [[0.0674265  0.02691004 0.022145   0.42734635]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 1012 is [False, False, True, False, False, True]
Current timestep = 1013. State = [[0.12017001 0.16978212]]. Action = [[-0.2365455   0.05108231  0.10827485 -0.2018624 ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 1013 is [False, False, True, False, False, True]
Scene graph at timestep 1013 is [False, False, True, False, False, True]
State prediction error at timestep 1013 is tensor(0.0399, grad_fn=<MseLossBackward0>)
Current timestep = 1014. State = [[0.11690139 0.17356454]]. Action = [[ 0.18780446  0.07414898 -0.03387427 -0.86963993]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 1014 is [False, False, True, False, False, True]
Current timestep = 1015. State = [[0.12254944 0.17675015]]. Action = [[ 0.10777187 -0.03225845  0.2417413   0.16432142]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 1015 is [False, False, True, False, False, True]
Scene graph at timestep 1015 is [False, False, True, False, False, True]
State prediction error at timestep 1015 is tensor(0.0351, grad_fn=<MseLossBackward0>)
Current timestep = 1016. State = [[0.12766548 0.18047276]]. Action = [[-0.04461022  0.21577626 -0.04856074 -0.79304975]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 1016 is [False, False, True, False, False, True]
Scene graph at timestep 1016 is [False, False, True, False, False, True]
State prediction error at timestep 1016 is tensor(0.0616, grad_fn=<MseLossBackward0>)
Current timestep = 1017. State = [[0.13111207 0.19204651]]. Action = [[ 0.1665273   0.21688369  0.237385   -0.05441201]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 1017 is [False, False, True, False, False, True]
Current timestep = 1018. State = [[0.13652207 0.20140663]]. Action = [[-0.01985373 -0.10380425  0.05703592  0.12224686]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 1018 is [False, False, True, False, False, True]
Scene graph at timestep 1018 is [False, False, True, False, False, True]
State prediction error at timestep 1018 is tensor(0.0405, grad_fn=<MseLossBackward0>)
Current timestep = 1019. State = [[0.1350367  0.20333879]]. Action = [[-0.20183432  0.10326961 -0.20692989 -0.6694067 ]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 1019 is [False, False, True, False, False, True]
Current timestep = 1020. State = [[0.12458365 0.21042931]]. Action = [[-0.2233406   0.2102519  -0.17348129 -0.6760888 ]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 1020 is [False, False, True, False, False, True]
Scene graph at timestep 1020 is [False, False, True, False, False, True]
State prediction error at timestep 1020 is tensor(0.0679, grad_fn=<MseLossBackward0>)
Current timestep = 1021. State = [[0.11217494 0.220348  ]]. Action = [[-0.075095    0.0353522  -0.20935209 -0.66972756]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 1021 is [False, False, True, False, False, True]
Scene graph at timestep 1021 is [False, False, True, False, False, True]
State prediction error at timestep 1021 is tensor(0.0658, grad_fn=<MseLossBackward0>)
Current timestep = 1022. State = [[0.1046583  0.22403519]]. Action = [[-0.02668309 -0.07179314 -0.23253118  0.25382912]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 1022 is [False, False, True, False, False, True]
Current timestep = 1023. State = [[0.100417   0.22117043]]. Action = [[-0.0488961  -0.11048445  0.10936648 -0.51790446]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 1023 is [False, False, True, False, False, True]
Current timestep = 1024. State = [[0.09671667 0.21752228]]. Action = [[-0.05452575  0.05969894 -0.17044792 -0.04514879]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 1024 is [False, False, True, False, False, True]
Scene graph at timestep 1024 is [False, False, True, False, False, True]
State prediction error at timestep 1024 is tensor(0.0479, grad_fn=<MseLossBackward0>)
Current timestep = 1025. State = [[0.09616397 0.21936812]]. Action = [[0.14668769 0.0818826  0.24740797 0.53611016]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 1025 is [False, False, True, False, False, True]
Current timestep = 1026. State = [[0.10179827 0.22169417]]. Action = [[ 0.09437549 -0.06424347  0.09508276 -0.7047712 ]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 1026 is [False, False, True, False, False, True]
Scene graph at timestep 1026 is [False, False, True, False, False, True]
State prediction error at timestep 1026 is tensor(0.0619, grad_fn=<MseLossBackward0>)
Current timestep = 1027. State = [[0.10673022 0.22216912]]. Action = [[-0.0247163   0.09519869 -0.0375213  -0.07900125]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 1027 is [False, False, True, False, False, True]
Current timestep = 1028. State = [[0.11038768 0.22461466]]. Action = [[ 0.15438232 -0.03695545  0.13812202  0.5770025 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 1028 is [False, False, True, False, False, True]
Scene graph at timestep 1028 is [False, False, True, False, False, True]
State prediction error at timestep 1028 is tensor(0.0425, grad_fn=<MseLossBackward0>)
Current timestep = 1029. State = [[0.11860225 0.22578111]]. Action = [[ 0.13807371  0.07499588  0.14421213 -0.56859726]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 1029 is [False, False, True, False, False, True]
Scene graph at timestep 1029 is [False, False, True, False, False, True]
State prediction error at timestep 1029 is tensor(0.0650, grad_fn=<MseLossBackward0>)
Current timestep = 1030. State = [[0.12644725 0.22759308]]. Action = [[ 0.03299397 -0.04646242 -0.03297901  0.5714809 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 1030 is [False, False, True, False, False, True]
Current timestep = 1031. State = [[0.12802431 0.22945707]]. Action = [[-0.15701078  0.16716701 -0.13674909 -0.02139705]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 1031 is [False, False, True, False, False, True]
Scene graph at timestep 1031 is [False, False, True, False, False, True]
State prediction error at timestep 1031 is tensor(0.0569, grad_fn=<MseLossBackward0>)
Current timestep = 1032. State = [[0.12390012 0.23784614]]. Action = [[ 0.0104616   0.15071523 -0.02436242 -0.4935966 ]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 1032 is [False, False, True, False, False, True]
Current timestep = 1033. State = [[0.11832013 0.24170208]]. Action = [[-0.22493854 -0.23832855  0.22518206 -0.24315941]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 1033 is [False, False, True, False, False, True]
Scene graph at timestep 1033 is [False, False, True, False, False, True]
State prediction error at timestep 1033 is tensor(0.0544, grad_fn=<MseLossBackward0>)
Current timestep = 1034. State = [[0.1127472  0.23260996]]. Action = [[ 0.19455862 -0.20890595 -0.19673727  0.7948768 ]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 1034 is [False, False, True, False, False, True]
Scene graph at timestep 1034 is [False, False, True, False, False, True]
State prediction error at timestep 1034 is tensor(0.0497, grad_fn=<MseLossBackward0>)
Current timestep = 1035. State = [[0.1132153  0.22292307]]. Action = [[-0.20729965  0.04853526  0.02131313 -0.04008788]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 1035 is [False, False, True, False, False, True]
Current timestep = 1036. State = [[0.10810116 0.2179094 ]]. Action = [[ 0.04760092 -0.1819466   0.083698    0.34697163]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 1036 is [False, False, True, False, False, True]
Scene graph at timestep 1036 is [False, False, True, False, False, True]
State prediction error at timestep 1036 is tensor(0.0383, grad_fn=<MseLossBackward0>)
Current timestep = 1037. State = [[0.10697164 0.21305849]]. Action = [[-0.032334    0.1480267   0.19844782 -0.33567202]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 1037 is [False, False, True, False, False, True]
Current timestep = 1038. State = [[0.10716876 0.2152976 ]]. Action = [[ 0.0941543  -0.03385544  0.2076298   0.53323567]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 1038 is [False, False, True, False, False, True]
Scene graph at timestep 1038 is [False, False, True, False, False, True]
State prediction error at timestep 1038 is tensor(0.0402, grad_fn=<MseLossBackward0>)
Current timestep = 1039. State = [[0.10972735 0.21254086]]. Action = [[-0.02112463 -0.17000477  0.00395271 -0.6106459 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 1039 is [False, False, True, False, False, True]
Scene graph at timestep 1039 is [False, False, True, False, False, True]
State prediction error at timestep 1039 is tensor(0.0568, grad_fn=<MseLossBackward0>)
Current timestep = 1040. State = [[0.11255276 0.20383362]]. Action = [[ 0.14529133 -0.14627953 -0.00410476 -0.12160766]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 1040 is [False, False, True, False, False, True]
Current timestep = 1041. State = [[0.11553192 0.19398087]]. Action = [[-0.15930644 -0.10007884 -0.24836518  0.56736994]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 1041 is [False, False, True, False, False, True]
Current timestep = 1042. State = [[0.10936146 0.1842198 ]]. Action = [[-0.14884102 -0.18313694 -0.06400695  0.3312087 ]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 1042 is [False, False, True, False, False, True]
Current timestep = 1043. State = [[0.10286431 0.1758786 ]]. Action = [[ 0.04960537  0.04780263 -0.12643188 -0.55158883]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 1043 is [False, False, True, False, False, True]
Current timestep = 1044. State = [[0.10599574 0.17836565]]. Action = [[ 0.24964505  0.24513087 -0.001463   -0.29405463]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 1044 is [False, False, True, False, False, True]
Current timestep = 1045. State = [[0.11586817 0.1853443 ]]. Action = [[ 0.05169421 -0.10999562  0.01187018  0.9547901 ]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 1045 is [False, False, True, False, False, True]
Current timestep = 1046. State = [[0.11930627 0.18395686]]. Action = [[-0.14918701 -0.03319894 -0.03181219  0.19046116]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 1046 is [False, False, True, False, False, True]
Scene graph at timestep 1046 is [False, False, True, False, False, True]
State prediction error at timestep 1046 is tensor(0.0339, grad_fn=<MseLossBackward0>)
Current timestep = 1047. State = [[0.11687447 0.17956226]]. Action = [[ 0.0767206  -0.17603745 -0.15216215 -0.0634523 ]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 1047 is [False, False, True, False, False, True]
Current timestep = 1048. State = [[0.11896201 0.17076603]]. Action = [[ 0.05458444 -0.09934023 -0.0171493  -0.45328498]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 1048 is [False, False, True, False, False, True]
Current timestep = 1049. State = [[0.12501223 0.16758795]]. Action = [[ 0.20938084  0.20584759 -0.19336836  0.5564741 ]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 1049 is [False, False, True, False, False, True]
Scene graph at timestep 1049 is [False, False, True, False, False, True]
State prediction error at timestep 1049 is tensor(0.0336, grad_fn=<MseLossBackward0>)
Current timestep = 1050. State = [[0.13181746 0.17116491]]. Action = [[-0.12754673 -0.09228536  0.04671645  0.22185814]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 1050 is [False, False, True, False, False, True]
Current timestep = 1051. State = [[0.13087349 0.16810212]]. Action = [[ 0.02246365 -0.12852694  0.05210733  0.99601007]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 1051 is [False, False, True, False, False, True]
Current timestep = 1052. State = [[0.1299354  0.16524607]]. Action = [[-0.06354329  0.1642847   0.16756624 -0.66212404]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 1052 is [False, False, True, False, False, True]
Current timestep = 1053. State = [[0.1245105  0.16777977]]. Action = [[-0.19590612 -0.08961695  0.05803925  0.6744528 ]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 1053 is [False, False, True, False, False, True]
Current timestep = 1054. State = [[0.11602242 0.16927513]]. Action = [[-0.00133274  0.18026465  0.09796122 -0.01992548]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 1054 is [False, False, True, False, False, True]
Current timestep = 1055. State = [[0.11251253 0.17417969]]. Action = [[-0.01738577 -0.07279378 -0.01548257  0.3158083 ]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 1055 is [False, False, True, False, False, True]
Scene graph at timestep 1055 is [False, False, True, False, False, True]
State prediction error at timestep 1055 is tensor(0.0290, grad_fn=<MseLossBackward0>)
Current timestep = 1056. State = [[0.11233728 0.1754887 ]]. Action = [[ 0.09648231  0.09794173 -0.18412912 -0.9458256 ]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 1056 is [False, False, True, False, False, True]
Current timestep = 1057. State = [[0.11921601 0.17928477]]. Action = [[0.24724227 0.02092546 0.17247438 0.3293903 ]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 1057 is [False, False, True, False, False, True]
Current timestep = 1058. State = [[0.12804438 0.1784558 ]]. Action = [[-0.11951004 -0.15546103  0.01826495 -0.4087019 ]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 1058 is [False, False, True, False, False, True]
Current timestep = 1059. State = [[0.12597209 0.17456095]]. Action = [[-0.11246455  0.07655728  0.11566561 -0.12226999]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 1059 is [False, False, True, False, False, True]
Current timestep = 1060. State = [[0.11728355 0.17778996]]. Action = [[-0.24805379  0.14960188 -0.11207709  0.93935084]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 1060 is [False, False, True, False, False, True]
Current timestep = 1061. State = [[0.10358076 0.18549451]]. Action = [[-0.14559957  0.08112615 -0.08184718 -0.31689984]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 1061 is [False, False, True, False, False, True]
Scene graph at timestep 1061 is [False, False, True, False, False, True]
State prediction error at timestep 1061 is tensor(0.0443, grad_fn=<MseLossBackward0>)
Current timestep = 1062. State = [[0.09119671 0.18756433]]. Action = [[-0.16978453 -0.21381712  0.08066726 -0.10924006]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 1062 is [False, False, True, False, False, True]
Scene graph at timestep 1062 is [False, False, True, False, False, True]
State prediction error at timestep 1062 is tensor(0.0333, grad_fn=<MseLossBackward0>)
Current timestep = 1063. State = [[0.07951376 0.18175755]]. Action = [[-0.09589723  0.01901245 -0.24027951  0.1677171 ]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 1063 is [False, False, True, False, False, True]
Current timestep = 1064. State = [[0.07132484 0.18360013]]. Action = [[-0.05856547  0.23905635 -0.19466557  0.474051  ]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 1064 is [False, False, True, False, False, True]
Scene graph at timestep 1064 is [False, False, True, False, False, True]
State prediction error at timestep 1064 is tensor(0.0340, grad_fn=<MseLossBackward0>)
Current timestep = 1065. State = [[0.06604458 0.19554472]]. Action = [[-0.04436912  0.22032803 -0.12839729  0.06256223]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 1065 is [False, False, True, False, False, True]
Current timestep = 1066. State = [[0.06257114 0.2083896 ]]. Action = [[-0.02008457  0.07056838 -0.07851422  0.4917779 ]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 1066 is [False, False, True, False, False, True]
Current timestep = 1067. State = [[0.05868987 0.21775497]]. Action = [[-0.13312076  0.1550479   0.09246832  0.33800828]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 1067 is [False, False, True, False, False, True]
Current timestep = 1068. State = [[0.04999517 0.22837618]]. Action = [[-0.19500488  0.14960119 -0.15720259 -0.40752316]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 1068 is [False, False, True, False, False, True]
Current timestep = 1069. State = [[0.03952238 0.2333989 ]]. Action = [[-0.06288251 -0.24034376 -0.11549747  0.06707072]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 1069 is [False, True, False, False, False, True]
Current timestep = 1070. State = [[0.03432411 0.2297826 ]]. Action = [[0.09014541 0.14773494 0.1014851  0.61523676]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 1070 is [False, True, False, False, False, True]
Current timestep = 1071. State = [[0.03917795 0.23483184]]. Action = [[ 0.23116326  0.13281459 -0.18358135 -0.9475179 ]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 1071 is [False, True, False, False, False, True]
Current timestep = 1072. State = [[0.04943955 0.24284865]]. Action = [[ 0.02885863  0.11105371 -0.18990585  0.25357187]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 1072 is [False, True, False, False, False, True]
Current timestep = 1073. State = [[0.05676478 0.25268564]]. Action = [[0.16841543 0.19681138 0.01796409 0.2754593 ]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 1073 is [False, True, False, False, False, True]
Scene graph at timestep 1073 is [False, False, True, False, False, True]
State prediction error at timestep 1073 is tensor(0.0520, grad_fn=<MseLossBackward0>)
Current timestep = 1074. State = [[0.06537515 0.26292846]]. Action = [[0.03149527 0.03174347 0.17048311 0.1009233 ]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 1074 is [False, False, True, False, False, True]
Current timestep = 1075. State = [[0.06994732 0.27140844]]. Action = [[ 0.02693376  0.22807637  0.17521542 -0.13697726]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 1075 is [False, False, True, False, False, True]
Current timestep = 1076. State = [[0.0746783 0.2828586]]. Action = [[0.148471   0.07277974 0.2475056  0.77140117]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 1076 is [False, False, True, False, False, True]
Current timestep = 1077. State = [[0.07782787 0.29023007]]. Action = [[-0.21554746  0.06720471 -0.16856542 -0.8619108 ]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 1077 is [False, False, True, False, False, True]
Current timestep = 1078. State = [[0.06859469 0.2988103 ]]. Action = [[-0.2170813   0.21885967  0.10183704  0.14542413]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 1078 is [False, False, True, False, False, True]
Current timestep = 1079. State = [[0.05727606 0.30525595]]. Action = [[-0.06250033 -0.24292657  0.22991428 -0.745737  ]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 1079 is [False, False, True, False, False, True]
Scene graph at timestep 1079 is [False, False, True, False, False, True]
State prediction error at timestep 1079 is tensor(0.0793, grad_fn=<MseLossBackward0>)
Current timestep = 1080. State = [[0.05397564 0.30148816]]. Action = [[ 0.23124236  0.09668964 -0.15114956 -0.38111818]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 1080 is [False, False, True, False, False, True]
Current timestep = 1081. State = [[0.058144  0.2989844]]. Action = [[-0.19234969 -0.19612344  0.09475887  0.26834846]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 1081 is [False, False, True, False, False, True]
Scene graph at timestep 1081 is [False, False, True, False, False, True]
State prediction error at timestep 1081 is tensor(0.0584, grad_fn=<MseLossBackward0>)
Current timestep = 1082. State = [[0.05573438 0.29291594]]. Action = [[0.15859962 0.02130198 0.10533363 0.05565238]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 1082 is [False, False, True, False, False, True]
Current timestep = 1083. State = [[0.0627098  0.29178798]]. Action = [[ 0.19009408  0.05265605 -0.12462431 -0.5194232 ]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 1083 is [False, False, True, False, False, True]
Current timestep = 1084. State = [[0.06887605 0.28940353]]. Action = [[-0.20063363 -0.2277411  -0.06283966 -0.26710796]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 1084 is [False, False, True, False, False, True]
Current timestep = 1085. State = [[0.06490445 0.28384024]]. Action = [[ 0.04427266  0.13384265 -0.13293904 -0.7641356 ]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 1085 is [False, False, True, False, False, True]
Scene graph at timestep 1085 is [False, False, True, False, False, True]
State prediction error at timestep 1085 is tensor(0.0825, grad_fn=<MseLossBackward0>)
Current timestep = 1086. State = [[0.06156932 0.28296825]]. Action = [[-0.22750959 -0.15138705 -0.12261356 -0.80686355]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 1086 is [False, False, True, False, False, True]
Current timestep = 1087. State = [[0.0499699  0.27733845]]. Action = [[-0.17161758 -0.08108784 -0.02486499  0.4985633 ]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 1087 is [False, False, True, False, False, True]
Current timestep = 1088. State = [[0.04083596 0.27205947]]. Action = [[ 0.07498902 -0.02497652  0.022329    0.92235494]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 1088 is [False, True, False, False, False, True]
Current timestep = 1089. State = [[0.03725469 0.26659587]]. Action = [[-0.22807878 -0.14274055 -0.12217282 -0.7561028 ]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 1089 is [False, True, False, False, False, True]
Scene graph at timestep 1089 is [False, True, False, False, False, True]
State prediction error at timestep 1089 is tensor(0.0653, grad_fn=<MseLossBackward0>)
Current timestep = 1090. State = [[0.02471825 0.25808513]]. Action = [[-0.24400325 -0.1501426  -0.00293082 -0.22185141]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 1090 is [False, True, False, False, False, True]
Current timestep = 1091. State = [[0.01234576 0.25311518]]. Action = [[ 0.08944845  0.18670762 -0.00162628  0.04578471]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 1091 is [False, True, False, False, False, True]
Scene graph at timestep 1091 is [False, True, False, False, False, True]
State prediction error at timestep 1091 is tensor(0.0537, grad_fn=<MseLossBackward0>)
Current timestep = 1092. State = [[0.01169194 0.25324833]]. Action = [[-0.02777438 -0.23216869  0.05334696  0.9041778 ]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 1092 is [False, True, False, False, False, True]
Current timestep = 1093. State = [[0.01178245 0.24528883]]. Action = [[ 0.08372605 -0.06702569 -0.06478707  0.29017448]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 1093 is [False, True, False, False, False, True]
Scene graph at timestep 1093 is [False, True, False, False, False, True]
State prediction error at timestep 1093 is tensor(0.0433, grad_fn=<MseLossBackward0>)
Current timestep = 1094. State = [[0.0133592  0.24200654]]. Action = [[-0.07289575  0.14030063 -0.21494669  0.1264646 ]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 1094 is [False, True, False, False, False, True]
Scene graph at timestep 1094 is [False, True, False, False, False, True]
State prediction error at timestep 1094 is tensor(0.0503, grad_fn=<MseLossBackward0>)
Current timestep = 1095. State = [[0.01455235 0.24462329]]. Action = [[ 0.18064734 -0.06158611 -0.18391047  0.1765126 ]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 1095 is [False, True, False, False, False, True]
Current timestep = 1096. State = [[0.02186956 0.24013674]]. Action = [[ 0.05221927 -0.22771634 -0.10164404 -0.5975807 ]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 1096 is [False, True, False, False, False, True]
Current timestep = 1097. State = [[0.02727187 0.23421167]]. Action = [[0.09710872 0.196787   0.14809704 0.54652596]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 1097 is [False, True, False, False, False, True]
Current timestep = 1098. State = [[0.0307765  0.23968855]]. Action = [[-0.13555504  0.10541898 -0.1575597   0.96172357]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 1098 is [False, True, False, False, False, True]
Current timestep = 1099. State = [[0.02576177 0.24252723]]. Action = [[-0.14691241 -0.1949585  -0.21228653  0.8623661 ]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 1099 is [False, True, False, False, False, True]
Current timestep = 1100. State = [[0.01588855 0.23859991]]. Action = [[-0.14449058  0.07934922 -0.00324789  0.4973917 ]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 1100 is [False, True, False, False, False, True]
Current timestep = 1101. State = [[0.01072239 0.23956732]]. Action = [[ 0.22568017  0.00571036  0.14570221 -0.15932077]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 1101 is [False, True, False, False, False, True]
Scene graph at timestep 1101 is [False, True, False, False, False, True]
State prediction error at timestep 1101 is tensor(0.0465, grad_fn=<MseLossBackward0>)
Current timestep = 1102. State = [[0.01805804 0.23729482]]. Action = [[ 0.07686326 -0.17682876 -0.23984356  0.4543202 ]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 1102 is [False, True, False, False, False, True]
Current timestep = 1103. State = [[0.02550382 0.22874732]]. Action = [[ 0.12833619 -0.13669468 -0.1849123   0.4729458 ]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 1103 is [False, True, False, False, False, True]
Current timestep = 1104. State = [[0.03557059 0.2184585 ]]. Action = [[ 0.22033876 -0.16898492 -0.15843956 -0.03466082]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 1104 is [False, True, False, False, False, True]
Current timestep = 1105. State = [[0.04944787 0.21111985]]. Action = [[ 0.21821663  0.11253569 -0.08597046  0.41396368]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 1105 is [False, True, False, False, False, True]
Current timestep = 1106. State = [[0.06075185 0.20926777]]. Action = [[-0.10883819 -0.16323076 -0.0671283   0.97683465]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 1106 is [False, True, False, False, False, True]
Current timestep = 1107. State = [[0.06115022 0.20188963]]. Action = [[-0.00409915 -0.11580044 -0.045633    0.2010014 ]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 1107 is [False, False, True, False, False, True]
Current timestep = 1108. State = [[0.05890119 0.19384104]]. Action = [[-0.11487946 -0.08408423  0.01202908  0.6286193 ]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 1108 is [False, False, True, False, False, True]
Current timestep = 1109. State = [[0.05539797 0.19174486]]. Action = [[ 0.10029143  0.21424288 -0.00974233 -0.29200697]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 1109 is [False, False, True, False, False, True]
Current timestep = 1110. State = [[0.0604592  0.19413626]]. Action = [[ 0.17764378 -0.23840228  0.04920763  0.05505395]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 1110 is [False, False, True, False, False, True]
Current timestep = 1111. State = [[0.0664887  0.18766019]]. Action = [[-0.12374055  0.00893167  0.11261314 -0.8297745 ]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 1111 is [False, False, True, False, False, True]
Scene graph at timestep 1111 is [False, False, True, False, False, True]
State prediction error at timestep 1111 is tensor(0.0476, grad_fn=<MseLossBackward0>)
Current timestep = 1112. State = [[0.06398072 0.18318217]]. Action = [[-0.05455242 -0.13447724  0.16625124  0.39956987]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 1112 is [False, False, True, False, False, True]
Current timestep = 1113. State = [[0.05960928 0.17666022]]. Action = [[-0.07819234 -0.04426838  0.0356935   0.28102612]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 1113 is [False, False, True, False, False, True]
Scene graph at timestep 1113 is [False, False, True, False, False, True]
State prediction error at timestep 1113 is tensor(0.0262, grad_fn=<MseLossBackward0>)
Current timestep = 1114. State = [[0.05520926 0.17659318]]. Action = [[-0.00084725  0.24004453 -0.09118289 -0.72028416]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 1114 is [False, False, True, False, False, True]
Scene graph at timestep 1114 is [False, False, True, False, False, True]
State prediction error at timestep 1114 is tensor(0.0474, grad_fn=<MseLossBackward0>)
Current timestep = 1115. State = [[0.05648148 0.18193781]]. Action = [[ 0.17635077 -0.1592944   0.10171178 -0.64322686]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 1115 is [False, False, True, False, False, True]
Current timestep = 1116. State = [[0.06635405 0.17535026]]. Action = [[ 0.23534021 -0.24031512 -0.13844217 -0.5761867 ]]. Reward = [0.]
Curr episode timestep = 576
Scene graph at timestep 1116 is [False, False, True, False, False, True]
Current timestep = 1117. State = [[0.07974405 0.16560276]]. Action = [[ 0.13158798  0.04830679 -0.15179434 -0.8410431 ]]. Reward = [0.]
Curr episode timestep = 577
Scene graph at timestep 1117 is [False, False, True, False, False, True]
Current timestep = 1118. State = [[0.08814933 0.16298622]]. Action = [[-0.07654572 -0.0166427  -0.22107337 -0.6764481 ]]. Reward = [0.]
Curr episode timestep = 578
Scene graph at timestep 1118 is [False, False, True, False, False, True]
Current timestep = 1119. State = [[0.09153134 0.16169074]]. Action = [[ 0.19840801 -0.02168211 -0.20850743 -0.27094448]]. Reward = [0.]
Curr episode timestep = 579
Scene graph at timestep 1119 is [False, False, True, False, False, True]
Current timestep = 1120. State = [[0.0962453 0.1624889]]. Action = [[-0.17846343  0.14395618  0.16262046 -0.56213   ]]. Reward = [0.]
Curr episode timestep = 580
Scene graph at timestep 1120 is [False, False, True, False, False, True]
Current timestep = 1121. State = [[0.08938993 0.17050074]]. Action = [[-0.1980967   0.19604275 -0.23028094  0.7685051 ]]. Reward = [0.]
Curr episode timestep = 581
Scene graph at timestep 1121 is [False, False, True, False, False, True]
Current timestep = 1122. State = [[0.07918752 0.1808117 ]]. Action = [[-0.04682198  0.04851902 -0.16493714 -0.6331923 ]]. Reward = [0.]
Curr episode timestep = 582
Scene graph at timestep 1122 is [False, False, True, False, False, True]
Current timestep = 1123. State = [[0.07294128 0.1855904 ]]. Action = [[-0.058899  -0.0391694 -0.0130786  0.5209148]]. Reward = [0.]
Curr episode timestep = 583
Scene graph at timestep 1123 is [False, False, True, False, False, True]
Current timestep = 1124. State = [[0.06945633 0.18339026]]. Action = [[ 0.03579113 -0.16489176  0.21489727 -0.64963424]]. Reward = [0.]
Curr episode timestep = 584
Scene graph at timestep 1124 is [False, False, True, False, False, True]
Current timestep = 1125. State = [[0.06739395 0.17932826]]. Action = [[-0.1325707   0.14662838 -0.20464136  0.1919713 ]]. Reward = [0.]
Curr episode timestep = 585
Scene graph at timestep 1125 is [False, False, True, False, False, True]
Scene graph at timestep 1125 is [False, False, True, False, False, True]
State prediction error at timestep 1125 is tensor(0.0326, grad_fn=<MseLossBackward0>)
Current timestep = 1126. State = [[0.06456908 0.18346295]]. Action = [[ 0.1426729   0.05763707 -0.00253354 -0.6911782 ]]. Reward = [0.]
Curr episode timestep = 586
Scene graph at timestep 1126 is [False, False, True, False, False, True]
Current timestep = 1127. State = [[0.06476418 0.18977234]]. Action = [[-0.22386958  0.18552405 -0.10432786 -0.946771  ]]. Reward = [0.]
Curr episode timestep = 587
Scene graph at timestep 1127 is [False, False, True, False, False, True]
Current timestep = 1128. State = [[0.05512745 0.19742428]]. Action = [[-0.17852151 -0.040218   -0.23315753  0.27415884]]. Reward = [0.]
Curr episode timestep = 588
Scene graph at timestep 1128 is [False, False, True, False, False, True]
Current timestep = 1129. State = [[0.04819195 0.1991956 ]]. Action = [[ 0.20129216 -0.00766987  0.04354295 -0.6425678 ]]. Reward = [0.]
Curr episode timestep = 589
Scene graph at timestep 1129 is [False, False, True, False, False, True]
Scene graph at timestep 1129 is [False, True, False, False, False, True]
State prediction error at timestep 1129 is tensor(0.0472, grad_fn=<MseLossBackward0>)
Current timestep = 1130. State = [[0.05182139 0.19528048]]. Action = [[-0.03895189 -0.24736151 -0.2431634   0.7079396 ]]. Reward = [0.]
Curr episode timestep = 590
Scene graph at timestep 1130 is [False, True, False, False, False, True]
Current timestep = 1131. State = [[0.0514818 0.1852133]]. Action = [[-0.05782644 -0.04213813 -0.11475176  0.60168076]]. Reward = [0.]
Curr episode timestep = 591
Scene graph at timestep 1131 is [False, False, True, False, False, True]
Current timestep = 1132. State = [[0.04568402 0.17813958]]. Action = [[-0.24146573 -0.10733888  0.15100223 -0.6546779 ]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 1132 is [False, False, True, False, False, True]
Current timestep = 1133. State = [[0.0360741 0.1717915]]. Action = [[ 0.04492575 -0.04194711  0.08686018  0.25757754]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 1133 is [False, True, False, False, False, True]
Scene graph at timestep 1133 is [False, True, False, False, False, True]
State prediction error at timestep 1133 is tensor(0.0243, grad_fn=<MseLossBackward0>)
Current timestep = 1134. State = [[0.03636959 0.16859709]]. Action = [[ 0.16823721  0.0268428  -0.12181753 -0.4664569 ]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 1134 is [False, True, False, False, False, True]
Current timestep = 1135. State = [[0.04117592 0.17106055]]. Action = [[-0.08350858  0.1817353   0.23329246 -0.6715493 ]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 1135 is [False, True, False, False, False, True]
Current timestep = 1136. State = [[0.03897374 0.1777022 ]]. Action = [[-0.10205704 -0.0127496   0.18705839  0.7397363 ]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 1136 is [False, True, False, False, False, True]
Current timestep = 1137. State = [[0.03299109 0.18143769]]. Action = [[-0.09991214  0.09395128 -0.01807205 -0.04586554]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 1137 is [False, True, False, False, False, True]
Scene graph at timestep 1137 is [False, True, False, False, False, True]
State prediction error at timestep 1137 is tensor(0.0329, grad_fn=<MseLossBackward0>)
Current timestep = 1138. State = [[0.03059359 0.18237363]]. Action = [[ 0.21468812 -0.21655151  0.10161906 -0.7593437 ]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 1138 is [False, True, False, False, False, True]
Current timestep = 1139. State = [[0.03480773 0.17296143]]. Action = [[-0.13071609 -0.16996439 -0.20501195  0.5707611 ]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 1139 is [False, True, False, False, False, True]
Scene graph at timestep 1139 is [False, True, False, False, False, True]
State prediction error at timestep 1139 is tensor(0.0259, grad_fn=<MseLossBackward0>)
Current timestep = 1140. State = [[0.02990358 0.1636096 ]]. Action = [[-0.17246085 -0.00901665 -0.19788454  0.8766681 ]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 1140 is [False, True, False, False, False, True]
Current timestep = 1141. State = [[0.02576167 0.16149114]]. Action = [[ 0.24552083  0.09105915 -0.06428772 -0.66993695]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 1141 is [False, True, False, False, False, True]
Scene graph at timestep 1141 is [False, True, False, False, False, True]
State prediction error at timestep 1141 is tensor(0.0370, grad_fn=<MseLossBackward0>)
Current timestep = 1142. State = [[0.0321703  0.16597961]]. Action = [[-6.1795115e-04  1.4438862e-01  1.2839127e-01  7.6931596e-01]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 1142 is [False, True, False, False, False, True]
Current timestep = 1143. State = [[0.03159521 0.17343941]]. Action = [[-0.2312102   0.07165787  0.05622765  0.97019506]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 1143 is [False, True, False, False, False, True]
Current timestep = 1144. State = [[0.02161844 0.18035915]]. Action = [[-0.13126235  0.10978109  0.04079282 -0.14698052]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 1144 is [False, True, False, False, False, True]
Current timestep = 1145. State = [[0.01416034 0.18798083]]. Action = [[ 0.06162304  0.09762707  0.12574452 -0.9870381 ]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 1145 is [False, True, False, False, False, True]
Current timestep = 1146. State = [[0.01056211 0.19332609]]. Action = [[-0.19645588 -0.03069919  0.03415012  0.17743301]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 1146 is [False, True, False, False, False, True]
Current timestep = 1147. State = [[0.00151292 0.19175594]]. Action = [[-0.10560322 -0.16004452  0.11511013 -0.159145  ]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 1147 is [False, True, False, False, False, True]
Current timestep = 1148. State = [[-0.00281061  0.18854216]]. Action = [[ 0.18113726  0.15932667 -0.03950791  0.77017283]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 1148 is [False, True, False, False, False, True]
Scene graph at timestep 1148 is [False, True, False, False, False, True]
State prediction error at timestep 1148 is tensor(0.0322, grad_fn=<MseLossBackward0>)
Current timestep = 1149. State = [[-0.00021147  0.18881744]]. Action = [[-0.12412032 -0.20992203 -0.19955477 -0.8095371 ]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 1149 is [False, True, False, False, False, True]
Scene graph at timestep 1149 is [False, True, False, False, False, True]
State prediction error at timestep 1149 is tensor(0.0399, grad_fn=<MseLossBackward0>)
Current timestep = 1150. State = [[-0.00537478  0.18362464]]. Action = [[-0.1661688   0.07080483 -0.10685712  0.03666735]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 1150 is [False, True, False, False, False, True]
Scene graph at timestep 1150 is [False, True, False, False, False, True]
State prediction error at timestep 1150 is tensor(0.0316, grad_fn=<MseLossBackward0>)
Current timestep = 1151. State = [[-0.01215323  0.18037848]]. Action = [[ 0.0588991  -0.21241152 -0.20386167 -0.15142047]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 1151 is [False, True, False, False, False, True]
Current timestep = 1152. State = [[-0.01235781  0.17077148]]. Action = [[ 0.03433391 -0.11782083 -0.12988816  0.20297682]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 1152 is [False, True, False, False, False, True]
Current timestep = 1153. State = [[-0.00932128  0.16675292]]. Action = [[ 0.10774985  0.2214114   0.20405462 -0.69664544]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 1153 is [False, True, False, False, False, True]
Current timestep = 1154. State = [[-0.0050628   0.17288041]]. Action = [[-0.0296894   0.03318843  0.04915181  0.10056651]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 1154 is [False, True, False, False, False, True]
Current timestep = 1155. State = [[-0.0070151   0.18033235]]. Action = [[-0.1764488   0.23807794 -0.02329752 -0.1659224 ]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 1155 is [False, True, False, False, False, True]
Current timestep = 1156. State = [[-0.01301837  0.1890936 ]]. Action = [[ 0.04783294 -0.09642738  0.10604089 -0.38425738]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 1156 is [False, True, False, False, False, True]
Current timestep = 1157. State = [[-0.01570286  0.19273686]]. Action = [[-0.12063037  0.21059883 -0.1790576   0.59904563]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 1157 is [False, True, False, False, False, True]
Current timestep = 1158. State = [[-0.01673647  0.19935994]]. Action = [[ 0.24223912 -0.07257581 -0.09693801 -0.42680848]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 1158 is [False, True, False, False, False, True]
Current timestep = 1159. State = [[-0.01081699  0.20116425]]. Action = [[-0.09829634  0.09775844  0.1341146  -0.1568563 ]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 1159 is [False, True, False, False, False, True]
Scene graph at timestep 1159 is [False, True, False, False, False, True]
State prediction error at timestep 1159 is tensor(0.0372, grad_fn=<MseLossBackward0>)
Current timestep = 1160. State = [[-0.01272255  0.20165217]]. Action = [[-0.08837821 -0.19088215  0.2001667  -0.17043209]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 1160 is [False, True, False, False, False, True]
Current timestep = 1161. State = [[-0.01888101  0.19781065]]. Action = [[-0.14731449  0.12261957 -0.05408862 -0.07255864]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 1161 is [False, True, False, False, False, True]
Current timestep = 1162. State = [[-0.02238385  0.19743751]]. Action = [[ 0.24378425 -0.16696534 -0.1255664  -0.78244394]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 1162 is [False, True, False, False, False, True]
Current timestep = 1163. State = [[-0.01664104  0.19574967]]. Action = [[-0.0579711   0.22452548 -0.08470252  0.5769861 ]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 1163 is [False, True, False, False, False, True]
Current timestep = 1164. State = [[-0.01743731  0.20046826]]. Action = [[-0.10140651 -0.09841621 -0.06298567 -0.36371022]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 1164 is [False, True, False, False, False, True]
Current timestep = 1165. State = [[-0.02426026  0.20013906]]. Action = [[-0.19500595  0.04529801 -0.13707988 -0.6232807 ]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 1165 is [False, True, False, False, False, True]
Scene graph at timestep 1165 is [False, True, False, False, False, True]
State prediction error at timestep 1165 is tensor(0.0411, grad_fn=<MseLossBackward0>)
Current timestep = 1166. State = [[-0.035166    0.19769603]]. Action = [[-0.11964688 -0.22183219 -0.18295148 -0.41338772]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 1166 is [False, True, False, False, False, True]
Current timestep = 1167. State = [[-0.04238932  0.19063652]]. Action = [[ 0.05582023  0.04792994  0.04563642 -0.9816757 ]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 1167 is [False, True, False, False, False, True]
Current timestep = 1168. State = [[-0.04115051  0.18807095]]. Action = [[ 0.12664384 -0.07758157 -0.18043406  0.138242  ]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 1168 is [False, True, False, False, False, True]
Scene graph at timestep 1168 is [False, True, False, False, False, True]
State prediction error at timestep 1168 is tensor(0.0317, grad_fn=<MseLossBackward0>)
Current timestep = 1169. State = [[-0.03847337  0.18806915]]. Action = [[-0.14944795  0.20355284 -0.05061154 -0.4279381 ]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 1169 is [False, True, False, False, False, True]
Current timestep = 1170. State = [[-0.04091966  0.19565873]]. Action = [[ 0.08423358  0.07146364  0.13539171 -0.35255313]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 1170 is [False, True, False, False, False, True]
Scene graph at timestep 1170 is [False, True, False, False, False, True]
State prediction error at timestep 1170 is tensor(0.0356, grad_fn=<MseLossBackward0>)
Current timestep = 1171. State = [[-0.03935825  0.19800921]]. Action = [[ 0.00790897 -0.18901803 -0.03722633 -0.39458627]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 1171 is [False, True, False, False, False, True]
Scene graph at timestep 1171 is [False, True, False, False, False, True]
State prediction error at timestep 1171 is tensor(0.0332, grad_fn=<MseLossBackward0>)
Current timestep = 1172. State = [[-0.03930938  0.19326603]]. Action = [[-0.06931503  0.03093955  0.12561277  0.20669138]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 1172 is [False, True, False, False, False, True]
Current timestep = 1173. State = [[-0.03832546  0.19390322]]. Action = [[ 0.21285462  0.10561815  0.18552741 -0.1713056 ]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 1173 is [False, True, False, False, False, True]
Current timestep = 1174. State = [[-0.0336912   0.19432467]]. Action = [[-0.16262981 -0.20520076  0.1236068  -0.14446902]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 1174 is [False, True, False, False, False, True]
Scene graph at timestep 1174 is [False, True, False, False, False, True]
State prediction error at timestep 1174 is tensor(0.0290, grad_fn=<MseLossBackward0>)
Current timestep = 1175. State = [[-0.03728101  0.18919294]]. Action = [[-0.0262779   0.0680874  -0.18526632  0.5638639 ]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 1175 is [False, True, False, False, False, True]
Current timestep = 1176. State = [[-0.03789299  0.18807566]]. Action = [[ 0.13147652 -0.07081147  0.02005705 -0.05899054]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 1176 is [False, True, False, False, False, True]
Scene graph at timestep 1176 is [False, True, False, False, False, True]
State prediction error at timestep 1176 is tensor(0.0306, grad_fn=<MseLossBackward0>)
Current timestep = 1177. State = [[-0.03075789  0.1885899 ]]. Action = [[ 0.19738287  0.18616343 -0.14203995 -0.6552356 ]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 1177 is [False, True, False, False, False, True]
Current timestep = 1178. State = [[-0.01905187  0.19392398]]. Action = [[ 0.16301596 -0.04249597  0.03114793  0.7008686 ]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 1178 is [False, True, False, False, False, True]
Current timestep = 1179. State = [[-0.01013489  0.19728085]]. Action = [[-0.0587167   0.16065684  0.06786078 -0.7174393 ]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 1179 is [False, True, False, False, False, True]
Current timestep = 1180. State = [[-0.00537451  0.19994976]]. Action = [[ 0.22782639 -0.22490987  0.15237758 -0.80435956]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 1180 is [False, True, False, False, False, True]
Current timestep = 1181. State = [[0.004948  0.1941086]]. Action = [[0.09811464 0.01445228 0.12624907 0.383466  ]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 1181 is [False, True, False, False, False, True]
Current timestep = 1182. State = [[0.01549007 0.18809524]]. Action = [[ 0.2169804  -0.24895826  0.16573012 -0.0943827 ]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 1182 is [False, True, False, False, False, True]
Scene graph at timestep 1182 is [False, True, False, False, False, True]
State prediction error at timestep 1182 is tensor(0.0285, grad_fn=<MseLossBackward0>)
Current timestep = 1183. State = [[0.02779274 0.1804582 ]]. Action = [[ 0.09476033  0.15138912 -0.11992106 -0.11962163]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 1183 is [False, True, False, False, False, True]
Current timestep = 1184. State = [[0.03371942 0.1813762 ]]. Action = [[-0.11527905 -0.05294642 -0.05373214  0.46590507]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 1184 is [False, True, False, False, False, True]
Scene graph at timestep 1184 is [False, True, False, False, False, True]
State prediction error at timestep 1184 is tensor(0.0267, grad_fn=<MseLossBackward0>)
Current timestep = 1185. State = [[0.03112612 0.18216324]]. Action = [[-0.05851424  0.10555243  0.19142109 -0.14011174]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 1185 is [False, True, False, False, False, True]
Scene graph at timestep 1185 is [False, True, False, False, False, True]
State prediction error at timestep 1185 is tensor(0.0339, grad_fn=<MseLossBackward0>)
Current timestep = 1186. State = [[0.02676576 0.182352  ]]. Action = [[-0.06921664 -0.21365741 -0.10666569 -0.8443073 ]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 1186 is [False, True, False, False, False, True]
Current timestep = 1187. State = [[0.02091343 0.17421435]]. Action = [[-0.12757358 -0.09661464 -0.24617188 -0.61022174]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 1187 is [False, True, False, False, False, True]
Current timestep = 1188. State = [[0.01723043 0.16880554]]. Action = [[ 0.16942048  0.05534366  0.11158103 -0.6257579 ]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 1188 is [False, True, False, False, False, True]
Current timestep = 1189. State = [[0.02418082 0.17058153]]. Action = [[ 0.18487903  0.1238496   0.12061653 -0.11819547]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 1189 is [False, True, False, False, False, True]
Current timestep = 1190. State = [[0.03530873 0.17411393]]. Action = [[ 0.14680696 -0.07650879  0.06399697 -0.9161533 ]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 1190 is [False, True, False, False, False, True]
Current timestep = 1191. State = [[0.04086531 0.17177841]]. Action = [[-0.22759947 -0.0735565   0.18510315 -0.12942147]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 1191 is [False, True, False, False, False, True]
Current timestep = 1192. State = [[0.03560067 0.1686469 ]]. Action = [[ 0.00666443  0.00741059 -0.05220553 -0.46104348]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 1192 is [False, True, False, False, False, True]
Scene graph at timestep 1192 is [False, True, False, False, False, True]
State prediction error at timestep 1192 is tensor(0.0332, grad_fn=<MseLossBackward0>)
Current timestep = 1193. State = [[0.02968065 0.16804357]]. Action = [[-0.23892622  0.0262526   0.01190242 -0.47393084]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 1193 is [False, True, False, False, False, True]
Scene graph at timestep 1193 is [False, True, False, False, False, True]
State prediction error at timestep 1193 is tensor(0.0324, grad_fn=<MseLossBackward0>)
Current timestep = 1194. State = [[0.0224134  0.16728973]]. Action = [[ 0.16906261 -0.09915647 -0.13541959  0.91911435]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 1194 is [False, True, False, False, False, True]
Current timestep = 1195. State = [[0.02826484 0.16242465]]. Action = [[ 0.21444526 -0.09529233  0.12940896 -0.07189494]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 1195 is [False, True, False, False, False, True]
Current timestep = 1196. State = [[0.04147028 0.15716934]]. Action = [[ 0.2343669  -0.01407936 -0.2088637   0.63893235]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 1196 is [False, True, False, False, False, True]
Current timestep = 1197. State = [[0.05340846 0.15227678]]. Action = [[-0.04787463 -0.14780894 -0.13473898 -0.75572693]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 1197 is [False, True, False, False, False, True]
Current timestep = 1198. State = [[0.0580571  0.14875244]]. Action = [[ 0.10966831  0.17431337  0.0057447  -0.2935878 ]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 1198 is [False, False, True, False, False, True]
Current timestep = 1199. State = [[0.0621744  0.14987554]]. Action = [[-0.04415669 -0.19095783  0.19102699  0.57641435]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 1199 is [False, False, True, False, False, True]
Scene graph at timestep 1199 is [False, False, True, False, False, True]
State prediction error at timestep 1199 is tensor(0.0189, grad_fn=<MseLossBackward0>)
Current timestep = 1200. State = [[0.05847904 0.1408138 ]]. Action = [[-0.24899064 -0.22959016  0.17456692 -0.16662353]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 1200 is [False, False, True, False, False, True]
Current timestep = 1201. State = [[0.05197875 0.13059133]]. Action = [[ 0.21140745  0.03074968 -0.22398312 -0.8888753 ]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 1201 is [False, False, True, False, False, True]
Scene graph at timestep 1201 is [False, False, True, False, False, True]
State prediction error at timestep 1201 is tensor(0.0407, grad_fn=<MseLossBackward0>)
Current timestep = 1202. State = [[0.05594901 0.12820196]]. Action = [[-0.02238712  0.04046309  0.13159275  0.8810878 ]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 1202 is [False, False, True, False, False, True]
Current timestep = 1203. State = [[0.05881207 0.13214928]]. Action = [[ 0.1024403   0.20404741  0.03936845 -0.28707886]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 1203 is [False, False, True, False, False, True]
Current timestep = 1204. State = [[0.06415161 0.14386225]]. Action = [[ 0.07708955  0.2285819  -0.12938498  0.10653341]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 1204 is [False, False, True, False, False, True]
Current timestep = 1205. State = [[0.06847335 0.15240304]]. Action = [[-0.01129307 -0.20390159 -0.21494229 -0.19108975]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 1205 is [False, False, True, False, False, True]
Current timestep = 1206. State = [[0.07228667 0.15011764]]. Action = [[ 0.16558093  0.06072444 -0.06392379  0.00244927]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 1206 is [False, False, True, False, False, True]
Current timestep = 1207. State = [[0.0766067 0.1513313]]. Action = [[-0.15509537  0.05105233  0.08250332  0.8999798 ]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 1207 is [False, False, True, False, False, True]
Current timestep = 1208. State = [[0.07436055 0.15601477]]. Action = [[ 0.06713817  0.1458646  -0.10730809  0.02318394]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 1208 is [False, False, True, False, False, True]
Scene graph at timestep 1208 is [False, False, True, False, False, True]
State prediction error at timestep 1208 is tensor(0.0300, grad_fn=<MseLossBackward0>)
Current timestep = 1209. State = [[0.07680253 0.16249901]]. Action = [[ 9.939307e-02  5.848110e-04  6.549212e-02 -6.065494e-01]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 1209 is [False, False, True, False, False, True]
Current timestep = 1210. State = [[0.07819086 0.16701895]]. Action = [[-0.19429123  0.14195359 -0.20838898 -0.15295988]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 1210 is [False, False, True, False, False, True]
Scene graph at timestep 1210 is [False, False, True, False, False, True]
State prediction error at timestep 1210 is tensor(0.0351, grad_fn=<MseLossBackward0>)
Current timestep = 1211. State = [[0.07597142 0.17537637]]. Action = [[ 0.23286584  0.12682766 -0.18417779  0.34403944]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 1211 is [False, False, True, False, False, True]
Current timestep = 1212. State = [[0.08015426 0.18230751]]. Action = [[-0.14514253 -0.00127271  0.14072776  0.6297126 ]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 1212 is [False, False, True, False, False, True]
Current timestep = 1213. State = [[0.07638475 0.1869517 ]]. Action = [[-0.0717341   0.13243192  0.10683334  0.21037054]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 1213 is [False, False, True, False, False, True]
Current timestep = 1214. State = [[0.07130444 0.19545145]]. Action = [[-0.06419992  0.16935322 -0.10661988 -0.20250857]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 1214 is [False, False, True, False, False, True]
Current timestep = 1215. State = [[0.06932778 0.20594989]]. Action = [[ 0.13496667  0.11062339  0.22038996 -0.3380177 ]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 1215 is [False, False, True, False, False, True]
Current timestep = 1216. State = [[0.07525591 0.21315476]]. Action = [[ 0.14759278 -0.01174377 -0.13901816 -0.28513652]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 1216 is [False, False, True, False, False, True]
Current timestep = 1217. State = [[0.08065123 0.21709035]]. Action = [[-0.12706245  0.12733352  0.19261807 -0.47061217]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 1217 is [False, False, True, False, False, True]
Scene graph at timestep 1217 is [False, False, True, False, False, True]
State prediction error at timestep 1217 is tensor(0.0519, grad_fn=<MseLossBackward0>)
Current timestep = 1218. State = [[0.07528359 0.22448443]]. Action = [[-0.21964872  0.12978798  0.183985   -0.91436535]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 1218 is [False, False, True, False, False, True]
Scene graph at timestep 1218 is [False, False, True, False, False, True]
State prediction error at timestep 1218 is tensor(0.0614, grad_fn=<MseLossBackward0>)
Current timestep = 1219. State = [[0.06542014 0.22906312]]. Action = [[-0.01887518 -0.16287604  0.24237731  0.35332608]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 1219 is [False, False, True, False, False, True]
Scene graph at timestep 1219 is [False, False, True, False, False, True]
State prediction error at timestep 1219 is tensor(0.0386, grad_fn=<MseLossBackward0>)
Current timestep = 1220. State = [[0.06201709 0.2222295 ]]. Action = [[ 0.07082483 -0.21302857 -0.10341522  0.27900898]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 1220 is [False, False, True, False, False, True]
Current timestep = 1221. State = [[0.06558809 0.21227454]]. Action = [[ 0.15072173 -0.02000113 -0.15820202 -0.21834117]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 1221 is [False, False, True, False, False, True]
Current timestep = 1222. State = [[0.06918102 0.20466447]]. Action = [[-0.19776706 -0.18154716  0.09399617  0.13514662]]. Reward = [0.]
Curr episode timestep = 682
Scene graph at timestep 1222 is [False, False, True, False, False, True]
Current timestep = 1223. State = [[0.06569956 0.1985889 ]]. Action = [[0.11020458 0.12720326 0.0724476  0.12459254]]. Reward = [0.]
Curr episode timestep = 683
Scene graph at timestep 1223 is [False, False, True, False, False, True]
Scene graph at timestep 1223 is [False, False, True, False, False, True]
State prediction error at timestep 1223 is tensor(0.0374, grad_fn=<MseLossBackward0>)
Current timestep = 1224. State = [[0.064349  0.2002004]]. Action = [[-0.22549373  0.00688478  0.07419777 -0.6626741 ]]. Reward = [0.]
Curr episode timestep = 684
Scene graph at timestep 1224 is [False, False, True, False, False, True]
Current timestep = 1225. State = [[0.06009487 0.20493801]]. Action = [[0.23335367 0.20666438 0.19594616 0.18930972]]. Reward = [0.]
Curr episode timestep = 685
Scene graph at timestep 1225 is [False, False, True, False, False, True]
Current timestep = 1226. State = [[0.06810123 0.2112594 ]]. Action = [[ 0.14615947 -0.11809333  0.1200116   0.7570331 ]]. Reward = [0.]
Curr episode timestep = 686
Scene graph at timestep 1226 is [False, False, True, False, False, True]
Current timestep = 1227. State = [[0.07822815 0.2114626 ]]. Action = [[ 0.12589568  0.09843236 -0.04459061  0.4539112 ]]. Reward = [0.]
Curr episode timestep = 687
Scene graph at timestep 1227 is [False, False, True, False, False, True]
Current timestep = 1228. State = [[0.08484636 0.21258746]]. Action = [[-0.08678991 -0.10230231  0.14011002 -0.4642864 ]]. Reward = [0.]
Curr episode timestep = 688
Scene graph at timestep 1228 is [False, False, True, False, False, True]
Current timestep = 1229. State = [[0.08784014 0.2092595 ]]. Action = [[ 0.22997537 -0.06062447  0.16229862 -0.3305667 ]]. Reward = [0.]
Curr episode timestep = 689
Scene graph at timestep 1229 is [False, False, True, False, False, True]
Current timestep = 1230. State = [[0.09961808 0.20485571]]. Action = [[ 0.23385352 -0.06299481 -0.12535873  0.4106437 ]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 1230 is [False, False, True, False, False, True]
Current timestep = 1231. State = [[0.11487848 0.20196125]]. Action = [[ 0.2043517   0.05330503  0.06447047 -0.08690995]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 1231 is [False, False, True, False, False, True]
Current timestep = 1232. State = [[0.12446196 0.20255855]]. Action = [[-0.17690736  0.02493644 -0.16751412 -0.00348252]]. Reward = [0.]
Curr episode timestep = 692
Scene graph at timestep 1232 is [False, False, True, False, False, True]
Current timestep = 1233. State = [[0.12199341 0.20385046]]. Action = [[-0.0252385  -0.018298    0.18762183 -0.6261109 ]]. Reward = [0.]
Curr episode timestep = 693
Scene graph at timestep 1233 is [False, False, True, False, False, True]
Current timestep = 1234. State = [[0.11877717 0.20480432]]. Action = [[-0.06138067  0.06311274 -0.09356934  0.28205538]]. Reward = [0.]
Curr episode timestep = 694
Scene graph at timestep 1234 is [False, False, True, False, False, True]
Current timestep = 1235. State = [[0.11287567 0.20777532]]. Action = [[-0.17740276  0.04838565  0.11066547 -0.91200346]]. Reward = [0.]
Curr episode timestep = 695
Scene graph at timestep 1235 is [False, False, True, False, False, True]
Scene graph at timestep 1235 is [False, False, True, False, False, True]
State prediction error at timestep 1235 is tensor(0.0641, grad_fn=<MseLossBackward0>)
Current timestep = 1236. State = [[0.10812479 0.20735176]]. Action = [[ 0.20925766 -0.22535758 -0.15848458  0.2991233 ]]. Reward = [0.]
Curr episode timestep = 696
Scene graph at timestep 1236 is [False, False, True, False, False, True]
Scene graph at timestep 1236 is [False, False, True, False, False, True]
State prediction error at timestep 1236 is tensor(0.0382, grad_fn=<MseLossBackward0>)
Current timestep = 1237. State = [[0.1114997  0.20338158]]. Action = [[-0.10235457  0.24505168 -0.09458208  0.69071627]]. Reward = [0.]
Curr episode timestep = 697
Scene graph at timestep 1237 is [False, False, True, False, False, True]
Current timestep = 1238. State = [[0.10924023 0.21173434]]. Action = [[-0.04158448  0.13418871 -0.05495328 -0.42238128]]. Reward = [0.]
Curr episode timestep = 698
Scene graph at timestep 1238 is [False, False, True, False, False, True]
Current timestep = 1239. State = [[0.10745171 0.22049469]]. Action = [[ 0.04031211  0.05615997 -0.12544726 -0.9162301 ]]. Reward = [0.]
Curr episode timestep = 699
Scene graph at timestep 1239 is [False, False, True, False, False, True]
Current timestep = 1240. State = [[0.10667402 0.22250773]]. Action = [[-0.0981268  -0.17413552  0.10001233  0.7603221 ]]. Reward = [0.]
Curr episode timestep = 700
Scene graph at timestep 1240 is [False, False, True, False, False, True]
Scene graph at timestep 1240 is [False, False, True, False, False, True]
State prediction error at timestep 1240 is tensor(0.0429, grad_fn=<MseLossBackward0>)
Current timestep = 1241. State = [[0.10311046 0.21969482]]. Action = [[ 0.00026518  0.11983681 -0.21961392  0.24496317]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 1241 is [False, False, True, False, False, True]
Current timestep = 1242. State = [[0.10052865 0.21886598]]. Action = [[-0.08942342 -0.19995776  0.21823025  0.5712428 ]]. Reward = [0.]
Curr episode timestep = 702
Scene graph at timestep 1242 is [False, False, True, False, False, True]
Current timestep = 1243. State = [[0.10018478 0.21456592]]. Action = [[0.24446988 0.11221716 0.1861228  0.07393587]]. Reward = [0.]
Curr episode timestep = 703
Scene graph at timestep 1243 is [False, False, True, False, False, True]
Current timestep = 1244. State = [[0.10856705 0.21871531]]. Action = [[ 0.036239    0.17364216  0.0086903  -0.4620999 ]]. Reward = [0.]
Curr episode timestep = 704
Scene graph at timestep 1244 is [False, False, True, False, False, True]
Scene graph at timestep 1244 is [False, False, True, False, False, True]
State prediction error at timestep 1244 is tensor(0.0579, grad_fn=<MseLossBackward0>)
Current timestep = 1245. State = [[0.11316336 0.22813624]]. Action = [[-0.00455023  0.13349792 -0.22720048  0.10577023]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 1245 is [False, False, True, False, False, True]
Current timestep = 1246. State = [[0.11716466 0.2331377 ]]. Action = [[ 0.15709955 -0.19094732  0.24481857 -0.01481837]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 1246 is [False, False, True, False, False, True]
Scene graph at timestep 1246 is [False, False, True, False, False, True]
State prediction error at timestep 1246 is tensor(0.0506, grad_fn=<MseLossBackward0>)
Current timestep = 1247. State = [[0.12216582 0.22653385]]. Action = [[-0.08614451 -0.1247716  -0.1132707   0.04348361]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 1247 is [False, False, True, False, False, True]
Current timestep = 1248. State = [[0.11908921 0.22323388]]. Action = [[-0.13551106  0.20758325 -0.0108386  -0.2788546 ]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 1248 is [False, False, True, False, False, True]
Current timestep = 1249. State = [[0.11379544 0.22774087]]. Action = [[ 0.01657671 -0.06941184  0.13577032  0.586884  ]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 1249 is [False, False, True, False, False, True]
Current timestep = 1250. State = [[0.11113787 0.22788228]]. Action = [[-0.07202438  0.03504315 -0.01590636  0.2156806 ]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 1250 is [False, False, True, False, False, True]
Current timestep = 1251. State = [[0.10892365 0.22932091]]. Action = [[ 0.06226817  0.017995    0.18723622 -0.7663365 ]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 1251 is [False, False, True, False, False, True]
Current timestep = 1252. State = [[0.11137503 0.22743052]]. Action = [[ 0.07335749 -0.18302114  0.19740707  0.817893  ]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 1252 is [False, False, True, False, False, True]
Current timestep = 1253. State = [[0.11183991 0.21735741]]. Action = [[-0.18299647 -0.20668131  0.08958179  0.10832012]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 1253 is [False, False, True, False, False, True]
Current timestep = 1254. State = [[0.10604268 0.20435724]]. Action = [[ 0.01159137 -0.18568704 -0.16092178 -0.38514292]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 1254 is [False, False, True, False, False, True]
Current timestep = 1255. State = [[0.10122301 0.19614628]]. Action = [[-0.18207364  0.17996332  0.0698553  -0.30292296]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 1255 is [False, False, True, False, False, True]
Current timestep = 1256. State = [[0.09395689 0.19638476]]. Action = [[ 0.00095701 -0.1772466   0.07162702 -0.52204114]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 1256 is [False, False, True, False, False, True]
Current timestep = 1257. State = [[0.08930317 0.18787363]]. Action = [[-0.12555191 -0.19815345  0.11107594  0.06362092]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 1257 is [False, False, True, False, False, True]
Current timestep = 1258. State = [[0.08594045 0.17700432]]. Action = [[ 0.17287284 -0.09129894  0.14335632  0.85710466]]. Reward = [0.]
Curr episode timestep = 718
Scene graph at timestep 1258 is [False, False, True, False, False, True]
Current timestep = 1259. State = [[0.08917144 0.1690699 ]]. Action = [[-0.09601426 -0.03609288 -0.23285028 -0.77575076]]. Reward = [0.]
Curr episode timestep = 719
Scene graph at timestep 1259 is [False, False, True, False, False, True]
Current timestep = 1260. State = [[0.08798373 0.16815266]]. Action = [[ 0.03952435  0.17779762  0.1905359  -0.24379146]]. Reward = [0.]
Curr episode timestep = 720
Scene graph at timestep 1260 is [False, False, True, False, False, True]
Scene graph at timestep 1260 is [False, False, True, False, False, True]
State prediction error at timestep 1260 is tensor(0.0379, grad_fn=<MseLossBackward0>)
Current timestep = 1261. State = [[0.08867746 0.17216632]]. Action = [[-0.01367122 -0.08749039  0.21872026  0.60538375]]. Reward = [0.]
Curr episode timestep = 721
Scene graph at timestep 1261 is [False, False, True, False, False, True]
Scene graph at timestep 1261 is [False, False, True, False, False, True]
State prediction error at timestep 1261 is tensor(0.0271, grad_fn=<MseLossBackward0>)
Current timestep = 1262. State = [[0.08490099 0.17088532]]. Action = [[-0.24217844 -0.00605531 -0.21537238  0.18789506]]. Reward = [0.]
Curr episode timestep = 722
Scene graph at timestep 1262 is [False, False, True, False, False, True]
Scene graph at timestep 1262 is [False, False, True, False, False, True]
State prediction error at timestep 1262 is tensor(0.0288, grad_fn=<MseLossBackward0>)
Current timestep = 1263. State = [[0.07146981 0.17413075]]. Action = [[-0.24211814  0.24676281  0.15192038 -0.07440323]]. Reward = [0.]
Curr episode timestep = 723
Scene graph at timestep 1263 is [False, False, True, False, False, True]
Current timestep = 1264. State = [[0.05811999 0.18379448]]. Action = [[-0.0190562   0.02507576 -0.20561722  0.13596094]]. Reward = [0.]
Curr episode timestep = 724
Scene graph at timestep 1264 is [False, False, True, False, False, True]
Current timestep = 1265. State = [[0.05137794 0.19190359]]. Action = [[-0.0819139   0.2377533   0.07297003 -0.8772502 ]]. Reward = [0.]
Curr episode timestep = 725
Scene graph at timestep 1265 is [False, False, True, False, False, True]
Current timestep = 1266. State = [[0.04490491 0.20482753]]. Action = [[-0.11883143  0.1590144   0.0332022   0.7481885 ]]. Reward = [0.]
Curr episode timestep = 726
Scene graph at timestep 1266 is [False, False, True, False, False, True]
Current timestep = 1267. State = [[0.03891326 0.21590623]]. Action = [[ 0.01810542  0.06293476 -0.06923683  0.6456461 ]]. Reward = [0.]
Curr episode timestep = 727
Scene graph at timestep 1267 is [False, True, False, False, False, True]
Scene graph at timestep 1267 is [False, True, False, False, False, True]
State prediction error at timestep 1267 is tensor(0.0375, grad_fn=<MseLossBackward0>)
Current timestep = 1268. State = [[0.03939412 0.21861918]]. Action = [[ 0.12455276 -0.20616859  0.24358356 -0.7300975 ]]. Reward = [0.]
Curr episode timestep = 728
Scene graph at timestep 1268 is [False, True, False, False, False, True]
Scene graph at timestep 1268 is [False, True, False, False, False, True]
State prediction error at timestep 1268 is tensor(0.0501, grad_fn=<MseLossBackward0>)
Current timestep = 1269. State = [[0.04204603 0.21349917]]. Action = [[-0.10946015  0.05207783  0.12205553  0.63020027]]. Reward = [0.]
Curr episode timestep = 729
Scene graph at timestep 1269 is [False, True, False, False, False, True]
Current timestep = 1270. State = [[0.03904055 0.21567793]]. Action = [[-0.02568869  0.1639657   0.05335131 -0.5775497 ]]. Reward = [0.]
Curr episode timestep = 730
Scene graph at timestep 1270 is [False, True, False, False, False, True]
Current timestep = 1271. State = [[0.03995652 0.22560318]]. Action = [[0.22070926 0.22764069 0.17550236 0.57314944]]. Reward = [0.]
Curr episode timestep = 731
Scene graph at timestep 1271 is [False, True, False, False, False, True]
Current timestep = 1272. State = [[0.05012591 0.23536257]]. Action = [[ 0.14389932 -0.07898644  0.1549784  -0.2901944 ]]. Reward = [0.]
Curr episode timestep = 732
Scene graph at timestep 1272 is [False, True, False, False, False, True]
Scene graph at timestep 1272 is [False, False, True, False, False, True]
State prediction error at timestep 1272 is tensor(0.0488, grad_fn=<MseLossBackward0>)
Current timestep = 1273. State = [[0.05939011 0.23775047]]. Action = [[ 0.03861588  0.07811326 -0.11990157  0.08981562]]. Reward = [0.]
Curr episode timestep = 733
Scene graph at timestep 1273 is [False, False, True, False, False, True]
Current timestep = 1274. State = [[0.06521038 0.24442908]]. Action = [[0.09127158 0.22555998 0.24081838 0.6758778 ]]. Reward = [0.]
Curr episode timestep = 734
Scene graph at timestep 1274 is [False, False, True, False, False, True]
Current timestep = 1275. State = [[0.0670004  0.25231105]]. Action = [[-0.24020636 -0.09848149 -0.06893444 -0.08942634]]. Reward = [0.]
Curr episode timestep = 735
Scene graph at timestep 1275 is [False, False, True, False, False, True]
Current timestep = 1276. State = [[0.05931613 0.25314343]]. Action = [[-5.7265460e-03  3.4265816e-02  1.3792515e-04 -6.9039005e-01]]. Reward = [0.]
Curr episode timestep = 736
Scene graph at timestep 1276 is [False, False, True, False, False, True]
Current timestep = 1277. State = [[0.05470609 0.25054154]]. Action = [[-0.10420366 -0.21590193 -0.10693425 -0.38025975]]. Reward = [0.]
Curr episode timestep = 737
Scene graph at timestep 1277 is [False, False, True, False, False, True]
Current timestep = 1278. State = [[0.0506629  0.24603927]]. Action = [[ 0.10624319  0.19929564 -0.00658226  0.16778636]]. Reward = [0.]
Curr episode timestep = 738
Scene graph at timestep 1278 is [False, False, True, False, False, True]
Current timestep = 1279. State = [[0.05361507 0.2521634 ]]. Action = [[0.03758088 0.11336488 0.19414479 0.07079756]]. Reward = [0.]
Curr episode timestep = 739
Scene graph at timestep 1279 is [False, False, True, False, False, True]
Current timestep = 1280. State = [[0.05393941 0.26049563]]. Action = [[-0.15864237  0.13600999  0.22035307  0.36385715]]. Reward = [0.]
Curr episode timestep = 740
Scene graph at timestep 1280 is [False, False, True, False, False, True]
Scene graph at timestep 1280 is [False, False, True, False, False, True]
State prediction error at timestep 1280 is tensor(0.0535, grad_fn=<MseLossBackward0>)
Current timestep = 1281. State = [[0.0512655 0.2646819]]. Action = [[ 0.14888835 -0.22262275 -0.15647268 -0.0775497 ]]. Reward = [0.]
Curr episode timestep = 741
Scene graph at timestep 1281 is [False, False, True, False, False, True]
Scene graph at timestep 1281 is [False, False, True, False, False, True]
State prediction error at timestep 1281 is tensor(0.0532, grad_fn=<MseLossBackward0>)
Current timestep = 1282. State = [[0.05618514 0.25621182]]. Action = [[ 0.09205309 -0.16903493  0.04292327 -0.0549354 ]]. Reward = [0.]
Curr episode timestep = 742
Scene graph at timestep 1282 is [False, False, True, False, False, True]
Current timestep = 1283. State = [[0.06057483 0.24940743]]. Action = [[-0.02795571  0.1330992   0.03371364 -0.0306347 ]]. Reward = [0.]
Curr episode timestep = 743
Scene graph at timestep 1283 is [False, False, True, False, False, True]
Current timestep = 1284. State = [[0.05811752 0.24717177]]. Action = [[-0.24474227 -0.24420847 -0.00608993 -0.636119  ]]. Reward = [0.]
Curr episode timestep = 744
Scene graph at timestep 1284 is [False, False, True, False, False, True]
Current timestep = 1285. State = [[0.04784827 0.23738676]]. Action = [[-0.03436512 -0.11386733 -0.09704335 -0.97320706]]. Reward = [0.]
Curr episode timestep = 745
Scene graph at timestep 1285 is [False, False, True, False, False, True]
Current timestep = 1286. State = [[0.04216824 0.22906081]]. Action = [[-0.03856874 -0.03776039  0.05947036 -0.42325926]]. Reward = [0.]
Curr episode timestep = 746
Scene graph at timestep 1286 is [False, True, False, False, False, True]
Current timestep = 1287. State = [[0.04009956 0.2227448 ]]. Action = [[ 0.0564568  -0.13702829 -0.19535136 -0.27124524]]. Reward = [0.]
Curr episode timestep = 747
Scene graph at timestep 1287 is [False, True, False, False, False, True]
Current timestep = 1288. State = [[0.04242247 0.2168425 ]]. Action = [[ 0.08581328  0.04299164 -0.24934296  0.37160897]]. Reward = [0.]
Curr episode timestep = 748
Scene graph at timestep 1288 is [False, True, False, False, False, True]
Current timestep = 1289. State = [[0.04557051 0.21765885]]. Action = [[-0.05417123  0.10931587  0.0713622  -0.24219596]]. Reward = [0.]
Curr episode timestep = 749
Scene graph at timestep 1289 is [False, True, False, False, False, True]
Current timestep = 1290. State = [[0.04232765 0.21981697]]. Action = [[-0.1998086  -0.11542878 -0.18479918 -0.6550987 ]]. Reward = [0.]
Curr episode timestep = 750
Scene graph at timestep 1290 is [False, True, False, False, False, True]
Scene graph at timestep 1290 is [False, True, False, False, False, True]
State prediction error at timestep 1290 is tensor(0.0498, grad_fn=<MseLossBackward0>)
Current timestep = 1291. State = [[0.03360761 0.21434915]]. Action = [[-0.0298921  -0.18252964  0.10698363 -0.3404389 ]]. Reward = [0.]
Curr episode timestep = 751
Scene graph at timestep 1291 is [False, True, False, False, False, True]
Current timestep = 1292. State = [[0.02835926 0.2086539 ]]. Action = [[-0.03469422  0.1586256  -0.13539675 -0.9780128 ]]. Reward = [0.]
Curr episode timestep = 752
Scene graph at timestep 1292 is [False, True, False, False, False, True]
Current timestep = 1293. State = [[0.02624956 0.20837858]]. Action = [[ 0.01458183 -0.20164408  0.03805101 -0.5320191 ]]. Reward = [0.]
Curr episode timestep = 753
Scene graph at timestep 1293 is [False, True, False, False, False, True]
Current timestep = 1294. State = [[0.02553705 0.2023968 ]]. Action = [[-0.01631317  0.01720893 -0.14235155  0.7329016 ]]. Reward = [0.]
Curr episode timestep = 754
Scene graph at timestep 1294 is [False, True, False, False, False, True]
Current timestep = 1295. State = [[0.02619094 0.20231663]]. Action = [[0.10039857 0.11182004 0.11187339 0.9179089 ]]. Reward = [0.]
Curr episode timestep = 755
Scene graph at timestep 1295 is [False, True, False, False, False, True]
Current timestep = 1296. State = [[0.0335638  0.20576762]]. Action = [[ 0.24994344 -0.02006881 -0.20784302 -0.48044074]]. Reward = [0.]
Curr episode timestep = 756
Scene graph at timestep 1296 is [False, True, False, False, False, True]
Current timestep = 1297. State = [[0.04676358 0.20311226]]. Action = [[ 0.12727809 -0.21225105  0.21205613 -0.8003256 ]]. Reward = [0.]
Curr episode timestep = 757
Scene graph at timestep 1297 is [False, True, False, False, False, True]
Current timestep = 1298. State = [[0.05942369 0.19288333]]. Action = [[ 0.24187323 -0.15764388  0.05644125 -0.78788316]]. Reward = [0.]
Curr episode timestep = 758
Scene graph at timestep 1298 is [False, True, False, False, False, True]
Current timestep = 1299. State = [[0.07289034 0.18283589]]. Action = [[ 0.06909016 -0.05798651 -0.18677273  0.00145566]]. Reward = [0.]
Curr episode timestep = 759
Scene graph at timestep 1299 is [False, False, True, False, False, True]
Current timestep = 1300. State = [[0.07925925 0.17708087]]. Action = [[-0.06927322 -0.00842533 -0.212256    0.48922706]]. Reward = [0.]
Curr episode timestep = 760
Scene graph at timestep 1300 is [False, False, True, False, False, True]
Current timestep = 1301. State = [[0.08196905 0.17446654]]. Action = [[ 0.199307   -0.0383203  -0.19136535 -0.7819587 ]]. Reward = [0.]
Curr episode timestep = 761
Scene graph at timestep 1301 is [False, False, True, False, False, True]
Current timestep = 1302. State = [[0.08598744 0.17387746]]. Action = [[-0.22152917  0.11824614  0.11069584 -0.2699204 ]]. Reward = [0.]
Curr episode timestep = 762
Scene graph at timestep 1302 is [False, False, True, False, False, True]
Current timestep = 1303. State = [[0.07993703 0.17768086]]. Action = [[-0.03187093 -0.00159255 -0.16350995 -0.6238828 ]]. Reward = [0.]
Curr episode timestep = 763
Scene graph at timestep 1303 is [False, False, True, False, False, True]
Current timestep = 1304. State = [[0.07688216 0.17812546]]. Action = [[ 0.0455457  -0.06883222 -0.14766923  0.1119504 ]]. Reward = [0.]
Curr episode timestep = 764
Scene graph at timestep 1304 is [False, False, True, False, False, True]
Current timestep = 1305. State = [[0.08007949 0.17600302]]. Action = [[ 0.18609676 -0.004025    0.16818625 -0.6155399 ]]. Reward = [0.]
Curr episode timestep = 765
Scene graph at timestep 1305 is [False, False, True, False, False, True]
Current timestep = 1306. State = [[0.08471943 0.17458871]]. Action = [[-0.19717994 -0.00491089  0.01450679  0.7709186 ]]. Reward = [0.]
Curr episode timestep = 766
Scene graph at timestep 1306 is [False, False, True, False, False, True]
Current timestep = 1307. State = [[0.08030187 0.17810468]]. Action = [[0.02349705 0.2461468  0.2046293  0.48650444]]. Reward = [0.]
Curr episode timestep = 767
Scene graph at timestep 1307 is [False, False, True, False, False, True]
Current timestep = 1308. State = [[0.07747571 0.18849422]]. Action = [[-0.1132375   0.0845176   0.04558951 -0.09096175]]. Reward = [0.]
Curr episode timestep = 768
Scene graph at timestep 1308 is [False, False, True, False, False, True]
Current timestep = 1309. State = [[0.07127256 0.19201869]]. Action = [[-0.11413246 -0.20770387  0.03921846 -0.5611831 ]]. Reward = [0.]
Curr episode timestep = 769
Scene graph at timestep 1309 is [False, False, True, False, False, True]
Current timestep = 1310. State = [[0.06223979 0.1857687 ]]. Action = [[-0.17672826 -0.04752493 -0.16992873  0.5172626 ]]. Reward = [0.]
Curr episode timestep = 770
Scene graph at timestep 1310 is [False, False, True, False, False, True]
Scene graph at timestep 1310 is [False, False, True, False, False, True]
State prediction error at timestep 1310 is tensor(0.0302, grad_fn=<MseLossBackward0>)
Current timestep = 1311. State = [[0.0551661 0.1830959]]. Action = [[ 0.15243232  0.08132431 -0.1108748  -0.91039985]]. Reward = [0.]
Curr episode timestep = 771
Scene graph at timestep 1311 is [False, False, True, False, False, True]
Scene graph at timestep 1311 is [False, False, True, False, False, True]
State prediction error at timestep 1311 is tensor(0.0528, grad_fn=<MseLossBackward0>)
Current timestep = 1312. State = [[0.05669928 0.18687463]]. Action = [[-0.07730839  0.14706725 -0.24996255  0.7023779 ]]. Reward = [0.]
Curr episode timestep = 772
Scene graph at timestep 1312 is [False, False, True, False, False, True]
Current timestep = 1313. State = [[0.05663941 0.19627257]]. Action = [[ 0.10477743  0.187368   -0.03190815  0.00772476]]. Reward = [0.]
Curr episode timestep = 773
Scene graph at timestep 1313 is [False, False, True, False, False, True]
Current timestep = 1314. State = [[0.05796597 0.20649956]]. Action = [[-0.13905364  0.05416089 -0.05104965  0.8825166 ]]. Reward = [0.]
Curr episode timestep = 774
Scene graph at timestep 1314 is [False, False, True, False, False, True]
Scene graph at timestep 1314 is [False, False, True, False, False, True]
State prediction error at timestep 1314 is tensor(0.0384, grad_fn=<MseLossBackward0>)
Current timestep = 1315. State = [[0.05666045 0.21424372]]. Action = [[ 0.1873455   0.12288719 -0.05421543  0.60208917]]. Reward = [0.]
Curr episode timestep = 775
Scene graph at timestep 1315 is [False, False, True, False, False, True]
Scene graph at timestep 1315 is [False, False, True, False, False, True]
State prediction error at timestep 1315 is tensor(0.0380, grad_fn=<MseLossBackward0>)
Current timestep = 1316. State = [[0.05924407 0.21833204]]. Action = [[-0.20924939 -0.13749291 -0.03636028 -0.21673805]]. Reward = [0.]
Curr episode timestep = 776
Scene graph at timestep 1316 is [False, False, True, False, False, True]
Scene graph at timestep 1316 is [False, False, True, False, False, True]
State prediction error at timestep 1316 is tensor(0.0409, grad_fn=<MseLossBackward0>)
Current timestep = 1317. State = [[0.05642393 0.21217534]]. Action = [[ 0.19857371 -0.2294303  -0.20410794  0.18080604]]. Reward = [0.]
Curr episode timestep = 777
Scene graph at timestep 1317 is [False, False, True, False, False, True]
Current timestep = 1318. State = [[0.06138637 0.20429583]]. Action = [[-0.00203459  0.14095253 -0.06334119  0.48095524]]. Reward = [0.]
Curr episode timestep = 778
Scene graph at timestep 1318 is [False, False, True, False, False, True]
Scene graph at timestep 1318 is [False, False, True, False, False, True]
State prediction error at timestep 1318 is tensor(0.0360, grad_fn=<MseLossBackward0>)
Current timestep = 1319. State = [[0.06722469 0.20876333]]. Action = [[ 0.2477524   0.17801195 -0.01550917 -0.7963705 ]]. Reward = [0.]
Curr episode timestep = 779
Scene graph at timestep 1319 is [False, False, True, False, False, True]
Current timestep = 1320. State = [[0.07708185 0.21476306]]. Action = [[-0.02800557 -0.0913972   0.06566533 -0.808341  ]]. Reward = [0.]
Curr episode timestep = 780
Scene graph at timestep 1320 is [False, False, True, False, False, True]
Current timestep = 1321. State = [[0.08103542 0.2152976 ]]. Action = [[0.06530493 0.06257841 0.13395041 0.93912196]]. Reward = [0.]
Curr episode timestep = 781
Scene graph at timestep 1321 is [False, False, True, False, False, True]
Current timestep = 1322. State = [[0.08163349 0.21697927]]. Action = [[-0.18208136 -0.00329237  0.19542089 -0.53354377]]. Reward = [0.]
Curr episode timestep = 782
Scene graph at timestep 1322 is [False, False, True, False, False, True]
Scene graph at timestep 1322 is [False, False, True, False, False, True]
State prediction error at timestep 1322 is tensor(0.0524, grad_fn=<MseLossBackward0>)
Current timestep = 1323. State = [[0.07767455 0.2162202 ]]. Action = [[ 0.12394971 -0.10520419  0.04385555 -0.6401779 ]]. Reward = [0.]
Curr episode timestep = 783
Scene graph at timestep 1323 is [False, False, True, False, False, True]
Current timestep = 1324. State = [[0.08105531 0.21341679]]. Action = [[ 0.0729897   0.0621759   0.1684885  -0.59076774]]. Reward = [0.]
Curr episode timestep = 784
Scene graph at timestep 1324 is [False, False, True, False, False, True]
Current timestep = 1325. State = [[0.08176371 0.2173792 ]]. Action = [[-0.21699865  0.21339327  0.19461685  0.06811118]]. Reward = [0.]
Curr episode timestep = 785
Scene graph at timestep 1325 is [False, False, True, False, False, True]
Scene graph at timestep 1325 is [False, False, True, False, False, True]
State prediction error at timestep 1325 is tensor(0.0479, grad_fn=<MseLossBackward0>)
Current timestep = 1326. State = [[0.07229381 0.22211249]]. Action = [[-0.18062228 -0.23947185 -0.06915525 -0.30648577]]. Reward = [0.]
Curr episode timestep = 786
Scene graph at timestep 1326 is [False, False, True, False, False, True]
Current timestep = 1327. State = [[0.06366382 0.21797885]]. Action = [[ 0.09978548  0.08401799 -0.18918335 -0.58971083]]. Reward = [0.]
Curr episode timestep = 787
Scene graph at timestep 1327 is [False, False, True, False, False, True]
Scene graph at timestep 1327 is [False, False, True, False, False, True]
State prediction error at timestep 1327 is tensor(0.0552, grad_fn=<MseLossBackward0>)
Current timestep = 1328. State = [[0.06418216 0.2179035 ]]. Action = [[ 0.01029459 -0.04156595 -0.0910342  -0.2322911 ]]. Reward = [0.]
Curr episode timestep = 788
Scene graph at timestep 1328 is [False, False, True, False, False, True]
Current timestep = 1329. State = [[0.06805572 0.21824627]]. Action = [[0.20134553 0.08534625 0.00362509 0.01439953]]. Reward = [0.]
Curr episode timestep = 789
Scene graph at timestep 1329 is [False, False, True, False, False, True]
Current timestep = 1330. State = [[0.07696003 0.21803565]]. Action = [[ 0.05282977 -0.17936862  0.03751829 -0.5074935 ]]. Reward = [0.]
Curr episode timestep = 790
Scene graph at timestep 1330 is [False, False, True, False, False, True]
Current timestep = 1331. State = [[0.08158935 0.21449704]]. Action = [[-0.03811093  0.13948387  0.17941302  0.6388998 ]]. Reward = [0.]
Curr episode timestep = 791
Scene graph at timestep 1331 is [False, False, True, False, False, True]
Scene graph at timestep 1331 is [False, False, True, False, False, True]
State prediction error at timestep 1331 is tensor(0.0424, grad_fn=<MseLossBackward0>)
Current timestep = 1332. State = [[0.08010466 0.2144933 ]]. Action = [[-0.12414792 -0.18226968 -0.07151194 -0.8999266 ]]. Reward = [0.]
Curr episode timestep = 792
Scene graph at timestep 1332 is [False, False, True, False, False, True]
Current timestep = 1333. State = [[0.07784203 0.21194592]]. Action = [[ 0.18251216  0.17097026 -0.03778809 -0.33931935]]. Reward = [0.]
Curr episode timestep = 793
Scene graph at timestep 1333 is [False, False, True, False, False, True]
Current timestep = 1334. State = [[0.08132713 0.21656626]]. Action = [[-0.10874574  0.04103974 -0.07447895  0.97703195]]. Reward = [0.]
Curr episode timestep = 794
Scene graph at timestep 1334 is [False, False, True, False, False, True]
Current timestep = 1335. State = [[0.07807942 0.21767382]]. Action = [[-0.09195496 -0.14103167  0.2259385   0.09468031]]. Reward = [0.]
Curr episode timestep = 795
Scene graph at timestep 1335 is [False, False, True, False, False, True]
Scene graph at timestep 1335 is [False, False, True, False, False, True]
State prediction error at timestep 1335 is tensor(0.0385, grad_fn=<MseLossBackward0>)
Current timestep = 1336. State = [[0.07601099 0.21187916]]. Action = [[ 0.16839242 -0.11952016  0.14656079  0.24956536]]. Reward = [0.]
Curr episode timestep = 796
Scene graph at timestep 1336 is [False, False, True, False, False, True]
Current timestep = 1337. State = [[0.08413109 0.20498586]]. Action = [[ 0.22686452 -0.04437178 -0.0040445   0.2416246 ]]. Reward = [0.]
Curr episode timestep = 797
Scene graph at timestep 1337 is [False, False, True, False, False, True]
Current timestep = 1338. State = [[0.09474619 0.20341296]]. Action = [[-0.00696521  0.16845357  0.06471798 -0.78855395]]. Reward = [0.]
Curr episode timestep = 798
Scene graph at timestep 1338 is [False, False, True, False, False, True]
Current timestep = 1339. State = [[0.10074627 0.2104059 ]]. Action = [[ 0.1407764   0.12023276  0.060103   -0.2902434 ]]. Reward = [0.]
Curr episode timestep = 799
Scene graph at timestep 1339 is [False, False, True, False, False, True]
Current timestep = 1340. State = [[0.10985706 0.21838893]]. Action = [[ 0.1777595   0.08016762 -0.09882709 -0.8651988 ]]. Reward = [0.]
Curr episode timestep = 800
Scene graph at timestep 1340 is [False, False, True, False, False, True]
Current timestep = 1341. State = [[0.11808207 0.2225934 ]]. Action = [[-0.06076136 -0.05950046  0.23660606  0.04775751]]. Reward = [0.]
Curr episode timestep = 801
Scene graph at timestep 1341 is [False, False, True, False, False, True]
Current timestep = 1342. State = [[0.11807416 0.22367527]]. Action = [[-0.08451858  0.08959144 -0.10043833 -0.08299869]]. Reward = [0.]
Curr episode timestep = 802
Scene graph at timestep 1342 is [False, False, True, False, False, True]
Current timestep = 1343. State = [[0.11575875 0.22790971]]. Action = [[ 0.04698363  0.05688137 -0.22477539  0.6145762 ]]. Reward = [0.]
Curr episode timestep = 803
Scene graph at timestep 1343 is [False, False, True, False, False, True]
Current timestep = 1344. State = [[0.11777794 0.23003271]]. Action = [[ 0.09718525 -0.08527771  0.10044214 -0.538831  ]]. Reward = [0.]
Curr episode timestep = 804
Scene graph at timestep 1344 is [False, False, True, False, False, True]
Current timestep = 1345. State = [[0.12549804 0.22960302]]. Action = [[ 0.24147084  0.0856398   0.24293754 -0.01301718]]. Reward = [0.]
Curr episode timestep = 805
Scene graph at timestep 1345 is [False, False, True, False, False, True]
Current timestep = 1346. State = [[0.13411985 0.23378006]]. Action = [[-0.1217041   0.1512304  -0.15787594  0.08457184]]. Reward = [0.]
Curr episode timestep = 806
Scene graph at timestep 1346 is [False, False, True, False, False, True]
Current timestep = 1347. State = [[0.13627    0.24314798]]. Action = [[ 0.15700385  0.14664084 -0.11739625 -0.12578547]]. Reward = [0.]
Curr episode timestep = 807
Scene graph at timestep 1347 is [False, False, True, False, False, True]
Current timestep = 1348. State = [[0.13987373 0.25207025]]. Action = [[-0.10191232  0.07658273 -0.08242181  0.29916036]]. Reward = [0.]
Curr episode timestep = 808
Scene graph at timestep 1348 is [False, False, True, False, False, True]
Current timestep = 1349. State = [[0.13483529 0.25392628]]. Action = [[-0.20435403 -0.2252322  -0.02980454 -0.87684935]]. Reward = [0.]
Curr episode timestep = 809
Scene graph at timestep 1349 is [False, False, True, False, False, True]
Current timestep = 1350. State = [[0.12707357 0.24936748]]. Action = [[ 0.04464069  0.07832885 -0.03932814  0.8024553 ]]. Reward = [0.]
Curr episode timestep = 810
Scene graph at timestep 1350 is [False, False, True, False, False, True]
Current timestep = 1351. State = [[0.12586717 0.24944691]]. Action = [[ 0.0166297  -0.01782763 -0.23628381 -0.84877074]]. Reward = [0.]
Curr episode timestep = 811
Scene graph at timestep 1351 is [False, False, True, False, False, True]
Current timestep = 1352. State = [[0.12390856 0.24547338]]. Action = [[-0.13826618 -0.2065529  -0.05869602 -0.2042833 ]]. Reward = [0.]
Curr episode timestep = 812
Scene graph at timestep 1352 is [False, False, True, False, False, True]
Current timestep = 1353. State = [[0.11917397 0.23942833]]. Action = [[ 0.02088141  0.10241228 -0.20144121 -0.4116683 ]]. Reward = [0.]
Curr episode timestep = 813
Scene graph at timestep 1353 is [False, False, True, False, False, True]
Current timestep = 1354. State = [[0.11751906 0.23675592]]. Action = [[-0.02841251 -0.21190006  0.15269971  0.97967863]]. Reward = [0.]
Curr episode timestep = 814
Scene graph at timestep 1354 is [False, False, True, False, False, True]
Current timestep = 1355. State = [[0.11284477 0.22768925]]. Action = [[-0.20861708 -0.08802299  0.24663651 -0.98970044]]. Reward = [0.]
Curr episode timestep = 815
Scene graph at timestep 1355 is [False, False, True, False, False, True]
Scene graph at timestep 1355 is [False, False, True, False, False, True]
State prediction error at timestep 1355 is tensor(0.0678, grad_fn=<MseLossBackward0>)
Current timestep = 1356. State = [[0.10252285 0.2243412 ]]. Action = [[-0.11976561  0.17729604 -0.14756343  0.39780867]]. Reward = [0.]
Curr episode timestep = 816
Scene graph at timestep 1356 is [False, False, True, False, False, True]
Current timestep = 1357. State = [[0.09226601 0.22598605]]. Action = [[-0.15638204 -0.17585444  0.1781688  -0.75059944]]. Reward = [0.]
Curr episode timestep = 817
Scene graph at timestep 1357 is [False, False, True, False, False, True]
Current timestep = 1358. State = [[0.0826371  0.22471628]]. Action = [[-0.05526802  0.20631588 -0.17795517  0.51232433]]. Reward = [0.]
Curr episode timestep = 818
Scene graph at timestep 1358 is [False, False, True, False, False, True]
Current timestep = 1359. State = [[0.08055576 0.23389997]]. Action = [[0.20813915 0.19773197 0.09652942 0.81603694]]. Reward = [0.]
Curr episode timestep = 819
Scene graph at timestep 1359 is [False, False, True, False, False, True]
Current timestep = 1360. State = [[0.08446662 0.24258609]]. Action = [[-0.14301738 -0.0560611  -0.09576461  0.9793464 ]]. Reward = [0.]
Curr episode timestep = 820
Scene graph at timestep 1360 is [False, False, True, False, False, True]
Scene graph at timestep 1360 is [False, False, True, False, False, True]
State prediction error at timestep 1360 is tensor(0.0514, grad_fn=<MseLossBackward0>)
Current timestep = 1361. State = [[0.08048251 0.2468632 ]]. Action = [[-0.0859713   0.17020285 -0.03441194  0.55365014]]. Reward = [0.]
Curr episode timestep = 821
Scene graph at timestep 1361 is [False, False, True, False, False, True]
Current timestep = 1362. State = [[0.07661892 0.25105378]]. Action = [[ 0.05094728 -0.16818514  0.0077844  -0.6228031 ]]. Reward = [0.]
Curr episode timestep = 822
Scene graph at timestep 1362 is [False, False, True, False, False, True]
Current timestep = 1363. State = [[0.07403714 0.24647732]]. Action = [[-0.18276621 -0.04295577  0.12477565  0.94984305]]. Reward = [0.]
Curr episode timestep = 823
Scene graph at timestep 1363 is [False, False, True, False, False, True]
Current timestep = 1364. State = [[0.06645511 0.23964024]]. Action = [[-0.05207473 -0.22576718 -0.07872918 -0.8478613 ]]. Reward = [0.]
Curr episode timestep = 824
Scene graph at timestep 1364 is [False, False, True, False, False, True]
Current timestep = 1365. State = [[0.0628378 0.2291889]]. Action = [[ 0.07440266 -0.05727392  0.0602062  -0.8185639 ]]. Reward = [0.]
Curr episode timestep = 825
Scene graph at timestep 1365 is [False, False, True, False, False, True]
Current timestep = 1366. State = [[0.06200865 0.22666319]]. Action = [[-0.14344141  0.21916726  0.13284284  0.61541736]]. Reward = [0.]
Curr episode timestep = 826
Scene graph at timestep 1366 is [False, False, True, False, False, True]
Current timestep = 1367. State = [[0.05663367 0.23197106]]. Action = [[-0.04083259 -0.0613949  -0.19001563 -0.2381354 ]]. Reward = [0.]
Curr episode timestep = 827
Scene graph at timestep 1367 is [False, False, True, False, False, True]
Scene graph at timestep 1367 is [False, False, True, False, False, True]
State prediction error at timestep 1367 is tensor(0.0486, grad_fn=<MseLossBackward0>)
Current timestep = 1368. State = [[0.05270039 0.22849387]]. Action = [[-0.02410713 -0.24272996 -0.15302381  0.8624433 ]]. Reward = [0.]
Curr episode timestep = 828
Scene graph at timestep 1368 is [False, False, True, False, False, True]
Current timestep = 1369. State = [[0.04712126 0.22049057]]. Action = [[-0.2238364   0.08407331 -0.10707289 -0.7963303 ]]. Reward = [0.]
Curr episode timestep = 829
Scene graph at timestep 1369 is [False, False, True, False, False, True]
Current timestep = 1370. State = [[0.04035617 0.22127935]]. Action = [[ 0.16838014  0.06828511 -0.15299958 -0.9273777 ]]. Reward = [0.]
Curr episode timestep = 830
Scene graph at timestep 1370 is [False, True, False, False, False, True]
Scene graph at timestep 1370 is [False, True, False, False, False, True]
State prediction error at timestep 1370 is tensor(0.0629, grad_fn=<MseLossBackward0>)
Current timestep = 1371. State = [[0.04427401 0.22156075]]. Action = [[ 0.06999093 -0.15201214 -0.01662232  0.4061731 ]]. Reward = [0.]
Curr episode timestep = 831
Scene graph at timestep 1371 is [False, True, False, False, False, True]
Current timestep = 1372. State = [[0.05176947 0.21451184]]. Action = [[ 0.21843475 -0.1518267   0.19998544 -0.3770007 ]]. Reward = [0.]
Curr episode timestep = 832
Scene graph at timestep 1372 is [False, True, False, False, False, True]
Current timestep = 1373. State = [[0.05994244 0.20558351]]. Action = [[-0.11515287 -0.06495276 -0.05951819 -0.1553638 ]]. Reward = [0.]
Curr episode timestep = 833
Scene graph at timestep 1373 is [False, False, True, False, False, True]
Scene graph at timestep 1373 is [False, False, True, False, False, True]
State prediction error at timestep 1373 is tensor(0.0388, grad_fn=<MseLossBackward0>)
Current timestep = 1374. State = [[0.05597481 0.19723248]]. Action = [[-0.22334698 -0.18709238 -0.05121171  0.52405   ]]. Reward = [0.]
Curr episode timestep = 834
Scene graph at timestep 1374 is [False, False, True, False, False, True]
Scene graph at timestep 1374 is [False, False, True, False, False, True]
State prediction error at timestep 1374 is tensor(0.0299, grad_fn=<MseLossBackward0>)
Current timestep = 1375. State = [[0.04677557 0.18724658]]. Action = [[ 0.00907037 -0.08009094  0.05806813 -0.4788034 ]]. Reward = [0.]
Curr episode timestep = 835
Scene graph at timestep 1375 is [False, False, True, False, False, True]
Current timestep = 1376. State = [[0.04263828 0.17736559]]. Action = [[-0.05717334 -0.22459285  0.02787209  0.28832114]]. Reward = [0.]
Curr episode timestep = 836
Scene graph at timestep 1376 is [False, True, False, False, False, True]
Scene graph at timestep 1376 is [False, True, False, False, False, True]
State prediction error at timestep 1376 is tensor(0.0235, grad_fn=<MseLossBackward0>)
Current timestep = 1377. State = [[0.03805796 0.16748424]]. Action = [[-0.09728658  0.05315703 -0.17934337 -0.5160359 ]]. Reward = [0.]
Curr episode timestep = 837
Scene graph at timestep 1377 is [False, True, False, False, False, True]
Current timestep = 1378. State = [[0.03060598 0.167922  ]]. Action = [[-0.17730765  0.15894878  0.17169917  0.75354576]]. Reward = [0.]
Curr episode timestep = 838
Scene graph at timestep 1378 is [False, True, False, False, False, True]
Current timestep = 1379. State = [[0.01849894 0.17092998]]. Action = [[-0.23177135 -0.1586573   0.14668325 -0.05879569]]. Reward = [0.]
Curr episode timestep = 839
Scene graph at timestep 1379 is [False, True, False, False, False, True]
Scene graph at timestep 1379 is [False, True, False, False, False, True]
State prediction error at timestep 1379 is tensor(0.0246, grad_fn=<MseLossBackward0>)
Current timestep = 1380. State = [[0.00608189 0.16583623]]. Action = [[-0.00996469 -0.09219684  0.13461116 -0.8868394 ]]. Reward = [0.]
Curr episode timestep = 840
Scene graph at timestep 1380 is [False, True, False, False, False, True]
Current timestep = 1381. State = [[-0.00074276  0.16021517]]. Action = [[-0.13083263 -0.03059176  0.11169705 -0.87840056]]. Reward = [0.]
Curr episode timestep = 841
Scene graph at timestep 1381 is [False, True, False, False, False, True]
Current timestep = 1382. State = [[-0.00789172  0.16027299]]. Action = [[-0.04766011  0.1968996  -0.2041577   0.9819441 ]]. Reward = [0.]
Curr episode timestep = 842
Scene graph at timestep 1382 is [False, True, False, False, False, True]
Current timestep = 1383. State = [[-0.01408959  0.16758004]]. Action = [[-0.1342385   0.05934796 -0.24775136  0.9408665 ]]. Reward = [0.]
Curr episode timestep = 843
Scene graph at timestep 1383 is [False, True, False, False, False, True]
Current timestep = 1384. State = [[-0.02078145  0.17224622]]. Action = [[-0.00609888 -0.01453425 -0.03736806  0.5666753 ]]. Reward = [0.]
Curr episode timestep = 844
Scene graph at timestep 1384 is [False, True, False, False, False, True]
Scene graph at timestep 1384 is [False, True, False, False, False, True]
State prediction error at timestep 1384 is tensor(0.0259, grad_fn=<MseLossBackward0>)
Current timestep = 1385. State = [[-0.02304951  0.17034425]]. Action = [[ 0.02960408 -0.19566336  0.1345908   0.20017159]]. Reward = [0.]
Curr episode timestep = 845
Scene graph at timestep 1385 is [False, True, False, False, False, True]
Current timestep = 1386. State = [[-0.02573683  0.15991162]]. Action = [[-0.20055059 -0.20848085  0.09057158 -0.8977506 ]]. Reward = [0.]
Curr episode timestep = 846
Scene graph at timestep 1386 is [False, True, False, False, False, True]
Current timestep = 1387. State = [[-0.03659688  0.14611922]]. Action = [[-0.21998806 -0.19606613  0.16754204 -0.9963842 ]]. Reward = [0.]
Curr episode timestep = 847
Scene graph at timestep 1387 is [False, True, False, False, False, True]
Current timestep = 1388. State = [[-0.0447297   0.13466257]]. Action = [[ 0.2312384  -0.01094575  0.12744015  0.9168606 ]]. Reward = [0.]
Curr episode timestep = 848
Scene graph at timestep 1388 is [False, True, False, False, False, True]
Scene graph at timestep 1388 is [False, True, False, False, False, True]
State prediction error at timestep 1388 is tensor(0.0198, grad_fn=<MseLossBackward0>)
Current timestep = 1389. State = [[-0.03932017  0.1302796 ]]. Action = [[0.04288876 0.01586807 0.24511468 0.79766417]]. Reward = [0.]
Curr episode timestep = 849
Scene graph at timestep 1389 is [False, True, False, False, False, True]
Current timestep = 1390. State = [[-0.03858841  0.12870872]]. Action = [[-0.22651388 -0.04118356  0.15181422  0.5601361 ]]. Reward = [0.]
Curr episode timestep = 850
Scene graph at timestep 1390 is [False, True, False, False, False, True]
Current timestep = 1391. State = [[-0.04329135  0.125451  ]]. Action = [[ 0.18036261 -0.1021997   0.04480088  0.16784978]]. Reward = [0.]
Curr episode timestep = 851
Scene graph at timestep 1391 is [False, True, False, False, False, True]
Scene graph at timestep 1391 is [False, True, False, False, False, True]
State prediction error at timestep 1391 is tensor(0.0169, grad_fn=<MseLossBackward0>)
Current timestep = 1392. State = [[-0.03904228  0.11771747]]. Action = [[ 0.02847669 -0.20263466 -0.03014946  0.42559135]]. Reward = [0.]
Curr episode timestep = 852
Scene graph at timestep 1392 is [False, True, False, False, False, True]
Scene graph at timestep 1392 is [False, True, False, False, True, False]
State prediction error at timestep 1392 is tensor(0.0146, grad_fn=<MseLossBackward0>)
Current timestep = 1393. State = [[-0.03714227  0.10939545]]. Action = [[-0.07143539  0.07043588 -0.04667535 -0.32955217]]. Reward = [0.]
Curr episode timestep = 853
Scene graph at timestep 1393 is [False, True, False, False, True, False]
Current timestep = 1394. State = [[-0.03870499  0.11193114]]. Action = [[ 0.00941947  0.21671349 -0.24366662  0.6636691 ]]. Reward = [0.]
Curr episode timestep = 854
Scene graph at timestep 1394 is [False, True, False, False, True, False]
Current timestep = 1395. State = [[-0.03950563  0.12372842]]. Action = [[-0.02222684  0.23012114  0.23655877 -0.04044628]]. Reward = [0.]
Curr episode timestep = 855
Scene graph at timestep 1395 is [False, True, False, False, True, False]
Current timestep = 1396. State = [[-0.03881231  0.13565895]]. Action = [[ 0.11444122 -0.00420915 -0.0521183  -0.87016034]]. Reward = [0.]
Curr episode timestep = 856
Scene graph at timestep 1396 is [False, True, False, False, True, False]
Current timestep = 1397. State = [[-0.032501   0.1392845]]. Action = [[ 0.15327802 -0.04241799  0.15854019  0.4939363 ]]. Reward = [0.]
Curr episode timestep = 857
Scene graph at timestep 1397 is [False, True, False, False, False, True]
Current timestep = 1398. State = [[-0.02311096  0.14008573]]. Action = [[ 0.1220383   0.07355124 -0.13471924 -0.2559666 ]]. Reward = [0.]
Curr episode timestep = 858
Scene graph at timestep 1398 is [False, True, False, False, False, True]
Scene graph at timestep 1398 is [False, True, False, False, False, True]
State prediction error at timestep 1398 is tensor(0.0239, grad_fn=<MseLossBackward0>)
Current timestep = 1399. State = [[-0.01497493  0.13946107]]. Action = [[ 0.03579351 -0.19710904 -0.23963405 -0.7817924 ]]. Reward = [0.]
Curr episode timestep = 859
Scene graph at timestep 1399 is [False, True, False, False, False, True]
Current timestep = 1400. State = [[-0.01011716  0.13040255]]. Action = [[ 0.05267641 -0.16623387  0.14776433  0.8758739 ]]. Reward = [0.]
Curr episode timestep = 860
Scene graph at timestep 1400 is [False, True, False, False, False, True]
Current timestep = 1401. State = [[-0.00934171  0.12242837]]. Action = [[-0.17312336  0.06406188 -0.00219437  0.4894439 ]]. Reward = [0.]
Curr episode timestep = 861
Scene graph at timestep 1401 is [False, True, False, False, False, True]
Scene graph at timestep 1401 is [False, True, False, False, True, False]
State prediction error at timestep 1401 is tensor(0.0166, grad_fn=<MseLossBackward0>)
Current timestep = 1402. State = [[-0.01481464  0.12439595]]. Action = [[-9.3728304e-06  1.7739227e-01 -1.7822887e-01 -4.0403998e-01]]. Reward = [0.]
Curr episode timestep = 862
Scene graph at timestep 1402 is [False, True, False, False, True, False]
Current timestep = 1403. State = [[-0.01867037  0.13099974]]. Action = [[-0.10365242 -0.00363201  0.1003505  -0.1231547 ]]. Reward = [0.]
Curr episode timestep = 863
Scene graph at timestep 1403 is [False, True, False, False, True, False]
Current timestep = 1404. State = [[-0.02262378  0.13710377]]. Action = [[ 0.05277446  0.22477818 -0.00948085  0.6396694 ]]. Reward = [0.]
Curr episode timestep = 864
Scene graph at timestep 1404 is [False, True, False, False, False, True]
Current timestep = 1405. State = [[-0.02125147  0.14425333]]. Action = [[ 0.07483765 -0.13627434 -0.2198      0.43707693]]. Reward = [0.]
Curr episode timestep = 865
Scene graph at timestep 1405 is [False, True, False, False, False, True]
Scene graph at timestep 1405 is [False, True, False, False, False, True]
State prediction error at timestep 1405 is tensor(0.0208, grad_fn=<MseLossBackward0>)
Current timestep = 1406. State = [[-0.01515038  0.13890314]]. Action = [[ 0.19717363 -0.2409649   0.21495122 -0.06264794]]. Reward = [0.]
Curr episode timestep = 866
Scene graph at timestep 1406 is [False, True, False, False, False, True]
Current timestep = 1407. State = [[-0.00268429  0.13228022]]. Action = [[0.24894011 0.22130609 0.20312643 0.6311238 ]]. Reward = [0.]
Curr episode timestep = 867
Scene graph at timestep 1407 is [False, True, False, False, False, True]
Current timestep = 1408. State = [[0.00675178 0.13929416]]. Action = [[-0.244354    0.17388391 -0.07409257 -0.17276174]]. Reward = [0.]
Curr episode timestep = 868
Scene graph at timestep 1408 is [False, True, False, False, False, True]
Scene graph at timestep 1408 is [False, True, False, False, False, True]
State prediction error at timestep 1408 is tensor(0.0246, grad_fn=<MseLossBackward0>)
Current timestep = 1409. State = [[0.00582143 0.14943074]]. Action = [[ 0.24516422  0.07899702 -0.12039742  0.7311609 ]]. Reward = [0.]
Curr episode timestep = 869
Scene graph at timestep 1409 is [False, True, False, False, False, True]
Current timestep = 1410. State = [[0.01641908 0.15449822]]. Action = [[ 0.24054968 -0.07944092 -0.02890582  0.8985534 ]]. Reward = [0.]
Curr episode timestep = 870
Scene graph at timestep 1410 is [False, True, False, False, False, True]
Scene graph at timestep 1410 is [False, True, False, False, False, True]
State prediction error at timestep 1410 is tensor(0.0230, grad_fn=<MseLossBackward0>)
Current timestep = 1411. State = [[0.02784982 0.1507558 ]]. Action = [[-0.06148675 -0.17303564  0.05569035  0.2721405 ]]. Reward = [0.]
Curr episode timestep = 871
Scene graph at timestep 1411 is [False, True, False, False, False, True]
Scene graph at timestep 1411 is [False, True, False, False, False, True]
State prediction error at timestep 1411 is tensor(0.0184, grad_fn=<MseLossBackward0>)
Current timestep = 1412. State = [[0.02775158 0.14306116]]. Action = [[-0.1603443  -0.03663385  0.12348464 -0.7957171 ]]. Reward = [0.]
Curr episode timestep = 872
Scene graph at timestep 1412 is [False, True, False, False, False, True]
Current timestep = 1413. State = [[0.02035192 0.14260146]]. Action = [[-0.10569946  0.23031276  0.10358047 -0.14720547]]. Reward = [0.]
Curr episode timestep = 873
Scene graph at timestep 1413 is [False, True, False, False, False, True]
Current timestep = 1414. State = [[0.01685509 0.14951015]]. Action = [[ 0.19867176 -0.03567937 -0.24672139 -0.3311345 ]]. Reward = [0.]
Curr episode timestep = 874
Scene graph at timestep 1414 is [False, True, False, False, False, True]
Scene graph at timestep 1414 is [False, True, False, False, False, True]
State prediction error at timestep 1414 is tensor(0.0271, grad_fn=<MseLossBackward0>)
Current timestep = 1415. State = [[0.02164939 0.14995435]]. Action = [[-0.03814244 -0.07325557 -0.2068084   0.76297355]]. Reward = [0.]
Curr episode timestep = 875
Scene graph at timestep 1415 is [False, True, False, False, False, True]
Current timestep = 1416. State = [[0.01964775 0.14717732]]. Action = [[-0.19867437 -0.02036829 -0.06387046  0.8974395 ]]. Reward = [0.]
Curr episode timestep = 876
Scene graph at timestep 1416 is [False, True, False, False, False, True]
Current timestep = 1417. State = [[0.01336506 0.14432043]]. Action = [[ 0.08833215 -0.07845002 -0.06496032  0.6889317 ]]. Reward = [0.]
Curr episode timestep = 877
Scene graph at timestep 1417 is [False, True, False, False, False, True]
Scene graph at timestep 1417 is [False, True, False, False, False, True]
State prediction error at timestep 1417 is tensor(0.0191, grad_fn=<MseLossBackward0>)
Current timestep = 1418. State = [[0.01570005 0.13826892]]. Action = [[ 0.13225782 -0.15963939  0.17036748  0.6466768 ]]. Reward = [0.]
Curr episode timestep = 878
Scene graph at timestep 1418 is [False, True, False, False, False, True]
Current timestep = 1419. State = [[0.02249945 0.13051094]]. Action = [[ 0.08316743 -0.01866658 -0.06681603  0.02020347]]. Reward = [0.]
Curr episode timestep = 879
Scene graph at timestep 1419 is [False, True, False, False, False, True]
Scene graph at timestep 1419 is [False, True, False, False, False, True]
State prediction error at timestep 1419 is tensor(0.0189, grad_fn=<MseLossBackward0>)
Current timestep = 1420. State = [[0.03070497 0.12386575]]. Action = [[ 0.18636614 -0.20061758 -0.00859058  0.52121985]]. Reward = [0.]
Curr episode timestep = 880
Scene graph at timestep 1420 is [False, True, False, False, False, True]
Current timestep = 1421. State = [[-0.2678322  -0.17314297]]. Action = [[ 0.05507278  0.00719222  0.05836833 -0.7209237 ]]. Reward = [1.]
Curr episode timestep = 881
Scene graph at timestep 1421 is [False, True, False, False, True, False]
Current timestep = 1422. State = [[-0.27098942 -0.17532702]]. Action = [[-0.19248036  0.05435869 -0.16908845  0.87397575]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1422 is [True, False, False, True, False, False]
Current timestep = 1423. State = [[-0.27625954 -0.17298174]]. Action = [[ 0.17168686  0.06118056  0.2221514  -0.6579847 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1423 is [True, False, False, True, False, False]
Current timestep = 1424. State = [[-0.27573255 -0.16731177]]. Action = [[-0.2248113   0.20882851 -0.09184071 -0.50027955]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1424 is [True, False, False, True, False, False]
Current timestep = 1425. State = [[-0.28321734 -0.16008675]]. Action = [[-0.02619165 -0.12431289 -0.14037396  0.03747094]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1425 is [True, False, False, True, False, False]
Current timestep = 1426. State = [[-0.28828198 -0.1596771 ]]. Action = [[-0.07519257  0.1031543  -0.06035842  0.5605725 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1426 is [True, False, False, True, False, False]
Current timestep = 1427. State = [[-0.29593065 -0.15344942]]. Action = [[-0.24720778  0.21405545 -0.1003278   0.26623762]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1427 is [True, False, False, True, False, False]
Current timestep = 1428. State = [[-0.3055059  -0.13993208]]. Action = [[0.12300181 0.24398375 0.14658052 0.28033924]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1428 is [True, False, False, True, False, False]
Current timestep = 1429. State = [[-0.3016738  -0.12589069]]. Action = [[ 0.23813993  0.06791463 -0.09315547 -0.5006823 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1429 is [True, False, False, True, False, False]
Scene graph at timestep 1429 is [True, False, False, True, False, False]
State prediction error at timestep 1429 is tensor(0.0440, grad_fn=<MseLossBackward0>)
Current timestep = 1430. State = [[-0.28995025 -0.11570133]]. Action = [[ 0.11287525  0.18590611 -0.05921684 -0.4256335 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1430 is [True, False, False, True, False, False]
Current timestep = 1431. State = [[-0.2781519  -0.10925449]]. Action = [[ 0.23760164 -0.19852291  0.17074502 -0.43949127]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1431 is [True, False, False, False, True, False]
Current timestep = 1432. State = [[-0.26760197 -0.11471835]]. Action = [[-0.11947951 -0.06913801  0.09246051 -0.988659  ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1432 is [True, False, False, False, True, False]
Current timestep = 1433. State = [[-0.26653266 -0.1195228 ]]. Action = [[ 0.0889532  -0.03219518  0.16391397 -0.5083436 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1433 is [True, False, False, False, True, False]
Current timestep = 1434. State = [[-0.26695603 -0.12183266]]. Action = [[-0.23878077  0.05398166 -0.06652188 -0.6639996 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1434 is [True, False, False, False, True, False]
Current timestep = 1435. State = [[-0.2770068  -0.12175322]]. Action = [[-0.12981619 -0.07168208 -0.19551867  0.01325774]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1435 is [True, False, False, False, True, False]
Current timestep = 1436. State = [[-0.28717592 -0.12101187]]. Action = [[-0.11503109  0.18145585  0.01000002  0.7110313 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1436 is [True, False, False, False, True, False]
Current timestep = 1437. State = [[-0.29148775 -0.11177431]]. Action = [[0.22999144 0.19308835 0.18119848 0.8989066 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1437 is [True, False, False, False, True, False]
Scene graph at timestep 1437 is [True, False, False, False, True, False]
State prediction error at timestep 1437 is tensor(0.0502, grad_fn=<MseLossBackward0>)
Current timestep = 1438. State = [[-0.28259662 -0.10172983]]. Action = [[ 0.17332     0.02559721 -0.07320419 -0.28548503]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1438 is [True, False, False, False, True, False]
Current timestep = 1439. State = [[-0.27086654 -0.09795579]]. Action = [[ 0.1424495  -0.05038805  0.02615961 -0.78481257]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1439 is [True, False, False, False, True, False]
Current timestep = 1440. State = [[-0.2604757 -0.0945365]]. Action = [[ 0.0913564   0.24561793 -0.20249861  0.94787943]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1440 is [True, False, False, False, True, False]
Scene graph at timestep 1440 is [True, False, False, False, True, False]
State prediction error at timestep 1440 is tensor(0.0444, grad_fn=<MseLossBackward0>)
Current timestep = 1441. State = [[-0.2503749  -0.08202173]]. Action = [[0.23538089 0.2154746  0.07993069 0.99782217]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1441 is [True, False, False, False, True, False]
Current timestep = 1442. State = [[-0.23864374 -0.06756366]]. Action = [[ 0.01165995  0.18209752 -0.12253964  0.7400501 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1442 is [True, False, False, False, True, False]
Current timestep = 1443. State = [[-0.2305224  -0.05651566]]. Action = [[ 0.23739445 -0.02161798 -0.01895383  0.06040668]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1443 is [True, False, False, False, True, False]
Current timestep = 1444. State = [[-0.22326104 -0.04977655]]. Action = [[-0.24505453  0.24897206 -0.01657294 -0.41195744]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1444 is [True, False, False, False, True, False]
Current timestep = 1445. State = [[-0.229131   -0.03789921]]. Action = [[-0.02193587  0.09050718  0.21957424 -0.8476604 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1445 is [True, False, False, False, True, False]
Current timestep = 1446. State = [[-0.23413566 -0.02905606]]. Action = [[-0.10351869  0.09518427 -0.0821754   0.22029376]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1446 is [True, False, False, False, True, False]
Scene graph at timestep 1446 is [True, False, False, False, True, False]
State prediction error at timestep 1446 is tensor(0.0391, grad_fn=<MseLossBackward0>)
Current timestep = 1447. State = [[-0.23626465 -0.0198959 ]]. Action = [[ 0.22733527  0.175556   -0.10755232  0.21291411]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1447 is [True, False, False, False, True, False]
Current timestep = 1448. State = [[-0.2268019 -0.0129032]]. Action = [[ 0.1560956  -0.10794447  0.05697662 -0.15279043]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1448 is [True, False, False, False, True, False]
Current timestep = 1449. State = [[-0.2170485  -0.01721974]]. Action = [[ 0.03632265 -0.21681999  0.02195328 -0.06629378]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1449 is [True, False, False, False, True, False]
Current timestep = 1450. State = [[-0.21354893 -0.02810191]]. Action = [[-0.07863444 -0.13003835 -0.1912988   0.79719496]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1450 is [True, False, False, False, True, False]
Scene graph at timestep 1450 is [True, False, False, False, True, False]
State prediction error at timestep 1450 is tensor(0.0307, grad_fn=<MseLossBackward0>)
Current timestep = 1451. State = [[-0.21706754 -0.03549466]]. Action = [[-0.12788588  0.05926144 -0.05465761  0.02246761]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1451 is [True, False, False, False, True, False]
Scene graph at timestep 1451 is [True, False, False, False, True, False]
State prediction error at timestep 1451 is tensor(0.0299, grad_fn=<MseLossBackward0>)
Current timestep = 1452. State = [[-0.21936122 -0.03757174]]. Action = [[ 0.24867111 -0.09874728  0.15296656 -0.9870909 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1452 is [True, False, False, False, True, False]
Scene graph at timestep 1452 is [True, False, False, False, True, False]
State prediction error at timestep 1452 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Current timestep = 1453. State = [[-0.20901103 -0.04450695]]. Action = [[ 0.19465154 -0.19140851 -0.14244464 -0.44438338]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1453 is [True, False, False, False, True, False]
Current timestep = 1454. State = [[-0.20004776 -0.05518632]]. Action = [[-0.13609709 -0.1060037   0.10235015 -0.072501  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1454 is [True, False, False, False, True, False]
Current timestep = 1455. State = [[-0.20099145 -0.06467158]]. Action = [[ 0.03210583 -0.15429612 -0.12053022 -0.00874412]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1455 is [True, False, False, False, True, False]
Current timestep = 1456. State = [[-0.20063068 -0.07471395]]. Action = [[ 0.01087227 -0.10432681 -0.15034659 -0.7072774 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1456 is [True, False, False, False, True, False]
Current timestep = 1457. State = [[-0.20113696 -0.08447063]]. Action = [[-0.0824877  -0.16769984  0.05332813 -0.67370635]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1457 is [True, False, False, False, True, False]
Current timestep = 1458. State = [[-0.20165268 -0.09695246]]. Action = [[ 0.18126738 -0.24792553 -0.20714028 -0.91014206]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1458 is [True, False, False, False, True, False]
Current timestep = 1459. State = [[-0.19532813 -0.11304656]]. Action = [[ 0.03433365 -0.20526212 -0.21773836  0.16473722]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1459 is [True, False, False, False, True, False]
Scene graph at timestep 1459 is [True, False, False, False, True, False]
State prediction error at timestep 1459 is tensor(0.0286, grad_fn=<MseLossBackward0>)
Current timestep = 1460. State = [[-0.1897639  -0.12266763]]. Action = [[ 0.12636265  0.18146151 -0.1883857  -0.3128065 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1460 is [True, False, False, False, True, False]
Current timestep = 1461. State = [[-0.18357295 -0.12074167]]. Action = [[-0.0163278  -0.02052501  0.11605904 -0.5485547 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1461 is [True, False, False, False, True, False]
Current timestep = 1462. State = [[-0.18343754 -0.12141456]]. Action = [[-0.11018431 -0.08225951 -0.15948232 -0.7705038 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1462 is [True, False, False, False, True, False]
Current timestep = 1463. State = [[-0.18513373 -0.12727766]]. Action = [[ 0.1627729  -0.20431986 -0.02506198  0.20010865]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1463 is [True, False, False, False, True, False]
Current timestep = 1464. State = [[-0.17831427 -0.13475114]]. Action = [[ 0.15118581  0.10075063  0.13861653 -0.32192743]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1464 is [True, False, False, True, False, False]
Scene graph at timestep 1464 is [True, False, False, True, False, False]
State prediction error at timestep 1464 is tensor(0.0169, grad_fn=<MseLossBackward0>)
Current timestep = 1465. State = [[-0.16690807 -0.1355426 ]]. Action = [[ 0.23232955 -0.07426438 -0.16455501 -0.2254268 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1465 is [True, False, False, True, False, False]
Current timestep = 1466. State = [[-0.1543251 -0.1367543]]. Action = [[ 0.03654388  0.08414179  0.06477359 -0.4078287 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1466 is [True, False, False, True, False, False]
Current timestep = 1467. State = [[-0.14740402 -0.1370888 ]]. Action = [[ 0.06814337 -0.17106734 -0.11389224 -0.9357827 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1467 is [True, False, False, True, False, False]
Current timestep = 1468. State = [[-0.14443858 -0.14201365]]. Action = [[-0.10236847  0.04482284  0.14726043 -0.59644294]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1468 is [True, False, False, True, False, False]
Current timestep = 1469. State = [[-0.14584005 -0.14452185]]. Action = [[ 0.07304806 -0.13616452 -0.19821747 -0.8247469 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1469 is [True, False, False, True, False, False]
Current timestep = 1470. State = [[-0.14695476 -0.14643756]]. Action = [[-0.19386593  0.2262106  -0.01179475 -0.7957287 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1470 is [True, False, False, True, False, False]
Current timestep = 1471. State = [[-0.15360276 -0.14069945]]. Action = [[ 0.00848648 -0.06293267  0.16277173  0.7863995 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1471 is [True, False, False, True, False, False]
Current timestep = 1472. State = [[-0.15465702 -0.13752437]]. Action = [[ 0.10339695  0.17139143  0.1891452  -0.75910056]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1472 is [True, False, False, True, False, False]
Current timestep = 1473. State = [[-0.15049566 -0.13060002]]. Action = [[ 0.05008605  0.03748822 -0.07871237  0.8815112 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1473 is [True, False, False, True, False, False]
Current timestep = 1474. State = [[-0.14796254 -0.12549166]]. Action = [[-0.0620909   0.07882702  0.15929827 -0.64845496]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1474 is [True, False, False, True, False, False]
Current timestep = 1475. State = [[-0.15109967 -0.11767996]]. Action = [[-0.13008726  0.22641128  0.02386802  0.86356854]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1475 is [True, False, False, True, False, False]
Current timestep = 1476. State = [[-0.15631264 -0.11110752]]. Action = [[ 0.03237295 -0.23970695 -0.21937841 -0.629161  ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1476 is [True, False, False, False, True, False]
Current timestep = 1477. State = [[-0.15985651 -0.1181265 ]]. Action = [[-0.17907785 -0.10818997 -0.10747159  0.87127304]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1477 is [True, False, False, False, True, False]
Scene graph at timestep 1477 is [True, False, False, False, True, False]
State prediction error at timestep 1477 is tensor(0.0218, grad_fn=<MseLossBackward0>)
Current timestep = 1478. State = [[-0.16842246 -0.12149132]]. Action = [[-0.0928511   0.19757015 -0.24503157 -0.59886295]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1478 is [True, False, False, False, True, False]
Current timestep = 1479. State = [[-0.17814596 -0.11514253]]. Action = [[-0.23681471  0.09254724  0.12862653  0.6064873 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1479 is [True, False, False, False, True, False]
Current timestep = 1480. State = [[-0.18886569 -0.10608065]]. Action = [[ 0.06596559  0.195279   -0.12101272 -0.9119801 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1480 is [True, False, False, False, True, False]
Current timestep = 1481. State = [[-0.18976875 -0.09484735]]. Action = [[0.05482209 0.11278737 0.20448905 0.20268762]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1481 is [True, False, False, False, True, False]
Current timestep = 1482. State = [[-0.18913506 -0.08776461]]. Action = [[-0.10273629 -0.0402026  -0.05217601  0.6220615 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1482 is [True, False, False, False, True, False]
Scene graph at timestep 1482 is [True, False, False, False, True, False]
State prediction error at timestep 1482 is tensor(0.0281, grad_fn=<MseLossBackward0>)
Current timestep = 1483. State = [[-0.19232135 -0.08616171]]. Action = [[ 0.00447187  0.0138247   0.06775671 -0.32596886]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1483 is [True, False, False, False, True, False]
Current timestep = 1484. State = [[-0.19151002 -0.08865266]]. Action = [[ 0.13692671 -0.21220072 -0.15111175  0.79768944]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1484 is [True, False, False, False, True, False]
Current timestep = 1485. State = [[-0.18360408 -0.09716362]]. Action = [[ 0.20921734 -0.04607627  0.03933725  0.14830208]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1485 is [True, False, False, False, True, False]
Current timestep = 1486. State = [[-0.17216106 -0.10119745]]. Action = [[ 0.08082825  0.06626844  0.15397298 -0.30198348]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1486 is [True, False, False, False, True, False]
Current timestep = 1487. State = [[-0.16683114 -0.09788789]]. Action = [[-0.12457719  0.16432166 -0.12260918  0.977036  ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1487 is [True, False, False, False, True, False]
Current timestep = 1488. State = [[-0.16952229 -0.09377364]]. Action = [[-0.0158821  -0.17961182  0.2414957   0.7691494 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1488 is [True, False, False, False, True, False]
Current timestep = 1489. State = [[-0.16887881 -0.10118333]]. Action = [[ 0.18903792 -0.22211187  0.07171595  0.31140244]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1489 is [True, False, False, False, True, False]
Scene graph at timestep 1489 is [True, False, False, False, True, False]
State prediction error at timestep 1489 is tensor(0.0220, grad_fn=<MseLossBackward0>)
Current timestep = 1490. State = [[-0.1607572  -0.11369719]]. Action = [[ 0.10001141 -0.12853369 -0.14697197 -0.08418   ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1490 is [True, False, False, False, True, False]
Current timestep = 1491. State = [[-0.15313512 -0.11992186]]. Action = [[ 0.06692517  0.17220068  0.129552   -0.33323383]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1491 is [True, False, False, False, True, False]
Current timestep = 1492. State = [[-0.15120345 -0.11982506]]. Action = [[-0.23140378 -0.1993776  -0.18484388 -0.3563043 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1492 is [True, False, False, False, True, False]
Current timestep = 1493. State = [[-0.15557474 -0.12466888]]. Action = [[ 0.2269142   0.03325504 -0.14876518 -0.31479347]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1493 is [True, False, False, False, True, False]
Current timestep = 1494. State = [[-0.15034148 -0.1237211 ]]. Action = [[-0.04021469  0.15895781  0.01895332 -0.04412127]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1494 is [True, False, False, False, True, False]
Current timestep = 1495. State = [[-0.14752796 -0.12127198]]. Action = [[ 0.11480993 -0.221387    0.13022286 -0.5517558 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1495 is [True, False, False, False, True, False]
Current timestep = 1496. State = [[-0.14441934 -0.13054633]]. Action = [[-0.10431898 -0.19577757  0.14925897 -0.32500124]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1496 is [True, False, False, False, True, False]
Scene graph at timestep 1496 is [True, False, False, True, False, False]
State prediction error at timestep 1496 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Current timestep = 1497. State = [[-0.14355196 -0.14125828]]. Action = [[ 0.23101717 -0.06201531  0.07373783 -0.8970027 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1497 is [True, False, False, True, False, False]
Current timestep = 1498. State = [[-0.13633548 -0.15095566]]. Action = [[-0.05789775 -0.20787759  0.07390189 -0.36174238]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1498 is [True, False, False, True, False, False]
Current timestep = 1499. State = [[-0.13560525 -0.15947965]]. Action = [[-0.01549166  0.08898932 -0.16466472  0.8975308 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1499 is [True, False, False, True, False, False]
Current timestep = 1500. State = [[-0.13811274 -0.16017732]]. Action = [[-0.14222768 -0.01687457  0.07885259  0.90320766]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1500 is [True, False, False, True, False, False]
Current timestep = 1501. State = [[-0.1437796 -0.1586247]]. Action = [[0.0143964  0.1153295  0.24628508 0.9310291 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1501 is [True, False, False, True, False, False]
Current timestep = 1502. State = [[-0.1443358  -0.15238062]]. Action = [[ 0.08242378  0.12782443 -0.23540874  0.8240664 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1502 is [True, False, False, True, False, False]
Scene graph at timestep 1502 is [True, False, False, True, False, False]
State prediction error at timestep 1502 is tensor(0.0202, grad_fn=<MseLossBackward0>)
Current timestep = 1503. State = [[-0.1417335  -0.14462744]]. Action = [[-0.01622063  0.08394906  0.1543678  -0.9310339 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1503 is [True, False, False, True, False, False]
Current timestep = 1504. State = [[-0.13867526 -0.1420778 ]]. Action = [[ 0.1747993  -0.21021064  0.1825288   0.00847363]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1504 is [True, False, False, True, False, False]
Current timestep = 1505. State = [[-0.13359289 -0.14938638]]. Action = [[-0.12748857 -0.06642781  0.15595758 -0.9698017 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1505 is [True, False, False, True, False, False]
Scene graph at timestep 1505 is [True, False, False, True, False, False]
State prediction error at timestep 1505 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 1506. State = [[-0.13381088 -0.15497334]]. Action = [[ 0.14491957 -0.04738808  0.06900564  0.83143306]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 1506 is [True, False, False, True, False, False]
Scene graph at timestep 1506 is [True, False, False, True, False, False]
State prediction error at timestep 1506 is tensor(0.0182, grad_fn=<MseLossBackward0>)
Current timestep = 1507. State = [[-0.12853178 -0.15590382]]. Action = [[ 0.05688256  0.17995459  0.1057598  -0.70894635]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1507 is [True, False, False, True, False, False]
Current timestep = 1508. State = [[-0.1271588  -0.15222102]]. Action = [[-0.20198    -0.10930026  0.1891244  -0.83088493]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1508 is [True, False, False, True, False, False]
Current timestep = 1509. State = [[-0.1314679 -0.1568644]]. Action = [[ 0.15888071 -0.20985444 -0.10034737  0.7132865 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1509 is [True, False, False, True, False, False]
Current timestep = 1510. State = [[-0.12579426 -0.16735072]]. Action = [[ 0.15977609 -0.09778197 -0.13167647 -0.46859086]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1510 is [True, False, False, True, False, False]
Current timestep = 1511. State = [[-0.11586862 -0.17672433]]. Action = [[ 0.13721454 -0.13255052 -0.03481971 -0.14648324]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 1511 is [True, False, False, True, False, False]
Current timestep = 1512. State = [[-0.11094966 -0.18519135]]. Action = [[-0.24314587 -0.0365684  -0.22101456  0.59059656]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1512 is [True, False, False, True, False, False]
Current timestep = 1513. State = [[-0.11695049 -0.18765621]]. Action = [[ 0.03875801  0.08510622 -0.24125108  0.8842988 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1513 is [True, False, False, True, False, False]
Current timestep = 1514. State = [[-0.11924475 -0.18731834]]. Action = [[-0.06375158 -0.09571078  0.23207241 -0.6723196 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 1514 is [True, False, False, True, False, False]
Current timestep = 1515. State = [[-0.11966731 -0.19045827]]. Action = [[ 0.16565138 -0.03809233 -0.01281518  0.12458003]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1515 is [True, False, False, True, False, False]
Current timestep = 1516. State = [[-0.11311311 -0.19535004]]. Action = [[ 0.07713476 -0.13879272  0.1897164  -0.6315275 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1516 is [True, False, False, True, False, False]
Current timestep = 1517. State = [[-0.10777361 -0.20514281]]. Action = [[ 0.0033794  -0.22618242  0.16747293  0.99288607]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 1517 is [True, False, False, True, False, False]
Current timestep = 1518. State = [[-0.10325555 -0.21427482]]. Action = [[0.18156385 0.11215815 0.0722844  0.03558469]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 1518 is [True, False, False, True, False, False]
Current timestep = 1519. State = [[-0.09345226 -0.21161623]]. Action = [[ 0.14504251  0.16785336 -0.12989134 -0.85930544]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 1519 is [True, False, False, True, False, False]
Current timestep = 1520. State = [[-0.08477142 -0.20329326]]. Action = [[-2.7748942e-04  9.9402219e-02  1.1469799e-01  4.3665123e-01]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1520 is [True, False, False, True, False, False]
Scene graph at timestep 1520 is [True, False, False, True, False, False]
State prediction error at timestep 1520 is tensor(0.0169, grad_fn=<MseLossBackward0>)
Current timestep = 1521. State = [[-0.08514442 -0.19663508]]. Action = [[-2.4261966e-01  2.0034313e-02 -1.1442602e-04 -8.5542548e-01]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1521 is [True, False, False, True, False, False]
Scene graph at timestep 1521 is [True, False, False, True, False, False]
State prediction error at timestep 1521 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 1522. State = [[-0.09192274 -0.192891  ]]. Action = [[ 0.13475487  0.01869702  0.2319752  -0.05900693]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1522 is [True, False, False, True, False, False]
Current timestep = 1523. State = [[-0.087303   -0.19318688]]. Action = [[ 0.20037115 -0.13073575  0.11222276 -0.34181857]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1523 is [True, False, False, True, False, False]
Current timestep = 1524. State = [[-0.07751825 -0.1997496 ]]. Action = [[ 0.060597   -0.12618218 -0.04698414  0.65645385]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1524 is [True, False, False, True, False, False]
Current timestep = 1525. State = [[-0.07190232 -0.20386524]]. Action = [[-0.01247194  0.16931424 -0.12584168 -0.73916453]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1525 is [True, False, False, True, False, False]
Current timestep = 1526. State = [[-0.06761764 -0.19792937]]. Action = [[ 0.1965335   0.11252865  0.1578232  -0.8942284 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1526 is [True, False, False, True, False, False]
Current timestep = 1527. State = [[-0.0612154  -0.19019361]]. Action = [[-0.1141367   0.11963069  0.08348763  0.31896913]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1527 is [True, False, False, True, False, False]
Current timestep = 1528. State = [[-0.06282336 -0.17989422]]. Action = [[-0.01652764  0.21818033  0.0226818  -0.36756432]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1528 is [True, False, False, True, False, False]
Current timestep = 1529. State = [[-0.06667415 -0.17249714]]. Action = [[-0.1675184  -0.21507888 -0.03037255 -0.6453821 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1529 is [True, False, False, True, False, False]
Current timestep = 1530. State = [[-0.07631274 -0.17622784]]. Action = [[-0.17121682  0.02318612 -0.24025358  0.21219873]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1530 is [True, False, False, True, False, False]
Current timestep = 1531. State = [[-0.08934787 -0.1782786 ]]. Action = [[-0.24996828 -0.05437325  0.1464631   0.85832524]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1531 is [True, False, False, True, False, False]
Current timestep = 1532. State = [[-0.10319878 -0.17866486]]. Action = [[-0.03858866  0.11543369  0.02609327  0.5494752 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1532 is [True, False, False, True, False, False]
Current timestep = 1533. State = [[-0.1109544  -0.17796388]]. Action = [[-0.10481673 -0.17353852 -0.10691974  0.48358107]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1533 is [True, False, False, True, False, False]
Scene graph at timestep 1533 is [True, False, False, True, False, False]
State prediction error at timestep 1533 is tensor(0.0218, grad_fn=<MseLossBackward0>)
Current timestep = 1534. State = [[-0.11538204 -0.18318853]]. Action = [[ 0.11761346 -0.01848753 -0.05908108 -0.45726693]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1534 is [True, False, False, True, False, False]
Current timestep = 1535. State = [[-0.11609308 -0.1898891 ]]. Action = [[-0.2267585  -0.22467996  0.06813091  0.8673451 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1535 is [True, False, False, True, False, False]
Current timestep = 1536. State = [[-0.12076276 -0.19969903]]. Action = [[ 0.21539432 -0.04887542  0.19553524 -0.17868888]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1536 is [True, False, False, True, False, False]
Current timestep = 1537. State = [[-0.11643998 -0.20198373]]. Action = [[-0.06840779  0.23497373  0.14318275  0.90798736]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1537 is [True, False, False, True, False, False]
Current timestep = 1538. State = [[-0.11692168 -0.19820777]]. Action = [[-0.02524194 -0.21869819 -0.18860799 -0.56074566]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1538 is [True, False, False, True, False, False]
Scene graph at timestep 1538 is [True, False, False, True, False, False]
State prediction error at timestep 1538 is tensor(0.0094, grad_fn=<MseLossBackward0>)
Current timestep = 1539. State = [[-0.12097207 -0.20389605]]. Action = [[-0.18517758 -0.0208777   0.06754589 -0.55325544]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1539 is [True, False, False, True, False, False]
Current timestep = 1540. State = [[-0.13147059 -0.20863076]]. Action = [[-0.19018899 -0.10459226 -0.1604339   0.6811545 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1540 is [True, False, False, True, False, False]
Current timestep = 1541. State = [[-0.14074613 -0.21500549]]. Action = [[ 0.08596671 -0.1059725   0.06582382  0.3542813 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1541 is [True, False, False, True, False, False]
Current timestep = 1542. State = [[-0.14137915 -0.22165443]]. Action = [[-0.01453756 -0.0345957   0.10252136  0.469761  ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1542 is [True, False, False, True, False, False]
Current timestep = 1543. State = [[-0.14153308 -0.22567634]]. Action = [[ 0.00712687 -0.03184766  0.13747126 -0.3305968 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1543 is [True, False, False, True, False, False]
Scene graph at timestep 1543 is [True, False, False, True, False, False]
State prediction error at timestep 1543 is tensor(0.0199, grad_fn=<MseLossBackward0>)
Current timestep = 1544. State = [[-0.14339933 -0.22684245]]. Action = [[-0.14220524  0.08824372 -0.16857329 -0.63738257]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1544 is [True, False, False, True, False, False]
Current timestep = 1545. State = [[-0.1501756  -0.22282754]]. Action = [[-0.08934128  0.09128487 -0.15157321 -0.66707754]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1545 is [True, False, False, True, False, False]
Current timestep = 1546. State = [[-0.15835659 -0.220417  ]]. Action = [[-0.17252237 -0.1388101   0.2046277   0.64923024]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1546 is [True, False, False, True, False, False]
Current timestep = 1547. State = [[-0.17005444 -0.22525245]]. Action = [[-0.20118238 -0.08656622 -0.22064956 -0.6899819 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1547 is [True, False, False, True, False, False]
Current timestep = 1548. State = [[-0.18018067 -0.22751112]]. Action = [[ 0.06851876  0.15445703 -0.20788582 -0.9139861 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1548 is [True, False, False, True, False, False]
Current timestep = 1549. State = [[-0.18235481 -0.22623338]]. Action = [[-0.06025386 -0.1685206  -0.10699815  0.4761808 ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 1549 is [True, False, False, True, False, False]
Current timestep = 1550. State = [[-0.18287443 -0.23114602]]. Action = [[ 0.13376868 -0.02912648 -0.09925418 -0.1555996 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 1550 is [True, False, False, True, False, False]
Current timestep = 1551. State = [[-0.18016659 -0.23286779]]. Action = [[-0.11406666  0.10948873  0.07485563 -0.84251654]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 1551 is [True, False, False, True, False, False]
Current timestep = 1552. State = [[-0.17948252 -0.22799002]]. Action = [[ 0.23131719  0.09357601 -0.12337504 -0.7901166 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 1552 is [True, False, False, True, False, False]
Scene graph at timestep 1552 is [True, False, False, True, False, False]
State prediction error at timestep 1552 is tensor(0.0135, grad_fn=<MseLossBackward0>)
Current timestep = 1553. State = [[-0.17182294 -0.22314616]]. Action = [[-0.00912058  0.00828475  0.24836129 -0.70551646]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 1553 is [True, False, False, True, False, False]
Scene graph at timestep 1553 is [True, False, False, True, False, False]
State prediction error at timestep 1553 is tensor(0.0164, grad_fn=<MseLossBackward0>)
Current timestep = 1554. State = [[-0.16630226 -0.21817054]]. Action = [[ 0.18552434  0.17247751 -0.16956481  0.22089267]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 1554 is [True, False, False, True, False, False]
Scene graph at timestep 1554 is [True, False, False, True, False, False]
State prediction error at timestep 1554 is tensor(0.0355, grad_fn=<MseLossBackward0>)
Current timestep = 1555. State = [[-0.16129535 -0.21093269]]. Action = [[-0.21318126  0.02183416 -0.2392987  -0.5131773 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 1555 is [True, False, False, True, False, False]
Current timestep = 1556. State = [[-0.16760838 -0.20652364]]. Action = [[-0.07168841  0.03962782  0.21268046 -0.15783286]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 1556 is [True, False, False, True, False, False]
Scene graph at timestep 1556 is [True, False, False, True, False, False]
State prediction error at timestep 1556 is tensor(0.0234, grad_fn=<MseLossBackward0>)
Current timestep = 1557. State = [[-0.17015873 -0.200805  ]]. Action = [[ 0.19076645  0.18143302  0.07544598 -0.21380079]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 1557 is [True, False, False, True, False, False]
Current timestep = 1558. State = [[-0.16327588 -0.19117802]]. Action = [[ 0.08573902  0.12406614 -0.22688474  0.5663972 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 1558 is [True, False, False, True, False, False]
Current timestep = 1559. State = [[-0.15444012 -0.18212643]]. Action = [[0.21212238 0.07675299 0.01336873 0.35557103]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 1559 is [True, False, False, True, False, False]
Current timestep = 1560. State = [[-0.14482465 -0.17511043]]. Action = [[-0.05357824  0.0999321  -0.0575154   0.3447293 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 1560 is [True, False, False, True, False, False]
Current timestep = 1561. State = [[-0.13976744 -0.16951361]]. Action = [[ 0.23056406 -0.02878143  0.01231131  0.34785795]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 1561 is [True, False, False, True, False, False]
Current timestep = 1562. State = [[-0.1286191  -0.17246547]]. Action = [[ 0.12870884 -0.24686961  0.22347039 -0.47278446]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 1562 is [True, False, False, True, False, False]
Scene graph at timestep 1562 is [True, False, False, True, False, False]
State prediction error at timestep 1562 is tensor(0.0098, grad_fn=<MseLossBackward0>)
Current timestep = 1563. State = [[-0.11884638 -0.18299958]]. Action = [[ 0.08202705 -0.07799497 -0.20059244  0.75536084]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 1563 is [True, False, False, True, False, False]
Current timestep = 1564. State = [[-0.11311503 -0.1870956 ]]. Action = [[-0.03748515  0.17453238 -0.01389033 -0.48162043]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 1564 is [True, False, False, True, False, False]
Current timestep = 1565. State = [[-0.11383625 -0.18450476]]. Action = [[-0.10384488 -0.11481656  0.20424873 -0.65664047]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 1565 is [True, False, False, True, False, False]
Current timestep = 1566. State = [[-0.11935896 -0.18647924]]. Action = [[-0.09791338  0.02152732 -0.20028707 -0.7373053 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 1566 is [True, False, False, True, False, False]
Current timestep = 1567. State = [[-0.12209667 -0.18532804]]. Action = [[ 0.19409624  0.07794231 -0.16623023 -0.20144552]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 1567 is [True, False, False, True, False, False]
Current timestep = 1568. State = [[-0.11416581 -0.17995225]]. Action = [[ 0.16198659  0.16358548 -0.17260817  0.4732356 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 1568 is [True, False, False, True, False, False]
Current timestep = 1569. State = [[-0.10552278 -0.17065886]]. Action = [[-0.01373787  0.13206816 -0.03317626  0.37834978]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 1569 is [True, False, False, True, False, False]
Scene graph at timestep 1569 is [True, False, False, True, False, False]
State prediction error at timestep 1569 is tensor(0.0173, grad_fn=<MseLossBackward0>)
Current timestep = 1570. State = [[-0.10285181 -0.16200686]]. Action = [[-0.00259484  0.04814088  0.14823142 -0.08321935]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 1570 is [True, False, False, True, False, False]
Current timestep = 1571. State = [[-0.10259909 -0.15740807]]. Action = [[-0.02409764 -0.00508605 -0.08040455  0.39519787]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 1571 is [True, False, False, True, False, False]
Current timestep = 1572. State = [[-0.10235323 -0.15456824]]. Action = [[ 0.07659221  0.08958796 -0.13830693 -0.9918606 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 1572 is [True, False, False, True, False, False]
Current timestep = 1573. State = [[-0.10171191 -0.1497211 ]]. Action = [[-0.14474124  0.08247668  0.10609451  0.5921525 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 1573 is [True, False, False, True, False, False]
Current timestep = 1574. State = [[-0.10936316 -0.143299  ]]. Action = [[-0.21373004  0.12062463 -0.18552187 -0.3042485 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 1574 is [True, False, False, True, False, False]
Current timestep = 1575. State = [[-0.1215599  -0.13963987]]. Action = [[-0.13186486 -0.17569086 -0.22998446 -0.23156142]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 1575 is [True, False, False, True, False, False]
Current timestep = 1576. State = [[-0.13069955 -0.14204815]]. Action = [[ 0.00309914  0.11429629  0.16850388 -0.57090026]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 1576 is [True, False, False, True, False, False]
Current timestep = 1577. State = [[-0.13477813 -0.1374287 ]]. Action = [[-0.07885265  0.15377563  0.20339271  0.13780665]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 1577 is [True, False, False, True, False, False]
Current timestep = 1578. State = [[-0.13946137 -0.12705795]]. Action = [[-0.05372836  0.22017258  0.01088974  0.51169   ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 1578 is [True, False, False, True, False, False]
Scene graph at timestep 1578 is [True, False, False, True, False, False]
State prediction error at timestep 1578 is tensor(0.0195, grad_fn=<MseLossBackward0>)
Current timestep = 1579. State = [[-0.14079419 -0.11303597]]. Action = [[0.16048956 0.18002525 0.03236035 0.62212944]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 1579 is [True, False, False, True, False, False]
Current timestep = 1580. State = [[-0.13860184 -0.10031335]]. Action = [[-0.20831294  0.13306639 -0.12535354 -0.44816828]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 1580 is [True, False, False, False, True, False]
Current timestep = 1581. State = [[-0.14776438 -0.08836813]]. Action = [[-0.21754636  0.18847966 -0.06245498  0.21832812]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 1581 is [True, False, False, False, True, False]
Current timestep = 1582. State = [[-0.15898469 -0.07531372]]. Action = [[ 0.00347868  0.16352022  0.02018952 -0.511785  ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 1582 is [True, False, False, False, True, False]
Scene graph at timestep 1582 is [True, False, False, False, True, False]
State prediction error at timestep 1582 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Current timestep = 1583. State = [[-0.1637849 -0.0625824]]. Action = [[-0.0584403   0.1801095   0.05638555  0.7690635 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 1583 is [True, False, False, False, True, False]
Scene graph at timestep 1583 is [True, False, False, False, True, False]
State prediction error at timestep 1583 is tensor(0.0195, grad_fn=<MseLossBackward0>)
Current timestep = 1584. State = [[-0.1665147  -0.05433435]]. Action = [[ 0.03399435 -0.14279759 -0.16696098  0.5771655 ]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 1584 is [True, False, False, False, True, False]
Scene graph at timestep 1584 is [True, False, False, False, True, False]
State prediction error at timestep 1584 is tensor(0.0217, grad_fn=<MseLossBackward0>)
Current timestep = 1585. State = [[-0.16681959 -0.0523532 ]]. Action = [[-0.01843193  0.22979945  0.04947406 -0.5194509 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 1585 is [True, False, False, False, True, False]
Scene graph at timestep 1585 is [True, False, False, False, True, False]
State prediction error at timestep 1585 is tensor(0.0124, grad_fn=<MseLossBackward0>)
Current timestep = 1586. State = [[-0.16543359 -0.0435155 ]]. Action = [[ 0.12870044  0.06788969 -0.03173329 -0.9118658 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 1586 is [True, False, False, False, True, False]
Current timestep = 1587. State = [[-0.15879259 -0.03854303]]. Action = [[ 0.12056234 -0.04440433  0.2233026  -0.19896579]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 1587 is [True, False, False, False, True, False]
Current timestep = 1588. State = [[-0.15064138 -0.03435879]]. Action = [[ 0.11791718  0.24857202  0.01513779 -0.7517694 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 1588 is [True, False, False, False, True, False]
Current timestep = 1589. State = [[-0.14080915 -0.02309247]]. Action = [[0.19090214 0.1202575  0.09406713 0.84044373]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 1589 is [True, False, False, False, True, False]
Current timestep = 1590. State = [[-0.12969017 -0.01766696]]. Action = [[ 0.06004724 -0.17414309  0.05589595 -0.900024  ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 1590 is [True, False, False, False, True, False]
Current timestep = 1591. State = [[-0.12208513 -0.01822726]]. Action = [[ 0.13361269  0.19345361 -0.20639409 -0.8863841 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 1591 is [True, False, False, False, True, False]
Current timestep = 1592. State = [[-0.11205615 -0.01234391]]. Action = [[ 0.19024986  0.01896837 -0.1397851  -0.02844095]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 1592 is [True, False, False, False, True, False]
Current timestep = 1593. State = [[-0.10245237 -0.01306934]]. Action = [[-0.05287343 -0.23112081  0.15898582 -0.5875644 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 1593 is [True, False, False, False, True, False]
Current timestep = 1594. State = [[-0.1008416  -0.01891342]]. Action = [[ 0.02280989  0.11337671 -0.1288337  -0.30819112]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 1594 is [True, False, False, False, True, False]
Current timestep = 1595. State = [[-0.0969473  -0.02052331]]. Action = [[ 0.1845743  -0.18047214 -0.22714509 -0.17006117]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 1595 is [True, False, False, False, True, False]
Scene graph at timestep 1595 is [True, False, False, False, True, False]
State prediction error at timestep 1595 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Current timestep = 1596. State = [[-0.09175971 -0.02556071]]. Action = [[-0.1664165   0.08699733  0.23656714  0.9758394 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 1596 is [True, False, False, False, True, False]
Current timestep = 1597. State = [[-0.09205706 -0.02170036]]. Action = [[ 0.24000323  0.1733878   0.17568761 -0.32911998]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 1597 is [True, False, False, False, True, False]
Current timestep = 1598. State = [[-0.08476904 -0.01572993]]. Action = [[-0.04566175 -0.05889504 -0.06261027 -0.11012316]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 1598 is [True, False, False, False, True, False]
Current timestep = 1599. State = [[-0.08051865 -0.01279424]]. Action = [[ 0.21287197  0.13869318 -0.22312321  0.23487604]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 1599 is [True, False, False, False, True, False]
Scene graph at timestep 1599 is [True, False, False, False, True, False]
State prediction error at timestep 1599 is tensor(0.0099, grad_fn=<MseLossBackward0>)
Current timestep = 1600. State = [[-0.0740944  -0.01167584]]. Action = [[-0.17555048 -0.24328424 -0.01006763 -0.99581367]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 1600 is [True, False, False, False, True, False]
Current timestep = 1601. State = [[-0.0808412  -0.01807374]]. Action = [[-0.20790067  0.04466218  0.2316032   0.17488277]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 1601 is [True, False, False, False, True, False]
Scene graph at timestep 1601 is [True, False, False, False, True, False]
State prediction error at timestep 1601 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Current timestep = 1602. State = [[-0.08913469 -0.02184729]]. Action = [[ 0.11651817 -0.17251872 -0.02240179  0.33055115]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 1602 is [True, False, False, False, True, False]
Current timestep = 1603. State = [[-0.08905251 -0.03164762]]. Action = [[-0.05521467 -0.18077946  0.02689809 -0.6190214 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 1603 is [True, False, False, False, True, False]
Current timestep = 1604. State = [[-0.09280507 -0.04227551]]. Action = [[-0.15993719 -0.06705591  0.07867083  0.33128715]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 1604 is [True, False, False, False, True, False]
Current timestep = 1605. State = [[-0.10179754 -0.05115734]]. Action = [[-0.15031089 -0.18334424  0.13553864 -0.03627282]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 1605 is [True, False, False, False, True, False]
Current timestep = 1606. State = [[-0.10964865 -0.06140076]]. Action = [[ 0.06039226 -0.09126455 -0.08821988 -0.14097309]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 1606 is [True, False, False, False, True, False]
Current timestep = 1607. State = [[-0.11354195 -0.06968968]]. Action = [[-0.22987202 -0.07475315  0.12490419  0.29803514]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 1607 is [True, False, False, False, True, False]
Current timestep = 1608. State = [[-0.12029073 -0.07629333]]. Action = [[ 0.15793449 -0.10884902  0.16000369 -0.3538506 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 1608 is [True, False, False, False, True, False]
Scene graph at timestep 1608 is [True, False, False, False, True, False]
State prediction error at timestep 1608 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Current timestep = 1609. State = [[-0.1157197  -0.07868493]]. Action = [[0.1418975  0.23480296 0.162207   0.44186306]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 1609 is [True, False, False, False, True, False]
Scene graph at timestep 1609 is [True, False, False, False, True, False]
State prediction error at timestep 1609 is tensor(0.0115, grad_fn=<MseLossBackward0>)
Current timestep = 1610. State = [[-0.11163649 -0.07443387]]. Action = [[-0.22963388 -0.13608731  0.14718103 -0.53730845]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 1610 is [True, False, False, False, True, False]
Current timestep = 1611. State = [[-0.11617452 -0.07457677]]. Action = [[0.14419088 0.11240971 0.14616686 0.0437175 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 1611 is [True, False, False, False, True, False]
Current timestep = 1612. State = [[-0.11390133 -0.07376742]]. Action = [[-0.05010943 -0.13534522  0.09618542 -0.47565675]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 1612 is [True, False, False, False, True, False]
Current timestep = 1613. State = [[-0.1171407  -0.07703006]]. Action = [[-0.18580088  0.04075897 -0.02235401 -0.69911927]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 1613 is [True, False, False, False, True, False]
Current timestep = 1614. State = [[-0.12441608 -0.07705352]]. Action = [[ 0.02957487 -0.00880942  0.02259496 -0.7524718 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 1614 is [True, False, False, False, True, False]
Current timestep = 1615. State = [[-0.12301243 -0.0809922 ]]. Action = [[ 0.22402889 -0.24577221  0.19047648  0.93678665]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 1615 is [True, False, False, False, True, False]
Current timestep = 1616. State = [[-0.11620492 -0.08839473]]. Action = [[-0.10289805  0.15100035 -0.17213933 -0.6951791 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 1616 is [True, False, False, False, True, False]
Current timestep = 1617. State = [[-0.11605773 -0.08808811]]. Action = [[ 0.04433036 -0.11745551 -0.23298395  0.7812964 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 1617 is [True, False, False, False, True, False]
Current timestep = 1618. State = [[-0.11243937 -0.09105083]]. Action = [[ 0.18282121  0.01340482 -0.13034712 -0.37286294]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 1618 is [True, False, False, False, True, False]
Current timestep = 1619. State = [[-0.10303429 -0.09482002]]. Action = [[ 0.11727118 -0.1777662   0.04720661 -0.7875025 ]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 1619 is [True, False, False, False, True, False]
Current timestep = 1620. State = [[-0.09737379 -0.10322224]]. Action = [[-0.13374332 -0.07841107  0.23719186 -0.23892117]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 1620 is [True, False, False, False, True, False]
Current timestep = 1621. State = [[-0.10020336 -0.10643452]]. Action = [[-0.01197831  0.15085924  0.0113084  -0.38310277]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 1621 is [True, False, False, False, True, False]
Current timestep = 1622. State = [[-0.10075465 -0.10499001]]. Action = [[ 0.08005339 -0.15138453 -0.07130289 -0.28399217]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 1622 is [True, False, False, False, True, False]
Current timestep = 1623. State = [[-0.09635825 -0.11183881]]. Action = [[ 0.13140708 -0.18081835 -0.00093716 -0.84352326]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 1623 is [True, False, False, False, True, False]
Current timestep = 1624. State = [[-0.08729798 -0.12172722]]. Action = [[ 0.20478126 -0.08059365  0.17991716  0.9050915 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 1624 is [True, False, False, False, True, False]
Scene graph at timestep 1624 is [True, False, False, False, True, False]
State prediction error at timestep 1624 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Current timestep = 1625. State = [[-0.07674953 -0.12453543]]. Action = [[ 0.01430166  0.24152595 -0.12374152 -0.01303387]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 1625 is [True, False, False, False, True, False]
Scene graph at timestep 1625 is [True, False, False, False, True, False]
State prediction error at timestep 1625 is tensor(0.0075, grad_fn=<MseLossBackward0>)
Current timestep = 1626. State = [[-0.06974579 -0.11786285]]. Action = [[ 0.17228335 -0.03207898 -0.08389449  0.07748735]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 1626 is [True, False, False, False, True, False]
Current timestep = 1627. State = [[-0.06396995 -0.11639081]]. Action = [[-0.16155161 -0.00628345  0.02976424  0.11960971]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 1627 is [True, False, False, False, True, False]
Current timestep = 1628. State = [[-0.06592447 -0.11392014]]. Action = [[ 0.10071242  0.13112175 -0.0416674   0.35668898]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 1628 is [True, False, False, False, True, False]
Scene graph at timestep 1628 is [True, False, False, False, True, False]
State prediction error at timestep 1628 is tensor(0.0079, grad_fn=<MseLossBackward0>)
Current timestep = 1629. State = [[-0.06074886 -0.1111587 ]]. Action = [[ 0.19752401 -0.14920779 -0.07868399 -0.58716345]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 1629 is [True, False, False, False, True, False]
Current timestep = 1630. State = [[-0.0499621  -0.11236214]]. Action = [[ 0.1465028   0.166933   -0.1338074   0.46743882]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 1630 is [True, False, False, False, True, False]
Current timestep = 1631. State = [[-0.04299822 -0.10637128]]. Action = [[-0.15751888  0.09866253  0.14475474 -0.33273357]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 1631 is [False, True, False, False, True, False]
Current timestep = 1632. State = [[-0.04346063 -0.10066805]]. Action = [[ 0.17224672 -0.02388947  0.01132527  0.9536979 ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 1632 is [False, True, False, False, True, False]
Current timestep = 1633. State = [[-0.04135805 -0.09961428]]. Action = [[-0.21851547  0.00963309 -0.02081026  0.03225243]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 1633 is [False, True, False, False, True, False]
Current timestep = 1634. State = [[-0.05090243 -0.10081037]]. Action = [[-0.22453573 -0.12044147 -0.04812223  0.42465246]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 1634 is [False, True, False, False, True, False]
Current timestep = 1635. State = [[-0.0611189  -0.10471181]]. Action = [[ 0.10573372  0.00918084 -0.04363443 -0.04878718]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 1635 is [True, False, False, False, True, False]
Scene graph at timestep 1635 is [True, False, False, False, True, False]
State prediction error at timestep 1635 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Current timestep = 1636. State = [[-0.05789839 -0.10470928]]. Action = [[ 0.23533398  0.09038308  0.16279    -0.17432445]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 1636 is [True, False, False, False, True, False]
Current timestep = 1637. State = [[-0.04737889 -0.10199516]]. Action = [[ 0.05257246 -0.0070032  -0.11106265 -0.00671107]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 1637 is [True, False, False, False, True, False]
Current timestep = 1638. State = [[-0.0431276  -0.10349949]]. Action = [[-0.12036602 -0.15359764  0.18696803 -0.9111524 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 1638 is [False, True, False, False, True, False]
Current timestep = 1639. State = [[-0.04649616 -0.10720555]]. Action = [[-0.02916665  0.11134759  0.00929525 -0.05539638]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 1639 is [False, True, False, False, True, False]
Current timestep = 1640. State = [[-0.04664312 -0.1032258 ]]. Action = [[ 0.16559422  0.11963558 -0.04131143 -0.71716523]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 1640 is [False, True, False, False, True, False]
Current timestep = 1641. State = [[-0.03915504 -0.10024222]]. Action = [[ 0.12116534 -0.16131367  0.2088139  -0.34355152]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 1641 is [False, True, False, False, True, False]
Scene graph at timestep 1641 is [False, True, False, False, True, False]
State prediction error at timestep 1641 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 1642. State = [[-0.28550637  0.18279709]]. Action = [[-0.04174802 -0.16024247 -0.12952343 -0.74286115]]. Reward = [1.]
Curr episode timestep = 220
Scene graph at timestep 1642 is [False, True, False, False, True, False]
Scene graph at timestep 1642 is [True, False, False, False, False, True]
State prediction error at timestep 1642 is tensor(0.0660, grad_fn=<MseLossBackward0>)
Current timestep = 1643. State = [[-0.28911752  0.18428095]]. Action = [[-0.22193    -0.09736311 -0.16556495  0.6640246 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1643 is [True, False, False, False, False, True]
Current timestep = 1644. State = [[-0.30017045  0.18393281]]. Action = [[-0.10975337  0.14046234 -0.02303803  0.11461151]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1644 is [True, False, False, False, False, True]
Current timestep = 1645. State = [[-0.30428517  0.18427506]]. Action = [[ 0.21440715 -0.24268997  0.0657053   0.567889  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1645 is [True, False, False, False, False, True]
Current timestep = 1646. State = [[-0.30115044  0.17488386]]. Action = [[-0.16231793 -0.11769682  0.050589   -0.840912  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1646 is [True, False, False, False, False, True]
Scene graph at timestep 1646 is [True, False, False, False, False, True]
State prediction error at timestep 1646 is tensor(0.0573, grad_fn=<MseLossBackward0>)
Current timestep = 1647. State = [[-0.30578634  0.17050052]]. Action = [[-0.01556115  0.19141603 -0.23140201  0.7214942 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1647 is [True, False, False, False, False, True]
Current timestep = 1648. State = [[-0.3063507   0.17153841]]. Action = [[ 0.06871837 -0.21909638  0.19546613 -0.28674734]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1648 is [True, False, False, False, False, True]
Scene graph at timestep 1648 is [True, False, False, False, False, True]
State prediction error at timestep 1648 is tensor(0.0616, grad_fn=<MseLossBackward0>)
Current timestep = 1649. State = [[-0.3066981  0.1676744]]. Action = [[-0.11562766  0.15224564  0.07232559  0.03275394]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1649 is [True, False, False, False, False, True]
Current timestep = 1650. State = [[-0.30893308  0.1719581 ]]. Action = [[0.0913792  0.06983763 0.09681135 0.31728196]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1650 is [True, False, False, False, False, True]
Current timestep = 1651. State = [[-0.30394596  0.1736433 ]]. Action = [[ 0.1755377  -0.15021007 -0.09579125 -0.25170743]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1651 is [True, False, False, False, False, True]
Current timestep = 1652. State = [[-0.30018455  0.17276241]]. Action = [[-0.24466413  0.21304128 -0.08583912 -0.64890885]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1652 is [True, False, False, False, False, True]
Current timestep = 1653. State = [[-0.30594867  0.17741376]]. Action = [[ 0.01916936 -0.11109723  0.22676477  0.6883905 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1653 is [True, False, False, False, False, True]
Current timestep = 1654. State = [[-0.30820686  0.17852251]]. Action = [[ 0.03262758  0.17413437  0.10967681 -0.6443768 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1654 is [True, False, False, False, False, True]
Current timestep = 1655. State = [[-0.3065815   0.18446976]]. Action = [[0.04156825 0.01435962 0.24439895 0.30843842]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1655 is [True, False, False, False, False, True]
Current timestep = 1656. State = [[-0.30312583  0.18358149]]. Action = [[ 0.06922215 -0.2399133   0.17113644 -0.10255241]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1656 is [True, False, False, False, False, True]
Current timestep = 1657. State = [[-0.29826695  0.17352307]]. Action = [[ 0.12533835 -0.12217796 -0.0631476  -0.21528304]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1657 is [True, False, False, False, False, True]
Scene graph at timestep 1657 is [True, False, False, False, False, True]
State prediction error at timestep 1657 is tensor(0.0672, grad_fn=<MseLossBackward0>)
Current timestep = 1658. State = [[-0.29050627  0.16891305]]. Action = [[0.15486664 0.2124908  0.05283809 0.23487806]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1658 is [True, False, False, False, False, True]
Current timestep = 1659. State = [[-0.2851012  0.1768175]]. Action = [[-0.23136728  0.17642945 -0.11580782 -0.14825642]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1659 is [True, False, False, False, False, True]
Current timestep = 1660. State = [[-0.29306215  0.18263212]]. Action = [[-0.18703148 -0.20932661 -0.24796776 -0.5912913 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1660 is [True, False, False, False, False, True]
Current timestep = 1661. State = [[-0.30334938  0.18014742]]. Action = [[ 0.03894365  0.13304085 -0.20015089  0.01676667]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1661 is [True, False, False, False, False, True]
Scene graph at timestep 1661 is [True, False, False, False, False, True]
State prediction error at timestep 1661 is tensor(0.0858, grad_fn=<MseLossBackward0>)
Current timestep = 1662. State = [[-0.3042162   0.18242723]]. Action = [[ 0.07070467 -0.03701159  0.20487219  0.49417293]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1662 is [True, False, False, False, False, True]
Current timestep = 1663. State = [[-0.29972807  0.18431446]]. Action = [[ 0.16731828  0.12421703  0.10398582 -0.9387069 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1663 is [True, False, False, False, False, True]
Scene graph at timestep 1663 is [True, False, False, False, False, True]
State prediction error at timestep 1663 is tensor(0.0649, grad_fn=<MseLossBackward0>)
Current timestep = 1664. State = [[-0.29450858  0.18736354]]. Action = [[-0.19755504 -0.09441812  0.08931091 -0.04167175]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1664 is [True, False, False, False, False, True]
Current timestep = 1665. State = [[-0.30249578  0.18224888]]. Action = [[-0.23115523 -0.21865088 -0.11804017  0.40793037]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1665 is [True, False, False, False, False, True]
Current timestep = 1666. State = [[-0.31325245  0.16932565]]. Action = [[ 0.05558544 -0.24003376  0.03686318 -0.37350094]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1666 is [True, False, False, False, False, True]
Current timestep = 1667. State = [[-0.3173637   0.16043843]]. Action = [[-0.07992733  0.2388882   0.05421913 -0.61297256]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1667 is [True, False, False, False, False, True]
Current timestep = 1668. State = [[-0.32137263  0.16313268]]. Action = [[-0.09451029 -0.10523364  0.1569205  -0.9659543 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1668 is [True, False, False, False, False, True]
Current timestep = 1669. State = [[-0.32527348  0.15771249]]. Action = [[ 0.06128272 -0.22868562  0.00808829 -0.59013903]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1669 is [True, False, False, False, False, True]
Current timestep = 1670. State = [[-0.32476613  0.14484768]]. Action = [[ 0.01024541 -0.20972587  0.11506191  0.40107584]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1670 is [True, False, False, False, False, True]
Current timestep = 1671. State = [[-0.32204202  0.1300683 ]]. Action = [[ 0.14485681 -0.20302822 -0.02345946 -0.48994422]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1671 is [True, False, False, False, False, True]
Current timestep = 1672. State = [[-0.31800905  0.11626358]]. Action = [[-0.13032399 -0.11426935  0.12952149  0.35711658]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1672 is [True, False, False, False, False, True]
Current timestep = 1673. State = [[-0.3224366   0.10595597]]. Action = [[-0.12092829 -0.11038896  0.12016371  0.9682554 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1673 is [True, False, False, False, True, False]
Current timestep = 1674. State = [[-0.3317775   0.09639966]]. Action = [[-0.23603453 -0.14603554 -0.19658168  0.5385126 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1674 is [True, False, False, False, True, False]
Current timestep = 1675. State = [[-0.34051654  0.08468079]]. Action = [[ 0.20186475 -0.23531437 -0.01058719  0.48030353]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1675 is [True, False, False, False, True, False]
Current timestep = 1676. State = [[-0.33609655  0.06893077]]. Action = [[ 0.06123406 -0.23920856  0.14217854  0.49652314]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1676 is [True, False, False, False, True, False]
Current timestep = 1677. State = [[-0.3338724   0.05860145]]. Action = [[-0.11547917  0.18815583 -0.08471051 -0.78129387]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1677 is [True, False, False, False, True, False]
Scene graph at timestep 1677 is [True, False, False, False, True, False]
State prediction error at timestep 1677 is tensor(0.0553, grad_fn=<MseLossBackward0>)
Current timestep = 1678. State = [[-0.3368303   0.05810167]]. Action = [[-0.04248659 -0.16427921  0.06517464  0.5153916 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1678 is [True, False, False, False, True, False]
Current timestep = 1679. State = [[-0.33799723  0.05371539]]. Action = [[ 0.15272963  0.02202436  0.11532488 -0.56542057]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1679 is [True, False, False, False, True, False]
Scene graph at timestep 1679 is [True, False, False, False, True, False]
State prediction error at timestep 1679 is tensor(0.0537, grad_fn=<MseLossBackward0>)
Current timestep = 1680. State = [[-0.33028105  0.04868812]]. Action = [[ 0.17390162 -0.2309588  -0.18626198 -0.64511365]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1680 is [True, False, False, False, True, False]
Current timestep = 1681. State = [[-0.31936362  0.04243036]]. Action = [[ 0.1772945   0.16860956 -0.09997439  0.27184558]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1681 is [True, False, False, False, True, False]
Current timestep = 1682. State = [[-0.30572605  0.04868606]]. Action = [[ 0.23369259  0.21699786 -0.18932134 -0.74899167]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1682 is [True, False, False, False, True, False]
Current timestep = 1683. State = [[-0.29393587  0.05463191]]. Action = [[-0.12641436 -0.21634735  0.12231007 -0.76011175]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1683 is [True, False, False, False, True, False]
Current timestep = 1684. State = [[-0.29695272  0.04761501]]. Action = [[-0.1796094  -0.17166844 -0.24276459  0.5597789 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1684 is [True, False, False, False, True, False]
Current timestep = 1685. State = [[-0.30557117  0.04271738]]. Action = [[-0.01926021  0.21602735  0.17667952  0.59883976]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1685 is [True, False, False, False, True, False]
Scene graph at timestep 1685 is [True, False, False, False, True, False]
State prediction error at timestep 1685 is tensor(0.0616, grad_fn=<MseLossBackward0>)
Current timestep = 1686. State = [[-0.31077623  0.05117846]]. Action = [[-0.10881177  0.23312199  0.05594015 -0.19850779]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1686 is [True, False, False, False, True, False]
Current timestep = 1687. State = [[-0.3185387   0.06499571]]. Action = [[-0.17234212  0.18130845 -0.09592804 -0.09769779]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1687 is [True, False, False, False, True, False]
Current timestep = 1688. State = [[-0.330316    0.07612117]]. Action = [[-0.22478281  0.01276267  0.16753417  0.3802408 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1688 is [True, False, False, False, True, False]
Current timestep = 1689. State = [[-0.33940136  0.08046426]]. Action = [[ 0.2204271  -0.01515418 -0.1756472  -0.35611665]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1689 is [True, False, False, False, True, False]
Current timestep = 1690. State = [[-0.33179414  0.07794563]]. Action = [[ 0.21739465 -0.20457391 -0.07523946  0.8354368 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1690 is [True, False, False, False, True, False]
Current timestep = 1691. State = [[-0.32406133  0.06637698]]. Action = [[-0.21964566 -0.24583969 -0.23990504 -0.0291959 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1691 is [True, False, False, False, True, False]
Current timestep = 1692. State = [[-0.33043665  0.05849994]]. Action = [[-0.07381363  0.2496346   0.18673009  0.03841424]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1692 is [True, False, False, False, True, False]
Scene graph at timestep 1692 is [True, False, False, False, True, False]
State prediction error at timestep 1692 is tensor(0.0703, grad_fn=<MseLossBackward0>)
Current timestep = 1693. State = [[-0.33220032  0.06259927]]. Action = [[ 0.21454555 -0.04172832 -0.0017944   0.72429824]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1693 is [True, False, False, False, True, False]
Current timestep = 1694. State = [[-0.32337776  0.06140887]]. Action = [[ 0.14219156 -0.11195442  0.0795722  -0.84171414]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1694 is [True, False, False, False, True, False]
Current timestep = 1695. State = [[-0.31585246  0.05348282]]. Action = [[-0.07399516 -0.23437135  0.13907522  0.00583839]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1695 is [True, False, False, False, True, False]
Scene graph at timestep 1695 is [True, False, False, False, True, False]
State prediction error at timestep 1695 is tensor(0.0533, grad_fn=<MseLossBackward0>)
Current timestep = 1696. State = [[-0.31619152  0.04393912]]. Action = [[-0.00120863  0.04321048 -0.04268807 -0.34769374]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1696 is [True, False, False, False, True, False]
Scene graph at timestep 1696 is [True, False, False, False, True, False]
State prediction error at timestep 1696 is tensor(0.0541, grad_fn=<MseLossBackward0>)
Current timestep = 1697. State = [[-0.31716594  0.04192975]]. Action = [[-0.05133405  0.02574521  0.11126342  0.31329632]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1697 is [True, False, False, False, True, False]
Current timestep = 1698. State = [[-0.32165653  0.04599431]]. Action = [[-0.15805218  0.23227197 -0.17629778  0.82626224]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1698 is [True, False, False, False, True, False]
Current timestep = 1699. State = [[-0.32905006  0.0555537 ]]. Action = [[-0.03583084  0.03948465 -0.05391514  0.6211436 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1699 is [True, False, False, False, True, False]
Current timestep = 1700. State = [[-0.3347127   0.06104257]]. Action = [[-0.11221009  0.03216937  0.10060924  0.27077866]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1700 is [True, False, False, False, True, False]
Current timestep = 1701. State = [[-0.33890194  0.06405836]]. Action = [[ 0.11283362  0.00222871  0.07282919 -0.60245746]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1701 is [True, False, False, False, True, False]
Current timestep = 1702. State = [[-0.3357447   0.06903496]]. Action = [[0.07671463 0.244412   0.22099221 0.8291602 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1702 is [True, False, False, False, True, False]
Current timestep = 1703. State = [[-0.3327516   0.07729452]]. Action = [[-0.11963259 -0.0727044   0.17673433 -0.6445944 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1703 is [True, False, False, False, True, False]
Current timestep = 1704. State = [[-0.3375703   0.07566097]]. Action = [[-0.12223408 -0.16815373 -0.0411112  -0.9417914 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1704 is [True, False, False, False, True, False]
Scene graph at timestep 1704 is [True, False, False, False, True, False]
State prediction error at timestep 1704 is tensor(0.0481, grad_fn=<MseLossBackward0>)
Current timestep = 1705. State = [[-0.34397897  0.07353111]]. Action = [[ 0.03563374  0.24880087 -0.07702166 -0.17685163]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1705 is [True, False, False, False, True, False]
Current timestep = 1706. State = [[-0.34402016  0.08021108]]. Action = [[ 0.03390849 -0.00270018  0.1027931   0.28601158]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1706 is [True, False, False, False, True, False]
Current timestep = 1707. State = [[-0.33992392  0.08135987]]. Action = [[ 0.19205639 -0.1057575  -0.10013166 -0.7853192 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1707 is [True, False, False, False, True, False]
Current timestep = 1708. State = [[-0.33437774  0.07505958]]. Action = [[-0.18801187 -0.19146708 -0.17831348  0.3841014 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1708 is [True, False, False, False, True, False]
Current timestep = 1709. State = [[-0.33956048  0.06613846]]. Action = [[-0.04235509 -0.03902632  0.23488513  0.6972134 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1709 is [True, False, False, False, True, False]
Current timestep = 1710. State = [[-0.34055978  0.05835988]]. Action = [[ 0.18540266 -0.19126077 -0.17325735  0.1637541 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1710 is [True, False, False, False, True, False]
Current timestep = 1711. State = [[-0.3324267   0.05088133]]. Action = [[0.17057246 0.08522332 0.1936084  0.86223197]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1711 is [True, False, False, False, True, False]
Current timestep = 1712. State = [[-0.31974223  0.04904191]]. Action = [[ 0.2204028  -0.10115482 -0.03574717 -0.34440064]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1712 is [True, False, False, False, True, False]
Current timestep = 1713. State = [[-0.3083857   0.04802569]]. Action = [[-0.03979935  0.1630866  -0.00587983 -0.5635049 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1713 is [True, False, False, False, True, False]
Current timestep = 1714. State = [[-0.30874848  0.05342177]]. Action = [[-0.2399344   0.04020143  0.11827916 -0.3203107 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1714 is [True, False, False, False, True, False]
Current timestep = 1715. State = [[-0.31920168  0.05494341]]. Action = [[-0.11953968 -0.12790379  0.22726539  0.25613427]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1715 is [True, False, False, False, True, False]
Current timestep = 1716. State = [[-0.32824188  0.0486071 ]]. Action = [[-0.04657704 -0.17395349  0.05899706 -0.94918835]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1716 is [True, False, False, False, True, False]
Current timestep = 1717. State = [[-0.33306012  0.04238501]]. Action = [[ 0.01894557  0.10919279 -0.20994084 -0.54741305]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1717 is [True, False, False, False, True, False]
Current timestep = 1718. State = [[-0.33392015  0.04604355]]. Action = [[-0.01048993  0.1675492   0.23865545 -0.20560122]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1718 is [True, False, False, False, True, False]
Current timestep = 1719. State = [[-0.33397284  0.05585808]]. Action = [[ 0.03059751  0.17704326 -0.16048041  0.31010675]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1719 is [True, False, False, False, True, False]
Current timestep = 1720. State = [[-0.33265483  0.06785957]]. Action = [[ 0.02064279  0.1641025  -0.05327043 -0.88126695]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1720 is [True, False, False, False, True, False]
Current timestep = 1721. State = [[-0.3281148   0.08064876]]. Action = [[0.24148089 0.19304234 0.13844973 0.07327771]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1721 is [True, False, False, False, True, False]
Scene graph at timestep 1721 is [True, False, False, False, True, False]
State prediction error at timestep 1721 is tensor(0.0722, grad_fn=<MseLossBackward0>)
Current timestep = 1722. State = [[-0.32007724  0.08869923]]. Action = [[-0.17281117 -0.15842427  0.15926653 -0.4700048 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1722 is [True, False, False, False, True, False]
Current timestep = 1723. State = [[-0.3205665   0.08960907]]. Action = [[0.20846885 0.16944945 0.02563193 0.97097135]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1723 is [True, False, False, False, True, False]
Current timestep = 1724. State = [[-0.31268227  0.09169822]]. Action = [[ 0.05741751 -0.18494923 -0.10315223  0.06477261]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1724 is [True, False, False, False, True, False]
Current timestep = 1725. State = [[-0.3047055   0.08900507]]. Action = [[0.23798299 0.12803698 0.09766847 0.14591539]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1725 is [True, False, False, False, True, False]
Current timestep = 1726. State = [[-0.29517263  0.0877573 ]]. Action = [[-0.14359763 -0.23770767 -0.08297071 -0.62459326]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1726 is [True, False, False, False, True, False]
Current timestep = 1727. State = [[-0.29407212  0.0764254 ]]. Action = [[ 0.19609395 -0.2432816   0.20642555  0.31924355]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 1727 is [True, False, False, False, True, False]
Current timestep = 1728. State = [[-0.28588384  0.0662646 ]]. Action = [[ 0.14492607  0.1380125  -0.16651748 -0.9450787 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1728 is [True, False, False, False, True, False]
Current timestep = 1729. State = [[-0.27642506  0.06700275]]. Action = [[ 0.04978943  0.01576421 -0.22006263 -0.12440169]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1729 is [True, False, False, False, True, False]
Scene graph at timestep 1729 is [True, False, False, False, True, False]
State prediction error at timestep 1729 is tensor(0.0529, grad_fn=<MseLossBackward0>)
Current timestep = 1730. State = [[-0.27388752  0.06614442]]. Action = [[-0.19263461 -0.13020171  0.07069024 -0.7694782 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1730 is [True, False, False, False, True, False]
Current timestep = 1731. State = [[-0.28329     0.05810132]]. Action = [[-0.2425794  -0.23023808  0.24008748 -0.41083002]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1731 is [True, False, False, False, True, False]
Current timestep = 1732. State = [[-0.29245934  0.04726324]]. Action = [[ 0.23930478 -0.03907174 -0.09462309  0.374992  ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 1732 is [True, False, False, False, True, False]
Current timestep = 1733. State = [[-0.28828213  0.04537379]]. Action = [[-0.04548426  0.23874968 -0.06884363 -0.44009387]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1733 is [True, False, False, False, True, False]
Scene graph at timestep 1733 is [True, False, False, False, True, False]
State prediction error at timestep 1733 is tensor(0.0498, grad_fn=<MseLossBackward0>)
Current timestep = 1734. State = [[-0.28942314  0.05042628]]. Action = [[-0.16393653 -0.12817694 -0.15797906 -0.57514924]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1734 is [True, False, False, False, True, False]
Current timestep = 1735. State = [[-0.29601133  0.05050689]]. Action = [[ 0.00915882  0.10855401 -0.139785    0.46660173]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 1735 is [True, False, False, False, True, False]
Scene graph at timestep 1735 is [True, False, False, False, True, False]
State prediction error at timestep 1735 is tensor(0.0628, grad_fn=<MseLossBackward0>)
Current timestep = 1736. State = [[-0.29642385  0.05109349]]. Action = [[ 0.11242488 -0.15682167  0.22147524  0.11957169]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1736 is [True, False, False, False, True, False]
Current timestep = 1737. State = [[-0.29378718  0.04829604]]. Action = [[-0.07075642  0.11600551 -0.08450563  0.00282085]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1737 is [True, False, False, False, True, False]
Scene graph at timestep 1737 is [True, False, False, False, True, False]
State prediction error at timestep 1737 is tensor(0.0573, grad_fn=<MseLossBackward0>)
Current timestep = 1738. State = [[-0.29411066  0.04704448]]. Action = [[ 0.03944212 -0.23301901  0.01360092  0.07816386]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 1738 is [True, False, False, False, True, False]
Scene graph at timestep 1738 is [True, False, False, False, True, False]
State prediction error at timestep 1738 is tensor(0.0505, grad_fn=<MseLossBackward0>)
Current timestep = 1739. State = [[-0.29387045  0.04283231]]. Action = [[-0.01731545  0.21185502  0.0308046   0.652194  ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 1739 is [True, False, False, False, True, False]
Current timestep = 1740. State = [[-0.29051796  0.04569875]]. Action = [[ 0.23526973 -0.12594017 -0.23071669 -0.496516  ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 1740 is [True, False, False, False, True, False]
Scene graph at timestep 1740 is [True, False, False, False, True, False]
State prediction error at timestep 1740 is tensor(0.0459, grad_fn=<MseLossBackward0>)
Current timestep = 1741. State = [[-0.28241426  0.04093119]]. Action = [[-0.07589121 -0.12744011  0.00169325  0.29891956]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1741 is [True, False, False, False, True, False]
Current timestep = 1742. State = [[-0.2821188   0.03111343]]. Action = [[-0.02375957 -0.24755865 -0.16670986  0.8790505 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1742 is [True, False, False, False, True, False]
Current timestep = 1743. State = [[-0.28505746  0.02077192]]. Action = [[-0.10874662  0.07481778 -0.03378013 -0.06202251]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1743 is [True, False, False, False, True, False]
Current timestep = 1744. State = [[-0.2916984   0.02161405]]. Action = [[-0.14070372  0.15456116 -0.19157806 -0.73516315]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1744 is [True, False, False, False, True, False]
Current timestep = 1745. State = [[-0.30090308  0.02720594]]. Action = [[-0.14243925  0.00585175  0.15645617  0.7308707 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1745 is [True, False, False, False, True, False]
Current timestep = 1746. State = [[-0.31009734  0.02808657]]. Action = [[-0.06509566 -0.10056242 -0.04397911  0.9874319 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1746 is [True, False, False, False, True, False]
Current timestep = 1747. State = [[-0.31323433  0.02612613]]. Action = [[0.18564144 0.05486643 0.0525701  0.3338132 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1747 is [True, False, False, False, True, False]
Scene graph at timestep 1747 is [True, False, False, False, True, False]
State prediction error at timestep 1747 is tensor(0.0639, grad_fn=<MseLossBackward0>)
Current timestep = 1748. State = [[-0.3075228   0.02508447]]. Action = [[ 0.00147057 -0.10495077  0.2002517   0.68858624]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1748 is [True, False, False, False, True, False]
Scene graph at timestep 1748 is [True, False, False, False, True, False]
State prediction error at timestep 1748 is tensor(0.0580, grad_fn=<MseLossBackward0>)
Current timestep = 1749. State = [[-0.30582273  0.0247853 ]]. Action = [[-0.03935763  0.19708824 -0.1174845  -0.36030018]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1749 is [True, False, False, False, True, False]
Scene graph at timestep 1749 is [True, False, False, False, True, False]
State prediction error at timestep 1749 is tensor(0.0546, grad_fn=<MseLossBackward0>)
Current timestep = 1750. State = [[-0.3070694   0.03069285]]. Action = [[-0.04176946 -0.0040496   0.03419504 -0.69173867]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1750 is [True, False, False, False, True, False]
Current timestep = 1751. State = [[-0.31217134  0.02988121]]. Action = [[-0.22579455 -0.19150592  0.19153976  0.8682227 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1751 is [True, False, False, False, True, False]
Current timestep = 1752. State = [[-0.32266605  0.02261669]]. Action = [[-0.04035342 -0.05952659 -0.14368638 -0.4346351 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1752 is [True, False, False, False, True, False]
Current timestep = 1753. State = [[-0.33106625  0.01667374]]. Action = [[-0.22064899 -0.06032072 -0.13084719  0.63212514]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1753 is [True, False, False, False, True, False]
Current timestep = 1754. State = [[-0.34324062  0.01589295]]. Action = [[-0.11591807  0.19465047 -0.04089931  0.5513848 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1754 is [True, False, False, False, True, False]
Scene graph at timestep 1754 is [True, False, False, False, True, False]
State prediction error at timestep 1754 is tensor(0.0776, grad_fn=<MseLossBackward0>)
Current timestep = 1755. State = [[-0.34990343  0.02129801]]. Action = [[ 0.11620867 -0.02737702 -0.11137307 -0.08765185]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1755 is [True, False, False, False, True, False]
Current timestep = 1756. State = [[-0.3500655   0.02236173]]. Action = [[-0.1369267  -0.0166422   0.08354497 -0.38110363]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1756 is [True, False, False, False, True, False]
Current timestep = 1757. State = [[-0.35457534  0.02158247]]. Action = [[-0.00753212 -0.03977332  0.02065098  0.19212437]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1757 is [True, False, False, False, True, False]
Current timestep = 1758. State = [[-0.35509777  0.02041859]]. Action = [[ 0.12095165  0.0254477  -0.20313066 -0.03108406]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1758 is [True, False, False, False, True, False]
Current timestep = 1759. State = [[-0.3476899  0.0220024]]. Action = [[ 0.23499405  0.07996401 -0.03720103  0.6351199 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1759 is [True, False, False, False, True, False]
Current timestep = 1760. State = [[-0.33513615  0.0252903 ]]. Action = [[0.10105613 0.01477513 0.10024947 0.51727974]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1760 is [True, False, False, False, True, False]
Current timestep = 1761. State = [[-0.3288053   0.02941695]]. Action = [[-0.12509802  0.14451808 -0.12941056  0.9466145 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1761 is [True, False, False, False, True, False]
Current timestep = 1762. State = [[-0.33364227  0.03209768]]. Action = [[-0.20584984 -0.20817053 -0.09903505  0.9887048 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1762 is [True, False, False, False, True, False]
Current timestep = 1763. State = [[-0.3400762   0.02543807]]. Action = [[ 0.2281102  -0.08750767  0.08762971  0.22977698]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1763 is [True, False, False, False, True, False]
Scene graph at timestep 1763 is [True, False, False, False, True, False]
State prediction error at timestep 1763 is tensor(0.0681, grad_fn=<MseLossBackward0>)
Current timestep = 1764. State = [[-0.3337075   0.02234648]]. Action = [[0.07907632 0.16854447 0.10601857 0.5547309 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1764 is [True, False, False, False, True, False]
Scene graph at timestep 1764 is [True, False, False, False, True, False]
State prediction error at timestep 1764 is tensor(0.0712, grad_fn=<MseLossBackward0>)
Current timestep = 1765. State = [[-0.3302996   0.02658235]]. Action = [[-0.18435024  0.00601459  0.11310855  0.38589776]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1765 is [True, False, False, False, True, False]
Current timestep = 1766. State = [[-0.33643422  0.02647195]]. Action = [[-0.06479228 -0.14462724 -0.12216607 -0.72889966]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1766 is [True, False, False, False, True, False]
Scene graph at timestep 1766 is [True, False, False, False, True, False]
State prediction error at timestep 1766 is tensor(0.0487, grad_fn=<MseLossBackward0>)
Current timestep = 1767. State = [[-0.34022427  0.0227196 ]]. Action = [[ 0.09806317  0.05388796  0.20396131 -0.37455046]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1767 is [True, False, False, False, True, False]
Current timestep = 1768. State = [[-0.34062102  0.02558599]]. Action = [[-0.18495879  0.18321007  0.11107743  0.2144723 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1768 is [True, False, False, False, True, False]
Current timestep = 1769. State = [[-0.34331906  0.03030463]]. Action = [[ 0.23596436 -0.15164539  0.18583131  0.41742063]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1769 is [True, False, False, False, True, False]
Current timestep = 1770. State = [[-0.33507422  0.02959182]]. Action = [[ 0.12519899  0.1394903   0.20787382 -0.07271636]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 1770 is [True, False, False, False, True, False]
Scene graph at timestep 1770 is [True, False, False, False, True, False]
State prediction error at timestep 1770 is tensor(0.0649, grad_fn=<MseLossBackward0>)
Current timestep = 1771. State = [[-0.3249456   0.03207817]]. Action = [[ 0.14244723 -0.08293383  0.1783145  -0.6158722 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 1771 is [True, False, False, False, True, False]
Current timestep = 1772. State = [[-0.31880122  0.03338491]]. Action = [[-0.15094668  0.17667383  0.12485591 -0.5504531 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 1772 is [True, False, False, False, True, False]
Current timestep = 1773. State = [[-0.32350695  0.04187848]]. Action = [[-0.1239728   0.15596098 -0.21176466 -0.42973542]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 1773 is [True, False, False, False, True, False]
Current timestep = 1774. State = [[-0.32683614  0.04964013]]. Action = [[ 0.2149843  -0.03412473  0.02786735 -0.36483973]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 1774 is [True, False, False, False, True, False]
Current timestep = 1775. State = [[-0.32014367  0.05502557]]. Action = [[ 0.05695486  0.2288456  -0.08902456 -0.9625704 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 1775 is [True, False, False, False, True, False]
Current timestep = 1776. State = [[-0.316145    0.06167037]]. Action = [[-0.09487922 -0.14683257 -0.22912416 -0.6483165 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 1776 is [True, False, False, False, True, False]
Current timestep = 1777. State = [[-0.31759882  0.06369318]]. Action = [[ 0.07545841  0.2413317  -0.03045714  0.18843246]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 1777 is [True, False, False, False, True, False]
Scene graph at timestep 1777 is [True, False, False, False, True, False]
State prediction error at timestep 1777 is tensor(0.0740, grad_fn=<MseLossBackward0>)
Current timestep = 1778. State = [[-0.31280452  0.0718727 ]]. Action = [[ 0.18696871  0.01607656 -0.22212389  0.8803129 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 1778 is [True, False, False, False, True, False]
Current timestep = 1779. State = [[-0.30189732  0.07862461]]. Action = [[0.19799846 0.18482938 0.08205682 0.2367853 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 1779 is [True, False, False, False, True, False]
Current timestep = 1780. State = [[-0.29142964  0.08828992]]. Action = [[-0.04548669  0.09521782 -0.02870417 -0.8032513 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 1780 is [True, False, False, False, True, False]
Current timestep = 1781. State = [[-0.28931627  0.09576164]]. Action = [[-0.00752588  0.04169187 -0.09574944 -0.49801904]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 1781 is [True, False, False, False, True, False]
Current timestep = 1782. State = [[-0.29144162  0.10077033]]. Action = [[-0.15483372  0.06846938  0.12852561  0.37807095]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 1782 is [True, False, False, False, True, False]
Current timestep = 1783. State = [[-0.2979532   0.10818635]]. Action = [[-0.01219042  0.21823877  0.0875541   0.06356204]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 1783 is [True, False, False, False, True, False]
Current timestep = 1784. State = [[-0.30153188  0.11596359]]. Action = [[-0.07444119 -0.11241654  0.18919235 -0.6610155 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 1784 is [True, False, False, False, True, False]
Current timestep = 1785. State = [[-0.3054331   0.11794667]]. Action = [[ 0.01671031  0.1561405  -0.21024314 -0.21841449]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 1785 is [True, False, False, False, True, False]
Current timestep = 1786. State = [[-0.30676082  0.12352527]]. Action = [[-0.0546336   0.01686195  0.23792207 -0.26141226]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 1786 is [True, False, False, False, True, False]
Current timestep = 1787. State = [[-0.30661628  0.12579876]]. Action = [[ 0.16794878 -0.03387916  0.19844157  0.56604433]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 1787 is [True, False, False, False, True, False]
Current timestep = 1788. State = [[-0.3001191   0.12649514]]. Action = [[0.06273788 0.07025248 0.08981612 0.05232263]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 1788 is [True, False, False, False, False, True]
Current timestep = 1789. State = [[-0.29355896  0.12886213]]. Action = [[ 0.11441815 -0.00475265 -0.13582112 -0.9704472 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 1789 is [True, False, False, False, False, True]
Current timestep = 1790. State = [[-0.2907095   0.12755212]]. Action = [[-0.24704546 -0.12919837  0.14878994 -0.99042743]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 1790 is [True, False, False, False, False, True]
Current timestep = 1791. State = [[-0.3000064  0.1190582]]. Action = [[-0.12874892 -0.24816781  0.17733759 -0.05182064]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 1791 is [True, False, False, False, False, True]
Scene graph at timestep 1791 is [True, False, False, False, True, False]
State prediction error at timestep 1791 is tensor(0.0541, grad_fn=<MseLossBackward0>)
Current timestep = 1792. State = [[-0.31208006  0.10927304]]. Action = [[-0.23174526  0.08911639  0.05303901 -0.9041183 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 1792 is [True, False, False, False, True, False]
Current timestep = 1793. State = [[-0.3239216   0.10858028]]. Action = [[0.00604215 0.01658458 0.0667389  0.4343866 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 1793 is [True, False, False, False, True, False]
Current timestep = 1794. State = [[-0.32964614  0.10972857]]. Action = [[-0.11585148  0.0360648   0.10221639  0.03533292]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 1794 is [True, False, False, False, True, False]
Current timestep = 1795. State = [[-0.33622605  0.11123272]]. Action = [[-0.06758092 -0.00973453  0.24769005  0.52578735]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 1795 is [True, False, False, False, True, False]
Scene graph at timestep 1795 is [True, False, False, False, True, False]
State prediction error at timestep 1795 is tensor(0.0763, grad_fn=<MseLossBackward0>)
Current timestep = 1796. State = [[-0.34053913  0.11475242]]. Action = [[0.05181289 0.20200789 0.23484552 0.6454046 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 1796 is [True, False, False, False, True, False]
Current timestep = 1797. State = [[-0.33873275  0.1249852 ]]. Action = [[ 0.09377456  0.16322577 -0.24079421 -0.8316934 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 1797 is [True, False, False, False, True, False]
Current timestep = 1798. State = [[-0.33715674  0.13796252]]. Action = [[-0.17211454  0.23850986  0.08581448 -0.6814844 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 1798 is [True, False, False, False, True, False]
Scene graph at timestep 1798 is [True, False, False, False, False, True]
State prediction error at timestep 1798 is tensor(0.0723, grad_fn=<MseLossBackward0>)
Current timestep = 1799. State = [[-0.34285012  0.1528322 ]]. Action = [[-0.03724918  0.15490904 -0.11466408 -0.4528246 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 1799 is [True, False, False, False, False, True]
Current timestep = 1800. State = [[-0.34566444  0.16102949]]. Action = [[ 0.03372756 -0.13593993 -0.23408431  0.3949399 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 1800 is [True, False, False, False, False, True]
Current timestep = 1801. State = [[-0.3486779   0.15744978]]. Action = [[-0.21913882 -0.13510586 -0.02018328 -0.9843183 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 1801 is [True, False, False, False, False, True]
Current timestep = 1802. State = [[-0.35441223  0.1499099 ]]. Action = [[ 0.20322204 -0.09958397 -0.05600949  0.15434706]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 1802 is [True, False, False, False, False, True]
Scene graph at timestep 1802 is [True, False, False, False, False, True]
State prediction error at timestep 1802 is tensor(0.0889, grad_fn=<MseLossBackward0>)
Current timestep = 1803. State = [[-0.35269025  0.14567229]]. Action = [[-0.18674825  0.11905372  0.17466551  0.7184806 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 1803 is [True, False, False, False, False, True]
Current timestep = 1804. State = [[-0.35437715  0.14793384]]. Action = [[ 0.2265765  -0.00617965  0.1276958  -0.991932  ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 1804 is [True, False, False, False, False, True]
Current timestep = 1805. State = [[-0.34770593  0.1461965 ]]. Action = [[-0.01862012 -0.15738523  0.16739717  0.43417978]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 1805 is [True, False, False, False, False, True]
Scene graph at timestep 1805 is [True, False, False, False, False, True]
State prediction error at timestep 1805 is tensor(0.0840, grad_fn=<MseLossBackward0>)
Current timestep = 1806. State = [[-0.34393725  0.1438487 ]]. Action = [[ 0.15644673  0.2026993  -0.11118625  0.20502663]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 1806 is [True, False, False, False, False, True]
Current timestep = 1807. State = [[-0.33919725  0.15080287]]. Action = [[-0.15516962  0.10192168  0.07322621 -0.3216324 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 1807 is [True, False, False, False, False, True]
Scene graph at timestep 1807 is [True, False, False, False, False, True]
State prediction error at timestep 1807 is tensor(0.0780, grad_fn=<MseLossBackward0>)
Current timestep = 1808. State = [[-0.34386364  0.15714455]]. Action = [[-0.09159951  0.00647271  0.0693863  -0.14769137]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 1808 is [True, False, False, False, False, True]
Current timestep = 1809. State = [[-0.35139957  0.1614891 ]]. Action = [[-0.14397638  0.11873564 -0.05955365 -0.39111662]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 1809 is [True, False, False, False, False, True]
Current timestep = 1810. State = [[-0.35751304  0.16524154]]. Action = [[ 0.08385137 -0.09145743  0.2182669  -0.18492502]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 1810 is [True, False, False, False, False, True]
Current timestep = 1811. State = [[-0.35382017  0.16465817]]. Action = [[ 0.24111676  0.05855086  0.0494509  -0.58817244]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 1811 is [True, False, False, False, False, True]
Current timestep = 1812. State = [[-0.34336004  0.16727492]]. Action = [[0.03032941 0.0753991  0.22771281 0.02520061]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 1812 is [True, False, False, False, False, True]
Current timestep = 1813. State = [[-0.3409491   0.17458218]]. Action = [[-0.1784678   0.23308688 -0.10860229 -0.86854815]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 1813 is [True, False, False, False, False, True]
Current timestep = 1814. State = [[-0.34750536  0.18455037]]. Action = [[-0.10260865 -0.01354757 -0.06143397 -0.7207546 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 1814 is [True, False, False, False, False, True]
Scene graph at timestep 1814 is [True, False, False, False, False, True]
State prediction error at timestep 1814 is tensor(0.0808, grad_fn=<MseLossBackward0>)
Current timestep = 1815. State = [[-0.35540956  0.19147034]]. Action = [[-0.07894392  0.2286802  -0.17065935 -0.43759048]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 1815 is [True, False, False, False, False, True]
Current timestep = 1816. State = [[-0.35716417  0.20184793]]. Action = [[0.23655713 0.05189785 0.11598694 0.2780068 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 1816 is [True, False, False, False, False, True]
Current timestep = 1817. State = [[-0.35101327  0.20430249]]. Action = [[-0.12259009 -0.20133111 -0.06136465  0.9070289 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 1817 is [True, False, False, False, False, True]
Current timestep = 1818. State = [[-0.3531441   0.19782627]]. Action = [[-0.00969622 -0.05672154 -0.22466798 -0.90705144]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 1818 is [True, False, False, False, False, True]
Current timestep = 1819. State = [[-0.35354742  0.19356774]]. Action = [[ 0.08229148  0.02138177 -0.1273345   0.08360457]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 1819 is [True, False, False, False, False, True]
Scene graph at timestep 1819 is [True, False, False, False, False, True]
State prediction error at timestep 1819 is tensor(0.1017, grad_fn=<MseLossBackward0>)
Current timestep = 1820. State = [[-0.34917638  0.19494934]]. Action = [[ 0.12046665  0.14098024  0.23024672 -0.47734702]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 1820 is [True, False, False, False, False, True]
Current timestep = 1821. State = [[-0.34580544  0.20180799]]. Action = [[-0.18827142  0.10528445  0.15979499 -0.9671959 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 1821 is [True, False, False, False, False, True]
Current timestep = 1822. State = [[-0.34896085  0.2055767 ]]. Action = [[ 0.11507249 -0.1477828  -0.08270693  0.5169382 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 1822 is [True, False, False, False, False, True]
Current timestep = 1823. State = [[-0.34511808  0.20320486]]. Action = [[ 0.14422363  0.07070881 -0.08264811  0.04053569]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 1823 is [True, False, False, False, False, True]
Current timestep = 1824. State = [[-0.33940732  0.20812997]]. Action = [[-0.05807015  0.24023595 -0.11154094  0.03250098]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 1824 is [True, False, False, False, False, True]
Current timestep = 1825. State = [[-0.3376644  0.2175196]]. Action = [[ 0.06852242 -0.02287991 -0.12719423  0.33181524]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 1825 is [True, False, False, False, False, True]
Scene graph at timestep 1825 is [True, False, False, False, False, True]
State prediction error at timestep 1825 is tensor(0.1013, grad_fn=<MseLossBackward0>)
Current timestep = 1826. State = [[-0.3318347   0.21773262]]. Action = [[ 0.21780968 -0.16734698 -0.05132495  0.8758216 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 1826 is [True, False, False, False, False, True]
Current timestep = 1827. State = [[-0.31999508  0.2134599 ]]. Action = [[ 0.1968719   0.08537245 -0.05620882  0.5709851 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 1827 is [True, False, False, False, False, True]
Scene graph at timestep 1827 is [True, False, False, False, False, True]
State prediction error at timestep 1827 is tensor(0.0935, grad_fn=<MseLossBackward0>)
Current timestep = 1828. State = [[-0.30697364  0.21299982]]. Action = [[ 0.09348932 -0.09111741  0.07053024  0.19752872]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 1828 is [True, False, False, False, False, True]
Current timestep = 1829. State = [[-0.29740214  0.21207891]]. Action = [[ 0.1466558   0.12056029 -0.23369223  0.23586345]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 1829 is [True, False, False, False, False, True]
Current timestep = 1830. State = [[-0.28712732  0.21462695]]. Action = [[ 0.11604601 -0.05402023 -0.22392808  0.45841348]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 1830 is [True, False, False, False, False, True]
Current timestep = 1831. State = [[-0.27667663  0.21705371]]. Action = [[ 0.22826326  0.1936582   0.06738022 -0.41867483]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 1831 is [True, False, False, False, False, True]
Current timestep = 1832. State = [[-0.26423508  0.22455433]]. Action = [[ 0.0417788   0.03545475  0.2496326  -0.7452286 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 1832 is [True, False, False, False, False, True]
Current timestep = 1833. State = [[-0.25634035  0.23168066]]. Action = [[ 0.151597    0.18720037  0.08814389 -0.15598786]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 1833 is [True, False, False, False, False, True]
Current timestep = 1834. State = [[-0.25085187  0.24231638]]. Action = [[-0.16774303  0.13782287  0.20377555  0.53340125]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 1834 is [True, False, False, False, False, True]
Current timestep = 1835. State = [[-0.25163823  0.2532971 ]]. Action = [[ 0.21109784  0.1575588  -0.11874199 -0.69487464]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 1835 is [True, False, False, False, False, True]
Current timestep = 1836. State = [[-0.24466856  0.26533166]]. Action = [[0.03897536 0.20044905 0.16634989 0.33564138]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 1836 is [True, False, False, False, False, True]
Scene graph at timestep 1836 is [True, False, False, False, False, True]
State prediction error at timestep 1836 is tensor(0.0867, grad_fn=<MseLossBackward0>)
Current timestep = 1837. State = [[-0.24028797  0.27985895]]. Action = [[0.00958672 0.23121887 0.0911907  0.2138387 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 1837 is [True, False, False, False, False, True]
Current timestep = 1838. State = [[-0.23615111  0.29528823]]. Action = [[0.16090885 0.16946414 0.2036942  0.04114521]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 1838 is [True, False, False, False, False, True]
Scene graph at timestep 1838 is [True, False, False, False, False, True]
State prediction error at timestep 1838 is tensor(0.0893, grad_fn=<MseLossBackward0>)
Current timestep = 1839. State = [[-0.23183797  0.30485323]]. Action = [[-0.20677632 -0.09780565 -0.11544068 -0.32096112]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 1839 is [True, False, False, False, False, True]
Current timestep = 1840. State = [[-0.23522507  0.30447847]]. Action = [[ 0.15465295 -0.04351752 -0.20015472 -0.90056103]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 1840 is [True, False, False, False, False, True]
Current timestep = 1841. State = [[-0.23047097  0.2991512 ]]. Action = [[ 0.06988674 -0.21227072 -0.23448847  0.34604418]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 1841 is [True, False, False, False, False, True]
Current timestep = 1842. State = [[-0.22541554  0.29084048]]. Action = [[0.07412225 0.03390253 0.02274919 0.4591936 ]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 1842 is [True, False, False, False, False, True]
Current timestep = 1843. State = [[-0.21929613  0.28617197]]. Action = [[ 0.0946883  -0.16581436  0.01825175 -0.7867352 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 1843 is [True, False, False, False, False, True]
Current timestep = 1844. State = [[-0.21291219  0.2788565 ]]. Action = [[ 0.08817846 -0.03014511 -0.02688232 -0.6131673 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 1844 is [True, False, False, False, False, True]
Current timestep = 1845. State = [[-0.20683265  0.27370548]]. Action = [[ 0.04428697 -0.08897874  0.14115101 -0.87961894]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 1845 is [True, False, False, False, False, True]
Current timestep = 1846. State = [[-0.20339243  0.26720876]]. Action = [[-1.85068250e-02 -1.20260686e-01 -1.98926196e-01 -7.97510147e-05]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 1846 is [True, False, False, False, False, True]
Current timestep = 1847. State = [[-0.20217235  0.2618654 ]]. Action = [[ 0.06960928  0.05821395  0.152028   -0.42326075]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 1847 is [True, False, False, False, False, True]
Scene graph at timestep 1847 is [True, False, False, False, False, True]
State prediction error at timestep 1847 is tensor(0.0650, grad_fn=<MseLossBackward0>)
Current timestep = 1848. State = [[-0.2024967   0.26280636]]. Action = [[-0.22199716  0.06904176 -0.17989895 -0.20932436]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 1848 is [True, False, False, False, False, True]
Scene graph at timestep 1848 is [True, False, False, False, False, True]
State prediction error at timestep 1848 is tensor(0.0761, grad_fn=<MseLossBackward0>)
Current timestep = 1849. State = [[-0.20731919  0.26321942]]. Action = [[ 0.17593262 -0.15730532  0.23089254  0.05258882]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 1849 is [True, False, False, False, False, True]
Scene graph at timestep 1849 is [True, False, False, False, False, True]
State prediction error at timestep 1849 is tensor(0.0657, grad_fn=<MseLossBackward0>)
Current timestep = 1850. State = [[-0.20195849  0.25456613]]. Action = [[ 0.10924703 -0.23466447 -0.1378557  -0.48414242]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 1850 is [True, False, False, False, False, True]
Current timestep = 1851. State = [[-0.19468583  0.24055497]]. Action = [[ 0.09395292 -0.19719967 -0.02337544 -0.14457983]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 1851 is [True, False, False, False, False, True]
Current timestep = 1852. State = [[-0.19137037  0.22720774]]. Action = [[-0.17384209 -0.11071041 -0.16758424  0.29021597]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 1852 is [True, False, False, False, False, True]
Scene graph at timestep 1852 is [True, False, False, False, False, True]
State prediction error at timestep 1852 is tensor(0.0599, grad_fn=<MseLossBackward0>)
Current timestep = 1853. State = [[-0.19886528  0.21817921]]. Action = [[-0.19047809 -0.04393846 -0.06167641 -0.73973894]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 1853 is [True, False, False, False, False, True]
Current timestep = 1854. State = [[-0.20730464  0.21398106]]. Action = [[0.08637625 0.02406487 0.05615851 0.5664904 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 1854 is [True, False, False, False, False, True]
Current timestep = 1855. State = [[-0.20995486  0.21387333]]. Action = [[-0.18105647  0.03842363 -0.0653218  -0.3604843 ]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 1855 is [True, False, False, False, False, True]
Current timestep = 1856. State = [[-0.21510252  0.21869156]]. Action = [[ 0.12798735  0.2160669  -0.04743244 -0.4434569 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 1856 is [True, False, False, False, False, True]
Current timestep = 1857. State = [[-0.21141705  0.22968383]]. Action = [[0.08694422 0.14640692 0.04466718 0.3772328 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 1857 is [True, False, False, False, False, True]
Scene graph at timestep 1857 is [True, False, False, False, False, True]
State prediction error at timestep 1857 is tensor(0.0680, grad_fn=<MseLossBackward0>)
Current timestep = 1858. State = [[-0.20646293  0.23676673]]. Action = [[-0.00460729 -0.12015161 -0.08358094 -0.07136655]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 1858 is [True, False, False, False, False, True]
Current timestep = 1859. State = [[-0.20515463  0.23476525]]. Action = [[-0.00745708 -0.04484749 -0.01685216 -0.44386387]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 1859 is [True, False, False, False, False, True]
Current timestep = 1860. State = [[-0.2066399   0.23429288]]. Action = [[-0.09416127  0.13113818 -0.05483755 -0.8171003 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 1860 is [True, False, False, False, False, True]
Current timestep = 1861. State = [[-0.21159568  0.23469688]]. Action = [[-0.12042487 -0.23242937  0.21320727 -0.02103025]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 1861 is [True, False, False, False, False, True]
Current timestep = 1862. State = [[-0.218759    0.22962274]]. Action = [[-0.03123319  0.1329065   0.02538845  0.2358551 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 1862 is [True, False, False, False, False, True]
Scene graph at timestep 1862 is [True, False, False, False, False, True]
State prediction error at timestep 1862 is tensor(0.0693, grad_fn=<MseLossBackward0>)
Current timestep = 1863. State = [[-0.21964154  0.23143752]]. Action = [[ 0.17301157 -0.01712103 -0.06250712 -0.60320383]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 1863 is [True, False, False, False, False, True]
Current timestep = 1864. State = [[-0.21064293  0.23049589]]. Action = [[ 0.22472447 -0.0836274  -0.24113289 -0.98405915]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 1864 is [True, False, False, False, False, True]
Scene graph at timestep 1864 is [True, False, False, False, False, True]
State prediction error at timestep 1864 is tensor(0.0673, grad_fn=<MseLossBackward0>)
Current timestep = 1865. State = [[-0.19667281  0.22568494]]. Action = [[ 0.19511801 -0.10765065  0.13456166 -0.65278006]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 1865 is [True, False, False, False, False, True]
Scene graph at timestep 1865 is [True, False, False, False, False, True]
State prediction error at timestep 1865 is tensor(0.0496, grad_fn=<MseLossBackward0>)
Current timestep = 1866. State = [[-0.1847649   0.21895657]]. Action = [[ 0.00745523 -0.0902018   0.18194318  0.68155754]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 1866 is [True, False, False, False, False, True]
Current timestep = 1867. State = [[-0.1786533   0.21511611]]. Action = [[ 0.12789053  0.09346282 -0.00535543 -0.21137404]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 1867 is [True, False, False, False, False, True]
Scene graph at timestep 1867 is [True, False, False, False, False, True]
State prediction error at timestep 1867 is tensor(0.0530, grad_fn=<MseLossBackward0>)
Current timestep = 1868. State = [[-0.17472164  0.21894255]]. Action = [[-0.16905756  0.14530843 -0.1396163  -0.960811  ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 1868 is [True, False, False, False, False, True]
Current timestep = 1869. State = [[-0.18162236  0.2248188 ]]. Action = [[-0.19894373 -0.03153312  0.24095368  0.67751694]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 1869 is [True, False, False, False, False, True]
Scene graph at timestep 1869 is [True, False, False, False, False, True]
State prediction error at timestep 1869 is tensor(0.0555, grad_fn=<MseLossBackward0>)
Current timestep = 1870. State = [[-0.194875    0.22897603]]. Action = [[-0.22232555  0.18906158 -0.2270401  -0.7391579 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 1870 is [True, False, False, False, False, True]
Current timestep = 1871. State = [[-0.20765351  0.23891646]]. Action = [[-0.02425168  0.16104162  0.08732364 -0.14355516]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 1871 is [True, False, False, False, False, True]
Current timestep = 1872. State = [[-0.21143778  0.24636309]]. Action = [[ 0.0904901  -0.09915778 -0.02297808 -0.8220624 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 1872 is [True, False, False, False, False, True]
Current timestep = 1873. State = [[-0.21180278  0.24831997]]. Action = [[-0.14143379  0.15610945 -0.05050826  0.4753623 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 1873 is [True, False, False, False, False, True]
Current timestep = 1874. State = [[-0.21440278  0.25362903]]. Action = [[ 0.12693408 -0.01443774 -0.05947079  0.8824527 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 1874 is [True, False, False, False, False, True]
Scene graph at timestep 1874 is [True, False, False, False, False, True]
State prediction error at timestep 1874 is tensor(0.0725, grad_fn=<MseLossBackward0>)
Current timestep = 1875. State = [[-0.21047975  0.25159115]]. Action = [[ 0.03950381 -0.23070565  0.13533193  0.6442177 ]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 1875 is [True, False, False, False, False, True]
Current timestep = 1876. State = [[-0.2062497   0.23958687]]. Action = [[ 0.09692827 -0.23730344  0.01970977 -0.89780515]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 1876 is [True, False, False, False, False, True]
Current timestep = 1877. State = [[-0.2009318   0.22752464]]. Action = [[0.06398422 0.01651645 0.11832571 0.13359904]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 1877 is [True, False, False, False, False, True]
Scene graph at timestep 1877 is [True, False, False, False, False, True]
State prediction error at timestep 1877 is tensor(0.0600, grad_fn=<MseLossBackward0>)
Current timestep = 1878. State = [[-0.19734013  0.22404638]]. Action = [[-0.05602488  0.02914929 -0.08432741 -0.2776661 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 1878 is [True, False, False, False, False, True]
Scene graph at timestep 1878 is [True, False, False, False, False, True]
State prediction error at timestep 1878 is tensor(0.0589, grad_fn=<MseLossBackward0>)
Current timestep = 1879. State = [[-0.19452892  0.22500472]]. Action = [[ 0.24418965  0.05980045  0.24050662 -0.18599921]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 1879 is [True, False, False, False, False, True]
Current timestep = 1880. State = [[-0.1874651   0.22637293]]. Action = [[-0.1592328  -0.05684449 -0.21192409 -0.40484488]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 1880 is [True, False, False, False, False, True]
Current timestep = 1881. State = [[-0.19294478  0.22474739]]. Action = [[-0.20163094 -0.03014211 -0.21814442  0.64593077]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 1881 is [True, False, False, False, False, True]
Current timestep = 1882. State = [[-0.20217644  0.2213503 ]]. Action = [[ 0.00858474 -0.10877205 -0.1108543  -0.736842  ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 1882 is [True, False, False, False, False, True]
Current timestep = 1883. State = [[-0.20729445  0.21829379]]. Action = [[-0.11520112  0.1114589   0.12608773 -0.74979585]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 1883 is [True, False, False, False, False, True]
Current timestep = 1884. State = [[-0.2119915   0.22005582]]. Action = [[ 0.03500772 -0.04332344  0.05799079 -0.43846136]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 1884 is [True, False, False, False, False, True]
Current timestep = 1885. State = [[-0.212333    0.21689609]]. Action = [[ 0.0096702  -0.16625784 -0.11860055 -0.6245581 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 1885 is [True, False, False, False, False, True]
Scene graph at timestep 1885 is [True, False, False, False, False, True]
State prediction error at timestep 1885 is tensor(0.0523, grad_fn=<MseLossBackward0>)
Current timestep = 1886. State = [[-0.21108148  0.21291156]]. Action = [[0.09254119 0.16524851 0.06840259 0.5057864 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 1886 is [True, False, False, False, False, True]
Current timestep = 1887. State = [[-0.20478122  0.214501  ]]. Action = [[ 0.15171647 -0.13303849 -0.23862267  0.5510063 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 1887 is [True, False, False, False, False, True]
Current timestep = 1888. State = [[-0.19747914  0.21278499]]. Action = [[ 0.00508583  0.10716709 -0.0665004   0.3914591 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 1888 is [True, False, False, False, False, True]
Current timestep = 1889. State = [[-0.1968098   0.21857087]]. Action = [[-0.15715492  0.20068145  0.16137284 -0.16862947]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 1889 is [True, False, False, False, False, True]
Scene graph at timestep 1889 is [True, False, False, False, False, True]
State prediction error at timestep 1889 is tensor(0.0608, grad_fn=<MseLossBackward0>)
Current timestep = 1890. State = [[-0.19866395  0.23043132]]. Action = [[0.24448016 0.19415009 0.15014169 0.7569704 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 1890 is [True, False, False, False, False, True]
Scene graph at timestep 1890 is [True, False, False, False, False, True]
State prediction error at timestep 1890 is tensor(0.0650, grad_fn=<MseLossBackward0>)
Current timestep = 1891. State = [[-0.19116437  0.24297985]]. Action = [[ 0.00222915  0.1231446  -0.17160462  0.6255307 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 1891 is [True, False, False, False, False, True]
Scene graph at timestep 1891 is [True, False, False, False, False, True]
State prediction error at timestep 1891 is tensor(0.0707, grad_fn=<MseLossBackward0>)
Current timestep = 1892. State = [[-0.18706247  0.24842288]]. Action = [[ 0.03260997 -0.2047505  -0.14250933  0.2858565 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 1892 is [True, False, False, False, False, True]
Current timestep = 1893. State = [[-0.18603876  0.24398683]]. Action = [[-0.06469212  0.01996547 -0.02698341  0.52175903]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 1893 is [True, False, False, False, False, True]
Current timestep = 1894. State = [[-0.18446985  0.24235424]]. Action = [[ 0.23724127 -0.01806216  0.07854173 -0.3664754 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 1894 is [True, False, False, False, False, True]
Current timestep = 1895. State = [[-0.17478287  0.2378176 ]]. Action = [[ 0.06688383 -0.215621   -0.1360893   0.9247385 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 1895 is [True, False, False, False, False, True]
Current timestep = 1896. State = [[-0.16854301  0.23197466]]. Action = [[ 0.04013047  0.17124486 -0.03051279 -0.91669804]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 1896 is [True, False, False, False, False, True]
Current timestep = 1897. State = [[-0.16218282  0.23858613]]. Action = [[ 0.19603097  0.21712011 -0.08500257 -0.11962384]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 1897 is [True, False, False, False, False, True]
Current timestep = 1898. State = [[-0.15247822  0.25183934]]. Action = [[ 0.05671203  0.22521815 -0.21091475  0.9265833 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 1898 is [True, False, False, False, False, True]
Current timestep = 1899. State = [[-0.14568637  0.26257795]]. Action = [[ 0.06019071 -0.10037696 -0.02627122  0.0570097 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 1899 is [True, False, False, False, False, True]
Scene graph at timestep 1899 is [True, False, False, False, False, True]
State prediction error at timestep 1899 is tensor(0.0597, grad_fn=<MseLossBackward0>)
Current timestep = 1900. State = [[-0.13841599  0.2648856 ]]. Action = [[ 0.22936586  0.102303   -0.10404882 -0.5168421 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 1900 is [True, False, False, False, False, True]
Scene graph at timestep 1900 is [True, False, False, False, False, True]
State prediction error at timestep 1900 is tensor(0.0638, grad_fn=<MseLossBackward0>)
Current timestep = 1901. State = [[-0.13046394  0.27129367]]. Action = [[-0.16740273  0.1760602  -0.08928767 -0.23200667]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 1901 is [True, False, False, False, False, True]
Current timestep = 1902. State = [[-0.1357687   0.27597722]]. Action = [[-0.22020166 -0.2254278   0.21385771 -0.61228156]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 1902 is [True, False, False, False, False, True]
Current timestep = 1903. State = [[-0.1429376   0.26985303]]. Action = [[ 0.22734469 -0.04393527  0.21830237  0.10127807]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 1903 is [True, False, False, False, False, True]
Scene graph at timestep 1903 is [True, False, False, False, False, True]
State prediction error at timestep 1903 is tensor(0.0590, grad_fn=<MseLossBackward0>)
Current timestep = 1904. State = [[-0.14045243  0.26547718]]. Action = [[-0.17653534 -0.0104548   0.01321092 -0.63273686]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 1904 is [True, False, False, False, False, True]
Current timestep = 1905. State = [[-0.14493637  0.2615007 ]]. Action = [[-0.01158334 -0.14379613 -0.09617278  0.80519223]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 1905 is [True, False, False, False, False, True]
Current timestep = 1906. State = [[-0.15100978  0.25693575]]. Action = [[-0.23521249  0.09661183  0.10604018  0.01888502]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 1906 is [True, False, False, False, False, True]
Scene graph at timestep 1906 is [True, False, False, False, False, True]
State prediction error at timestep 1906 is tensor(0.0620, grad_fn=<MseLossBackward0>)
Current timestep = 1907. State = [[-0.16337213  0.26219806]]. Action = [[-0.1546386   0.24924773 -0.0796434  -0.29132795]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 1907 is [True, False, False, False, False, True]
Current timestep = 1908. State = [[-0.172398    0.27113163]]. Action = [[ 0.03813335 -0.07989064 -0.22473077  0.6241839 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 1908 is [True, False, False, False, False, True]
Scene graph at timestep 1908 is [True, False, False, False, False, True]
State prediction error at timestep 1908 is tensor(0.0725, grad_fn=<MseLossBackward0>)
Current timestep = 1909. State = [[-0.1708973   0.26875338]]. Action = [[ 0.23417383 -0.21030957  0.15076262  0.79502475]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 1909 is [True, False, False, False, False, True]
Current timestep = 1910. State = [[-0.1647874   0.26173276]]. Action = [[-0.1655988   0.06390589  0.1921019   0.16498303]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 1910 is [True, False, False, False, False, True]
Current timestep = 1911. State = [[-0.1652888   0.25957125]]. Action = [[ 0.15964189 -0.10031942 -0.17200638 -0.7955507 ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 1911 is [True, False, False, False, False, True]
Current timestep = 1912. State = [[-0.1598958   0.25565854]]. Action = [[ 0.05330643 -0.00255652  0.05616921 -0.0360083 ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 1912 is [True, False, False, False, False, True]
Scene graph at timestep 1912 is [True, False, False, False, False, True]
State prediction error at timestep 1912 is tensor(0.0603, grad_fn=<MseLossBackward0>)
Current timestep = 1913. State = [[-0.15337187  0.25435066]]. Action = [[0.17069799 0.01621163 0.19227836 0.8329289 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 1913 is [True, False, False, False, False, True]
Current timestep = 1914. State = [[-0.1482007   0.25579712]]. Action = [[-0.20384628  0.0884648   0.05135521 -0.43288934]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 1914 is [True, False, False, False, False, True]
Current timestep = 1915. State = [[-0.14952     0.25917062]]. Action = [[ 0.24769792 -0.01245448  0.23494843  0.62182903]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 1915 is [True, False, False, False, False, True]
Scene graph at timestep 1915 is [True, False, False, False, False, True]
State prediction error at timestep 1915 is tensor(0.0577, grad_fn=<MseLossBackward0>)
Current timestep = 1916. State = [[-0.14229006  0.2624007 ]]. Action = [[-0.00109906  0.15632054  0.09248582  0.8357227 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 1916 is [True, False, False, False, False, True]
Scene graph at timestep 1916 is [True, False, False, False, False, True]
State prediction error at timestep 1916 is tensor(0.0646, grad_fn=<MseLossBackward0>)
Current timestep = 1917. State = [[-0.14042747  0.27152443]]. Action = [[-0.08763543  0.18710911  0.08037621 -0.53706956]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 1917 is [True, False, False, False, False, True]
Current timestep = 1918. State = [[-0.14377517  0.27938282]]. Action = [[-0.07651806 -0.10611765 -0.16241218  0.01519597]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 1918 is [True, False, False, False, False, True]
Scene graph at timestep 1918 is [True, False, False, False, False, True]
State prediction error at timestep 1918 is tensor(0.0656, grad_fn=<MseLossBackward0>)
Current timestep = 1919. State = [[-0.14551952  0.27845427]]. Action = [[ 0.17302185 -0.02786131 -0.02311523 -0.44941175]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 1919 is [True, False, False, False, False, True]
Current timestep = 1920. State = [[-0.13706997  0.27330884]]. Action = [[ 0.21533296 -0.22530271  0.12199283  0.7015209 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 1920 is [True, False, False, False, False, True]
Current timestep = 1921. State = [[-0.13005227  0.2642044 ]]. Action = [[-0.24137054  0.00728428 -0.15530549 -0.44221234]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 1921 is [True, False, False, False, False, True]
Current timestep = 1922. State = [[-0.1338153  0.2625643]]. Action = [[0.12317845 0.09427109 0.11677223 0.025563  ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 1922 is [True, False, False, False, False, True]
Current timestep = 1923. State = [[-0.1280197   0.26192227]]. Action = [[ 0.23168695 -0.20505533  0.06139943 -0.8494651 ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 1923 is [True, False, False, False, False, True]
Current timestep = 1924. State = [[-0.11676706  0.25281584]]. Action = [[ 0.0777545  -0.15128101 -0.15485753 -0.7527748 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 1924 is [True, False, False, False, False, True]
Scene graph at timestep 1924 is [True, False, False, False, False, True]
State prediction error at timestep 1924 is tensor(0.0545, grad_fn=<MseLossBackward0>)
Current timestep = 1925. State = [[-0.11263414  0.24095455]]. Action = [[-0.19834812 -0.21686772  0.09231216  0.30174935]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 1925 is [True, False, False, False, False, True]
Current timestep = 1926. State = [[-0.11560155  0.22863446]]. Action = [[ 0.18455568 -0.07264221 -0.04615769 -0.9945331 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 1926 is [True, False, False, False, False, True]
Current timestep = 1927. State = [[-0.11121783  0.22258209]]. Action = [[-0.02439746  0.06031883  0.2001144  -0.71156275]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 1927 is [True, False, False, False, False, True]
Current timestep = 1928. State = [[-0.10978376  0.22300014]]. Action = [[ 0.00769204  0.02988181 -0.09443662  0.13226604]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 1928 is [True, False, False, False, False, True]
Current timestep = 1929. State = [[-0.10895707  0.22379687]]. Action = [[ 0.01349345 -0.03414281 -0.00105338 -0.3632689 ]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 1929 is [True, False, False, False, False, True]
Current timestep = 1930. State = [[-0.10557066  0.21941492]]. Action = [[ 0.17984855 -0.2309958  -0.02064914 -0.40194207]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 1930 is [True, False, False, False, False, True]
Current timestep = 1931. State = [[-0.09960938  0.2107926 ]]. Action = [[-0.07734822  0.03545314 -0.24424933  0.9570453 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 1931 is [True, False, False, False, False, True]
Current timestep = 1932. State = [[-0.09632781  0.2120392 ]]. Action = [[ 0.24462548  0.2140789  -0.22810066  0.35764384]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 1932 is [True, False, False, False, False, True]
Scene graph at timestep 1932 is [True, False, False, False, False, True]
State prediction error at timestep 1932 is tensor(0.0474, grad_fn=<MseLossBackward0>)
Current timestep = 1933. State = [[-0.08545965  0.22084078]]. Action = [[0.1135419  0.08312258 0.17332259 0.18296921]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 1933 is [True, False, False, False, False, True]
Current timestep = 1934. State = [[-0.076635    0.22548348]]. Action = [[ 0.04131371 -0.09700795 -0.08177607  0.10014617]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 1934 is [True, False, False, False, False, True]
Scene graph at timestep 1934 is [True, False, False, False, False, True]
State prediction error at timestep 1934 is tensor(0.0421, grad_fn=<MseLossBackward0>)
Current timestep = 1935. State = [[-0.07523707  0.22105303]]. Action = [[-0.21392678 -0.18353629 -0.20520549 -0.71858597]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 1935 is [True, False, False, False, False, True]
Current timestep = 1936. State = [[-0.08055169  0.21052073]]. Action = [[ 0.11775589 -0.1844871  -0.02633138  0.07728112]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 1936 is [True, False, False, False, False, True]
Current timestep = 1937. State = [[-0.08170574  0.19720156]]. Action = [[-0.19146746 -0.20802099 -0.19962397  0.06608105]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 1937 is [True, False, False, False, False, True]
Scene graph at timestep 1937 is [True, False, False, False, False, True]
State prediction error at timestep 1937 is tensor(0.0343, grad_fn=<MseLossBackward0>)
Current timestep = 1938. State = [[-0.0885516   0.18864396]]. Action = [[-0.01181416  0.1864255  -0.03772464 -0.2111175 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 1938 is [True, False, False, False, False, True]
Current timestep = 1939. State = [[-0.09364319  0.19195485]]. Action = [[-0.14739841  0.03353724 -0.06245768 -0.04310924]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 1939 is [True, False, False, False, False, True]
Scene graph at timestep 1939 is [True, False, False, False, False, True]
State prediction error at timestep 1939 is tensor(0.0373, grad_fn=<MseLossBackward0>)
Current timestep = 1940. State = [[-0.1027279   0.19686526]]. Action = [[-0.16558772  0.13788366 -0.13588132 -0.81402886]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 1940 is [True, False, False, False, False, True]
Current timestep = 1941. State = [[-0.11233572  0.20243646]]. Action = [[-0.04638745 -0.04012081 -0.08605884  0.6030483 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 1941 is [True, False, False, False, False, True]
Current timestep = 1942. State = [[-0.11969514  0.20542054]]. Action = [[-0.1507355   0.13686275 -0.1377529  -0.16989905]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 1942 is [True, False, False, False, False, True]
Current timestep = 1943. State = [[-0.1281517  0.2122681]]. Action = [[-0.06546152  0.09734848 -0.06681529  0.76405525]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 1943 is [True, False, False, False, False, True]
Current timestep = 1944. State = [[-0.13169166  0.21542928]]. Action = [[ 0.11541629 -0.16603813 -0.01321988 -0.32693553]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 1944 is [True, False, False, False, False, True]
Current timestep = 1945. State = [[-0.13115562  0.21452582]]. Action = [[-0.13647464  0.21004206 -0.01835643  0.69868803]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 1945 is [True, False, False, False, False, True]
Current timestep = 1946. State = [[-0.13712952  0.2179458 ]]. Action = [[-0.1474386  -0.17134441  0.22259945 -0.17874587]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 1946 is [True, False, False, False, False, True]
Current timestep = 1947. State = [[-0.14544995  0.2157399 ]]. Action = [[-0.0480707   0.10891297  0.04631621  0.8976617 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 1947 is [True, False, False, False, False, True]
Current timestep = 1948. State = [[-0.14712712  0.21457832]]. Action = [[ 0.19657421 -0.22410117 -0.18388185 -0.43865162]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 1948 is [True, False, False, False, False, True]
Current timestep = 1949. State = [[-0.14000385  0.2093266 ]]. Action = [[0.08149868 0.13885537 0.01179546 0.49738586]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 1949 is [True, False, False, False, False, True]
Current timestep = 1950. State = [[-0.13293609  0.21435189]]. Action = [[0.08661216 0.17457366 0.15926045 0.06436622]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 1950 is [True, False, False, False, False, True]
Current timestep = 1951. State = [[-0.12433987  0.2202809 ]]. Action = [[ 0.21029273 -0.11651999  0.01143867 -0.9476278 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 1951 is [True, False, False, False, False, True]
Current timestep = 1952. State = [[-0.11769561  0.21531029]]. Action = [[-0.23974265 -0.21853654  0.16357136  0.7396283 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 1952 is [True, False, False, False, False, True]
Current timestep = 1953. State = [[-0.12542716  0.2058523 ]]. Action = [[-0.14155546 -0.02757779 -0.23109666  0.02065635]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 1953 is [True, False, False, False, False, True]
Current timestep = 1954. State = [[-0.13201071  0.20500992]]. Action = [[ 0.11519611  0.23457551 -0.06943935  0.6761241 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 1954 is [True, False, False, False, False, True]
Current timestep = 1955. State = [[-0.13389151  0.20989326]]. Action = [[-0.24730197 -0.1545066  -0.02354737 -0.62611085]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 1955 is [True, False, False, False, False, True]
Current timestep = 1956. State = [[-0.14302269  0.20755474]]. Action = [[-0.02721249  0.02750587 -0.03728487  0.5186994 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 1956 is [True, False, False, False, False, True]
Scene graph at timestep 1956 is [True, False, False, False, False, True]
State prediction error at timestep 1956 is tensor(0.0477, grad_fn=<MseLossBackward0>)
Current timestep = 1957. State = [[-0.14865193  0.20721525]]. Action = [[-0.07029778 -0.00140798 -0.11007324 -0.3329451 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 1957 is [True, False, False, False, False, True]
Current timestep = 1958. State = [[-0.15542853  0.20553532]]. Action = [[-0.18237539 -0.10119975  0.1587582  -0.9995785 ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 1958 is [True, False, False, False, False, True]
Current timestep = 1959. State = [[-0.16149577  0.20269592]]. Action = [[ 0.17878103  0.06254113 -0.0294832  -0.08228993]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 1959 is [True, False, False, False, False, True]
Current timestep = 1960. State = [[-0.15565352  0.20702371]]. Action = [[ 0.15052903  0.22086048 -0.21067406 -0.7281508 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 1960 is [True, False, False, False, False, True]
Scene graph at timestep 1960 is [True, False, False, False, False, True]
State prediction error at timestep 1960 is tensor(0.0564, grad_fn=<MseLossBackward0>)
Current timestep = 1961. State = [[-0.14892721  0.21691674]]. Action = [[-0.07207772  0.06798249 -0.21724549  0.9162741 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 1961 is [True, False, False, False, False, True]
Scene graph at timestep 1961 is [True, False, False, False, False, True]
State prediction error at timestep 1961 is tensor(0.0543, grad_fn=<MseLossBackward0>)
Current timestep = 1962. State = [[-0.14558557  0.22052912]]. Action = [[ 0.21558133 -0.15383111 -0.21889329 -0.92893785]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 1962 is [True, False, False, False, False, True]
Scene graph at timestep 1962 is [True, False, False, False, False, True]
State prediction error at timestep 1962 is tensor(0.0540, grad_fn=<MseLossBackward0>)
Current timestep = 1963. State = [[-0.13457909  0.21741284]]. Action = [[ 0.20817828  0.04320008  0.18254066 -0.14213955]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 1963 is [True, False, False, False, False, True]
Current timestep = 1964. State = [[-0.12419005  0.21566288]]. Action = [[-0.06837443 -0.10558045  0.18409717 -0.9941507 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 1964 is [True, False, False, False, False, True]
Current timestep = 1965. State = [[-0.12588507  0.21510969]]. Action = [[-0.21434857  0.20787978 -0.06107862 -0.3595277 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 1965 is [True, False, False, False, False, True]
Current timestep = 1966. State = [[-0.13686396  0.22016378]]. Action = [[-0.19646952 -0.08040997  0.21707392  0.6325824 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 1966 is [True, False, False, False, False, True]
Current timestep = 1967. State = [[-0.1498391   0.21641323]]. Action = [[-0.13929051 -0.21002579  0.21131003  0.289719  ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 1967 is [True, False, False, False, False, True]
Current timestep = 1968. State = [[-0.15711221  0.20495014]]. Action = [[ 0.15447739 -0.2018571   0.1332317   0.96263146]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 1968 is [True, False, False, False, False, True]
Scene graph at timestep 1968 is [True, False, False, False, False, True]
State prediction error at timestep 1968 is tensor(0.0445, grad_fn=<MseLossBackward0>)
Current timestep = 1969. State = [[-0.15745668  0.192921  ]]. Action = [[-0.20444386 -0.07245603 -0.18991968  0.05190408]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 1969 is [True, False, False, False, False, True]
Current timestep = 1970. State = [[-0.16478848  0.18231729]]. Action = [[-0.06576112 -0.24630898  0.17337346 -0.2865715 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 1970 is [True, False, False, False, False, True]
Current timestep = 1971. State = [[-0.17045097  0.17205434]]. Action = [[-0.01697072  0.08372596 -0.08489792  0.25545692]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 1971 is [True, False, False, False, False, True]
Current timestep = 1972. State = [[-0.1705449   0.17007121]]. Action = [[ 0.1542944  -0.04783487  0.16328567 -0.9253608 ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 1972 is [True, False, False, False, False, True]
Current timestep = 1973. State = [[-0.16331789  0.17072552]]. Action = [[ 0.13826519  0.16305292 -0.07438685 -0.815568  ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 1973 is [True, False, False, False, False, True]
Scene graph at timestep 1973 is [True, False, False, False, False, True]
State prediction error at timestep 1973 is tensor(0.0431, grad_fn=<MseLossBackward0>)
Current timestep = 1974. State = [[-0.15664092  0.17524819]]. Action = [[-0.06690106 -0.05772988  0.09757885  0.83225536]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 1974 is [True, False, False, False, False, True]
Current timestep = 1975. State = [[-0.15737997  0.17878057]]. Action = [[-0.05792531  0.21985757 -0.03319773 -0.9756312 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 1975 is [True, False, False, False, False, True]
Current timestep = 1976. State = [[-0.161306    0.18594466]]. Action = [[-0.10267779 -0.06066833  0.22385451 -0.7285663 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 1976 is [True, False, False, False, False, True]
Current timestep = 1977. State = [[-0.16856481  0.18794717]]. Action = [[-0.15328272  0.06941575 -0.03627059 -0.168688  ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 1977 is [True, False, False, False, False, True]
Current timestep = 1978. State = [[-0.17621657  0.18678278]]. Action = [[ 0.00665838 -0.24391879 -0.03860579  0.39189458]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 1978 is [True, False, False, False, False, True]
Scene graph at timestep 1978 is [True, False, False, False, False, True]
State prediction error at timestep 1978 is tensor(0.0447, grad_fn=<MseLossBackward0>)
Current timestep = 1979. State = [[-0.18191165  0.18098967]]. Action = [[-0.19646236  0.14847821  0.04133582 -0.33743644]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 1979 is [True, False, False, False, False, True]
Scene graph at timestep 1979 is [True, False, False, False, False, True]
State prediction error at timestep 1979 is tensor(0.0463, grad_fn=<MseLossBackward0>)
Current timestep = 1980. State = [[-0.19077525  0.18107869]]. Action = [[-0.04388899 -0.13840668 -0.04276025  0.18342328]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 1980 is [True, False, False, False, False, True]
Scene graph at timestep 1980 is [True, False, False, False, False, True]
State prediction error at timestep 1980 is tensor(0.0475, grad_fn=<MseLossBackward0>)
Current timestep = 1981. State = [[-0.19674258  0.17502292]]. Action = [[-0.06955703 -0.12731932 -0.05853918 -0.4492985 ]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 1981 is [True, False, False, False, False, True]
Scene graph at timestep 1981 is [True, False, False, False, False, True]
State prediction error at timestep 1981 is tensor(0.0406, grad_fn=<MseLossBackward0>)
Current timestep = 1982. State = [[-0.2009103   0.16955322]]. Action = [[ 0.01608312  0.06260404 -0.02874655  0.45152557]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 1982 is [True, False, False, False, False, True]
Current timestep = 1983. State = [[-0.19971724  0.17337601]]. Action = [[0.14492756 0.24489829 0.19941783 0.46332192]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 1983 is [True, False, False, False, False, True]
Current timestep = 1984. State = [[-0.19119935  0.1866634 ]]. Action = [[ 0.21080846  0.24242264  0.1067546  -0.05535614]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 1984 is [True, False, False, False, False, True]
Scene graph at timestep 1984 is [True, False, False, False, False, True]
State prediction error at timestep 1984 is tensor(0.0520, grad_fn=<MseLossBackward0>)
Current timestep = 1985. State = [[-0.18262321  0.20164058]]. Action = [[-0.12473287  0.13127244 -0.1359957  -0.8506044 ]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 1985 is [True, False, False, False, False, True]
Current timestep = 1986. State = [[-0.1859292   0.21401285]]. Action = [[-0.16288292  0.1670788  -0.23287657 -0.6935396 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 1986 is [True, False, False, False, False, True]
Current timestep = 1987. State = [[-0.19636087  0.22688513]]. Action = [[-0.2233091   0.20303464 -0.0714522  -0.4415728 ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 1987 is [True, False, False, False, False, True]
Scene graph at timestep 1987 is [True, False, False, False, False, True]
State prediction error at timestep 1987 is tensor(0.0626, grad_fn=<MseLossBackward0>)
Current timestep = 1988. State = [[-0.20458327  0.23945783]]. Action = [[0.21949509 0.09340376 0.06918508 0.9890914 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 1988 is [True, False, False, False, False, True]
Scene graph at timestep 1988 is [True, False, False, False, False, True]
State prediction error at timestep 1988 is tensor(0.0661, grad_fn=<MseLossBackward0>)
Current timestep = 1989. State = [[-0.20264417  0.24975245]]. Action = [[-0.15751351  0.16568226 -0.20079789  0.09720099]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 1989 is [True, False, False, False, False, True]
Current timestep = 1990. State = [[-0.20540223  0.25578558]]. Action = [[ 0.06678578 -0.18313381  0.00204113  0.42938566]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 1990 is [True, False, False, False, False, True]
Scene graph at timestep 1990 is [True, False, False, False, False, True]
State prediction error at timestep 1990 is tensor(0.0677, grad_fn=<MseLossBackward0>)
Current timestep = 1991. State = [[-0.20473067  0.2545438 ]]. Action = [[-5.7190657e-05  1.5277350e-01 -2.2899115e-01 -3.3126646e-01]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 1991 is [True, False, False, False, False, True]
Current timestep = 1992. State = [[-0.2054245   0.25672814]]. Action = [[-0.09062104 -0.11906259  0.2397927   0.6087055 ]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 1992 is [True, False, False, False, False, True]
Current timestep = 1993. State = [[-0.207877   0.2529922]]. Action = [[ 0.06292617 -0.05983877  0.12070084 -0.5766648 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 1993 is [True, False, False, False, False, True]
Current timestep = 1994. State = [[-0.2036324   0.24690501]]. Action = [[ 0.21584612 -0.16147365 -0.05978458 -0.7368869 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 1994 is [True, False, False, False, False, True]
Current timestep = 1995. State = [[-0.19517227  0.24241713]]. Action = [[-0.0226935   0.18341702  0.21569532 -0.9911474 ]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 1995 is [True, False, False, False, False, True]
Current timestep = 1996. State = [[-0.19190037  0.24626374]]. Action = [[ 0.03386524 -0.02161188  0.21381229 -0.617622  ]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 1996 is [True, False, False, False, False, True]
Current timestep = 1997. State = [[-0.19200562  0.24797043]]. Action = [[-0.1425855   0.04021937 -0.08253896  0.42634094]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 1997 is [True, False, False, False, False, True]
Current timestep = 1998. State = [[-0.19358931  0.2496334 ]]. Action = [[ 0.22224087 -0.00650662 -0.14894159  0.78637934]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 1998 is [True, False, False, False, False, True]
Current timestep = 1999. State = [[-0.18790275  0.25385338]]. Action = [[-0.05468269  0.24042934 -0.07657444  0.33612144]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 1999 is [True, False, False, False, False, True]
Current timestep = 2000. State = [[-0.1876562   0.25996426]]. Action = [[-0.04599658 -0.1972949  -0.02598545 -0.75423366]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 2000 is [True, False, False, False, False, True]
Scene graph at timestep 2000 is [True, False, False, False, False, True]
State prediction error at timestep 2000 is tensor(0.0583, grad_fn=<MseLossBackward0>)
Current timestep = 2001. State = [[-0.18803628  0.25367057]]. Action = [[ 0.09825122 -0.15354866  0.06219614  0.57055974]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 2001 is [True, False, False, False, False, True]
Current timestep = 2002. State = [[-0.18639734  0.24913557]]. Action = [[-0.08477728  0.1948109   0.21383154 -0.6323316 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 2002 is [True, False, False, False, False, True]
Scene graph at timestep 2002 is [True, False, False, False, False, True]
State prediction error at timestep 2002 is tensor(0.0626, grad_fn=<MseLossBackward0>)
Current timestep = 2003. State = [[-0.18733236  0.25300035]]. Action = [[ 0.06156713 -0.04042205 -0.12603654 -0.05576843]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 2003 is [True, False, False, False, False, True]
Current timestep = 2004. State = [[-0.18636228  0.25156197]]. Action = [[-0.03755324 -0.12122962  0.2291297  -0.99177885]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 2004 is [True, False, False, False, False, True]
Scene graph at timestep 2004 is [True, False, False, False, False, True]
State prediction error at timestep 2004 is tensor(0.0562, grad_fn=<MseLossBackward0>)
Current timestep = 2005. State = [[-0.1879938  0.2490435]]. Action = [[-0.04948819  0.12705418  0.20116141 -0.30226696]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 2005 is [True, False, False, False, False, True]
Current timestep = 2006. State = [[-0.18818782  0.25444612]]. Action = [[ 0.14460945  0.15243885 -0.00853451 -0.549524  ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 2006 is [True, False, False, False, False, True]
Current timestep = 2007. State = [[-0.18393797  0.26198393]]. Action = [[-0.03659895  0.02840614 -0.23406596 -0.37278426]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 2007 is [True, False, False, False, False, True]
Current timestep = 2008. State = [[-0.18075122  0.26853502]]. Action = [[ 0.18217623  0.17601064  0.08712119 -0.23616749]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 2008 is [True, False, False, False, False, True]
Current timestep = 2009. State = [[-0.17694293  0.2802261 ]]. Action = [[-0.2186544   0.23922187 -0.19686078 -0.56235707]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 2009 is [True, False, False, False, False, True]
Current timestep = 2010. State = [[-0.18005873  0.29533407]]. Action = [[ 0.17444909  0.1940723  -0.03117657  0.83541965]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 2010 is [True, False, False, False, False, True]
Current timestep = 2011. State = [[-0.17929392  0.30901292]]. Action = [[-0.23424752  0.11860144  0.1958645   0.77844644]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 2011 is [True, False, False, False, False, True]
Current timestep = 2012. State = [[-0.18633506  0.31774643]]. Action = [[ 0.13053128  0.16364753 -0.21501058 -0.3842436 ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 2012 is [True, False, False, False, False, True]
Current timestep = 2013. State = [[-0.18936086  0.3207843 ]]. Action = [[0.2162103  0.00748956 0.00159752 0.43281794]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 2013 is [True, False, False, False, False, True]
Current timestep = 2014. State = [[-0.18834816  0.31862065]]. Action = [[ 0.12364066 -0.19510983  0.20608675  0.75924635]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 2014 is [True, False, False, False, False, True]
Current timestep = 2015. State = [[-0.18668358  0.30936298]]. Action = [[-0.19221707 -0.1369428   0.0875701  -0.13536572]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 2015 is [True, False, False, False, False, True]
Current timestep = 2016. State = [[-0.18883303  0.3000576 ]]. Action = [[ 0.24018547 -0.08307582  0.13345432  0.6485381 ]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 2016 is [True, False, False, False, False, True]
Current timestep = 2017. State = [[-0.18253839  0.29508728]]. Action = [[-0.03535172  0.0700444  -0.01934786 -0.31246215]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 2017 is [True, False, False, False, False, True]
Scene graph at timestep 2017 is [True, False, False, False, False, True]
State prediction error at timestep 2017 is tensor(0.0763, grad_fn=<MseLossBackward0>)
Current timestep = 2018. State = [[-0.18085738  0.29170445]]. Action = [[-0.01007301 -0.2491679   0.05777407 -0.91786206]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 2018 is [True, False, False, False, False, True]
Current timestep = 2019. State = [[-0.18111484  0.27919865]]. Action = [[-0.0095911  -0.22085789 -0.03684333 -0.00244194]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 2019 is [True, False, False, False, False, True]
Current timestep = 2020. State = [[-0.17918645  0.2689548 ]]. Action = [[0.17678803 0.10598919 0.09755132 0.76328635]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 2020 is [True, False, False, False, False, True]
Current timestep = 2021. State = [[-0.17534183  0.2707293 ]]. Action = [[-0.19803168  0.13713369  0.13452506 -0.26100504]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 2021 is [True, False, False, False, False, True]
Current timestep = 2022. State = [[-0.18382007  0.27834806]]. Action = [[-0.24271575  0.13833055 -0.08310185  0.53768444]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 2022 is [True, False, False, False, False, True]
Current timestep = 2023. State = [[-0.19394827  0.28457105]]. Action = [[ 0.10159618 -0.06694303  0.03790343 -0.7612077 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 2023 is [True, False, False, False, False, True]
Current timestep = 2024. State = [[-0.19333982  0.2873083 ]]. Action = [[ 0.08285838  0.16084343 -0.19112466  0.0473088 ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 2024 is [True, False, False, False, False, True]
Current timestep = 2025. State = [[-0.18847732  0.29658535]]. Action = [[ 0.09483817  0.21009576 -0.11032806 -0.49733347]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 2025 is [True, False, False, False, False, True]
Current timestep = 2026. State = [[-0.18006039  0.30879915]]. Action = [[ 0.22634906  0.12284774 -0.12909532 -0.63170516]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 2026 is [True, False, False, False, False, True]
Current timestep = 2027. State = [[-0.16593355  0.31855232]]. Action = [[ 0.23673981  0.08313763 -0.20827763 -0.94050926]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 2027 is [True, False, False, False, False, True]
Current timestep = 2028. State = [[-0.15246844  0.32276493]]. Action = [[ 0.0101144  -0.12040858 -0.07637519  0.4992628 ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 2028 is [True, False, False, False, False, True]
Current timestep = 2029. State = [[-0.14725378  0.32042244]]. Action = [[-0.11096954  0.05510342 -0.11277291  0.87175775]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 2029 is [True, False, False, False, False, True]
Scene graph at timestep 2029 is [True, False, False, False, False, True]
State prediction error at timestep 2029 is tensor(0.0858, grad_fn=<MseLossBackward0>)
Current timestep = 2030. State = [[-0.14573409  0.31932023]]. Action = [[0.21255553 0.04600587 0.15181813 0.6358783 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 2030 is [True, False, False, False, False, True]
Scene graph at timestep 2030 is [True, False, False, False, False, True]
State prediction error at timestep 2030 is tensor(0.0791, grad_fn=<MseLossBackward0>)
Current timestep = 2031. State = [[-0.14532031  0.31896564]]. Action = [[ 0.13305044  0.13280138  0.0687992  -0.28732598]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 2031 is [True, False, False, False, False, True]
Scene graph at timestep 2031 is [True, False, False, False, False, True]
State prediction error at timestep 2031 is tensor(0.0811, grad_fn=<MseLossBackward0>)
Current timestep = 2032. State = [[-0.14521348  0.3188588 ]]. Action = [[ 0.10968161  0.11157432 -0.08287282 -0.38781798]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 2032 is [True, False, False, False, False, True]
Current timestep = 2033. State = [[-0.14403011  0.31507957]]. Action = [[ 0.07272729 -0.23671961 -0.15887353  0.93039596]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 2033 is [True, False, False, False, False, True]
Current timestep = 2034. State = [[-0.14099036  0.303211  ]]. Action = [[ 0.02071977 -0.18336117  0.2431649   0.294708  ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 2034 is [True, False, False, False, False, True]
Current timestep = 2035. State = [[-0.1379288  0.2929543]]. Action = [[0.08595824 0.01160491 0.13351262 0.6922443 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 2035 is [True, False, False, False, False, True]
Scene graph at timestep 2035 is [True, False, False, False, False, True]
State prediction error at timestep 2035 is tensor(0.0682, grad_fn=<MseLossBackward0>)
Current timestep = 2036. State = [[-0.13379368  0.2892586 ]]. Action = [[ 0.00616851 -0.0194995  -0.19311807  0.07935989]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 2036 is [True, False, False, False, False, True]
Current timestep = 2037. State = [[-0.13289082  0.28372622]]. Action = [[-0.06618088 -0.24047607 -0.0360681   0.99141645]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 2037 is [True, False, False, False, False, True]
Scene graph at timestep 2037 is [True, False, False, False, False, True]
State prediction error at timestep 2037 is tensor(0.0667, grad_fn=<MseLossBackward0>)
Current timestep = 2038. State = [[-0.13367802  0.2701613 ]]. Action = [[ 0.09052241 -0.24770363  0.02780154  0.8792069 ]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 2038 is [True, False, False, False, False, True]
Current timestep = 2039. State = [[-0.12806226  0.25708264]]. Action = [[ 0.21510881  0.0027253  -0.16852364 -0.45563734]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 2039 is [True, False, False, False, False, True]
Current timestep = 2040. State = [[-0.11750872  0.25561976]]. Action = [[ 0.07280466  0.21071762 -0.13513373 -0.2897681 ]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 2040 is [True, False, False, False, False, True]
Current timestep = 2041. State = [[-0.11115959  0.26079977]]. Action = [[-0.02125406 -0.08591595  0.23532236 -0.46610296]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 2041 is [True, False, False, False, False, True]
Current timestep = 2042. State = [[-0.10960693  0.26101592]]. Action = [[0.01340628 0.04558033 0.13769093 0.86702156]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 2042 is [True, False, False, False, False, True]
Scene graph at timestep 2042 is [True, False, False, False, False, True]
State prediction error at timestep 2042 is tensor(0.0572, grad_fn=<MseLossBackward0>)
Current timestep = 2043. State = [[-0.10692985  0.26242605]]. Action = [[ 0.12831911  0.00628152  0.1854068  -0.9757382 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 2043 is [True, False, False, False, False, True]
Current timestep = 2044. State = [[-0.10134271  0.26008397]]. Action = [[ 0.01925713 -0.19504382  0.20750931  0.18623102]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 2044 is [True, False, False, False, False, True]
Current timestep = 2045. State = [[-0.09977972  0.25223917]]. Action = [[-0.07994233 -0.04601786  0.11769158  0.784688  ]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 2045 is [True, False, False, False, False, True]
Current timestep = 2046. State = [[-0.10367301  0.24511193]]. Action = [[-0.11588028 -0.16291755 -0.02568114 -0.9895856 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 2046 is [True, False, False, False, False, True]
Current timestep = 2047. State = [[-0.10580635  0.23654447]]. Action = [[ 0.23326707 -0.05642155 -0.2431644   0.29008365]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 2047 is [True, False, False, False, False, True]
Current timestep = 2048. State = [[-0.09907837  0.22806928]]. Action = [[-0.01900214 -0.21401633 -0.07620645  0.8167281 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 2048 is [True, False, False, False, False, True]
Current timestep = 2049. State = [[-0.09790435  0.22215214]]. Action = [[-0.06773366  0.24625838 -0.0372598   0.42490292]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 2049 is [True, False, False, False, False, True]
Current timestep = 2050. State = [[-0.09913201  0.22605439]]. Action = [[ 0.04494947 -0.10925233  0.03303432  0.38122118]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 2050 is [True, False, False, False, False, True]
Scene graph at timestep 2050 is [True, False, False, False, False, True]
State prediction error at timestep 2050 is tensor(0.0424, grad_fn=<MseLossBackward0>)
Current timestep = 2051. State = [[-0.10090055  0.22665252]]. Action = [[-0.17495118  0.14072517  0.09609514 -0.5972911 ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 2051 is [True, False, False, False, False, True]
Current timestep = 2052. State = [[-0.10904827  0.23457849]]. Action = [[-0.11991566  0.21768722 -0.00197624  0.2823969 ]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 2052 is [True, False, False, False, False, True]
Current timestep = 2053. State = [[-0.11608808  0.24352925]]. Action = [[ 0.00823697 -0.06988662 -0.22537547 -0.25975764]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 2053 is [True, False, False, False, False, True]
Scene graph at timestep 2053 is [True, False, False, False, False, True]
State prediction error at timestep 2053 is tensor(0.0533, grad_fn=<MseLossBackward0>)
Current timestep = 2054. State = [[-0.12079568  0.2408967 ]]. Action = [[-0.16870725 -0.24024487 -0.21430811 -0.2942068 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 2054 is [True, False, False, False, False, True]
Current timestep = 2055. State = [[-0.13116932  0.22806074]]. Action = [[-0.21227665 -0.2472413   0.03326991 -0.06949967]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 2055 is [True, False, False, False, False, True]
Current timestep = 2056. State = [[-0.14239486  0.21636143]]. Action = [[-0.00137903  0.07392132  0.1644159   0.3672341 ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 2056 is [True, False, False, False, False, True]
Scene graph at timestep 2056 is [True, False, False, False, False, True]
State prediction error at timestep 2056 is tensor(0.0472, grad_fn=<MseLossBackward0>)
Current timestep = 2057. State = [[-0.14688212  0.2163376 ]]. Action = [[-0.03119078  0.12547725 -0.06728956  0.8339248 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 2057 is [True, False, False, False, False, True]
Current timestep = 2058. State = [[-0.14905427  0.2232177 ]]. Action = [[ 0.00954497  0.15819043  0.05800366 -0.7627143 ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 2058 is [True, False, False, False, False, True]
Scene graph at timestep 2058 is [True, False, False, False, False, True]
State prediction error at timestep 2058 is tensor(0.0520, grad_fn=<MseLossBackward0>)
Current timestep = 2059. State = [[-0.14679374  0.2272802 ]]. Action = [[ 0.17380545 -0.23364085 -0.20395909 -0.20010078]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 2059 is [True, False, False, False, False, True]
Current timestep = 2060. State = [[-0.13983001  0.22153372]]. Action = [[ 0.0270443   0.01297548  0.19497272 -0.69107825]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 2060 is [True, False, False, False, False, True]
Current timestep = 2061. State = [[-0.1356679   0.22269788]]. Action = [[ 0.03318048  0.20493096  0.09417865 -0.00707263]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 2061 is [True, False, False, False, False, True]
Current timestep = 2062. State = [[-0.13635087  0.22941819]]. Action = [[-0.22313245 -0.02552065  0.19716716 -0.42405927]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 2062 is [True, False, False, False, False, True]
Current timestep = 2063. State = [[-0.14773844  0.23189299]]. Action = [[-0.24280706  0.03750858  0.06923047  0.7074919 ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 2063 is [True, False, False, False, False, True]
Current timestep = 2064. State = [[-0.16056798  0.23375438]]. Action = [[-0.01071133 -0.00170667  0.22689992 -0.752047  ]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 2064 is [True, False, False, False, False, True]
Current timestep = 2065. State = [[-0.16285089  0.23726568]]. Action = [[0.19856301 0.18036166 0.12863114 0.09679008]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 2065 is [True, False, False, False, False, True]
Current timestep = 2066. State = [[-0.15756837  0.24744043]]. Action = [[-0.062279    0.19869387  0.02086839 -0.79732025]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 2066 is [True, False, False, False, False, True]
Current timestep = 2067. State = [[-0.16029888  0.25710437]]. Action = [[-0.21199997 -0.03918841 -0.04321164 -0.6002094 ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 2067 is [True, False, False, False, False, True]
Current timestep = 2068. State = [[-0.1703142   0.25590605]]. Action = [[-0.11532182 -0.22103001  0.03925246  0.58801365]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 2068 is [True, False, False, False, False, True]
Scene graph at timestep 2068 is [True, False, False, False, False, True]
State prediction error at timestep 2068 is tensor(0.0598, grad_fn=<MseLossBackward0>)
Current timestep = 2069. State = [[-0.17954732  0.24793482]]. Action = [[-0.08469939 -0.01214859  0.03312653  0.26109397]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 2069 is [True, False, False, False, False, True]
Current timestep = 2070. State = [[-0.18596976  0.2418258 ]]. Action = [[-0.03017768 -0.15965204 -0.11611438  0.17806613]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 2070 is [True, False, False, False, False, True]
Current timestep = 2071. State = [[-0.18980739  0.23721787]]. Action = [[-0.03162092  0.16194719 -0.07578766  0.54064584]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 2071 is [True, False, False, False, False, True]
Current timestep = 2072. State = [[-0.18931395  0.23969238]]. Action = [[ 0.17996484 -0.05787964  0.02991885  0.90249157]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 2072 is [True, False, False, False, False, True]
Current timestep = 2073. State = [[-0.18004446  0.24257676]]. Action = [[ 0.21606731  0.22297841  0.00555062 -0.6403414 ]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 2073 is [True, False, False, False, False, True]
Current timestep = 2074. State = [[-0.17133878  0.25116807]]. Action = [[-0.15910314  0.03231573 -0.00891797  0.4911661 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 2074 is [True, False, False, False, False, True]
Current timestep = 2075. State = [[-0.17574246  0.2522851 ]]. Action = [[-0.16884157 -0.21191628 -0.01695052  0.04979265]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 2075 is [True, False, False, False, False, True]
Current timestep = 2076. State = [[-0.18435544  0.24471822]]. Action = [[-0.04220395 -0.06432816  0.02079394  0.12862206]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 2076 is [True, False, False, False, False, True]
Scene graph at timestep 2076 is [True, False, False, False, False, True]
State prediction error at timestep 2076 is tensor(0.0611, grad_fn=<MseLossBackward0>)
Current timestep = 2077. State = [[-0.18848118  0.2431673 ]]. Action = [[ 0.04711238  0.22783142  0.02174714 -0.7956827 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 2077 is [True, False, False, False, False, True]
Scene graph at timestep 2077 is [True, False, False, False, False, True]
State prediction error at timestep 2077 is tensor(0.0651, grad_fn=<MseLossBackward0>)
Current timestep = 2078. State = [[-0.1893109  0.2498603]]. Action = [[-0.08923627 -0.00746241 -0.16701986 -0.23409432]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 2078 is [True, False, False, False, False, True]
Current timestep = 2079. State = [[-0.19075601  0.25538412]]. Action = [[ 0.12070796  0.1872819   0.06948441 -0.75553083]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 2079 is [True, False, False, False, False, True]
Current timestep = 2080. State = [[-0.18669987  0.26649722]]. Action = [[0.04770947 0.20588809 0.21156138 0.14143789]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 2080 is [True, False, False, False, False, True]
Current timestep = 2081. State = [[-0.18197091  0.27770174]]. Action = [[ 0.08871064  0.02488896 -0.19303736  0.46577454]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 2081 is [True, False, False, False, False, True]
Current timestep = 2082. State = [[-0.17660624  0.28267482]]. Action = [[ 0.04773217  0.00330874  0.20211041 -0.7810366 ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 2082 is [True, False, False, False, False, True]
Scene graph at timestep 2082 is [True, False, False, False, False, True]
State prediction error at timestep 2082 is tensor(0.0672, grad_fn=<MseLossBackward0>)
Current timestep = 2083. State = [[-0.17287698  0.28749812]]. Action = [[ 0.01786989  0.19653547 -0.0842123   0.58581245]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 2083 is [True, False, False, False, False, True]
Current timestep = 2084. State = [[-0.1682427   0.29637197]]. Action = [[ 0.18256682  0.0579761  -0.11354475 -0.231027  ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 2084 is [True, False, False, False, False, True]
Current timestep = 2085. State = [[-0.15836872  0.3052547 ]]. Action = [[ 0.15474352  0.2313087  -0.04162824  0.13015485]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 2085 is [True, False, False, False, False, True]
Current timestep = 2086. State = [[-0.14927433  0.31580976]]. Action = [[-0.21502283  0.20235115  0.17596495  0.5229237 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 2086 is [True, False, False, False, False, True]
Current timestep = 2087. State = [[-0.14921956  0.31795925]]. Action = [[-0.22834778 -0.11756271 -0.04721583 -0.6615395 ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 2087 is [True, False, False, False, False, True]
Current timestep = 2088. State = [[-0.15727305  0.31475216]]. Action = [[-0.21136457  0.21486825  0.24658275  0.21774268]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 2088 is [True, False, False, False, False, True]
Current timestep = 2089. State = [[-0.15960972  0.31371877]]. Action = [[0.06797647 0.02325204 0.177463   0.43241477]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 2089 is [True, False, False, False, False, True]
Current timestep = 2090. State = [[-0.15788005  0.31415954]]. Action = [[-0.22030398  0.15503508  0.2405015  -0.6041136 ]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 2090 is [True, False, False, False, False, True]
Current timestep = 2091. State = [[-0.15706158  0.31439096]]. Action = [[-0.17150462  0.13839     0.20224169  0.8060503 ]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 2091 is [True, False, False, False, False, True]
Current timestep = 2092. State = [[-0.15444121  0.31134716]]. Action = [[ 0.1567812  -0.19753227  0.04015702  0.23205304]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 2092 is [True, False, False, False, False, True]
Current timestep = 2093. State = [[-0.14842845  0.303857  ]]. Action = [[ 0.09961906  0.20050424 -0.06489158  0.6872227 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 2093 is [True, False, False, False, False, True]
Current timestep = 2094. State = [[-0.14597975  0.30095267]]. Action = [[-0.08371472  0.19757551 -0.02258669  0.02512085]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 2094 is [True, False, False, False, False, True]
Scene graph at timestep 2094 is [True, False, False, False, False, True]
State prediction error at timestep 2094 is tensor(0.0791, grad_fn=<MseLossBackward0>)
Current timestep = 2095. State = [[-0.14519131  0.29832622]]. Action = [[-1.19790435e-04 -1.09645218e-01  3.03291082e-02 -8.93986762e-01]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 2095 is [True, False, False, False, False, True]
Current timestep = 2096. State = [[-0.14822945  0.29258507]]. Action = [[-0.21926905 -0.08310518  0.06096429  0.63767314]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 2096 is [True, False, False, False, False, True]
Scene graph at timestep 2096 is [True, False, False, False, False, True]
State prediction error at timestep 2096 is tensor(0.0699, grad_fn=<MseLossBackward0>)
Current timestep = 2097. State = [[-0.1558983   0.28794846]]. Action = [[0.06299013 0.01288891 0.2405009  0.557729  ]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 2097 is [True, False, False, False, False, True]
Current timestep = 2098. State = [[-0.16008751  0.28312036]]. Action = [[-0.23343894 -0.22327937  0.14958936 -0.8784566 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 2098 is [True, False, False, False, False, True]
Current timestep = 2099. State = [[-0.1683213   0.27291226]]. Action = [[ 0.0677104  -0.09091316 -0.14011751  0.8979167 ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 2099 is [True, False, False, False, False, True]
Current timestep = 2100. State = [[-0.17060877  0.26258877]]. Action = [[-0.09431271 -0.21883382 -0.12638341  0.6047181 ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 2100 is [True, False, False, False, False, True]
Current timestep = 2101. State = [[-0.17297213  0.2497989 ]]. Action = [[ 0.09400338 -0.13498941 -0.03441474  0.81614864]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 2101 is [True, False, False, False, False, True]
Current timestep = 2102. State = [[-0.17209531  0.24046671]]. Action = [[-0.08986753 -0.01420896 -0.05246003  0.46291196]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 2102 is [True, False, False, False, False, True]
Scene graph at timestep 2102 is [True, False, False, False, False, True]
State prediction error at timestep 2102 is tensor(0.0608, grad_fn=<MseLossBackward0>)
Current timestep = 2103. State = [[-0.17488019  0.23914719]]. Action = [[-0.02249691  0.1538235   0.03304335 -0.26625884]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 2103 is [True, False, False, False, False, True]
Scene graph at timestep 2103 is [True, False, False, False, False, True]
State prediction error at timestep 2103 is tensor(0.0591, grad_fn=<MseLossBackward0>)
Current timestep = 2104. State = [[-0.17674033  0.24073619]]. Action = [[-0.00999254 -0.19194871  0.1316691  -0.11340195]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 2104 is [True, False, False, False, False, True]
Current timestep = 2105. State = [[-0.17853542  0.23927537]]. Action = [[-0.0343004   0.24486005 -0.0706739  -0.1266011 ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 2105 is [True, False, False, False, False, True]
Current timestep = 2106. State = [[-0.17974494  0.24743399]]. Action = [[ 0.02603096  0.07225665 -0.18877892 -0.2645594 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 2106 is [True, False, False, False, False, True]
Current timestep = 2107. State = [[-0.18086925  0.25016993]]. Action = [[-0.11734146 -0.18569298 -0.00155467  0.06064236]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 2107 is [True, False, False, False, False, True]
Current timestep = 2108. State = [[-0.18462184  0.24127261]]. Action = [[ 0.04309133 -0.24569955 -0.17253779  0.5913948 ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 2108 is [True, False, False, False, False, True]
Current timestep = 2109. State = [[-0.18402077  0.23141594]]. Action = [[ 0.06710172  0.10188791 -0.10768837 -0.8339719 ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 2109 is [True, False, False, False, False, True]
Current timestep = 2110. State = [[-0.18452035  0.23016208]]. Action = [[-0.23838732 -0.04802817  0.2471689   0.5150596 ]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 2110 is [True, False, False, False, False, True]
Scene graph at timestep 2110 is [True, False, False, False, False, True]
State prediction error at timestep 2110 is tensor(0.0572, grad_fn=<MseLossBackward0>)
Current timestep = 2111. State = [[-0.19124612  0.22638427]]. Action = [[ 0.09523419 -0.13720118  0.09853107  0.5630684 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 2111 is [True, False, False, False, False, True]
Current timestep = 2112. State = [[-0.1884622   0.21960911]]. Action = [[ 0.17595631 -0.05813012 -0.18443295  0.42746723]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 2112 is [True, False, False, False, False, True]
Scene graph at timestep 2112 is [True, False, False, False, False, True]
State prediction error at timestep 2112 is tensor(0.0598, grad_fn=<MseLossBackward0>)
Current timestep = 2113. State = [[-0.18080173  0.21794794]]. Action = [[ 0.02855173  0.17772573  0.20084986 -0.2682743 ]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 2113 is [True, False, False, False, False, True]
Current timestep = 2114. State = [[-0.17381562  0.22605649]]. Action = [[ 0.20323515  0.18346012 -0.1820482  -0.03155702]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 2114 is [True, False, False, False, False, True]
Current timestep = 2115. State = [[-0.16248302  0.2357356 ]]. Action = [[ 0.14230418  0.03650141 -0.21022132 -0.20085979]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 2115 is [True, False, False, False, False, True]
Current timestep = 2116. State = [[-0.15220958  0.2426173 ]]. Action = [[0.08777022 0.132783   0.09418508 0.11386538]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 2116 is [True, False, False, False, False, True]
Current timestep = 2117. State = [[-0.14312722  0.24899127]]. Action = [[ 0.16446725 -0.01639847 -0.13889702  0.79923844]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 2117 is [True, False, False, False, False, True]
Current timestep = 2118. State = [[-0.13736662  0.24732968]]. Action = [[-0.2103816  -0.21859728 -0.2109044  -0.02850908]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 2118 is [True, False, False, False, False, True]
Current timestep = 2119. State = [[-0.14394332  0.23890308]]. Action = [[-0.09526291 -0.04581995  0.1671986  -0.06442159]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 2119 is [True, False, False, False, False, True]
Current timestep = 2120. State = [[-0.14914523  0.23727469]]. Action = [[ 0.07345587  0.20396489  0.08941615 -0.53980607]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 2120 is [True, False, False, False, False, True]
Scene graph at timestep 2120 is [True, False, False, False, False, True]
State prediction error at timestep 2120 is tensor(0.0551, grad_fn=<MseLossBackward0>)
Current timestep = 2121. State = [[-0.14959876  0.24673516]]. Action = [[-0.08290014  0.21684411  0.08150774  0.8640981 ]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 2121 is [True, False, False, False, False, True]
Current timestep = 2122. State = [[-0.14988834  0.2575441 ]]. Action = [[ 0.15679133  0.00103119 -0.19522104 -0.9144186 ]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 2122 is [True, False, False, False, False, True]
Scene graph at timestep 2122 is [True, False, False, False, False, True]
State prediction error at timestep 2122 is tensor(0.0686, grad_fn=<MseLossBackward0>)
Current timestep = 2123. State = [[-0.14421809  0.26287043]]. Action = [[ 0.04907981  0.08274782 -0.1276452  -0.9582584 ]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 2123 is [True, False, False, False, False, True]
Current timestep = 2124. State = [[-0.13730441  0.26346654]]. Action = [[ 0.18653357 -0.24123725  0.17315736 -0.28582716]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 2124 is [True, False, False, False, False, True]
Current timestep = 2125. State = [[-0.12693763  0.25456995]]. Action = [[ 0.12896246 -0.08280021 -0.17658757 -0.9045368 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 2125 is [True, False, False, False, False, True]
Current timestep = 2126. State = [[-0.12213296  0.25137922]]. Action = [[-0.23087983  0.19651541  0.19347739 -0.72759354]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 2126 is [True, False, False, False, False, True]
Scene graph at timestep 2126 is [True, False, False, False, False, True]
State prediction error at timestep 2126 is tensor(0.0578, grad_fn=<MseLossBackward0>)
Current timestep = 2127. State = [[-0.13165227  0.25773564]]. Action = [[-0.24682589  0.06719604 -0.05279149  0.04487443]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 2127 is [True, False, False, False, False, True]
Current timestep = 2128. State = [[-0.14233454  0.26453194]]. Action = [[0.12384185 0.12526089 0.03681582 0.6937523 ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 2128 is [True, False, False, False, False, True]
Current timestep = 2129. State = [[-0.13911109  0.26970944]]. Action = [[ 0.20037878 -0.08053187 -0.18977818  0.56379175]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 2129 is [True, False, False, False, False, True]
Current timestep = 2130. State = [[-0.13123548  0.26929668]]. Action = [[-0.03608634  0.01903448 -0.0797614   0.43078148]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 2130 is [True, False, False, False, False, True]
Current timestep = 2131. State = [[-0.13135295  0.26649535]]. Action = [[-0.15047184 -0.19680858  0.19587088 -0.22877592]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 2131 is [True, False, False, False, False, True]
Scene graph at timestep 2131 is [True, False, False, False, False, True]
State prediction error at timestep 2131 is tensor(0.0532, grad_fn=<MseLossBackward0>)
Current timestep = 2132. State = [[-0.13481148  0.25922903]]. Action = [[ 1.5589392e-01 -3.3572316e-05 -1.7300075e-01  5.4511690e-01]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 2132 is [True, False, False, False, False, True]
Scene graph at timestep 2132 is [True, False, False, False, False, True]
State prediction error at timestep 2132 is tensor(0.0621, grad_fn=<MseLossBackward0>)
Current timestep = 2133. State = [[-0.13259473  0.2585401 ]]. Action = [[-0.10742071  0.14442402  0.03960609  0.42210066]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 2133 is [True, False, False, False, False, True]
Scene graph at timestep 2133 is [True, False, False, False, False, True]
State prediction error at timestep 2133 is tensor(0.0612, grad_fn=<MseLossBackward0>)
Current timestep = 2134. State = [[-0.13441879  0.2606313 ]]. Action = [[ 0.0231328  -0.16117671  0.10597849 -0.02676791]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 2134 is [True, False, False, False, False, True]
Scene graph at timestep 2134 is [True, False, False, False, False, True]
State prediction error at timestep 2134 is tensor(0.0542, grad_fn=<MseLossBackward0>)
Current timestep = 2135. State = [[-0.13499635  0.2580152 ]]. Action = [[-0.01042387  0.10261923 -0.03617597 -0.83360785]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 2135 is [True, False, False, False, False, True]
Current timestep = 2136. State = [[-0.13505398  0.26239565]]. Action = [[ 0.02579537  0.14617723 -0.10120577 -0.66757333]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 2136 is [True, False, False, False, False, True]
Current timestep = 2137. State = [[-0.13197564  0.26543108]]. Action = [[ 0.13462359 -0.22092134 -0.24611393 -0.5978467 ]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 2137 is [True, False, False, False, False, True]
Current timestep = 2138. State = [[-0.12767844  0.26217264]]. Action = [[-0.07219198  0.17089856 -0.0810948   0.25242615]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 2138 is [True, False, False, False, False, True]
Current timestep = 2139. State = [[-0.12961704  0.2662064 ]]. Action = [[-0.09892014  0.00329429 -0.05123553 -0.6237473 ]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 2139 is [True, False, False, False, False, True]
Current timestep = 2140. State = [[-0.13662374  0.27026796]]. Action = [[-0.17877564  0.1393202  -0.18097635  0.41017556]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 2140 is [True, False, False, False, False, True]
Current timestep = 2141. State = [[-0.14292201  0.2791615 ]]. Action = [[0.17240003 0.19004214 0.06813458 0.39526093]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 2141 is [True, False, False, False, False, True]
Current timestep = 2142. State = [[-0.1406674   0.28638068]]. Action = [[-0.1055128  -0.1384133  -0.07102877  0.3278165 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 2142 is [True, False, False, False, False, True]
Current timestep = 2143. State = [[-0.13971001  0.28273377]]. Action = [[ 0.23229241 -0.12676807 -0.19634874  0.25669396]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 2143 is [True, False, False, False, False, True]
Scene graph at timestep 2143 is [True, False, False, False, False, True]
State prediction error at timestep 2143 is tensor(0.0687, grad_fn=<MseLossBackward0>)
Current timestep = 2144. State = [[-0.13128324  0.27616826]]. Action = [[ 0.040923   -0.04420397 -0.14238463 -0.4864266 ]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 2144 is [True, False, False, False, False, True]
Current timestep = 2145. State = [[-0.12364618  0.26903298]]. Action = [[ 0.18371329 -0.20947509  0.22323376 -0.2483105 ]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 2145 is [True, False, False, False, False, True]
Current timestep = 2146. State = [[-0.11569673  0.25777847]]. Action = [[-0.04997194 -0.12249406 -0.19611965 -0.5657355 ]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 2146 is [True, False, False, False, False, True]
Current timestep = 2147. State = [[-0.11193265  0.24946314]]. Action = [[ 0.17634216 -0.01503316  0.19923481  0.5297451 ]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 2147 is [True, False, False, False, False, True]
Current timestep = 2148. State = [[-0.10549531  0.2483214 ]]. Action = [[-0.0382508   0.15237182 -0.17333889  0.08125448]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 2148 is [True, False, False, False, False, True]
Current timestep = 2149. State = [[-0.10417033  0.25160247]]. Action = [[-0.01188856 -0.09108752 -0.10496023 -0.05551994]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 2149 is [True, False, False, False, False, True]
Scene graph at timestep 2149 is [True, False, False, False, False, True]
State prediction error at timestep 2149 is tensor(0.0518, grad_fn=<MseLossBackward0>)
Current timestep = 2150. State = [[-0.10190988  0.25030792]]. Action = [[0.1811806  0.01080519 0.18850729 0.4897406 ]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 2150 is [True, False, False, False, False, True]
Current timestep = 2151. State = [[-0.09169895  0.24662036]]. Action = [[ 0.22099847 -0.21004193 -0.02078354  0.5895183 ]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 2151 is [True, False, False, False, False, True]
Current timestep = 2152. State = [[-0.08108535  0.24236092]]. Action = [[-0.03371374  0.24672097  0.24931979  0.45355403]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 2152 is [True, False, False, False, False, True]
Scene graph at timestep 2152 is [True, False, False, False, False, True]
State prediction error at timestep 2152 is tensor(0.0521, grad_fn=<MseLossBackward0>)
Current timestep = 2153. State = [[-0.07646954  0.25035584]]. Action = [[0.11142302 0.10793453 0.17696643 0.2850517 ]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 2153 is [True, False, False, False, False, True]
Scene graph at timestep 2153 is [True, False, False, False, False, True]
State prediction error at timestep 2153 is tensor(0.0497, grad_fn=<MseLossBackward0>)
Current timestep = 2154. State = [[-0.07412457  0.25514796]]. Action = [[-0.20020641 -0.12343043  0.11896855  0.76049757]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 2154 is [True, False, False, False, False, True]
Scene graph at timestep 2154 is [True, False, False, False, False, True]
State prediction error at timestep 2154 is tensor(0.0490, grad_fn=<MseLossBackward0>)
Current timestep = 2155. State = [[-0.07894243  0.25693277]]. Action = [[ 0.09695107  0.24818921 -0.06646809 -0.9814966 ]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 2155 is [True, False, False, False, False, True]
Current timestep = 2156. State = [[-0.07419631  0.26748118]]. Action = [[0.2444188  0.14154938 0.1006541  0.331236  ]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 2156 is [True, False, False, False, False, True]
Current timestep = 2157. State = [[-0.0614933   0.27808872]]. Action = [[0.1681171  0.12563148 0.09287423 0.6159346 ]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 2157 is [True, False, False, False, False, True]
Current timestep = 2158. State = [[-0.05324939  0.28742722]]. Action = [[-0.1660329   0.0974524   0.04046214 -0.59012383]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 2158 is [True, False, False, False, False, True]
Current timestep = 2159. State = [[-0.05653768  0.2939314 ]]. Action = [[-0.04939717 -0.00844623 -0.00659421  0.24703228]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 2159 is [True, False, False, False, False, True]
Current timestep = 2160. State = [[-0.05776763  0.29413202]]. Action = [[ 0.1492612  -0.11798513  0.24957487  0.6456208 ]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 2160 is [True, False, False, False, False, True]
Scene graph at timestep 2160 is [True, False, False, False, False, True]
State prediction error at timestep 2160 is tensor(0.0577, grad_fn=<MseLossBackward0>)
Current timestep = 2161. State = [[-0.05421325  0.2938375 ]]. Action = [[-0.05460188  0.2351461  -0.05480367  0.9785106 ]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 2161 is [True, False, False, False, False, True]
Current timestep = 2162. State = [[-0.05772365  0.3023764 ]]. Action = [[-0.24115615  0.07937282  0.1810508   0.361583  ]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 2162 is [True, False, False, False, False, True]
Scene graph at timestep 2162 is [True, False, False, False, False, True]
State prediction error at timestep 2162 is tensor(0.0654, grad_fn=<MseLossBackward0>)
Current timestep = 2163. State = [[-0.06680141  0.30915976]]. Action = [[ 0.0421873   0.04152122 -0.17976342 -0.03141552]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 2163 is [True, False, False, False, False, True]
Scene graph at timestep 2163 is [True, False, False, False, False, True]
State prediction error at timestep 2163 is tensor(0.0739, grad_fn=<MseLossBackward0>)
Current timestep = 2164. State = [[-0.06654564  0.31028667]]. Action = [[ 0.15754837 -0.16224375 -0.08767428 -0.47294277]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 2164 is [True, False, False, False, False, True]
Current timestep = 2165. State = [[-0.05881328  0.3050243 ]]. Action = [[ 0.14965302 -0.02074632  0.05982494 -0.28896034]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 2165 is [True, False, False, False, False, True]
Current timestep = 2166. State = [[-0.05244134  0.30301377]]. Action = [[-0.121351    0.05672288 -0.03653766 -0.54068977]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 2166 is [True, False, False, False, False, True]
Current timestep = 2167. State = [[-0.05317454  0.30296972]]. Action = [[ 0.06057075 -0.08657384  0.09461692  0.10304976]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 2167 is [True, False, False, False, False, True]
Scene graph at timestep 2167 is [True, False, False, False, False, True]
State prediction error at timestep 2167 is tensor(0.0624, grad_fn=<MseLossBackward0>)
Current timestep = 2168. State = [[-0.05190476  0.30022573]]. Action = [[0.02810541 0.22802156 0.09098276 0.54353046]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 2168 is [True, False, False, False, False, True]
Scene graph at timestep 2168 is [True, False, False, False, False, True]
State prediction error at timestep 2168 is tensor(0.0686, grad_fn=<MseLossBackward0>)
Current timestep = 2169. State = [[-0.05129348  0.2991169 ]]. Action = [[-0.07463898  0.2443996  -0.12419215 -0.45153636]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 2169 is [True, False, False, False, False, True]
Current timestep = 2170. State = [[-0.04922823  0.2959908 ]]. Action = [[ 0.1212855  -0.17893621 -0.20570095 -0.2613684 ]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 2170 is [True, False, False, False, False, True]
Current timestep = 2171. State = [[-0.0420413   0.28892446]]. Action = [[ 0.17844272 -0.01139483 -0.20412306 -0.02302194]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 2171 is [False, True, False, False, False, True]
Current timestep = 2172. State = [[-0.03619189  0.2889834 ]]. Action = [[-0.19815528  0.21120414  0.24923211  0.3693819 ]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 2172 is [False, True, False, False, False, True]
Scene graph at timestep 2172 is [False, True, False, False, False, True]
State prediction error at timestep 2172 is tensor(0.0642, grad_fn=<MseLossBackward0>)
Current timestep = 2173. State = [[-0.03868622  0.29385513]]. Action = [[ 0.11376616 -0.14544919 -0.01158328 -0.60837436]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 2173 is [False, True, False, False, False, True]
Current timestep = 2174. State = [[-0.03428821  0.29487035]]. Action = [[0.16381389 0.23354858 0.15549043 0.40819025]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 2174 is [False, True, False, False, False, True]
Current timestep = 2175. State = [[-0.02360068  0.30618548]]. Action = [[ 0.23118457  0.23878568 -0.13220268 -0.48320496]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 2175 is [False, True, False, False, False, True]
Current timestep = 2176. State = [[-0.01163521  0.318138  ]]. Action = [[-0.13551483  0.16190588  0.06346822  0.05744565]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 2176 is [False, True, False, False, False, True]
Current timestep = 2177. State = [[-0.00932884  0.32023948]]. Action = [[-0.16596267 -0.14920196 -0.15145303  0.6093099 ]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 2177 is [False, True, False, False, False, True]
Current timestep = 2178. State = [[-0.01468175  0.3161499 ]]. Action = [[ 0.16768622  0.06003195 -0.09147775 -0.92634344]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 2178 is [False, True, False, False, False, True]
Current timestep = 2179. State = [[-0.01936295  0.31383923]]. Action = [[-0.17118487 -0.02739185  0.06428927  0.7559736 ]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 2179 is [False, True, False, False, False, True]
Current timestep = 2180. State = [[-0.02406727  0.30931184]]. Action = [[ 0.17504302 -0.19519629 -0.03029011  0.7883053 ]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 2180 is [False, True, False, False, False, True]
Scene graph at timestep 2180 is [False, True, False, False, False, True]
State prediction error at timestep 2180 is tensor(0.0661, grad_fn=<MseLossBackward0>)
Current timestep = 2181. State = [[-0.01887107  0.30085045]]. Action = [[ 0.08932951 -0.02572699 -0.16203482  0.3796159 ]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 2181 is [False, True, False, False, False, True]
Scene graph at timestep 2181 is [False, True, False, False, False, True]
State prediction error at timestep 2181 is tensor(0.0644, grad_fn=<MseLossBackward0>)
Current timestep = 2182. State = [[-0.01348315  0.2967539 ]]. Action = [[ 0.00352594  0.2370502  -0.22192048 -0.9516715 ]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 2182 is [False, True, False, False, False, True]
Scene graph at timestep 2182 is [False, True, False, False, False, True]
State prediction error at timestep 2182 is tensor(0.0878, grad_fn=<MseLossBackward0>)
Current timestep = 2183. State = [[-0.00826766  0.2978925 ]]. Action = [[0.23297626 0.14857876 0.16235256 0.43006313]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 2183 is [False, True, False, False, False, True]
Current timestep = 2184. State = [[0.00106407 0.30522963]]. Action = [[-0.01682253  0.14339608  0.1284545   0.35201216]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 2184 is [False, True, False, False, False, True]
Current timestep = 2185. State = [[0.00240135 0.31029287]]. Action = [[-0.1415028  -0.15121451 -0.11758456 -0.07982153]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 2185 is [False, True, False, False, False, True]
Current timestep = 2186. State = [[-0.00241254  0.30750498]]. Action = [[ 0.04346636  0.17349297  0.0237259  -0.5629725 ]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 2186 is [False, True, False, False, False, True]
Current timestep = 2187. State = [[-0.00582109  0.30637172]]. Action = [[-0.09728819  0.01940009  0.21528941 -0.20655668]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 2187 is [False, True, False, False, False, True]
Current timestep = 2188. State = [[-0.00941756  0.3075797 ]]. Action = [[ 0.05454308  0.05315495 -0.18121302  0.6086421 ]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 2188 is [False, True, False, False, False, True]
Scene graph at timestep 2188 is [False, True, False, False, False, True]
State prediction error at timestep 2188 is tensor(0.0684, grad_fn=<MseLossBackward0>)
Current timestep = 2189. State = [[-0.01093443  0.30694392]]. Action = [[-0.15306327 -0.16938491 -0.16207443 -0.75148433]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 2189 is [False, True, False, False, False, True]
Current timestep = 2190. State = [[-0.01677295  0.30147967]]. Action = [[ 0.02734488  0.13809949  0.04194143 -0.47878927]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 2190 is [False, True, False, False, False, True]
Current timestep = 2191. State = [[-0.01754653  0.3016213 ]]. Action = [[ 0.11180761  0.15037319 -0.06576698  0.18427753]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 2191 is [False, True, False, False, False, True]
Current timestep = 2192. State = [[-0.01630539  0.30342948]]. Action = [[-0.19739598 -0.19054241  0.02920839 -0.16125149]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 2192 is [False, True, False, False, False, True]
Current timestep = 2193. State = [[-0.02291576  0.29928377]]. Action = [[-0.01304579  0.05662063 -0.11642967  0.14739466]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 2193 is [False, True, False, False, False, True]
Scene graph at timestep 2193 is [False, True, False, False, False, True]
State prediction error at timestep 2193 is tensor(0.0660, grad_fn=<MseLossBackward0>)
Current timestep = 2194. State = [[-0.0269321   0.29864097]]. Action = [[-0.06803313 -0.03507911 -0.22878505  0.6419735 ]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 2194 is [False, True, False, False, False, True]
Current timestep = 2195. State = [[-0.02981112  0.29374966]]. Action = [[ 0.0368734  -0.23779     0.10184711  0.53708935]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 2195 is [False, True, False, False, False, True]
Current timestep = 2196. State = [[-0.02931573  0.2811792 ]]. Action = [[ 0.03314304 -0.19477229 -0.03395538  0.85433865]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 2196 is [False, True, False, False, False, True]
Current timestep = 2197. State = [[-0.02548407  0.27363965]]. Action = [[ 0.19228053  0.22053504 -0.07424241  0.53582335]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 2197 is [False, True, False, False, False, True]
Current timestep = 2198. State = [[-0.01554545  0.27563325]]. Action = [[ 0.10362756 -0.14343292 -0.06242271  0.24095166]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 2198 is [False, True, False, False, False, True]
Current timestep = 2199. State = [[-0.00931551  0.2722883 ]]. Action = [[-0.06830083  0.00605664  0.14812079  0.84162426]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 2199 is [False, True, False, False, False, True]
Current timestep = 2200. State = [[-0.01180877  0.27251014]]. Action = [[-0.15675643  0.09215343 -0.21321878  0.01620352]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 2200 is [False, True, False, False, False, True]
Current timestep = 2201. State = [[-0.01976679  0.27946115]]. Action = [[-0.09189519  0.23873103  0.15306169  0.24134636]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 2201 is [False, True, False, False, False, True]
Scene graph at timestep 2201 is [False, True, False, False, False, True]
State prediction error at timestep 2201 is tensor(0.0620, grad_fn=<MseLossBackward0>)
Current timestep = 2202. State = [[-0.02491215  0.2884634 ]]. Action = [[ 0.0485177  -0.08552241  0.19521493  0.35098505]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 2202 is [False, True, False, False, False, True]
Current timestep = 2203. State = [[-0.02828135  0.28547996]]. Action = [[-0.22946125 -0.22488146  0.10328865  0.8220775 ]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 2203 is [False, True, False, False, False, True]
Current timestep = 2204. State = [[-0.04013881  0.27360818]]. Action = [[-0.18993871 -0.21287239 -0.14318329  0.11855435]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 2204 is [False, True, False, False, False, True]
Current timestep = 2205. State = [[-0.05473398  0.2620652 ]]. Action = [[-0.22734314 -0.00478654  0.03454226  0.61165476]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 2205 is [False, True, False, False, False, True]
Current timestep = 2206. State = [[-0.06928801  0.26054403]]. Action = [[-0.11181709  0.1814079   0.21432865  0.26886892]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 2206 is [True, False, False, False, False, True]
Current timestep = 2207. State = [[-0.07430825  0.26261327]]. Action = [[ 0.2384187  -0.22554603 -0.15723561 -0.55322117]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 2207 is [True, False, False, False, False, True]
Current timestep = 2208. State = [[-0.06704119  0.2547569 ]]. Action = [[ 0.06698713 -0.09474832 -0.03831777 -0.689134  ]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 2208 is [True, False, False, False, False, True]
Current timestep = 2209. State = [[-0.06182909  0.24518356]]. Action = [[-0.02557075 -0.21264754  0.13738477 -0.519642  ]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 2209 is [True, False, False, False, False, True]
Current timestep = 2210. State = [[-0.06359441  0.23842785]]. Action = [[-0.15659527  0.22590077 -0.09447503  0.6644423 ]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 2210 is [True, False, False, False, False, True]
Scene graph at timestep 2210 is [True, False, False, False, False, True]
State prediction error at timestep 2210 is tensor(0.0519, grad_fn=<MseLossBackward0>)
Current timestep = 2211. State = [[-0.0700909   0.24522886]]. Action = [[-0.0415096   0.12810561  0.07348692  0.98345995]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 2211 is [True, False, False, False, False, True]
Current timestep = 2212. State = [[-0.07595379  0.25476047]]. Action = [[-0.13220581  0.16026175 -0.07831281 -0.46247208]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 2212 is [True, False, False, False, False, True]
Current timestep = 2213. State = [[-0.08096359  0.26572314]]. Action = [[0.10672572 0.1411067  0.02163094 0.19846761]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 2213 is [True, False, False, False, False, True]
Current timestep = 2214. State = [[-0.08190688  0.2739009 ]]. Action = [[-0.20543933 -0.01531634  0.06301424 -0.93090284]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 2214 is [True, False, False, False, False, True]
Scene graph at timestep 2214 is [True, False, False, False, False, True]
State prediction error at timestep 2214 is tensor(0.0632, grad_fn=<MseLossBackward0>)
Current timestep = 2215. State = [[-0.08704894  0.27489626]]. Action = [[ 0.13286656 -0.10391656  0.09776223  0.7906537 ]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 2215 is [True, False, False, False, False, True]
Scene graph at timestep 2215 is [True, False, False, False, False, True]
State prediction error at timestep 2215 is tensor(0.0566, grad_fn=<MseLossBackward0>)
Current timestep = 2216. State = [[-0.0827773   0.27467775]]. Action = [[ 0.16348058  0.19718426  0.2415559  -0.15008253]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 2216 is [True, False, False, False, False, True]
Current timestep = 2217. State = [[-0.07831322  0.28169414]]. Action = [[-0.23623045  0.06352651  0.06564862  0.6247692 ]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 2217 is [True, False, False, False, False, True]
Current timestep = 2218. State = [[-0.08667522  0.28754961]]. Action = [[-0.1447498   0.05071008 -0.01634413 -0.98727816]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 2218 is [True, False, False, False, False, True]
Current timestep = 2219. State = [[-0.0963212   0.29102916]]. Action = [[-0.07921816 -0.01171683 -0.00915565  0.13791227]]. Reward = [0.]
Curr episode timestep = 576
Scene graph at timestep 2219 is [True, False, False, False, False, True]
Current timestep = 2220. State = [[-0.10182473  0.29285938]]. Action = [[0.05524465 0.06480581 0.21910188 0.5096328 ]]. Reward = [0.]
Curr episode timestep = 577
Scene graph at timestep 2220 is [True, False, False, False, False, True]
Scene graph at timestep 2220 is [True, False, False, False, False, True]
State prediction error at timestep 2220 is tensor(0.0644, grad_fn=<MseLossBackward0>)
Current timestep = 2221. State = [[-0.10433722  0.29246923]]. Action = [[-0.2063989  -0.19060542  0.09561607 -0.7856792 ]]. Reward = [0.]
Curr episode timestep = 578
Scene graph at timestep 2221 is [True, False, False, False, False, True]
Scene graph at timestep 2221 is [True, False, False, False, False, True]
State prediction error at timestep 2221 is tensor(0.0632, grad_fn=<MseLossBackward0>)
Current timestep = 2222. State = [[-0.11384226  0.28958344]]. Action = [[-0.07865173  0.20949134 -0.07966226 -0.6143121 ]]. Reward = [0.]
Curr episode timestep = 579
Scene graph at timestep 2222 is [True, False, False, False, False, True]
Current timestep = 2223. State = [[-0.12159434  0.2974457 ]]. Action = [[-0.11376011  0.15658647 -0.06223771  0.13148558]]. Reward = [0.]
Curr episode timestep = 580
Scene graph at timestep 2223 is [True, False, False, False, False, True]
Current timestep = 2224. State = [[-0.12982103  0.30540702]]. Action = [[-0.13847911 -0.02225544  0.01397839  0.2870977 ]]. Reward = [0.]
Curr episode timestep = 581
Scene graph at timestep 2224 is [True, False, False, False, False, True]
Current timestep = 2225. State = [[-0.1394624   0.30481032]]. Action = [[-0.14344299 -0.18065967 -0.20290619 -0.58858305]]. Reward = [0.]
Curr episode timestep = 582
Scene graph at timestep 2225 is [True, False, False, False, False, True]
Scene graph at timestep 2225 is [True, False, False, False, False, True]
State prediction error at timestep 2225 is tensor(0.0704, grad_fn=<MseLossBackward0>)
Current timestep = 2226. State = [[-0.14827034  0.29852334]]. Action = [[-0.0403036   0.18357807 -0.05472088  0.7671554 ]]. Reward = [0.]
Curr episode timestep = 583
Scene graph at timestep 2226 is [True, False, False, False, False, True]
Scene graph at timestep 2226 is [True, False, False, False, False, True]
State prediction error at timestep 2226 is tensor(0.0785, grad_fn=<MseLossBackward0>)
Current timestep = 2227. State = [[-0.15276274  0.2957286 ]]. Action = [[-0.10694304 -0.01518053 -0.15146935  0.11976123]]. Reward = [0.]
Curr episode timestep = 584
Scene graph at timestep 2227 is [True, False, False, False, False, True]
Current timestep = 2228. State = [[-0.15414722  0.29195002]]. Action = [[ 0.24160677 -0.16028312  0.21840739  0.3307836 ]]. Reward = [0.]
Curr episode timestep = 585
Scene graph at timestep 2228 is [True, False, False, False, False, True]
Current timestep = 2229. State = [[-0.14580975  0.28234842]]. Action = [[ 0.05047953 -0.18998815  0.09790546  0.22956169]]. Reward = [0.]
Curr episode timestep = 586
Scene graph at timestep 2229 is [True, False, False, False, False, True]
Current timestep = 2230. State = [[-0.14201008  0.26897612]]. Action = [[-0.09839982 -0.23362833 -0.04506704 -0.10525775]]. Reward = [0.]
Curr episode timestep = 587
Scene graph at timestep 2230 is [True, False, False, False, False, True]
Scene graph at timestep 2230 is [True, False, False, False, False, True]
State prediction error at timestep 2230 is tensor(0.0567, grad_fn=<MseLossBackward0>)
Current timestep = 2231. State = [[-0.14335217  0.25702265]]. Action = [[ 0.0857628   0.029167   -0.00798753  0.34746027]]. Reward = [0.]
Curr episode timestep = 588
Scene graph at timestep 2231 is [True, False, False, False, False, True]
Current timestep = 2232. State = [[-0.13922104  0.2537168 ]]. Action = [[0.121409   0.01218536 0.13313335 0.11576641]]. Reward = [0.]
Curr episode timestep = 589
Scene graph at timestep 2232 is [True, False, False, False, False, True]
Scene graph at timestep 2232 is [True, False, False, False, False, True]
State prediction error at timestep 2232 is tensor(0.0561, grad_fn=<MseLossBackward0>)
Current timestep = 2233. State = [[-0.13610926  0.25227892]]. Action = [[-0.20446607 -0.05598946 -0.1189117   0.8135123 ]]. Reward = [0.]
Curr episode timestep = 590
Scene graph at timestep 2233 is [True, False, False, False, False, True]
Current timestep = 2234. State = [[-0.14289059  0.2519985 ]]. Action = [[-0.05165368  0.11837476 -0.21559805 -0.01213801]]. Reward = [0.]
Curr episode timestep = 591
Scene graph at timestep 2234 is [True, False, False, False, False, True]
Current timestep = 2235. State = [[-0.14668459  0.25344694]]. Action = [[ 0.03573048 -0.13940002  0.16667569  0.544976  ]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 2235 is [True, False, False, False, False, True]
Current timestep = 2236. State = [[-0.14961916  0.24764736]]. Action = [[-0.19636372 -0.12412369 -0.15376927 -0.22006178]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 2236 is [True, False, False, False, False, True]
Current timestep = 2237. State = [[-0.16074431  0.24163628]]. Action = [[-0.23085874  0.01910919  0.15123522 -0.9450005 ]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 2237 is [True, False, False, False, False, True]
Current timestep = 2238. State = [[-0.16994795  0.2362653 ]]. Action = [[ 0.18541685 -0.24215847  0.10000798  0.94519615]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 2238 is [True, False, False, False, False, True]
Scene graph at timestep 2238 is [True, False, False, False, False, True]
State prediction error at timestep 2238 is tensor(0.0545, grad_fn=<MseLossBackward0>)
Current timestep = 2239. State = [[-0.16736133  0.22727023]]. Action = [[0.00364414 0.04562575 0.20266289 0.44433236]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 2239 is [True, False, False, False, False, True]
Current timestep = 2240. State = [[-0.16707303  0.22500092]]. Action = [[-0.11566553 -0.01887219  0.09358776  0.9868734 ]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 2240 is [True, False, False, False, False, True]
Current timestep = 2241. State = [[-0.1686709   0.22749233]]. Action = [[ 0.18571037  0.22912893 -0.09925869 -0.33000588]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 2241 is [True, False, False, False, False, True]
Current timestep = 2242. State = [[-0.15956037  0.235321  ]]. Action = [[ 0.22318912 -0.02579726  0.22487149 -0.00138092]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 2242 is [True, False, False, False, False, True]
Current timestep = 2243. State = [[-0.15002662  0.24057075]]. Action = [[-0.10427424  0.19201562 -0.06818977 -0.05976117]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 2243 is [True, False, False, False, False, True]
Scene graph at timestep 2243 is [True, False, False, False, False, True]
State prediction error at timestep 2243 is tensor(0.0616, grad_fn=<MseLossBackward0>)
Current timestep = 2244. State = [[-0.14984109  0.25239235]]. Action = [[0.01298568 0.24205327 0.20026052 0.17915154]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 2244 is [True, False, False, False, False, True]
Current timestep = 2245. State = [[-0.15266989  0.26523   ]]. Action = [[-0.22049658  0.04044151 -0.06719723 -0.20072961]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 2245 is [True, False, False, False, False, True]
Scene graph at timestep 2245 is [True, False, False, False, False, True]
State prediction error at timestep 2245 is tensor(0.0642, grad_fn=<MseLossBackward0>)
Current timestep = 2246. State = [[-0.16055907  0.270111  ]]. Action = [[ 0.04237932 -0.08000097 -0.03662612  0.12186801]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 2246 is [True, False, False, False, False, True]
Current timestep = 2247. State = [[-0.16537468  0.26730123]]. Action = [[-0.21590346 -0.10632169 -0.07326934 -0.15935159]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 2247 is [True, False, False, False, False, True]
Current timestep = 2248. State = [[-0.17201914  0.2600132 ]]. Action = [[ 0.15355659 -0.17322728 -0.05013847  0.13776469]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 2248 is [True, False, False, False, False, True]
Scene graph at timestep 2248 is [True, False, False, False, False, True]
State prediction error at timestep 2248 is tensor(0.0622, grad_fn=<MseLossBackward0>)
Current timestep = 2249. State = [[-0.16993505  0.25202152]]. Action = [[-0.01835679  0.02845824  0.0595654   0.69018304]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 2249 is [True, False, False, False, False, True]
Current timestep = 2250. State = [[-0.17170157  0.24839596]]. Action = [[-0.1887177  -0.10202798 -0.01815499  0.3256961 ]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 2250 is [True, False, False, False, False, True]
Current timestep = 2251. State = [[-0.17909414  0.24386999]]. Action = [[-4.4307113e-04 -1.1532545e-02  1.5793711e-02 -9.6771663e-01]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 2251 is [True, False, False, False, False, True]
Current timestep = 2252. State = [[-0.18383142  0.24231839]]. Action = [[-0.12495822  0.03862712 -0.06581557 -0.42129135]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 2252 is [True, False, False, False, False, True]
Scene graph at timestep 2252 is [True, False, False, False, False, True]
State prediction error at timestep 2252 is tensor(0.0603, grad_fn=<MseLossBackward0>)
Current timestep = 2253. State = [[-0.1873455   0.24515764]]. Action = [[ 0.16282624  0.12880242  0.04804647 -0.6103716 ]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 2253 is [True, False, False, False, False, True]
Current timestep = 2254. State = [[-0.18266205  0.24834022]]. Action = [[ 0.00661585 -0.12922034 -0.19283265 -0.7831228 ]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 2254 is [True, False, False, False, False, True]
Scene graph at timestep 2254 is [True, False, False, False, False, True]
State prediction error at timestep 2254 is tensor(0.0614, grad_fn=<MseLossBackward0>)
Current timestep = 2255. State = [[-0.18037717  0.24354008]]. Action = [[ 0.0076389  -0.12611723  0.02229843 -0.5419403 ]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 2255 is [True, False, False, False, False, True]
Current timestep = 2256. State = [[-0.17857437  0.2339292 ]]. Action = [[ 0.05871096 -0.22620857  0.14251167  0.4746908 ]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 2256 is [True, False, False, False, False, True]
Current timestep = 2257. State = [[-0.17684333  0.22331315]]. Action = [[-0.03745915  0.01771     0.043044    0.8252821 ]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 2257 is [True, False, False, False, False, True]
Scene graph at timestep 2257 is [True, False, False, False, False, True]
State prediction error at timestep 2257 is tensor(0.0558, grad_fn=<MseLossBackward0>)
Current timestep = 2258. State = [[-0.17523487  0.22355391]]. Action = [[ 0.16004586  0.23013592 -0.0880551  -0.40211797]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 2258 is [True, False, False, False, False, True]
Scene graph at timestep 2258 is [True, False, False, False, False, True]
State prediction error at timestep 2258 is tensor(0.0592, grad_fn=<MseLossBackward0>)
Current timestep = 2259. State = [[-0.16970992  0.23163356]]. Action = [[-0.05591193  0.0250161   0.0092667   0.53453565]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 2259 is [True, False, False, False, False, True]
Current timestep = 2260. State = [[-0.16859318  0.23373994]]. Action = [[ 0.04881909 -0.13135152  0.17289388 -0.94425863]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 2260 is [True, False, False, False, False, True]
Current timestep = 2261. State = [[-0.17029122  0.23073453]]. Action = [[-0.22532336  0.04359967 -0.03185652 -0.29513443]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 2261 is [True, False, False, False, False, True]
Current timestep = 2262. State = [[-0.181221    0.22926141]]. Action = [[-0.20216401 -0.1010876  -0.11157699 -0.39516294]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 2262 is [True, False, False, False, False, True]
Current timestep = 2263. State = [[-0.19572148  0.22782862]]. Action = [[-0.20699151  0.14575332 -0.02779832  0.15763283]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 2263 is [True, False, False, False, False, True]
Current timestep = 2264. State = [[-0.20744756  0.22844183]]. Action = [[ 0.0060764  -0.22012085  0.10796744 -0.84402925]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 2264 is [True, False, False, False, False, True]
Current timestep = 2265. State = [[-0.20833153  0.21901356]]. Action = [[ 0.23359454 -0.17980193 -0.03717491  0.35346377]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 2265 is [True, False, False, False, False, True]
Current timestep = 2266. State = [[-0.1972915   0.20665845]]. Action = [[ 0.21776247 -0.1752336  -0.00595193  0.22105277]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 2266 is [True, False, False, False, False, True]
Current timestep = 2267. State = [[-0.18772204  0.1997114 ]]. Action = [[-0.13983636  0.20814127 -0.07086995 -0.44499582]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 2267 is [True, False, False, False, False, True]
Current timestep = 2268. State = [[-0.19044971  0.20049313]]. Action = [[-0.15150623 -0.22497815 -0.2280598   0.48855495]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 2268 is [True, False, False, False, False, True]
Current timestep = 2269. State = [[-0.19935551  0.19433846]]. Action = [[-0.12104161  0.02282068 -0.10113209 -0.88927567]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 2269 is [True, False, False, False, False, True]
Scene graph at timestep 2269 is [True, False, False, False, False, True]
State prediction error at timestep 2269 is tensor(0.0501, grad_fn=<MseLossBackward0>)
Current timestep = 2270. State = [[-0.20534477  0.18879206]]. Action = [[ 0.08638954 -0.23443073  0.16362786 -0.42840934]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 2270 is [True, False, False, False, False, True]
Current timestep = 2271. State = [[-0.2040694   0.17756845]]. Action = [[ 0.04747978 -0.11511883 -0.00796564 -0.330163  ]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 2271 is [True, False, False, False, False, True]
Current timestep = 2272. State = [[-0.20516409  0.17302653]]. Action = [[-0.23166735  0.22136226  0.16422606 -0.80526674]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 2272 is [True, False, False, False, False, True]
Scene graph at timestep 2272 is [True, False, False, False, False, True]
State prediction error at timestep 2272 is tensor(0.0469, grad_fn=<MseLossBackward0>)
Current timestep = 2273. State = [[-0.21482764  0.17707254]]. Action = [[-0.1326461  -0.09677076 -0.00133625  0.8087342 ]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 2273 is [True, False, False, False, False, True]
Current timestep = 2274. State = [[-0.22664858  0.17968616]]. Action = [[-0.20764801  0.24569255 -0.06344691  0.43930912]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 2274 is [True, False, False, False, False, True]
Current timestep = 2275. State = [[-0.23626311  0.18862507]]. Action = [[ 0.09664443  0.02017987  0.24318999 -0.24899554]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 2275 is [True, False, False, False, False, True]
Current timestep = 2276. State = [[-0.23773864  0.19462891]]. Action = [[-0.08264722  0.11514762 -0.17899078  0.4388218 ]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 2276 is [True, False, False, False, False, True]
Current timestep = 2277. State = [[-0.24068986  0.19783838]]. Action = [[-0.02590334 -0.16347206  0.10434997 -0.53306276]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 2277 is [True, False, False, False, False, True]
Current timestep = 2278. State = [[-0.24117006  0.19486777]]. Action = [[0.14011383 0.06825039 0.18796253 0.59412384]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 2278 is [True, False, False, False, False, True]
Current timestep = 2279. State = [[-0.23836377  0.19330733]]. Action = [[-0.15515792 -0.14619008  0.19408146 -0.96148276]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 2279 is [True, False, False, False, False, True]
Current timestep = 2280. State = [[-0.24120666  0.1857787 ]]. Action = [[ 0.09128684 -0.1606909   0.1748907  -0.3502559 ]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 2280 is [True, False, False, False, False, True]
Scene graph at timestep 2280 is [True, False, False, False, False, True]
State prediction error at timestep 2280 is tensor(0.0492, grad_fn=<MseLossBackward0>)
Current timestep = 2281. State = [[-0.23979186  0.17567363]]. Action = [[-0.01433793 -0.1240975  -0.23335342  0.7176914 ]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 2281 is [True, False, False, False, False, True]
Current timestep = 2282. State = [[-0.24306868  0.16801383]]. Action = [[-0.24566495  0.00390321 -0.00818796  0.5207255 ]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 2282 is [True, False, False, False, False, True]
Current timestep = 2283. State = [[-0.25130904  0.16267757]]. Action = [[ 0.08334973 -0.17405432 -0.07371843  0.24723577]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 2283 is [True, False, False, False, False, True]
Current timestep = 2284. State = [[-0.25261277  0.15598327]]. Action = [[-0.03836925  0.04510349 -0.10958275  0.85972583]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 2284 is [True, False, False, False, False, True]
Scene graph at timestep 2284 is [True, False, False, False, False, True]
State prediction error at timestep 2284 is tensor(0.0608, grad_fn=<MseLossBackward0>)
Current timestep = 2285. State = [[-0.25011796  0.15110789]]. Action = [[ 0.24693954 -0.23889555 -0.24379435 -0.8586029 ]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 2285 is [True, False, False, False, False, True]
Current timestep = 2286. State = [[-0.23892961  0.14361511]]. Action = [[ 0.1663478   0.11462468 -0.20506626 -0.29282844]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 2286 is [True, False, False, False, False, True]
Current timestep = 2287. State = [[-0.23186888  0.14599061]]. Action = [[-0.23235515  0.1070329   0.02083024 -0.04683822]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 2287 is [True, False, False, False, False, True]
Current timestep = 2288. State = [[-0.2399284   0.15390638]]. Action = [[-0.17893484  0.20044088 -0.01124397 -0.3032838 ]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 2288 is [True, False, False, False, False, True]
Current timestep = 2289. State = [[-0.25164935  0.16481264]]. Action = [[-0.14995033  0.10009965  0.03390479 -0.22781527]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 2289 is [True, False, False, False, False, True]
Current timestep = 2290. State = [[-0.25807542  0.16842553]]. Action = [[ 0.19123948 -0.23375516 -0.23945881  0.02441168]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 2290 is [True, False, False, False, False, True]
Current timestep = 2291. State = [[-0.2541352   0.16462523]]. Action = [[ 0.01189777  0.15626985 -0.03247203  0.6703582 ]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 2291 is [True, False, False, False, False, True]
Current timestep = 2292. State = [[-0.2490821  0.1700682]]. Action = [[0.17457819 0.13741809 0.09515914 0.5641866 ]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 2292 is [True, False, False, False, False, True]
Current timestep = 2293. State = [[-0.24390933  0.17555472]]. Action = [[-0.19405669 -0.07350396 -0.16778056  0.09123755]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 2293 is [True, False, False, False, False, True]
Current timestep = 2294. State = [[-0.24749884  0.17318511]]. Action = [[ 0.07592574 -0.13890694 -0.18910828  0.30464077]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 2294 is [True, False, False, False, False, True]
Current timestep = 2295. State = [[-0.24631691  0.17091437]]. Action = [[ 0.05603078  0.21141434 -0.14704242  0.880244  ]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 2295 is [True, False, False, False, False, True]
Current timestep = 2296. State = [[-0.24326093  0.17421746]]. Action = [[-0.00602092 -0.15740132  0.10962683  0.54886305]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 2296 is [True, False, False, False, False, True]
Current timestep = 2297. State = [[-0.24293052  0.17343085]]. Action = [[-0.01278532  0.16852254  0.20679414  0.6565075 ]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 2297 is [True, False, False, False, False, True]
Current timestep = 2298. State = [[-0.24200651  0.17614846]]. Action = [[ 0.06741402 -0.13202548 -0.03062701 -0.20612621]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 2298 is [True, False, False, False, False, True]
Current timestep = 2299. State = [[-0.24037541  0.16943723]]. Action = [[-0.06604242 -0.23485431 -0.24767031  0.7254354 ]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 2299 is [True, False, False, False, False, True]
Current timestep = 2300. State = [[-0.24505605  0.16266958]]. Action = [[-0.18263102  0.21590048 -0.02438702 -0.56343734]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 2300 is [True, False, False, False, False, True]
Current timestep = 2301. State = [[-0.2543005   0.16310507]]. Action = [[-0.12616163 -0.23810153 -0.19820185 -0.94860715]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 2301 is [True, False, False, False, False, True]
Scene graph at timestep 2301 is [True, False, False, False, False, True]
State prediction error at timestep 2301 is tensor(0.0462, grad_fn=<MseLossBackward0>)
Current timestep = 2302. State = [[-0.2596343   0.15232918]]. Action = [[ 0.1928632  -0.24014424  0.012346    0.76327515]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 2302 is [True, False, False, False, False, True]
Current timestep = 2303. State = [[-0.2527615  0.1413443]]. Action = [[ 0.17676955  0.06542176  0.07856849 -0.08575213]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 2303 is [True, False, False, False, False, True]
Current timestep = 2304. State = [[-0.24595334  0.14323682]]. Action = [[-0.16613357  0.24951565 -0.0098332  -0.25550914]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 2304 is [True, False, False, False, False, True]
Current timestep = 2305. State = [[-0.25216144  0.15363859]]. Action = [[-0.23371546  0.0823628   0.03101879  0.82549095]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 2305 is [True, False, False, False, False, True]
Scene graph at timestep 2305 is [True, False, False, False, False, True]
State prediction error at timestep 2305 is tensor(0.0607, grad_fn=<MseLossBackward0>)
Current timestep = 2306. State = [[-0.2599344   0.15938847]]. Action = [[ 0.20719594 -0.06113893  0.10931271 -0.7340076 ]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 2306 is [True, False, False, False, False, True]
Current timestep = 2307. State = [[-0.25912377  0.1603548 ]]. Action = [[-0.21756594  0.06215584  0.07501024  0.90922093]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 2307 is [True, False, False, False, False, True]
Current timestep = 2308. State = [[-0.26486853  0.16094208]]. Action = [[ 0.04165876 -0.10340248  0.16132075  0.59108233]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 2308 is [True, False, False, False, False, True]
Current timestep = 2309. State = [[-0.26599696  0.15802202]]. Action = [[ 0.00924852  0.00918809 -0.130037    0.43234515]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 2309 is [True, False, False, False, False, True]
Current timestep = 2310. State = [[-0.2685694   0.15651713]]. Action = [[-0.1996296  -0.03627053 -0.18430547 -0.4018578 ]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 2310 is [True, False, False, False, False, True]
Current timestep = 2311. State = [[-0.27293476  0.15594842]]. Action = [[ 0.24325949  0.06506801 -0.05300368  0.9175018 ]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 2311 is [True, False, False, False, False, True]
Current timestep = 2312. State = [[-0.26503634  0.16063385]]. Action = [[0.10546228 0.18453282 0.21107244 0.5775633 ]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 2312 is [True, False, False, False, False, True]
Current timestep = 2313. State = [[-0.25563553  0.17065029]]. Action = [[0.15981582 0.14074522 0.03144133 0.25181973]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 2313 is [True, False, False, False, False, True]
Current timestep = 2314. State = [[-0.2476286   0.17766018]]. Action = [[-0.06783949 -0.08293436  0.0208945   0.9624667 ]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 2314 is [True, False, False, False, False, True]
Current timestep = 2315. State = [[-0.24748193  0.17805113]]. Action = [[-0.01747522  0.02883297  0.12143296  0.31357586]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 2315 is [True, False, False, False, False, True]
Current timestep = 2316. State = [[-0.24756837  0.1781716 ]]. Action = [[ 0.05187169 -0.04034616 -0.19079469  0.27028203]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 2316 is [True, False, False, False, False, True]
Current timestep = 2317. State = [[-0.24725181  0.1751629 ]]. Action = [[-0.10101205 -0.11205213 -0.18762422  0.7093508 ]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 2317 is [True, False, False, False, False, True]
Current timestep = 2318. State = [[-0.2512472   0.16865937]]. Action = [[-0.0490023  -0.11341178 -0.07145427 -0.54597336]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 2318 is [True, False, False, False, False, True]
Scene graph at timestep 2318 is [True, False, False, False, False, True]
State prediction error at timestep 2318 is tensor(0.0494, grad_fn=<MseLossBackward0>)
Current timestep = 2319. State = [[-0.25564528  0.1641771 ]]. Action = [[-0.06282096  0.10303894 -0.24206999  0.40857887]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 2319 is [True, False, False, False, False, True]
Current timestep = 2320. State = [[-0.25654924  0.16805485]]. Action = [[ 0.1900017   0.14223742  0.24649787 -0.0902909 ]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 2320 is [True, False, False, False, False, True]
Current timestep = 2321. State = [[-0.24785541  0.17048979]]. Action = [[ 0.1523084  -0.23522994 -0.05549544 -0.956288  ]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 2321 is [True, False, False, False, False, True]
Current timestep = 2322. State = [[-0.2410575   0.16546167]]. Action = [[-0.10271865  0.0931026  -0.08654515 -0.31073022]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 2322 is [True, False, False, False, False, True]
Current timestep = 2323. State = [[-0.23999631  0.16704217]]. Action = [[ 0.13582885  0.04716328 -0.18388565 -0.5636757 ]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 2323 is [True, False, False, False, False, True]
Scene graph at timestep 2323 is [True, False, False, False, False, True]
State prediction error at timestep 2323 is tensor(0.0544, grad_fn=<MseLossBackward0>)
Current timestep = 2324. State = [[-0.2353525  0.1719587]]. Action = [[ 0.00706986  0.16743672  0.22738087 -0.7332104 ]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 2324 is [True, False, False, False, False, True]
Current timestep = 2325. State = [[-0.23000385  0.18242122]]. Action = [[ 0.22609061  0.2042405  -0.10420722  0.85945606]]. Reward = [0.]
Curr episode timestep = 682
Scene graph at timestep 2325 is [True, False, False, False, False, True]
Current timestep = 2326. State = [[-0.22075288  0.19367051]]. Action = [[-0.02703768  0.05067876 -0.13890284  0.9154525 ]]. Reward = [0.]
Curr episode timestep = 683
Scene graph at timestep 2326 is [True, False, False, False, False, True]
Current timestep = 2327. State = [[-0.21797459  0.19892171]]. Action = [[ 0.00393504 -0.03840724 -0.02895728 -0.02405816]]. Reward = [0.]
Curr episode timestep = 684
Scene graph at timestep 2327 is [True, False, False, False, False, True]
Current timestep = 2328. State = [[-0.21606176  0.20279275]]. Action = [[0.09584612 0.21481726 0.17910504 0.36813188]]. Reward = [0.]
Curr episode timestep = 685
Scene graph at timestep 2328 is [True, False, False, False, False, True]
Current timestep = 2329. State = [[-0.21276456  0.21187788]]. Action = [[-0.05629234  0.06167993 -0.02488798  0.8946967 ]]. Reward = [0.]
Curr episode timestep = 686
Scene graph at timestep 2329 is [True, False, False, False, False, True]
Current timestep = 2330. State = [[-0.21149878  0.22107571]]. Action = [[ 0.14379922  0.23150593 -0.09545103  0.01557839]]. Reward = [0.]
Curr episode timestep = 687
Scene graph at timestep 2330 is [True, False, False, False, False, True]
Scene graph at timestep 2330 is [True, False, False, False, False, True]
State prediction error at timestep 2330 is tensor(0.0699, grad_fn=<MseLossBackward0>)
Current timestep = 2331. State = [[-0.20806617  0.23541994]]. Action = [[-0.12079172  0.23531657 -0.09722969 -0.47708952]]. Reward = [0.]
Curr episode timestep = 688
Scene graph at timestep 2331 is [True, False, False, False, False, True]
Current timestep = 2332. State = [[-0.21201442  0.24997728]]. Action = [[-0.09986383  0.10064024 -0.12247166 -0.5783481 ]]. Reward = [0.]
Curr episode timestep = 689
Scene graph at timestep 2332 is [True, False, False, False, False, True]
Current timestep = 2333. State = [[-0.21936621  0.2559059 ]]. Action = [[-0.16912705 -0.16483572 -0.18244919 -0.8676255 ]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 2333 is [True, False, False, False, False, True]
Current timestep = 2334. State = [[-0.22763555  0.2558258 ]]. Action = [[ 0.04685166  0.20774215  0.14013147 -0.4082569 ]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 2334 is [True, False, False, False, False, True]
Current timestep = 2335. State = [[-0.22735311  0.2595557 ]]. Action = [[ 0.08854002 -0.16244146  0.08858198 -0.22705168]]. Reward = [0.]
Curr episode timestep = 692
Scene graph at timestep 2335 is [True, False, False, False, False, True]
Current timestep = 2336. State = [[-0.22345172  0.25496396]]. Action = [[ 0.04615736 -0.06571606 -0.07866368 -0.51416993]]. Reward = [0.]
Curr episode timestep = 693
Scene graph at timestep 2336 is [True, False, False, False, False, True]
Current timestep = 2337. State = [[-0.21986704  0.24957322]]. Action = [[ 0.03219372 -0.08363906  0.1823917  -0.9002783 ]]. Reward = [0.]
Curr episode timestep = 694
Scene graph at timestep 2337 is [True, False, False, False, False, True]
Current timestep = 2338. State = [[-0.21923022  0.24730645]]. Action = [[-0.10648592  0.15093821  0.17301333 -0.4977548 ]]. Reward = [0.]
Curr episode timestep = 695
Scene graph at timestep 2338 is [True, False, False, False, False, True]
Current timestep = 2339. State = [[-0.22043553  0.2476751 ]]. Action = [[ 0.12476492 -0.2335342   0.04792124 -0.92426467]]. Reward = [0.]
Curr episode timestep = 696
Scene graph at timestep 2339 is [True, False, False, False, False, True]
Current timestep = 2340. State = [[-0.21792646  0.23957261]]. Action = [[-0.05289616 -0.05990824 -0.24582559 -0.43981493]]. Reward = [0.]
Curr episode timestep = 697
Scene graph at timestep 2340 is [True, False, False, False, False, True]
Current timestep = 2341. State = [[-0.21922678  0.23730496]]. Action = [[-0.03564689  0.18451005 -0.04347184  0.70138836]]. Reward = [0.]
Curr episode timestep = 698
Scene graph at timestep 2341 is [True, False, False, False, False, True]
Current timestep = 2342. State = [[-0.21836011  0.24571487]]. Action = [[ 0.1826002   0.20530105  0.14157876 -0.80607766]]. Reward = [0.]
Curr episode timestep = 699
Scene graph at timestep 2342 is [True, False, False, False, False, True]
Scene graph at timestep 2342 is [True, False, False, False, False, True]
State prediction error at timestep 2342 is tensor(0.0671, grad_fn=<MseLossBackward0>)
Current timestep = 2343. State = [[-0.20891681  0.2579746 ]]. Action = [[ 0.20216691  0.14776582  0.07578662 -0.8053168 ]]. Reward = [0.]
Curr episode timestep = 700
Scene graph at timestep 2343 is [True, False, False, False, False, True]
Scene graph at timestep 2343 is [True, False, False, False, False, True]
State prediction error at timestep 2343 is tensor(0.0694, grad_fn=<MseLossBackward0>)
Current timestep = 2344. State = [[-0.19690244  0.2676228 ]]. Action = [[ 0.09294152  0.02023959  0.16619843 -0.79864454]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 2344 is [True, False, False, False, False, True]
Current timestep = 2345. State = [[-0.18659689  0.27416927]]. Action = [[ 0.2003184   0.15340734 -0.24908297  0.36159086]]. Reward = [0.]
Curr episode timestep = 702
Scene graph at timestep 2345 is [True, False, False, False, False, True]
Current timestep = 2346. State = [[-0.17885107  0.28344616]]. Action = [[-0.18399     0.13790694 -0.05356441 -0.4256835 ]]. Reward = [0.]
Curr episode timestep = 703
Scene graph at timestep 2346 is [True, False, False, False, False, True]
Scene graph at timestep 2346 is [True, False, False, False, False, True]
State prediction error at timestep 2346 is tensor(0.0747, grad_fn=<MseLossBackward0>)
Current timestep = 2347. State = [[-0.18403322  0.28774023]]. Action = [[-0.14988051 -0.23505762  0.16208765  0.22681534]]. Reward = [0.]
Curr episode timestep = 704
Scene graph at timestep 2347 is [True, False, False, False, False, True]
Current timestep = 2348. State = [[-0.19040413  0.27780637]]. Action = [[ 0.09557581 -0.24632217 -0.114043   -0.1072644 ]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 2348 is [True, False, False, False, False, True]
Scene graph at timestep 2348 is [True, False, False, False, False, True]
State prediction error at timestep 2348 is tensor(0.0681, grad_fn=<MseLossBackward0>)
Current timestep = 2349. State = [[-0.1911136   0.26897392]]. Action = [[-0.07560678  0.20227224 -0.09144738  0.9980638 ]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 2349 is [True, False, False, False, False, True]
Current timestep = 2350. State = [[-0.19329959  0.2738601 ]]. Action = [[-0.00561161  0.10654181  0.18985283  0.826308  ]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 2350 is [True, False, False, False, False, True]
Current timestep = 2351. State = [[-0.19570208  0.28060716]]. Action = [[-0.09440324  0.06818974 -0.07846402 -0.9647839 ]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 2351 is [True, False, False, False, False, True]
Scene graph at timestep 2351 is [True, False, False, False, False, True]
State prediction error at timestep 2351 is tensor(0.0780, grad_fn=<MseLossBackward0>)
Current timestep = 2352. State = [[-0.19662896  0.28645664]]. Action = [[ 0.21877185  0.07306075 -0.00585084  0.21247113]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 2352 is [True, False, False, False, False, True]
Current timestep = 2353. State = [[-0.18988018  0.29074684]]. Action = [[-0.02191244 -0.00532462 -0.00256041 -0.11832917]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 2353 is [True, False, False, False, False, True]
Current timestep = 2354. State = [[-0.18439116  0.29496965]]. Action = [[ 0.24767017  0.17786208 -0.03664997  0.6701782 ]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 2354 is [True, False, False, False, False, True]
Current timestep = 2355. State = [[-0.1734594   0.29996002]]. Action = [[ 0.03365889 -0.13396524 -0.06886744  0.806762  ]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 2355 is [True, False, False, False, False, True]
Current timestep = 2356. State = [[-0.17017826  0.30089572]]. Action = [[-0.12287702  0.20465901 -0.16710658  0.4747938 ]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 2356 is [True, False, False, False, False, True]
Current timestep = 2357. State = [[-0.17318699  0.30757037]]. Action = [[0.1542368  0.23501372 0.22734207 0.9730718 ]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 2357 is [True, False, False, False, False, True]
Current timestep = 2358. State = [[-0.17424923  0.3069373 ]]. Action = [[ 0.01311672 -0.20968957 -0.12222417  0.8115201 ]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 2358 is [True, False, False, False, False, True]
Current timestep = 2359. State = [[-0.17333601  0.30068594]]. Action = [[ 0.07901359  0.05657119  0.17230195 -0.34305614]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 2359 is [True, False, False, False, False, True]
Scene graph at timestep 2359 is [True, False, False, False, False, True]
State prediction error at timestep 2359 is tensor(0.0734, grad_fn=<MseLossBackward0>)
Current timestep = 2360. State = [[-0.17011596  0.29994732]]. Action = [[0.14439738 0.2409944  0.24744254 0.5247146 ]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 2360 is [True, False, False, False, False, True]
Current timestep = 2361. State = [[-0.16961278  0.29699793]]. Action = [[-0.06670144 -0.18574288 -0.19401874  0.8307245 ]]. Reward = [0.]
Curr episode timestep = 718
Scene graph at timestep 2361 is [True, False, False, False, False, True]
Current timestep = 2362. State = [[-0.17478338  0.2932062 ]]. Action = [[-0.17732224  0.20742702  0.13682109 -0.4493041 ]]. Reward = [0.]
Curr episode timestep = 719
Scene graph at timestep 2362 is [True, False, False, False, False, True]
Current timestep = 2363. State = [[-0.18563837  0.29868367]]. Action = [[-0.21759886  0.02455318  0.04098451 -0.18475449]]. Reward = [0.]
Curr episode timestep = 720
Scene graph at timestep 2363 is [True, False, False, False, False, True]
Scene graph at timestep 2363 is [True, False, False, False, False, True]
State prediction error at timestep 2363 is tensor(0.0771, grad_fn=<MseLossBackward0>)
Current timestep = 2364. State = [[-0.19810995  0.30175167]]. Action = [[-0.06911975 -0.00384723  0.22086617 -0.5570759 ]]. Reward = [0.]
Curr episode timestep = 721
Scene graph at timestep 2364 is [True, False, False, False, False, True]
Current timestep = 2365. State = [[-0.20711857  0.30195996]]. Action = [[-0.14624482 -0.04414317  0.15418625  0.69322205]]. Reward = [0.]
Curr episode timestep = 722
Scene graph at timestep 2365 is [True, False, False, False, False, True]
Scene graph at timestep 2365 is [True, False, False, False, False, True]
State prediction error at timestep 2365 is tensor(0.0829, grad_fn=<MseLossBackward0>)
Current timestep = 2366. State = [[-0.21403596  0.29900065]]. Action = [[ 0.07209861 -0.09955493 -0.05959783 -0.844432  ]]. Reward = [0.]
Curr episode timestep = 723
Scene graph at timestep 2366 is [True, False, False, False, False, True]
Scene graph at timestep 2366 is [True, False, False, False, False, True]
State prediction error at timestep 2366 is tensor(0.0794, grad_fn=<MseLossBackward0>)
Current timestep = 2367. State = [[-0.21527047  0.29825822]]. Action = [[-0.05915681  0.23064232  0.11422363 -0.8611882 ]]. Reward = [0.]
Curr episode timestep = 724
Scene graph at timestep 2367 is [True, False, False, False, False, True]
Scene graph at timestep 2367 is [True, False, False, False, False, True]
State prediction error at timestep 2367 is tensor(0.0852, grad_fn=<MseLossBackward0>)
Current timestep = 2368. State = [[-0.21653888  0.3089938 ]]. Action = [[ 0.05035162  0.22964913 -0.2164213   0.6071397 ]]. Reward = [0.]
Curr episode timestep = 725
Scene graph at timestep 2368 is [True, False, False, False, False, True]
Current timestep = 2369. State = [[-0.21666019  0.319696  ]]. Action = [[-0.11180715 -0.06043968  0.08013242 -0.94164586]]. Reward = [0.]
Curr episode timestep = 726
Scene graph at timestep 2369 is [True, False, False, False, False, True]
Current timestep = 2370. State = [[-0.22051029  0.32174236]]. Action = [[ 0.11132485 -0.00742736 -0.09593497  0.9962249 ]]. Reward = [0.]
Curr episode timestep = 727
Scene graph at timestep 2370 is [True, False, False, False, False, True]
Scene graph at timestep 2370 is [True, False, False, False, False, True]
State prediction error at timestep 2370 is tensor(0.0967, grad_fn=<MseLossBackward0>)
Current timestep = 2371. State = [[-0.22212651  0.32214206]]. Action = [[ 0.06896293 -0.04562828 -0.18148628 -0.6632991 ]]. Reward = [0.]
Curr episode timestep = 728
Scene graph at timestep 2371 is [True, False, False, False, False, True]
Current timestep = 2372. State = [[-0.22339223  0.31996733]]. Action = [[-0.06651977 -0.14613466 -0.21283558 -0.5220115 ]]. Reward = [0.]
Curr episode timestep = 729
Scene graph at timestep 2372 is [True, False, False, False, False, True]
Scene graph at timestep 2372 is [True, False, False, False, False, True]
State prediction error at timestep 2372 is tensor(0.0902, grad_fn=<MseLossBackward0>)
Current timestep = 2373. State = [[-0.22426763  0.31299376]]. Action = [[ 0.13547063 -0.09255204  0.13128158 -0.7863721 ]]. Reward = [0.]
Curr episode timestep = 730
Scene graph at timestep 2373 is [True, False, False, False, False, True]
Current timestep = 2374. State = [[-0.22185852  0.3062633 ]]. Action = [[-0.12255123 -0.0657859   0.05867183  0.3908074 ]]. Reward = [0.]
Curr episode timestep = 731
Scene graph at timestep 2374 is [True, False, False, False, False, True]
Current timestep = 2375. State = [[-0.22490165  0.30176735]]. Action = [[-0.07870132  0.23204064  0.10750949 -0.31398165]]. Reward = [0.]
Curr episode timestep = 732
Scene graph at timestep 2375 is [True, False, False, False, False, True]
Scene graph at timestep 2375 is [True, False, False, False, False, True]
State prediction error at timestep 2375 is tensor(0.0894, grad_fn=<MseLossBackward0>)
Current timestep = 2376. State = [[-0.22679044  0.2969239 ]]. Action = [[-0.05015846 -0.20582652 -0.15783621  0.25026262]]. Reward = [0.]
Curr episode timestep = 733
Scene graph at timestep 2376 is [True, False, False, False, False, True]
Current timestep = 2377. State = [[-0.22706982  0.29226002]]. Action = [[ 0.17672145  0.23280567 -0.09263673  0.46287155]]. Reward = [0.]
Curr episode timestep = 734
Scene graph at timestep 2377 is [True, False, False, False, False, True]
Scene graph at timestep 2377 is [True, False, False, False, False, True]
State prediction error at timestep 2377 is tensor(0.0953, grad_fn=<MseLossBackward0>)
Current timestep = 2378. State = [[-0.21971947  0.30081162]]. Action = [[ 0.08774984  0.18864915 -0.11319911 -0.14438248]]. Reward = [0.]
Curr episode timestep = 735
Scene graph at timestep 2378 is [True, False, False, False, False, True]
Current timestep = 2379. State = [[-0.21628982  0.30975276]]. Action = [[-0.19571032 -0.04753849 -0.12655188  0.8158958 ]]. Reward = [0.]
Curr episode timestep = 736
Scene graph at timestep 2379 is [True, False, False, False, False, True]
Current timestep = 2380. State = [[-0.21903881  0.3102403 ]]. Action = [[ 0.19316435 -0.08476269  0.08714807  0.6683686 ]]. Reward = [0.]
Curr episode timestep = 737
Scene graph at timestep 2380 is [True, False, False, False, False, True]
Current timestep = 2381. State = [[-0.21355267  0.30455685]]. Action = [[ 0.025848   -0.1773863  -0.22655687 -0.8518116 ]]. Reward = [0.]
Curr episode timestep = 738
Scene graph at timestep 2381 is [True, False, False, False, False, True]
Scene graph at timestep 2381 is [True, False, False, False, False, True]
State prediction error at timestep 2381 is tensor(0.0834, grad_fn=<MseLossBackward0>)
Current timestep = 2382. State = [[-0.21185073  0.29442424]]. Action = [[-0.10429969 -0.14515945  0.09600011 -0.24271488]]. Reward = [0.]
Curr episode timestep = 739
Scene graph at timestep 2382 is [True, False, False, False, False, True]
Current timestep = 2383. State = [[-0.21846046  0.28382036]]. Action = [[-0.24564411 -0.13560204 -0.13558431  0.46485102]]. Reward = [0.]
Curr episode timestep = 740
Scene graph at timestep 2383 is [True, False, False, False, False, True]
Current timestep = 2384. State = [[-0.23245247  0.277062  ]]. Action = [[-0.19598618  0.09535128  0.05828568  0.93056357]]. Reward = [0.]
Curr episode timestep = 741
Scene graph at timestep 2384 is [True, False, False, False, False, True]
Scene graph at timestep 2384 is [True, False, False, False, False, True]
State prediction error at timestep 2384 is tensor(0.0872, grad_fn=<MseLossBackward0>)
Current timestep = 2385. State = [[-0.24721126  0.28095046]]. Action = [[-0.19780849  0.20949805 -0.18134604  0.55188465]]. Reward = [0.]
Curr episode timestep = 742
Scene graph at timestep 2385 is [True, False, False, False, False, True]
Scene graph at timestep 2385 is [True, False, False, False, False, True]
State prediction error at timestep 2385 is tensor(0.0980, grad_fn=<MseLossBackward0>)
Current timestep = 2386. State = [[-0.25759658  0.28591523]]. Action = [[ 0.07617521 -0.22045292  0.18968785 -0.33770943]]. Reward = [0.]
Curr episode timestep = 743
Scene graph at timestep 2386 is [True, False, False, False, False, True]
Current timestep = 2387. State = [[-0.26283664  0.2844071 ]]. Action = [[-0.24704947  0.22511995 -0.1864021   0.64115834]]. Reward = [0.]
Curr episode timestep = 744
Scene graph at timestep 2387 is [True, False, False, False, False, True]
Scene graph at timestep 2387 is [True, False, False, False, False, True]
State prediction error at timestep 2387 is tensor(0.1038, grad_fn=<MseLossBackward0>)
Current timestep = 2388. State = [[-0.26887825  0.2880538 ]]. Action = [[ 0.22637519 -0.16904704 -0.23128726  0.60220385]]. Reward = [0.]
Curr episode timestep = 745
Scene graph at timestep 2388 is [True, False, False, False, False, True]
Current timestep = 2389. State = [[-0.2634385   0.28684312]]. Action = [[ 0.04859254  0.16659     0.02994999 -0.5176612 ]]. Reward = [0.]
Curr episode timestep = 746
Scene graph at timestep 2389 is [True, False, False, False, False, True]
Current timestep = 2390. State = [[-0.25841126  0.29219452]]. Action = [[ 0.03976396  0.04125124  0.21307623 -0.09305209]]. Reward = [0.]
Curr episode timestep = 747
Scene graph at timestep 2390 is [True, False, False, False, False, True]
Current timestep = 2391. State = [[-0.25237775  0.2954469 ]]. Action = [[ 0.20188788 -0.01693915  0.21844608  0.5170171 ]]. Reward = [0.]
Curr episode timestep = 748
Scene graph at timestep 2391 is [True, False, False, False, False, True]
Current timestep = 2392. State = [[-0.24422789  0.29525486]]. Action = [[-0.05468687 -0.0479667   0.22278664 -0.48815215]]. Reward = [0.]
Curr episode timestep = 749
Scene graph at timestep 2392 is [True, False, False, False, False, True]
Current timestep = 2393. State = [[-0.24443227  0.29397413]]. Action = [[-0.09748948  0.02223122  0.14781395  0.05336082]]. Reward = [0.]
Curr episode timestep = 750
Scene graph at timestep 2393 is [True, False, False, False, False, True]
Current timestep = 2394. State = [[-0.24964887  0.29552034]]. Action = [[-0.10039166  0.08810917  0.09937352  0.5789099 ]]. Reward = [0.]
Curr episode timestep = 751
Scene graph at timestep 2394 is [True, False, False, False, False, True]
Current timestep = 2395. State = [[-0.25547466  0.29503846]]. Action = [[-0.05132762 -0.24957593  0.01211381  0.64217937]]. Reward = [0.]
Curr episode timestep = 752
Scene graph at timestep 2395 is [True, False, False, False, False, True]
Current timestep = 2396. State = [[-0.25726688  0.2847307 ]]. Action = [[ 0.16660911 -0.13608389 -0.21244048 -0.09744024]]. Reward = [0.]
Curr episode timestep = 753
Scene graph at timestep 2396 is [True, False, False, False, False, True]
Current timestep = 2397. State = [[-0.2514979   0.27315256]]. Action = [[ 0.04544154 -0.19704965  0.00867325  0.91097915]]. Reward = [0.]
Curr episode timestep = 754
Scene graph at timestep 2397 is [True, False, False, False, False, True]
Current timestep = 2398. State = [[-0.24974525  0.26100546]]. Action = [[-0.1485211  -0.10761057 -0.10385805  0.93664956]]. Reward = [0.]
Curr episode timestep = 755
Scene graph at timestep 2398 is [True, False, False, False, False, True]
Current timestep = 2399. State = [[-0.25549996  0.2531587 ]]. Action = [[-0.08451685  0.00279057  0.2333045  -0.5151357 ]]. Reward = [0.]
Curr episode timestep = 756
Scene graph at timestep 2399 is [True, False, False, False, False, True]
Current timestep = 2400. State = [[-0.26028416  0.24828976]]. Action = [[ 0.01846823 -0.13720421  0.05398649 -0.9280269 ]]. Reward = [0.]
Curr episode timestep = 757
Scene graph at timestep 2400 is [True, False, False, False, False, True]
Current timestep = 2401. State = [[-0.25879773  0.24370642]]. Action = [[0.19754347 0.09253824 0.16070354 0.84771323]]. Reward = [0.]
Curr episode timestep = 758
Scene graph at timestep 2401 is [True, False, False, False, False, True]
Current timestep = 2402. State = [[-0.24985023  0.24829228]]. Action = [[ 0.10612661  0.21522969  0.10906234 -0.5855863 ]]. Reward = [0.]
Curr episode timestep = 759
Scene graph at timestep 2402 is [True, False, False, False, False, True]
Current timestep = 2403. State = [[-0.2429854  0.2544133]]. Action = [[-0.05039893 -0.17579845 -0.08365074  0.25223148]]. Reward = [0.]
Curr episode timestep = 760
Scene graph at timestep 2403 is [True, False, False, False, False, True]
Current timestep = 2404. State = [[-0.24524038  0.24912414]]. Action = [[-0.1915475  -0.13381572  0.2252714  -0.68146443]]. Reward = [0.]
Curr episode timestep = 761
Scene graph at timestep 2404 is [True, False, False, False, False, True]
Scene graph at timestep 2404 is [True, False, False, False, False, True]
State prediction error at timestep 2404 is tensor(0.0620, grad_fn=<MseLossBackward0>)
Current timestep = 2405. State = [[-0.25174657  0.24137774]]. Action = [[ 0.0868085  -0.07003471  0.09120709  0.01447785]]. Reward = [0.]
Curr episode timestep = 762
Scene graph at timestep 2405 is [True, False, False, False, False, True]
Scene graph at timestep 2405 is [True, False, False, False, False, True]
State prediction error at timestep 2405 is tensor(0.0733, grad_fn=<MseLossBackward0>)
Current timestep = 2406. State = [[-0.25389412  0.23238714]]. Action = [[-0.18539475 -0.2420325  -0.21766344  0.726493  ]]. Reward = [0.]
Curr episode timestep = 763
Scene graph at timestep 2406 is [True, False, False, False, False, True]
Scene graph at timestep 2406 is [True, False, False, False, False, True]
State prediction error at timestep 2406 is tensor(0.0761, grad_fn=<MseLossBackward0>)
Current timestep = 2407. State = [[-0.25905162  0.22213282]]. Action = [[ 0.13369513  0.04365724 -0.24785627  0.21035254]]. Reward = [0.]
Curr episode timestep = 764
Scene graph at timestep 2407 is [True, False, False, False, False, True]
Current timestep = 2408. State = [[-0.25329262  0.22218208]]. Action = [[0.23026031 0.16570044 0.04923713 0.8668808 ]]. Reward = [0.]
Curr episode timestep = 765
Scene graph at timestep 2408 is [True, False, False, False, False, True]
Scene graph at timestep 2408 is [True, False, False, False, False, True]
State prediction error at timestep 2408 is tensor(0.0745, grad_fn=<MseLossBackward0>)
Current timestep = 2409. State = [[-0.24637926  0.23154171]]. Action = [[-0.24089181  0.23496127  0.22040606  0.9897475 ]]. Reward = [0.]
Curr episode timestep = 766
Scene graph at timestep 2409 is [True, False, False, False, False, True]
Current timestep = 2410. State = [[-0.2506396   0.24490981]]. Action = [[ 0.0772064   0.13266581  0.04409504 -0.5279586 ]]. Reward = [0.]
Curr episode timestep = 767
Scene graph at timestep 2410 is [True, False, False, False, False, True]
Current timestep = 2411. State = [[-0.24667305  0.25259966]]. Action = [[ 0.23404956 -0.09372489  0.04839012  0.72778773]]. Reward = [0.]
Curr episode timestep = 768
Scene graph at timestep 2411 is [True, False, False, False, False, True]
Current timestep = 2412. State = [[-0.23730619  0.24966411]]. Action = [[-0.01904537 -0.16531244 -0.17414479  0.17423868]]. Reward = [0.]
Curr episode timestep = 769
Scene graph at timestep 2412 is [True, False, False, False, False, True]
Scene graph at timestep 2412 is [True, False, False, False, False, True]
State prediction error at timestep 2412 is tensor(0.0755, grad_fn=<MseLossBackward0>)
Current timestep = 2413. State = [[-0.23509371  0.24059959]]. Action = [[-0.05129877 -0.15699078  0.06844658 -0.72144675]]. Reward = [0.]
Curr episode timestep = 770
Scene graph at timestep 2413 is [True, False, False, False, False, True]
Current timestep = 2414. State = [[-0.23433842  0.23398066]]. Action = [[ 0.15910265  0.12151679 -0.22065808 -0.25697386]]. Reward = [0.]
Curr episode timestep = 771
Scene graph at timestep 2414 is [True, False, False, False, False, True]
Scene graph at timestep 2414 is [True, False, False, False, False, True]
State prediction error at timestep 2414 is tensor(0.0755, grad_fn=<MseLossBackward0>)
Current timestep = 2415. State = [[-0.2321159   0.23754255]]. Action = [[-0.24231414  0.13535565 -0.04561678 -0.60349345]]. Reward = [0.]
Curr episode timestep = 772
Scene graph at timestep 2415 is [True, False, False, False, False, True]
Current timestep = 2416. State = [[-0.2386723   0.24724412]]. Action = [[ 0.04692256  0.23422265  0.08358333 -0.9196693 ]]. Reward = [0.]
Curr episode timestep = 773
Scene graph at timestep 2416 is [True, False, False, False, False, True]
Current timestep = 2417. State = [[-0.23636495  0.25991982]]. Action = [[ 0.24269655  0.09953871 -0.20430411 -0.02761453]]. Reward = [0.]
Curr episode timestep = 774
Scene graph at timestep 2417 is [True, False, False, False, False, True]
Current timestep = 2418. State = [[-0.22816491  0.2641857 ]]. Action = [[-0.10131939 -0.2333754   0.0378736  -0.17537403]]. Reward = [0.]
Curr episode timestep = 775
Scene graph at timestep 2418 is [True, False, False, False, False, True]
Current timestep = 2419. State = [[-0.22888914  0.25584885]]. Action = [[-0.02607498 -0.14854954 -0.10343999 -0.97794604]]. Reward = [0.]
Curr episode timestep = 776
Scene graph at timestep 2419 is [True, False, False, False, False, True]
Current timestep = 2420. State = [[-0.23309632  0.24727327]]. Action = [[-0.17234598 -0.01991174 -0.07327211  0.6896211 ]]. Reward = [0.]
Curr episode timestep = 777
Scene graph at timestep 2420 is [True, False, False, False, False, True]
Current timestep = 2421. State = [[-0.24173915  0.24490814]]. Action = [[-0.0875417   0.09143832 -0.03417209  0.5412648 ]]. Reward = [0.]
Curr episode timestep = 778
Scene graph at timestep 2421 is [True, False, False, False, False, True]
Scene graph at timestep 2421 is [True, False, False, False, False, True]
State prediction error at timestep 2421 is tensor(0.0796, grad_fn=<MseLossBackward0>)
Current timestep = 2422. State = [[-0.24808584  0.24859229]]. Action = [[-0.00981182  0.09415346 -0.01647952 -0.6353849 ]]. Reward = [0.]
Curr episode timestep = 779
Scene graph at timestep 2422 is [True, False, False, False, False, True]
Current timestep = 2423. State = [[-0.25138578  0.256804  ]]. Action = [[-0.04590701  0.232059    0.21157697  0.8649193 ]]. Reward = [0.]
Curr episode timestep = 780
Scene graph at timestep 2423 is [True, False, False, False, False, True]
Current timestep = 2424. State = [[-0.25221843  0.26762453]]. Action = [[ 0.10585153  0.01921776  0.12383682 -0.84886336]]. Reward = [0.]
Curr episode timestep = 781
Scene graph at timestep 2424 is [True, False, False, False, False, True]
Scene graph at timestep 2424 is [True, False, False, False, False, True]
State prediction error at timestep 2424 is tensor(0.0743, grad_fn=<MseLossBackward0>)
Current timestep = 2425. State = [[-0.2463461  0.2739009]]. Action = [[ 0.18771249  0.10202286  0.23742914 -0.11141467]]. Reward = [0.]
Curr episode timestep = 782
Scene graph at timestep 2425 is [True, False, False, False, False, True]
Current timestep = 2426. State = [[-0.23904812  0.27883208]]. Action = [[-0.10825735 -0.02888757  0.00306299 -0.26961815]]. Reward = [0.]
Curr episode timestep = 783
Scene graph at timestep 2426 is [True, False, False, False, False, True]
Scene graph at timestep 2426 is [True, False, False, False, False, True]
State prediction error at timestep 2426 is tensor(0.0791, grad_fn=<MseLossBackward0>)
Current timestep = 2427. State = [[-0.23966253  0.2791653 ]]. Action = [[ 0.01975262 -0.03862232  0.08283123  0.56721187]]. Reward = [0.]
Curr episode timestep = 784
Scene graph at timestep 2427 is [True, False, False, False, False, True]
Scene graph at timestep 2427 is [True, False, False, False, False, True]
State prediction error at timestep 2427 is tensor(0.0836, grad_fn=<MseLossBackward0>)
Current timestep = 2428. State = [[-0.23672232  0.2778519 ]]. Action = [[ 0.20825094  0.00165188 -0.13391052  0.7181016 ]]. Reward = [0.]
Curr episode timestep = 785
Scene graph at timestep 2428 is [True, False, False, False, False, True]
Current timestep = 2429. State = [[-0.22528648  0.27346033]]. Action = [[ 0.21013236 -0.24492186 -0.01060797 -0.12211442]]. Reward = [0.]
Curr episode timestep = 786
Scene graph at timestep 2429 is [True, False, False, False, False, True]
Current timestep = 2430. State = [[-0.2129502  0.2611803]]. Action = [[ 0.06986585 -0.18021186 -0.1485025  -0.407161  ]]. Reward = [0.]
Curr episode timestep = 787
Scene graph at timestep 2430 is [True, False, False, False, False, True]
Current timestep = 2431. State = [[-0.20966731  0.25204363]]. Action = [[-0.23547901  0.08315042 -0.17976277 -0.65003383]]. Reward = [0.]
Curr episode timestep = 788
Scene graph at timestep 2431 is [True, False, False, False, False, True]
Current timestep = 2432. State = [[-0.21879695  0.25236034]]. Action = [[-0.15837827  0.05885831 -0.24831018  0.12915635]]. Reward = [0.]
Curr episode timestep = 789
Scene graph at timestep 2432 is [True, False, False, False, False, True]
Current timestep = 2433. State = [[-0.22516151  0.25644475]]. Action = [[ 0.20055652  0.11650604 -0.04508017 -0.01466256]]. Reward = [0.]
Curr episode timestep = 790
Scene graph at timestep 2433 is [True, False, False, False, False, True]
Current timestep = 2434. State = [[-0.21746698  0.26339796]]. Action = [[ 0.2205022   0.10673276 -0.14075972 -0.8985079 ]]. Reward = [0.]
Curr episode timestep = 791
Scene graph at timestep 2434 is [True, False, False, False, False, True]
Current timestep = 2435. State = [[-0.20840086  0.27065793]]. Action = [[-0.1249615   0.07535431 -0.20231283 -0.98121554]]. Reward = [0.]
Curr episode timestep = 792
Scene graph at timestep 2435 is [True, False, False, False, False, True]
Current timestep = 2436. State = [[-0.20655386  0.2775329 ]]. Action = [[ 0.18688601  0.10924834 -0.12614378 -0.40803063]]. Reward = [0.]
Curr episode timestep = 793
Scene graph at timestep 2436 is [True, False, False, False, False, True]
Scene graph at timestep 2436 is [True, False, False, False, False, True]
State prediction error at timestep 2436 is tensor(0.0785, grad_fn=<MseLossBackward0>)
Current timestep = 2437. State = [[-0.19823577  0.28430623]]. Action = [[ 0.12356797  0.05660686 -0.16411056 -0.21383548]]. Reward = [0.]
Curr episode timestep = 794
Scene graph at timestep 2437 is [True, False, False, False, False, True]
Scene graph at timestep 2437 is [True, False, False, False, False, True]
State prediction error at timestep 2437 is tensor(0.0802, grad_fn=<MseLossBackward0>)
Current timestep = 2438. State = [[-0.18867321  0.29039264]]. Action = [[0.14784276 0.11325961 0.15662143 0.23667109]]. Reward = [0.]
Curr episode timestep = 795
Scene graph at timestep 2438 is [True, False, False, False, False, True]
Current timestep = 2439. State = [[-0.17747548  0.29279044]]. Action = [[ 0.17195028 -0.21605153 -0.2125628  -0.09240472]]. Reward = [0.]
Curr episode timestep = 796
Scene graph at timestep 2439 is [True, False, False, False, False, True]
Current timestep = 2440. State = [[-0.16727358  0.28417495]]. Action = [[ 0.03598475 -0.16186841  0.08123356  0.6562693 ]]. Reward = [0.]
Curr episode timestep = 797
Scene graph at timestep 2440 is [True, False, False, False, False, True]
Current timestep = 2441. State = [[-0.16338962  0.27431634]]. Action = [[-0.0627     -0.07642233  0.08697474 -0.19696194]]. Reward = [0.]
Curr episode timestep = 798
Scene graph at timestep 2441 is [True, False, False, False, False, True]
Current timestep = 2442. State = [[-0.16623566  0.26692545]]. Action = [[-0.13526404 -0.08362803 -0.1628482  -0.7522435 ]]. Reward = [0.]
Curr episode timestep = 799
Scene graph at timestep 2442 is [True, False, False, False, False, True]
Scene graph at timestep 2442 is [True, False, False, False, False, True]
State prediction error at timestep 2442 is tensor(0.0645, grad_fn=<MseLossBackward0>)
Current timestep = 2443. State = [[-0.17329644  0.25923297]]. Action = [[-0.08781773 -0.15317927 -0.17271815 -0.12690389]]. Reward = [0.]
Curr episode timestep = 800
Scene graph at timestep 2443 is [True, False, False, False, False, True]
Current timestep = 2444. State = [[-0.1801789   0.25536987]]. Action = [[-0.05989829  0.2442368   0.05815345 -0.75348806]]. Reward = [0.]
Curr episode timestep = 801
Scene graph at timestep 2444 is [True, False, False, False, False, True]
Current timestep = 2445. State = [[-0.18143599  0.26396802]]. Action = [[ 0.20655322  0.13966516  0.17773736 -0.53446203]]. Reward = [0.]
Curr episode timestep = 802
Scene graph at timestep 2445 is [True, False, False, False, False, True]
Current timestep = 2446. State = [[-0.17534317  0.26873675]]. Action = [[-0.05807006 -0.20652734  0.01450357  0.22086906]]. Reward = [0.]
Curr episode timestep = 803
Scene graph at timestep 2446 is [True, False, False, False, False, True]
Current timestep = 2447. State = [[-0.17169341  0.26137796]]. Action = [[ 0.22180802 -0.16103514 -0.17832139  0.4907186 ]]. Reward = [0.]
Curr episode timestep = 804
Scene graph at timestep 2447 is [True, False, False, False, False, True]
Scene graph at timestep 2447 is [True, False, False, False, False, True]
State prediction error at timestep 2447 is tensor(0.0661, grad_fn=<MseLossBackward0>)
Current timestep = 2448. State = [[-0.1606806   0.25367862]]. Action = [[ 0.18637854  0.04100826 -0.03935857  0.15181231]]. Reward = [0.]
Curr episode timestep = 805
Scene graph at timestep 2448 is [True, False, False, False, False, True]
Scene graph at timestep 2448 is [True, False, False, False, False, True]
State prediction error at timestep 2448 is tensor(0.0628, grad_fn=<MseLossBackward0>)
Current timestep = 2449. State = [[-0.14790328  0.25402784]]. Action = [[0.1465925  0.12107381 0.02191699 0.200647  ]]. Reward = [0.]
Curr episode timestep = 806
Scene graph at timestep 2449 is [True, False, False, False, False, True]
Current timestep = 2450. State = [[-0.13702524  0.2594034 ]]. Action = [[ 0.08267537  0.06979385 -0.1512687   0.54398346]]. Reward = [0.]
Curr episode timestep = 807
Scene graph at timestep 2450 is [True, False, False, False, False, True]
Current timestep = 2451. State = [[-0.1322261   0.26179293]]. Action = [[-0.13685383 -0.12640727 -0.10637489 -0.6032784 ]]. Reward = [0.]
Curr episode timestep = 808
Scene graph at timestep 2451 is [True, False, False, False, False, True]
Current timestep = 2452. State = [[-0.1369156  0.2552405]]. Action = [[-0.10982695 -0.21507898  0.19252509 -0.45633996]]. Reward = [0.]
Curr episode timestep = 809
Scene graph at timestep 2452 is [True, False, False, False, False, True]
Current timestep = 2453. State = [[-0.14445311  0.24514878]]. Action = [[-0.10313025 -0.03427133  0.09993142 -0.9109053 ]]. Reward = [0.]
Curr episode timestep = 810
Scene graph at timestep 2453 is [True, False, False, False, False, True]
Scene graph at timestep 2453 is [True, False, False, False, False, True]
State prediction error at timestep 2453 is tensor(0.0534, grad_fn=<MseLossBackward0>)
Current timestep = 2454. State = [[-0.15303384  0.24299324]]. Action = [[-0.15746786  0.17871028 -0.2386538   0.60763645]]. Reward = [0.]
Curr episode timestep = 811
Scene graph at timestep 2454 is [True, False, False, False, False, True]
Current timestep = 2455. State = [[-0.16284853  0.24802245]]. Action = [[-0.11511195 -0.00596152 -0.23631606 -0.4428718 ]]. Reward = [0.]
Curr episode timestep = 812
Scene graph at timestep 2455 is [True, False, False, False, False, True]
Current timestep = 2456. State = [[-0.1731794   0.25098982]]. Action = [[-0.19164762  0.06602323  0.11510068  0.7422123 ]]. Reward = [0.]
Curr episode timestep = 813
Scene graph at timestep 2456 is [True, False, False, False, False, True]
Current timestep = 2457. State = [[-0.18113509  0.2543197 ]]. Action = [[0.1511426  0.01410314 0.02161652 0.21503532]]. Reward = [0.]
Curr episode timestep = 814
Scene graph at timestep 2457 is [True, False, False, False, False, True]
Scene graph at timestep 2457 is [True, False, False, False, False, True]
State prediction error at timestep 2457 is tensor(0.0649, grad_fn=<MseLossBackward0>)
Current timestep = 2458. State = [[-0.18194526  0.25276387]]. Action = [[-0.22261496 -0.20457448  0.15371901  0.9315195 ]]. Reward = [0.]
Curr episode timestep = 815
Scene graph at timestep 2458 is [True, False, False, False, False, True]
Current timestep = 2459. State = [[-0.19273807  0.24886641]]. Action = [[-0.21070845  0.20602119 -0.03164595  0.11611247]]. Reward = [0.]
Curr episode timestep = 816
Scene graph at timestep 2459 is [True, False, False, False, False, True]
Current timestep = 2460. State = [[-0.20589507  0.2512831 ]]. Action = [[-0.14843482 -0.15737157  0.11076778  0.1907047 ]]. Reward = [0.]
Curr episode timestep = 817
Scene graph at timestep 2460 is [True, False, False, False, False, True]
Current timestep = 2461. State = [[-0.21708117  0.245591  ]]. Action = [[-0.08401802 -0.11996379  0.20046672 -0.51455176]]. Reward = [0.]
Curr episode timestep = 818
Scene graph at timestep 2461 is [True, False, False, False, False, True]
Current timestep = 2462. State = [[-0.22248338  0.24045952]]. Action = [[ 0.10865852  0.06685269 -0.18694295  0.34980893]]. Reward = [0.]
Curr episode timestep = 819
Scene graph at timestep 2462 is [True, False, False, False, False, True]
Current timestep = 2463. State = [[-0.22245055  0.23985033]]. Action = [[-0.14724207 -0.05351233 -0.17732029  0.01640713]]. Reward = [0.]
Curr episode timestep = 820
Scene graph at timestep 2463 is [True, False, False, False, False, True]
Current timestep = 2464. State = [[-0.22502409  0.2396487 ]]. Action = [[0.15407813 0.09446639 0.24292293 0.69222116]]. Reward = [0.]
Curr episode timestep = 821
Scene graph at timestep 2464 is [True, False, False, False, False, True]
Current timestep = 2465. State = [[-0.22001831  0.24014376]]. Action = [[ 0.05114168 -0.14929113 -0.03288817  0.2591853 ]]. Reward = [0.]
Curr episode timestep = 822
Scene graph at timestep 2465 is [True, False, False, False, False, True]
Current timestep = 2466. State = [[-0.2147326   0.23734799]]. Action = [[ 0.1164543   0.10500082 -0.08416158  0.85855854]]. Reward = [0.]
Curr episode timestep = 823
Scene graph at timestep 2466 is [True, False, False, False, False, True]
Current timestep = 2467. State = [[-0.21207373  0.24238707]]. Action = [[-0.22114114  0.18474889  0.19956076 -0.491754  ]]. Reward = [0.]
Curr episode timestep = 824
Scene graph at timestep 2467 is [True, False, False, False, False, True]
Current timestep = 2468. State = [[-0.21788743  0.24846587]]. Action = [[ 0.02678838 -0.12398332  0.05813265 -0.57937807]]. Reward = [0.]
Curr episode timestep = 825
Scene graph at timestep 2468 is [True, False, False, False, False, True]
Scene graph at timestep 2468 is [True, False, False, False, False, True]
State prediction error at timestep 2468 is tensor(0.0595, grad_fn=<MseLossBackward0>)
Current timestep = 2469. State = [[-0.21731098  0.24394898]]. Action = [[ 0.16271704 -0.18028076 -0.06153549 -0.810282  ]]. Reward = [0.]
Curr episode timestep = 826
Scene graph at timestep 2469 is [True, False, False, False, False, True]
Current timestep = 2470. State = [[-0.21182095  0.23920731]]. Action = [[-0.00669231  0.1921682   0.04409876  0.8029479 ]]. Reward = [0.]
Curr episode timestep = 827
Scene graph at timestep 2470 is [True, False, False, False, False, True]
Current timestep = 2471. State = [[-0.21210559  0.2400288 ]]. Action = [[-0.198606   -0.2239651   0.17805898  0.76806307]]. Reward = [0.]
Curr episode timestep = 828
Scene graph at timestep 2471 is [True, False, False, False, False, True]
Current timestep = 2472. State = [[-0.21766053  0.23481874]]. Action = [[ 0.1322093   0.08262271 -0.22310181 -0.12055641]]. Reward = [0.]
Curr episode timestep = 829
Scene graph at timestep 2472 is [True, False, False, False, False, True]
Current timestep = 2473. State = [[-0.21536231  0.2390693 ]]. Action = [[ 0.01395836  0.24708176 -0.2338251  -0.4527114 ]]. Reward = [0.]
Curr episode timestep = 830
Scene graph at timestep 2473 is [True, False, False, False, False, True]
Current timestep = 2474. State = [[-0.2125068   0.24897388]]. Action = [[ 0.0589202   0.00857183 -0.13964245 -0.2919948 ]]. Reward = [0.]
Curr episode timestep = 831
Scene graph at timestep 2474 is [True, False, False, False, False, True]
Scene graph at timestep 2474 is [True, False, False, False, False, True]
State prediction error at timestep 2474 is tensor(0.0690, grad_fn=<MseLossBackward0>)
Current timestep = 2475. State = [[-0.21295397  0.25467956]]. Action = [[-0.23001309  0.09972933 -0.0115523   0.5029223 ]]. Reward = [0.]
Curr episode timestep = 832
Scene graph at timestep 2475 is [True, False, False, False, False, True]
Scene graph at timestep 2475 is [True, False, False, False, False, True]
State prediction error at timestep 2475 is tensor(0.0748, grad_fn=<MseLossBackward0>)
Current timestep = 2476. State = [[-0.21724041  0.25747806]]. Action = [[ 0.24180892 -0.14591357  0.08539405 -0.958554  ]]. Reward = [0.]
Curr episode timestep = 833
Scene graph at timestep 2476 is [True, False, False, False, False, True]
Scene graph at timestep 2476 is [True, False, False, False, False, True]
State prediction error at timestep 2476 is tensor(0.0659, grad_fn=<MseLossBackward0>)
Current timestep = 2477. State = [[-0.21394174  0.2518396 ]]. Action = [[-0.19832554 -0.12095048  0.01743737  0.55005765]]. Reward = [0.]
Curr episode timestep = 834
Scene graph at timestep 2477 is [True, False, False, False, False, True]
Scene graph at timestep 2477 is [True, False, False, False, False, True]
State prediction error at timestep 2477 is tensor(0.0698, grad_fn=<MseLossBackward0>)
Current timestep = 2478. State = [[-0.22085033  0.24490483]]. Action = [[-0.12889929 -0.05234204 -0.06674039  0.09132564]]. Reward = [0.]
Curr episode timestep = 835
Scene graph at timestep 2478 is [True, False, False, False, False, True]
Scene graph at timestep 2478 is [True, False, False, False, False, True]
State prediction error at timestep 2478 is tensor(0.0701, grad_fn=<MseLossBackward0>)
Current timestep = 2479. State = [[-0.2280181   0.24253045]]. Action = [[ 0.02082479  0.12265921 -0.04127149  0.82477355]]. Reward = [0.]
Curr episode timestep = 836
Scene graph at timestep 2479 is [True, False, False, False, False, True]
Current timestep = 2480. State = [[-0.22635834  0.2480906 ]]. Action = [[ 0.24322665  0.15483123 -0.08965456  0.81440556]]. Reward = [0.]
Curr episode timestep = 837
Scene graph at timestep 2480 is [True, False, False, False, False, True]
Current timestep = 2481. State = [[-0.21529089  0.25890717]]. Action = [[ 0.13957566  0.22992733 -0.01519331  0.5779135 ]]. Reward = [0.]
Curr episode timestep = 838
Scene graph at timestep 2481 is [True, False, False, False, False, True]
Current timestep = 2482. State = [[-0.20809986  0.26947993]]. Action = [[-0.15480222 -0.0594115  -0.10158901  0.12937653]]. Reward = [0.]
Curr episode timestep = 839
Scene graph at timestep 2482 is [True, False, False, False, False, True]
Current timestep = 2483. State = [[-0.20800616  0.2708089 ]]. Action = [[ 0.20385289 -0.04758301 -0.03013571  0.6552421 ]]. Reward = [0.]
Curr episode timestep = 840
Scene graph at timestep 2483 is [True, False, False, False, False, True]
Current timestep = 2484. State = [[-0.19787404  0.26571846]]. Action = [[ 0.24928805 -0.2299111  -0.23043475 -0.51828533]]. Reward = [0.]
Curr episode timestep = 841
Scene graph at timestep 2484 is [True, False, False, False, False, True]
Current timestep = 2485. State = [[-0.18336746  0.25283945]]. Action = [[ 0.14969426 -0.22206889  0.10259134 -0.00424856]]. Reward = [0.]
Curr episode timestep = 842
Scene graph at timestep 2485 is [True, False, False, False, False, True]
Current timestep = 2486. State = [[-0.17407517  0.24284475]]. Action = [[-0.0627556   0.12778473 -0.12383267  0.61279464]]. Reward = [0.]
Curr episode timestep = 843
Scene graph at timestep 2486 is [True, False, False, False, False, True]
Current timestep = 2487. State = [[-0.17123854  0.24522588]]. Action = [[ 0.10771313  0.11795008  0.09929806 -0.8316215 ]]. Reward = [0.]
Curr episode timestep = 844
Scene graph at timestep 2487 is [True, False, False, False, False, True]
Current timestep = 2488. State = [[-0.16835538  0.25261644]]. Action = [[-0.09885186  0.15122437 -0.0887565  -0.15047985]]. Reward = [0.]
Curr episode timestep = 845
Scene graph at timestep 2488 is [True, False, False, False, False, True]
Current timestep = 2489. State = [[-0.16842386  0.2625507 ]]. Action = [[ 0.14906335  0.14038342  0.06938213 -0.02655429]]. Reward = [0.]
Curr episode timestep = 846
Scene graph at timestep 2489 is [True, False, False, False, False, True]
Scene graph at timestep 2489 is [True, False, False, False, False, True]
State prediction error at timestep 2489 is tensor(0.0665, grad_fn=<MseLossBackward0>)
Current timestep = 2490. State = [[-0.16726021  0.27381703]]. Action = [[-0.2477316   0.20059395 -0.07064962  0.6685997 ]]. Reward = [0.]
Curr episode timestep = 847
Scene graph at timestep 2490 is [True, False, False, False, False, True]
Current timestep = 2491. State = [[-0.17325138  0.28673872]]. Action = [[ 0.11773598  0.14635104  0.09304592 -0.79400724]]. Reward = [0.]
Curr episode timestep = 848
Scene graph at timestep 2491 is [True, False, False, False, False, True]
Current timestep = 2492. State = [[-0.17498372  0.2967425 ]]. Action = [[-0.22163396  0.03992671  0.00818864  0.27245915]]. Reward = [0.]
Curr episode timestep = 849
Scene graph at timestep 2492 is [True, False, False, False, False, True]
Current timestep = 2493. State = [[-0.18372184  0.3008133 ]]. Action = [[-0.07238354 -0.05347221 -0.02179831  0.41795206]]. Reward = [0.]
Curr episode timestep = 850
Scene graph at timestep 2493 is [True, False, False, False, False, True]
Current timestep = 2494. State = [[-0.19090196  0.2987542 ]]. Action = [[-0.082931   -0.10285988 -0.08370671  0.5495231 ]]. Reward = [0.]
Curr episode timestep = 851
Scene graph at timestep 2494 is [True, False, False, False, False, True]
Current timestep = 2495. State = [[-0.19276603  0.29243743]]. Action = [[ 0.24017495 -0.13367012  0.0749476   0.40560603]]. Reward = [0.]
Curr episode timestep = 852
Scene graph at timestep 2495 is [True, False, False, False, False, True]
Scene graph at timestep 2495 is [True, False, False, False, False, True]
State prediction error at timestep 2495 is tensor(0.0746, grad_fn=<MseLossBackward0>)
Current timestep = 2496. State = [[-0.18446508  0.2838184 ]]. Action = [[ 0.0557344  -0.11808242 -0.06068747  0.06957185]]. Reward = [0.]
Curr episode timestep = 853
Scene graph at timestep 2496 is [True, False, False, False, False, True]
Current timestep = 2497. State = [[-0.18023518  0.27431288]]. Action = [[-0.08419532 -0.16430205 -0.11443897 -0.53409684]]. Reward = [0.]
Curr episode timestep = 854
Scene graph at timestep 2497 is [True, False, False, False, False, True]
Scene graph at timestep 2497 is [True, False, False, False, False, True]
State prediction error at timestep 2497 is tensor(0.0633, grad_fn=<MseLossBackward0>)
Current timestep = 2498. State = [[-0.17931618  0.26757863]]. Action = [[ 0.18736348  0.12460843 -0.08764362  0.37210405]]. Reward = [0.]
Curr episode timestep = 855
Scene graph at timestep 2498 is [True, False, False, False, False, True]
Current timestep = 2499. State = [[-0.1722421   0.27099738]]. Action = [[ 0.0346773   0.12481818 -0.09966312 -0.5771678 ]]. Reward = [0.]
Curr episode timestep = 856
Scene graph at timestep 2499 is [True, False, False, False, False, True]
Current timestep = 2500. State = [[-0.168839    0.27636826]]. Action = [[-0.0574428  -0.01690799 -0.20851511 -0.26800203]]. Reward = [0.]
Curr episode timestep = 857
Scene graph at timestep 2500 is [True, False, False, False, False, True]
Current timestep = 2501. State = [[-0.16753936  0.28088155]]. Action = [[ 0.16841018  0.18962908 -0.02775289 -0.5362177 ]]. Reward = [0.]
Curr episode timestep = 858
Scene graph at timestep 2501 is [True, False, False, False, False, True]
Current timestep = 2502. State = [[-0.1587398  0.2875095]]. Action = [[ 0.17958081 -0.05874236  0.19511646  0.7064636 ]]. Reward = [0.]
Curr episode timestep = 859
Scene graph at timestep 2502 is [True, False, False, False, False, True]
Current timestep = 2503. State = [[-0.14759037  0.28679177]]. Action = [[ 0.11324534 -0.08877191  0.00458312  0.72235894]]. Reward = [0.]
Curr episode timestep = 860
Scene graph at timestep 2503 is [True, False, False, False, False, True]
Current timestep = 2504. State = [[-0.13981795  0.28405613]]. Action = [[-0.00188591  0.0377385  -0.08636728 -0.02136475]]. Reward = [0.]
Curr episode timestep = 861
Scene graph at timestep 2504 is [True, False, False, False, False, True]
Current timestep = 2505. State = [[-0.1346888   0.28726387]]. Action = [[ 0.1805824   0.19392958 -0.07005666  0.55881953]]. Reward = [0.]
Curr episode timestep = 862
Scene graph at timestep 2505 is [True, False, False, False, False, True]
Current timestep = 2506. State = [[-0.12824914  0.29627505]]. Action = [[-0.0998517   0.09934413  0.11933789  0.94584346]]. Reward = [0.]
Curr episode timestep = 863
Scene graph at timestep 2506 is [True, False, False, False, False, True]
Current timestep = 2507. State = [[-0.130035   0.2995607]]. Action = [[-0.08216599 -0.22102903  0.10551971 -0.7322469 ]]. Reward = [0.]
Curr episode timestep = 864
Scene graph at timestep 2507 is [True, False, False, False, False, True]
Current timestep = 2508. State = [[-0.13671927  0.29696563]]. Action = [[-0.1643975   0.21774322 -0.01851808  0.08922732]]. Reward = [0.]
Curr episode timestep = 865
Scene graph at timestep 2508 is [True, False, False, False, False, True]
Current timestep = 2509. State = [[-0.14408916  0.3037427 ]]. Action = [[ 0.03312197  0.06820992 -0.13891944 -0.5005284 ]]. Reward = [0.]
Curr episode timestep = 866
Scene graph at timestep 2509 is [True, False, False, False, False, True]
Scene graph at timestep 2509 is [True, False, False, False, False, True]
State prediction error at timestep 2509 is tensor(0.0766, grad_fn=<MseLossBackward0>)
Current timestep = 2510. State = [[-0.146903   0.3071921]]. Action = [[-0.09405482 -0.09827195  0.04024178  0.11264396]]. Reward = [0.]
Curr episode timestep = 867
Scene graph at timestep 2510 is [True, False, False, False, False, True]
Current timestep = 2511. State = [[-0.14824378  0.3040129 ]]. Action = [[ 0.18864268 -0.08071762  0.03959373 -0.26627207]]. Reward = [0.]
Curr episode timestep = 868
Scene graph at timestep 2511 is [True, False, False, False, False, True]
Current timestep = 2512. State = [[-0.14020267  0.30249375]]. Action = [[0.17035294 0.1603365  0.0953967  0.6473894 ]]. Reward = [0.]
Curr episode timestep = 869
Scene graph at timestep 2512 is [True, False, False, False, False, True]
Current timestep = 2513. State = [[-0.13109495  0.3089232 ]]. Action = [[-0.00674354  0.11886358 -0.14697185 -0.27137804]]. Reward = [0.]
Curr episode timestep = 870
Scene graph at timestep 2513 is [True, False, False, False, False, True]
Current timestep = 2514. State = [[-0.12683915  0.31245372]]. Action = [[ 0.05812007 -0.18905738  0.03889266 -0.65067637]]. Reward = [0.]
Curr episode timestep = 871
Scene graph at timestep 2514 is [True, False, False, False, False, True]
Current timestep = 2515. State = [[-0.12702444  0.30521762]]. Action = [[-0.22633766 -0.15279123 -0.22278003 -0.90296096]]. Reward = [0.]
Curr episode timestep = 872
Scene graph at timestep 2515 is [True, False, False, False, False, True]
Current timestep = 2516. State = [[-0.1374189  0.293896 ]]. Action = [[-0.17761004 -0.2203015   0.15150821  0.26077783]]. Reward = [0.]
Curr episode timestep = 873
Scene graph at timestep 2516 is [True, False, False, False, False, True]
Scene graph at timestep 2516 is [True, False, False, False, False, True]
State prediction error at timestep 2516 is tensor(0.0629, grad_fn=<MseLossBackward0>)
Current timestep = 2517. State = [[-0.15070648  0.2804723 ]]. Action = [[-2.04317331e-01 -1.34145230e-01 -1.05965376e-01  1.96218491e-04]]. Reward = [0.]
Curr episode timestep = 874
Scene graph at timestep 2517 is [True, False, False, False, False, True]
Current timestep = 2518. State = [[-0.16236192  0.27075428]]. Action = [[ 0.00767991 -0.02948979  0.06236482 -0.6565775 ]]. Reward = [0.]
Curr episode timestep = 875
Scene graph at timestep 2518 is [True, False, False, False, False, True]
Current timestep = 2519. State = [[-0.16349946  0.26461005]]. Action = [[ 0.1795522  -0.1122462   0.19975442  0.9515178 ]]. Reward = [0.]
Curr episode timestep = 876
Scene graph at timestep 2519 is [True, False, False, False, False, True]
Current timestep = 2520. State = [[-0.15684332  0.25757837]]. Action = [[ 0.04379106 -0.08386946  0.11056775 -0.6092942 ]]. Reward = [0.]
Curr episode timestep = 877
Scene graph at timestep 2520 is [True, False, False, False, False, True]
Current timestep = 2521. State = [[-0.15040316  0.25381786]]. Action = [[ 0.1616242   0.09360367  0.20547152 -0.6941138 ]]. Reward = [0.]
Curr episode timestep = 878
Scene graph at timestep 2521 is [True, False, False, False, False, True]
Current timestep = 2522. State = [[-0.1389269   0.25231856]]. Action = [[ 0.24416262 -0.20006697 -0.14539951 -0.2172162 ]]. Reward = [0.]
Curr episode timestep = 879
Scene graph at timestep 2522 is [True, False, False, False, False, True]
Current timestep = 2523. State = [[-0.12432853  0.24276567]]. Action = [[ 0.15336096 -0.17338832  0.2478185  -0.03076357]]. Reward = [0.]
Curr episode timestep = 880
Scene graph at timestep 2523 is [True, False, False, False, False, True]
Scene graph at timestep 2523 is [True, False, False, False, False, True]
State prediction error at timestep 2523 is tensor(0.0471, grad_fn=<MseLossBackward0>)
Current timestep = 2524. State = [[-0.11686265  0.23338278]]. Action = [[-0.20880936 -0.00397687 -0.1527229   0.37601972]]. Reward = [0.]
Curr episode timestep = 881
Scene graph at timestep 2524 is [True, False, False, False, False, True]
Current timestep = 2525. State = [[-0.12180281  0.22746499]]. Action = [[-0.04021071 -0.1572463  -0.24764645  0.6577717 ]]. Reward = [0.]
Curr episode timestep = 882
Scene graph at timestep 2525 is [True, False, False, False, False, True]
Scene graph at timestep 2525 is [True, False, False, False, False, True]
State prediction error at timestep 2525 is tensor(0.0505, grad_fn=<MseLossBackward0>)
Current timestep = 2526. State = [[-0.12729393  0.21819639]]. Action = [[-0.11514321 -0.14051601 -0.02654715  0.19644606]]. Reward = [0.]
Curr episode timestep = 883
Scene graph at timestep 2526 is [True, False, False, False, False, True]
Current timestep = 2527. State = [[-0.12965387  0.20744206]]. Action = [[ 0.24152052 -0.17961203  0.13252589  0.40642428]]. Reward = [0.]
Curr episode timestep = 884
Scene graph at timestep 2527 is [True, False, False, False, False, True]
Scene graph at timestep 2527 is [True, False, False, False, False, True]
State prediction error at timestep 2527 is tensor(0.0388, grad_fn=<MseLossBackward0>)
Current timestep = 2528. State = [[-0.12564671  0.19654503]]. Action = [[-0.22582349 -0.06635946  0.17081138  0.15026045]]. Reward = [0.]
Curr episode timestep = 885
Scene graph at timestep 2528 is [True, False, False, False, False, True]
Current timestep = 2529. State = [[-0.13228595  0.19446835]]. Action = [[-0.05690394  0.24905121  0.09610355 -0.02858669]]. Reward = [0.]
Curr episode timestep = 886
Scene graph at timestep 2529 is [True, False, False, False, False, True]
Current timestep = 2530. State = [[-0.1345496   0.19960785]]. Action = [[ 0.15745032 -0.14263079  0.22948915  0.03777456]]. Reward = [0.]
Curr episode timestep = 887
Scene graph at timestep 2530 is [True, False, False, False, False, True]
Current timestep = 2531. State = [[-0.12644543  0.19371852]]. Action = [[ 0.24643594 -0.22591536  0.19926468 -0.4335091 ]]. Reward = [0.]
Curr episode timestep = 888
Scene graph at timestep 2531 is [True, False, False, False, False, True]
Current timestep = 2532. State = [[-0.11367033  0.18252112]]. Action = [[ 0.09174189 -0.08794761  0.2167218   0.7084638 ]]. Reward = [0.]
Curr episode timestep = 889
Scene graph at timestep 2532 is [True, False, False, False, False, True]
Current timestep = 2533. State = [[-0.10540147  0.17909157]]. Action = [[ 0.02861774  0.23079214  0.00110009 -0.5575656 ]]. Reward = [0.]
Curr episode timestep = 890
Scene graph at timestep 2533 is [True, False, False, False, False, True]
Current timestep = 2534. State = [[-0.10456764  0.18670651]]. Action = [[-0.21744551  0.08448136  0.20344731 -0.7527542 ]]. Reward = [0.]
Curr episode timestep = 891
Scene graph at timestep 2534 is [True, False, False, False, False, True]
Current timestep = 2535. State = [[-0.11090019  0.1948339 ]]. Action = [[ 0.07062414  0.13541383  0.08283874 -0.802934  ]]. Reward = [0.]
Curr episode timestep = 892
Scene graph at timestep 2535 is [True, False, False, False, False, True]
Current timestep = 2536. State = [[-0.10853717  0.2047178 ]]. Action = [[ 0.1893642   0.16723216 -0.13357341  0.6036782 ]]. Reward = [0.]
Curr episode timestep = 893
Scene graph at timestep 2536 is [True, False, False, False, False, True]
Scene graph at timestep 2536 is [True, False, False, False, False, True]
State prediction error at timestep 2536 is tensor(0.0449, grad_fn=<MseLossBackward0>)
Current timestep = 2537. State = [[-0.09904174  0.21462634]]. Action = [[ 0.1277712   0.06035501 -0.02138768  0.7198541 ]]. Reward = [0.]
Curr episode timestep = 894
Scene graph at timestep 2537 is [True, False, False, False, False, True]
Scene graph at timestep 2537 is [True, False, False, False, False, True]
State prediction error at timestep 2537 is tensor(0.0432, grad_fn=<MseLossBackward0>)
Current timestep = 2538. State = [[-0.09376423  0.22263876]]. Action = [[-0.18901764  0.1584095   0.13128066 -0.9550939 ]]. Reward = [0.]
Curr episode timestep = 895
Scene graph at timestep 2538 is [True, False, False, False, False, True]
Current timestep = 2539. State = [[-0.09871687  0.22849078]]. Action = [[-0.04210995 -0.135382    0.22550529 -0.915445  ]]. Reward = [0.]
Curr episode timestep = 896
Scene graph at timestep 2539 is [True, False, False, False, False, True]
Current timestep = 2540. State = [[-0.1010924   0.22838707]]. Action = [[ 0.11184475  0.13181695 -0.20911062  0.435076  ]]. Reward = [0.]
Curr episode timestep = 897
Scene graph at timestep 2540 is [True, False, False, False, False, True]
Scene graph at timestep 2540 is [True, False, False, False, False, True]
State prediction error at timestep 2540 is tensor(0.0513, grad_fn=<MseLossBackward0>)
Current timestep = 2541. State = [[-0.09550838  0.2325261 ]]. Action = [[ 1.6946548e-01  1.8241167e-02 -1.5046872e-01 -8.9347363e-05]]. Reward = [0.]
Curr episode timestep = 898
Scene graph at timestep 2541 is [True, False, False, False, False, True]
Current timestep = 2542. State = [[-0.08724476  0.23108828]]. Action = [[ 0.00481141 -0.2335585   0.1612271  -0.15835506]]. Reward = [0.]
Curr episode timestep = 899
Scene graph at timestep 2542 is [True, False, False, False, False, True]
Current timestep = 2543. State = [[-0.0807225   0.22493452]]. Action = [[ 0.24808526  0.11791125 -0.20078512 -0.6326416 ]]. Reward = [0.]
Curr episode timestep = 900
Scene graph at timestep 2543 is [True, False, False, False, False, True]
Scene graph at timestep 2543 is [True, False, False, False, False, True]
State prediction error at timestep 2543 is tensor(0.0523, grad_fn=<MseLossBackward0>)
Current timestep = 2544. State = [[-0.18793195  0.11526021]]. Action = [[-0.16144454 -0.15367761 -0.00398156 -0.79091513]]. Reward = [0.]
Curr episode timestep = 901
Scene graph at timestep 2544 is [True, False, False, False, False, True]
Current timestep = 2545. State = [[-0.18835491  0.11808842]]. Action = [[-0.09285805  0.07159138  0.08530727 -0.6379385 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 2545 is [True, False, False, False, True, False]
Scene graph at timestep 2545 is [True, False, False, False, True, False]
State prediction error at timestep 2545 is tensor(0.0296, grad_fn=<MseLossBackward0>)
Current timestep = 2546. State = [[-0.19174266  0.12081175]]. Action = [[-0.00957784 -0.0404187  -0.23790014 -0.56745934]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 2546 is [True, False, False, False, True, False]
Current timestep = 2547. State = [[-0.19183691  0.11770564]]. Action = [[ 0.11077011 -0.18097663 -0.19769374 -0.9918813 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2547 is [True, False, False, False, True, False]
Scene graph at timestep 2547 is [True, False, False, False, True, False]
State prediction error at timestep 2547 is tensor(0.0309, grad_fn=<MseLossBackward0>)
Current timestep = 2548. State = [[-0.1861498   0.11402815]]. Action = [[0.15705979 0.21581185 0.14758697 0.5664115 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 2548 is [True, False, False, False, True, False]
Current timestep = 2549. State = [[-0.18122908  0.11624777]]. Action = [[-0.22800778 -0.19231889 -0.00745995 -0.7973538 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 2549 is [True, False, False, False, True, False]
Scene graph at timestep 2549 is [True, False, False, False, True, False]
State prediction error at timestep 2549 is tensor(0.0246, grad_fn=<MseLossBackward0>)
Current timestep = 2550. State = [[-0.18580878  0.11506072]]. Action = [[0.13022849 0.22394353 0.13426054 0.03378201]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 2550 is [True, False, False, False, True, False]
Current timestep = 2551. State = [[-0.17964377  0.11931439]]. Action = [[ 0.24828288 -0.13042045  0.13421184 -0.07224518]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 2551 is [True, False, False, False, True, False]
Current timestep = 2552. State = [[-0.16687286  0.11441709]]. Action = [[ 0.12365824 -0.1605587  -0.10635883 -0.6918456 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 2552 is [True, False, False, False, True, False]
Current timestep = 2553. State = [[-0.16003147  0.11079945]]. Action = [[-0.13795151  0.23034114  0.2162348   0.53416157]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 2553 is [True, False, False, False, True, False]
Current timestep = 2554. State = [[-0.16501915  0.11975195]]. Action = [[-0.1955483   0.1861794  -0.13725355  0.6995411 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 2554 is [True, False, False, False, True, False]
Current timestep = 2555. State = [[-0.17587322  0.13287792]]. Action = [[-0.12835339  0.2200982   0.1838324   0.75725126]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 2555 is [True, False, False, False, True, False]
Current timestep = 2556. State = [[-0.18160601  0.14639828]]. Action = [[ 0.19122842  0.09775367 -0.01885153  0.97829413]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 2556 is [True, False, False, False, False, True]
Scene graph at timestep 2556 is [True, False, False, False, False, True]
State prediction error at timestep 2556 is tensor(0.0398, grad_fn=<MseLossBackward0>)
Current timestep = 2557. State = [[-0.17980911  0.1521591 ]]. Action = [[-0.21350661 -0.14088711 -0.21100672 -0.5595941 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 2557 is [True, False, False, False, False, True]
Current timestep = 2558. State = [[-0.18920763  0.15199672]]. Action = [[-0.20359758  0.14683628 -0.04074058 -0.51650596]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 2558 is [True, False, False, False, False, True]
Current timestep = 2559. State = [[-0.19823276  0.1600227 ]]. Action = [[ 0.1275985   0.23477685 -0.14719947  0.28436637]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 2559 is [True, False, False, False, False, True]
Current timestep = 2560. State = [[-0.1983736   0.16902469]]. Action = [[-0.11332856 -0.10113615  0.2353625  -0.60807735]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 2560 is [True, False, False, False, False, True]
Scene graph at timestep 2560 is [True, False, False, False, False, True]
State prediction error at timestep 2560 is tensor(0.0362, grad_fn=<MseLossBackward0>)
Current timestep = 2561. State = [[-0.19940686  0.17119266]]. Action = [[ 0.19471076  0.11657301 -0.17182559  0.5240011 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 2561 is [True, False, False, False, False, True]
Current timestep = 2562. State = [[-0.19150779  0.17836753]]. Action = [[ 0.12849909  0.19646251 -0.00324503 -0.9671297 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 2562 is [True, False, False, False, False, True]
Current timestep = 2563. State = [[-0.18301046  0.18984146]]. Action = [[ 0.05382907  0.1520783   0.2339364  -0.39905018]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 2563 is [True, False, False, False, False, True]
Scene graph at timestep 2563 is [True, False, False, False, False, True]
State prediction error at timestep 2563 is tensor(0.0458, grad_fn=<MseLossBackward0>)
Current timestep = 2564. State = [[-0.1745745   0.19607745]]. Action = [[ 0.22812688 -0.19245529  0.05475357 -0.7971611 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 2564 is [True, False, False, False, False, True]
Current timestep = 2565. State = [[-0.16145664  0.19065303]]. Action = [[ 0.19188306 -0.08725336 -0.167109   -0.657499  ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 2565 is [True, False, False, False, False, True]
Scene graph at timestep 2565 is [True, False, False, False, False, True]
State prediction error at timestep 2565 is tensor(0.0427, grad_fn=<MseLossBackward0>)
Current timestep = 2566. State = [[-0.150312   0.1861354]]. Action = [[-0.016076    0.04590636 -0.20863856 -0.5861041 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 2566 is [True, False, False, False, False, True]
Scene graph at timestep 2566 is [True, False, False, False, False, True]
State prediction error at timestep 2566 is tensor(0.0439, grad_fn=<MseLossBackward0>)
Current timestep = 2567. State = [[-0.1463124  0.1857155]]. Action = [[ 0.02838889 -0.02256361 -0.14168185  0.7787981 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 2567 is [True, False, False, False, False, True]
Current timestep = 2568. State = [[-0.14117993  0.18613458]]. Action = [[ 0.2262904   0.07049298 -0.01807785  0.02482343]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 2568 is [True, False, False, False, False, True]
Scene graph at timestep 2568 is [True, False, False, False, False, True]
State prediction error at timestep 2568 is tensor(0.0417, grad_fn=<MseLossBackward0>)
Current timestep = 2569. State = [[-0.13106215  0.1912906 ]]. Action = [[ 0.04616576  0.18309498 -0.16240767 -0.44352007]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 2569 is [True, False, False, False, False, True]
Current timestep = 2570. State = [[-0.12876566  0.20065522]]. Action = [[-0.23593433  0.09411299 -0.07557957  0.8275516 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 2570 is [True, False, False, False, False, True]
Current timestep = 2571. State = [[-0.13743334  0.20808092]]. Action = [[-0.08347884  0.04695964 -0.23488367  0.84840536]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 2571 is [True, False, False, False, False, True]
Current timestep = 2572. State = [[-0.14601326  0.21416517]]. Action = [[-0.14087476  0.1284821   0.12956801 -0.99972737]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 2572 is [True, False, False, False, False, True]
Current timestep = 2573. State = [[-0.15053667  0.22173099]]. Action = [[ 0.22646862  0.07900617  0.22474092 -0.73716635]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 2573 is [True, False, False, False, False, True]
Scene graph at timestep 2573 is [True, False, False, False, False, True]
State prediction error at timestep 2573 is tensor(0.0477, grad_fn=<MseLossBackward0>)
Current timestep = 2574. State = [[-0.14458673  0.22910629]]. Action = [[-0.00928268  0.13648635 -0.09063441  0.85959077]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 2574 is [True, False, False, False, False, True]
Scene graph at timestep 2574 is [True, False, False, False, False, True]
State prediction error at timestep 2574 is tensor(0.0560, grad_fn=<MseLossBackward0>)
Current timestep = 2575. State = [[-0.14351389  0.23765634]]. Action = [[-0.09729052  0.09164798 -0.07972537  0.459795  ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 2575 is [True, False, False, False, False, True]
Current timestep = 2576. State = [[-0.14800179  0.24014214]]. Action = [[-0.11623824 -0.23202513  0.20135301  0.77468705]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 2576 is [True, False, False, False, False, True]
Current timestep = 2577. State = [[-0.15110089  0.23091991]]. Action = [[ 0.2126976  -0.16721697  0.1185669  -0.8999358 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 2577 is [True, False, False, False, False, True]
Scene graph at timestep 2577 is [True, False, False, False, False, True]
State prediction error at timestep 2577 is tensor(0.0493, grad_fn=<MseLossBackward0>)
Current timestep = 2578. State = [[-0.14861304  0.2243491 ]]. Action = [[-0.23175208  0.17434412 -0.18223082 -0.8047031 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 2578 is [True, False, False, False, False, True]
Current timestep = 2579. State = [[-0.15392035  0.2250529 ]]. Action = [[ 0.04837763 -0.16982797 -0.1493099   0.867067  ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 2579 is [True, False, False, False, False, True]
Current timestep = 2580. State = [[-0.1550614   0.22055088]]. Action = [[-0.00285256  0.02535245 -0.1993474  -0.05585432]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 2580 is [True, False, False, False, False, True]
Scene graph at timestep 2580 is [True, False, False, False, False, True]
State prediction error at timestep 2580 is tensor(0.0552, grad_fn=<MseLossBackward0>)
Current timestep = 2581. State = [[-0.15750143  0.21746945]]. Action = [[-0.170104   -0.12634836 -0.05105697 -0.07857317]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 2581 is [True, False, False, False, False, True]
Current timestep = 2582. State = [[-0.16371574  0.21534573]]. Action = [[0.06088182 0.18238518 0.02548429 0.85501313]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 2582 is [True, False, False, False, False, True]
Current timestep = 2583. State = [[-0.16470118  0.21816184]]. Action = [[-0.07693383 -0.1332948  -0.04440627 -0.8750507 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 2583 is [True, False, False, False, False, True]
Scene graph at timestep 2583 is [True, False, False, False, False, True]
State prediction error at timestep 2583 is tensor(0.0477, grad_fn=<MseLossBackward0>)
Current timestep = 2584. State = [[-0.16721965  0.21407251]]. Action = [[ 0.02538237 -0.0762462   0.22032997 -0.0878284 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 2584 is [True, False, False, False, False, True]
Current timestep = 2585. State = [[-0.16888538  0.21082774]]. Action = [[-0.09560513  0.06382486  0.22837263  0.34365392]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 2585 is [True, False, False, False, False, True]
Scene graph at timestep 2585 is [True, False, False, False, False, True]
State prediction error at timestep 2585 is tensor(0.0509, grad_fn=<MseLossBackward0>)
Current timestep = 2586. State = [[-0.1710111   0.20901522]]. Action = [[ 0.09192604 -0.17353241  0.11594117  0.7800448 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 2586 is [True, False, False, False, False, True]
Current timestep = 2587. State = [[-0.16983034  0.19916862]]. Action = [[-7.0423126e-02 -2.3526250e-01 -5.6385994e-05 -9.7149146e-01]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 2587 is [True, False, False, False, False, True]
Scene graph at timestep 2587 is [True, False, False, False, False, True]
State prediction error at timestep 2587 is tensor(0.0409, grad_fn=<MseLossBackward0>)
Current timestep = 2588. State = [[-0.17403674  0.18757275]]. Action = [[-0.15077217 -0.01704262  0.12597257  0.7661772 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 2588 is [True, False, False, False, False, True]
Current timestep = 2589. State = [[-0.18024242  0.18255526]]. Action = [[ 0.03613365 -0.00968872  0.18807644  0.6900823 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 2589 is [True, False, False, False, False, True]
Current timestep = 2590. State = [[-0.17973056  0.17756459]]. Action = [[ 0.10047016 -0.19339745 -0.22771056  0.72432876]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 2590 is [True, False, False, False, False, True]
Current timestep = 2591. State = [[-0.17302456  0.17179678]]. Action = [[0.22951934 0.13768703 0.15214157 0.25117922]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 2591 is [True, False, False, False, False, True]
Current timestep = 2592. State = [[-0.16542558  0.17484166]]. Action = [[-0.21952817  0.06525481 -0.05972639 -0.8848199 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 2592 is [True, False, False, False, False, True]
Scene graph at timestep 2592 is [True, False, False, False, False, True]
State prediction error at timestep 2592 is tensor(0.0420, grad_fn=<MseLossBackward0>)
Current timestep = 2593. State = [[-0.16964227  0.17485711]]. Action = [[ 0.00603187 -0.23669052 -0.15283045 -0.3386742 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 2593 is [True, False, False, False, False, True]
Current timestep = 2594. State = [[-0.17125987  0.1635092 ]]. Action = [[ 0.03989074 -0.23699315  0.17318535  0.41449797]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 2594 is [True, False, False, False, False, True]
Current timestep = 2595. State = [[-0.17123882  0.15047319]]. Action = [[-0.04942313 -0.05789089  0.07498375 -0.7950017 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 2595 is [True, False, False, False, False, True]
Current timestep = 2596. State = [[-0.17052312  0.14333151]]. Action = [[ 0.15298796 -0.0311749   0.20142522  0.6627672 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 2596 is [True, False, False, False, False, True]
Scene graph at timestep 2596 is [True, False, False, False, False, True]
State prediction error at timestep 2596 is tensor(0.0340, grad_fn=<MseLossBackward0>)
Current timestep = 2597. State = [[-0.16620688  0.139223  ]]. Action = [[-0.08278929 -0.03709343 -0.03370191  0.1107111 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 2597 is [True, False, False, False, False, True]
Scene graph at timestep 2597 is [True, False, False, False, False, True]
State prediction error at timestep 2597 is tensor(0.0355, grad_fn=<MseLossBackward0>)
Current timestep = 2598. State = [[-0.16865581  0.13815247]]. Action = [[-0.09449622  0.09075686 -0.0221194   0.15228307]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 2598 is [True, False, False, False, False, True]
Current timestep = 2599. State = [[-0.1738678   0.13883482]]. Action = [[-0.0715518  -0.11890841  0.09138945  0.9462832 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 2599 is [True, False, False, False, False, True]
Current timestep = 2600. State = [[-0.17877066  0.13597195]]. Action = [[-0.01859005  0.03378579  0.16272822 -0.37767065]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 2600 is [True, False, False, False, False, True]
Current timestep = 2601. State = [[-0.18442729  0.1353462 ]]. Action = [[-0.24023438 -0.02512789 -0.10485116  0.268713  ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 2601 is [True, False, False, False, False, True]
Current timestep = 2602. State = [[-0.19129717  0.13621794]]. Action = [[ 0.24360329  0.10446846 -0.04157822  0.30444694]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 2602 is [True, False, False, False, False, True]
Current timestep = 2603. State = [[-0.18437123  0.14156108]]. Action = [[ 0.08828354  0.11688638  0.07850382 -0.9674787 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 2603 is [True, False, False, False, False, True]
Scene graph at timestep 2603 is [True, False, False, False, False, True]
State prediction error at timestep 2603 is tensor(0.0349, grad_fn=<MseLossBackward0>)
Current timestep = 2604. State = [[-0.17987883  0.14816788]]. Action = [[-0.1256403   0.04750457  0.03941807  0.43475735]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 2604 is [True, False, False, False, False, True]
Current timestep = 2605. State = [[-0.18353686  0.15583338]]. Action = [[-0.05009219  0.22859871  0.17435917 -0.13779122]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 2605 is [True, False, False, False, False, True]
Scene graph at timestep 2605 is [True, False, False, False, False, True]
State prediction error at timestep 2605 is tensor(0.0439, grad_fn=<MseLossBackward0>)
Current timestep = 2606. State = [[-0.18518803  0.16739482]]. Action = [[ 0.11302036  0.09596556  0.06495661 -0.10884106]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 2606 is [True, False, False, False, False, True]
Current timestep = 2607. State = [[-0.18188027  0.17731361]]. Action = [[ 0.00557172  0.15911353 -0.15547247 -0.41356897]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 2607 is [True, False, False, False, False, True]
Current timestep = 2608. State = [[-0.17815001  0.18747403]]. Action = [[ 0.14327168  0.09478462 -0.13874009  0.73309517]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 2608 is [True, False, False, False, False, True]
Current timestep = 2609. State = [[-0.17273001  0.19552577]]. Action = [[-0.05216171  0.0863567   0.04686472  0.4541328 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 2609 is [True, False, False, False, False, True]
Current timestep = 2610. State = [[-0.17414938  0.20144558]]. Action = [[-0.12780342  0.01609111  0.24569315  0.7684703 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 2610 is [True, False, False, False, False, True]
Current timestep = 2611. State = [[-0.18085204  0.20611879]]. Action = [[-0.09852614  0.1309048  -0.04366027  0.7693281 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 2611 is [True, False, False, False, False, True]
Current timestep = 2612. State = [[-0.18978412  0.2097816 ]]. Action = [[-0.23046944 -0.13322149  0.14200121  0.06071043]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 2612 is [True, False, False, False, False, True]
Current timestep = 2613. State = [[-0.2045153   0.20760714]]. Action = [[-0.23393938  0.04436839  0.164583    0.90855813]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 2613 is [True, False, False, False, False, True]
Current timestep = 2614. State = [[-0.21518517  0.20487763]]. Action = [[ 0.17658257 -0.19799675  0.09806123 -0.12485981]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 2614 is [True, False, False, False, False, True]
Current timestep = 2615. State = [[-0.21605416  0.19705321]]. Action = [[-0.20029496 -0.03206432  0.20316088  0.26100945]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 2615 is [True, False, False, False, False, True]
Current timestep = 2616. State = [[-0.2214413   0.19510052]]. Action = [[ 0.08814073  0.12809026 -0.12035967 -0.26843584]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 2616 is [True, False, False, False, False, True]
Current timestep = 2617. State = [[-0.21913832  0.19926308]]. Action = [[ 0.09570527  0.04468781 -0.16325925  0.68371224]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 2617 is [True, False, False, False, False, True]
Current timestep = 2618. State = [[-0.21513513  0.20080467]]. Action = [[-0.0408421  -0.1045137   0.08780307 -0.22385496]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 2618 is [True, False, False, False, False, True]
Scene graph at timestep 2618 is [True, False, False, False, False, True]
State prediction error at timestep 2618 is tensor(0.0514, grad_fn=<MseLossBackward0>)
Current timestep = 2619. State = [[-0.21434386  0.200698  ]]. Action = [[ 0.07381427  0.16936696  0.08576638 -0.1860627 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 2619 is [True, False, False, False, False, True]
Current timestep = 2620. State = [[-0.20916675  0.2089361 ]]. Action = [[0.17433617 0.19425988 0.0344179  0.91513085]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 2620 is [True, False, False, False, False, True]
Current timestep = 2621. State = [[-0.20283182  0.22138156]]. Action = [[-0.1170205   0.18996781 -0.06605281 -0.65554297]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 2621 is [True, False, False, False, False, True]
Current timestep = 2622. State = [[-0.2043356   0.23012811]]. Action = [[-0.02042793 -0.1343529  -0.04311487  0.10221803]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 2622 is [True, False, False, False, False, True]
Scene graph at timestep 2622 is [True, False, False, False, False, True]
State prediction error at timestep 2622 is tensor(0.0613, grad_fn=<MseLossBackward0>)
Current timestep = 2623. State = [[-0.20733257  0.22678427]]. Action = [[-0.08368133 -0.13640644 -0.18535912 -0.00683254]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 2623 is [True, False, False, False, False, True]
Current timestep = 2624. State = [[-0.20899586  0.21922316]]. Action = [[ 0.16864878 -0.10104528 -0.09012124 -0.47296858]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 2624 is [True, False, False, False, False, True]
Current timestep = 2625. State = [[-0.20691349  0.2151694 ]]. Action = [[-0.19957674  0.13847345 -0.18750452  0.12349665]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 2625 is [True, False, False, False, False, True]
Current timestep = 2626. State = [[-0.21555915  0.21658146]]. Action = [[-0.23591335 -0.10670096  0.13602552  0.94088364]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 2626 is [True, False, False, False, False, True]
Current timestep = 2627. State = [[-0.22523886  0.21098793]]. Action = [[ 0.14251554 -0.18813501  0.15779173 -0.68911844]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 2627 is [True, False, False, False, False, True]
Current timestep = 2628. State = [[-0.22728363  0.2048636 ]]. Action = [[-0.20119262  0.14211369 -0.13891187  0.62243915]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 2628 is [True, False, False, False, False, True]
Current timestep = 2629. State = [[-0.23274536  0.20763116]]. Action = [[ 0.09038562  0.02983201  0.17664838 -0.21321326]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 2629 is [True, False, False, False, False, True]
Scene graph at timestep 2629 is [True, False, False, False, False, True]
State prediction error at timestep 2629 is tensor(0.0585, grad_fn=<MseLossBackward0>)
Current timestep = 2630. State = [[-0.23539668  0.21338135]]. Action = [[-0.22903106  0.22416952  0.0647015  -0.07420415]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 2630 is [True, False, False, False, False, True]
Current timestep = 2631. State = [[-0.24101126  0.22357199]]. Action = [[ 0.18410414  0.05221421 -0.07920578  0.65735245]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 2631 is [True, False, False, False, False, True]
Current timestep = 2632. State = [[-0.23400234  0.22589739]]. Action = [[ 0.19926539 -0.2018184  -0.06198145  0.34012926]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 2632 is [True, False, False, False, False, True]
Current timestep = 2633. State = [[-0.22376706  0.22196718]]. Action = [[0.06573924 0.11942685 0.0625186  0.28261745]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 2633 is [True, False, False, False, False, True]
Current timestep = 2634. State = [[-0.21634644  0.22226991]]. Action = [[ 0.0632664  -0.11714074 -0.16765377  0.80982256]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 2634 is [True, False, False, False, False, True]
Current timestep = 2635. State = [[-0.20965287  0.22037338]]. Action = [[0.16780418 0.09551689 0.10072219 0.3760054 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 2635 is [True, False, False, False, False, True]
Current timestep = 2636. State = [[-0.20014071  0.22160417]]. Action = [[ 0.084997   -0.0592669  -0.00503765 -0.24417591]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 2636 is [True, False, False, False, False, True]
Current timestep = 2637. State = [[-0.19036733  0.21834143]]. Action = [[ 0.23442942 -0.12901087 -0.163192   -0.06922531]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 2637 is [True, False, False, False, False, True]
Scene graph at timestep 2637 is [True, False, False, False, False, True]
State prediction error at timestep 2637 is tensor(0.0562, grad_fn=<MseLossBackward0>)
Current timestep = 2638. State = [[-0.18197541  0.209597  ]]. Action = [[-0.2120406  -0.20279777  0.22063619  0.06523848]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 2638 is [True, False, False, False, False, True]
Scene graph at timestep 2638 is [True, False, False, False, False, True]
State prediction error at timestep 2638 is tensor(0.0462, grad_fn=<MseLossBackward0>)
Current timestep = 2639. State = [[-0.18802616  0.1964327 ]]. Action = [[-0.13238606 -0.22269107  0.2320072   0.66792893]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 2639 is [True, False, False, False, False, True]
Current timestep = 2640. State = [[-0.19410786  0.18572505]]. Action = [[0.11505994 0.08604214 0.01048663 0.05681074]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 2640 is [True, False, False, False, False, True]
Scene graph at timestep 2640 is [True, False, False, False, False, True]
State prediction error at timestep 2640 is tensor(0.0508, grad_fn=<MseLossBackward0>)
Current timestep = 2641. State = [[-0.19017223  0.18294738]]. Action = [[ 0.12699926 -0.09658784 -0.0867388  -0.61440694]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 2641 is [True, False, False, False, False, True]
Scene graph at timestep 2641 is [True, False, False, False, False, True]
State prediction error at timestep 2641 is tensor(0.0420, grad_fn=<MseLossBackward0>)
Current timestep = 2642. State = [[-0.18260951  0.18010989]]. Action = [[ 0.11195999  0.05802405  0.10622841 -0.17053127]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 2642 is [True, False, False, False, False, True]
Current timestep = 2643. State = [[-0.17766547  0.18398474]]. Action = [[-0.11750045  0.19962803 -0.09360136  0.12731218]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 2643 is [True, False, False, False, False, True]
Current timestep = 2644. State = [[-0.17780298  0.19281217]]. Action = [[ 0.136565    0.04700917 -0.21666041  0.7677164 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 2644 is [True, False, False, False, False, True]
Current timestep = 2645. State = [[-0.17377168  0.19561101]]. Action = [[-0.02430381 -0.12988405  0.12947834 -0.5150837 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 2645 is [True, False, False, False, False, True]
Current timestep = 2646. State = [[-0.17166048  0.19097148]]. Action = [[ 0.09722891 -0.09003222 -0.16774848 -0.82983935]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 2646 is [True, False, False, False, False, True]
Current timestep = 2647. State = [[-0.166407    0.18574601]]. Action = [[ 0.09940353 -0.02213368  0.2475087   0.34248316]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 2647 is [True, False, False, False, False, True]
Current timestep = 2648. State = [[-0.16183001  0.18358068]]. Action = [[-0.06031537  0.03238559  0.05522031 -0.5800099 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 2648 is [True, False, False, False, False, True]
Scene graph at timestep 2648 is [True, False, False, False, False, True]
State prediction error at timestep 2648 is tensor(0.0385, grad_fn=<MseLossBackward0>)
Current timestep = 2649. State = [[-0.16287385  0.18502907]]. Action = [[-0.04684158  0.06729543  0.10987359 -0.66368353]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 2649 is [True, False, False, False, False, True]
Current timestep = 2650. State = [[-0.16270143  0.1889582 ]]. Action = [[ 0.16939104  0.07107988  0.09915581 -0.4256171 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 2650 is [True, False, False, False, False, True]
Current timestep = 2651. State = [[-0.15565692  0.19506705]]. Action = [[0.09213707 0.15257132 0.19921482 0.8003371 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 2651 is [True, False, False, False, False, True]
Current timestep = 2652. State = [[-0.1490467   0.20210229]]. Action = [[ 0.03110242 -0.00901547 -0.05483778  0.5887494 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 2652 is [True, False, False, False, False, True]
Current timestep = 2653. State = [[-0.1456303  0.2031996]]. Action = [[ 2.3278594e-04 -8.1471771e-02  1.3233298e-01  6.5414584e-01]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 2653 is [True, False, False, False, False, True]
Current timestep = 2654. State = [[-0.14826714  0.20329496]]. Action = [[-0.24508698  0.16149461  0.13657999 -0.80800843]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 2654 is [True, False, False, False, False, True]
Scene graph at timestep 2654 is [True, False, False, False, False, True]
State prediction error at timestep 2654 is tensor(0.0463, grad_fn=<MseLossBackward0>)
Current timestep = 2655. State = [[-0.15860377  0.20694338]]. Action = [[-0.08455053 -0.0947904  -0.14490326 -0.38134873]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 2655 is [True, False, False, False, False, True]
Current timestep = 2656. State = [[-0.1660268   0.20681639]]. Action = [[-0.01327868  0.08844355 -0.01770489  0.18052149]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 2656 is [True, False, False, False, False, True]
Current timestep = 2657. State = [[-0.1682414   0.21211532]]. Action = [[ 0.05349374  0.1727696  -0.17472339  0.47679985]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 2657 is [True, False, False, False, False, True]
Current timestep = 2658. State = [[-0.16384043  0.21656565]]. Action = [[ 0.19554242 -0.20341887 -0.2438117  -0.30824643]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 2658 is [True, False, False, False, False, True]
Current timestep = 2659. State = [[-0.15248953  0.20975244]]. Action = [[ 0.22456533 -0.1255971  -0.18710169 -0.71907306]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 2659 is [True, False, False, False, False, True]
Current timestep = 2660. State = [[-0.14254873  0.20531178]]. Action = [[-0.12682769  0.16244963 -0.05034572  0.8243017 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 2660 is [True, False, False, False, False, True]
Current timestep = 2661. State = [[-0.14392616  0.21164376]]. Action = [[-0.06159128  0.16607004  0.16437626 -0.42757303]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 2661 is [True, False, False, False, False, True]
Scene graph at timestep 2661 is [True, False, False, False, False, True]
State prediction error at timestep 2661 is tensor(0.0466, grad_fn=<MseLossBackward0>)
Current timestep = 2662. State = [[-0.14726686  0.22070032]]. Action = [[-0.02076286  0.06921169  0.15641183  0.87010264]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 2662 is [True, False, False, False, False, True]
Current timestep = 2663. State = [[-0.14774066  0.2234858 ]]. Action = [[ 0.09266084 -0.18125963 -0.08611482  0.79797506]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 2663 is [True, False, False, False, False, True]
Current timestep = 2664. State = [[-0.14228304  0.2151362 ]]. Action = [[ 0.17770416 -0.2172997  -0.10797271 -0.7418033 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 2664 is [True, False, False, False, False, True]
Current timestep = 2665. State = [[-0.1378423   0.20733082]]. Action = [[-0.23937693  0.1691609  -0.07052153 -0.12293863]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 2665 is [True, False, False, False, False, True]
Scene graph at timestep 2665 is [True, False, False, False, False, True]
State prediction error at timestep 2665 is tensor(0.0493, grad_fn=<MseLossBackward0>)
Current timestep = 2666. State = [[-0.14323588  0.21305871]]. Action = [[ 0.05951646  0.19745675 -0.1639903   0.44548154]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 2666 is [True, False, False, False, False, True]
Scene graph at timestep 2666 is [True, False, False, False, False, True]
State prediction error at timestep 2666 is tensor(0.0544, grad_fn=<MseLossBackward0>)
Current timestep = 2667. State = [[-0.14053649  0.22476293]]. Action = [[ 0.22221273  0.18371093  0.15560585 -0.79280114]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 2667 is [True, False, False, False, False, True]
Current timestep = 2668. State = [[-0.1321558  0.2366566]]. Action = [[-0.03575903  0.10379601 -0.05303675  0.2848308 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 2668 is [True, False, False, False, False, True]
Current timestep = 2669. State = [[-0.13061298  0.24829632]]. Action = [[-0.03177011  0.24715155  0.21980298  0.9223201 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 2669 is [True, False, False, False, False, True]
Current timestep = 2670. State = [[-0.13149804  0.2602745 ]]. Action = [[-0.0205617  -0.00695097  0.18271142 -0.09119451]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 2670 is [True, False, False, False, False, True]
Current timestep = 2671. State = [[-0.1310061   0.26467222]]. Action = [[ 0.11786541  0.01016766 -0.03737095  0.28771257]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 2671 is [True, False, False, False, False, True]
Current timestep = 2672. State = [[-0.1253422   0.26616126]]. Action = [[ 0.1041151  -0.00799491 -0.0434479   0.598976  ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 2672 is [True, False, False, False, False, True]
Current timestep = 2673. State = [[-0.12050556  0.26958787]]. Action = [[-0.05395344  0.20675051  0.06849879 -0.5013866 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 2673 is [True, False, False, False, False, True]
Scene graph at timestep 2673 is [True, False, False, False, False, True]
State prediction error at timestep 2673 is tensor(0.0628, grad_fn=<MseLossBackward0>)
Current timestep = 2674. State = [[-0.11906505  0.27781427]]. Action = [[ 0.08938584  0.01809061 -0.03979284 -0.05578923]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 2674 is [True, False, False, False, False, True]
Current timestep = 2675. State = [[-0.1180653   0.28283703]]. Action = [[-0.15675534  0.08328104  0.05047813  0.06199205]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 2675 is [True, False, False, False, False, True]
Current timestep = 2676. State = [[-0.12523083  0.28699374]]. Action = [[-0.16771963 -0.01582427 -0.18283688 -0.6067143 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 2676 is [True, False, False, False, False, True]
Scene graph at timestep 2676 is [True, False, False, False, False, True]
State prediction error at timestep 2676 is tensor(0.0687, grad_fn=<MseLossBackward0>)
Current timestep = 2677. State = [[-0.13713321  0.28599828]]. Action = [[-0.23225974 -0.12524474  0.0930407   0.9811382 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 2677 is [True, False, False, False, False, True]
Current timestep = 2678. State = [[-0.1478398   0.28098997]]. Action = [[ 0.12657791 -0.03323905 -0.15263778 -0.57310766]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 2678 is [True, False, False, False, False, True]
Current timestep = 2679. State = [[-0.15026139  0.28075767]]. Action = [[-0.19066644  0.18477172 -0.17727934  0.35741305]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 2679 is [True, False, False, False, False, True]
Current timestep = 2680. State = [[-0.15725122  0.29023916]]. Action = [[-0.01600038  0.2195414  -0.10089041 -0.3678316 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 2680 is [True, False, False, False, False, True]
Current timestep = 2681. State = [[-0.16274776  0.30044588]]. Action = [[-0.1711783  -0.03512062 -0.13349967  0.276438  ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 2681 is [True, False, False, False, False, True]
Scene graph at timestep 2681 is [True, False, False, False, False, True]
State prediction error at timestep 2681 is tensor(0.0779, grad_fn=<MseLossBackward0>)
Current timestep = 2682. State = [[-0.16856652  0.29965565]]. Action = [[ 0.12094623 -0.22475435  0.24250019  0.96364665]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 2682 is [True, False, False, False, False, True]
Scene graph at timestep 2682 is [True, False, False, False, False, True]
State prediction error at timestep 2682 is tensor(0.0744, grad_fn=<MseLossBackward0>)
Current timestep = 2683. State = [[-0.1671456  0.2891067]]. Action = [[-0.01572777 -0.17002985 -0.10234338 -0.8542386 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 2683 is [True, False, False, False, False, True]
Current timestep = 2684. State = [[-0.16540681  0.2759332 ]]. Action = [[ 0.09462914 -0.2305296   0.18112886  0.540535  ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 2684 is [True, False, False, False, False, True]
Current timestep = 2685. State = [[-0.15885513  0.259946  ]]. Action = [[ 0.19948149 -0.23818348 -0.23155794  0.13374424]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 2685 is [True, False, False, False, False, True]
Current timestep = 2686. State = [[-0.15181105  0.2463769 ]]. Action = [[-0.14250942  0.00193083 -0.24831314 -0.7955977 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 2686 is [True, False, False, False, False, True]
Current timestep = 2687. State = [[-0.15090586  0.23907062]]. Action = [[ 0.1912669  -0.16404809 -0.20291644  0.4318732 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 2687 is [True, False, False, False, False, True]
Scene graph at timestep 2687 is [True, False, False, False, False, True]
State prediction error at timestep 2687 is tensor(0.0565, grad_fn=<MseLossBackward0>)
Current timestep = 2688. State = [[-0.14613557  0.23242313]]. Action = [[-0.09597161  0.07131562  0.19346714  0.21275723]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 2688 is [True, False, False, False, False, True]
Current timestep = 2689. State = [[-0.14425822  0.23413165]]. Action = [[ 0.21182758  0.11466995 -0.10778381  0.702803  ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 2689 is [True, False, False, False, False, True]
Current timestep = 2690. State = [[-0.13921522  0.2411285 ]]. Action = [[-0.17712823  0.16278791 -0.12369131 -0.4407581 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 2690 is [True, False, False, False, False, True]
Scene graph at timestep 2690 is [True, False, False, False, False, True]
State prediction error at timestep 2690 is tensor(0.0582, grad_fn=<MseLossBackward0>)
Current timestep = 2691. State = [[-0.1448692   0.25227466]]. Action = [[-0.12374668  0.20003724  0.04420859 -0.56361985]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 2691 is [True, False, False, False, False, True]
Current timestep = 2692. State = [[-0.14978395  0.2629625 ]]. Action = [[ 0.11741126  0.00714314 -0.12421411 -0.35360658]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 2692 is [True, False, False, False, False, True]
Current timestep = 2693. State = [[-0.145367    0.26318517]]. Action = [[ 0.14475876 -0.248765   -0.01291195 -0.7129658 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 2693 is [True, False, False, False, False, True]
Scene graph at timestep 2693 is [True, False, False, False, False, True]
State prediction error at timestep 2693 is tensor(0.0557, grad_fn=<MseLossBackward0>)
Current timestep = 2694. State = [[-0.14067766  0.2545301 ]]. Action = [[-0.13065666 -0.02991025 -0.07135689  0.78860104]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 2694 is [True, False, False, False, False, True]
Current timestep = 2695. State = [[-0.14583246  0.25385225]]. Action = [[-0.16754223  0.23803118 -0.10851364 -0.810299  ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 2695 is [True, False, False, False, False, True]
Current timestep = 2696. State = [[-0.15268488  0.26347294]]. Action = [[ 0.06248209  0.13094497 -0.01281065  0.70144486]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 2696 is [True, False, False, False, False, True]
Current timestep = 2697. State = [[-0.15255672  0.27419716]]. Action = [[ 0.04676151  0.17050171 -0.16127837  0.63283205]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 2697 is [True, False, False, False, False, True]
Current timestep = 2698. State = [[-0.1491754   0.28208345]]. Action = [[ 0.08399695 -0.09611967  0.01157159  0.53180194]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 2698 is [True, False, False, False, False, True]
Current timestep = 2699. State = [[-0.14486761  0.2825302 ]]. Action = [[ 0.02346194  0.04305816  0.15549558 -0.92509604]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 2699 is [True, False, False, False, False, True]
Current timestep = 2700. State = [[-0.14547709  0.28334525]]. Action = [[-0.21623388 -0.02658118 -0.02488938  0.01087904]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 2700 is [True, False, False, False, False, True]
Scene graph at timestep 2700 is [True, False, False, False, False, True]
State prediction error at timestep 2700 is tensor(0.0666, grad_fn=<MseLossBackward0>)
Current timestep = 2701. State = [[-0.15092467  0.2834389 ]]. Action = [[ 0.15936211  0.03496829 -0.198224    0.75572133]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 2701 is [True, False, False, False, False, True]
Current timestep = 2702. State = [[-0.14939615  0.28314903]]. Action = [[-0.11575085 -0.08348292  0.04818758  0.93778324]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 2702 is [True, False, False, False, False, True]
Current timestep = 2703. State = [[-0.15330036  0.2835658 ]]. Action = [[-0.05110681  0.19751403  0.10381985 -0.3019727 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 2703 is [True, False, False, False, False, True]
Scene graph at timestep 2703 is [True, False, False, False, False, True]
State prediction error at timestep 2703 is tensor(0.0698, grad_fn=<MseLossBackward0>)
Current timestep = 2704. State = [[-0.15444079  0.29065216]]. Action = [[ 0.14505729  0.04370886 -0.04805931  0.7820828 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 2704 is [True, False, False, False, False, True]
Current timestep = 2705. State = [[-0.15254337  0.29700583]]. Action = [[-0.17830743  0.13680291  0.20148152  0.16383386]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 2705 is [True, False, False, False, False, True]
Scene graph at timestep 2705 is [True, False, False, False, False, True]
State prediction error at timestep 2705 is tensor(0.0758, grad_fn=<MseLossBackward0>)
Current timestep = 2706. State = [[-0.16069648  0.30510193]]. Action = [[-0.21841756  0.09069234 -0.04862511 -0.92897356]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 2706 is [True, False, False, False, False, True]
Current timestep = 2707. State = [[-0.17098874  0.310719  ]]. Action = [[ 0.02714151 -0.01774417  0.22171378 -0.6352171 ]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 2707 is [True, False, False, False, False, True]
Scene graph at timestep 2707 is [True, False, False, False, False, True]
State prediction error at timestep 2707 is tensor(0.0753, grad_fn=<MseLossBackward0>)
Current timestep = 2708. State = [[-0.17431526  0.31113532]]. Action = [[-0.02738446 -0.06316096 -0.18759538 -0.679761  ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 2708 is [True, False, False, False, False, True]
Current timestep = 2709. State = [[-0.17617273  0.30913824]]. Action = [[ 0.11097074  0.16574049 -0.06193468  0.41012943]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 2709 is [True, False, False, False, False, True]
Current timestep = 2710. State = [[-0.17369092  0.30924034]]. Action = [[ 0.2190522   0.05863243 -0.10879512 -0.11763901]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 2710 is [True, False, False, False, False, True]
Scene graph at timestep 2710 is [True, False, False, False, False, True]
State prediction error at timestep 2710 is tensor(0.0835, grad_fn=<MseLossBackward0>)
Current timestep = 2711. State = [[-0.16662666  0.30940518]]. Action = [[-0.10272598 -0.11176184  0.07173306  0.8657315 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 2711 is [True, False, False, False, False, True]
Current timestep = 2712. State = [[-0.16735272  0.3060275 ]]. Action = [[-0.1472658   0.16770583  0.08537614  0.62803364]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 2712 is [True, False, False, False, False, True]
Current timestep = 2713. State = [[-0.16801211  0.30461034]]. Action = [[-0.11547668  0.18903083 -0.21335107  0.16204894]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 2713 is [True, False, False, False, False, True]
Current timestep = 2714. State = [[-0.16837926  0.30324244]]. Action = [[-0.01548858 -0.05864719  0.13805419  0.96322906]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 2714 is [True, False, False, False, False, True]
Current timestep = 2715. State = [[-0.16560172  0.2982125 ]]. Action = [[ 0.23394895 -0.17040013 -0.03697795  0.056301  ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 2715 is [True, False, False, False, False, True]
Current timestep = 2716. State = [[-0.15479049  0.2928296 ]]. Action = [[0.15275377 0.13019824 0.04078421 0.3213141 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 2716 is [True, False, False, False, False, True]
Current timestep = 2717. State = [[-0.14674954  0.29829586]]. Action = [[-0.10007709  0.21279651  0.0852468  -0.588984  ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 2717 is [True, False, False, False, False, True]
Current timestep = 2718. State = [[-0.14885482  0.3035548 ]]. Action = [[-0.13665402 -0.24985138 -0.1353871   0.82860947]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 2718 is [True, False, False, False, False, True]
Current timestep = 2719. State = [[-0.15803277  0.29684803]]. Action = [[-0.21277927 -0.04621562  0.0352681  -0.8036489 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 2719 is [True, False, False, False, False, True]
Current timestep = 2720. State = [[-0.17144653  0.29317868]]. Action = [[-0.18287684  0.04778123 -0.09366523 -0.05546844]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 2720 is [True, False, False, False, False, True]
Current timestep = 2721. State = [[-0.18245223  0.29123625]]. Action = [[-0.00459674 -0.14057231 -0.04071191  0.1951487 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 2721 is [True, False, False, False, False, True]
Scene graph at timestep 2721 is [True, False, False, False, False, True]
State prediction error at timestep 2721 is tensor(0.0747, grad_fn=<MseLossBackward0>)
Current timestep = 2722. State = [[-0.18609817  0.28716937]]. Action = [[ 0.04237625  0.06297487 -0.08239406  0.66831446]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 2722 is [True, False, False, False, False, True]
Current timestep = 2723. State = [[-0.1836115   0.28802127]]. Action = [[ 0.13623375  0.02825165 -0.06472579 -0.02923751]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 2723 is [True, False, False, False, False, True]
Current timestep = 2724. State = [[-0.18119387  0.2906998 ]]. Action = [[-0.22491007  0.08168384 -0.04165138 -0.03341711]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 2724 is [True, False, False, False, False, True]
Current timestep = 2725. State = [[-0.18636562  0.2935322 ]]. Action = [[ 0.08785978 -0.05269627  0.12480646  0.692173  ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 2725 is [True, False, False, False, False, True]
Scene graph at timestep 2725 is [True, False, False, False, False, True]
State prediction error at timestep 2725 is tensor(0.0755, grad_fn=<MseLossBackward0>)
Current timestep = 2726. State = [[-0.18343824  0.28954443]]. Action = [[ 0.15699455 -0.21156631  0.21662426  0.8399378 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 2726 is [True, False, False, False, False, True]
Current timestep = 2727. State = [[-0.17571683  0.2816149 ]]. Action = [[ 0.08933142  0.02960017 -0.11393462 -0.481462  ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 2727 is [True, False, False, False, False, True]
Current timestep = 2728. State = [[-0.16828205  0.28267393]]. Action = [[ 0.09976     0.20199338 -0.12953168 -0.6240837 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 2728 is [True, False, False, False, False, True]
Current timestep = 2729. State = [[-0.1618478  0.2909046]]. Action = [[ 0.0206348   0.07187125  0.16114846 -0.81028014]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 2729 is [True, False, False, False, False, True]
Scene graph at timestep 2729 is [True, False, False, False, False, True]
State prediction error at timestep 2729 is tensor(0.0702, grad_fn=<MseLossBackward0>)
Current timestep = 2730. State = [[-0.15816584  0.29726124]]. Action = [[ 0.04711363  0.04891661 -0.13984099 -0.123294  ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 2730 is [True, False, False, False, False, True]
Current timestep = 2731. State = [[-0.15372479  0.29887602]]. Action = [[ 0.10864991 -0.13867147 -0.20010921 -0.27159083]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 2731 is [True, False, False, False, False, True]
Current timestep = 2732. State = [[-0.1501609   0.29371387]]. Action = [[-0.10970353 -0.07639807 -0.18290249  0.46640635]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 2732 is [True, False, False, False, False, True]
Current timestep = 2733. State = [[-0.14930964  0.28768384]]. Action = [[ 0.21986496 -0.09589347 -0.15384988 -0.771493  ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 2733 is [True, False, False, False, False, True]
Current timestep = 2734. State = [[-0.13965295  0.27864555]]. Action = [[ 0.15177858 -0.23414394  0.07826731 -0.61388224]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 2734 is [True, False, False, False, False, True]
Current timestep = 2735. State = [[-0.1322794  0.266059 ]]. Action = [[-0.10291085 -0.11199331  0.1315575   0.13192165]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 2735 is [True, False, False, False, False, True]
Scene graph at timestep 2735 is [True, False, False, False, False, True]
State prediction error at timestep 2735 is tensor(0.0560, grad_fn=<MseLossBackward0>)
Current timestep = 2736. State = [[-0.1314597   0.25560454]]. Action = [[ 0.10817558 -0.14071918 -0.13735467  0.83427334]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 2736 is [True, False, False, False, False, True]
Scene graph at timestep 2736 is [True, False, False, False, False, True]
State prediction error at timestep 2736 is tensor(0.0576, grad_fn=<MseLossBackward0>)
Current timestep = 2737. State = [[-0.12991896  0.24785325]]. Action = [[-0.13224347  0.03246784 -0.20830768 -0.8728494 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 2737 is [True, False, False, False, False, True]
Current timestep = 2738. State = [[-0.13522962  0.24292704]]. Action = [[-0.12166566 -0.20208435 -0.12881853  0.84891343]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 2738 is [True, False, False, False, False, True]
Current timestep = 2739. State = [[-0.14496654  0.23127739]]. Action = [[-0.22831303 -0.22663382  0.00279811  0.30952   ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 2739 is [True, False, False, False, False, True]
Current timestep = 2740. State = [[-0.15655823  0.2224065 ]]. Action = [[-0.00639266  0.17784548 -0.13500512 -0.89631915]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 2740 is [True, False, False, False, False, True]
Current timestep = 2741. State = [[-0.16213946  0.22810207]]. Action = [[-0.08305806  0.21098316  0.00132397 -0.52309   ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 2741 is [True, False, False, False, False, True]
Current timestep = 2742. State = [[-0.16945511  0.23866504]]. Action = [[-0.19989821  0.08089206 -0.24165092  0.3300705 ]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 2742 is [True, False, False, False, False, True]
Current timestep = 2743. State = [[-0.17554107  0.24169455]]. Action = [[ 0.2173171  -0.22615714 -0.01916468  0.809443  ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 2743 is [True, False, False, False, False, True]
Current timestep = 2744. State = [[-0.1716328   0.23267554]]. Action = [[-0.06194398 -0.17521998 -0.06371778  0.38511384]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 2744 is [True, False, False, False, False, True]
Scene graph at timestep 2744 is [True, False, False, False, False, True]
State prediction error at timestep 2744 is tensor(0.0556, grad_fn=<MseLossBackward0>)
Current timestep = 2745. State = [[-0.17203291  0.22159494]]. Action = [[-0.01885757 -0.11681618  0.21312794  0.79781616]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 2745 is [True, False, False, False, False, True]
Current timestep = 2746. State = [[-0.17605266  0.21354118]]. Action = [[-0.20571549 -0.01327379  0.23606387  0.4932841 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 2746 is [True, False, False, False, False, True]
Current timestep = 2747. State = [[-0.18251479  0.21159059]]. Action = [[0.1308446  0.08563703 0.199556   0.2912644 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 2747 is [True, False, False, False, False, True]
Scene graph at timestep 2747 is [True, False, False, False, False, True]
State prediction error at timestep 2747 is tensor(0.0532, grad_fn=<MseLossBackward0>)
Current timestep = 2748. State = [[-0.181798    0.21674518]]. Action = [[-0.07297203  0.18819565  0.1898897   0.7836579 ]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 2748 is [True, False, False, False, False, True]
Scene graph at timestep 2748 is [True, False, False, False, False, True]
State prediction error at timestep 2748 is tensor(0.0588, grad_fn=<MseLossBackward0>)
Current timestep = 2749. State = [[-0.18172814  0.22171675]]. Action = [[ 0.11317849 -0.19645646  0.08772579 -0.39988542]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 2749 is [True, False, False, False, False, True]
Current timestep = 2750. State = [[-0.1794102   0.21768013]]. Action = [[-0.08099245  0.01817563 -0.03207526  0.9710295 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 2750 is [True, False, False, False, False, True]
Scene graph at timestep 2750 is [True, False, False, False, False, True]
State prediction error at timestep 2750 is tensor(0.0570, grad_fn=<MseLossBackward0>)
Current timestep = 2751. State = [[-0.17879829  0.21674992]]. Action = [[ 0.15366158  0.02125105  0.22411302 -0.5641043 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 2751 is [True, False, False, False, False, True]
Current timestep = 2752. State = [[-0.17604655  0.21438871]]. Action = [[-0.17036258 -0.17660595  0.01710212  0.1178894 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 2752 is [True, False, False, False, False, True]
Current timestep = 2753. State = [[-0.18146752  0.20996943]]. Action = [[-0.05573407  0.12670654  0.14902765  0.27799237]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 2753 is [True, False, False, False, False, True]
Current timestep = 2754. State = [[-0.18614203  0.20996265]]. Action = [[-0.04070131 -0.13973406  0.12549207 -0.40494668]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 2754 is [True, False, False, False, False, True]
Current timestep = 2755. State = [[-0.18892996  0.2083016 ]]. Action = [[0.02470616 0.15931395 0.1318835  0.603421  ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 2755 is [True, False, False, False, False, True]
Scene graph at timestep 2755 is [True, False, False, False, False, True]
State prediction error at timestep 2755 is tensor(0.0563, grad_fn=<MseLossBackward0>)
Current timestep = 2756. State = [[-0.18788035  0.21587512]]. Action = [[ 0.07861075  0.20721489 -0.05367941 -0.64952296]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 2756 is [True, False, False, False, False, True]
Current timestep = 2757. State = [[-0.1835566   0.22379644]]. Action = [[ 0.07271063 -0.10957696 -0.21804796  0.2390753 ]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 2757 is [True, False, False, False, False, True]
Current timestep = 2758. State = [[-0.17737646  0.22116798]]. Action = [[ 0.14264435 -0.13429075  0.0206944  -0.03368843]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 2758 is [True, False, False, False, False, True]
Current timestep = 2759. State = [[-0.17302792  0.21908079]]. Action = [[-0.17287557  0.22047192  0.10946801 -0.82009655]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 2759 is [True, False, False, False, False, True]
Scene graph at timestep 2759 is [True, False, False, False, False, True]
State prediction error at timestep 2759 is tensor(0.0548, grad_fn=<MseLossBackward0>)
Current timestep = 2760. State = [[-0.18001023  0.22618969]]. Action = [[-0.20452455  0.05899861 -0.09758453 -0.68190455]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 2760 is [True, False, False, False, False, True]
Current timestep = 2761. State = [[-0.19146824  0.22725683]]. Action = [[-0.11606169 -0.23474136 -0.08080406 -0.7378843 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 2761 is [True, False, False, False, False, True]
Scene graph at timestep 2761 is [True, False, False, False, False, True]
State prediction error at timestep 2761 is tensor(0.0492, grad_fn=<MseLossBackward0>)
Current timestep = 2762. State = [[-0.19947867  0.22048523]]. Action = [[ 0.02711201  0.024719   -0.02146366 -0.04620987]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 2762 is [True, False, False, False, False, True]
Current timestep = 2763. State = [[-0.20415194  0.21897417]]. Action = [[-0.19554631  0.03151944  0.09152856 -0.36245006]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 2763 is [True, False, False, False, False, True]
Scene graph at timestep 2763 is [True, False, False, False, False, True]
State prediction error at timestep 2763 is tensor(0.0551, grad_fn=<MseLossBackward0>)
Current timestep = 2764. State = [[-0.20920295  0.21633595]]. Action = [[ 0.19063935 -0.20346577 -0.00977957 -0.6063938 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 2764 is [True, False, False, False, False, True]
Current timestep = 2765. State = [[-0.20711778  0.20606212]]. Action = [[-0.14491849 -0.18420926 -0.13978323 -0.95038116]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 2765 is [True, False, False, False, False, True]
Current timestep = 2766. State = [[-0.21020584  0.1946252 ]]. Action = [[ 0.0375976  -0.101312    0.10628599 -0.87822706]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 2766 is [True, False, False, False, False, True]
Scene graph at timestep 2766 is [True, False, False, False, False, True]
State prediction error at timestep 2766 is tensor(0.0440, grad_fn=<MseLossBackward0>)
Current timestep = 2767. State = [[-0.21384755  0.1868443 ]]. Action = [[-0.2366636  -0.02072668 -0.1097993  -0.6255229 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 2767 is [True, False, False, False, False, True]
Current timestep = 2768. State = [[-0.22163713  0.18317239]]. Action = [[ 0.08498794 -0.01428741 -0.21624449 -0.22105908]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 2768 is [True, False, False, False, False, True]
Current timestep = 2769. State = [[-0.21951006  0.18002672]]. Action = [[ 0.1654172  -0.09173945 -0.00236765  0.6342242 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 2769 is [True, False, False, False, False, True]
Current timestep = 2770. State = [[-0.20954129  0.17717835]]. Action = [[ 0.22754472  0.07444414 -0.13653871 -0.8909727 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 2770 is [True, False, False, False, False, True]
Current timestep = 2771. State = [[-0.1950276   0.18004498]]. Action = [[ 0.20526746  0.10078418 -0.15105355 -0.7761109 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 2771 is [True, False, False, False, False, True]
Current timestep = 2772. State = [[-0.1849268   0.18610522]]. Action = [[-0.1484863   0.09914422 -0.04046562  0.13627315]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 2772 is [True, False, False, False, False, True]
Current timestep = 2773. State = [[-0.18324968  0.1907161 ]]. Action = [[ 0.20336533 -0.05503941 -0.18318318  0.81789136]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 2773 is [True, False, False, False, False, True]
Scene graph at timestep 2773 is [True, False, False, False, False, True]
State prediction error at timestep 2773 is tensor(0.0502, grad_fn=<MseLossBackward0>)
Current timestep = 2774. State = [[-0.17724471  0.19140075]]. Action = [[-0.05272579  0.04882327 -0.24186508 -0.6613146 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 2774 is [True, False, False, False, False, True]
Scene graph at timestep 2774 is [True, False, False, False, False, True]
State prediction error at timestep 2774 is tensor(0.0502, grad_fn=<MseLossBackward0>)
Current timestep = 2775. State = [[-0.17288162  0.19165711]]. Action = [[ 2.42039502e-01 -8.71033669e-02 -1.08376145e-04 -9.18259442e-01]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 2775 is [True, False, False, False, False, True]
Scene graph at timestep 2775 is [True, False, False, False, False, True]
State prediction error at timestep 2775 is tensor(0.0439, grad_fn=<MseLossBackward0>)
Current timestep = 2776. State = [[-0.16417848  0.19042574]]. Action = [[-0.0424196   0.09050122  0.17328426 -0.7407212 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 2776 is [True, False, False, False, False, True]
Current timestep = 2777. State = [[-0.1626168   0.19385615]]. Action = [[-0.03797412  0.06746256  0.23026353 -0.9968195 ]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 2777 is [True, False, False, False, False, True]
Scene graph at timestep 2777 is [True, False, False, False, False, True]
State prediction error at timestep 2777 is tensor(0.0416, grad_fn=<MseLossBackward0>)
Current timestep = 2778. State = [[-0.16428751  0.1960589 ]]. Action = [[-0.04502898 -0.08564629 -0.06166811 -0.24254602]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 2778 is [True, False, False, False, False, True]
Current timestep = 2779. State = [[-0.16367735  0.19150852]]. Action = [[ 0.19486117 -0.16580741  0.14870471  0.97712564]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 2779 is [True, False, False, False, False, True]
Current timestep = 2780. State = [[-0.15716222  0.18761586]]. Action = [[-0.00447239  0.20571521  0.00022289  0.10792255]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 2780 is [True, False, False, False, False, True]
Current timestep = 2781. State = [[-0.15467443  0.19287972]]. Action = [[-0.01440474  0.01366857 -0.24310434  0.02683735]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 2781 is [True, False, False, False, False, True]
Scene graph at timestep 2781 is [True, False, False, False, False, True]
State prediction error at timestep 2781 is tensor(0.0494, grad_fn=<MseLossBackward0>)
Current timestep = 2782. State = [[-0.15618604  0.19550736]]. Action = [[-0.11688089 -0.00707355  0.21395835  0.3562752 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 2782 is [True, False, False, False, False, True]
Current timestep = 2783. State = [[-0.15862945  0.19521186]]. Action = [[ 0.14428353 -0.05846436 -0.2295374  -0.06932986]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 2783 is [True, False, False, False, False, True]
Current timestep = 2784. State = [[-0.1569983   0.19157632]]. Action = [[-0.1447819  -0.09983513 -0.16326983 -0.329854  ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 2784 is [True, False, False, False, False, True]
Current timestep = 2785. State = [[-0.16035943  0.19009697]]. Action = [[ 0.04186377  0.19500732 -0.23988451  0.5013808 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 2785 is [True, False, False, False, False, True]
Current timestep = 2786. State = [[-0.16308011  0.19196141]]. Action = [[-0.18539678 -0.23610249 -0.01056735 -0.90174854]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 2786 is [True, False, False, False, False, True]
Current timestep = 2787. State = [[-0.16708556  0.18813153]]. Action = [[ 0.21611947  0.1708484  -0.2308642  -0.04746723]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 2787 is [True, False, False, False, False, True]
Current timestep = 2788. State = [[-0.15992355  0.18909292]]. Action = [[ 0.10367522 -0.16929632  0.06688792  0.5159576 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 2788 is [True, False, False, False, False, True]
Current timestep = 2789. State = [[-0.15664054  0.18389419]]. Action = [[-0.22127925 -0.03025207 -0.06880017  0.36519408]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 2789 is [True, False, False, False, False, True]
Scene graph at timestep 2789 is [True, False, False, False, False, True]
State prediction error at timestep 2789 is tensor(0.0436, grad_fn=<MseLossBackward0>)
Current timestep = 2790. State = [[-0.16267315  0.17863   ]]. Action = [[ 0.01140589 -0.13656977  0.01503372 -0.5453553 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 2790 is [True, False, False, False, False, True]
Scene graph at timestep 2790 is [True, False, False, False, False, True]
State prediction error at timestep 2790 is tensor(0.0349, grad_fn=<MseLossBackward0>)
Current timestep = 2791. State = [[-0.16406663  0.17193294]]. Action = [[ 0.06683996 -0.02982615 -0.08725223 -0.8190418 ]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 2791 is [True, False, False, False, False, True]
Current timestep = 2792. State = [[-0.15988764  0.1659815 ]]. Action = [[ 0.14923903 -0.154531   -0.23361927 -0.3836245 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 2792 is [True, False, False, False, False, True]
Current timestep = 2793. State = [[-0.15502292  0.15792842]]. Action = [[-0.12028325 -0.06506184  0.06871814  0.7454214 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 2793 is [True, False, False, False, False, True]
Scene graph at timestep 2793 is [True, False, False, False, False, True]
State prediction error at timestep 2793 is tensor(0.0358, grad_fn=<MseLossBackward0>)
Current timestep = 2794. State = [[-0.16004844  0.15162405]]. Action = [[-0.20109314 -0.07766944  0.15516394 -0.51389384]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 2794 is [True, False, False, False, False, True]
Scene graph at timestep 2794 is [True, False, False, False, False, True]
State prediction error at timestep 2794 is tensor(0.0298, grad_fn=<MseLossBackward0>)
Current timestep = 2795. State = [[-0.16843201  0.14622213]]. Action = [[ 0.03826365 -0.04402658 -0.15191846 -0.25842887]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 2795 is [True, False, False, False, False, True]
Current timestep = 2796. State = [[-0.168129   0.1410649]]. Action = [[ 0.15480924 -0.11272165  0.02737227  0.8152212 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 2796 is [True, False, False, False, False, True]
Current timestep = 2797. State = [[-0.16273674  0.13915105]]. Action = [[-0.01146547  0.22233033  0.0041908   0.80981326]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 2797 is [True, False, False, False, False, True]
Current timestep = 2798. State = [[-0.1630861  0.1418321]]. Action = [[-0.15916805 -0.23238668 -0.22538833 -0.03186578]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 2798 is [True, False, False, False, False, True]
Current timestep = 2799. State = [[-0.1713182   0.13570277]]. Action = [[-0.16827084 -0.00469951 -0.10278779 -0.42569113]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 2799 is [True, False, False, False, False, True]
Scene graph at timestep 2799 is [True, False, False, False, False, True]
State prediction error at timestep 2799 is tensor(0.0332, grad_fn=<MseLossBackward0>)
