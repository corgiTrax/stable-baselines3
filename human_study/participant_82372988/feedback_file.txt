Current timestep = 0. State = [[-0.32604954 -0.08628346]]. Action = [[0.00435984 0.09830839 0.         0.7271644 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0668, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.33171427 -0.08338373]]. Action = [[-0.08950701 -0.00990611  0.          0.9261981 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0582, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.33283383 -0.08530019]]. Action = [[ 0.05919518 -0.05022885  0.          0.01753342]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0498, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.32827216 -0.08361269]]. Action = [[ 0.07941671  0.04839007  0.         -0.508215  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0282, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of -1
Current timestep = 4. State = [[-0.32541144 -0.08398328]]. Action = [[ 0.02240024 -0.05187739  0.          0.0605756 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0375, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.32796615 -0.0831292 ]]. Action = [[-0.0650194   0.04303385  0.         -0.7176986 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of 1
Current timestep = 6. State = [[-0.33477747 -0.08681658]]. Action = [[-0.09400102 -0.09524931  0.         -0.5530466 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of 0
Current timestep = 7. State = [[-0.34229743 -0.08951548]]. Action = [[-0.08571365  0.01568285  0.          0.7501471 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0277, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.34299242 -0.09307019]]. Action = [[ 0.06538839 -0.06981377  0.          0.35922492]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0267, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.33896387 -0.09811388]]. Action = [[ 0.05114711 -0.04872011  0.          0.96607447]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0148, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of -1
Current timestep = 10. State = [[-0.34125662 -0.09593562]]. Action = [[-0.0936795   0.09747537  0.          0.9562638 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of 1
Current timestep = 11. State = [[-0.347937   -0.08932574]]. Action = [[-0.06636304  0.0879086   0.         -0.7748558 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of 1
Current timestep = 12. State = [[-0.35245895 -0.09056526]]. Action = [[-0.02061976 -0.0799693   0.         -0.95386094]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.35142738 -0.09695148]]. Action = [[ 0.06854368 -0.08356377  0.          0.7989632 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.34906027 -0.10318851]]. Action = [[ 0.03605657 -0.0731989   0.          0.99145484]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of 1
Current timestep = 15. State = [[-0.3491582  -0.10754307]]. Action = [[-0.00919399 -0.03286369  0.         -0.21517026]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of 1
Current timestep = 16. State = [[-0.35159686 -0.11430893]]. Action = [[-0.03362578 -0.09509318  0.         -0.46856833]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.35041428 -0.11493057]]. Action = [[ 0.06311332  0.07901955  0.         -0.04884696]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of 1
Current timestep = 18. State = [[-0.35247672 -0.11035859]]. Action = [[-0.07430334  0.06387223  0.         -0.8008181 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 18 of 1
Current timestep = 19. State = [[-0.35576934 -0.10814585]]. Action = [[-0.01476232  0.01620017  0.         -0.6163686 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(8.2106e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 19 of -1
Current timestep = 20. State = [[-0.3527969  -0.10618698]]. Action = [[0.0859086  0.02489357 0.         0.30391502]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 20 of -1
Current timestep = 21. State = [[-0.35510543 -0.10476934]]. Action = [[-0.09338522  0.00967163  0.          0.02597177]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 21 of 1
Current timestep = 22. State = [[-0.3558565 -0.1086238]]. Action = [[ 0.06760686 -0.09258603  0.         -0.5443939 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.35022223 -0.11564177]]. Action = [[ 0.08779662 -0.08991017  0.          0.19433308]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 23 of -1
Current timestep = 24. State = [[-0.34377527 -0.1225692 ]]. Action = [[ 0.0724557  -0.08296435  0.         -0.63268447]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 24 of 1
Current timestep = 25. State = [[-0.33786282 -0.12889537]]. Action = [[ 0.05780076 -0.06582139  0.         -0.71202564]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, True, False, False]
State prediction error at timestep 25 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.3341107  -0.13460898]]. Action = [[ 0.01661585 -0.05225054  0.         -0.50149906]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, True, False, False]
State prediction error at timestep 26 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 26 of 1
Current timestep = 27. State = [[-0.32861793 -0.14018314]]. Action = [[ 0.07365342 -0.04873691  0.         -0.8403323 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, True, False, False]
State prediction error at timestep 27 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 27 of 1
Current timestep = 28. State = [[-0.32853043 -0.14039958]]. Action = [[-0.08001557  0.07215705  0.          0.7055719 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, True, False, False]
State prediction error at timestep 28 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 28 of 1
Current timestep = 29. State = [[-0.32660982 -0.13691859]]. Action = [[ 0.07606304  0.05757519  0.         -0.21326494]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, True, False, False]
State prediction error at timestep 29 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 29 of 1
Current timestep = 30. State = [[-0.3223505  -0.13347417]]. Action = [[0.03571806 0.04255299 0.         0.9099513 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, True, False, False]
State prediction error at timestep 30 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 30 of 1
Current timestep = 31. State = [[-0.32131797 -0.12955974]]. Action = [[-0.00947033  0.05546027  0.         -0.69560695]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, True, False, False]
State prediction error at timestep 31 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.31705108 -0.13114032]]. Action = [[ 0.09093369 -0.07165007  0.         -0.03030491]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, True, False, False]
State prediction error at timestep 32 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 32 of 1
Current timestep = 33. State = [[-0.31500593 -0.13016509]]. Action = [[-0.03279749  0.06480945  0.         -0.44897437]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, True, False, False]
State prediction error at timestep 33 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 33 of 1
Current timestep = 34. State = [[-0.317425   -0.12553489]]. Action = [[-0.03772257  0.0554461   0.         -0.98411494]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, True, False, False]
State prediction error at timestep 34 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 34 of 1
Current timestep = 35. State = [[-0.31641144 -0.12461003]]. Action = [[ 0.04452584 -0.0261028   0.          0.7215905 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.31868047 -0.12723799]]. Action = [[-0.07528131 -0.04712501  0.         -0.94116336]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, True, False, False]
State prediction error at timestep 36 is tensor(6.9230e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 36 of 1
Current timestep = 37. State = [[-0.32197866 -0.13318212]]. Action = [[-0.02735583 -0.08648137  0.         -0.6851636 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, True, False, False]
State prediction error at timestep 37 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 37 of -1
Current timestep = 38. State = [[-0.32162875 -0.13477327]]. Action = [[ 0.02136596  0.02731981  0.         -0.7006999 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, True, False, False]
State prediction error at timestep 38 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 38 of -1
Current timestep = 39. State = [[-0.3249747  -0.13068232]]. Action = [[-0.08704504  0.07488611  0.         -0.53043926]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, True, False, False]
State prediction error at timestep 39 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 39 of -1
Current timestep = 40. State = [[-0.32519108 -0.13203436]]. Action = [[ 0.05906571 -0.07919733  0.         -0.5106552 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, True, False, False]
State prediction error at timestep 40 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[-0.31829992 -0.13600896]]. Action = [[ 0.09650803 -0.03557142  0.         -0.46227455]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, True, False, False]
State prediction error at timestep 41 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 41 of -1
Current timestep = 42. State = [[-0.31280345 -0.13229765]]. Action = [[ 0.0317275   0.09646463  0.         -0.70922565]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.31194472 -0.13107228]]. Action = [[-0.01806988 -0.03452919  0.          0.881155  ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, True, False, False]
State prediction error at timestep 43 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.3100039  -0.13135146]]. Action = [[ 0.03889859  0.01301794  0.         -0.3277173 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, True, False, False]
State prediction error at timestep 44 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 44 of 1
Current timestep = 45. State = [[-0.3058166  -0.13293101]]. Action = [[ 0.05210984 -0.04091416  0.          0.5086813 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, True, False, False]
State prediction error at timestep 45 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 45 of 1
Current timestep = 46. State = [[-0.30322078 -0.13098262]]. Action = [[0.00805924 0.06132948 0.         0.56975925]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, True, False, False]
State prediction error at timestep 46 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 46 of 1
Current timestep = 47. State = [[-0.30410588 -0.12463465]]. Action = [[-0.03444599  0.09068585  0.         -0.25386465]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.30142462 -0.12376364]]. Action = [[ 0.07389016 -0.05063499  0.         -0.7492084 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(2.2595e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 48 of 1
Current timestep = 49. State = [[-0.29523966 -0.12083274]]. Action = [[ 0.06729896  0.07222939  0.         -0.48797393]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 49 of 1
Current timestep = 50. State = [[-0.29028478 -0.1139145 ]]. Action = [[ 0.04206853  0.07745887  0.         -0.8591104 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 50 of 1
Current timestep = 51. State = [[-0.29289567 -0.11016572]]. Action = [[-0.09524187  0.00883991  0.          0.36265266]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 51 of 1
Current timestep = 52. State = [[-0.2967499  -0.11345706]]. Action = [[-0.03709531 -0.08275785  0.         -0.52078533]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.30107147 -0.11712522]]. Action = [[-0.07561423 -0.02692529  0.         -0.35096455]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
State prediction error at timestep 53 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 53 of -1
Current timestep = 54. State = [[-0.30087593 -0.12218341]]. Action = [[ 0.0407184  -0.08230476  0.         -0.06719959]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 54 of -1
Current timestep = 55. State = [[-0.30180976 -0.12853959]]. Action = [[-0.05558755 -0.07007192  0.          0.26421094]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, True, False, False]
State prediction error at timestep 55 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 55 of -1
Current timestep = 56. State = [[-0.30360165 -0.1346371 ]]. Action = [[-0.01982187 -0.05848297  0.         -0.2868439 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, True, False, False]
State prediction error at timestep 56 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 56 of -1
Current timestep = 57. State = [[-0.30711707 -0.13476467]]. Action = [[-0.07008545  0.06071966  0.          0.8532826 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, True, False, False]
State prediction error at timestep 57 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 57 of -1
Current timestep = 58. State = [[-0.3056856  -0.13059065]]. Action = [[0.07052327 0.06656683 0.         0.19503653]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, True, False, False]
State prediction error at timestep 58 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 58 of -1
Current timestep = 59. State = [[-0.3043743  -0.12534717]]. Action = [[-0.01229422  0.06553014  0.         -0.61663127]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, True, False, False]
State prediction error at timestep 59 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 59 of -1
Current timestep = 60. State = [[-0.3022089  -0.12417253]]. Action = [[ 0.05442966 -0.02271769  0.          0.37900794]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
State prediction error at timestep 60 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 60 of 1
Current timestep = 61. State = [[-0.30511802 -0.1252793 ]]. Action = [[-0.08805075 -0.01184581  0.         -0.6476592 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, True, False, False]
State prediction error at timestep 61 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 61 of 1
Current timestep = 62. State = [[-0.30696398 -0.12762581]]. Action = [[ 0.02498291 -0.03744888  0.          0.76495266]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, True, False, False]
State prediction error at timestep 62 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.30491218 -0.1280657 ]]. Action = [[ 0.04082996  0.01071249  0.         -0.54617596]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, True, False, False]
State prediction error at timestep 63 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 63 of -1
Current timestep = 64. State = [[-0.30673566 -0.12822753]]. Action = [[-0.0526388  -0.01197345  0.          0.13989305]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, True, False, False]
State prediction error at timestep 64 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 64 of -1
Current timestep = 65. State = [[-0.31059614 -0.1288062 ]]. Action = [[-3.9597522e-02 -5.3716451e-04  0.0000000e+00  5.7604349e-01]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, True, False, False]
State prediction error at timestep 65 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 65 of -1
Current timestep = 66. State = [[-0.31694394 -0.13154747]]. Action = [[-0.0927123  -0.04529658  0.          0.8100915 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, True, False, False]
State prediction error at timestep 66 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 66 of -1
Current timestep = 67. State = [[-0.32304332 -0.13828255]]. Action = [[-0.05298885 -0.09275126  0.         -0.90174454]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, True, False, False]
State prediction error at timestep 67 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.32347444 -0.14376594]]. Action = [[ 0.04204471 -0.03787021  0.         -0.04088843]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, True, False, False]
State prediction error at timestep 68 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 68 of -1
Current timestep = 69. State = [[-0.32035357 -0.145055  ]]. Action = [[ 0.0516902   0.00882889  0.         -0.09340698]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, True, False, False]
State prediction error at timestep 69 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 69 of -1
Current timestep = 70. State = [[-0.31506425 -0.14763446]]. Action = [[ 0.08332544 -0.05220137  0.         -0.72702676]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, True, False, False]
State prediction error at timestep 70 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 70 of 1
Current timestep = 71. State = [[-0.31649542 -0.1515249 ]]. Action = [[-0.08011484 -0.0328217   0.          0.5159339 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, True, False, False]
State prediction error at timestep 71 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 71 of 1
Current timestep = 72. State = [[-0.3223677  -0.15468909]]. Action = [[-0.05849033 -0.02071813  0.         -0.6477884 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, True, False, False]
State prediction error at timestep 72 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 72 of -1
Current timestep = 73. State = [[-0.32269505 -0.15743303]]. Action = [[ 0.04953132 -0.02108718  0.          0.81359863]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, True, False, False]
State prediction error at timestep 73 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-0.3230238  -0.16321819]]. Action = [[-0.02026118 -0.0863732   0.          0.626626  ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, True, False, False]
State prediction error at timestep 74 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.32059965 -0.16801846]]. Action = [[ 0.06937046 -0.02517717  0.          0.05248284]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, True, False, False]
State prediction error at timestep 75 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.31479537 -0.17140082]]. Action = [[ 0.0791321  -0.03607316  0.          0.1245991 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, True, False, False]
State prediction error at timestep 76 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 76 of 1
Current timestep = 77. State = [[-0.31506974 -0.17493597]]. Action = [[-0.05710145 -0.02849147  0.          0.68987274]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, True, False, False]
State prediction error at timestep 77 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 77 of 1
Current timestep = 78. State = [[-0.31281617 -0.17283349]]. Action = [[ 0.08121193  0.08764491  0.         -0.9029387 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, True, False, False]
State prediction error at timestep 78 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 78 of 1
Current timestep = 79. State = [[-0.31145167 -0.17039587]]. Action = [[-0.01694228  0.01065417  0.          0.4120407 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, True, False, False]
State prediction error at timestep 79 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 79 of 1
Current timestep = 80. State = [[-0.30802193 -0.1733389 ]]. Action = [[ 0.08106718 -0.06125671  0.         -0.74395007]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, True, False, False]
State prediction error at timestep 80 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 80 of 1
Current timestep = 81. State = [[-0.3092608  -0.17181152]]. Action = [[-0.08759603  0.09073531  0.         -0.42126954]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, True, False, False]
State prediction error at timestep 81 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 81 of 1
Current timestep = 82. State = [[-0.3108964 -0.1670465]]. Action = [[ 0.03276426  0.04493595  0.         -0.40753424]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, True, False, False]
State prediction error at timestep 82 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 82 of 1
Current timestep = 83. State = [[-0.3082738  -0.16685763]]. Action = [[ 0.05427503 -0.04246703  0.          0.6591883 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, True, False, False]
State prediction error at timestep 83 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 83 of 1
Current timestep = 84. State = [[-0.30932105 -0.1630062 ]]. Action = [[-0.04595318  0.08996891  0.          0.34185648]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, True, False, False]
State prediction error at timestep 84 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 84 of 1
Current timestep = 85. State = [[-0.31648993 -0.16075793]]. Action = [[-0.09457217 -0.02056873  0.         -0.27947593]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.32347825 -0.1580737 ]]. Action = [[-0.05451756  0.05340282  0.         -0.11968732]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, True, False, False]
State prediction error at timestep 86 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 86 of -1
Current timestep = 87. State = [[-0.33045667 -0.15438575]]. Action = [[-0.07601557  0.03122545  0.         -0.91275406]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, True, False, False]
State prediction error at timestep 87 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 87 of -1
Current timestep = 88. State = [[-0.332719   -0.14921315]]. Action = [[0.03047992 0.06539113 0.         0.07755256]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, True, False, False]
State prediction error at timestep 88 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 88 of -1
Current timestep = 89. State = [[-0.3317696  -0.14124969]]. Action = [[0.02990932 0.08694781 0.         0.8246733 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, True, False, False]
State prediction error at timestep 89 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 89 of -1
Current timestep = 90. State = [[-0.330612  -0.1373574]]. Action = [[ 0.0295243  -0.01646926  0.         -0.39034277]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, True, False, False]
State prediction error at timestep 90 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 90 of 1
Current timestep = 91. State = [[-0.33245298 -0.1407729 ]]. Action = [[-0.0305763  -0.09828264  0.          0.7810054 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, True, False, False]
State prediction error at timestep 91 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 91 of 1
Current timestep = 92. State = [[-0.33714    -0.14572221]]. Action = [[-0.05481712 -0.05896717  0.          0.04383707]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, True, False, False]
State prediction error at timestep 92 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 92 of -1
Current timestep = 93. State = [[-0.33787858 -0.14322698]]. Action = [[ 0.03848978  0.08441658  0.         -0.8415959 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, True, False, False]
State prediction error at timestep 93 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 93 of -1
Current timestep = 94. State = [[-0.3390113  -0.14213608]]. Action = [[-0.03273983 -0.03835465  0.         -0.6706357 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, True, False, False]
State prediction error at timestep 94 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.3402728  -0.13865516]]. Action = [[ 0.00253521  0.08223969  0.         -0.9214864 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, True, False, False]
State prediction error at timestep 95 is tensor(8.4239e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 95 of -1
Current timestep = 96. State = [[-0.34228152 -0.13845839]]. Action = [[-0.0312465  -0.05236474  0.         -0.6642747 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, True, False, False]
State prediction error at timestep 96 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 96 of 1
Current timestep = 97. State = [[-0.3402687  -0.13956621]]. Action = [[ 0.06803686 -0.00141998  0.         -0.12620914]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, True, False, False]
State prediction error at timestep 97 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 97 of 1
Current timestep = 98. State = [[-0.3386634  -0.13774712]]. Action = [[-0.0031804   0.02725554  0.         -0.66163677]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, True, False, False]
State prediction error at timestep 98 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 98 of 1
Current timestep = 99. State = [[-0.33839315 -0.13227737]]. Action = [[0.00803898 0.08507495 0.         0.08757603]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, True, False, False]
State prediction error at timestep 99 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 99 of 1
Current timestep = 100. State = [[-0.33547297 -0.12648672]]. Action = [[ 0.05865536  0.04813396  0.         -0.64055216]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, True, False, False]
State prediction error at timestep 100 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 100 of 1
Current timestep = 101. State = [[-0.3295405  -0.11910552]]. Action = [[ 0.08954789  0.08667827  0.         -0.90798503]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
State prediction error at timestep 101 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 101 of 1
Current timestep = 102. State = [[-0.32800877 -0.11228143]]. Action = [[-0.0171745   0.04926879  0.         -0.5679182 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
State prediction error at timestep 102 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 102 of 1
Current timestep = 103. State = [[-0.3261096  -0.11253967]]. Action = [[ 0.05548177 -0.06868718  0.          0.41684413]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
State prediction error at timestep 103 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 103 of 1
Current timestep = 104. State = [[-0.32160234 -0.11164058]]. Action = [[ 0.05984356  0.02887452  0.         -0.35929316]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
State prediction error at timestep 104 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 104 of 1
Current timestep = 105. State = [[-0.31619924 -0.11303026]]. Action = [[ 0.0653548  -0.06888779  0.         -0.30116767]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
State prediction error at timestep 105 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 105 of 1
Current timestep = 106. State = [[-0.31279823 -0.10925072]]. Action = [[ 0.01163681  0.09830096  0.         -0.18892324]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 106 of 1
Current timestep = 107. State = [[-0.3093609  -0.10637908]]. Action = [[ 0.05097637 -0.01756877  0.          0.51302445]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 107 of 1
Current timestep = 108. State = [[-0.30378225 -0.10488365]]. Action = [[0.06818856 0.01733526 0.         0.5432646 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 108 of 1
Current timestep = 109. State = [[-0.30225846 -0.10204617]]. Action = [[-0.0285906   0.03079858  0.          0.47636485]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
State prediction error at timestep 109 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 109 of 1
Current timestep = 110. State = [[-0.30367807 -0.09615479]]. Action = [[-0.03172767  0.08883335  0.          0.4477358 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
State prediction error at timestep 110 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 110 of 1
Current timestep = 111. State = [[-0.3058913  -0.08887375]]. Action = [[-0.03815966  0.07810193  0.          0.07176709]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
State prediction error at timestep 111 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 111 of 1
Current timestep = 112. State = [[-0.30671677 -0.08981255]]. Action = [[-0.00339596 -0.08850174  0.         -0.84233385]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
State prediction error at timestep 112 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 112 of 1
Current timestep = 113. State = [[-0.3024905  -0.09535158]]. Action = [[ 0.07421016 -0.07817961  0.          0.7232802 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.30271715 -0.09714337]]. Action = [[-0.07543875  0.00769342  0.         -0.2146092 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
State prediction error at timestep 114 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 114 of 1
Current timestep = 115. State = [[-0.3018839  -0.09213402]]. Action = [[0.04876726 0.09217129 0.         0.4541012 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
State prediction error at timestep 115 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 115 of 1
Current timestep = 116. State = [[-0.29585442 -0.0929042 ]]. Action = [[ 0.09699307 -0.08730958  0.          0.36803067]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 116 of 1
Current timestep = 117. State = [[-0.29283306 -0.09661722]]. Action = [[-0.00944413 -0.03338299  0.          0.8392025 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
State prediction error at timestep 117 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 117 of 1
Current timestep = 118. State = [[-0.292321   -0.09316283]]. Action = [[ 0.00079056  0.09287911  0.         -0.69523335]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
State prediction error at timestep 118 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 118 of 1
Current timestep = 119. State = [[-0.2948539 -0.0868949]]. Action = [[-0.05860753  0.07153665  0.         -0.27144444]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.2945571  -0.08603828]]. Action = [[ 0.03967405 -0.03509729  0.          0.11217546]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
State prediction error at timestep 120 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 120 of 1
Current timestep = 121. State = [[-0.28873965 -0.09061424]]. Action = [[ 0.09083361 -0.08184336  0.          0.1590563 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 121 of 1
Current timestep = 122. State = [[-0.28446507 -0.09181315]]. Action = [[0.02000129 0.02186316 0.         0.61730886]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
State prediction error at timestep 122 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 122 of 1
Current timestep = 123. State = [[-0.2869184  -0.09449753]]. Action = [[-0.08063941 -0.06030492  0.         -0.46697176]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 123 of 1
Current timestep = 124. State = [[-0.2860622 -0.0927439]]. Action = [[ 0.04900754  0.08185997  0.         -0.00836706]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.28111517 -0.08707777]]. Action = [[ 0.06741073  0.06339111  0.         -0.80350655]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
State prediction error at timestep 125 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.275804   -0.08632315]]. Action = [[ 0.06272691 -0.03652696  0.         -0.17129475]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.27229795 -0.08299807]]. Action = [[0.02540798 0.07613913 0.         0.6201123 ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 127 of 1
Current timestep = 128. State = [[-0.27310115 -0.08066327]]. Action = [[-0.03986037 -0.00689657  0.         -0.7910332 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
State prediction error at timestep 128 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 128 of 1
Current timestep = 129. State = [[-0.27237087 -0.08260902]]. Action = [[ 0.02619041 -0.04518757  0.         -0.9391626 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
State prediction error at timestep 129 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 129 of -1
Current timestep = 130. State = [[-0.2714109  -0.08509523]]. Action = [[-0.00914468 -0.02839167  0.          0.88739455]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 130 of -1
Current timestep = 131. State = [[-0.26882327 -0.08535326]]. Action = [[ 0.04092183  0.01089287  0.         -0.90620095]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.27116048 -0.08637405]]. Action = [[-0.0895367 -0.0255215  0.         0.8142557]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 132 of 1
Current timestep = 133. State = [[-0.2704343  -0.08678422]]. Action = [[ 0.05143023  0.0094116   0.         -0.5236752 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 133 of 1
Current timestep = 134. State = [[-0.26898712 -0.0819372 ]]. Action = [[-0.00804614  0.09108222  0.          0.06283665]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.26616946 -0.07538892]]. Action = [[ 0.05796131  0.06778074  0.         -0.15657991]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 135 of 1
Current timestep = 136. State = [[-0.26581177 -0.07617649]]. Action = [[-0.02735404 -0.0758423   0.         -0.9061425 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.2674113  -0.07735803]]. Action = [[-0.02491261  0.01116148  0.         -0.08901095]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 137 of 1
Current timestep = 138. State = [[-0.26539555 -0.08017211]]. Action = [[ 0.05015209 -0.06612033  0.          0.791625  ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 138 of 1
Current timestep = 139. State = [[-0.25926647 -0.08645981]]. Action = [[ 0.08763675 -0.08905476  0.         -0.11757445]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 139 of 1
Current timestep = 140. State = [[-0.2534881  -0.08871343]]. Action = [[ 0.05082216  0.0151007   0.         -0.5095084 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 140 of 1
Current timestep = 141. State = [[-0.25470912 -0.08493374]]. Action = [[-0.07801808  0.08288638  0.         -0.7094703 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.25652474 -0.08094782]]. Action = [[-0.00071754  0.03854694  0.         -0.02628416]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 142 of 1
Current timestep = 143. State = [[-0.25838113 -0.08449207]]. Action = [[-0.038352   -0.09454923  0.          0.59933734]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
State prediction error at timestep 143 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 143 of 1
Current timestep = 144. State = [[-0.25506428 -0.08411427]]. Action = [[0.08879202 0.06878414 0.         0.1764698 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 144 of -1
Current timestep = 145. State = [[-0.25667703 -0.08514032]]. Action = [[-0.08888941 -0.06282501  0.          0.34518814]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.26238483 -0.08314609]]. Action = [[-0.07241456  0.07899899  0.          0.63014567]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 146 of -1
Current timestep = 147. State = [[-0.2616702  -0.08492706]]. Action = [[ 0.07116681 -0.08417684  0.          0.02522004]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 147 of 1
Current timestep = 148. State = [[-0.25703552 -0.08670857]]. Action = [[ 0.04867194  0.0128863   0.         -0.8024344 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.25252044 -0.09065896]]. Action = [[ 0.05059854 -0.08088298  0.         -0.95193976]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 149 of -1
Current timestep = 150. State = [[-0.24578457 -0.08946855]]. Action = [[0.09452704 0.07791468 0.         0.35190833]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 150 of 1
Current timestep = 151. State = [[-0.23977573 -0.08812729]]. Action = [[ 0.05645395 -0.0139135   0.         -0.63941646]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, True, False]
State prediction error at timestep 151 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 151 of 1
Current timestep = 152. State = [[-0.23933947 -0.09265824]]. Action = [[-0.04346154 -0.07793256  0.          0.16453314]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 152 of 1
Current timestep = 153. State = [[-0.23862709 -0.0907674 ]]. Action = [[0.0182627  0.09814704 0.         0.717458  ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 153 of 1
Current timestep = 154. State = [[-0.23945579 -0.08577158]]. Action = [[-0.04010311  0.05001745  0.         -0.8843449 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 154 of 1
Current timestep = 155. State = [[-0.24166155 -0.08819381]]. Action = [[-0.03640845 -0.08062347  0.         -0.3874461 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 155 of 1
Current timestep = 156. State = [[-0.24239627 -0.0882296 ]]. Action = [[-0.00731391  0.04703552  0.         -0.30644095]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 156 of -1
Current timestep = 157. State = [[-0.24081111 -0.08474953]]. Action = [[0.02935696 0.04054866 0.         0.5480087 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 157 of -1
Current timestep = 158. State = [[-0.24042685 -0.08232997]]. Action = [[-0.01305761  0.01348877  0.         -0.6406215 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 158 of 1
Current timestep = 159. State = [[-0.24060923 -0.0764739 ]]. Action = [[ 0.00135537  0.09597952  0.         -0.5342809 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 159 of 1
Current timestep = 160. State = [[-0.24093866 -0.07325818]]. Action = [[-0.00292102 -0.01138473  0.          0.24379885]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 160 of 1
Current timestep = 161. State = [[-0.24043058 -0.07030645]]. Action = [[ 0.01661074  0.04194201  0.         -0.7866637 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.23806602 -0.06742983]]. Action = [[0.04568058 0.00990951 0.         0.07490742]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.23697464 -0.06683157]]. Action = [[ 1.6317517e-04 -1.7653540e-02  0.0000000e+00  6.0916066e-01]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 163 of 1
Current timestep = 164. State = [[-0.24097362 -0.0641117 ]]. Action = [[-0.08496241  0.04743626  0.         -0.19570172]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 164 of 1
Current timestep = 165. State = [[-0.24546954 -0.06421939]]. Action = [[-0.0438075  -0.04541432  0.          0.8872466 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 165 of 1
Current timestep = 166. State = [[-0.24581492 -0.06860784]]. Action = [[ 0.01969166 -0.07480378  0.          0.47727466]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 166 of -1
Current timestep = 167. State = [[-0.24735802 -0.06722109]]. Action = [[-0.04287509  0.06767683  0.          0.37122202]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 167 of -1
Current timestep = 168. State = [[-0.24427645 -0.06957724]]. Action = [[ 0.09725115 -0.09184265  0.          0.52332664]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 168 of -1
Current timestep = 169. State = [[-0.2366497  -0.07318307]]. Action = [[ 0.09563632 -0.01862077  0.         -0.93990666]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 169 of 1
Current timestep = 170. State = [[-0.2294196  -0.07849091]]. Action = [[ 0.07664237 -0.08364484  0.         -0.79648465]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 170 of 1
Current timestep = 171. State = [[-0.2298856  -0.07932299]]. Action = [[-0.0889427   0.05527223  0.         -0.9065503 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 171 of 1
Current timestep = 172. State = [[-0.22833015 -0.08161164]]. Action = [[ 0.08138014 -0.06031863  0.          0.9012451 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 172 of 1
Current timestep = 173. State = [[-0.22713618 -0.08754683]]. Action = [[-0.03425391 -0.06369251  0.          0.8802316 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 173 of 1
Current timestep = 174. State = [[-0.22746523 -0.09408639]]. Action = [[-0.00908112 -0.06392235  0.         -0.9172    ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 174 of 1
Current timestep = 175. State = [[-0.22755958 -0.0927955 ]]. Action = [[-0.01349799  0.09611871  0.         -0.80516946]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.22593082 -0.09098717]]. Action = [[ 0.03175712  0.00335149  0.         -0.3695984 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 176 of 1
Current timestep = 177. State = [[-0.22337243 -0.09110575]]. Action = [[0.02875843 0.00623801 0.         0.23010254]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 177 of 1
Current timestep = 178. State = [[-0.22658041 -0.09421512]]. Action = [[-0.09121292 -0.05372075  0.          0.31241512]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 178 of 1
Current timestep = 179. State = [[-0.22784187 -0.09830461]]. Action = [[ 0.01723921 -0.03592561  0.          0.94820404]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 179 of -1
Current timestep = 180. State = [[-0.22534195 -0.09584934]]. Action = [[ 0.04074328  0.08270114  0.         -0.81467426]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 180 of -1
Current timestep = 181. State = [[-0.22039925 -0.09494504]]. Action = [[ 0.08058447 -0.0280364   0.         -0.8278915 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 181 of -1
Current timestep = 182. State = [[-0.21755749 -0.09236599]]. Action = [[0.01326766 0.0649501  0.         0.3141632 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
State prediction error at timestep 182 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 182 of 1
Current timestep = 183. State = [[-0.22046956 -0.08552919]]. Action = [[-0.06658486  0.09648497  0.          0.31133127]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 183 of 1
Current timestep = 184. State = [[-0.22692366 -0.08282972]]. Action = [[-0.0908584  -0.01360779  0.          0.73657393]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 184 of 1
Current timestep = 185. State = [[-0.23433936 -0.08691267]]. Action = [[-0.09618896 -0.08746641  0.          0.19745219]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 185 of -1
Current timestep = 186. State = [[-0.2334748  -0.08997829]]. Action = [[ 0.08566556 -0.01891463  0.         -0.1748541 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 186 of -1
Current timestep = 187. State = [[-0.23473626 -0.08763125]]. Action = [[-0.07697381  0.05615961  0.          0.15349329]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 187 of -1
Current timestep = 188. State = [[-0.23717074 -0.08116618]]. Action = [[0.00346522 0.08701771 0.         0.9135095 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 188 of -1
Current timestep = 189. State = [[-0.24045926 -0.07512944]]. Action = [[-0.04677007  0.05015915  0.          0.7543285 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 189 of -1
Current timestep = 190. State = [[-0.24621773 -0.07713546]]. Action = [[-0.07121459 -0.09173783  0.          0.69477975]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 190 of -1
Current timestep = 191. State = [[-0.24696025 -0.07523829]]. Action = [[ 0.04922173  0.07957301  0.         -0.6685267 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.24570908 -0.07027729]]. Action = [[0.02503838 0.03432161 0.         0.7300426 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 192 of -1
Current timestep = 193. State = [[-0.24626547 -0.06325683]]. Action = [[-0.00080435  0.0947387   0.          0.09873974]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
State prediction error at timestep 193 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 193 of -1
Current timestep = 194. State = [[-0.24502942 -0.06122354]]. Action = [[ 0.05216923 -0.04224072  0.          0.78819025]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
State prediction error at timestep 194 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 194 of -1
Current timestep = 195. State = [[-0.24579683 -0.05882231]]. Action = [[-0.02547674  0.04695185  0.         -0.70752704]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 195 of 1
Current timestep = 196. State = [[-0.24461456 -0.06085904]]. Action = [[ 0.05430258 -0.08761244  0.          0.7717165 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 196 of 1
Current timestep = 197. State = [[-0.24360615 -0.06352978]]. Action = [[-0.00388751 -0.01625287  0.         -0.0340541 ]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.24312632 -0.06614101]]. Action = [[ 0.00958196 -0.04508412  0.          0.3871479 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 198 of 1
Current timestep = 199. State = [[-0.24578862 -0.06553192]]. Action = [[-0.06654984  0.04127272  0.         -0.8347688 ]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 199 of 1
Current timestep = 200. State = [[-0.24410783 -0.06257205]]. Action = [[0.07302959 0.03682453 0.         0.7998409 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 200 of 1
Current timestep = 201. State = [[-0.2404911  -0.05761481]]. Action = [[ 0.03391457  0.07244172  0.         -0.7912922 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
State prediction error at timestep 201 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 201 of 1
Current timestep = 202. State = [[-0.23569994 -0.05641663]]. Action = [[ 0.07656976 -0.0274791   0.          0.45824206]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 202 of 1
Current timestep = 203. State = [[-0.22971141 -0.053202  ]]. Action = [[ 0.07509292  0.07035194  0.         -0.3861177 ]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 203 of 1
Current timestep = 204. State = [[-0.22984022 -0.04886061]]. Action = [[-0.05395817  0.03720126  0.          0.13344097]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 204 of 1
Current timestep = 205. State = [[-0.23139054 -0.04793078]]. Action = [[-0.00946227 -0.01798759  0.          0.52615285]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 205 of 1
Current timestep = 206. State = [[-0.23570378 -0.04490187]]. Action = [[-0.09135728  0.05624104  0.          0.32747686]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 206 of 1
Current timestep = 207. State = [[-0.24099752 -0.04466082]]. Action = [[-0.06585272 -0.04444689  0.          0.4073032 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 207 of -1
Current timestep = 208. State = [[-0.24700455 -0.04053197]]. Action = [[-0.08916794  0.08999711  0.         -0.4282555 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 208 of -1
Current timestep = 209. State = [[-0.24782558 -0.04166082]]. Action = [[ 0.0439396  -0.09573039  0.          0.25601602]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 209 of -1
Current timestep = 210. State = [[-0.24599597 -0.04233365]]. Action = [[ 0.0117416   0.02618957  0.         -0.93022376]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[-0.24132662 -0.04591687]]. Action = [[ 0.08529589 -0.09370675  0.         -0.48642504]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 211 of -1
Current timestep = 212. State = [[-0.23861519 -0.04675027]]. Action = [[-9.5278025e-05  3.7158869e-02  0.0000000e+00  3.3277082e-01]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[-0.2365035  -0.04457936]]. Action = [[0.03682487 0.02630761 0.         0.45215082]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 213 of 1
Current timestep = 214. State = [[-0.23795313 -0.0460182 ]]. Action = [[-0.0559076  -0.04322153  0.         -0.75914913]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 214 of 1
Current timestep = 215. State = [[-0.23604971 -0.04866487]]. Action = [[ 0.06322844 -0.02286462  0.         -0.7315309 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 215 of 1
Current timestep = 216. State = [[-0.23079123 -0.05192015]]. Action = [[ 0.06482852 -0.04163617  0.         -0.0438357 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 216 of 1
Current timestep = 217. State = [[-0.23234992 -0.05058616]]. Action = [[-0.08975849  0.06574426  0.          0.8661524 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 217 of 1
Current timestep = 218. State = [[-0.23689094 -0.04592294]]. Action = [[-0.04252465  0.06326414  0.         -0.1556459 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 218 of 1
Current timestep = 219. State = [[-0.23480141 -0.04196325]]. Action = [[ 0.08660156  0.03637446  0.         -0.5327793 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 219 of 1
Current timestep = 220. State = [[-0.22880304 -0.04486541]]. Action = [[ 0.08645513 -0.08877223  0.         -0.7462335 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 220 of -1
Current timestep = 221. State = [[-0.22656333 -0.05193255]]. Action = [[-0.0094624  -0.09027666  0.         -0.79716897]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, True, False]
State prediction error at timestep 221 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 221 of 1
Current timestep = 222. State = [[-0.22246516 -0.0556079 ]]. Action = [[ 0.07522357 -0.00859282  0.         -0.02362239]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 222 of 1
Current timestep = 223. State = [[-0.2196     -0.05212717]]. Action = [[ 0.00519522  0.09005398  0.         -0.9734074 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
State prediction error at timestep 223 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 223 of 1
Current timestep = 224. State = [[-0.21614826 -0.04497827]]. Action = [[ 0.06103534  0.09735896  0.         -0.96240145]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 224 of 1
Current timestep = 225. State = [[-0.21065295 -0.04461015]]. Action = [[ 0.07508316 -0.05436892  0.         -0.8056862 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 225 of 1
Current timestep = 226. State = [[-0.20720014 -0.0421155 ]]. Action = [[ 0.01650312  0.07651465  0.         -0.3875451 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 226 of 1
Current timestep = 227. State = [[-0.20292777 -0.03477806]]. Action = [[ 0.06904321  0.0961104   0.         -0.7345994 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
State prediction error at timestep 227 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 227 of 1
Current timestep = 228. State = [[-0.20260413 -0.03468914]]. Action = [[-0.04368999 -0.07500306  0.         -0.8289139 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 228 of 1
Current timestep = 229. State = [[-0.19842729 -0.03684017]]. Action = [[ 0.08934081 -0.01591229  0.         -0.3156441 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 229 of 1
Current timestep = 230. State = [[-0.19666885 -0.04018854]]. Action = [[-0.04367778 -0.06476716  0.          0.8798791 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 230 of 1
Current timestep = 231. State = [[-0.19784246 -0.03895219]]. Action = [[-0.03408166  0.05751709  0.          0.18402517]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 231 of 1
Current timestep = 232. State = [[-0.19678812 -0.04080878]]. Action = [[ 0.01388916 -0.07565454  0.         -0.3809421 ]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
State prediction error at timestep 232 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 232 of 1
Current timestep = 233. State = [[-0.19793648 -0.04694653]]. Action = [[-0.06193347 -0.08126424  0.         -0.22437131]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, True, False]
State prediction error at timestep 233 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 233 of 1
Current timestep = 234. State = [[-0.20212078 -0.04878826]]. Action = [[-0.08470607  0.01914637  0.          0.25960457]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 234 of 1
Current timestep = 235. State = [[-0.20144118 -0.04603218]]. Action = [[ 0.04517288  0.05179005  0.         -0.91115713]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 235 of -1
Current timestep = 236. State = [[-0.19815704 -0.04907322]]. Action = [[ 0.03083386 -0.08826334  0.          0.9286505 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, True, False]
State prediction error at timestep 236 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 236 of -1
Current timestep = 237. State = [[-0.19513789 -0.0472072 ]]. Action = [[0.03296936 0.09608632 0.         0.02550757]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 237 of -1
Current timestep = 238. State = [[-0.19455855 -0.04401806]]. Action = [[-0.01144259  0.01254834  0.          0.5359838 ]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, True, False]
State prediction error at timestep 238 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 238 of -1
Current timestep = 239. State = [[-0.19205625 -0.04102314]]. Action = [[0.0566575  0.04822449 0.         0.08754706]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
State prediction error at timestep 239 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 239 of -1
Current timestep = 240. State = [[-0.18845929 -0.03875479]]. Action = [[0.04642055 0.01133579 0.         0.3222928 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 240 of 1
Current timestep = 241. State = [[-0.18355177 -0.03980646]]. Action = [[ 0.07471039 -0.03485955  0.          0.5807986 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
State prediction error at timestep 241 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 241 of 1
Current timestep = 242. State = [[-0.18491003 -0.03744404]]. Action = [[-0.08291123  0.06495338  0.         -0.48193598]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
State prediction error at timestep 242 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 242 of 1
Current timestep = 243. State = [[-0.18870582 -0.03945487]]. Action = [[-0.03187948 -0.08477199  0.         -0.14282608]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, False, True, False]
State prediction error at timestep 243 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 243 of 1
Current timestep = 244. State = [[-0.19262676 -0.04076799]]. Action = [[-0.0613178   0.01613948  0.         -0.81208605]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 244 of -1
Current timestep = 245. State = [[-0.19272175 -0.04106513]]. Action = [[ 0.03572883 -0.01768951  0.         -0.7574467 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, False, True, False]
State prediction error at timestep 245 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 245 of -1
Current timestep = 246. State = [[-0.19406156 -0.0377944 ]]. Action = [[-0.04249274  0.0716644   0.         -0.10136569]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, False, True, False]
State prediction error at timestep 246 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 246 of -1
Current timestep = 247. State = [[-0.19791578 -0.03074982]]. Action = [[-0.04613274  0.0911449   0.         -0.20297039]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
State prediction error at timestep 247 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 247 of 1
Current timestep = 248. State = [[-0.1986366  -0.02705955]]. Action = [[0.02874146 0.00190071 0.         0.34478283]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 248 of -1
Current timestep = 249. State = [[-0.19854096 -0.0286572 ]]. Action = [[ 0.00115921 -0.05318131  0.         -0.81480086]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
State prediction error at timestep 249 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 249 of -1
Current timestep = 250. State = [[-0.20137617 -0.03194305]]. Action = [[-0.05285711 -0.04657866  0.          0.0321964 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, True, False]
State prediction error at timestep 250 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 250 of -1
Current timestep = 251. State = [[-0.19900946 -0.03322006]]. Action = [[ 0.08710975 -0.00280322  0.          0.93734443]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, True, False]
State prediction error at timestep 251 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 251 of -1
Current timestep = 252. State = [[-0.1928618  -0.02868668]]. Action = [[ 0.08634243  0.09173951  0.         -0.6492634 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.1931137  -0.02103136]]. Action = [[-0.04986778  0.09536903  0.         -0.87573797]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, True, False]
State prediction error at timestep 253 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 253 of 1
Current timestep = 254. State = [[-0.19642422 -0.01698639]]. Action = [[-0.02945825  0.00982064  0.         -0.7388288 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
State prediction error at timestep 254 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 254 of -1
Current timestep = 255. State = [[-0.19311419 -0.0179431 ]]. Action = [[ 0.09947651 -0.04397329  0.         -0.912248  ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, True, False]
State prediction error at timestep 255 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 255 of 1
Current timestep = 256. State = [[-0.19399123 -0.01489645]]. Action = [[-0.0793456   0.07430894  0.          0.3930472 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 256 of 1
Current timestep = 257. State = [[-0.19946317 -0.01091895]]. Action = [[-0.06038565  0.01997457  0.         -0.36664826]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 257 of 1
Current timestep = 258. State = [[-0.20556527 -0.00827215]]. Action = [[-0.07745931  0.01762332  0.          0.4005568 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
State prediction error at timestep 258 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 258 of -1
Current timestep = 259. State = [[-0.207722   -0.00878886]]. Action = [[ 0.01384628 -0.04392212  0.          0.0282383 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
State prediction error at timestep 259 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 259 of -1
Current timestep = 260. State = [[-0.2101322  -0.01237284]]. Action = [[-0.04672384 -0.06356426  0.          0.7517232 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 260 of -1
Current timestep = 261. State = [[-0.21293145 -0.01694357]]. Action = [[-0.02753735 -0.06222719  0.          0.734257  ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, True, False]
State prediction error at timestep 261 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 261 of -1
Current timestep = 262. State = [[-0.21270844 -0.02118837]]. Action = [[ 0.02371137 -0.0461527   0.          0.6823348 ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, True, False]
State prediction error at timestep 262 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 262 of -1
Current timestep = 263. State = [[-0.20772177 -0.01932383]]. Action = [[ 0.09788016  0.07531083  0.         -0.6259775 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 263 of -1
Current timestep = 264. State = [[-0.20669411 -0.01383833]]. Action = [[-0.02172699  0.07586152  0.          0.41738117]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, True, False]
State prediction error at timestep 264 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 264 of 1
Current timestep = 265. State = [[-0.2090865  -0.00760738]]. Action = [[-0.02075323  0.08033865  0.          0.4981984 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 265 of 1
Current timestep = 266. State = [[-0.20997283 -0.00364326]]. Action = [[0.01586135 0.02423756 0.         0.6549469 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 266 of -1
Current timestep = 267. State = [[-0.21237777 -0.00068736]]. Action = [[-0.03831039  0.03185932  0.         -0.22166842]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
State prediction error at timestep 267 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 267 of -1
Current timestep = 268. State = [[-0.21346301 -0.003887  ]]. Action = [[ 0.01359532 -0.09741873  0.          0.33648562]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
State prediction error at timestep 268 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 268 of -1
Current timestep = 269. State = [[-0.21729499 -0.00950811]]. Action = [[-0.0829313  -0.06598876  0.          0.50127506]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 269 of 1
Current timestep = 270. State = [[-0.21636258 -0.00849241]]. Action = [[ 0.07329763  0.05871402  0.         -0.57471776]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
State prediction error at timestep 270 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 270 of -1
Current timestep = 271. State = [[-0.21710414 -0.00265688]]. Action = [[-0.05071036  0.08078129  0.          0.18902588]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
State prediction error at timestep 271 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 271 of -1
Current timestep = 272. State = [[-0.21992338 -0.00192447]]. Action = [[-0.02468841 -0.0436784   0.          0.6318536 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
State prediction error at timestep 272 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 272 of -1
Current timestep = 273. State = [[-0.21908289 -0.00612143]]. Action = [[ 0.03500558 -0.06788492  0.          0.9531801 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
State prediction error at timestep 273 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 273 of -1
Current timestep = 274. State = [[-0.21547438 -0.00866466]]. Action = [[ 0.05521715 -0.00891903  0.         -0.24552882]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 274 of 1
Current timestep = 275. State = [[-0.21381623 -0.01415501]]. Action = [[-0.00232173 -0.09625717  0.         -0.2717514 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 275 of 1
Current timestep = 276. State = [[-0.21064663 -0.01698007]]. Action = [[0.05530361 0.01394209 0.         0.7162366 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 276 of 1
Current timestep = 277. State = [[-0.20989597 -0.01297632]]. Action = [[-0.02182049  0.09150321  0.          0.45489526]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, True, False]
State prediction error at timestep 277 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 277 of 1
Current timestep = 278. State = [[-0.2147003  -0.00590036]]. Action = [[-0.08847862  0.09659801  0.         -0.6279688 ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 278 of 1
Current timestep = 279. State = [[-0.21333021 -0.00684669]]. Action = [[ 0.09184123 -0.08050815  0.         -0.83009875]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, True, False]
State prediction error at timestep 279 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 279 of -1
Current timestep = 280. State = [[-0.21099429 -0.01180244]]. Action = [[-0.01275097 -0.0537692   0.          0.13726044]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, True, False]
State prediction error at timestep 280 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 280 of -1
Current timestep = 281. State = [[-0.21393196 -0.01040996]]. Action = [[-0.07262274  0.06489707  0.         -0.59466493]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 281 of -1
Current timestep = 282. State = [[-0.21542709 -0.00705274]]. Action = [[0.00116529 0.02788045 0.         0.61064255]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, True, False]
State prediction error at timestep 282 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 282 of 1
Current timestep = 283. State = [[-0.2171684  -0.00179221]]. Action = [[-0.03542481  0.07835162  0.         -0.93110347]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 283 of 1
Current timestep = 284. State = [[-2.2223279e-01  5.9398863e-05]]. Action = [[-0.08015065 -0.02549241  0.         -0.7423279 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 284 of 1
Current timestep = 285. State = [[-0.22043777 -0.00367418]]. Action = [[ 0.09449586 -0.07754089  0.          0.90383816]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, True, False]
State prediction error at timestep 285 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 285 of -1
Current timestep = 286. State = [[-0.22170718 -0.00129012]]. Action = [[-0.08734652  0.08681681  0.          0.9041743 ]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, True, False]
State prediction error at timestep 286 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 286 of -1
Current timestep = 287. State = [[-0.22832128  0.00442972]]. Action = [[-0.08077381  0.05267908  0.          0.14788723]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 287 of -1
Current timestep = 288. State = [[-0.22881769  0.01030788]]. Action = [[ 0.06249953  0.06463668  0.         -0.4590906 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
State prediction error at timestep 288 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 288 of -1
Current timestep = 289. State = [[-0.22538339  0.00895491]]. Action = [[ 0.05847801 -0.08612818  0.          0.54998386]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
State prediction error at timestep 289 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 289 of -1
Current timestep = 290. State = [[-0.22778462  0.00629629]]. Action = [[-0.0810893  -0.01726271  0.         -0.27898252]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 290 of -1
Current timestep = 291. State = [[-0.23340695  0.00324171]]. Action = [[-0.06517059 -0.05826117  0.         -0.20393628]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 291 of -1
Current timestep = 292. State = [[-0.23671448  0.00213086]]. Action = [[-0.02084114  0.00768103  0.          0.32369447]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 292 of -1
Current timestep = 293. State = [[-0.23922142 -0.00108147]]. Action = [[-0.02714396 -0.06759632  0.         -0.8166933 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 293 of -1
Current timestep = 294. State = [[-0.23607849 -0.0069709 ]]. Action = [[ 0.09169731 -0.07088767  0.          0.5731797 ]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, True, False]
State prediction error at timestep 294 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 294 of -1
Current timestep = 295. State = [[-0.23509148 -0.00717893]]. Action = [[-0.02550624  0.05613787  0.         -0.91468596]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, True, False]
State prediction error at timestep 295 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 295 of 1
Current timestep = 296. State = [[-0.23212345 -0.01101988]]. Action = [[ 0.08181427 -0.09034113  0.          0.8222002 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, True, False]
State prediction error at timestep 296 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 296 of 1
Current timestep = 297. State = [[-0.23111352 -0.01034528]]. Action = [[-0.0215055   0.08706691  0.          0.61210334]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 297 of 1
Current timestep = 298. State = [[-0.22994944 -0.0048872 ]]. Action = [[0.04585411 0.07767371 0.         0.38263583]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 298 of 1
Current timestep = 299. State = [[-0.2293166  -0.00652902]]. Action = [[ 1.8441677e-04 -7.1941778e-02  0.0000000e+00 -9.0668118e-01]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 299 of 1
Current timestep = 300. State = [[-0.23395368 -0.00541641]]. Action = [[-0.09348194  0.07028342  0.         -0.7031642 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, True, False]
State prediction error at timestep 300 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 300 of 1
Current timestep = 301. State = [[-0.2335169  -0.00605892]]. Action = [[ 0.07652511 -0.0499369   0.          0.47671556]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, True, False]
State prediction error at timestep 301 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 301 of -1
Current timestep = 302. State = [[-0.2355462  -0.00532669]]. Action = [[-0.08905461  0.04433227  0.         -0.312657  ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 302 of -1
Current timestep = 303. State = [[-0.2371654  -0.00172758]]. Action = [[0.02276597 0.04411431 0.         0.54314065]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, True, False]
State prediction error at timestep 303 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 303 of -1
Current timestep = 304. State = [[-0.23883833 -0.00137589]]. Action = [[-0.03493074 -0.02817215  0.         -0.6980233 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, True, False]
State prediction error at timestep 304 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 304 of -1
Current timestep = 305. State = [[-0.24189639  0.00315544]]. Action = [[-0.03351351  0.09652922  0.          0.3471353 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 305 of -1
Current timestep = 306. State = [[-0.24173391  0.00567994]]. Action = [[ 0.04039969 -0.01997622  0.          0.66658616]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, True, False]
State prediction error at timestep 306 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 306 of -1
Current timestep = 307. State = [[-0.23677249  0.00864386]]. Action = [[ 0.09880561  0.05387241  0.         -0.26681042]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, True, False]
State prediction error at timestep 307 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 307 of -1
Current timestep = 308. State = [[-0.23646282  0.00697196]]. Action = [[-0.0414342  -0.07757454  0.         -0.6950594 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, True, False]
State prediction error at timestep 308 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 308 of 1
Current timestep = 309. State = [[-0.24156515  0.00456725]]. Action = [[-0.08034206 -0.01523283  0.          0.98604536]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, True, False]
State prediction error at timestep 309 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 309 of -1
Current timestep = 310. State = [[-0.24144049  0.00837116]]. Action = [[0.05965953 0.07918597 0.         0.20115304]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, True, False]
State prediction error at timestep 310 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 310 of -1
Current timestep = 311. State = [[-0.24135171  0.01154335]]. Action = [[-0.02110649  0.00835051  0.          0.82647884]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 311 of 1
Current timestep = 312. State = [[-0.23943172  0.01113029]]. Action = [[ 0.0536456  -0.02528177  0.         -0.9900131 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 312 of -1
Current timestep = 313. State = [[-0.23865615  0.01324962]]. Action = [[-0.01248313  0.05061186  0.         -0.96644026]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 313 of -1
Current timestep = 314. State = [[-0.23485199  0.01849803]]. Action = [[0.09018207 0.06859799 0.         0.786628  ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
State prediction error at timestep 314 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 314 of -1
Current timestep = 315. State = [[-0.22846493  0.02316791]]. Action = [[ 0.08802015  0.04325528  0.         -0.85407025]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 315 of -1
Current timestep = 316. State = [[-0.2261118   0.02962017]]. Action = [[-5.765185e-04  9.128926e-02  0.000000e+00  7.788696e-01]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
State prediction error at timestep 316 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 316 of -1
Current timestep = 317. State = [[-0.22982822  0.03517345]]. Action = [[-0.07827289  0.03769273  0.          0.84712017]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 317 of 1
Current timestep = 318. State = [[-0.2291989   0.04242036]]. Action = [[ 0.06283166  0.0932802   0.         -0.5465059 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 318 of 1
Current timestep = 319. State = [[-0.2260907   0.04209285]]. Action = [[ 0.02681757 -0.09583641  0.         -0.6036533 ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, True, False]
State prediction error at timestep 319 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 319 of -1
Current timestep = 320. State = [[-0.22494817  0.03741512]]. Action = [[-0.01393859 -0.06917933  0.         -0.62853026]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, True, False]
State prediction error at timestep 320 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 320 of -1
Current timestep = 321. State = [[-0.22141525  0.03782621]]. Action = [[ 0.05360354  0.03127595  0.         -0.40658164]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, True, False]
State prediction error at timestep 321 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 321 of 1
Current timestep = 322. State = [[-0.22214086  0.04392317]]. Action = [[-0.06687143  0.08931071  0.          0.4178567 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, True, False]
State prediction error at timestep 322 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 322 of 1
Current timestep = 323. State = [[-0.22548774  0.05244061]]. Action = [[-0.04252899  0.09482963  0.         -0.3243699 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, True, False]
State prediction error at timestep 323 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 323 of 1
Current timestep = 324. State = [[-0.2222457   0.05457199]]. Action = [[ 0.09214634 -0.04189498  0.          0.08188629]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 324 of 1
Current timestep = 325. State = [[-0.22045057  0.05933062]]. Action = [[-0.0199032   0.09489394  0.          0.0159241 ]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, True, False]
State prediction error at timestep 325 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 325 of -1
Current timestep = 326. State = [[-0.22304401  0.05995216]]. Action = [[-0.0507716  -0.06941223  0.         -0.76191866]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 326 of -1
Current timestep = 327. State = [[-0.22656757  0.0629108 ]]. Action = [[-0.05353917  0.06907306  0.          0.72061884]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 327 of -1
Current timestep = 328. State = [[-0.22748493  0.06555213]]. Action = [[ 0.011052   -0.01386532  0.         -0.8633876 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, True, False]
State prediction error at timestep 328 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 328 of -1
Current timestep = 329. State = [[-0.22385657  0.06657741]]. Action = [[0.06996451 0.00475474 0.         0.21122682]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, True, False]
State prediction error at timestep 329 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 329 of -1
Current timestep = 330. State = [[-0.22612442  0.06903271]]. Action = [[-0.09164878  0.02860373  0.         -0.9503875 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, True, False]
State prediction error at timestep 330 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 330 of -1
Current timestep = 331. State = [[-0.23132032  0.0717745 ]]. Action = [[-0.05007663  0.01714401  0.          0.65114725]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 331 of -1
Current timestep = 332. State = [[-0.23254515  0.07809711]]. Action = [[ 0.02253864  0.09644995  0.         -0.0856753 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, True, False]
State prediction error at timestep 332 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 332 of -1
Current timestep = 333. State = [[-0.2330839   0.08446728]]. Action = [[ 0.00340788  0.05211291  0.         -0.9698287 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 333 of -1
Current timestep = 334. State = [[-0.23630011  0.0895256 ]]. Action = [[-0.04276131  0.04602466  0.         -0.6534309 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, True, False]
State prediction error at timestep 334 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 334 of -1
Current timestep = 335. State = [[-0.24185435  0.09403561]]. Action = [[-0.06406561  0.03464767  0.         -0.7340139 ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, True, False]
State prediction error at timestep 335 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 335 of -1
Current timestep = 336. State = [[-0.24347496  0.09628098]]. Action = [[ 0.03048294 -0.00564291  0.         -0.04637361]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 336 of -1
Current timestep = 337. State = [[-0.24855755  0.10201018]]. Action = [[-0.09373041  0.09019374  0.         -0.43939614]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, True, False]
State prediction error at timestep 337 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 337 of -1
Current timestep = 338. State = [[-0.25320584  0.11036097]]. Action = [[-0.00828712  0.08438919  0.         -0.64563924]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, True, False]
State prediction error at timestep 338 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 338 of -1
Current timestep = 339. State = [[-0.2594093   0.11399508]]. Action = [[-0.08468778 -0.01314257  0.         -0.8945891 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, True, False]
State prediction error at timestep 339 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 339 of -1
Current timestep = 340. State = [[-0.26755986  0.11760542]]. Action = [[-0.08496877  0.03926186  0.         -0.6343324 ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, True, False]
State prediction error at timestep 340 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 340 of -1
Current timestep = 341. State = [[-0.2674203   0.11566529]]. Action = [[ 0.08959127 -0.09494722  0.         -0.7430169 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 341 of -1
Current timestep = 342. State = [[-0.27108353  0.11728397]]. Action = [[-0.09766428  0.07011262  0.         -0.3791752 ]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 342 of -1
Current timestep = 343. State = [[-0.2707501   0.12048024]]. Action = [[ 0.09342145  0.00584147  0.         -0.56298554]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 343 of -1
Current timestep = 344. State = [[-0.27034175  0.12451001]]. Action = [[-0.01938608  0.06151921  0.         -0.8108982 ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 344 of -1
Current timestep = 345. State = [[-0.2679707   0.13106486]]. Action = [[ 0.07957671  0.08375264  0.         -0.7027174 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, False, True]
State prediction error at timestep 345 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 345 of -1
Current timestep = 346. State = [[-0.26506543  0.13364515]]. Action = [[ 0.03123262 -0.00731436  0.         -0.9822946 ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, False, True]
State prediction error at timestep 346 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 346 of -1
Current timestep = 347. State = [[-0.2601526   0.13305745]]. Action = [[ 0.08325059 -0.01953015  0.         -0.8675898 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, False, True]
State prediction error at timestep 347 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 347 of -1
Current timestep = 348. State = [[-0.25762513  0.13707082]]. Action = [[-0.00139397  0.08684299  0.         -0.01634079]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, False, True]
State prediction error at timestep 348 is tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 348 of 1
Current timestep = 349. State = [[-0.254785    0.13526553]]. Action = [[ 0.04690538 -0.09847809  0.         -0.22998726]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, False, True]
State prediction error at timestep 349 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 349 of 1
Current timestep = 350. State = [[-0.25379053  0.13105957]]. Action = [[-0.03222431 -0.04172369  0.          0.54527164]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, False, True]
State prediction error at timestep 350 is tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 350 of 1
Current timestep = 351. State = [[-0.24938005  0.13455063]]. Action = [[ 0.08722281  0.09339482  0.         -0.5674117 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, False, True]
State prediction error at timestep 351 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 351 of 1
Current timestep = 352. State = [[-0.24308826  0.13949433]]. Action = [[ 0.05247153  0.04488022  0.         -0.09252024]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, False, True]
State prediction error at timestep 352 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 352 of 1
Current timestep = 353. State = [[-0.24452135  0.1458293 ]]. Action = [[-0.0906799   0.0884933   0.          0.12055528]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, False, True]
State prediction error at timestep 353 is tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 353 of 1
Current timestep = 354. State = [[-0.24761975  0.14803739]]. Action = [[-0.03345494 -0.03271224  0.         -0.37537998]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, False, True]
State prediction error at timestep 354 is tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 354 of 1
Current timestep = 355. State = [[-0.24374244  0.15242164]]. Action = [[0.09239807 0.08406702 0.         0.52890956]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, False, True]
State prediction error at timestep 355 is tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 355 of -1
Current timestep = 356. State = [[-0.24297194  0.15721409]]. Action = [[-0.05665497  0.02290394  0.          0.30567467]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, False, True]
State prediction error at timestep 356 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 356 of -1
Current timestep = 357. State = [[-0.24242362  0.16150872]]. Action = [[ 0.02943694  0.04053324  0.         -0.9205959 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, False, True]
State prediction error at timestep 357 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 357 of -1
Current timestep = 358. State = [[-0.2377208   0.16678911]]. Action = [[ 0.07484407  0.05709454  0.         -0.88023627]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, False, True]
State prediction error at timestep 358 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 358 of -1
Current timestep = 359. State = [[-0.23237011  0.16876256]]. Action = [[ 0.05701711 -0.01541181  0.          0.7907345 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, False, True]
State prediction error at timestep 359 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 359 of -1
Current timestep = 360. State = [[-0.22886425  0.17288509]]. Action = [[0.02140577 0.06932817 0.         0.4184363 ]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, False, True]
State prediction error at timestep 360 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 360 of -1
Current timestep = 361. State = [[-0.22336213  0.1774289 ]]. Action = [[0.08354061 0.03199465 0.         0.8801179 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, False, True]
State prediction error at timestep 361 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 361 of -1
Current timestep = 362. State = [[-0.21863167  0.18106964]]. Action = [[ 0.02858181  0.03534275  0.         -0.43230295]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, False, True]
State prediction error at timestep 362 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 362 of -1
Current timestep = 363. State = [[-0.22127396  0.18824492]]. Action = [[-0.09303157  0.0977179   0.          0.87841713]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, False, True]
State prediction error at timestep 363 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 363 of -1
Current timestep = 364. State = [[-0.21836534  0.1896663 ]]. Action = [[ 0.09511226 -0.06680207  0.         -0.691121  ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, False, True]
State prediction error at timestep 364 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 364 of -1
Current timestep = 365. State = [[-0.21206562  0.18807116]]. Action = [[ 0.04973943 -0.02178986  0.          0.47031248]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, False, True]
State prediction error at timestep 365 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 365 of -1
Current timestep = 366. State = [[-0.21091925  0.18870306]]. Action = [[-0.04006532  0.00401475  0.         -0.720078  ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, False, True]
State prediction error at timestep 366 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 366 of -1
Current timestep = 367. State = [[-0.20724747  0.19307216]]. Action = [[ 0.07011909  0.064932    0.         -0.11763453]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, False, True]
State prediction error at timestep 367 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 367 of -1
Current timestep = 368. State = [[-0.20735116  0.19878633]]. Action = [[-0.06777605  0.05538065  0.         -0.5381417 ]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, False, True]
State prediction error at timestep 368 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 368 of 1
Current timestep = 369. State = [[-0.20888327  0.2050642 ]]. Action = [[-0.01065971  0.062773    0.         -0.53917235]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, False, True]
State prediction error at timestep 369 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 369 of -1
Current timestep = 370. State = [[-0.20433806  0.21225518]]. Action = [[ 0.09637087  0.08236635  0.         -0.45487916]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, False, True]
State prediction error at timestep 370 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 370 of -1
Current timestep = 371. State = [[-0.20143709  0.21900967]]. Action = [[-1.3457239e-04  6.5504752e-02  0.0000000e+00  2.4079061e-01]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, False, True]
State prediction error at timestep 371 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 371 of -1
Current timestep = 372. State = [[-0.19705433  0.2186735 ]]. Action = [[ 0.07556566 -0.07510747  0.          0.584273  ]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, False, True]
State prediction error at timestep 372 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 372 of -1
Current timestep = 373. State = [[-0.19622771  0.2157243 ]]. Action = [[-0.05243449 -0.0489438   0.         -0.85516757]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, False, False, True]
State prediction error at timestep 373 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 373 of -1
Current timestep = 374. State = [[-0.19235435  0.21532185]]. Action = [[ 0.08177032 -0.00418194  0.          0.53159785]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, False, False, True]
State prediction error at timestep 374 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 374 of 1
Current timestep = 375. State = [[-0.19045727  0.21321881]]. Action = [[-0.04378717 -0.06156622  0.         -0.83046967]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, False, False, True]
State prediction error at timestep 375 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 375 of 1
Current timestep = 376. State = [[-0.19311793  0.21533969]]. Action = [[-0.0643507   0.05480448  0.          0.4559852 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, False, False, True]
State prediction error at timestep 376 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 376 of 1
Current timestep = 377. State = [[-0.1922235   0.22187284]]. Action = [[ 0.04254618  0.08325287  0.         -0.10893625]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, False, False, True]
State prediction error at timestep 377 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 377 of 1
Current timestep = 378. State = [[-0.18983084  0.22810376]]. Action = [[0.02063062 0.06279432 0.         0.1324848 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, False, False, True]
State prediction error at timestep 378 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 378 of -1
Current timestep = 379. State = [[-0.19226521  0.23405895]]. Action = [[-0.06389721  0.06303269  0.          0.5863092 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, False, False, True]
State prediction error at timestep 379 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 379 of -1
Current timestep = 380. State = [[-0.19867739  0.23690264]]. Action = [[-0.09230301 -0.0132186   0.         -0.04685491]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, False, False, True]
State prediction error at timestep 380 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 380 of -1
Current timestep = 381. State = [[-0.20378956  0.23687744]]. Action = [[-0.04632097 -0.0362645   0.         -0.7539764 ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, False, False, True]
State prediction error at timestep 381 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 381 of -1
Current timestep = 382. State = [[-0.20541818  0.23290762]]. Action = [[-0.00218808 -0.09716233  0.          0.47490978]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, False, False, True]
State prediction error at timestep 382 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 382 of -1
Current timestep = 383. State = [[-0.20691018  0.23592739]]. Action = [[-0.02354284  0.09528761  0.         -0.87544537]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, False, False, True]
State prediction error at timestep 383 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 383 of -1
Current timestep = 384. State = [[-0.21194434  0.23980512]]. Action = [[-0.0719597   0.00293678  0.          0.6381141 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, False, False, True]
State prediction error at timestep 384 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 384 of -1
Current timestep = 385. State = [[-0.21546952  0.23798598]]. Action = [[-0.01295421 -0.06732561  0.         -0.34039134]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, False, False, True]
State prediction error at timestep 385 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 385 of -1
Current timestep = 386. State = [[-0.21932924  0.23330472]]. Action = [[-0.05822504 -0.07750628  0.         -0.5033074 ]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, False, False, True]
State prediction error at timestep 386 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.22528735  0.2322543 ]]. Action = [[-0.07505177  0.00910268  0.         -0.7739217 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, False, False, True]
State prediction error at timestep 387 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 387 of -1
Current timestep = 388. State = [[-0.23003231  0.23507789]]. Action = [[-0.02785141  0.04552043  0.          0.0638938 ]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, False, False, True]
State prediction error at timestep 388 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 388 of -1
Current timestep = 389. State = [[-0.22932917  0.23335166]]. Action = [[ 0.06240476 -0.05939515  0.          0.60719645]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, False, False, True]
State prediction error at timestep 389 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 389 of -1
Current timestep = 390. State = [[-0.23033331  0.23245811]]. Action = [[-0.03175573  0.02762956  0.         -0.02667022]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, False, False, True]
State prediction error at timestep 390 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 390 of -1
Current timestep = 391. State = [[-0.22953816  0.23146448]]. Action = [[ 0.05690522 -0.01994652  0.          0.6472572 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, False, False, True]
State prediction error at timestep 391 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 391 of -1
Current timestep = 392. State = [[-0.2326269   0.23372777]]. Action = [[-0.07182448  0.07605951  0.         -0.6686728 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, False, False, True]
State prediction error at timestep 392 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 392 of -1
Current timestep = 393. State = [[-0.23246455  0.23740119]]. Action = [[ 0.07328717  0.04693804  0.         -0.38306284]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, False, False, True]
State prediction error at timestep 393 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 393 of -1
Current timestep = 394. State = [[-0.23159787  0.24049997]]. Action = [[0.00527088 0.05325287 0.         0.32737732]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, False, False, True]
State prediction error at timestep 394 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 394 of -1
Current timestep = 395. State = [[-0.23182368  0.2415109 ]]. Action = [[0.01276497 0.00107197 0.         0.14370322]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, False, False, True]
State prediction error at timestep 395 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 395 of -1
Current timestep = 396. State = [[-0.22759566  0.2385715 ]]. Action = [[ 0.09090472 -0.05159772  0.         -0.60727155]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, False, False, True]
State prediction error at timestep 396 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 396 of -1
Current timestep = 397. State = [[-0.22625525  0.23899294]]. Action = [[-0.02335499  0.05118703  0.         -0.22794706]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, False, False, True]
State prediction error at timestep 397 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 397 of -1
Current timestep = 398. State = [[-0.23155047  0.23927873]]. Action = [[-0.09820788 -0.02550372  0.         -0.0194788 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, False, False, True]
State prediction error at timestep 398 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 398 of -1
Current timestep = 399. State = [[-0.23558985  0.23891394]]. Action = [[-0.02346937 -0.00748015  0.         -0.1486373 ]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, False, False, True]
State prediction error at timestep 399 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 399 of -1
Current timestep = 400. State = [[-0.23798108  0.23485243]]. Action = [[-0.03007262 -0.09045611  0.         -0.17470038]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, False, False, True]
State prediction error at timestep 400 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 400 of 1
Current timestep = 401. State = [[-0.2368747   0.23049177]]. Action = [[ 0.0368527  -0.04493433  0.         -0.98022056]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, False, False, True]
State prediction error at timestep 401 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 401 of -1
Current timestep = 402. State = [[-0.23975304  0.2299534 ]]. Action = [[-0.08274085  0.01690868  0.          0.7815509 ]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, False, False, True]
State prediction error at timestep 402 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 402 of -1
Current timestep = 403. State = [[-0.24041532  0.22633518]]. Action = [[ 0.03344839 -0.0845607   0.         -0.6396431 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, False, False, True]
State prediction error at timestep 403 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 403 of -1
Current timestep = 404. State = [[-0.23641911  0.22385655]]. Action = [[ 0.06150333  0.0084845   0.         -0.44050515]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, False, False, True]
State prediction error at timestep 404 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 404 of -1
Current timestep = 405. State = [[-0.23525874  0.22587316]]. Action = [[-0.01273271  0.05856849  0.         -0.5229008 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, False, False, True]
State prediction error at timestep 405 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 405 of -1
Current timestep = 406. State = [[-0.23470962  0.2247824 ]]. Action = [[ 0.01901896 -0.03988544  0.         -0.61303943]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, False, False, True]
State prediction error at timestep 406 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 406 of 1
Current timestep = 407. State = [[-0.2372976  0.2261119]]. Action = [[-0.06223565  0.06068463  0.         -0.42347652]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, False, False, True]
State prediction error at timestep 407 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 407 of -1
Current timestep = 408. State = [[-0.23577796  0.23200919]]. Action = [[0.08215215 0.0996324  0.         0.62371314]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, False, False, True]
State prediction error at timestep 408 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 408 of 1
Current timestep = 409. State = [[-0.23664367  0.238     ]]. Action = [[-0.04977484  0.07184955  0.         -0.5046928 ]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, False, False, True]
State prediction error at timestep 409 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 409 of -1
Current timestep = 410. State = [[-0.23840934  0.2385915 ]]. Action = [[ 0.00528393 -0.03578279  0.         -0.50003934]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, False, False, True]
State prediction error at timestep 410 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 410 of 1
Current timestep = 411. State = [[-0.23957051  0.2377536 ]]. Action = [[-0.01648204 -0.00873689  0.          0.72517085]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, False, False, True]
State prediction error at timestep 411 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 411 of -1
Current timestep = 412. State = [[-0.23836578  0.23358189]]. Action = [[ 0.03673884 -0.0904564   0.          0.39758492]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, False, False, True]
State prediction error at timestep 412 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 412 of -1
Current timestep = 413. State = [[-0.23425236  0.22842799]]. Action = [[ 0.05448877 -0.0575232   0.         -0.6019645 ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, False, False, True]
State prediction error at timestep 413 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 413 of -1
Current timestep = 414. State = [[-0.23063631  0.22288308]]. Action = [[ 0.02421378 -0.07285486  0.          0.84631777]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, False, False, True]
State prediction error at timestep 414 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 414 of -1
Current timestep = 415. State = [[-0.22777823  0.22405581]]. Action = [[ 0.02427882  0.07933974  0.         -0.8995021 ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, False, False, True]
State prediction error at timestep 415 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 415 of 1
Current timestep = 416. State = [[-0.22869937  0.22725126]]. Action = [[-0.04466163  0.03321726  0.         -0.40828693]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, False, False, True]
State prediction error at timestep 416 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 416 of -1
Current timestep = 417. State = [[-0.22648507  0.22686657]]. Action = [[ 0.06100661 -0.02285203  0.          0.5721942 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, False, False, True]
State prediction error at timestep 417 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 417 of -1
Current timestep = 418. State = [[-0.22491524  0.22158673]]. Action = [[-0.01802653 -0.08777717  0.          0.15558994]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, False, False, True]
State prediction error at timestep 418 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 418 of -1
Current timestep = 419. State = [[-0.22592987  0.2220217 ]]. Action = [[-0.02814873  0.06547583  0.          0.26586127]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, False, False, True]
State prediction error at timestep 419 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 419 of 1
Current timestep = 420. State = [[-0.22225492  0.2204083 ]]. Action = [[ 0.08361981 -0.05996719  0.         -0.05597979]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, False, False, True]
State prediction error at timestep 420 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 420 of 1
Current timestep = 421. State = [[-0.21509835  0.22151564]]. Action = [[0.08841703 0.07598176 0.         0.42364597]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, False, False, True]
State prediction error at timestep 421 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 421 of 1
Current timestep = 422. State = [[-0.21128888  0.21815023]]. Action = [[ 0.00448481 -0.09365012  0.         -0.2579589 ]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 422 is [True, False, False, False, False, True]
State prediction error at timestep 422 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 422 of 1
Current timestep = 423. State = [[-0.20550077  0.21243772]]. Action = [[ 0.09553888 -0.04208049  0.          0.56103563]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 423 is [True, False, False, False, False, True]
State prediction error at timestep 423 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 423 of 1
Current timestep = 424. State = [[-0.20604093  0.20904328]]. Action = [[-0.09282846 -0.02113859  0.         -0.6107143 ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 424 is [True, False, False, False, False, True]
State prediction error at timestep 424 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 424 of 1
Current timestep = 425. State = [[-0.20458995  0.20984966]]. Action = [[ 0.07280094  0.05354176  0.         -0.68180716]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 425 is [True, False, False, False, False, True]
State prediction error at timestep 425 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 425 of 1
Current timestep = 426. State = [[-0.19796056  0.21460198]]. Action = [[0.08587762 0.09256317 0.         0.7614174 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 426 is [True, False, False, False, False, True]
State prediction error at timestep 426 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 426 of 1
Current timestep = 427. State = [[-0.19791283  0.22010691]]. Action = [[-0.06074486  0.07431664  0.          0.43986464]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 427 is [True, False, False, False, False, True]
State prediction error at timestep 427 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 427 of 1
Current timestep = 428. State = [[-0.20065637  0.22536968]]. Action = [[-0.02163853  0.06313164  0.          0.29853678]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 428 is [True, False, False, False, False, True]
State prediction error at timestep 428 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 428 of 1
Current timestep = 429. State = [[-0.19715749  0.22934389]]. Action = [[0.0933808  0.03906656 0.         0.83625543]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 429 is [True, False, False, False, False, True]
State prediction error at timestep 429 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 429 of -1
Current timestep = 430. State = [[-0.19503948  0.23008968]]. Action = [[-0.00917687 -0.01555316  0.          0.7165381 ]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 430 is [True, False, False, False, False, True]
State prediction error at timestep 430 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 430 of -1
Current timestep = 431. State = [[-0.19743757  0.2298215 ]]. Action = [[-0.05073195 -0.01562975  0.         -0.8553612 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 431 is [True, False, False, False, False, True]
State prediction error at timestep 431 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 431 of -1
Current timestep = 432. State = [[-0.19435772  0.23452368]]. Action = [[0.09820964 0.09179435 0.         0.25062323]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 432 is [True, False, False, False, False, True]
State prediction error at timestep 432 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 432 of 1
Current timestep = 433. State = [[-0.19448921  0.2341964 ]]. Action = [[-0.08900841 -0.09500899  0.         -0.3039654 ]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 433 is [True, False, False, False, False, True]
State prediction error at timestep 433 is tensor(2.1487e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 433 of 1
Current timestep = 434. State = [[-0.19435325  0.23550327]]. Action = [[ 0.05512536  0.06294382  0.         -0.31056035]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 434 is [True, False, False, False, False, True]
State prediction error at timestep 434 is tensor(9.9544e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 434 of -1
Current timestep = 435. State = [[-0.18851188  0.23554459]]. Action = [[ 0.08333824 -0.0439623   0.          0.5043714 ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 435 is [True, False, False, False, False, True]
State prediction error at timestep 435 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 435 of -1
Current timestep = 436. State = [[-0.18961135  0.23774934]]. Action = [[-0.08747717  0.05921862  0.          0.32845688]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 436 is [True, False, False, False, False, True]
State prediction error at timestep 436 is tensor(9.1154e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 436 of -1
Current timestep = 437. State = [[-0.19242635  0.2419658 ]]. Action = [[-0.01713271  0.02962852  0.          0.8675741 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 437 is [True, False, False, False, False, True]
State prediction error at timestep 437 is tensor(9.1185e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 437 of -1
Current timestep = 438. State = [[-0.19235542  0.24729036]]. Action = [[ 0.01382098  0.06735513  0.         -0.89322644]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 438 is [True, False, False, False, False, True]
State prediction error at timestep 438 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 438 of -1
Current timestep = 439. State = [[-0.19533367  0.25345322]]. Action = [[-0.06140321  0.0629021   0.         -0.73379135]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 439 is [True, False, False, False, False, True]
State prediction error at timestep 439 is tensor(7.3635e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 439 of -1
Current timestep = 440. State = [[-0.19481115  0.25188056]]. Action = [[ 0.05092891 -0.09908228  0.          0.9116061 ]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 440 is [True, False, False, False, False, True]
State prediction error at timestep 440 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 440 of -1
Current timestep = 441. State = [[-0.19092429  0.25408146]]. Action = [[ 0.05047276  0.08357375  0.         -0.7979525 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 441 is [True, False, False, False, False, True]
State prediction error at timestep 441 is tensor(5.4106e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 441 of -1
Current timestep = 442. State = [[-0.19210693  0.25445065]]. Action = [[-0.06540577 -0.06140974  0.         -0.5784062 ]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 442 is [True, False, False, False, False, True]
State prediction error at timestep 442 is tensor(4.1546e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 442 of -1
Current timestep = 443. State = [[-0.19463582  0.25514883]]. Action = [[-0.0153484   0.02794228  0.          0.49793196]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 443 is [True, False, False, False, False, True]
State prediction error at timestep 443 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 443 of -1
Current timestep = 444. State = [[-0.19268627  0.2543985 ]]. Action = [[ 0.05443806 -0.04070197  0.         -0.6794142 ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 444 is [True, False, False, False, False, True]
State prediction error at timestep 444 is tensor(9.1898e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 444 of -1
Current timestep = 445. State = [[-0.19383751  0.25192434]]. Action = [[-0.05786612 -0.03477468  0.         -0.63671744]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 445 is [True, False, False, False, False, True]
State prediction error at timestep 445 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 445 of -1
Current timestep = 446. State = [[-0.1997204   0.25439432]]. Action = [[-0.08956522  0.05799527  0.         -0.12272441]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 446 is [True, False, False, False, False, True]
State prediction error at timestep 446 is tensor(6.2215e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 446 of -1
Current timestep = 447. State = [[-0.20157738  0.2535006 ]]. Action = [[ 0.02260797 -0.0645777   0.         -0.4696747 ]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 447 is [True, False, False, False, False, True]
State prediction error at timestep 447 is tensor(1.1441e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 447 of -1
Current timestep = 448. State = [[-0.1979766   0.24872012]]. Action = [[ 0.06399851 -0.06434834  0.          0.13115835]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 448 is [True, False, False, False, False, True]
State prediction error at timestep 448 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 448 of -1
Current timestep = 449. State = [[-0.20060474  0.24722748]]. Action = [[-0.09491745  0.0136869   0.         -0.52600825]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 449 is [True, False, False, False, False, True]
State prediction error at timestep 449 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 449 of -1
Current timestep = 450. State = [[-0.20221783  0.24843366]]. Action = [[0.02478312 0.0181251  0.         0.81893253]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 450 is [True, False, False, False, False, True]
State prediction error at timestep 450 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 450 of 1
Current timestep = 451. State = [[-0.19992992  0.24631204]]. Action = [[ 0.04099721 -0.04676585  0.         -0.30517304]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 451 is [True, False, False, False, False, True]
State prediction error at timestep 451 is tensor(2.7120e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 451 of -1
Current timestep = 452. State = [[-0.20101142  0.24105078]]. Action = [[-0.04855629 -0.07225333  0.         -0.34252006]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 452 is [True, False, False, False, False, True]
State prediction error at timestep 452 is tensor(9.0944e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 452 of -1
Current timestep = 453. State = [[-0.20222522  0.23440003]]. Action = [[-0.00399111 -0.08595651  0.          0.5274439 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 453 is [True, False, False, False, False, True]
State prediction error at timestep 453 is tensor(9.7535e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 453 of 1
Current timestep = 454. State = [[-0.19982193  0.22949861]]. Action = [[ 0.0448261  -0.03175805  0.          0.7996013 ]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 454 is [True, False, False, False, False, True]
State prediction error at timestep 454 is tensor(5.9836e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 454 of 1
Current timestep = 455. State = [[-0.20099919  0.23079525]]. Action = [[-0.0524477   0.07288771  0.          0.63803136]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 455 is [True, False, False, False, False, True]
State prediction error at timestep 455 is tensor(3.5080e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 455 of 1
Current timestep = 456. State = [[-0.20047957  0.23055615]]. Action = [[ 0.04596975 -0.02329878  0.          0.09812081]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 456 is [True, False, False, False, False, True]
State prediction error at timestep 456 is tensor(6.0596e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 456 of 1
Current timestep = 457. State = [[-0.19832651  0.23034477]]. Action = [[ 0.02457257  0.03323007  0.         -0.98591423]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 457 is [True, False, False, False, False, True]
State prediction error at timestep 457 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 457 of 1
Current timestep = 458. State = [[-0.19801337  0.23360749]]. Action = [[0.00117646 0.0735467  0.         0.7970668 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 458 is [True, False, False, False, False, True]
State prediction error at timestep 458 is tensor(4.9719e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 458 of 1
Current timestep = 459. State = [[-0.2025682   0.23740704]]. Action = [[-0.07842904  0.04938451  0.         -0.22239071]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 459 is [True, False, False, False, False, True]
State prediction error at timestep 459 is tensor(9.6966e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 459 of -1
Current timestep = 460. State = [[-0.20504093  0.23524806]]. Action = [[ 0.0105911  -0.07287379  0.          0.8600106 ]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 460 is [True, False, False, False, False, True]
State prediction error at timestep 460 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 460 of -1
Current timestep = 461. State = [[-0.20399816  0.23272835]]. Action = [[ 0.02701629 -0.00846659  0.         -0.9428489 ]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 461 is [True, False, False, False, False, True]
State prediction error at timestep 461 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 461 of -1
Current timestep = 462. State = [[-0.204783    0.23147035]]. Action = [[-0.02456363 -0.01431256  0.          0.5849291 ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 462 is [True, False, False, False, False, True]
State prediction error at timestep 462 is tensor(5.9407e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 462 of -1
Current timestep = 463. State = [[-0.20891261  0.2284909 ]]. Action = [[-0.06719143 -0.05437079  0.          0.72125053]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 463 is [True, False, False, False, False, True]
State prediction error at timestep 463 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 463 of -1
Current timestep = 464. State = [[-0.20678954  0.2273258 ]]. Action = [[0.09307367 0.01201906 0.         0.5924376 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 464 is [True, False, False, False, False, True]
State prediction error at timestep 464 is tensor(8.2941e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 464 of -1
Current timestep = 465. State = [[-0.20614229  0.23011474]]. Action = [[-0.03891727  0.05693955  0.          0.8701161 ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 465 is [True, False, False, False, False, True]
State prediction error at timestep 465 is tensor(8.0955e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 465 of -1
Current timestep = 466. State = [[-0.20269832  0.23523104]]. Action = [[0.09557416 0.07630911 0.         0.537693  ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 466 is [True, False, False, False, False, True]
State prediction error at timestep 466 is tensor(9.4760e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 466 of 1
Current timestep = 467. State = [[-0.20321663  0.24119379]]. Action = [[-0.06127552  0.0787195   0.          0.42449617]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 467 is [True, False, False, False, False, True]
State prediction error at timestep 467 is tensor(7.1301e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 467 of -1
Current timestep = 468. State = [[-0.20836072  0.24177586]]. Action = [[-0.06667445 -0.04907893  0.         -0.44623613]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 468 is [True, False, False, False, False, True]
State prediction error at timestep 468 is tensor(7.4533e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 468 of -1
Current timestep = 469. State = [[-0.21088977  0.24120271]]. Action = [[-0.0079376  -0.00806714  0.          0.894361  ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 469 is [True, False, False, False, False, True]
State prediction error at timestep 469 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 469 of -1
Current timestep = 470. State = [[-0.21365717  0.23813711]]. Action = [[-0.04706718 -0.07766546  0.         -0.84688044]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 470 is [True, False, False, False, False, True]
State prediction error at timestep 470 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 470 of -1
Current timestep = 471. State = [[-0.21838337  0.23854324]]. Action = [[-0.06737588  0.03058458  0.         -0.4802603 ]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 471 is [True, False, False, False, False, True]
State prediction error at timestep 471 is tensor(7.5808e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 471 of -1
Current timestep = 472. State = [[-0.22513856  0.23785502]]. Action = [[-0.08832894 -0.0528245   0.         -0.30639994]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 472 is [True, False, False, False, False, True]
State prediction error at timestep 472 is tensor(3.2716e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 472 of -1
Current timestep = 473. State = [[-0.22829942  0.2335478 ]]. Action = [[-0.00240692 -0.08042473  0.          0.83165526]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 473 is [True, False, False, False, False, True]
State prediction error at timestep 473 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 473 of -1
Current timestep = 474. State = [[-0.22740275  0.22984993]]. Action = [[ 0.03025457 -0.03466088  0.         -0.51026905]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 474 is [True, False, False, False, False, True]
State prediction error at timestep 474 is tensor(4.9660e-05, device='cuda:0', grad_fn=<MseLossBackward>)
