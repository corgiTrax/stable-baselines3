Current timestep = 0. State = [[-0.32604954 -0.08628346]]. Action = [[0.00435984 0.09830839 0.         0.7271644 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0668, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of None
Current timestep = 1. State = [[-0.33171427 -0.08338373]]. Action = [[-0.08950701 -0.00990611  0.          0.9261981 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0582, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of None
Current timestep = 2. State = [[-0.33283383 -0.08530018]]. Action = [[ 0.05919524 -0.05022868  0.          0.01754153]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0498, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of None
Current timestep = 3. State = [[-0.32827222 -0.08361281]]. Action = [[ 0.07941509  0.0483874   0.         -0.50816584]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0282, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of None
Current timestep = 4. State = [[-0.3254116  -0.08398479]]. Action = [[ 0.02239919 -0.05190226  0.          0.06075954]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0375, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of None
Current timestep = 5. State = [[-0.3279671  -0.08313566]]. Action = [[-0.06503823  0.04295453  0.         -0.71753365]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of None
Current timestep = 6. State = [[-0.33477935 -0.08682702]]. Action = [[-0.09400872 -0.0952624   0.         -0.55266756]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of None
Current timestep = 7. State = [[-0.3423005  -0.08953982]]. Action = [[-0.08573388  0.01543184  0.          0.75047183]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0277, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of None
Current timestep = 8. State = [[-0.34299555 -0.0931111 ]]. Action = [[ 0.06539624 -0.06995545  0.          0.36012435]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0267, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of None
Current timestep = 9. State = [[-0.33896556 -0.09817613]]. Action = [[ 0.05116934 -0.04899728  0.          0.96619225]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0148, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of None
Current timestep = 10. State = [[-0.34125704 -0.09601232]]. Action = [[-0.09370168  0.09744289  0.          0.9564408 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0123, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of None
Current timestep = 11. State = [[-0.34794155 -0.08941568]]. Action = [[-0.06642361  0.08775141  0.         -0.7740712 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of None
Current timestep = 12. State = [[-0.3524674  -0.09067384]]. Action = [[-0.02062605 -0.08017505  0.         -0.9536822 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of None
Current timestep = 13. State = [[-0.35143182 -0.09707875]]. Action = [[ 0.06866229 -0.083746    0.          0.8004637 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of None
Current timestep = 14. State = [[-0.3490534  -0.10334155]]. Action = [[ 0.03622013 -0.07354388  0.          0.9915631 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of None
Current timestep = 15. State = [[-0.34913963 -0.10775554]]. Action = [[-0.00907946 -0.03376415  0.         -0.21082371]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of None
Current timestep = 16. State = [[-0.3515684  -0.11455903]]. Action = [[-0.03354903 -0.0951744   0.         -0.46467394]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of None
Current timestep = 17. State = [[-0.35036102 -0.11521903]]. Action = [[ 0.06343738  0.07837261  0.         -0.04249555]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of None
Current timestep = 18. State = [[-0.3524108  -0.11073043]]. Action = [[-0.07434311  0.06280682  0.         -0.7986902 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 18 of None
Current timestep = 19. State = [[-0.35568303 -0.10865618]]. Action = [[-0.01442628  0.01446489  0.         -0.61173034]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(8.1447e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 19 of None
Current timestep = 20. State = [[-0.3526776  -0.10687124]]. Action = [[0.08623167 0.02300803 0.         0.31270623]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 20 of None
Current timestep = 21. State = [[-0.35496762 -0.10565591]]. Action = [[-0.09344417  0.00754379  0.          0.03627145]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 21 of None
Current timestep = 22. State = [[-0.35567173 -0.1096269 ]]. Action = [[ 0.06830675 -0.09284499  0.         -0.53725266]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 22 of None
Current timestep = 23. State = [[-0.34997484 -0.11670735]]. Action = [[ 0.08822257 -0.09029897  0.          0.20686066]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 23 of None
Current timestep = 24. State = [[-0.34345004 -0.12370998]]. Action = [[ 0.07326324 -0.08367714  0.         -0.62574905]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 24 of None
Current timestep = 25. State = [[-0.33743477 -0.13015684]]. Action = [[ 0.05893933 -0.06729642  0.         -0.7062875 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, True, False, False]
State prediction error at timestep 25 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 25 of None
Current timestep = 26. State = [[-0.3335562 -0.1360468]]. Action = [[ 0.01803955 -0.05434812  0.         -0.49133933]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, True, False, False]
State prediction error at timestep 26 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 26 of None
Current timestep = 27. State = [[-0.3279426  -0.14183766]]. Action = [[ 0.07475772 -0.05114451  0.         -0.83698577]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, True, False, False]
State prediction error at timestep 27 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 27 of None
Current timestep = 28. State = [[-0.32778814 -0.14228137]]. Action = [[-0.07985155  0.07023907  0.          0.716177  ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, True, False, False]
State prediction error at timestep 28 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 28 of None
Current timestep = 29. State = [[-0.3257879  -0.13905415]]. Action = [[ 0.07731897  0.0548152   0.         -0.19668382]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, True, False, False]
State prediction error at timestep 29 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 29 of None
Current timestep = 30. State = [[-0.32135898 -0.13593896]]. Action = [[0.03792714 0.03898365 0.         0.9145137 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, True, False, False]
State prediction error at timestep 30 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 30 of None
Current timestep = 31. State = [[-0.32012093 -0.13237548]]. Action = [[-0.00734532  0.05215795  0.         -0.68890154]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, True, False, False]
State prediction error at timestep 31 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 31 of None
Current timestep = 32. State = [[-0.3156949  -0.13425143]]. Action = [[ 0.09171552 -0.07368196  0.         -0.01193464]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, True, False, False]
State prediction error at timestep 32 is tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 32 of None
Current timestep = 33. State = [[-0.31349254 -0.1335877 ]]. Action = [[-0.03082895  0.06172993  0.         -0.43724132]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, True, False, False]
State prediction error at timestep 33 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 33 of None
Current timestep = 34. State = [[-0.31570745 -0.1293414 ]]. Action = [[-0.03572042  0.05166627  0.         -0.984189  ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, True, False, False]
State prediction error at timestep 34 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 34 of None
Current timestep = 35. State = [[-0.3144213  -0.12887667]]. Action = [[ 0.04760996 -0.03087246  0.          0.73354495]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, True, False, False]
State prediction error at timestep 35 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 35 of None
Current timestep = 36. State = [[-0.3164821 -0.131975 ]]. Action = [[-0.07450292 -0.05111818  0.         -0.9411027 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, True, False, False]
State prediction error at timestep 36 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 36 of None
Current timestep = 37. State = [[-0.31953534 -0.13823329]]. Action = [[-0.02443783 -0.08774377  0.         -0.680884  ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, True, False, False]
State prediction error at timestep 37 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 37 of None
Current timestep = 38. State = [[-0.31880757 -0.14025575]]. Action = [[ 0.02529877  0.02194778  0.         -0.69732094]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, True, False, False]
State prediction error at timestep 38 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 38 of None
Current timestep = 39. State = [[-0.32189897 -0.13661738]]. Action = [[-0.08657759  0.07219615  0.         -0.52458906]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, True, False, False]
State prediction error at timestep 39 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 39 of None
Current timestep = 40. State = [[-0.3218579  -0.13827516]]. Action = [[ 0.0623932 -0.0812559  0.        -0.505362 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, True, False, False]
State prediction error at timestep 40 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 40 of None
Current timestep = 41. State = [[-0.31472388 -0.14271297]]. Action = [[ 0.0969845  -0.04060469  0.         -0.45733178]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, True, False, False]
State prediction error at timestep 41 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 41 of None
Current timestep = 42. State = [[-0.30885214 -0.1393448 ]]. Action = [[ 0.03615708  0.09605473  0.         -0.7085937 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 42 of None
Current timestep = 43. State = [[-0.3075256 -0.1385312]]. Action = [[-0.01379284 -0.03964251  0.          0.885164  ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, True, False, False]
State prediction error at timestep 43 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 43 of None
Current timestep = 44. State = [[-0.30511642 -0.13938357]]. Action = [[ 0.04331518  0.00724858  0.         -0.32532144]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, True, False, False]
State prediction error at timestep 44 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 44 of None
Current timestep = 45. State = [[-0.3004675  -0.14153159]]. Action = [[ 0.05608428 -0.04577896  0.          0.5140588 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, True, False, False]
State prediction error at timestep 45 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 45 of None
Current timestep = 46. State = [[-0.2973581  -0.14006814]]. Action = [[0.01304604 0.0577567  0.         0.5734048 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, True, False, False]
State prediction error at timestep 46 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 46 of None
Current timestep = 47. State = [[-0.29775846 -0.13404134]]. Action = [[-0.03035113  0.08973774  0.         -0.25634837]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, True, False, False]
State prediction error at timestep 47 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 47 of None
Current timestep = 48. State = [[-0.2946827  -0.13356327]]. Action = [[ 0.07656788 -0.05497192  0.         -0.7528407 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, True, False, False]
State prediction error at timestep 48 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 48 of None
Current timestep = 49. State = [[-0.28808215 -0.13106784]]. Action = [[ 0.07046223  0.06966154  0.         -0.49434084]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, True, False, False]
State prediction error at timestep 49 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 49 of None
Current timestep = 50. State = [[-0.28263673 -0.12450146]]. Action = [[ 0.04658066  0.07537553  0.         -0.86270106]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 50 of None
Current timestep = 51. State = [[-0.2849465  -0.12122615]]. Action = [[-0.09492144  0.00337236  0.          0.3534478 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 51 of None
Current timestep = 52. State = [[-0.2884364  -0.12493194]]. Action = [[-0.03289726 -0.0846393   0.         -0.5317908 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 52 of None
Current timestep = 53. State = [[-0.2924004  -0.12905939]]. Action = [[-0.07372832 -0.03210803  0.         -0.3655308 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, True, False, False]
State prediction error at timestep 53 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 53 of None
Current timestep = 54. State = [[-0.29175925 -0.1345024 ]]. Action = [[ 0.04515976 -0.08424617  0.         -0.08591765]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, True, False, False]
State prediction error at timestep 54 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 54 of None
Current timestep = 55. State = [[-0.29224348 -0.14118977]]. Action = [[-0.05234873 -0.07303961  0.          0.24395955]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, True, False, False]
State prediction error at timestep 55 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 55 of None
Current timestep = 56. State = [[-0.2935808  -0.14767027]]. Action = [[-0.01509072 -0.06223612  0.         -0.30916786]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, True, False, False]
State prediction error at timestep 56 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 56 of None
Current timestep = 57. State = [[-0.296698   -0.14820628]]. Action = [[-0.06777793  0.05757075  0.          0.8447294 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, True, False, False]
State prediction error at timestep 57 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 57 of None
Current timestep = 58. State = [[-0.29488915 -0.1444477 ]]. Action = [[0.0731707  0.06383226 0.         0.16354978]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, True, False, False]
State prediction error at timestep 58 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 58 of None
Current timestep = 59. State = [[-0.29312417 -0.13960184]]. Action = [[-0.00756636  0.06271797  0.         -0.6371225 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, True, False, False]
State prediction error at timestep 59 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 59 of None
Current timestep = 60. State = [[-0.29052037 -0.13890968]]. Action = [[ 0.05789507 -0.02778776  0.          0.34296846]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, True, False, False]
State prediction error at timestep 60 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 60 of None
Current timestep = 61. State = [[-0.29315203 -0.14057347]]. Action = [[-0.08712094 -0.01703764  0.         -0.6709282 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, True, False, False]
State prediction error at timestep 61 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 61 of None
Current timestep = 62. State = [[-0.2946062  -0.14345214]]. Action = [[ 0.02928352 -0.04201383  0.          0.7420423 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, True, False, False]
State prediction error at timestep 62 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 62 of None
Current timestep = 63. State = [[-0.2921044  -0.14441872]]. Action = [[ 0.04457492  0.00559046  0.         -0.58027   ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, True, False, False]
State prediction error at timestep 63 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 63 of None
Current timestep = 64. State = [[-0.2934974  -0.14515385]]. Action = [[-0.04956371 -0.01710913  0.          0.08260489]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, True, False, False]
State prediction error at timestep 64 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 64 of None
Current timestep = 65. State = [[-0.2969364  -0.14632499]]. Action = [[-0.03606131 -0.00573687  0.          0.5306635 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, True, False, False]
State prediction error at timestep 65 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 65 of None
Current timestep = 66. State = [[-0.30299938 -0.14961572]]. Action = [[-0.09214138 -0.04942728  0.          0.7834394 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, True, False, False]
State prediction error at timestep 66 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 66 of None
Current timestep = 67. State = [[-0.3087747  -0.15668295]]. Action = [[-0.05005399 -0.09348495  0.         -0.9126737 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, True, False, False]
State prediction error at timestep 67 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 67 of None
Current timestep = 68. State = [[-0.30875462 -0.1625674 ]]. Action = [[ 0.04520542 -0.04233116  0.         -0.11649919]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, True, False, False]
State prediction error at timestep 68 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 68 of None
Current timestep = 69. State = [[-0.3051849  -0.16444488]]. Action = [[ 0.0544023   0.00353122  0.         -0.17321903]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, True, False, False]
State prediction error at timestep 69 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 69 of None
Current timestep = 70. State = [[-0.29955998 -0.16759995]]. Action = [[ 0.08436743 -0.05597566  0.         -0.7629528 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, True, False, False]
State prediction error at timestep 70 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 70 of None
Current timestep = 71. State = [[-0.30078626 -0.17202455]]. Action = [[-0.07863273 -0.0375288   0.          0.44133997]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, True, False, False]
State prediction error at timestep 71 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 71 of None
Current timestep = 72. State = [[-0.30629405 -0.17581381]]. Action = [[-0.05588006 -0.02587659  0.         -0.6986717 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, True, False, False]
State prediction error at timestep 72 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 72 of None
Current timestep = 73. State = [[-0.30621335 -0.17921014]]. Action = [[ 0.05203088 -0.02631958  0.          0.7714429 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, True, False, False]
State prediction error at timestep 73 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 73 of None
Current timestep = 74. State = [[-0.30612662 -0.18541655]]. Action = [[-0.01667716 -0.08764011  0.          0.54999256]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, True, False, False]
State prediction error at timestep 74 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 74 of None
Current timestep = 75. State = [[-0.30335984 -0.19070832]]. Action = [[ 0.07094366 -0.03047304  0.         -0.06648868]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, True, False, False]
State prediction error at timestep 75 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 75 of None
Current timestep = 76. State = [[-0.29732487 -0.19472417]]. Action = [[ 0.08019779 -0.04101266  0.         -0.00120997]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, True, False, False]
State prediction error at timestep 76 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 76 of None
Current timestep = 77. State = [[-0.29738328 -0.1988749 ]]. Action = [[-0.05424083 -0.03388353  0.          0.60991466]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, True, False, False]
State prediction error at timestep 77 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 77 of None
Current timestep = 78. State = [[-0.29488298 -0.19722204]]. Action = [[ 0.08216538  0.08587495  0.         -0.92393196]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, True, False, False]
State prediction error at timestep 78 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 78 of None
Current timestep = 79. State = [[-0.29323092 -0.19543213]]. Action = [[-0.01305099  0.00414294  0.          0.280133  ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, True, False, False]
State prediction error at timestep 79 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 79 of None
Current timestep = 80. State = [[-0.28953698 -0.19895992]]. Action = [[ 0.08208812 -0.06493977  0.         -0.8023042 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, True, False, False]
State prediction error at timestep 80 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 80 of None
Current timestep = 81. State = [[-0.29066485 -0.19782409]]. Action = [[-0.0863136   0.08920132  0.         -0.5418714 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, True, False, False]
State prediction error at timestep 81 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 81 of None
Current timestep = 82. State = [[-0.29197884 -0.19356984]]. Action = [[ 0.03636044  0.03875247  0.         -0.53592783]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, True, False, False]
State prediction error at timestep 82 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 82 of None
Current timestep = 83. State = [[-0.28895164 -0.19397621]]. Action = [[ 0.05714736 -0.04794896  0.          0.54164505]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, True, False, False]
State prediction error at timestep 83 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 83 of None
Current timestep = 84. State = [[-0.28954238 -0.19054149]]. Action = [[-0.04169829  0.08812124  0.          0.16435027]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, True, False, False]
State prediction error at timestep 84 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 84 of None
Current timestep = 85. State = [[-0.29637164 -0.18881868]]. Action = [[-0.09386183 -0.02766778  0.         -0.4471004 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 85 of None
Current timestep = 86. State = [[-0.30292216 -0.18676987]]. Action = [[-0.05021065  0.04696802  0.         -0.31456292]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, True, False, False]
State prediction error at timestep 86 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 86 of None
Current timestep = 87. State = [[-0.30942342 -0.18381153]]. Action = [[-0.07318203  0.02330372  0.         -0.94092625]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, True, False, False]
State prediction error at timestep 87 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 87 of None
Current timestep = 88. State = [[-0.31107888 -0.17933653]]. Action = [[ 0.03608038  0.0597463   0.         -0.1485101 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, True, False, False]
State prediction error at timestep 88 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 88 of None
Current timestep = 89. State = [[-0.3093929  -0.17189798]]. Action = [[0.03588101 0.08424745 0.         0.72728515]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, True, False, False]
State prediction error at timestep 89 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 89 of None
Current timestep = 90. State = [[-0.3074474  -0.16868968]]. Action = [[ 0.03582566 -0.02468487  0.         -0.57426333]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, True, False, False]
State prediction error at timestep 90 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 90 of None
Current timestep = 91. State = [[-0.30842522 -0.17246784]]. Action = [[-0.02372159 -0.09848043  0.          0.65254605]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, True, False, False]
State prediction error at timestep 91 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 91 of None
Current timestep = 92. State = [[-0.31231016 -0.17773314]]. Action = [[-0.04921351 -0.06410162  0.         -0.21959925]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, True, False, False]
State prediction error at timestep 92 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 92 of None
Current timestep = 93. State = [[-0.3121777  -0.17555882]]. Action = [[ 0.04527733  0.08112764  0.         -0.9027257 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, True, False, False]
State prediction error at timestep 93 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 93 of None
Current timestep = 94. State = [[-0.31248632 -0.1751134 ]]. Action = [[-0.0250743  -0.04552994  0.         -0.7957489 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, True, False, False]
State prediction error at timestep 94 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 94 of None
Current timestep = 95. State = [[-0.31284434 -0.17221117]]. Action = [[ 0.01128552  0.07857841  0.         -0.953551  ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, True, False, False]
State prediction error at timestep 95 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 95 of None
Current timestep = 96. State = [[-0.31393522 -0.17263283]]. Action = [[-0.02295548 -0.05828817  0.         -0.79793316]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, True, False, False]
State prediction error at timestep 96 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 96 of None
Current timestep = 97. State = [[-0.31114185 -0.17453703]]. Action = [[ 0.07296499 -0.01049364  0.         -0.41482598]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, True, False, False]
State prediction error at timestep 97 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 97 of None
Current timestep = 98. State = [[-0.30863374 -0.1736454 ]]. Action = [[ 0.00643849  0.0184052   0.         -0.8022589 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, True, False, False]
State prediction error at timestep 98 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 98 of None
Current timestep = 99. State = [[-0.3073743  -0.16883315]]. Action = [[ 0.01777432  0.08199792  0.         -0.24452662]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, True, False, False]
State prediction error at timestep 99 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 99 of None
Current timestep = 100. State = [[-0.30361223 -0.16379371]]. Action = [[ 0.06508752  0.04058089  0.         -0.7943448 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, True, False, False]
State prediction error at timestep 100 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 100 of None
Current timestep = 101. State = [[-0.29713145 -0.15706937]]. Action = [[ 0.09153306  0.08392868  0.         -0.9493565 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, True, False, False]
State prediction error at timestep 101 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 101 of None
Current timestep = 102. State = [[-0.2947682  -0.15097114]]. Action = [[-0.00710889  0.04199185  0.         -0.75457567]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, True, False, False]
State prediction error at timestep 102 is tensor(3.8019e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 102 of None
Current timestep = 103. State = [[-0.29191485 -0.15177433]]. Action = [[ 0.06249606 -0.07246204  0.          0.06477809]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, True, False, False]
State prediction error at timestep 103 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 103 of None
Current timestep = 104. State = [[-0.2865393 -0.1515061]]. Action = [[ 0.06639535  0.0207387   0.         -0.62643766]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, True, False, False]
State prediction error at timestep 104 is tensor(2.2039e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 104 of None
Current timestep = 105. State = [[-0.28035995 -0.15345265]]. Action = [[ 0.0712591  -0.07239728  0.         -0.5898253 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, True, False, False]
State prediction error at timestep 105 is tensor(5.3580e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 105 of None
Current timestep = 106. State = [[-0.27602306 -0.14997862]]. Action = [[ 0.02201173  0.09787326  0.         -0.5130818 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, True, False, False]
State prediction error at timestep 106 is tensor(3.0439e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 106 of None
Current timestep = 107. State = [[-0.27173048 -0.14759202]]. Action = [[ 0.05863035 -0.02467187  0.          0.15600681]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, True, False, False]
State prediction error at timestep 107 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 107 of None
Current timestep = 108. State = [[-0.26544854 -0.14680995]]. Action = [[0.07366601 0.00974835 0.         0.19119322]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, True, False, False]
State prediction error at timestep 108 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 108 of None
Current timestep = 109. State = [[-0.26308972 -0.14471605]]. Action = [[-0.01889743  0.02365838  0.          0.1033498 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, True, False, False]
State prediction error at timestep 109 is tensor(6.3254e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 109 of None
Current timestep = 110. State = [[-0.2635116  -0.13935491]]. Action = [[-0.02226926  0.08669998  0.          0.06672692]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, True, False, False]
State prediction error at timestep 110 is tensor(4.8894e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 110 of None
Current timestep = 111. State = [[-0.2646824  -0.13255192]]. Action = [[-0.02919118  0.07460263  0.         -0.31455213]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, True, False, False]
State prediction error at timestep 111 is tensor(4.1393e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 111 of None
Current timestep = 112. State = [[-0.26440695 -0.13380767]]. Action = [[ 0.00673946 -0.08929274  0.         -0.9165057 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, True, False, False]
State prediction error at timestep 112 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 112 of None
Current timestep = 113. State = [[-0.25923845 -0.13953757]]. Action = [[ 0.07839321 -0.07971199  0.          0.44455087]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, True, False, False]
State prediction error at timestep 113 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 113 of None
Current timestep = 114. State = [[-0.2587347 -0.1418054]]. Action = [[-0.07074497  0.00189976  0.         -0.5383673 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, True, False, False]
State prediction error at timestep 114 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 114 of None
Current timestep = 115. State = [[-0.25698492 -0.13720593]]. Action = [[0.05568183 0.0908032  0.         0.07338595]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, True, False, False]
State prediction error at timestep 115 is tensor(1.6380e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 115 of None
Current timestep = 116. State = [[-0.250266   -0.13818064]]. Action = [[ 0.09744205 -0.0879102   0.         -0.02250725]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, True, False, False]
State prediction error at timestep 116 is tensor(2.8357e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 116 of None
Current timestep = 117. State = [[-0.24637754 -0.1421957 ]]. Action = [[-1.8393248e-04 -3.7044920e-02  0.0000000e+00  6.4866734e-01]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, True, False, False]
State prediction error at timestep 117 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 117 of None
Current timestep = 118. State = [[-0.24472731 -0.13906732]]. Action = [[ 0.00969952  0.09172004  0.         -0.8351518 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, True, False, False]
State prediction error at timestep 118 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 118 of None
Current timestep = 119. State = [[-0.2462078  -0.13319235]]. Action = [[-0.05208472  0.06840052  0.         -0.57194847]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, True, False, False]
State prediction error at timestep 119 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 119 of None
Current timestep = 120. State = [[-0.24492815 -0.13272007]]. Action = [[ 0.04597933 -0.03798363  0.         -0.26840603]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, True, False, False]
State prediction error at timestep 120 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 120 of None
Current timestep = 121. State = [[-0.23838215 -0.1375143 ]]. Action = [[ 0.09177988 -0.08230245  0.         -0.22404808]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, True, False, False]
State prediction error at timestep 121 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 121 of None
Current timestep = 122. State = [[-0.2333303  -0.13902129]]. Action = [[0.02691147 0.01803006 0.         0.29837286]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, True, False, False]
State prediction error at timestep 122 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 122 of None
Current timestep = 123. State = [[-0.23509097 -0.14202282]]. Action = [[-0.07719038 -0.06139922  0.         -0.6976466 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, True, False, False]
State prediction error at timestep 123 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 123 of None
Current timestep = 124. State = [[-0.23358153 -0.14048833]]. Action = [[ 0.05345894  0.07995074  0.         -0.36949706]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, True, False, False]
State prediction error at timestep 124 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 124 of None
Current timestep = 125. State = [[-0.22799757 -0.13519369]]. Action = [[ 0.07012018  0.06067916  0.         -0.89316255]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, True, False, False]
State prediction error at timestep 125 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 125 of None
Current timestep = 126. State = [[-0.2221043  -0.13472365]]. Action = [[ 0.06557637 -0.03788507  0.         -0.49634492]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, True, False, False]
State prediction error at timestep 126 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 126 of None
Current timestep = 127. State = [[-0.21795946 -0.13163573]]. Action = [[0.03025316 0.07409523 0.         0.3022282 ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, True, False, False]
State prediction error at timestep 127 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 127 of None
Current timestep = 128. State = [[-0.21806177 -0.12954257]]. Action = [[-0.03462861 -0.00886088  0.         -0.88619226]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, True, False, False]
State prediction error at timestep 128 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 128 of None
Current timestep = 129. State = [[-0.21671921 -0.131645  ]]. Action = [[ 0.03025208 -0.04577982  0.         -0.9661095 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, True, False, False]
State prediction error at timestep 129 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 129 of None
Current timestep = 130. State = [[-0.21520136 -0.1342449 ]]. Action = [[-0.00448696 -0.02939345  0.          0.74524343]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, True, False, False]
State prediction error at timestep 130 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 130 of None
Current timestep = 131. State = [[-0.21216169 -0.13469827]]. Action = [[ 0.0437078   0.00884126  0.         -0.94852537]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, True, False, False]
State prediction error at timestep 131 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 131 of None
Current timestep = 132. State = [[-0.21421279 -0.1359184 ]]. Action = [[-0.08823476 -0.0264411   0.          0.6057162 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, True, False, False]
State prediction error at timestep 132 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 132 of None
Current timestep = 133. State = [[-0.21313398 -0.13657638]]. Action = [[ 0.05329754  0.00743739  0.         -0.73417974]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, True, False, False]
State prediction error at timestep 133 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 133 of None
Current timestep = 134. State = [[-0.21127485 -0.13194796]]. Action = [[-0.00472779  0.08996452  0.         -0.313873  ]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, True, False, False]
State prediction error at timestep 134 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 134 of None
Current timestep = 135. State = [[-0.20802891 -0.125654  ]]. Action = [[ 0.05927698  0.06546011  0.         -0.48902696]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, True, False, False]
State prediction error at timestep 135 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 135 of None
Current timestep = 136. State = [[-0.20724745 -0.12659886]]. Action = [[-0.02438033 -0.07531552  0.         -0.94854784]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, True, False, False]
State prediction error at timestep 136 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 136 of None
Current timestep = 137. State = [[-0.20845602 -0.12790743]]. Action = [[-0.02210756  0.00905782  0.         -0.4377808 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, True, False, False]
State prediction error at timestep 137 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 137 of None
Current timestep = 138. State = [[-0.20615013 -0.13078499]]. Action = [[ 0.05144646 -0.065789    0.          0.5631981 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, True, False, False]
State prediction error at timestep 138 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 138 of None
Current timestep = 139. State = [[-0.19990681 -0.13703004]]. Action = [[ 0.08778626 -0.0885314   0.         -0.46138155]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, True, False, False]
State prediction error at timestep 139 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 139 of None
Current timestep = 140. State = [[-0.19408539 -0.13939078]]. Action = [[ 0.05194949  0.01271038  0.         -0.7271328 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, True, False, False]
State prediction error at timestep 140 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 140 of None
Current timestep = 141. State = [[-0.19520785 -0.13586338]]. Action = [[-0.07671288  0.08099077  0.         -0.8429633 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, True, False, False]
State prediction error at timestep 141 is tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 141 of None
Current timestep = 142. State = [[-0.19679634 -0.1321382 ]]. Action = [[ 0.0012453   0.03556318  0.         -0.39457023]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, True, False, False]
State prediction error at timestep 142 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 142 of None
Current timestep = 143. State = [[-0.19842954 -0.13578635]]. Action = [[-0.03633756 -0.09418394  0.          0.25695074]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, True, False, False]
State prediction error at timestep 143 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 143 of None
Current timestep = 144. State = [[-0.19494997 -0.13544767]]. Action = [[ 0.08882966  0.06598433  0.         -0.22281373]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, True, False, False]
State prediction error at timestep 144 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 144 of None
Current timestep = 145. State = [[-0.1964832  -0.13661253]]. Action = [[-0.08816331 -0.06302179  0.         -0.05668837]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, True, False, False]
State prediction error at timestep 145 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 145 of None
Current timestep = 146. State = [[-0.20193388 -0.1348661 ]]. Action = [[-0.07111715  0.07659329  0.          0.29248834]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, True, False, False]
State prediction error at timestep 146 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 146 of None
Current timestep = 147. State = [[-0.20099334 -0.13671984]]. Action = [[ 0.07136879 -0.08393641  0.         -0.36177433]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, True, False, False]
State prediction error at timestep 147 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 147 of None
Current timestep = 148. State = [[-0.1961222 -0.1387938]]. Action = [[ 0.04925121  0.00942131  0.         -0.8954313 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, True, False, False]
State prediction error at timestep 148 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 148 of None
Current timestep = 149. State = [[-0.19141138 -0.14296982]]. Action = [[ 0.05104961 -0.08083465  0.         -0.9734423 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, True, False, False]
State prediction error at timestep 149 is tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 149 of None
Current timestep = 150. State = [[-0.18455817 -0.1419974 ]]. Action = [[ 0.09441175  0.0751619   0.         -0.06481206]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, True, False, False]
State prediction error at timestep 150 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 150 of None
Current timestep = 151. State = [[-0.17843871 -0.14099292]]. Action = [[ 0.05671098 -0.01705305  0.         -0.80955017]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, True, False, False]
State prediction error at timestep 151 is tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 151 of None
Current timestep = 152. State = [[-0.17789203 -0.14571768]]. Action = [[-0.04195581 -0.07819287  0.         -0.2532825 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, True, False, False]
State prediction error at timestep 152 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 152 of None
Current timestep = 153. State = [[-0.17704599 -0.1439068 ]]. Action = [[0.01923206 0.0977012  0.         0.40493035]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, True, False, False]
State prediction error at timestep 153 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 153 of None
Current timestep = 154. State = [[-0.17763177 -0.13924597]]. Action = [[-0.0385046   0.04565517  0.         -0.93924767]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, True, False, False]
State prediction error at timestep 154 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 154 of None
Current timestep = 155. State = [[-0.17956078 -0.14191306]]. Action = [[-0.03469849 -0.08091599  0.         -0.66715074]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, True, False, False]
State prediction error at timestep 155 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 155 of None
Current timestep = 156. State = [[-0.18004823 -0.14227691]]. Action = [[-0.00570323  0.04251546  0.         -0.6177148 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, True, False, False]
State prediction error at timestep 156 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 156 of None
Current timestep = 157. State = [[-0.17819144 -0.13927889]]. Action = [[0.03045905 0.03587116 0.         0.1512059 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, True, False, False]
State prediction error at timestep 157 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 157 of None
Current timestep = 158. State = [[-0.17748776 -0.1373541 ]]. Action = [[-0.01113888  0.00889014  0.         -0.81470823]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, True, False, False]
State prediction error at timestep 158 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 158 of None
Current timestep = 159. State = [[-0.17730966 -0.1318122 ]]. Action = [[ 0.00324055  0.09510059  0.         -0.7571435 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, True, False, False]
State prediction error at timestep 159 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 159 of None
Current timestep = 160. State = [[-0.17721584 -0.12892571]]. Action = [[-0.00079836 -0.01528897  0.         -0.19361317]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, True, False, False]
State prediction error at timestep 160 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 160 of None
Current timestep = 161. State = [[-0.17631982 -0.1264225 ]]. Action = [[ 0.01856641  0.03714008  0.         -0.8919489 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, True, False, False]
State prediction error at timestep 161 is tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 161 of None
Current timestep = 162. State = [[-0.17360185 -0.12400767]]. Action = [[ 0.04707891  0.00530438  0.         -0.34793043]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 162 of None
Current timestep = 163. State = [[-0.17217927 -0.12381842]]. Action = [[ 0.00273812 -0.02125858  0.          0.22714484]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 163 of None
Current timestep = 164. State = [[-0.17589608 -0.12160581]]. Action = [[-0.0837265   0.04268847  0.         -0.5530472 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 164 of None
Current timestep = 165. State = [[-0.18008384 -0.1221605 ]]. Action = [[-0.04095819 -0.04755416  0.          0.710667  ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 165 of None
Current timestep = 166. State = [[-0.18015839 -0.12681702]]. Action = [[ 0.02244838 -0.07530906  0.          0.04876173]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, True, False, False]
State prediction error at timestep 166 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 166 of None
Current timestep = 167. State = [[-0.18141024 -0.1258222 ]]. Action = [[-0.03968007  0.0636952   0.         -0.07502997]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, True, False, False]
State prediction error at timestep 167 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 167 of None
Current timestep = 168. State = [[-0.17828114 -0.12835516]]. Action = [[ 0.09731426 -0.09170576  0.          0.10280859]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, True, False, False]
State prediction error at timestep 168 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 168 of None
Current timestep = 169. State = [[-0.17069711 -0.13226652]]. Action = [[ 0.09577601 -0.0221171   0.         -0.96921426]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, True, False, False]
State prediction error at timestep 169 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 169 of None
Current timestep = 170. State = [[-0.16354159 -0.1377996 ]]. Action = [[ 0.07780256 -0.08372957  0.         -0.8996382 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, True, False, False]
State prediction error at timestep 170 is tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 170 of None
Current timestep = 171. State = [[-0.16403943 -0.13912414]]. Action = [[-0.08767621  0.05081899  0.         -0.95316285]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, True, False, False]
State prediction error at timestep 171 is tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 171 of None
Current timestep = 172. State = [[-0.16255262 -0.1416017 ]]. Action = [[ 0.08241279 -0.06146271  0.          0.7341882 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, True, False, False]
State prediction error at timestep 172 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 172 of None
Current timestep = 173. State = [[-0.16130605 -0.14768581]]. Action = [[-0.02979808 -0.06459431  0.          0.68503535]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, True, False, False]
State prediction error at timestep 173 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 173 of None
Current timestep = 174. State = [[-0.16143858 -0.1544118 ]]. Action = [[-0.00428994 -0.06474148  0.         -0.9585862 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, True, False, False]
State prediction error at timestep 174 is tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 174 of None
Current timestep = 175. State = [[-0.16124834 -0.15331414]]. Action = [[-0.00858837  0.09524328  0.         -0.9053191 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, True, False, False]
State prediction error at timestep 175 is tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 175 of None
Current timestep = 176. State = [[-0.15921879 -0.15184827]]. Action = [[ 3.5827689e-02 -4.1664392e-04  0.0000000e+00 -6.7738444e-01]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, True, False, False]
State prediction error at timestep 176 is tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 176 of None
Current timestep = 177. State = [[-0.15620872 -0.15235552]]. Action = [[ 0.03305223  0.00249888  0.         -0.2402196 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, True, False, False]
State prediction error at timestep 177 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 177 of None
Current timestep = 178. State = [[-0.15910229 -0.15572979]]. Action = [[-0.09001112 -0.05458039  0.         -0.16368663]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, True, False, False]
State prediction error at timestep 178 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 178 of None
Current timestep = 179. State = [[-0.15992647 -0.16006799]]. Action = [[ 0.02207436 -0.03756168  0.          0.8393779 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, True, False, False]
State prediction error at timestep 179 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 179 of None
Current timestep = 180. State = [[-0.15688512 -0.15790798]]. Action = [[ 0.0446971   0.08004446  0.         -0.91132325]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, True, False, False]
State prediction error at timestep 180 is tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 180 of None
Current timestep = 181. State = [[-0.15148467 -0.15727104]]. Action = [[ 0.08200116 -0.0299238   0.         -0.91764426]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, True, False, False]
State prediction error at timestep 181 is tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 181 of None
Current timestep = 182. State = [[-0.14807618 -0.15501536]]. Action = [[ 0.01850939  0.06111563  0.         -0.17120665]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, True, False, False]
State prediction error at timestep 182 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 182 of None
Current timestep = 183. State = [[-0.1503008  -0.14841369]]. Action = [[-0.06275274  0.09559447  0.         -0.17459524]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, True, False, False]
State prediction error at timestep 183 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 183 of None
Current timestep = 184. State = [[-0.15614532 -0.14595054]]. Action = [[-0.0894238  -0.01610518  0.          0.38157284]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, True, False, False]
State prediction error at timestep 184 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 184 of None
Current timestep = 185. State = [[-0.16309932 -0.15019141]]. Action = [[-0.09550122 -0.08684864  0.         -0.28130263]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, True, False, False]
State prediction error at timestep 185 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 185 of None
Current timestep = 186. State = [[-0.1617991  -0.15343854]]. Action = [[ 0.08684722 -0.02104642  0.         -0.5656564 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, True, False, False]
State prediction error at timestep 186 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 186 of None
Current timestep = 187. State = [[-0.1626489  -0.15148222]]. Action = [[-0.0736371   0.05216948  0.         -0.32180786]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, True, False, False]
State prediction error at timestep 187 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 187 of None
Current timestep = 188. State = [[-0.16439761 -0.14531045]]. Action = [[0.00998636 0.08479091 0.         0.74505746]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, True, False, False]
State prediction error at timestep 188 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 188 of None
Current timestep = 189. State = [[-0.16687442 -0.13958317]]. Action = [[-0.04081595  0.04607918  0.          0.41192472]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, True, False, False]
State prediction error at timestep 189 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 189 of None
Current timestep = 190. State = [[-0.17196244 -0.14175296]]. Action = [[-0.06706262 -0.09120143  0.          0.31058538]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, True, False, False]
State prediction error at timestep 190 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 190 of None
Current timestep = 191. State = [[-0.1720679  -0.13992967]]. Action = [[ 0.05394008  0.0766312   0.         -0.84216917]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, True, False, False]
State prediction error at timestep 191 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 191 of None
Current timestep = 192. State = [[-0.17007163 -0.13529651]]. Action = [[0.03140029 0.03012384 0.         0.36797857]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, True, False, False]
State prediction error at timestep 192 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 192 of None
Current timestep = 193. State = [[-0.16978692 -0.12848854]]. Action = [[ 0.00656237  0.09354534  0.         -0.36648762]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, True, False, False]
State prediction error at timestep 193 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 193 of None
Current timestep = 194. State = [[-0.16781454 -0.12655236]]. Action = [[ 0.05708151 -0.04371686  0.          0.47513294]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, True, False, False]
State prediction error at timestep 194 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 194 of None
Current timestep = 195. State = [[-0.16782583 -0.12442905]]. Action = [[-0.01779375  0.04235553  0.         -0.8609843 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 195 of None
Current timestep = 196. State = [[-0.16602357 -0.12658134]]. Action = [[ 0.05932758 -0.08730996  0.          0.4440391 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, True, False, False]
State prediction error at timestep 196 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 196 of None
Current timestep = 197. State = [[-0.16438141 -0.12946422]]. Action = [[ 0.00450791 -0.01971347  0.         -0.46942592]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, True, False, False]
State prediction error at timestep 197 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 197 of None
Current timestep = 198. State = [[-0.16320781 -0.13237536]]. Action = [[ 0.01796357 -0.04709416  0.         -0.09800684]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, True, False, False]
State prediction error at timestep 198 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 198 of None
Current timestep = 199. State = [[-0.16528031 -0.13224012]]. Action = [[-0.06066329  0.03592093  0.         -0.92163295]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, True, False, False]
State prediction error at timestep 199 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 199 of None
Current timestep = 200. State = [[-0.16308592 -0.1298561 ]]. Action = [[0.0766081  0.03129532 0.         0.49557257]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, True, False, False]
State prediction error at timestep 200 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 200 of None
Current timestep = 201. State = [[-0.15881953 -0.12540364]]. Action = [[ 0.04170042  0.06818094  0.         -0.9016541 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, True, False, False]
State prediction error at timestep 201 is tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 201 of None
Current timestep = 202. State = [[-0.15348615 -0.12461194]]. Action = [[ 0.07982295 -0.03120009  0.         -0.01796281]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 202 of None
Current timestep = 203. State = [[-0.14709911 -0.12181418]]. Action = [[ 0.07860499  0.06573229  0.         -0.69627917]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 203 of None
Current timestep = 204. State = [[-0.14658117 -0.11799689]]. Action = [[-0.04576306  0.03109819  0.         -0.34049845]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 204 of None
Current timestep = 205. State = [[-0.147107   -0.11763189]]. Action = [[ 0.00085052 -0.0227257   0.          0.06691444]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 205 of None
Current timestep = 206. State = [[-0.1506425  -0.11525325]]. Action = [[-0.08906895  0.05045999  0.         -0.15999377]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 206 of None
Current timestep = 207. State = [[-0.15512384 -0.11562476]]. Action = [[-0.0588933  -0.04783098  0.         -0.07499313]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 207 of None
Current timestep = 208. State = [[-0.16035646 -0.11201043]]. Action = [[-0.08636668  0.08767145  0.         -0.7197887 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 208 of None
Current timestep = 209. State = [[-0.16042021 -0.11334401]]. Action = [[ 0.05153228 -0.09572721  0.         -0.23051631]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 209 of None
Current timestep = 210. State = [[-0.1573447  -0.11462112]]. Action = [[ 0.021734    0.01939516  0.         -0.9660799 ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 210 of None
Current timestep = 211. State = [[-0.1517947 -0.1186257]]. Action = [[ 0.08753536 -0.09383938  0.         -0.7511918 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 211 of None
Current timestep = 212. State = [[-0.14810584 -0.1200159 ]]. Action = [[ 0.01020353  0.03045822  0.         -0.15673643]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 212 of None
Current timestep = 213. State = [[-0.14492624 -0.11856889]]. Action = [[ 0.04492814  0.01952668  0.         -0.0274756 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 213 of None
Current timestep = 214. State = [[-0.14543083 -0.12059633]]. Action = [[-0.04797783 -0.04706627  0.         -0.8874194 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 214 of None
Current timestep = 215. State = [[-0.14275788 -0.12383542]]. Action = [[ 0.06835457 -0.02792741  0.         -0.8747029 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 215 of None
Current timestep = 216. State = [[-0.13692202 -0.12760039]]. Action = [[ 0.06969178 -0.04537316  0.         -0.48320687]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, True, False, False]
State prediction error at timestep 216 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 216 of None
Current timestep = 217. State = [[-0.13793874 -0.12693758]]. Action = [[-0.08729779  0.06075687  0.          0.62777877]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, True, False, False]
State prediction error at timestep 217 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 217 of None
Current timestep = 218. State = [[-0.14167175 -0.12284006]]. Action = [[-0.03395958  0.05820105  0.         -0.56183386]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 218 of None
Current timestep = 219. State = [[-0.13883501 -0.11938652]]. Action = [[ 0.08842202  0.03041712  0.         -0.7791199 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 219 of None
Current timestep = 220. State = [[-0.13241167 -0.12248179]]. Action = [[ 0.08825677 -0.08891974  0.         -0.8828833 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 220 of None
Current timestep = 221. State = [[-0.12967391 -0.12960869]]. Action = [[-3.9216131e-04 -9.0303965e-02  0.0000000e+00 -9.0667701e-01]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, True, False, False]
State prediction error at timestep 221 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 221 of None
Current timestep = 222. State = [[-0.12520452 -0.13366793]]. Action = [[ 0.0783845  -0.01325782  0.         -0.47893226]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, True, False, False]
State prediction error at timestep 222 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 222 of None
Current timestep = 223. State = [[-0.12180389 -0.13056546]]. Action = [[ 0.01376997  0.08796317  0.         -0.98658085]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, True, False, False]
State prediction error at timestep 223 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 223 of None
Current timestep = 224. State = [[-0.11772322 -0.12359157]]. Action = [[ 0.06568214  0.09662368  0.         -0.9814874 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 224 of None
Current timestep = 225. State = [[-0.11175412 -0.12329891]]. Action = [[ 0.07810546 -0.05605414  0.         -0.9116442 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 225 of None
Current timestep = 226. State = [[-0.10770214 -0.12100485]]. Action = [[ 0.02440158  0.07289266  0.         -0.70836556]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 226 of None
Current timestep = 227. State = [[-0.10280323 -0.11375333]]. Action = [[ 0.07269251  0.09512161  0.         -0.8798352 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
State prediction error at timestep 227 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 227 of None
Current timestep = 228. State = [[-0.10175988 -0.11367599]]. Action = [[-0.03612902 -0.07547991  0.         -0.92198867]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 228 of None
Current timestep = 229. State = [[-0.09711868 -0.11605868]]. Action = [[ 0.09056532 -0.019853    0.         -0.66732836]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(0.0085, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 229 of None
Current timestep = 230. State = [[-0.09495109 -0.11965802]]. Action = [[-0.03603417 -0.06595498  0.          0.6482564 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 230 of None
Current timestep = 231. State = [[-0.09536769 -0.1189028 ]]. Action = [[-0.02589919  0.0526622   0.         -0.31883043]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 231 of None
Current timestep = 232. State = [[-0.09344714 -0.12116462]]. Action = [[ 0.02196995 -0.07633873  0.         -0.7025927 ]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
State prediction error at timestep 232 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 232 of None
Current timestep = 233. State = [[-0.09388387 -0.1276313 ]]. Action = [[-0.05558281 -0.08173689  0.         -0.6128802 ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, True, False, False]
State prediction error at timestep 233 is tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 233 of None
Current timestep = 234. State = [[-0.09749436 -0.1301453 ]]. Action = [[-0.08147253  0.01356877  0.         -0.25118566]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, True, False, False]
State prediction error at timestep 234 is tensor(0.0076, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 234 of None
Current timestep = 235. State = [[-0.0959801  -0.12813494]]. Action = [[ 0.05166782  0.04652192  0.         -0.9584807 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, True, False, False]
State prediction error at timestep 235 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 235 of None
Current timestep = 236. State = [[-0.09177445 -0.13159607]]. Action = [[ 0.03860941 -0.08854376  0.          0.77237105]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, True, False, False]
State prediction error at timestep 236 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 236 of None
Current timestep = 237. State = [[-0.08785226 -0.1299776 ]]. Action = [[ 0.04075079  0.09514409  0.         -0.44616377]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, True, False, False]
State prediction error at timestep 237 is tensor(0.0087, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 237 of None
Current timestep = 238. State = [[-0.08617593 -0.12726302]]. Action = [[-0.00191407  0.006726    0.          0.05251479]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, True, False, False]
State prediction error at timestep 238 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 238 of None
Current timestep = 239. State = [[-0.08271077 -0.12484033]]. Action = [[ 0.06249734  0.04261472  0.         -0.39751267]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
State prediction error at timestep 239 is tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 239 of None
Current timestep = 240. State = [[-0.07819421 -0.12309613]]. Action = [[ 0.0534177   0.00535719  0.         -0.18938398]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(0.0094, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 240 of None
Current timestep = 241. State = [[-0.07260642 -0.12459502]]. Action = [[ 0.07849348 -0.03903358  0.          0.11425495]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
State prediction error at timestep 241 is tensor(0.0080, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 241 of None
Current timestep = 242. State = [[-0.07327331 -0.12281477]]. Action = [[-0.07928754  0.0601706   0.         -0.75666744]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
State prediction error at timestep 242 is tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 242 of None
Current timestep = 243. State = [[-0.07621196 -0.12510502]]. Action = [[-0.02245636 -0.08554407  0.         -0.56010056]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, True, False, False]
State prediction error at timestep 243 is tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 243 of None
Current timestep = 244. State = [[-0.07933091 -0.12692016]]. Action = [[-0.05432862  0.00989646  0.         -0.91395444]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, True, False, False]
State prediction error at timestep 244 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 244 of None
Current timestep = 245. State = [[-0.07856935 -0.12784502]]. Action = [[ 0.04419396 -0.02299352  0.         -0.88941836]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, True, False, False]
State prediction error at timestep 245 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 245 of None
Current timestep = 246. State = [[-0.07889704 -0.12513839]]. Action = [[-0.03340079  0.06752204  0.         -0.5340969 ]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, True, False, False]
State prediction error at timestep 246 is tensor(0.0087, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 246 of None
Current timestep = 247. State = [[-0.08156182 -0.11845809]]. Action = [[-0.0372306   0.08939011  0.         -0.5996798 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
State prediction error at timestep 247 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 247 of None
Current timestep = 248. State = [[-0.08103497 -0.11518127]]. Action = [[ 0.03807179 -0.00397983  0.         -0.16885108]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(0.0083, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 248 of None
Current timestep = 249. State = [[-0.07971642 -0.11718803]]. Action = [[ 0.01185431 -0.05619736  0.         -0.9152822 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
State prediction error at timestep 249 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 249 of None
Current timestep = 250. State = [[-0.08151709 -0.12093799]]. Action = [[-0.04437668 -0.04995789  0.         -0.4438148 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, True, False]
State prediction error at timestep 250 is tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 250 of None
Current timestep = 251. State = [[-0.07847848 -0.12282401]]. Action = [[ 0.08941735 -0.00826125  0.          0.79118454]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, True, False]
State prediction error at timestep 251 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 251 of None
Current timestep = 252. State = [[-0.07199185 -0.11867189]]. Action = [[ 0.08880136  0.09011982  0.         -0.8408416 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 252 of None
Current timestep = 253. State = [[-0.07149649 -0.11118452]]. Action = [[-0.04087759  0.09437845  0.         -0.9426048 ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, True, False]
State prediction error at timestep 253 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 253 of None
Current timestep = 254. State = [[-0.0736263  -0.10740632]]. Action = [[-0.01887534  0.00470914  0.         -0.88149875]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
State prediction error at timestep 254 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 254 of None
Current timestep = 255. State = [[-0.06971951 -0.10859825]]. Action = [[ 0.09957046 -0.04670478  0.         -0.9585923 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, True, False]
State prediction error at timestep 255 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 255 of None
Current timestep = 256. State = [[-0.06995042 -0.10597369]]. Action = [[-0.07462709  0.07112799  0.         -0.12121713]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 256 of None
Current timestep = 257. State = [[-0.07457298 -0.10232142]]. Action = [[-0.05257046  0.01552172  0.         -0.6954086 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 257 of None
Current timestep = 258. State = [[-0.07982037 -0.10004878]]. Action = [[-0.07234057  0.01344629  0.         -0.10722435]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
State prediction error at timestep 258 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 258 of None
Current timestep = 259. State = [[-0.08097588 -0.10088588]]. Action = [[ 0.02452465 -0.04589802  0.         -0.43979377]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
State prediction error at timestep 259 is tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 259 of None
Current timestep = 260. State = [[-0.0823664  -0.10477456]]. Action = [[-0.03729273 -0.06448126  0.          0.3881147 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 260 of None
Current timestep = 261. State = [[-0.08415487 -0.10973743]]. Action = [[-0.01674557 -0.06308734  0.          0.35896087]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, True, False]
State prediction error at timestep 261 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 261 of None
Current timestep = 262. State = [[-0.08299139 -0.11441307]]. Action = [[ 0.0338257  -0.04755974  0.          0.27368164]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, True, False]
State prediction error at timestep 262 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 262 of None
Current timestep = 263. State = [[-0.07760362 -0.11291598]]. Action = [[ 0.09825271  0.07306144  0.         -0.8265477 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 263 of None
Current timestep = 264. State = [[-0.07584672 -0.10769747]]. Action = [[-0.01077856  0.07376336  0.         -0.07547539]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, True, False]
State prediction error at timestep 264 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 264 of None
Current timestep = 265. State = [[-0.07700337 -0.1016254 ]]. Action = [[-0.00984494  0.07857186  0.          0.02294111]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 265 of None
Current timestep = 266. State = [[-0.07658838 -0.09775688]]. Action = [[0.02622204 0.02156143 0.         0.24346161]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 266 of None
Current timestep = 267. State = [[-0.07773773 -0.09491994]]. Action = [[-0.02837231  0.02925659  0.         -0.60072744]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
State prediction error at timestep 267 is tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 267 of None
Current timestep = 268. State = [[-0.07759653 -0.09816907]]. Action = [[ 0.02398013 -0.09731371  0.         -0.15434766]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
State prediction error at timestep 268 is tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 268 of None
Current timestep = 269. State = [[-0.08063384 -0.10396546]]. Action = [[-0.0789359  -0.06632335  0.          0.03132367]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 269 of None
Current timestep = 270. State = [[-0.07901564 -0.10323166]]. Action = [[ 0.07775206  0.05648781  0.         -0.80053717]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
State prediction error at timestep 270 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 270 of None
Current timestep = 271. State = [[-0.0789854  -0.09759409]]. Action = [[-0.0418909   0.07929084  0.         -0.29894078]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
State prediction error at timestep 271 is tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 271 of None
Current timestep = 272. State = [[-0.08075091 -0.09698773]]. Action = [[-0.01402382 -0.04455657  0.          0.20696151]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
State prediction error at timestep 272 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 272 of None
Current timestep = 273. State = [[-0.07902152 -0.10130339]]. Action = [[ 0.04417313 -0.06809179  0.          0.85092926]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
State prediction error at timestep 273 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 273 of None
Current timestep = 274. State = [[-0.07481825 -0.10403358]]. Action = [[ 0.06232005 -0.01075452  0.         -0.6231805 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 274 of None
Current timestep = 275. State = [[-0.07250135 -0.10970042]]. Action = [[ 0.00882664 -0.0961043   0.         -0.64135456]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 275 of None
Current timestep = 276. State = [[-0.06880277 -0.11284747]]. Action = [[0.06254917 0.01142285 0.         0.3280499 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 276 of None
Current timestep = 277. State = [[-0.06730445 -0.10909937]]. Action = [[-0.01073998  0.09050272  0.         -0.04665679]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, True, False]
State prediction error at timestep 277 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 277 of None
Current timestep = 278. State = [[-0.0712785  -0.10215452]]. Action = [[-0.08570568  0.09609223  0.         -0.8342241 ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 278 of None
Current timestep = 279. State = [[-0.06953257 -0.10291668]]. Action = [[ 0.09345431 -0.08052297  0.         -0.92570287]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, True, False]
State prediction error at timestep 279 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 279 of None
Current timestep = 280. State = [[-0.06614398 -0.10801713]]. Action = [[-0.0013587  -0.05469605  0.         -0.37102276]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, True, False]
State prediction error at timestep 280 is tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 280 of None
Current timestep = 281. State = [[-0.06785389 -0.10703381]]. Action = [[-0.06680836  0.0620558   0.         -0.8225328 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 281 of None
Current timestep = 282. State = [[-0.06783619 -0.10410515]]. Action = [[0.01259848 0.02437731 0.         0.13708079]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, True, False]
State prediction error at timestep 282 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 282 of None
Current timestep = 283. State = [[-0.06789996 -0.09914204]]. Action = [[-0.0250494   0.07598232  0.         -0.96977854]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 283 of None
Current timestep = 284. State = [[-0.07146994 -0.09756666]]. Action = [[-0.07575284 -0.02814543  0.         -0.8902079 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 284 of None
Current timestep = 285. State = [[-0.06879434 -0.10139219]]. Action = [[ 0.09565008 -0.07804678  0.          0.70493805]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, True, False]
State prediction error at timestep 285 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 285 of None
Current timestep = 286. State = [[-0.06891967 -0.09938551]]. Action = [[-0.08442501  0.0850196   0.          0.706267  ]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, True, False]
State prediction error at timestep 286 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 286 of None
Current timestep = 287. State = [[-0.07444139 -0.09398496]]. Action = [[-0.07648765  0.04874492  0.         -0.36686844]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 287 of None
Current timestep = 288. State = [[-0.07375656 -0.08829901]]. Action = [[ 0.06920148  0.06112327  0.         -0.7576747 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
State prediction error at timestep 288 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 288 of None
Current timestep = 289. State = [[-0.06927942 -0.08963893]]. Action = [[ 0.06573725 -0.08649232  0.          0.0662601 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
State prediction error at timestep 289 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 289 of None
Current timestep = 290. State = [[-0.07090641 -0.09261093]]. Action = [[-0.07681118 -0.02102212  0.         -0.6566087 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 290 of None
Current timestep = 291. State = [[-0.07585315 -0.09605452]]. Action = [[-0.0580017  -0.06023573  0.         -0.6094841 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 291 of None
Current timestep = 292. State = [[-0.07821991 -0.09768245]]. Action = [[-0.00926689  0.00316113  0.         -0.1858362 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 292 of None
Current timestep = 293. State = [[-0.07967516 -0.10134121]]. Action = [[-0.01577621 -0.06921993  0.         -0.9212457 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 293 of None
Current timestep = 294. State = [[-0.07613061 -0.10757962]]. Action = [[ 0.09351457 -0.07241698  0.          0.12498856]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, True, False]
State prediction error at timestep 294 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 294 of None
Current timestep = 295. State = [[-0.07458556 -0.10827214]]. Action = [[-0.0138489   0.05249805  0.         -0.96284735]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, True, False]
State prediction error at timestep 295 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 295 of None
Current timestep = 296. State = [[-0.07139631 -0.11238738]]. Action = [[ 0.08568268 -0.09083103  0.          0.5596614 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, True, False]
State prediction error at timestep 296 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 296 of None
Current timestep = 297. State = [[-0.06979098 -0.11201414]]. Action = [[-0.0094654   0.08573849  0.          0.20355439]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 297 of None
Current timestep = 298. State = [[-0.06790879 -0.10676375]]. Action = [[ 0.05543529  0.07574027  0.         -0.08405113]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 298 of None
Current timestep = 299. State = [[-0.06644528 -0.10853694]]. Action = [[ 0.01275221 -0.07359684  0.         -0.95834327]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 299 of None
Current timestep = 300. State = [[-0.07044248 -0.10771839]]. Action = [[-0.09188253  0.06810571  0.         -0.86397994]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, True, False]
State prediction error at timestep 300 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 300 of None
Current timestep = 301. State = [[-0.06982242 -0.10844053]]. Action = [[ 0.0816297  -0.0525126   0.          0.04302537]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, True, False]
State prediction error at timestep 301 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 301 of None
Current timestep = 302. State = [[-0.07127889 -0.10816055]]. Action = [[-0.08639731  0.04129194  0.         -0.64740026]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 302 of None
Current timestep = 303. State = [[-0.07191429 -0.1048737 ]]. Action = [[0.03486221 0.04113758 0.         0.13628578]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, True, False]
State prediction error at timestep 303 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 303 of None
Current timestep = 304. State = [[-0.07236037 -0.10480254]]. Action = [[-0.02334759 -0.03109773  0.         -0.8594745 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, True, False]
State prediction error at timestep 304 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 304 of None
Current timestep = 305. State = [[-0.07408246 -0.10046863]]. Action = [[-0.02179058  0.09622443  0.         -0.09737033]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 305 of None
Current timestep = 306. State = [[-0.07254257 -0.09809271]]. Action = [[ 0.05113504 -0.02282911  0.          0.322621  ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, True, False]
State prediction error at timestep 306 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 306 of None
Current timestep = 307. State = [[-0.06679458 -0.09527321]]. Action = [[ 0.09913371  0.05133148  0.         -0.6141987 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, True, False]
State prediction error at timestep 307 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 307 of None
Current timestep = 308. State = [[-0.06574693 -0.09701783]]. Action = [[-0.03049731 -0.07833579  0.         -0.85786146]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, True, False]
State prediction error at timestep 308 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 308 of None
Current timestep = 309. State = [[-0.06982157 -0.099775  ]]. Action = [[-0.07578351 -0.01784189  0.          0.96148634]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, True, False]
State prediction error at timestep 309 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 309 of None
Current timestep = 310. State = [[-0.06845433 -0.09628151]]. Action = [[ 0.06788141  0.0775881   0.         -0.24912238]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, True, False]
State prediction error at timestep 310 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 310 of None
Current timestep = 311. State = [[-0.06700291 -0.09331838]]. Action = [[-0.00876398  0.00552112  0.          0.5992118 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 311 of None
Current timestep = 312. State = [[-0.06390388 -0.09397099]]. Action = [[ 0.06263942 -0.02738083  0.         -0.995379  ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(6.5755e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 312 of None
Current timestep = 313. State = [[-0.06196669 -0.09211253]]. Action = [[-1.5832484e-05  4.7849543e-02  0.0000000e+00 -9.8492742e-01]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(4.6140e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 313 of None
Current timestep = 314. State = [[-0.0572894  -0.08704426]]. Action = [[0.09259333 0.06622834 0.         0.5162505 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
State prediction error at timestep 314 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 314 of None
Current timestep = 315. State = [[-0.05044387 -0.08240774]]. Action = [[ 0.09090281  0.04024484  0.         -0.9352383 ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 315 of None
Current timestep = 316. State = [[-0.18604495 -0.03221656]]. Action = [[0.01185153 0.09014822 0.         0.49807823]]. Reward = [100.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
State prediction error at timestep 316 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 316 of None
Current timestep = 317. State = [[-0.1934449  -0.03317354]]. Action = [[-0.07373514  0.035932    0.          0.6331867 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 317 of None
Current timestep = 318. State = [[-0.19408157 -0.02741897]]. Action = [[ 0.06983908  0.09250028  0.         -0.78581196]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 318 of None
Current timestep = 319. State = [[-0.19229497 -0.02905893]]. Action = [[ 0.03759862 -0.0954158   0.         -0.8149729 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 319 is [True, False, False, False, True, False]
State prediction error at timestep 319 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 319 of None
Current timestep = 320. State = [[-0.1925582  -0.03534839]]. Action = [[-0.00234128 -0.06812812  0.         -0.82736313]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 320 is [True, False, False, False, True, False]
State prediction error at timestep 320 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 320 of None
Current timestep = 321. State = [[-0.19059803 -0.03669779]]. Action = [[ 0.06181519  0.02952261  0.         -0.7082916 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 321 is [True, False, False, False, True, False]
State prediction error at timestep 321 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 321 of None
Current timestep = 322. State = [[-0.19285886 -0.03229027]]. Action = [[-0.06012329  0.08797897  0.         -0.0436061 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 322 is [True, False, False, False, True, False]
State prediction error at timestep 322 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 322 of None
Current timestep = 323. State = [[-0.19759028 -0.02529373]]. Action = [[-0.03237077  0.09400576  0.         -0.65993226]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 323 is [True, False, False, False, True, False]
State prediction error at timestep 323 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 323 of None
Current timestep = 324. State = [[-0.19594637 -0.02428262]]. Action = [[ 0.09405703 -0.0414823   0.         -0.3707137 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 324 of None
Current timestep = 325. State = [[-0.19508158 -0.02045687]]. Action = [[-0.00761622  0.09413958  0.         -0.4225403 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 325 is [True, False, False, False, True, False]
State prediction error at timestep 325 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 325 of None
Current timestep = 326. State = [[-0.19825706 -0.02073514]]. Action = [[-0.04113787 -0.06863061  0.         -0.89098907]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 326 of None
Current timestep = 327. State = [[-0.20224468 -0.0188073 ]]. Action = [[-0.04413927  0.06716467  0.          0.3922112 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 327 is [True, False, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 327 of None
Current timestep = 328. State = [[-0.20311321 -0.01704209]]. Action = [[ 0.02435204 -0.01461549  0.         -0.9379036 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 328 is [True, False, False, False, True, False]
State prediction error at timestep 328 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 328 of None
Current timestep = 329. State = [[-0.19946484 -0.01660273]]. Action = [[ 0.07696142  0.00361027  0.         -0.24558246]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 329 is [True, False, False, False, True, False]
State prediction error at timestep 329 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 329 of None
Current timestep = 330. State = [[-0.20178531 -0.01470437]]. Action = [[-0.08968778  0.02715828  0.         -0.97710234]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 330 is [True, False, False, False, True, False]
State prediction error at timestep 330 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 330 of None
Current timestep = 331. State = [[-0.20682015 -0.01248803]]. Action = [[-0.03966068  0.01607104  0.          0.2875929 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 331 of None
Current timestep = 332. State = [[-0.20734   -0.0064551]]. Action = [[ 0.03598507  0.09611126  0.         -0.49144018]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 332 is [True, False, False, False, True, False]
State prediction error at timestep 332 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 332 of None
Current timestep = 333. State = [[-2.0690946e-01 -1.1433615e-04]]. Action = [[ 0.01770635  0.05103337  0.         -0.9859259 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 333 of None
Current timestep = 334. State = [[-0.2091163   0.00491456]]. Action = [[-0.03104451  0.04515808  0.         -0.8364887 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 334 is [True, False, False, False, True, False]
State prediction error at timestep 334 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 334 of None
Current timestep = 335. State = [[-0.21372278  0.00933472]]. Action = [[-0.05569751  0.0340815   0.         -0.8766092 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 335 is [True, False, False, False, True, False]
State prediction error at timestep 335 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 335 of None
Current timestep = 336. State = [[-0.21418758  0.01155927]]. Action = [[ 0.04369695 -0.00549036  0.         -0.4613874 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 336 of None
Current timestep = 337. State = [[-0.21828209  0.01733153]]. Action = [[-0.09220613  0.08986337  0.         -0.72134674]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 337 is [True, False, False, False, True, False]
State prediction error at timestep 337 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 337 of None
Current timestep = 338. State = [[-0.22176231  0.02574595]]. Action = [[ 0.00639872  0.08406541  0.         -0.8329116 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 338 is [True, False, False, False, True, False]
State prediction error at timestep 338 is tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 338 of None
Current timestep = 339. State = [[-0.22698136  0.02950851]]. Action = [[-0.0807749  -0.01257744  0.         -0.95212734]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 339 is [True, False, False, False, True, False]
State prediction error at timestep 339 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 339 of None
Current timestep = 340. State = [[-0.23427822  0.03310697]]. Action = [[-0.08109045  0.03940845  0.         -0.8274143 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 340 is [True, False, False, False, True, False]
State prediction error at timestep 340 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 340 of None
Current timestep = 341. State = [[-0.23360257  0.03118879]]. Action = [[ 0.09262551 -0.09472864  0.         -0.88172936]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 341 of None
Current timestep = 342. State = [[-0.2362441   0.03310738]]. Action = [[-0.09712332  0.07019674  0.         -0.6871707 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 342 of None
Current timestep = 343. State = [[-0.2355367   0.03642889]]. Action = [[ 0.09542974  0.00679877  0.         -0.7911701 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 343 of None
Current timestep = 344. State = [[-0.23396546  0.04065847]]. Action = [[-0.00492574  0.0619627   0.         -0.91450304]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 344 of None
Current timestep = 345. State = [[-0.23055637  0.04747154]]. Action = [[ 0.08516229  0.08394561  0.         -0.86286813]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 345 is [True, False, False, False, True, False]
State prediction error at timestep 345 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 345 of None
Current timestep = 346. State = [[-0.22666588  0.05047208]]. Action = [[ 0.04471678 -0.0057978   0.         -0.99193394]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 346 is [True, False, False, False, True, False]
State prediction error at timestep 346 is tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 346 of None
Current timestep = 347. State = [[-0.22107932  0.05018592]]. Action = [[ 0.0879839  -0.01781151  0.         -0.94075984]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 347 is [True, False, False, False, True, False]
State prediction error at timestep 347 is tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 347 of None
Current timestep = 348. State = [[-0.21762526  0.05446697]]. Action = [[ 0.01353999  0.08718143  0.         -0.4362285 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 348 is [True, False, False, False, True, False]
State prediction error at timestep 348 is tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 348 of None
Current timestep = 349. State = [[-0.2136817   0.05273863]]. Action = [[ 0.05863182 -0.09838398  0.         -0.5915705 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 349 is [True, False, False, False, True, False]
State prediction error at timestep 349 is tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 349 of None
Current timestep = 350. State = [[-0.21158908  0.04845076]]. Action = [[-0.01903856 -0.03954691  0.          0.14760208]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 350 is [True, False, False, False, True, False]
State prediction error at timestep 350 is tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 350 of None
Current timestep = 351. State = [[-0.20626143  0.05196981]]. Action = [[ 0.09104259  0.09367571  0.         -0.7939128 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 351 is [True, False, False, False, True, False]
State prediction error at timestep 351 is tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 351 of None
Current timestep = 352. State = [[-0.1993609   0.05708737]]. Action = [[ 0.06346586  0.04724199  0.         -0.49365246]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 352 is [True, False, False, False, True, False]
State prediction error at timestep 352 is tensor(0.0077, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 352 of None
Current timestep = 353. State = [[-0.20023598  0.06349615]]. Action = [[-0.08867712  0.08914562  0.         -0.3189423 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0095, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 353 of None
Current timestep = 354. State = [[-0.20257032  0.06575321]]. Action = [[-0.02080842 -0.02907237  0.         -0.6844312 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 354 is [True, False, False, False, True, False]
State prediction error at timestep 354 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 354 of None
Current timestep = 355. State = [[-0.19809462  0.07033327]]. Action = [[0.0948169  0.08517284 0.         0.12938285]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(0.0111, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 355 of None
Current timestep = 356. State = [[-0.19693826  0.0753085 ]]. Action = [[-0.04757467  0.02716831  0.         -0.13715506]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 356 is [True, False, False, False, True, False]
State prediction error at timestep 356 is tensor(0.0116, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 356 of None
Current timestep = 357. State = [[-0.19549543  0.07985259]]. Action = [[ 0.04282906  0.04446075  0.         -0.965465  ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 357 is [True, False, False, False, True, False]
State prediction error at timestep 357 is tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 357 of None
Current timestep = 358. State = [[-0.19011408  0.08550338]]. Action = [[ 0.08161964  0.06033658  0.         -0.9477744 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0095, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 358 of None
Current timestep = 359. State = [[-0.18415466  0.08800472]]. Action = [[ 0.06708188 -0.00977646  0.          0.54889464]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 359 is [True, False, False, False, True, False]
State prediction error at timestep 359 is tensor(0.0125, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 359 of None
Current timestep = 360. State = [[-0.17974466  0.09250002]]. Action = [[ 0.03508932  0.07189072  0.         -0.00216669]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 360 is [True, False, False, False, True, False]
State prediction error at timestep 360 is tensor(0.0133, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 360 of None
Current timestep = 361. State = [[-0.1736311   0.09751624]]. Action = [[0.08829207 0.03698904 0.         0.72948015]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0142, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 361 of None
Current timestep = 362. State = [[-0.1681501   0.10172497]]. Action = [[ 0.04163463  0.04022165  0.         -0.7161324 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 362 is [True, False, False, False, True, False]
State prediction error at timestep 362 is tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 362 of None
Current timestep = 363. State = [[-0.17012273  0.10921957]]. Action = [[-0.0919165   0.09788194  0.          0.7330234 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 363 is [True, False, False, False, True, False]
State prediction error at timestep 363 is tensor(0.0144, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 363 of None
Current timestep = 364. State = [[-0.1669867   0.11077838]]. Action = [[ 0.09670176 -0.06280027  0.         -0.85736406]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 364 is [True, False, False, False, True, False]
State prediction error at timestep 364 is tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 364 of None
Current timestep = 365. State = [[-0.15994753  0.10971774]]. Action = [[ 0.0603688  -0.01575383  0.          0.08580399]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 365 is [True, False, False, False, True, False]
State prediction error at timestep 365 is tensor(0.0127, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 365 of None
Current timestep = 366. State = [[-0.15775424  0.11086081]]. Action = [[-0.0298913   0.00985235  0.         -0.87130105]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 366 is [True, False, False, False, True, False]
State prediction error at timestep 366 is tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 366 of None
Current timestep = 367. State = [[-0.15306193  0.11561587]]. Action = [[ 0.07743875  0.06772221  0.         -0.49214935]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 367 is [True, False, False, False, True, False]
State prediction error at timestep 367 is tensor(0.0102, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 367 of None
Current timestep = 368. State = [[-0.15226974  0.12170437]]. Action = [[-0.0621279  0.0587666  0.        -0.7718304]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0099, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 368 of None
Current timestep = 369. State = [[-0.15261117  0.12834458]]. Action = [[ 0.00153668  0.06563554  0.         -0.77121174]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 369 is [True, False, False, False, False, True]
State prediction error at timestep 369 is tensor(0.0104, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 369 of None
Current timestep = 370. State = [[-0.14731982  0.1359743 ]]. Action = [[ 0.09753089  0.08375744  0.         -0.7193367 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 370 is [True, False, False, False, False, True]
State prediction error at timestep 370 is tensor(0.0109, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 370 of None
Current timestep = 371. State = [[-0.14355735  0.14326061]]. Action = [[ 0.01206796  0.06808909  0.         -0.1484226 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 371 is [True, False, False, False, False, True]
State prediction error at timestep 371 is tensor(0.0145, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 371 of None
Current timestep = 372. State = [[-0.13832293  0.14331119]]. Action = [[ 0.08147437 -0.07267924  0.          0.28157055]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 372 is [True, False, False, False, False, True]
State prediction error at timestep 372 is tensor(0.0152, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 372 of None
Current timestep = 373. State = [[-0.13668989  0.14056516]]. Action = [[-0.04483504 -0.04516206  0.         -0.93585247]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 373 is [True, False, False, False, False, True]
State prediction error at timestep 373 is tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 373 of None
Current timestep = 374. State = [[-0.13215403  0.14036626]]. Action = [[8.6417936e-02 1.9238144e-04 0.0000000e+00 2.1290123e-01]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 374 is [True, False, False, False, False, True]
State prediction error at timestep 374 is tensor(0.0142, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 374 of None
Current timestep = 375. State = [[-0.12947431  0.13833345]]. Action = [[-0.03515954 -0.05876913  0.         -0.9239963 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 375 is [True, False, False, False, False, True]
State prediction error at timestep 375 is tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 375 of None
Current timestep = 376. State = [[-0.1312296   0.14043869]]. Action = [[-0.05864388  0.05733129  0.          0.11837578]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 376 is [True, False, False, False, False, True]
State prediction error at timestep 376 is tensor(0.0140, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 376 of None
Current timestep = 377. State = [[-0.12925804  0.14710358]]. Action = [[ 0.05294404  0.08426946  0.         -0.46221733]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 377 is [True, False, False, False, False, True]
State prediction error at timestep 377 is tensor(0.0097, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 377 of None
Current timestep = 378. State = [[-0.1255887   0.15365629]]. Action = [[ 0.03236107  0.06480313  0.         -0.24293602]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 378 is [True, False, False, False, False, True]
State prediction error at timestep 378 is tensor(0.0120, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 378 of None
Current timestep = 379. State = [[-0.126926    0.15997979]]. Action = [[-0.05798434  0.06494939  0.          0.30246925]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 379 is [True, False, False, False, False, True]
State prediction error at timestep 379 is tensor(0.0153, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 379 of None
Current timestep = 380. State = [[-0.13248536  0.16321263]]. Action = [[-0.09123563 -0.0098684   0.         -0.4061725 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 380 is [True, False, False, False, False, True]
State prediction error at timestep 380 is tensor(0.0101, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 380 of None
Current timestep = 381. State = [[-0.13644637  0.1634927 ]]. Action = [[-0.03771004 -0.03331573  0.         -0.8857828 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 381 is [True, False, False, False, False, True]
State prediction error at timestep 381 is tensor(0.0084, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 381 of None
Current timestep = 382. State = [[-0.13661854  0.15953927]]. Action = [[ 0.0098634  -0.09694251  0.          0.15162456]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 382 is [True, False, False, False, False, True]
State prediction error at timestep 382 is tensor(0.0139, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 382 of None
Current timestep = 383. State = [[-0.13662837  0.16246669]]. Action = [[-0.01236192  0.09550025  0.         -0.9462384 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 383 is [True, False, False, False, False, True]
State prediction error at timestep 383 is tensor(0.0080, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 383 of None
Current timestep = 384. State = [[-0.14049107  0.16659607]]. Action = [[-0.06694706  0.00591914  0.          0.3810197 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 384 is [True, False, False, False, False, True]
State prediction error at timestep 384 is tensor(0.0141, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 384 of None
Current timestep = 385. State = [[-0.1426555  0.1649577]]. Action = [[-0.0008333  -0.0654867   0.         -0.63849175]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 385 is [True, False, False, False, False, True]
State prediction error at timestep 385 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 385 of None
Current timestep = 386. State = [[-0.14522536  0.16021651]]. Action = [[-0.05066428 -0.07608692  0.         -0.7467245 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 386 is [True, False, False, False, False, True]
State prediction error at timestep 386 is tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 386 of None
Current timestep = 387. State = [[-0.1501006   0.15916334]]. Action = [[-0.07035406  0.01191242  0.         -0.89853334]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 387 is [True, False, False, False, False, True]
State prediction error at timestep 387 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 387 of None
Current timestep = 388. State = [[-0.15350868  0.16218723]]. Action = [[-0.01628955  0.04753966  0.         -0.31506962]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 388 is [True, False, False, False, False, True]
State prediction error at timestep 388 is tensor(0.0083, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 388 of None
Current timestep = 389. State = [[-0.15160847  0.16075715]]. Action = [[ 0.07088231 -0.05704239  0.          0.32690752]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 389 is [True, False, False, False, False, True]
State prediction error at timestep 389 is tensor(0.0123, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 389 of None
Current timestep = 390. State = [[-0.15150467  0.16015173]]. Action = [[-0.02031896  0.03026891  0.         -0.40286708]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 390 is [True, False, False, False, False, True]
State prediction error at timestep 390 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 390 of None
Current timestep = 391. State = [[-0.14959906  0.15951146]]. Action = [[ 0.06639422 -0.01657367  0.          0.38169467]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 391 is [True, False, False, False, False, True]
State prediction error at timestep 391 is tensor(0.0111, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 391 of None
Current timestep = 392. State = [[-0.15187715  0.16213161]]. Action = [[-0.06622058  0.0770124   0.         -0.8483203 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 392 is [True, False, False, False, False, True]
State prediction error at timestep 392 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 392 of None
Current timestep = 393. State = [[-0.15100949  0.16623692]]. Action = [[ 0.08005103  0.0492112   0.         -0.68046904]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 393 is [True, False, False, False, False, True]
State prediction error at timestep 393 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 393 of None
Current timestep = 394. State = [[-0.1490956   0.16993068]]. Action = [[ 0.018935    0.05533818  0.         -0.05422103]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 394 is [True, False, False, False, False, True]
State prediction error at timestep 394 is tensor(0.0091, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 394 of None
Current timestep = 395. State = [[-0.14805278  0.17152819]]. Action = [[ 0.02647097  0.00491761  0.         -0.25470078]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 395 is [True, False, False, False, False, True]
State prediction error at timestep 395 is tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 395 of None
Current timestep = 396. State = [[-0.1431563  0.1690817]]. Action = [[ 0.09368201 -0.04795967  0.         -0.81853527]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 396 is [True, False, False, False, False, True]
State prediction error at timestep 396 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 396 of None
Current timestep = 397. State = [[-0.1409995   0.16991928]]. Action = [[-0.01036333  0.05367155  0.         -0.57695645]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 397 is [True, False, False, False, False, True]
State prediction error at timestep 397 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 397 of None
Current timestep = 398. State = [[-0.14552015  0.17060286]]. Action = [[-0.09796377 -0.02093121  0.         -0.40949917]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 398 is [True, False, False, False, False, True]
State prediction error at timestep 398 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 398 of None
Current timestep = 399. State = [[-0.148407    0.17064409]]. Action = [[-0.01016746 -0.00275965  0.         -0.51793283]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 399 is [True, False, False, False, False, True]
State prediction error at timestep 399 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 399 of None
Current timestep = 400. State = [[-0.1493581   0.16673926]]. Action = [[-0.01724321 -0.08908166  0.         -0.53926814]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 400 is [True, False, False, False, False, True]
State prediction error at timestep 400 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 400 of None
Current timestep = 401. State = [[-0.14680952  0.16245371]]. Action = [[ 0.05001435 -0.04025743  0.         -0.9934623 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 401 is [True, False, False, False, False, True]
State prediction error at timestep 401 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 401 of None
Current timestep = 402. State = [[-0.1486541  0.1621818]]. Action = [[-0.07908053  0.02167807  0.          0.60197806]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 402 is [True, False, False, False, False, True]
State prediction error at timestep 402 is tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 402 of None
Current timestep = 403. State = [[-0.14797013  0.15871637]]. Action = [[ 0.04718464 -0.08235101  0.         -0.8415303 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 403 is [True, False, False, False, False, True]
State prediction error at timestep 403 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 403 of None
Current timestep = 404. State = [[-0.14277282  0.1564247 ]]. Action = [[ 0.07153029  0.0137526   0.         -0.7283066 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 404 is [True, False, False, False, False, True]
State prediction error at timestep 404 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 404 of None
Current timestep = 405. State = [[-0.14038037  0.15875746]]. Action = [[ 0.00195732  0.06149139  0.         -0.77827895]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 405 is [True, False, False, False, False, True]
State prediction error at timestep 405 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 405 of None
Current timestep = 406. State = [[-0.13841256  0.15806556]]. Action = [[ 0.03399021 -0.03458209  0.         -0.8287349 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 406 is [True, False, False, False, False, True]
State prediction error at timestep 406 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 406 of None
Current timestep = 407. State = [[-0.13981682  0.15972385]]. Action = [[-0.05421577  0.06355592  0.         -0.71773875]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 407 is [True, False, False, False, False, True]
State prediction error at timestep 407 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 407 of None
Current timestep = 408. State = [[-0.13744837  0.16601872]]. Action = [[0.08777755 0.09964433 0.         0.35871136]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 408 is [True, False, False, False, False, True]
State prediction error at timestep 408 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 408 of None
Current timestep = 409. State = [[-0.13727705  0.17258841]]. Action = [[-0.03960049  0.07394571  0.         -0.7673674 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 409 is [True, False, False, False, False, True]
State prediction error at timestep 409 is tensor(6.1613e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 409 of None
Current timestep = 410. State = [[-0.13760334  0.17387408]]. Action = [[ 0.02032544 -0.03052529  0.         -0.7641954 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 410 is [True, False, False, False, False, True]
State prediction error at timestep 410 is tensor(1.8293e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 410 of None
Current timestep = 411. State = [[-0.13722053  0.17363189]]. Action = [[-0.00245807 -0.0033558   0.          0.5297909 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 411 is [True, False, False, False, False, True]
State prediction error at timestep 411 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 411 of None
Current timestep = 412. State = [[-0.13451488  0.16973117]]. Action = [[ 0.05014073 -0.08918399  0.          0.05031383]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 412 is [True, False, False, False, False, True]
State prediction error at timestep 412 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 412 of None
Current timestep = 413. State = [[-0.12910841  0.16467957]]. Action = [[ 0.06561085 -0.05378416  0.         -0.82324415]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 413 is [True, False, False, False, False, True]
State prediction error at timestep 413 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 413 of None
Current timestep = 414. State = [[-0.12416272  0.15916122]]. Action = [[ 0.0383283  -0.07025295  0.          0.73958945]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 414 is [True, False, False, False, False, True]
State prediction error at timestep 414 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 414 of None
Current timestep = 415. State = [[-0.11986996  0.16028081]]. Action = [[ 0.03833384  0.08048587  0.         -0.9651355 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 415 is [True, False, False, False, False, True]
State prediction error at timestep 415 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 415 of None
Current timestep = 416. State = [[-0.11951022  0.16367221]]. Action = [[-0.03444133  0.03630342  0.         -0.70805544]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 416 is [True, False, False, False, False, True]
State prediction error at timestep 416 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 416 of None
Current timestep = 417. State = [[-0.11606888  0.16359347]]. Action = [[ 0.07100191 -0.01936097  0.          0.2970091 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 417 is [True, False, False, False, False, True]
State prediction error at timestep 417 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 417 of None
Current timestep = 418. State = [[-0.11320465  0.15839015]]. Action = [[-0.00459515 -0.08675406  0.         -0.230219  ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 418 is [True, False, False, False, False, True]
State prediction error at timestep 418 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 418 of None
Current timestep = 419. State = [[-0.11272145  0.15878265]]. Action = [[-0.01558597  0.06668124  0.         -0.10705018]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 419 is [True, False, False, False, False, True]
State prediction error at timestep 419 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 419 of None
Current timestep = 420. State = [[-0.10792295  0.15735108]]. Action = [[ 0.08892933 -0.05786657  0.         -0.4370476 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 420 is [True, False, False, False, False, True]
State prediction error at timestep 420 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 420 of None
Current timestep = 421. State = [[-0.10013652  0.1586926 ]]. Action = [[0.09242637 0.07652202 0.         0.09023726]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 421 is [True, False, False, False, False, True]
State prediction error at timestep 421 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 421 of None
Current timestep = 422. State = [[-0.09540709  0.15540074]]. Action = [[ 0.01971923 -0.09312426  0.         -0.60313267]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 422 is [True, False, False, False, False, True]
State prediction error at timestep 422 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 422 of None
Current timestep = 423. State = [[-0.08870494  0.14987797]]. Action = [[ 0.09730012 -0.04001926  0.          0.2828709 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 423 is [True, False, False, False, False, True]
State prediction error at timestep 423 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 423 of None
Current timestep = 424. State = [[-0.08844958  0.14672594]]. Action = [[-0.09181233 -0.01921806  0.         -0.83109766]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 424 is [True, False, False, False, False, True]
State prediction error at timestep 424 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 424 of None
Current timestep = 425. State = [[-0.08628498  0.14776598]]. Action = [[ 0.0812682   0.05395044  0.         -0.868702  ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 425 is [True, False, False, False, False, True]
State prediction error at timestep 425 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 425 of None
Current timestep = 426. State = [[-0.07899759  0.15285262]]. Action = [[0.09091359 0.09235809 0.         0.598106  ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 426 is [True, False, False, False, False, True]
State prediction error at timestep 426 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 426 of None
Current timestep = 427. State = [[-0.07809311  0.15877682]]. Action = [[-0.05220569  0.07412284  0.          0.10936797]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 427 is [True, False, False, False, False, True]
State prediction error at timestep 427 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 427 of None
Current timestep = 428. State = [[-0.07949328  0.16447626]]. Action = [[-0.00645233  0.06305242  0.         -0.0701645 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 428 is [True, False, False, False, False, True]
State prediction error at timestep 428 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 428 of None
Current timestep = 429. State = [[-0.0751892   0.16891903]]. Action = [[0.09607125 0.0394846  0.         0.7274227 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 429 is [True, False, False, False, False, True]
State prediction error at timestep 429 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 429 of None
Current timestep = 430. State = [[-0.07203169  0.17007725]]. Action = [[ 0.00756101 -0.01379795  0.          0.5263125 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 430 is [True, False, False, False, False, True]
State prediction error at timestep 430 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 430 of None
Current timestep = 431. State = [[-0.07302023  0.170069  ]]. Action = [[-0.03942317 -0.01377114  0.         -0.94864035]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 431 is [True, False, False, False, False, True]
State prediction error at timestep 431 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 431 of None
Current timestep = 432. State = [[-0.06917971  0.17504217]]. Action = [[ 0.09904965  0.09146481  0.         -0.12769777]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 432 is [True, False, False, False, False, True]
State prediction error at timestep 432 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 432 of None
Current timestep = 433. State = [[-0.06868783  0.17457855]]. Action = [[-0.08692148 -0.09438004  0.         -0.63977927]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 433 is [True, False, False, False, False, True]
State prediction error at timestep 433 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 433 of None
Current timestep = 434. State = [[-0.06745455  0.17600009]]. Action = [[ 0.06846143  0.0629921   0.         -0.6451024 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 434 is [True, False, False, False, False, True]
State prediction error at timestep 434 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 434 of None
Current timestep = 435. State = [[-0.06068081  0.17619409]]. Action = [[ 0.08961923 -0.04145808  0.          0.19869888]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 435 is [True, False, False, False, False, True]
State prediction error at timestep 435 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 435 of None
Current timestep = 436. State = [[-0.06100394  0.17854577]]. Action = [[-0.08498625  0.05956627  0.         -0.03712451]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 436 is [True, False, False, False, False, True]
State prediction error at timestep 436 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 436 of None
Current timestep = 437. State = [[-0.06247284  0.18278204]]. Action = [[-3.5867095e-04  3.1027384e-02  0.0000000e+00  7.8097582e-01]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 437 is [True, False, False, False, False, True]
State prediction error at timestep 437 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 437 of None
Current timestep = 438. State = [[-0.06070391  0.18812302]]. Action = [[ 0.03165197  0.06762481  0.         -0.96456945]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 438 is [True, False, False, False, False, True]
State prediction error at timestep 438 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 438 of None
Current timestep = 439. State = [[-0.06233489  0.19437684]]. Action = [[-0.05228326  0.06337228  0.         -0.89675266]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 439 is [True, False, False, False, False, True]
State prediction error at timestep 439 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 439 of None
Current timestep = 440. State = [[-0.060377    0.19279067]]. Action = [[ 0.06497986 -0.09891482  0.          0.8566419 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 440 is [True, False, False, False, False, True]
State prediction error at timestep 440 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 440 of None
Current timestep = 441. State = [[-0.05511896  0.19482413]]. Action = [[ 0.06448378  0.08357132  0.         -0.92626995]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 441 is [True, False, False, False, False, True]
State prediction error at timestep 441 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 441 of None
Current timestep = 442. State = [[-0.05516386  0.19508749]]. Action = [[-0.05756077 -0.05872153  0.         -0.81888294]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 442 is [True, False, False, False, False, True]
State prediction error at timestep 442 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 442 of None
Current timestep = 443. State = [[-0.05625702  0.19573896]]. Action = [[0.00074176 0.02995834 0.         0.1802324 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 443 is [True, False, False, False, False, True]
State prediction error at timestep 443 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 443 of None
Current timestep = 444. State = [[-0.05290508  0.19503386]]. Action = [[ 0.0673781  -0.03761851  0.         -0.871663  ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 444 is [True, False, False, False, False, True]
State prediction error at timestep 444 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 444 of None
Current timestep = 445. State = [[-0.05288314  0.19256915]]. Action = [[-0.04886719 -0.03133871  0.         -0.8509049 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 445 is [True, False, False, False, False, True]
State prediction error at timestep 445 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 445 of None
Current timestep = 446. State = [[-0.05782246  0.19497307]]. Action = [[-0.08785035  0.05911205  0.         -0.5092126 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 446 is [True, False, False, False, False, True]
State prediction error at timestep 446 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 446 of None
Current timestep = 447. State = [[-0.05819075  0.19422166]]. Action = [[ 0.03819413 -0.06173578  0.         -0.757402  ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 447 is [True, False, False, False, False, True]
State prediction error at timestep 447 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 447 of None
Current timestep = 448. State = [[-0.05333278  0.1894615 ]]. Action = [[ 0.07419535 -0.06157604  0.         -0.26733863]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 448 is [True, False, False, False, False, True]
State prediction error at timestep 448 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 448 of None
Current timestep = 449. State = [[-0.05509795  0.18809038]]. Action = [[-0.09427935  0.01628031  0.         -0.791532  ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 449 is [True, False, False, False, False, True]
State prediction error at timestep 449 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 449 of None
Current timestep = 450. State = [[-0.05564714  0.18933961]]. Action = [[0.039079   0.02061701 0.         0.6999843 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 450 is [True, False, False, False, False, True]
State prediction error at timestep 450 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 450 of None
Current timestep = 451. State = [[-0.05209192  0.18740498]]. Action = [[ 0.0535842  -0.04322369  0.         -0.65210915]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 451 is [True, False, False, False, False, True]
State prediction error at timestep 451 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 451 of None
Current timestep = 452. State = [[-0.05203281  0.18218006]]. Action = [[-0.03975767 -0.06947058  0.         -0.6796356 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 452 is [True, False, False, False, False, True]
State prediction error at timestep 452 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 452 of None
Current timestep = 453. State = [[-0.05187664  0.17533918]]. Action = [[ 0.00886096 -0.08408941  0.          0.223688  ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 453 is [True, False, False, False, False, True]
State prediction error at timestep 453 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 453 of None
Current timestep = 454. State = [[-0.0481794  0.170285 ]]. Action = [[ 0.05593581 -0.02821187  0.          0.66351104]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 454 is [False, True, False, False, False, True]
State prediction error at timestep 454 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 454 of None
Current timestep = 455. State = [[-0.04835662  0.1715141 ]]. Action = [[-0.04521712  0.07275031  0.          0.38863897]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 455 is [False, True, False, False, False, True]
State prediction error at timestep 455 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 455 of None
Current timestep = 456. State = [[-0.04668597  0.1714712 ]]. Action = [[ 0.05644295 -0.02031497  0.         -0.31971574]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 456 is [False, True, False, False, False, True]
State prediction error at timestep 456 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 456 of None
Current timestep = 457. State = [[-0.04347973  0.17143522]]. Action = [[ 0.03616876  0.03398333  0.         -0.99689347]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 457 is [False, True, False, False, False, True]
State prediction error at timestep 457 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 457 of None
Current timestep = 458. State = [[-0.04211959  0.17489058]]. Action = [[0.0125974  0.07278246 0.         0.6528033 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 458 is [False, True, False, False, False, True]
State prediction error at timestep 458 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 458 of None
Current timestep = 459. State = [[-0.0458126   0.17900833]]. Action = [[-0.07572332  0.04887991  0.         -0.6079391 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 459 is [False, True, False, False, False, True]
State prediction error at timestep 459 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 459 of None
Current timestep = 460. State = [[-0.04716538  0.17721568]]. Action = [[ 0.02187307 -0.07089917  0.          0.76403284]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 460 is [False, True, False, False, False, True]
State prediction error at timestep 460 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 460 of None
Current timestep = 461. State = [[-0.04496385  0.17485182]]. Action = [[ 0.0378954  -0.00775312  0.         -0.9849755 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 461 is [False, True, False, False, False, True]
State prediction error at timestep 461 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 461 of None
Current timestep = 462. State = [[-0.04470884  0.1736782 ]]. Action = [[-0.01505198 -0.01385823  0.          0.28755498]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 462 is [False, True, False, False, False, True]
State prediction error at timestep 462 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 462 of None
Current timestep = 463. State = [[-0.04786699  0.17073081]]. Action = [[-0.06277445 -0.05328531  0.          0.5096462 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 463 is [False, True, False, False, False, True]
State prediction error at timestep 463 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 463 of None
Current timestep = 464. State = [[-0.04522403  0.16950732]]. Action = [[0.09507061 0.01049405 0.         0.29248524]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 464 is [False, True, False, False, False, True]
State prediction error at timestep 464 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 464 of None
Current timestep = 465. State = [[-0.0437723   0.17218788]]. Action = [[-0.0306132   0.05435573  0.          0.77313066]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 465 is [False, True, False, False, False, True]
State prediction error at timestep 465 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 465 of None
Current timestep = 466. State = [[-0.03985877  0.17729934]]. Action = [[0.09692433 0.07409088 0.         0.20028007]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 466 is [False, True, False, False, False, True]
State prediction error at timestep 466 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 466 of None
Current timestep = 467. State = [[-0.03975108  0.18345442]]. Action = [[-0.0560289   0.07659016  0.          0.03656256]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 467 is [False, True, False, False, False, True]
State prediction error at timestep 467 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 467 of None
Current timestep = 468. State = [[-0.04415587  0.18418495]]. Action = [[-0.06241982 -0.04926784  0.         -0.7642311 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 468 is [False, True, False, False, False, True]
State prediction error at timestep 468 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 468 of None
Current timestep = 469. State = [[-0.04550421  0.1835544 ]]. Action = [[ 0.00236021 -0.00979757  0.          0.81110954]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 469 is [False, True, False, False, False, True]
State prediction error at timestep 469 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 469 of None
Current timestep = 470. State = [[-0.04708346  0.18038768]]. Action = [[-0.04060962 -0.07709982  0.         -0.95276743]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 470 is [False, True, False, False, False, True]
State prediction error at timestep 470 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 470 of None
Current timestep = 471. State = [[-0.05067399  0.18051524]]. Action = [[-0.06385813  0.02793127  0.         -0.7864066 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 471 is [True, False, False, False, False, True]
State prediction error at timestep 471 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 471 of None
Current timestep = 472. State = [[-0.05631123  0.17977048]]. Action = [[-0.08771055 -0.05275123  0.         -0.68305767]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 472 is [True, False, False, False, False, True]
State prediction error at timestep 472 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 472 of None
Current timestep = 473. State = [[-0.05796317  0.17539835]]. Action = [[ 0.00722432 -0.07977861  0.          0.68337405]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 473 is [True, False, False, False, False, True]
State prediction error at timestep 473 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 473 of None
Current timestep = 474. State = [[-0.05556286  0.17150684]]. Action = [[ 0.04011638 -0.03509398  0.         -0.8046633 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 474 is [True, False, False, False, False, True]
State prediction error at timestep 474 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 474 of None
Current timestep = 475. State = [[-0.05418949  0.17095986]]. Action = [[ 0.00333098  0.0203289   0.         -0.9433328 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 475 is [True, False, False, False, False, True]
State prediction error at timestep 475 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 475 of None
Current timestep = 476. State = [[-0.05021369  0.1663721 ]]. Action = [[ 0.07958306 -0.08735388  0.         -0.77915967]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 476 is [True, False, False, False, False, True]
State prediction error at timestep 476 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 476 of None
Current timestep = 477. State = [[-0.05069807  0.15911466]]. Action = [[-0.06769274 -0.0740193   0.         -0.7942422 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 477 is [True, False, False, False, False, True]
State prediction error at timestep 477 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 477 of None
Current timestep = 478. State = [[-0.05174647  0.15566257]]. Action = [[ 0.00988408  0.00114918  0.         -0.05230415]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 478 is [True, False, False, False, False, True]
State prediction error at timestep 478 is tensor(9.0406e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 478 of None
Current timestep = 479. State = [[-0.05009476  0.15350291]]. Action = [[ 0.03242589 -0.01064234  0.         -0.74662197]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 479 is [True, False, False, False, False, True]
State prediction error at timestep 479 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 479 of None
Current timestep = 480. State = [[-0.05368069  0.14831145]]. Action = [[-0.09840836 -0.07138821  0.         -0.08760995]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 480 is [True, False, False, False, False, True]
State prediction error at timestep 480 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 480 of None
Current timestep = 481. State = [[-0.05294428  0.14244534]]. Action = [[ 0.08613933 -0.04291958  0.         -0.9798343 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 481 is [True, False, False, False, False, True]
State prediction error at timestep 481 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 481 of None
Current timestep = 482. State = [[-0.05170412  0.13614526]]. Action = [[-0.02255277 -0.06267504  0.         -0.9690105 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 482 is [True, False, False, False, False, True]
State prediction error at timestep 482 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 482 of None
Current timestep = 483. State = [[-0.05641312  0.13511252]]. Action = [[-0.09073891  0.04814532  0.          0.48626256]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 483 is [True, False, False, False, False, True]
State prediction error at timestep 483 is tensor(6.0128e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 483 of None
Current timestep = 484. State = [[-0.05687074  0.13276272]]. Action = [[ 0.04794984 -0.04979857  0.          0.73534036]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 484 is [True, False, False, False, False, True]
State prediction error at timestep 484 is tensor(4.1278e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 484 of None
Current timestep = 485. State = [[-0.05964873  0.13392046]]. Action = [[-0.07268779  0.07521311  0.         -0.42847276]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 485 is [True, False, False, False, False, True]
State prediction error at timestep 485 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 485 of None
Current timestep = 486. State = [[-0.05943348  0.13310604]]. Action = [[ 0.0611145  -0.04367035  0.          0.02552819]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 486 is [True, False, False, False, False, True]
State prediction error at timestep 486 is tensor(1.9417e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 486 of None
Current timestep = 487. State = [[-0.05638402  0.12936972]]. Action = [[ 0.04525454 -0.03095014  0.          0.31174433]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 487 is [True, False, False, False, False, True]
State prediction error at timestep 487 is tensor(7.0397e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 487 of None
Current timestep = 488. State = [[-0.05622638  0.12620072]]. Action = [[-0.01371291 -0.02148351  0.         -0.651771  ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 488 is [True, False, False, False, False, True]
State prediction error at timestep 488 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 488 of None
Current timestep = 489. State = [[-0.05393421  0.12809914]]. Action = [[ 0.06666458  0.07413066  0.         -0.8805777 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 489 is [True, False, False, False, False, True]
State prediction error at timestep 489 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 489 of None
Current timestep = 490. State = [[-0.0492068   0.13093989]]. Action = [[0.07857919 0.03761297 0.         0.8724617 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 490 is [False, True, False, False, False, True]
State prediction error at timestep 490 is tensor(2.2053e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 490 of None
Current timestep = 491. State = [[-0.05175731  0.13594007]]. Action = [[-0.09123781  0.08912777  0.         -0.8861072 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 491 is [True, False, False, False, False, True]
State prediction error at timestep 491 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 491 of None
Current timestep = 492. State = [[-0.05961718  0.13583915]]. Action = [[-0.09934918 -0.06087896  0.         -0.8164157 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 492 is [True, False, False, False, False, True]
State prediction error at timestep 492 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 492 of None
Current timestep = 493. State = [[-0.06691379  0.13699998]]. Action = [[-0.08191719  0.03873125  0.          0.41963613]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 493 is [True, False, False, False, False, True]
State prediction error at timestep 493 is tensor(9.1801e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 493 of None
Current timestep = 494. State = [[-0.07085647  0.14168273]]. Action = [[-0.01028489  0.05232174  0.          0.1006887 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 494 is [True, False, False, False, False, True]
State prediction error at timestep 494 is tensor(8.2519e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 494 of None
Current timestep = 495. State = [[-0.0752199   0.14028607]]. Action = [[-0.05939303 -0.08350846  0.         -0.8734267 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 495 is [True, False, False, False, False, True]
State prediction error at timestep 495 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 495 of None
Current timestep = 496. State = [[-0.0759005  0.1374623]]. Action = [[ 0.03725865 -0.02798805  0.          0.41580582]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 496 is [True, False, False, False, False, True]
State prediction error at timestep 496 is tensor(3.6495e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 496 of None
Current timestep = 497. State = [[-0.07284296  0.14022022]]. Action = [[ 0.06396467  0.07074595  0.         -0.16293776]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 497 is [True, False, False, False, False, True]
State prediction error at timestep 497 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 497 of None
Current timestep = 498. State = [[-0.0720001   0.13793473]]. Action = [[-0.0029472  -0.08542409  0.         -0.9153493 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 498 is [True, False, False, False, False, True]
State prediction error at timestep 498 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 498 of None
Current timestep = 499. State = [[-0.06889968  0.13292156]]. Action = [[ 0.07228012 -0.04513639  0.         -0.47093332]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 499 is [True, False, False, False, False, True]
State prediction error at timestep 499 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 499 of None
Current timestep = 500. State = [[-0.06303347  0.13030735]]. Action = [[ 0.08552033 -0.00121915  0.          0.00141442]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 500 is [True, False, False, False, False, True]
State prediction error at timestep 500 is tensor(4.5360e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 500 of None
Current timestep = 501. State = [[-0.06333228  0.1329061 ]]. Action = [[-0.05575528  0.07898221  0.         -0.2139731 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 501 is [True, False, False, False, False, True]
State prediction error at timestep 501 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 501 of None
Current timestep = 502. State = [[-0.06313476  0.1313838 ]]. Action = [[ 0.03668465 -0.06094838  0.         -0.9359514 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 502 is [True, False, False, False, False, True]
State prediction error at timestep 502 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 502 of None
Current timestep = 503. State = [[-0.06213683  0.12510523]]. Action = [[-0.00345428 -0.07593755  0.         -0.00957137]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 503 is [True, False, False, False, False, True]
State prediction error at timestep 503 is tensor(9.0548e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 503 of None
Current timestep = 504. State = [[-0.06027256  0.11691274]]. Action = [[ 0.02509788 -0.09790611  0.         -0.8804873 ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 504 is [True, False, False, False, True, False]
State prediction error at timestep 504 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 504 of None
Current timestep = 505. State = [[-0.05569888  0.11398189]]. Action = [[0.06378982 0.03102066 0.         0.5671929 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 505 is [True, False, False, False, True, False]
State prediction error at timestep 505 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 505 of None
Current timestep = 506. State = [[-0.05394944  0.11427677]]. Action = [[-0.01186339  0.02355348  0.         -0.585575  ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 506 is [True, False, False, False, True, False]
State prediction error at timestep 506 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 506 of None
Current timestep = 507. State = [[-0.05046522  0.11774746]]. Action = [[ 0.07139004  0.0821235   0.         -0.6152159 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 507 is [True, False, False, False, True, False]
State prediction error at timestep 507 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 507 of None
Current timestep = 508. State = [[-0.05170989  0.11994974]]. Action = [[-0.07755039  0.01138115  0.          0.7337606 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 508 is [True, False, False, False, True, False]
State prediction error at timestep 508 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 508 of None
Current timestep = 509. State = [[-0.05009175  0.12317397]]. Action = [[ 0.08828562  0.06556135  0.         -0.99313545]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 509 is [True, False, False, False, True, False]
State prediction error at timestep 509 is tensor(7.6700e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 509 of None
Current timestep = 510. State = [[-0.15367858  0.07896432]]. Action = [[ 0.07787991 -0.01192182  0.         -0.64921033]]. Reward = [100.]
Curr episode timestep = 193
Scene graph at timestep 510 is [True, False, False, False, True, False]
State prediction error at timestep 510 is tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 510 of None
Current timestep = 511. State = [[-0.15575679  0.08645852]]. Action = [[-0.05455391  0.07491086  0.         -0.7948948 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 511 is [True, False, False, False, True, False]
State prediction error at timestep 511 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 511 of None
Current timestep = 512. State = [[-0.15483223  0.09367724]]. Action = [[ 0.04478926  0.07287122  0.         -0.98999643]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 512 is [True, False, False, False, True, False]
State prediction error at timestep 512 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 512 of None
Current timestep = 513. State = [[-0.15031043  0.09457821]]. Action = [[ 0.06359791 -0.05338441  0.          0.11007965]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 513 is [True, False, False, False, True, False]
State prediction error at timestep 513 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 513 of None
Current timestep = 514. State = [[-0.14812973  0.09142245]]. Action = [[-0.00976153 -0.0582801   0.          0.77830815]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 514 is [True, False, False, False, True, False]
State prediction error at timestep 514 is tensor(1.1635e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 514 of None
Current timestep = 515. State = [[-0.14520621  0.09262457]]. Action = [[ 0.04104842  0.04001445  0.         -0.97895885]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 515 is [True, False, False, False, True, False]
State prediction error at timestep 515 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 515 of None
Current timestep = 516. State = [[-0.1438323   0.09916034]]. Action = [[-0.01113202  0.09273403  0.         -0.8861686 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 516 is [True, False, False, False, True, False]
State prediction error at timestep 516 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 516 of None
Current timestep = 517. State = [[-0.14630696  0.09853563]]. Action = [[-0.0631564  -0.09263217  0.         -0.24746609]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 517 is [True, False, False, False, True, False]
State prediction error at timestep 517 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 517 of None
Current timestep = 518. State = [[-0.14591478  0.09824727]]. Action = [[ 0.02369791  0.02289695  0.         -0.37573653]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 518 is [True, False, False, False, True, False]
State prediction error at timestep 518 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 518 of None
Current timestep = 519. State = [[-0.13985404  0.09693633]]. Action = [[ 0.09472007 -0.04936346  0.         -0.93243814]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 519 is [True, False, False, False, True, False]
State prediction error at timestep 519 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 519 of None
Current timestep = 520. State = [[-0.1378175   0.09536415]]. Action = [[-0.03603081 -0.00729632  0.         -0.8566656 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 520 is [True, False, False, False, True, False]
State prediction error at timestep 520 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 520 of None
Current timestep = 521. State = [[-0.13784993  0.09135944]]. Action = [[-0.00692641 -0.07865517  0.         -0.35375464]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 521 is [True, False, False, False, True, False]
State prediction error at timestep 521 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 521 of None
Current timestep = 522. State = [[-0.14073284  0.09381987]]. Action = [[-0.076865   0.0958504  0.         0.8152894]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 522 is [True, False, False, False, True, False]
State prediction error at timestep 522 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 522 of None
Current timestep = 523. State = [[-0.13827874  0.09558031]]. Action = [[ 0.0885627  -0.02278001  0.          0.5103345 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 523 is [True, False, False, False, True, False]
State prediction error at timestep 523 is tensor(3.4841e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 523 of None
Current timestep = 524. State = [[-0.13288027  0.09320977]]. Action = [[ 0.05100156 -0.03443407  0.         -0.15612322]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 524 is [True, False, False, False, True, False]
State prediction error at timestep 524 is tensor(5.1063e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 524 of None
Current timestep = 525. State = [[-0.1265113   0.09182698]]. Action = [[ 0.08397891  0.00304289  0.         -0.6039998 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 525 is [True, False, False, False, True, False]
State prediction error at timestep 525 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 525 of None
Current timestep = 526. State = [[-0.1246101   0.09242839]]. Action = [[-0.02713408  0.02328877  0.         -0.9920753 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 526 is [True, False, False, False, True, False]
State prediction error at timestep 526 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 526 of None
Current timestep = 527. State = [[-0.12073065  0.09739599]]. Action = [[ 0.08482382  0.0947305   0.         -0.42623782]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 527 is [True, False, False, False, True, False]
State prediction error at timestep 527 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 527 of None
Current timestep = 528. State = [[-0.11567675  0.09883364]]. Action = [[ 0.04709201 -0.02080694  0.         -0.91535544]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 528 is [True, False, False, False, True, False]
State prediction error at timestep 528 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 528 of None
Current timestep = 529. State = [[-0.11683427  0.10083918]]. Action = [[-0.07104699  0.04915578  0.          0.07545269]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 529 is [True, False, False, False, True, False]
State prediction error at timestep 529 is tensor(6.9087e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 529 of None
Current timestep = 530. State = [[-0.11521939  0.09930509]]. Action = [[ 0.0554798  -0.06751062  0.          0.36440635]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 530 is [True, False, False, False, True, False]
State prediction error at timestep 530 is tensor(2.0425e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 530 of None
Current timestep = 531. State = [[-0.10962488  0.09681965]]. Action = [[ 0.06520689 -0.01368806  0.          0.67698765]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 531 is [True, False, False, False, True, False]
State prediction error at timestep 531 is tensor(6.8211e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 531 of None
Current timestep = 532. State = [[-0.10395669  0.09885646]]. Action = [[ 0.05896478  0.05519748  0.         -0.47881126]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 532 is [True, False, False, False, True, False]
State prediction error at timestep 532 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 532 of None
Current timestep = 533. State = [[-0.1057274   0.10438335]]. Action = [[-0.09397481  0.0759637   0.         -0.70420414]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 533 is [True, False, False, False, True, False]
State prediction error at timestep 533 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 533 of None
Current timestep = 534. State = [[-0.1056574   0.10744854]]. Action = [[ 0.04542672  0.00220773  0.         -0.6691308 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 534 is [True, False, False, False, True, False]
State prediction error at timestep 534 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 534 of None
Current timestep = 535. State = [[-0.09960841  0.10830426]]. Action = [[0.09481149 0.00389504 0.         0.56124353]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 535 is [True, False, False, False, True, False]
State prediction error at timestep 535 is tensor(1.4519e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 535 of None
Current timestep = 536. State = [[-0.09743216  0.10876859]]. Action = [[-2.2724733e-02  2.5876611e-04  0.0000000e+00  5.5326223e-01]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 536 is [True, False, False, False, True, False]
State prediction error at timestep 536 is tensor(2.7597e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 536 of None
Current timestep = 537. State = [[-0.09425437  0.1046903 ]]. Action = [[ 0.05843461 -0.08986419  0.         -0.91948223]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 537 is [True, False, False, False, True, False]
State prediction error at timestep 537 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 537 of None
Current timestep = 538. State = [[-0.09475305  0.10487343]]. Action = [[-0.06567232  0.05423158  0.         -0.20949358]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 538 is [True, False, False, False, True, False]
State prediction error at timestep 538 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 538 of None
Current timestep = 539. State = [[-0.0933897   0.11095747]]. Action = [[ 0.05177612  0.08510358  0.         -0.99338025]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(6.3213e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 539 of None
Current timestep = 540. State = [[-0.09588574  0.11413474]]. Action = [[-0.09874459 -0.00430997  0.         -0.885375  ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 540 is [True, False, False, False, True, False]
State prediction error at timestep 540 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 540 of None
Current timestep = 541. State = [[-0.1020548   0.11513714]]. Action = [[-0.08248148 -0.00478257  0.         -0.8303036 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 541 is [True, False, False, False, True, False]
State prediction error at timestep 541 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 541 of None
Current timestep = 542. State = [[-0.10298777  0.11335424]]. Action = [[ 0.02570329 -0.05734837  0.         -0.98716235]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 542 is [True, False, False, False, True, False]
State prediction error at timestep 542 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 542 of None
Current timestep = 543. State = [[-0.10310677  0.10805684]]. Action = [[-0.0224773  -0.08723488  0.          0.3303119 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 543 is [True, False, False, False, True, False]
State prediction error at timestep 543 is tensor(2.2714e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 543 of None
Current timestep = 544. State = [[-0.10618936  0.11050035]]. Action = [[-0.05681566  0.09543245  0.         -0.9356353 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 544 is [True, False, False, False, True, False]
State prediction error at timestep 544 is tensor(7.3022e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 544 of None
Current timestep = 545. State = [[-0.10499322  0.10938659]]. Action = [[ 0.06179453 -0.08549089  0.          0.44329095]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 545 is [True, False, False, False, True, False]
State prediction error at timestep 545 is tensor(1.0139e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 545 of None
Current timestep = 546. State = [[-0.09942818  0.11110695]]. Action = [[ 0.08997076  0.08859264  0.         -0.7499742 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 546 is [True, False, False, False, True, False]
State prediction error at timestep 546 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 546 of None
Current timestep = 547. State = [[-0.09410947  0.1163419 ]]. Action = [[ 0.07378072  0.06791382  0.         -0.4312389 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 547 is [True, False, False, False, True, False]
State prediction error at timestep 547 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 547 of None
Current timestep = 548. State = [[-0.0881773   0.12056421]]. Action = [[ 0.09273645  0.0542526   0.         -0.04003352]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 548 is [True, False, False, False, True, False]
State prediction error at timestep 548 is tensor(3.4637e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 548 of None
Current timestep = 549. State = [[-0.08327189  0.11730503]]. Action = [[ 0.05037384 -0.08866217  0.         -0.5403863 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 549 is [True, False, False, False, True, False]
State prediction error at timestep 549 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 549 of None
Current timestep = 550. State = [[-0.08024796  0.11220176]]. Action = [[ 0.01845197 -0.04167136  0.         -0.61198634]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 550 is [True, False, False, False, True, False]
State prediction error at timestep 550 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 550 of None
Current timestep = 551. State = [[-0.07769644  0.10767371]]. Action = [[ 0.01812482 -0.0509989   0.         -0.57754076]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 551 is [True, False, False, False, True, False]
State prediction error at timestep 551 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 551 of None
Current timestep = 552. State = [[-0.07147727  0.10377191]]. Action = [[ 0.08927567 -0.02868698  0.         -0.97523606]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 552 is [True, False, False, False, True, False]
State prediction error at timestep 552 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 552 of None
Current timestep = 553. State = [[-0.06801116  0.10000532]]. Action = [[-0.0125605  -0.033888    0.         -0.63755894]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 553 is [True, False, False, False, True, False]
State prediction error at timestep 553 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 553 of None
Current timestep = 554. State = [[-0.06795342  0.09486064]]. Action = [[-0.03334014 -0.06313461  0.         -0.6811187 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 554 is [True, False, False, False, True, False]
State prediction error at timestep 554 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 554 of None
Current timestep = 555. State = [[-0.06673533  0.09253994]]. Action = [[ 0.00523585  0.00989383  0.         -0.97257245]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 555 is [True, False, False, False, True, False]
State prediction error at timestep 555 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 555 of None
Current timestep = 556. State = [[-0.06112857  0.0918948 ]]. Action = [[ 0.08488526  0.00471036  0.         -0.828215  ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 556 is [True, False, False, False, True, False]
State prediction error at timestep 556 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 556 of None
Current timestep = 557. State = [[-0.05804756  0.09567707]]. Action = [[-0.00598137  0.09413853  0.         -0.45436877]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 557 is [True, False, False, False, True, False]
State prediction error at timestep 557 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 557 of None
Current timestep = 558. State = [[-0.05302097  0.09429113]]. Action = [[ 0.08738204 -0.07050566  0.         -0.7150705 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 558 is [True, False, False, False, True, False]
State prediction error at timestep 558 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 558 of None
Current timestep = 559. State = [[-0.33844447  0.17249212]]. Action = [[0.08087962 0.09648389 0.         0.4302752 ]]. Reward = [100.]
Curr episode timestep = 48
Scene graph at timestep 559 is [True, False, False, False, False, True]
State prediction error at timestep 559 is tensor(0.0423, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 559 of None
Current timestep = 560. State = [[-0.3458586  0.1760356]]. Action = [[-0.07481678  0.01634103  0.          0.15289104]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 560 is [True, False, False, False, False, True]
State prediction error at timestep 560 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 560 of None
Current timestep = 561. State = [[-0.34932995  0.17450996]]. Action = [[-0.00317583 -0.06989611  0.         -0.9837991 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 561 is [True, False, False, False, False, True]
State prediction error at timestep 561 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 561 of None
Current timestep = 562. State = [[-0.3542831   0.17361999]]. Action = [[-0.07868577  0.00056628  0.          0.25656855]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 562 is [True, False, False, False, False, True]
State prediction error at timestep 562 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 562 of None
Current timestep = 563. State = [[-0.35539147  0.17799692]]. Action = [[ 0.05477179  0.07963195  0.         -0.7304185 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 563 is [True, False, False, False, False, True]
State prediction error at timestep 563 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 563 of None
Current timestep = 564. State = [[-0.35185143  0.1841879 ]]. Action = [[ 0.07264747  0.07957477  0.         -0.57663023]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 564 is [True, False, False, False, False, True]
State prediction error at timestep 564 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 564 of None
Current timestep = 565. State = [[-0.3463381  0.1865168]]. Action = [[ 0.08989149  0.00567225  0.         -0.05512255]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 565 is [True, False, False, False, False, True]
State prediction error at timestep 565 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 565 of None
Current timestep = 566. State = [[-0.34765074  0.18344769]]. Action = [[-0.08559708 -0.07820754  0.         -0.12908006]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 566 is [True, False, False, False, False, True]
State prediction error at timestep 566 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 566 of None
Current timestep = 567. State = [[-0.3481586   0.18181512]]. Action = [[ 0.04937271  0.00928976  0.         -0.81188226]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 567 is [True, False, False, False, False, True]
State prediction error at timestep 567 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 567 of None
Current timestep = 568. State = [[-0.34762514  0.18086207]]. Action = [[-0.00981151 -0.02027389  0.         -0.35185385]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 568 is [True, False, False, False, False, True]
State prediction error at timestep 568 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 568 of None
Current timestep = 569. State = [[-0.34525135  0.18339173]]. Action = [[0.05576172 0.06785766 0.         0.03660917]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 569 is [True, False, False, False, False, True]
State prediction error at timestep 569 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 569 of None
Current timestep = 570. State = [[-0.34812418  0.18700655]]. Action = [[-0.08955034  0.03592894  0.          0.09634864]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 570 is [True, False, False, False, False, True]
State prediction error at timestep 570 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 570 of None
Current timestep = 571. State = [[-0.35495675  0.19240527]]. Action = [[-0.07515022  0.07774586  0.         -0.6502125 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 571 is [True, False, False, False, False, True]
State prediction error at timestep 571 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 571 of None
Current timestep = 572. State = [[-0.36084253  0.19432862]]. Action = [[-0.05063985 -0.02247103  0.         -0.7186309 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 572 is [True, False, False, False, False, True]
State prediction error at timestep 572 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 572 of None
Current timestep = 573. State = [[-0.36751956  0.19812313]]. Action = [[-0.07811721  0.067366    0.         -0.9639782 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 573 is [True, False, False, False, False, True]
State prediction error at timestep 573 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 573 of None
Current timestep = 574. State = [[-0.37476385  0.20563504]]. Action = [[-0.0603688   0.09365318  0.         -0.2743988 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 574 is [True, False, False, False, False, True]
State prediction error at timestep 574 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 574 of None
Current timestep = 575. State = [[-0.3788643   0.20936196]]. Action = [[ 0.          0.          0.         -0.87405586]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 575 is [True, False, False, False, False, True]
State prediction error at timestep 575 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 575 of None
Current timestep = 576. State = [[-0.37703496  0.21087621]]. Action = [[ 0.07806211  0.01362626  0.         -0.16845089]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 576 is [True, False, False, False, False, True]
State prediction error at timestep 576 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 576 of None
Current timestep = 577. State = [[-0.37617663  0.21170555]]. Action = [[ 0.          0.          0.         -0.85999095]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 577 is [True, False, False, False, False, True]
State prediction error at timestep 577 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 577 of None
Current timestep = 578. State = [[-0.37278074  0.2156576 ]]. Action = [[0.08709898 0.0723651  0.         0.0897727 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 578 is [True, False, False, False, False, True]
State prediction error at timestep 578 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 578 of None
Current timestep = 579. State = [[-0.36747506  0.21575299]]. Action = [[ 0.06355289 -0.0436518   0.         -0.49836302]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 579 is [True, False, False, False, False, True]
State prediction error at timestep 579 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 579 of None
Current timestep = 580. State = [[-0.36303723  0.2111607 ]]. Action = [[ 0.03971662 -0.07985608  0.         -0.7669274 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 580 is [True, False, False, False, False, True]
State prediction error at timestep 580 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 580 of None
Current timestep = 581. State = [[-0.36442727  0.20743191]]. Action = [[-0.07682313 -0.04622988  0.          0.36436534]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 581 is [True, False, False, False, False, True]
State prediction error at timestep 581 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 581 of None
Current timestep = 582. State = [[-0.36106488  0.20649098]]. Action = [[ 0.09146688 -0.00230528  0.         -0.927411  ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 582 is [True, False, False, False, False, True]
State prediction error at timestep 582 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 582 of None
Current timestep = 583. State = [[-0.3530516   0.20584914]]. Action = [[ 0.07888923 -0.00925609  0.         -0.80217665]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 583 is [True, False, False, False, False, True]
State prediction error at timestep 583 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 583 of None
Current timestep = 584. State = [[-0.344487    0.20308888]]. Action = [[ 0.08377207 -0.04175016  0.         -0.37514126]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 584 is [True, False, False, False, False, True]
State prediction error at timestep 584 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 584 of None
Current timestep = 585. State = [[-0.3439175   0.20038676]]. Action = [[-0.08788439 -0.02625225  0.         -0.5847831 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 585 is [True, False, False, False, False, True]
State prediction error at timestep 585 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 585 of None
Current timestep = 586. State = [[-0.34329563  0.1997745 ]]. Action = [[ 0.02353217  0.00202602  0.         -0.30630726]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 586 is [True, False, False, False, False, True]
State prediction error at timestep 586 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 586 of None
Current timestep = 587. State = [[-0.34538332  0.19631079]]. Action = [[-0.08647256 -0.07209126  0.         -0.2562604 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 587 is [True, False, False, False, False, True]
State prediction error at timestep 587 is tensor(6.0967e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 587 of None
Current timestep = 588. State = [[-0.34827614  0.19776052]]. Action = [[-0.02586249  0.07099742  0.          0.3371222 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 588 is [True, False, False, False, False, True]
State prediction error at timestep 588 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 588 of None
Current timestep = 589. State = [[-0.34401923  0.19872531]]. Action = [[ 0.09739711 -0.01241609  0.          0.6788218 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 589 is [True, False, False, False, False, True]
State prediction error at timestep 589 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 589 of None
Current timestep = 590. State = [[-0.336202    0.19464464]]. Action = [[ 0.09153926 -0.05628297  0.         -0.8765905 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 590 is [True, False, False, False, False, True]
State prediction error at timestep 590 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 590 of None
Current timestep = 591. State = [[-0.3322671  0.1955339]]. Action = [[ 0.00659671  0.07650862  0.         -0.6527873 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 591 is [True, False, False, False, False, True]
State prediction error at timestep 591 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 591 of None
Current timestep = 592. State = [[-0.32978603  0.19998127]]. Action = [[ 0.03550252  0.06602284  0.         -0.40060747]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 592 is [True, False, False, False, False, True]
State prediction error at timestep 592 is tensor(8.6803e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 592 of None
Current timestep = 593. State = [[-0.33204243  0.20024012]]. Action = [[-0.07770424 -0.03056848  0.         -0.9990724 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 593 is [True, False, False, False, False, True]
State prediction error at timestep 593 is tensor(4.1995e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 593 of None
Current timestep = 594. State = [[-0.33484262  0.19839446]]. Action = [[-0.01453017 -0.02524221  0.         -0.7067156 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 594 is [True, False, False, False, False, True]
State prediction error at timestep 594 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 594 of None
Current timestep = 595. State = [[-0.33134812  0.20205903]]. Action = [[ 0.09095774  0.09863707  0.         -0.9078261 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 595 is [True, False, False, False, False, True]
State prediction error at timestep 595 is tensor(7.0647e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 595 of None
Current timestep = 596. State = [[-0.33228338  0.2083962 ]]. Action = [[-0.0695235   0.07551303  0.         -0.81383604]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 596 is [True, False, False, False, False, True]
State prediction error at timestep 596 is tensor(5.2380e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 596 of None
Current timestep = 597. State = [[-0.3307293   0.21238564]]. Action = [[ 0.07826222  0.0297216   0.         -0.8877925 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 597 is [True, False, False, False, False, True]
State prediction error at timestep 597 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 597 of None
Current timestep = 598. State = [[-0.3270834  0.210499 ]]. Action = [[ 0.03120924 -0.06269739  0.         -0.7965759 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 598 is [True, False, False, False, False, True]
State prediction error at timestep 598 is tensor(5.6415e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 598 of None
Current timestep = 599. State = [[-0.3284245   0.20662265]]. Action = [[-0.05861041 -0.0594977   0.         -0.98397595]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 599 is [True, False, False, False, False, True]
State prediction error at timestep 599 is tensor(5.8665e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 599 of None
Current timestep = 600. State = [[-0.32567942  0.20988268]]. Action = [[ 0.08838887  0.09594963  0.         -0.9379067 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 600 is [True, False, False, False, False, True]
State prediction error at timestep 600 is tensor(3.0896e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 600 of None
Current timestep = 601. State = [[-0.3252917   0.20959538]]. Action = [[-0.07537161 -0.08886627  0.         -0.94796556]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 601 is [True, False, False, False, False, True]
State prediction error at timestep 601 is tensor(6.5184e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 601 of None
Current timestep = 602. State = [[-0.3306284   0.20742747]]. Action = [[-0.08210637 -0.02011269  0.         -0.8482857 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 602 is [True, False, False, False, False, True]
State prediction error at timestep 602 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 602 of None
Current timestep = 603. State = [[-0.32980156  0.20787887]]. Action = [[ 0.06664736  0.01035118  0.         -0.9645829 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 603 is [True, False, False, False, False, True]
State prediction error at timestep 603 is tensor(5.8743e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 603 of None
Current timestep = 604. State = [[-0.3238282  0.2111914]]. Action = [[ 0.08297012  0.06404174  0.         -0.0879786 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 604 is [True, False, False, False, False, True]
State prediction error at timestep 604 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 604 of None
Current timestep = 605. State = [[-0.31799603  0.20781752]]. Action = [[ 0.0574734  -0.0988246   0.         -0.49383008]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 605 is [True, False, False, False, False, True]
State prediction error at timestep 605 is tensor(7.5170e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 605 of None
Current timestep = 606. State = [[-0.31135646  0.20808455]]. Action = [[0.08355542 0.07536071 0.         0.41712475]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 606 is [True, False, False, False, False, True]
State prediction error at timestep 606 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 606 of None
Current timestep = 607. State = [[-0.30623037  0.2054954 ]]. Action = [[ 0.02873773 -0.08177964  0.         -0.8690293 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 607 is [True, False, False, False, False, True]
State prediction error at timestep 607 is tensor(4.6446e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 607 of None
Current timestep = 608. State = [[-0.3006578   0.20699179]]. Action = [[ 0.07176108  0.09299018  0.         -0.924362  ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 608 is [True, False, False, False, False, True]
State prediction error at timestep 608 is tensor(1.4691e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 608 of None
Current timestep = 609. State = [[-0.2942203   0.20742954]]. Action = [[ 0.06093001 -0.02519904  0.         -0.42758965]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 609 is [True, False, False, False, False, True]
State prediction error at timestep 609 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 609 of None
Current timestep = 610. State = [[-0.28627712  0.21053626]]. Action = [[ 0.09187192  0.09225894  0.         -0.00030965]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 610 is [True, False, False, False, False, True]
State prediction error at timestep 610 is tensor(4.2138e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 610 of None
Current timestep = 611. State = [[-0.280709    0.20956472]]. Action = [[ 0.01869567 -0.06181797  0.         -0.4008453 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 611 is [True, False, False, False, False, True]
State prediction error at timestep 611 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 611 of None
Current timestep = 612. State = [[-0.2771763   0.21229905]]. Action = [[ 0.0244455   0.09626503  0.         -0.79703677]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 612 is [True, False, False, False, False, True]
State prediction error at timestep 612 is tensor(2.8848e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 612 of None
Current timestep = 613. State = [[-0.270359   0.2106447]]. Action = [[ 0.09206829 -0.08294318  0.         -0.7330631 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 613 is [True, False, False, False, False, True]
State prediction error at timestep 613 is tensor(5.4591e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 613 of None
Current timestep = 614. State = [[-0.2608962   0.20572136]]. Action = [[ 0.09375852 -0.04730086  0.         -0.9370782 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 614 is [True, False, False, False, False, True]
State prediction error at timestep 614 is tensor(3.9763e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 614 of None
Current timestep = 615. State = [[-0.25827858  0.20487481]]. Action = [[-0.05406648  0.0191981   0.          0.70596504]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 615 is [True, False, False, False, False, True]
State prediction error at timestep 615 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 615 of None
Current timestep = 616. State = [[-0.25601152  0.2046565 ]]. Action = [[ 0.02852095 -0.01786149  0.         -0.86152446]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 616 is [True, False, False, False, False, True]
State prediction error at timestep 616 is tensor(4.0906e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 616 of None
Current timestep = 617. State = [[-0.25404423  0.19966401]]. Action = [[-0.02037843 -0.0960578   0.         -0.9229336 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 617 is [True, False, False, False, False, True]
State prediction error at timestep 617 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 617 of None
Current timestep = 618. State = [[-0.25629044  0.19951037]]. Action = [[-0.07899954  0.0446322   0.          0.44405615]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 618 is [True, False, False, False, False, True]
State prediction error at timestep 618 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 618 of None
Current timestep = 619. State = [[-0.2617758   0.19927931]]. Action = [[-0.09445044 -0.04428436  0.         -0.9880715 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 619 is [True, False, False, False, False, True]
State prediction error at timestep 619 is tensor(3.8390e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 619 of None
Current timestep = 620. State = [[-0.26123792  0.20100665]]. Action = [[ 0.06096568  0.05138946  0.         -0.12642717]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 620 is [True, False, False, False, False, True]
State prediction error at timestep 620 is tensor(1.8525e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 620 of None
Current timestep = 621. State = [[-0.25405297  0.19970492]]. Action = [[ 0.09848437 -0.05780016  0.         -0.84140456]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 621 is [True, False, False, False, False, True]
State prediction error at timestep 621 is tensor(4.1408e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 621 of None
Current timestep = 622. State = [[-0.2477542  0.1988828]]. Action = [[ 0.04674495  0.02137645  0.         -0.84299153]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 622 is [True, False, False, False, False, True]
State prediction error at timestep 622 is tensor(7.0107e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 622 of None
Current timestep = 623. State = [[-0.24308144  0.20271993]]. Action = [[ 0.04561526  0.07599802  0.         -0.23985577]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 623 is [True, False, False, False, False, True]
State prediction error at timestep 623 is tensor(6.3059e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 623 of None
Current timestep = 624. State = [[-0.24334936  0.20543383]]. Action = [[-0.04872105  0.01334139  0.          0.37829137]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 624 is [True, False, False, False, False, True]
State prediction error at timestep 624 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 624 of None
Current timestep = 625. State = [[-0.2485784  0.2044489]]. Action = [[-0.0926365  -0.04114735  0.         -0.6823578 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 625 is [True, False, False, False, False, True]
State prediction error at timestep 625 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 625 of None
Current timestep = 626. State = [[-0.25044072  0.20326263]]. Action = [[ 0.00790635 -0.01815805  0.          0.22903311]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 626 is [True, False, False, False, False, True]
State prediction error at timestep 626 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 626 of None
Current timestep = 627. State = [[-0.25430372  0.20194854]]. Action = [[-0.08576979 -0.02905196  0.         -0.9786542 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 627 is [True, False, False, False, False, True]
State prediction error at timestep 627 is tensor(4.9680e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 627 of None
Current timestep = 628. State = [[-0.25527307  0.20070933]]. Action = [[ 0.02931156 -0.02134444  0.         -0.7460973 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 628 is [True, False, False, False, False, True]
State prediction error at timestep 628 is tensor(5.9621e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 628 of None
Current timestep = 629. State = [[-0.25080964  0.19854061]]. Action = [[ 0.07759208 -0.03019757  0.         -0.44757903]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 629 is [True, False, False, False, False, True]
State prediction error at timestep 629 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 629 of None
Current timestep = 630. State = [[-0.24378805  0.19850208]]. Action = [[0.09749503 0.033984   0.         0.06271482]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 630 is [True, False, False, False, False, True]
State prediction error at timestep 630 is tensor(1.7690e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 630 of None
Current timestep = 631. State = [[-0.2362498  0.194881 ]]. Action = [[ 0.08864775 -0.06692711  0.          0.6907854 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 631 is [True, False, False, False, False, True]
State prediction error at timestep 631 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 631 of None
Current timestep = 632. State = [[-0.22894543  0.19586058]]. Action = [[ 0.07890714  0.08785111  0.         -0.99468154]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 632 is [True, False, False, False, False, True]
State prediction error at timestep 632 is tensor(3.8429e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 632 of None
Current timestep = 633. State = [[-0.22108641  0.19453633]]. Action = [[ 0.09399367 -0.04399243  0.         -0.8239386 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 633 is [True, False, False, False, False, True]
State prediction error at timestep 633 is tensor(8.8242e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 633 of None
Current timestep = 634. State = [[-0.2129394   0.19574732]]. Action = [[ 0.0828483  0.0791489  0.        -0.7261174]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 634 is [True, False, False, False, False, True]
State prediction error at timestep 634 is tensor(6.9344e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 634 of None
Current timestep = 635. State = [[-0.20812994  0.20107488]]. Action = [[ 0.02235992  0.09180338  0.         -0.97722065]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 635 is [True, False, False, False, False, True]
State prediction error at timestep 635 is tensor(6.6083e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 635 of None
Current timestep = 636. State = [[-0.20793952  0.19889694]]. Action = [[-0.04709195 -0.09352652  0.         -0.02950972]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 636 is [True, False, False, False, False, True]
State prediction error at timestep 636 is tensor(1.6742e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 636 of None
Current timestep = 637. State = [[-0.20816725  0.19712277]]. Action = [[-0.00825642  0.01305436  0.          0.08223927]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 637 is [True, False, False, False, False, True]
State prediction error at timestep 637 is tensor(6.2521e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 637 of None
Current timestep = 638. State = [[-0.20549515  0.20118617]]. Action = [[ 0.04187209  0.07768012  0.         -0.880095  ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 638 is [True, False, False, False, False, True]
State prediction error at timestep 638 is tensor(6.2109e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 638 of None
Current timestep = 639. State = [[-0.19896314  0.20674562]]. Action = [[ 0.09859139  0.07118354  0.         -0.4972914 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 639 is [True, False, False, False, False, True]
State prediction error at timestep 639 is tensor(8.3432e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 639 of None
Current timestep = 640. State = [[-0.19364324  0.20427632]]. Action = [[ 0.02874706 -0.09478816  0.          0.770751  ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 640 is [True, False, False, False, False, True]
State prediction error at timestep 640 is tensor(8.6524e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 640 of None
Current timestep = 641. State = [[-0.19544445  0.19756849]]. Action = [[-0.08874512 -0.09548942  0.         -0.9475864 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 641 is [True, False, False, False, False, True]
State prediction error at timestep 641 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 641 of None
Current timestep = 642. State = [[-0.19555365  0.19437739]]. Action = [[ 0.01915906 -0.01939958  0.         -0.8775045 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 642 is [True, False, False, False, False, True]
State prediction error at timestep 642 is tensor(6.6183e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 642 of None
Current timestep = 643. State = [[-0.19754036  0.19342935]]. Action = [[-0.07604833 -0.01415092  0.         -0.44371533]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 643 is [True, False, False, False, False, True]
State prediction error at timestep 643 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 643 of None
Current timestep = 644. State = [[-0.20239103  0.19569404]]. Action = [[-0.07567173  0.03949643  0.         -0.5922185 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 644 is [True, False, False, False, False, True]
State prediction error at timestep 644 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 644 of None
Current timestep = 645. State = [[-0.20704725  0.20124236]]. Action = [[-0.05119219  0.07225261  0.         -0.9836193 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 645 is [True, False, False, False, False, True]
State prediction error at timestep 645 is tensor(2.4185e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 645 of None
Current timestep = 646. State = [[-0.21397711  0.20747992]]. Action = [[-0.09477343  0.06180889  0.          0.53159714]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 646 is [True, False, False, False, False, True]
State prediction error at timestep 646 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 646 of None
Current timestep = 647. State = [[-0.21623029  0.21106291]]. Action = [[ 0.03586108  0.0099887   0.         -0.74267554]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 647 is [True, False, False, False, False, True]
State prediction error at timestep 647 is tensor(4.3794e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 647 of None
Current timestep = 648. State = [[-0.22020105  0.21345362]]. Action = [[-0.07099564  0.02001761  0.         -0.9844585 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 648 is [True, False, False, False, False, True]
State prediction error at timestep 648 is tensor(2.2287e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 648 of None
Current timestep = 649. State = [[-0.22659343  0.21577294]]. Action = [[-0.06025081  0.00900584  0.         -0.9542409 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 649 is [True, False, False, False, False, True]
State prediction error at timestep 649 is tensor(2.2477e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 649 of None
Current timestep = 650. State = [[-0.22596888  0.22075202]]. Action = [[0.09271664 0.07988923 0.         0.3213768 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 650 is [True, False, False, False, False, True]
State prediction error at timestep 650 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 650 of None
Current timestep = 651. State = [[-0.22743413  0.22507882]]. Action = [[-0.05365675  0.02400132  0.          0.39675796]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 651 is [True, False, False, False, False, True]
State prediction error at timestep 651 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 651 of None
Current timestep = 652. State = [[-0.23367293  0.22359927]]. Action = [[-0.07413442 -0.07145747  0.         -0.9704362 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 652 is [True, False, False, False, False, True]
State prediction error at timestep 652 is tensor(3.0375e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 652 of None
Current timestep = 653. State = [[-0.24033731  0.22104669]]. Action = [[-0.07228331 -0.04150636  0.         -0.9649498 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 653 is [True, False, False, False, False, True]
State prediction error at timestep 653 is tensor(3.3351e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 653 of None
Current timestep = 654. State = [[-0.24386132  0.21762724]]. Action = [[-0.00897346 -0.0668564   0.          0.09964621]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 654 is [True, False, False, False, False, True]
State prediction error at timestep 654 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 654 of None
Current timestep = 655. State = [[-0.24581861  0.22056587]]. Action = [[-0.01096582  0.09117777  0.         -0.33086097]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 655 is [True, False, False, False, False, True]
State prediction error at timestep 655 is tensor(5.4003e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 655 of None
Current timestep = 656. State = [[-0.24543907  0.22115862]]. Action = [[ 0.04098373 -0.03921248  0.         -0.66430426]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 656 is [True, False, False, False, False, True]
State prediction error at timestep 656 is tensor(7.4150e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 656 of None
Current timestep = 657. State = [[-0.24513121  0.21727437]]. Action = [[-0.00133132 -0.0548162   0.         -0.8661055 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 657 is [True, False, False, False, False, True]
State prediction error at timestep 657 is tensor(6.0004e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 657 of None
Current timestep = 658. State = [[-0.24783719  0.21987088]]. Action = [[-0.04181717  0.09233918  0.         -0.72249055]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 658 is [True, False, False, False, False, True]
State prediction error at timestep 658 is tensor(2.1778e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 658 of None
Current timestep = 659. State = [[-0.24596918  0.22274065]]. Action = [[ 0.08431611  0.01609199  0.         -0.63080114]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 659 is [True, False, False, False, False, True]
State prediction error at timestep 659 is tensor(3.3415e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 659 of None
Current timestep = 660. State = [[-0.24122916  0.22021829]]. Action = [[ 0.06275976 -0.04571314  0.         -0.10829455]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 660 is [True, False, False, False, False, True]
State prediction error at timestep 660 is tensor(2.4730e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 660 of None
Current timestep = 661. State = [[-0.23675181  0.2182385 ]]. Action = [[ 0.0531683   0.0049869   0.         -0.35500515]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 661 is [True, False, False, False, False, True]
State prediction error at timestep 661 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 661 of None
Current timestep = 662. State = [[-0.23268823  0.21688741]]. Action = [[ 0.04372063 -0.00857604  0.          0.735559  ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 662 is [True, False, False, False, False, True]
State prediction error at timestep 662 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 662 of None
Current timestep = 663. State = [[-0.23488975  0.2182345 ]]. Action = [[-0.08275677  0.04598222  0.         -0.80637765]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 663 is [True, False, False, False, False, True]
State prediction error at timestep 663 is tensor(7.4028e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 663 of None
Current timestep = 664. State = [[-0.23348771  0.21932101]]. Action = [[0.0682798  0.00106657 0.         0.8464353 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 664 is [True, False, False, False, False, True]
State prediction error at timestep 664 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 664 of None
Current timestep = 665. State = [[-0.2319701   0.22119457]]. Action = [[-0.01305398  0.04409815  0.         -0.5842229 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 665 is [True, False, False, False, False, True]
State prediction error at timestep 665 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 665 of None
Current timestep = 666. State = [[-0.23272476  0.22088721]]. Action = [[-0.01639304 -0.03170468  0.         -0.98920864]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 666 is [True, False, False, False, False, True]
State prediction error at timestep 666 is tensor(2.7800e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 666 of None
Current timestep = 667. State = [[-0.23687616  0.218717  ]]. Action = [[-0.08640352 -0.03727583  0.         -0.9855602 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 667 is [True, False, False, False, False, True]
State prediction error at timestep 667 is tensor(4.8712e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 667 of None
Current timestep = 668. State = [[-0.2372967   0.21882173]]. Action = [[ 0.03600959  0.01339464  0.         -0.39592373]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 668 is [True, False, False, False, False, True]
State prediction error at timestep 668 is tensor(9.1486e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 668 of None
Current timestep = 669. State = [[-0.23653045  0.22088313]]. Action = [[-0.00793909  0.02860726  0.         -0.33672547]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 669 is [True, False, False, False, False, True]
State prediction error at timestep 669 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 669 of None
Current timestep = 670. State = [[-0.23631418  0.22267702]]. Action = [[ 0.00597535  0.01255739  0.         -0.92794293]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 670 is [True, False, False, False, False, True]
State prediction error at timestep 670 is tensor(7.3727e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 670 of None
Current timestep = 671. State = [[-0.23157983  0.21946958]]. Action = [[ 0.08839173 -0.07600591  0.         -0.88182104]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 671 is [True, False, False, False, False, True]
State prediction error at timestep 671 is tensor(3.7220e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 671 of None
Current timestep = 672. State = [[-0.22725192  0.21624812]]. Action = [[ 0.02246039 -0.01670019  0.         -0.5258692 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 672 is [True, False, False, False, False, True]
State prediction error at timestep 672 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 672 of None
Current timestep = 673. State = [[-0.22102132  0.21579637]]. Action = [[ 0.09420461  0.01364545  0.         -0.01952273]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 673 is [True, False, False, False, False, True]
State prediction error at timestep 673 is tensor(1.3700e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 673 of None
Current timestep = 674. State = [[-0.21821406  0.21434234]]. Action = [[-0.01926376 -0.02296376  0.          0.26151466]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 674 is [True, False, False, False, False, True]
State prediction error at timestep 674 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 674 of None
Current timestep = 675. State = [[-0.22174418  0.21157195]]. Action = [[-0.08713161 -0.04018925  0.         -0.79504156]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 675 is [True, False, False, False, False, True]
State prediction error at timestep 675 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 675 of None
Current timestep = 676. State = [[-0.22413974  0.21365859]]. Action = [[-0.01256095  0.06444701  0.          0.3670429 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 676 is [True, False, False, False, False, True]
State prediction error at timestep 676 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 676 of None
Current timestep = 677. State = [[-0.22349495  0.21255602]]. Action = [[ 0.01439345 -0.05946186  0.         -0.8416075 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 677 is [True, False, False, False, False, True]
State prediction error at timestep 677 is tensor(7.7336e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 677 of None
Current timestep = 678. State = [[-0.2268349   0.21035635]]. Action = [[-0.08731909 -0.01552775  0.         -0.04377836]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 678 is [True, False, False, False, False, True]
State prediction error at timestep 678 is tensor(1.7997e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 678 of None
Current timestep = 679. State = [[-0.2315199   0.20713422]]. Action = [[-0.05243193 -0.06518163  0.         -0.9369584 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 679 is [True, False, False, False, False, True]
State prediction error at timestep 679 is tensor(5.1295e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 679 of None
Current timestep = 680. State = [[-0.23124807  0.20133068]]. Action = [[ 0.03355675 -0.08498718  0.         -0.8815718 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 680 is [True, False, False, False, False, True]
State prediction error at timestep 680 is tensor(4.2613e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 680 of None
Current timestep = 681. State = [[-0.22786607  0.19482899]]. Action = [[ 0.0440359  -0.07082949  0.         -0.44301546]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 681 is [True, False, False, False, False, True]
State prediction error at timestep 681 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 681 of None
Current timestep = 682. State = [[-0.22191216  0.18945542]]. Action = [[ 0.08519159 -0.0403239   0.         -0.15979469]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 682 is [True, False, False, False, False, True]
State prediction error at timestep 682 is tensor(8.9868e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 682 of None
Current timestep = 683. State = [[-0.21743594  0.18174064]]. Action = [[ 0.02591098 -0.09581873  0.         -0.8748261 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 683 is [True, False, False, False, False, True]
State prediction error at timestep 683 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 683 of None
Current timestep = 684. State = [[-0.21333481  0.17792825]]. Action = [[ 0.04686034  0.01823232  0.         -0.5184479 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 684 is [True, False, False, False, False, True]
State prediction error at timestep 684 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 684 of None
Current timestep = 685. State = [[-0.20845804  0.1756235 ]]. Action = [[ 0.05714538 -0.00822493  0.         -0.28413397]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 685 is [True, False, False, False, False, True]
State prediction error at timestep 685 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 685 of None
Current timestep = 686. State = [[-0.20911844  0.1689854 ]]. Action = [[-0.07160991 -0.09235428  0.         -0.1055249 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 686 is [True, False, False, False, False, True]
State prediction error at timestep 686 is tensor(4.2307e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 686 of None
Current timestep = 687. State = [[-0.2094627   0.16681692]]. Action = [[ 0.02530081  0.04818561  0.         -0.36090094]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 687 is [True, False, False, False, False, True]
State prediction error at timestep 687 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 687 of None
Current timestep = 688. State = [[-0.20827118  0.16166879]]. Action = [[ 0.00448124 -0.09167729  0.         -0.54868484]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 688 is [True, False, False, False, False, True]
State prediction error at timestep 688 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 688 of None
Current timestep = 689. State = [[-0.20437616  0.16136105]]. Action = [[ 0.07229792  0.0858844   0.         -0.8707937 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 689 is [True, False, False, False, False, True]
State prediction error at timestep 689 is tensor(3.9873e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 689 of None
Current timestep = 690. State = [[-0.19930463  0.16044246]]. Action = [[ 0.05800996 -0.02692471  0.         -0.13413543]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 690 is [True, False, False, False, False, True]
State prediction error at timestep 690 is tensor(6.6281e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 690 of None
Current timestep = 691. State = [[-0.19410199  0.16203056]]. Action = [[ 0.06374992  0.0805077   0.         -0.37404597]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 691 is [True, False, False, False, False, True]
State prediction error at timestep 691 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 691 of None
Current timestep = 692. State = [[-0.19543074  0.16577092]]. Action = [[-0.07240522  0.05286013  0.         -0.7750932 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 692 is [True, False, False, False, False, True]
State prediction error at timestep 692 is tensor(7.8195e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 692 of None
Current timestep = 693. State = [[-0.1942847   0.16265224]]. Action = [[ 0.06205503 -0.08588717  0.         -0.6878755 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 693 is [True, False, False, False, False, True]
State prediction error at timestep 693 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 693 of None
Current timestep = 694. State = [[-0.19275907  0.15518062]]. Action = [[-0.01352839 -0.09315361  0.         -0.41945982]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 694 is [True, False, False, False, False, True]
State prediction error at timestep 694 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 694 of None
Current timestep = 695. State = [[-0.19382747  0.15250129]]. Action = [[-0.03344549  0.00944833  0.          0.08609581]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 695 is [True, False, False, False, False, True]
State prediction error at timestep 695 is tensor(7.7194e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 695 of None
Current timestep = 696. State = [[-0.19156834  0.14759408]]. Action = [[ 0.04995548 -0.0933864   0.         -0.9164132 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 696 is [True, False, False, False, False, True]
State prediction error at timestep 696 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 696 of None
Current timestep = 697. State = [[-0.18710549  0.14569545]]. Action = [[ 0.04605327  0.03356037  0.         -0.89172107]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 697 is [True, False, False, False, False, True]
State prediction error at timestep 697 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 697 of None
Current timestep = 698. State = [[-0.1833998   0.14602661]]. Action = [[0.03498278 0.01111311 0.         0.21686244]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 698 is [True, False, False, False, False, True]
State prediction error at timestep 698 is tensor(1.0201e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 698 of None
Current timestep = 699. State = [[-0.17708772  0.14903232]]. Action = [[ 0.09845764  0.07619444  0.         -0.6925963 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 699 is [True, False, False, False, False, True]
State prediction error at timestep 699 is tensor(6.2244e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 699 of None
Current timestep = 700. State = [[-0.17027943  0.15030925]]. Action = [[ 0.07389463  0.00485037  0.         -0.7112819 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 700 is [True, False, False, False, False, True]
State prediction error at timestep 700 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 700 of None
Current timestep = 701. State = [[-0.16498777  0.154265  ]]. Action = [[0.05006295 0.09374186 0.         0.4150772 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 701 is [True, False, False, False, False, True]
State prediction error at timestep 701 is tensor(7.8123e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 701 of None
Current timestep = 702. State = [[-0.16528334  0.15710425]]. Action = [[-0.05336875  0.00870063  0.         -0.54655534]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 702 is [True, False, False, False, False, True]
State prediction error at timestep 702 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 702 of None
Current timestep = 703. State = [[-0.16228144  0.16069123]]. Action = [[0.08303607 0.06308196 0.         0.28545046]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 703 is [True, False, False, False, False, True]
State prediction error at timestep 703 is tensor(3.0838e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 703 of None
Current timestep = 704. State = [[-0.16253714  0.16673383]]. Action = [[-0.06455845  0.07499553  0.         -0.09456444]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 704 is [True, False, False, False, False, True]
State prediction error at timestep 704 is tensor(1.6618e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 704 of None
Current timestep = 705. State = [[-0.15969037  0.1657738 ]]. Action = [[ 0.08099215 -0.0860699   0.         -0.8776778 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 705 is [True, False, False, False, False, True]
State prediction error at timestep 705 is tensor(3.4134e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 705 of None
Current timestep = 706. State = [[-0.15767622  0.16764551]]. Action = [[-0.02161039  0.06947843  0.         -0.45366234]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 706 is [True, False, False, False, False, True]
State prediction error at timestep 706 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 706 of None
Current timestep = 707. State = [[-0.15686238  0.17432913]]. Action = [[0.01543687 0.073479   0.         0.00144446]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 707 is [True, False, False, False, False, True]
State prediction error at timestep 707 is tensor(2.5519e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 707 of None
Current timestep = 708. State = [[-0.1591824   0.18165414]]. Action = [[-0.06125847  0.07362426  0.         -0.95208585]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 708 is [True, False, False, False, False, True]
State prediction error at timestep 708 is tensor(1.7218e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 708 of None
Current timestep = 709. State = [[-0.159296    0.18560125]]. Action = [[ 0.02910168 -0.00277322  0.         -0.6312248 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 709 is [True, False, False, False, False, True]
State prediction error at timestep 709 is tensor(5.3474e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 709 of None
Current timestep = 710. State = [[-0.16219701  0.18404758]]. Action = [[-0.08801895 -0.07487424  0.         -0.7066295 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 710 is [True, False, False, False, False, True]
State prediction error at timestep 710 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 710 of None
Current timestep = 711. State = [[-0.16722609  0.18351777]]. Action = [[-0.06638209 -0.01224699  0.         -0.79488623]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 711 is [True, False, False, False, False, True]
State prediction error at timestep 711 is tensor(5.7990e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 711 of None
Current timestep = 712. State = [[-0.1719394   0.18600741]]. Action = [[-0.06004999  0.01945768  0.          0.03730249]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 712 is [True, False, False, False, False, True]
State prediction error at timestep 712 is tensor(3.1941e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 712 of None
Current timestep = 713. State = [[-0.17528525  0.18663351]]. Action = [[-0.02921057 -0.03419071  0.         -0.82578933]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 713 is [True, False, False, False, False, True]
State prediction error at timestep 713 is tensor(4.6658e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 713 of None
Current timestep = 714. State = [[-0.18024524  0.19012125]]. Action = [[-0.07490948  0.05958103  0.         -0.9929851 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 714 is [True, False, False, False, False, True]
State prediction error at timestep 714 is tensor(3.8945e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 714 of None
Current timestep = 715. State = [[-0.18629068  0.19312108]]. Action = [[-0.06125618 -0.00299867  0.          0.11581945]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 715 is [True, False, False, False, False, True]
State prediction error at timestep 715 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 715 of None
Current timestep = 716. State = [[-0.18765023  0.19246374]]. Action = [[ 0.03033621 -0.03789983  0.         -0.33604884]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 716 is [True, False, False, False, False, True]
State prediction error at timestep 716 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 716 of None
Current timestep = 717. State = [[-0.19209035  0.19276838]]. Action = [[-0.08551618  0.01362847  0.         -0.54537874]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 717 is [True, False, False, False, False, True]
State prediction error at timestep 717 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 717 of None
Current timestep = 718. State = [[-0.19360991  0.19325513]]. Action = [[ 0.042078   -0.00858474  0.         -0.8512873 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 718 is [True, False, False, False, False, True]
State prediction error at timestep 718 is tensor(1.8070e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 718 of None
Current timestep = 719. State = [[-0.19686791  0.1956259 ]]. Action = [[-0.06211795  0.04945264  0.          0.28851616]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 719 is [True, False, False, False, False, True]
State prediction error at timestep 719 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 719 of None
Current timestep = 720. State = [[-0.2005901   0.20089844]]. Action = [[-0.0086655   0.07402278  0.         -0.71818745]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 720 is [True, False, False, False, False, True]
State prediction error at timestep 720 is tensor(1.2731e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 720 of None
Current timestep = 721. State = [[-0.19780618  0.2000557 ]]. Action = [[ 0.09895653 -0.06011579  0.         -0.8354593 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 721 is [True, False, False, False, False, True]
State prediction error at timestep 721 is tensor(2.8276e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 721 of None
Current timestep = 722. State = [[-0.20009048  0.20056912]]. Action = [[-0.07921505  0.05199658  0.         -0.02548915]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 722 is [True, False, False, False, False, True]
State prediction error at timestep 722 is tensor(1.6862e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 722 of None
Current timestep = 723. State = [[-0.20355757  0.20633519]]. Action = [[ 0.00144859  0.08666148  0.         -0.7722773 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 723 is [True, False, False, False, False, True]
State prediction error at timestep 723 is tensor(1.9469e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 723 of None
Current timestep = 724. State = [[-0.20255353  0.2114253 ]]. Action = [[ 0.05666017  0.05177224  0.         -0.57580626]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 724 is [True, False, False, False, False, True]
State prediction error at timestep 724 is tensor(2.2671e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 724 of None
Current timestep = 725. State = [[-0.19952844  0.21255091]]. Action = [[ 0.05685186 -0.00686871  0.          0.35779536]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 725 is [True, False, False, False, False, True]
State prediction error at timestep 725 is tensor(6.5647e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 725 of None
Current timestep = 726. State = [[-0.19419904  0.20770468]]. Action = [[ 0.08492806 -0.09248821  0.         -0.36755216]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 726 is [True, False, False, False, False, True]
State prediction error at timestep 726 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 726 of None
Current timestep = 727. State = [[-0.1868536   0.20086703]]. Action = [[ 0.09021271 -0.07614396  0.         -0.4142195 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 727 is [True, False, False, False, False, True]
State prediction error at timestep 727 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 727 of None
Current timestep = 728. State = [[-0.18472053  0.19750474]]. Action = [[-0.03364623 -0.00803602  0.          0.83901906]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 728 is [True, False, False, False, False, True]
State prediction error at timestep 728 is tensor(1.0281e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 728 of None
Current timestep = 729. State = [[-0.18090275  0.19411837]]. Action = [[ 0.06637654 -0.04978693  0.         -0.38389885]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 729 is [True, False, False, False, False, True]
State prediction error at timestep 729 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 729 of None
Current timestep = 730. State = [[-0.18137434  0.1913752 ]]. Action = [[-0.08145909 -0.0131226   0.          0.19101381]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 730 is [True, False, False, False, False, True]
State prediction error at timestep 730 is tensor(6.1725e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 730 of None
Current timestep = 731. State = [[-0.1854289   0.19373879]]. Action = [[-0.06649695  0.061144    0.         -0.7846142 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 731 is [True, False, False, False, False, True]
State prediction error at timestep 731 is tensor(2.9802e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 731 of None
Current timestep = 732. State = [[-0.184528    0.19096729]]. Action = [[ 0.04259891 -0.0893927   0.          0.54204977]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 732 is [True, False, False, False, False, True]
State prediction error at timestep 732 is tensor(6.0834e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 732 of None
Current timestep = 733. State = [[-0.18725148  0.19147645]]. Action = [[-0.09312073  0.0679263   0.         -0.9412025 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 733 is [True, False, False, False, False, True]
State prediction error at timestep 733 is tensor(7.8132e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 733 of None
Current timestep = 734. State = [[-0.18650334  0.19381897]]. Action = [[ 0.06845092  0.01049823  0.         -0.74334   ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 734 is [True, False, False, False, False, True]
State prediction error at timestep 734 is tensor(3.9975e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 734 of None
Current timestep = 735. State = [[-0.1879877   0.19111441]]. Action = [[-0.07435589 -0.06626314  0.         -0.6629184 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 735 is [True, False, False, False, False, True]
State prediction error at timestep 735 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 735 of None
Current timestep = 736. State = [[-0.18686388  0.18450111]]. Action = [[ 0.06213946 -0.09496504  0.         -0.0999583 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 736 is [True, False, False, False, False, True]
State prediction error at timestep 736 is tensor(3.0541e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 736 of None
Current timestep = 737. State = [[-0.18671843  0.17912717]]. Action = [[-0.03638103 -0.04028258  0.         -0.93581367]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 737 is [True, False, False, False, False, True]
State prediction error at timestep 737 is tensor(6.1743e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 737 of None
Current timestep = 738. State = [[-0.18642357  0.17824845]]. Action = [[ 0.02146723  0.02236868  0.         -0.81616735]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 738 is [True, False, False, False, False, True]
State prediction error at timestep 738 is tensor(3.9414e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 738 of None
Current timestep = 739. State = [[-0.18938038  0.18159968]]. Action = [[-0.06649239  0.07195691  0.         -0.7614515 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 739 is [True, False, False, False, False, True]
State prediction error at timestep 739 is tensor(2.8924e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 739 of None
Current timestep = 740. State = [[-0.19602308  0.18546943]]. Action = [[-0.08288784  0.04066379  0.         -0.60546315]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 740 is [True, False, False, False, False, True]
State prediction error at timestep 740 is tensor(2.4765e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 740 of None
Current timestep = 741. State = [[-0.19552056  0.18568431]]. Action = [[ 0.08350687 -0.01938571  0.          0.49074173]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 741 is [True, False, False, False, False, True]
State prediction error at timestep 741 is tensor(2.2341e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 741 of None
Current timestep = 742. State = [[-0.19389246  0.1879456 ]]. Action = [[0.00914097 0.0648182  0.         0.45006108]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 742 is [True, False, False, False, False, True]
State prediction error at timestep 742 is tensor(5.8743e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 742 of None
Current timestep = 743. State = [[-0.19159642  0.19212002]]. Action = [[ 0.06239968  0.05615527  0.         -0.89179873]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 743 is [True, False, False, False, False, True]
State prediction error at timestep 743 is tensor(5.8746e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 743 of None
Current timestep = 744. State = [[-0.190715    0.19598654]]. Action = [[0.00363304 0.05123831 0.         0.9591639 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 744 is [True, False, False, False, False, True]
State prediction error at timestep 744 is tensor(4.3267e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 744 of None
Current timestep = 745. State = [[-0.19484313  0.19433399]]. Action = [[-0.07911585 -0.07326297  0.         -0.7796792 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 745 is [True, False, False, False, False, True]
State prediction error at timestep 745 is tensor(6.3736e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 745 of None
Current timestep = 746. State = [[-0.19324672  0.19098064]]. Action = [[ 0.09206713 -0.03250124  0.         -0.89114326]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 746 is [True, False, False, False, False, True]
State prediction error at timestep 746 is tensor(1.8391e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 746 of None
Current timestep = 747. State = [[-0.19386102  0.19281854]]. Action = [[-0.06245117  0.05651661  0.          0.32625747]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 747 is [True, False, False, False, False, True]
State prediction error at timestep 747 is tensor(3.9956e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 747 of None
Current timestep = 748. State = [[-0.19103707  0.1949288 ]]. Action = [[0.09414282 0.0051381  0.         0.17089558]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 748 is [True, False, False, False, False, True]
State prediction error at timestep 748 is tensor(1.1111e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 748 of None
Current timestep = 749. State = [[-0.18999241  0.19206989]]. Action = [[-0.04240106 -0.06889234  0.          0.5379673 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 749 is [True, False, False, False, False, True]
State prediction error at timestep 749 is tensor(2.7201e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 749 of None
Current timestep = 750. State = [[-0.19053848  0.186194  ]]. Action = [[-0.00203671 -0.08575591  0.         -0.16753787]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 750 is [True, False, False, False, False, True]
State prediction error at timestep 750 is tensor(4.6952e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 750 of None
Current timestep = 751. State = [[-0.19310011  0.18264452]]. Action = [[-0.06616767 -0.0233991   0.         -0.01850593]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 751 is [True, False, False, False, False, True]
State prediction error at timestep 751 is tensor(4.4908e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 751 of None
Current timestep = 752. State = [[-0.19566026  0.18603365]]. Action = [[-0.02124311  0.0830744   0.         -0.9954066 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 752 is [True, False, False, False, False, True]
State prediction error at timestep 752 is tensor(1.5876e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 752 of None
Current timestep = 753. State = [[-0.1917676   0.18338764]]. Action = [[ 0.09484438 -0.09882001  0.         -0.00982201]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 753 is [True, False, False, False, False, True]
State prediction error at timestep 753 is tensor(1.3498e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 753 of None
Current timestep = 754. State = [[-0.18385343  0.17566226]]. Action = [[ 0.09822422 -0.08116858  0.         -0.00965673]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 754 is [True, False, False, False, False, True]
State prediction error at timestep 754 is tensor(3.8672e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 754 of None
Current timestep = 755. State = [[-0.17701465  0.17502117]]. Action = [[ 0.06485412  0.07072622  0.         -0.4990071 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 755 is [True, False, False, False, False, True]
State prediction error at timestep 755 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 755 of None
Current timestep = 756. State = [[-0.17790227  0.17222404]]. Action = [[-0.08729248 -0.07455187  0.         -0.951071  ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 756 is [True, False, False, False, False, True]
State prediction error at timestep 756 is tensor(4.8058e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 756 of None
Current timestep = 757. State = [[-0.17537342  0.16708261]]. Action = [[ 0.09766882 -0.03400389  0.         -0.53300124]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 757 is [True, False, False, False, False, True]
State prediction error at timestep 757 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 757 of None
Current timestep = 758. State = [[-0.1755212   0.16589896]]. Action = [[-0.06972717  0.02744371  0.         -0.26083332]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 758 is [True, False, False, False, False, True]
State prediction error at timestep 758 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 758 of None
Current timestep = 759. State = [[-0.17434442  0.16861539]]. Action = [[ 0.05975468  0.0647266   0.         -0.52785385]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 759 is [True, False, False, False, False, True]
State prediction error at timestep 759 is tensor(4.5526e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 759 of None
Current timestep = 760. State = [[-0.17414704  0.16870168]]. Action = [[-0.03111105 -0.01751219  0.          0.36060333]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 760 is [True, False, False, False, False, True]
State prediction error at timestep 760 is tensor(1.1353e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 760 of None
Current timestep = 761. State = [[-0.17867404  0.1717898 ]]. Action = [[-0.07638167  0.0779795   0.          0.5446844 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 761 is [True, False, False, False, False, True]
State prediction error at timestep 761 is tensor(1.5320e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 761 of None
Current timestep = 762. State = [[-0.17946213  0.17363118]]. Action = [[ 0.03780248 -0.0087795   0.         -0.5450827 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 762 is [True, False, False, False, False, True]
State prediction error at timestep 762 is tensor(4.3937e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 762 of None
Current timestep = 763. State = [[-0.18201646  0.17742221]]. Action = [[-0.05967756  0.07582349  0.         -0.98712957]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 763 is [True, False, False, False, False, True]
State prediction error at timestep 763 is tensor(9.2117e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 763 of None
Current timestep = 764. State = [[-0.18393613  0.17723499]]. Action = [[ 0.00795119 -0.05990616  0.         -0.01541817]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 764 is [True, False, False, False, False, True]
State prediction error at timestep 764 is tensor(8.5345e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 764 of None
Current timestep = 765. State = [[-0.18164843  0.17211229]]. Action = [[ 0.04944909 -0.08039597  0.         -0.51731336]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 765 is [True, False, False, False, False, True]
State prediction error at timestep 765 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 765 of None
Current timestep = 766. State = [[-0.18169984  0.16927576]]. Action = [[-0.0314877  -0.01182826  0.         -0.3450958 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 766 is [True, False, False, False, False, True]
State prediction error at timestep 766 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 766 of None
Current timestep = 767. State = [[-0.18482043  0.17089827]]. Action = [[-0.04778857  0.03722338  0.         -0.761178  ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 767 is [True, False, False, False, False, True]
State prediction error at timestep 767 is tensor(5.3105e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 767 of None
Current timestep = 768. State = [[-0.19001585  0.17550106]]. Action = [[-0.07015808  0.06272685  0.         -0.2930509 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 768 is [True, False, False, False, False, True]
State prediction error at timestep 768 is tensor(4.0836e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 768 of None
Current timestep = 769. State = [[-0.19278331  0.18214957]]. Action = [[ 0.00498321  0.08471084  0.         -0.97801226]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 769 is [True, False, False, False, False, True]
State prediction error at timestep 769 is tensor(2.9799e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 769 of None
Current timestep = 770. State = [[-0.19303966  0.18982819]]. Action = [[ 0.02555949  0.09421455  0.         -0.7884393 ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 770 is [True, False, False, False, False, True]
State prediction error at timestep 770 is tensor(2.1809e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 770 of None
Current timestep = 771. State = [[-0.19706428  0.1902237 ]]. Action = [[-0.07542247 -0.0694467   0.         -0.82025856]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 771 is [True, False, False, False, False, True]
State prediction error at timestep 771 is tensor(3.9852e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 771 of None
Current timestep = 772. State = [[-0.20487505  0.18543203]]. Action = [[-0.09350871 -0.08489807  0.          0.1664561 ]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 772 is [True, False, False, False, False, True]
State prediction error at timestep 772 is tensor(8.4050e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 772 of None
Current timestep = 773. State = [[-0.20668301  0.17992477]]. Action = [[ 0.04065578 -0.07902218  0.         -0.6890601 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 773 is [True, False, False, False, False, True]
State prediction error at timestep 773 is tensor(5.4331e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 773 of None
Current timestep = 774. State = [[-0.20382467  0.17896521]]. Action = [[ 0.05450998  0.02899978  0.         -0.11727142]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 774 is [True, False, False, False, False, True]
State prediction error at timestep 774 is tensor(4.1107e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 774 of None
Current timestep = 775. State = [[-0.20208383  0.18302657]]. Action = [[ 0.02272024  0.07983773  0.         -0.9178512 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 775 is [True, False, False, False, False, True]
State prediction error at timestep 775 is tensor(2.5486e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 775 of None
Current timestep = 776. State = [[-0.20379758  0.18163835]]. Action = [[-0.03530105 -0.0683025   0.         -0.7157455 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 776 is [True, False, False, False, False, True]
State prediction error at timestep 776 is tensor(7.8881e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 776 of None
Current timestep = 777. State = [[-0.20194182  0.17606503]]. Action = [[ 0.06669145 -0.06740217  0.         -0.8612726 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 777 is [True, False, False, False, False, True]
State prediction error at timestep 777 is tensor(4.8588e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 777 of None
Current timestep = 778. State = [[-0.1977369   0.17376553]]. Action = [[ 0.05178187  0.0151819   0.         -0.02401131]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 778 is [True, False, False, False, False, True]
State prediction error at timestep 778 is tensor(2.0023e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 778 of None
Current timestep = 779. State = [[-0.19924554  0.17152181]]. Action = [[-0.06659424 -0.03325915  0.         -0.79453534]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 779 is [True, False, False, False, False, True]
State prediction error at timestep 779 is tensor(8.3636e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 779 of None
Current timestep = 780. State = [[-0.20477085  0.1656378 ]]. Action = [[-0.08136597 -0.09014343  0.          0.33865452]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 780 is [True, False, False, False, False, True]
State prediction error at timestep 780 is tensor(1.8309e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 780 of None
Current timestep = 781. State = [[-0.20291376  0.16290477]]. Action = [[ 0.09623004  0.0187216   0.         -0.9742958 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 781 is [True, False, False, False, False, True]
State prediction error at timestep 781 is tensor(2.4899e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 781 of None
Current timestep = 782. State = [[-0.1950584   0.15948184]]. Action = [[ 0.09563532 -0.05115573  0.          0.19943571]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 782 is [True, False, False, False, False, True]
State prediction error at timestep 782 is tensor(2.6911e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 782 of None
Current timestep = 783. State = [[-0.18808073  0.15660761]]. Action = [[ 0.06319001  0.0029721   0.         -0.9405144 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 783 is [True, False, False, False, False, True]
State prediction error at timestep 783 is tensor(9.6946e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 783 of None
Current timestep = 784. State = [[-0.18112078  0.15607288]]. Action = [[0.07991765 0.02307348 0.         0.7791686 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 784 is [True, False, False, False, False, True]
State prediction error at timestep 784 is tensor(6.8498e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 784 of None
Current timestep = 785. State = [[-0.18014075  0.15609987]]. Action = [[-0.05479734  0.01517282  0.          0.29862905]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 785 is [True, False, False, False, False, True]
State prediction error at timestep 785 is tensor(2.1225e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 785 of None
Current timestep = 786. State = [[-0.17841743  0.15977378]]. Action = [[ 0.04913033  0.08503402  0.         -0.37618297]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 786 is [True, False, False, False, False, True]
State prediction error at timestep 786 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 786 of None
Current timestep = 787. State = [[-0.1788852   0.15793566]]. Action = [[-0.05649011 -0.07784439  0.         -0.4015621 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 787 is [True, False, False, False, False, True]
State prediction error at timestep 787 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 787 of None
Current timestep = 788. State = [[-0.17960013  0.15990649]]. Action = [[ 0.00803082  0.08802976  0.         -0.7093947 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 788 is [True, False, False, False, False, True]
State prediction error at timestep 788 is tensor(3.7687e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 788 of None
Current timestep = 789. State = [[-0.1771965   0.16354458]]. Action = [[ 0.04630091  0.02762879  0.         -0.06905138]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 789 is [True, False, False, False, False, True]
State prediction error at timestep 789 is tensor(9.3241e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 789 of None
Current timestep = 790. State = [[-0.17971955  0.16294879]]. Action = [[-0.08638468 -0.03741048  0.          0.83164144]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 790 is [True, False, False, False, False, True]
State prediction error at timestep 790 is tensor(7.9247e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 790 of None
Current timestep = 791. State = [[-0.18581548  0.1598439 ]]. Action = [[-0.08123295 -0.05831217  0.         -0.97657406]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 791 is [True, False, False, False, False, True]
State prediction error at timestep 791 is tensor(1.9225e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 791 of None
Current timestep = 792. State = [[-0.19252323  0.16197816]]. Action = [[-0.08639898  0.05900807  0.         -0.8389355 ]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 792 is [True, False, False, False, False, True]
State prediction error at timestep 792 is tensor(7.1960e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 792 of None
Current timestep = 793. State = [[-0.19154832  0.16470109]]. Action = [[0.09414358 0.00993919 0.         0.24174225]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 793 is [True, False, False, False, False, True]
State prediction error at timestep 793 is tensor(8.7087e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 793 of None
Current timestep = 794. State = [[-0.19231847  0.16503559]]. Action = [[-0.05716166 -0.01107396  0.         -0.8720465 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 794 is [True, False, False, False, False, True]
State prediction error at timestep 794 is tensor(4.3350e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 794 of None
Current timestep = 795. State = [[-0.19177277  0.16444689]]. Action = [[ 0.05318899 -0.01439317  0.         -0.9853858 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 795 is [True, False, False, False, False, True]
State prediction error at timestep 795 is tensor(5.7856e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 795 of None
Current timestep = 796. State = [[-0.19145918  0.1613872 ]]. Action = [[-0.01473095 -0.05496125  0.         -0.04101241]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 796 is [True, False, False, False, False, True]
State prediction error at timestep 796 is tensor(1.6144e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 796 of None
Current timestep = 797. State = [[-0.19285472  0.15883732]]. Action = [[-0.0198672  -0.02039111  0.         -0.7592256 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 797 is [True, False, False, False, False, True]
State prediction error at timestep 797 is tensor(7.7905e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 797 of None
Current timestep = 798. State = [[-0.19454546  0.15604301]]. Action = [[-0.0218766  -0.04218298  0.         -0.7852765 ]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 798 is [True, False, False, False, False, True]
State prediction error at timestep 798 is tensor(7.9742e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 798 of None
Current timestep = 799. State = [[-0.1952285   0.15547429]]. Action = [[ 0.00078012  0.01820868  0.         -0.6721428 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 799 is [True, False, False, False, False, True]
State prediction error at timestep 799 is tensor(7.4490e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 799 of None
Current timestep = 800. State = [[-0.19296008  0.15643573]]. Action = [[ 0.05318733  0.02104495  0.         -0.9283992 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 800 is [True, False, False, False, False, True]
State prediction error at timestep 800 is tensor(3.2419e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 800 of None
Current timestep = 801. State = [[-0.18724671  0.15404256]]. Action = [[ 0.09341408 -0.04232084  0.         -0.43570817]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 801 is [True, False, False, False, False, True]
State prediction error at timestep 801 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 801 of None
Current timestep = 802. State = [[-0.18346453  0.14725874]]. Action = [[ 0.01723769 -0.09009178  0.          0.80735683]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 802 is [True, False, False, False, False, True]
State prediction error at timestep 802 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 802 of None
Current timestep = 803. State = [[-0.18448077  0.14466867]]. Action = [[-0.04701221  0.02222691  0.         -0.8684809 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 803 is [True, False, False, False, False, True]
State prediction error at timestep 803 is tensor(5.5534e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 803 of None
Current timestep = 804. State = [[-0.18229365  0.14644657]]. Action = [[ 0.06520989  0.04654033  0.         -0.21148145]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 804 is [True, False, False, False, False, True]
State prediction error at timestep 804 is tensor(6.4421e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 804 of None
Current timestep = 805. State = [[-0.17599805  0.14574836]]. Action = [[ 0.08770282 -0.01661402  0.         -0.17325044]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 805 is [True, False, False, False, False, True]
State prediction error at timestep 805 is tensor(8.0568e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 805 of None
Current timestep = 806. State = [[-0.169079    0.13979802]]. Action = [[ 0.07502035 -0.0842034   0.         -0.4269899 ]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 806 is [True, False, False, False, False, True]
State prediction error at timestep 806 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 806 of None
Current timestep = 807. State = [[-0.16223837  0.13937537]]. Action = [[ 0.07280185  0.07264955  0.         -0.07721984]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 807 is [True, False, False, False, False, True]
State prediction error at timestep 807 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 807 of None
Current timestep = 808. State = [[-0.15734512  0.14474335]]. Action = [[0.03995537 0.09839117 0.         0.15999722]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 808 is [True, False, False, False, False, True]
State prediction error at timestep 808 is tensor(5.4691e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 808 of None
Current timestep = 809. State = [[-0.15686202  0.14468586]]. Action = [[-0.03611841 -0.04542102  0.          0.5728071 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 809 is [True, False, False, False, False, True]
State prediction error at timestep 809 is tensor(3.3411e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 809 of None
Current timestep = 810. State = [[-0.15348883  0.14078014]]. Action = [[ 0.06449436 -0.04853399  0.         -0.5285346 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 810 is [True, False, False, False, False, True]
State prediction error at timestep 810 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 810 of None
Current timestep = 811. State = [[-0.15199581  0.14349656]]. Action = [[-0.02817736  0.09283025  0.         -0.301669  ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 811 is [True, False, False, False, False, True]
State prediction error at timestep 811 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 811 of None
Current timestep = 812. State = [[-0.14937373  0.14421788]]. Action = [[ 0.04984509 -0.03862877  0.         -0.6406912 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 812 is [True, False, False, False, False, True]
State prediction error at timestep 812 is tensor(9.8742e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 812 of None
Current timestep = 813. State = [[-0.14621268  0.14184316]]. Action = [[ 0.01328947 -0.02961161  0.         -0.84497255]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 813 is [True, False, False, False, False, True]
State prediction error at timestep 813 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 813 of None
Current timestep = 814. State = [[-0.14795142  0.14021719]]. Action = [[-0.07316679 -0.02063358  0.         -0.8969563 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 814 is [True, False, False, False, False, True]
State prediction error at timestep 814 is tensor(6.1882e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 814 of None
Current timestep = 815. State = [[-0.14641204  0.13713911]]. Action = [[ 0.0444978  -0.05875156  0.         -0.9042825 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 815 is [True, False, False, False, False, True]
State prediction error at timestep 815 is tensor(5.3147e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 815 of None
Current timestep = 816. State = [[-0.13986456  0.14006256]]. Action = [[ 0.09090043  0.09690524  0.         -0.9060341 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 816 is [True, False, False, False, False, True]
State prediction error at timestep 816 is tensor(4.9169e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 816 of None
Current timestep = 817. State = [[-0.13367622  0.1418546 ]]. Action = [[ 0.05868029 -0.01219621  0.         -0.5054985 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 817 is [True, False, False, False, False, True]
State prediction error at timestep 817 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 817 of None
Current timestep = 818. State = [[-0.12687269  0.13722406]]. Action = [[ 0.07802653 -0.08281392  0.         -0.90674835]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 818 is [True, False, False, False, False, True]
State prediction error at timestep 818 is tensor(9.9182e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 818 of None
Current timestep = 819. State = [[-0.12199892  0.13697645]]. Action = [[ 0.02250241  0.05541243  0.         -0.74435097]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 819 is [True, False, False, False, False, True]
State prediction error at timestep 819 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 819 of None
Current timestep = 820. State = [[-0.12111092  0.14031115]]. Action = [[-0.02385489  0.04344568  0.         -0.6893322 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 820 is [True, False, False, False, False, True]
State prediction error at timestep 820 is tensor(8.9144e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 820 of None
Current timestep = 821. State = [[-0.12058946  0.14465453]]. Action = [[ 0.00105224  0.05748702  0.         -0.7532946 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 821 is [True, False, False, False, False, True]
State prediction error at timestep 821 is tensor(4.8506e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 821 of None
Current timestep = 822. State = [[-0.12370177  0.14701755]]. Action = [[-8.3768308e-02 -2.7845055e-04  0.0000000e+00 -3.2602310e-01]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 822 is [True, False, False, False, False, True]
State prediction error at timestep 822 is tensor(8.6634e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 822 of None
Current timestep = 823. State = [[-0.1238711   0.14867082]]. Action = [[0.02856057 0.01160879 0.         0.6682048 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 823 is [True, False, False, False, False, True]
State prediction error at timestep 823 is tensor(1.7407e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 823 of None
Current timestep = 824. State = [[-0.12460189  0.14798395]]. Action = [[-0.04379533 -0.04085473  0.         -0.8989877 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 824 is [True, False, False, False, False, True]
State prediction error at timestep 824 is tensor(4.1602e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 824 of None
Current timestep = 825. State = [[-0.12253784  0.15134344]]. Action = [[ 0.05869288  0.07370894  0.         -0.82746816]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 825 is [True, False, False, False, False, True]
State prediction error at timestep 825 is tensor(3.0416e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 825 of None
Current timestep = 826. State = [[-0.11672271  0.1521524 ]]. Action = [[ 0.07892118 -0.03736067  0.         -0.8511992 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 826 is [True, False, False, False, False, True]
State prediction error at timestep 826 is tensor(5.5267e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 826 of None
Current timestep = 827. State = [[-0.11538351  0.14982833]]. Action = [[-0.03853248 -0.03683665  0.          0.58830893]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 827 is [True, False, False, False, False, True]
State prediction error at timestep 827 is tensor(6.8600e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 827 of None
Current timestep = 828. State = [[-0.11316169  0.15137492]]. Action = [[ 0.04888571  0.04592545  0.         -0.9445583 ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 828 is [True, False, False, False, False, True]
State prediction error at timestep 828 is tensor(1.8103e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 828 of None
Current timestep = 829. State = [[-0.11409966  0.15411086]]. Action = [[-0.06114295  0.02110831  0.         -0.267821  ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 829 is [True, False, False, False, False, True]
State prediction error at timestep 829 is tensor(9.2322e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 829 of None
Current timestep = 830. State = [[-0.11499421  0.15608087]]. Action = [[ 0.00409018  0.01170845  0.         -0.40159595]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 830 is [True, False, False, False, False, True]
State prediction error at timestep 830 is tensor(7.7186e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 830 of None
Current timestep = 831. State = [[-0.11851908  0.15505446]]. Action = [[-0.08485473 -0.04531053  0.         -0.42926586]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 831 is [True, False, False, False, False, True]
State prediction error at timestep 831 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 831 of None
Current timestep = 832. State = [[-0.11876421  0.15885918]]. Action = [[ 0.04640882  0.08958895  0.         -0.952247  ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 832 is [True, False, False, False, False, True]
State prediction error at timestep 832 is tensor(4.4158e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 832 of None
Current timestep = 833. State = [[-0.11421316  0.15808877]]. Action = [[ 0.06463823 -0.0776719   0.         -0.7500523 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 833 is [True, False, False, False, False, True]
State prediction error at timestep 833 is tensor(6.4979e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 833 of None
Current timestep = 834. State = [[-0.11059     0.16032977]]. Action = [[ 0.0254031   0.08487298  0.         -0.9707994 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 834 is [True, False, False, False, False, True]
State prediction error at timestep 834 is tensor(9.5935e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 834 of None
Current timestep = 835. State = [[-0.10706402  0.16705056]]. Action = [[ 0.05439211  0.08461481  0.         -0.10137272]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 835 is [True, False, False, False, False, True]
State prediction error at timestep 835 is tensor(6.7327e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 835 of None
Current timestep = 836. State = [[-0.10161368  0.17386071]]. Action = [[ 0.08472612  0.08274011  0.         -0.68601465]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 836 is [True, False, False, False, False, True]
State prediction error at timestep 836 is tensor(5.5577e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 836 of None
Current timestep = 837. State = [[-0.09607015  0.17918642]]. Action = [[ 0.06780443  0.05397529  0.         -0.5958774 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 837 is [True, False, False, False, False, True]
State prediction error at timestep 837 is tensor(7.6568e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 837 of None
Current timestep = 838. State = [[-0.08974023  0.17738132]]. Action = [[ 0.08159382 -0.07596143  0.         -0.9453525 ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 838 is [True, False, False, False, False, True]
State prediction error at timestep 838 is tensor(5.5817e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 838 of None
Current timestep = 839. State = [[-0.08323535  0.17367321]]. Action = [[ 0.06138403 -0.03627026  0.         -0.82736075]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 839 is [True, False, False, False, False, True]
State prediction error at timestep 839 is tensor(8.6612e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 839 of None
Current timestep = 840. State = [[-0.07649596  0.17382854]]. Action = [[ 0.06973734  0.02835866  0.         -0.8155819 ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 840 is [True, False, False, False, False, True]
State prediction error at timestep 840 is tensor(8.6993e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 840 of None
Current timestep = 841. State = [[-0.07703693  0.17882343]]. Action = [[-0.09003887  0.08160429  0.          0.37636757]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 841 is [True, False, False, False, False, True]
State prediction error at timestep 841 is tensor(1.6048e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 841 of None
Current timestep = 842. State = [[-0.07499424  0.18376917]]. Action = [[ 0.06702308  0.03801759  0.         -0.50322247]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 842 is [True, False, False, False, False, True]
State prediction error at timestep 842 is tensor(4.7452e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 842 of None
Current timestep = 843. State = [[-0.07432514  0.18718126]]. Action = [[-0.04830685  0.02592341  0.         -0.66850996]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 843 is [True, False, False, False, False, True]
State prediction error at timestep 843 is tensor(7.9231e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 843 of None
Current timestep = 844. State = [[-0.07072003  0.18722238]]. Action = [[ 0.0727281  -0.03921443  0.          0.1903255 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 844 is [True, False, False, False, False, True]
State prediction error at timestep 844 is tensor(7.2959e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 844 of None
Current timestep = 845. State = [[-0.0676621   0.18731199]]. Action = [[-0.00767609  0.00541653  0.         -0.75277686]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 845 is [True, False, False, False, False, True]
State prediction error at timestep 845 is tensor(7.9021e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 845 of None
Current timestep = 846. State = [[-0.06159602  0.18794732]]. Action = [[ 0.09753457 -0.00417178  0.         -0.73881674]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 846 is [True, False, False, False, False, True]
State prediction error at timestep 846 is tensor(6.8350e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 846 of None
Current timestep = 847. State = [[-0.05720254  0.18523808]]. Action = [[-0.00128826 -0.06201497  0.          0.15117264]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 847 is [True, False, False, False, False, True]
State prediction error at timestep 847 is tensor(4.8895e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 847 of None
Current timestep = 848. State = [[-0.05552093  0.17928529]]. Action = [[-0.01206763 -0.09314666  0.         -0.9650483 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 848 is [True, False, False, False, False, True]
State prediction error at timestep 848 is tensor(7.9964e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 848 of None
Current timestep = 849. State = [[-0.05674069  0.1762072 ]]. Action = [[-0.06923755 -0.01380563  0.         -0.45466882]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 849 is [True, False, False, False, False, True]
State prediction error at timestep 849 is tensor(5.3302e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 849 of None
Current timestep = 850. State = [[-0.05369406  0.1712636 ]]. Action = [[ 0.05615849 -0.09306554  0.         -0.9394925 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 850 is [True, False, False, False, False, True]
State prediction error at timestep 850 is tensor(3.2114e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 850 of None
Current timestep = 851. State = [[-0.0489368   0.16936362]]. Action = [[0.02638597 0.02666112 0.         0.03922749]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 851 is [False, True, False, False, False, True]
State prediction error at timestep 851 is tensor(8.0480e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 851 of None
Current timestep = 852. State = [[-0.04816472  0.17113498]]. Action = [[-0.03131233  0.03443254  0.         -0.45389378]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 852 is [False, True, False, False, False, True]
State prediction error at timestep 852 is tensor(4.6035e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 852 of None
Current timestep = 853. State = [[-0.0443237   0.17474337]]. Action = [[ 0.080655    0.06163654  0.         -0.9429861 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 853 is [False, True, False, False, False, True]
State prediction error at timestep 853 is tensor(4.2214e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 853 of None
Current timestep = 854. State = [[-0.04440028  0.17918709]]. Action = [[-0.06034715  0.05686305  0.         -0.9860719 ]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 854 is [False, True, False, False, False, True]
State prediction error at timestep 854 is tensor(3.7286e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 854 of None
Current timestep = 855. State = [[-0.04547225  0.17804046]]. Action = [[ 0.00214121 -0.06443019  0.         -0.98794264]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 855 is [False, True, False, False, False, True]
State prediction error at timestep 855 is tensor(1.7638e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 855 of None
Current timestep = 856. State = [[-0.04527114  0.17348659]]. Action = [[-0.00923176 -0.0615316   0.          0.06395268]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 856 is [False, True, False, False, False, True]
State prediction error at timestep 856 is tensor(1.2072e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 856 of None
Current timestep = 857. State = [[-0.04154351  0.16723348]]. Action = [[ 0.0654515  -0.08580338  0.         -0.7366477 ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 857 is [False, True, False, False, False, True]
State prediction error at timestep 857 is tensor(8.7004e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 857 of None
Current timestep = 858. State = [[-0.04169757  0.16592833]]. Action = [[-0.05692024  0.0384692   0.         -0.5745712 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 858 is [False, True, False, False, False, True]
State prediction error at timestep 858 is tensor(5.1386e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 858 of None
Current timestep = 859. State = [[-0.04682346  0.16331224]]. Action = [[-0.0921061  -0.07225795  0.         -0.14371043]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 859 is [False, True, False, False, False, True]
State prediction error at timestep 859 is tensor(2.4226e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 859 of None
Current timestep = 860. State = [[-0.05298929  0.16166012]]. Action = [[-9.073537e-02  7.869601e-04  0.000000e+00 -9.509663e-01]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 860 is [True, False, False, False, False, True]
State prediction error at timestep 860 is tensor(1.5778e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 860 of None
Current timestep = 861. State = [[-0.05919439  0.16246848]]. Action = [[-0.07854428  0.00769665  0.         -0.31575406]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 861 is [True, False, False, False, False, True]
State prediction error at timestep 861 is tensor(4.5799e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 861 of None
Current timestep = 862. State = [[-0.06246668  0.16404177]]. Action = [[-0.01238865  0.01660591  0.          0.5029113 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 862 is [True, False, False, False, False, True]
State prediction error at timestep 862 is tensor(1.5590e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 862 of None
Current timestep = 863. State = [[-0.06421455  0.16562745]]. Action = [[-0.00688659  0.01587838  0.         -0.5727068 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 863 is [True, False, False, False, False, True]
State prediction error at timestep 863 is tensor(1.8553e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 863 of None
Current timestep = 864. State = [[-0.06977747  0.17062795]]. Action = [[-0.08118087  0.08493636  0.         -0.75866765]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 864 is [True, False, False, False, False, True]
State prediction error at timestep 864 is tensor(1.0283e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 864 of None
Current timestep = 865. State = [[-0.07198377  0.17686872]]. Action = [[ 0.0454281   0.06619022  0.         -0.6172999 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 865 is [True, False, False, False, False, True]
State prediction error at timestep 865 is tensor(1.0670e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 865 of None
Current timestep = 866. State = [[-0.07593336  0.17715935]]. Action = [[-0.06789801 -0.04891085  0.         -0.70940125]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 866 is [True, False, False, False, False, True]
State prediction error at timestep 866 is tensor(1.8486e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 866 of None
Current timestep = 867. State = [[-0.07699606  0.18029834]]. Action = [[0.06982625 0.08606017 0.         0.44941294]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 867 is [True, False, False, False, False, True]
State prediction error at timestep 867 is tensor(2.7226e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 867 of None
Current timestep = 868. State = [[-0.07966451  0.18449065]]. Action = [[-0.0518306   0.0294896   0.         -0.55441666]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 868 is [True, False, False, False, False, True]
State prediction error at timestep 868 is tensor(2.4390e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 868 of None
Current timestep = 869. State = [[-0.08646439  0.18877497]]. Action = [[-0.07073341  0.0530088   0.         -0.7865992 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 869 is [True, False, False, False, False, True]
State prediction error at timestep 869 is tensor(1.6792e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 869 of None
Current timestep = 870. State = [[-0.08620988  0.19235493]]. Action = [[ 0.09657706  0.02842025  0.         -0.3785575 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 870 is [True, False, False, False, False, True]
State prediction error at timestep 870 is tensor(6.7634e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 870 of None
Current timestep = 871. State = [[-0.08323582  0.18976273]]. Action = [[ 0.04121806 -0.07608715  0.         -0.6798469 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 871 is [True, False, False, False, False, True]
State prediction error at timestep 871 is tensor(5.6377e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 871 of None
Current timestep = 872. State = [[-0.07828485  0.18863113]]. Action = [[ 0.09301568  0.02311287  0.         -0.8230147 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 872 is [True, False, False, False, False, True]
State prediction error at timestep 872 is tensor(4.6730e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 872 of None
Current timestep = 873. State = [[-0.07499794  0.18516277]]. Action = [[ 0.01930475 -0.07161572  0.         -0.78354394]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 873 is [True, False, False, False, False, True]
State prediction error at timestep 873 is tensor(6.2158e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 873 of None
Current timestep = 874. State = [[-0.07693833  0.18475679]]. Action = [[-0.05795069  0.04157702  0.         -0.72488606]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 874 is [True, False, False, False, False, True]
State prediction error at timestep 874 is tensor(3.4829e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 874 of None
Current timestep = 875. State = [[-0.07376758  0.18084338]]. Action = [[ 0.0899455  -0.09503918  0.         -0.6805855 ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 875 is [True, False, False, False, False, True]
State prediction error at timestep 875 is tensor(4.3390e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 875 of None
Current timestep = 876. State = [[-0.07398156  0.17333408]]. Action = [[-0.08141106 -0.08817455  0.         -0.5531831 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 876 is [True, False, False, False, False, True]
State prediction error at timestep 876 is tensor(7.6496e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 876 of None
Current timestep = 877. State = [[-0.07857022  0.17423879]]. Action = [[-0.06996746  0.08161021  0.         -0.85892665]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 877 is [True, False, False, False, False, True]
State prediction error at timestep 877 is tensor(2.6687e-07, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 877 of None
Current timestep = 878. State = [[-0.07636388  0.17182982]]. Action = [[ 0.08338077 -0.08491488  0.         -0.5486183 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 878 is [True, False, False, False, False, True]
State prediction error at timestep 878 is tensor(2.7141e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 878 of None
Current timestep = 879. State = [[-0.06963731  0.16667683]]. Action = [[ 0.08689015 -0.03187789  0.         -0.5567169 ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 879 is [True, False, False, False, False, True]
State prediction error at timestep 879 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 879 of None
Current timestep = 880. State = [[-0.0622835   0.16172567]]. Action = [[ 0.09005759 -0.04199257  0.          0.21169436]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 880 is [True, False, False, False, False, True]
State prediction error at timestep 880 is tensor(6.1150e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 880 of None
Current timestep = 881. State = [[-0.05513646  0.16048656]]. Action = [[ 0.08128626  0.04509116  0.         -0.48183787]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 881 is [True, False, False, False, False, True]
State prediction error at timestep 881 is tensor(9.2127e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 881 of None
Current timestep = 882. State = [[-0.05129732  0.15928876]]. Action = [[ 0.01784156 -0.00626888  0.         -0.6464832 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 882 is [True, False, False, False, False, True]
State prediction error at timestep 882 is tensor(7.7135e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 882 of None
Current timestep = 883. State = [[-0.04544553  0.15722056]]. Action = [[ 0.09152757 -0.0014891   0.         -0.1368342 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 883 is [False, True, False, False, False, True]
State prediction error at timestep 883 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 883 of None
Current timestep = 884. State = [[-0.03731196  0.15254843]]. Action = [[ 0.09536042 -0.05225141  0.          0.52518487]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 884 is [False, True, False, False, False, True]
State prediction error at timestep 884 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 884 of None
Current timestep = 885. State = [[-0.03131837  0.14989221]]. Action = [[ 0.04113264  0.01882945  0.         -0.90335095]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 885 is [False, True, False, False, False, True]
State prediction error at timestep 885 is tensor(5.1127e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 885 of None
Current timestep = 886. State = [[-0.03110533  0.1467367 ]]. Action = [[-0.05803394 -0.0425142   0.         -0.84024   ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 886 is [False, True, False, False, False, True]
State prediction error at timestep 886 is tensor(1.8044e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 886 of None
Current timestep = 887. State = [[-0.03420101  0.14342175]]. Action = [[-0.07415251 -0.02701037  0.         -0.6873187 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 887 is [False, True, False, False, False, True]
State prediction error at timestep 887 is tensor(3.1221e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 887 of None
Current timestep = 888. State = [[-0.0325626   0.14389652]]. Action = [[ 0.05142046  0.03761484  0.         -0.9643406 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 888 is [False, True, False, False, False, True]
State prediction error at timestep 888 is tensor(9.4241e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 888 of None
Current timestep = 889. State = [[-0.02988888  0.1427215 ]]. Action = [[ 1.7291307e-04 -3.8742136e-02  0.0000000e+00  3.0637443e-01]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 889 is [False, True, False, False, False, True]
State prediction error at timestep 889 is tensor(5.8241e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 889 of None
Current timestep = 890. State = [[-0.0243742   0.13650207]]. Action = [[ 0.07970456 -0.09739367  0.         -0.6277181 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 890 is [False, True, False, False, False, True]
State prediction error at timestep 890 is tensor(6.6380e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 890 of None
Current timestep = 891. State = [[-0.01798324  0.13223289]]. Action = [[ 0.05343916 -0.00995098  0.         -0.6912418 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 891 is [False, True, False, False, False, True]
State prediction error at timestep 891 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 891 of None
Current timestep = 892. State = [[-0.01021268  0.12732269]]. Action = [[ 0.09637154 -0.06407495  0.         -0.3368503 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 892 is [False, True, False, False, False, True]
State prediction error at timestep 892 is tensor(9.0483e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 892 of None
Current timestep = 893. State = [[-0.35961157  0.12521824]]. Action = [[ 0.08677707 -0.03397493  0.         -0.12736076]]. Reward = [100.]
Curr episode timestep = 333
Scene graph at timestep 893 is [True, False, False, False, False, True]
State prediction error at timestep 893 is tensor(0.0594, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 893 of None
Current timestep = 894. State = [[-0.36283404  0.1256434 ]]. Action = [[-0.04919813 -0.09688291  0.         -0.52008146]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 894 is [True, False, False, False, False, True]
State prediction error at timestep 894 is tensor(6.4863e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 894 of None
Current timestep = 895. State = [[-0.35971552  0.12227283]]. Action = [[ 0.09033526 -0.05415378  0.         -0.7808078 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 896. State = [[-0.35915488  0.11989598]]. Action = [[-0.05932966 -0.04600285  0.         -0.3601765 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 897. State = [[-0.3586689   0.12112746]]. Action = [[ 0.02152586  0.02463274  0.         -0.512477  ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 898. State = [[-0.3560947   0.12398622]]. Action = [[ 0.02585328  0.02614724  0.         -0.7618888 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 899. State = [[-0.35679895  0.12263398]]. Action = [[-0.04826049 -0.06019225  0.         -0.62658507]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 900. State = [[-0.3540604   0.11940464]]. Action = [[ 0.06243967 -0.04277776  0.         -0.74622726]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 901. State = [[-0.35010764  0.11946958]]. Action = [[ 0.02470702  0.02931387  0.         -0.8490958 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 902. State = [[-0.34975073  0.115549  ]]. Action = [[-0.03173082 -0.09302239  0.         -0.8194065 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 903. State = [[-0.35027686  0.11157005]]. Action = [[-0.01794145 -0.0246997   0.         -0.96572214]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 904. State = [[-0.35321367  0.10656044]]. Action = [[-0.07012568 -0.07884294  0.         -0.76188505]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 905. State = [[-0.35551965  0.09958321]]. Action = [[-0.02628541 -0.08701055  0.         -0.7699176 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 906. State = [[-0.35295498  0.09721673]]. Action = [[ 0.06008095  0.02615469  0.         -0.1736061 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 907. State = [[-0.3547671   0.09719221]]. Action = [[-0.07718369  0.01013073  0.         -0.28709215]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 908. State = [[-0.35585356  0.09909347]]. Action = [[ 0.02996496  0.05134962  0.         -0.51156276]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 909. State = [[-0.35868517  0.09564725]]. Action = [[-0.06172827 -0.08391473  0.         -0.26524407]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 910. State = [[-0.36565313  0.0943883 ]]. Action = [[-0.08977943  0.03508741  0.         -0.67101526]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 911. State = [[-0.3682798   0.09332757]]. Action = [[ 0.02852464 -0.02850483  0.         -0.33169246]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 912. State = [[-0.3691172   0.09176164]]. Action = [[ 0.          0.          0.         -0.55806446]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 913. State = [[-0.36684278  0.09374686]]. Action = [[ 0.08088965  0.06032831  0.         -0.5034304 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 914. State = [[-0.36586243  0.09354278]]. Action = [[ 0.0061457  -0.01951186  0.         -0.2474854 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 915. State = [[-0.36803183  0.09445977]]. Action = [[-0.02520682  0.04147715  0.         -0.58140236]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 916. State = [[-0.36987835  0.09527333]]. Action = [[ 0.        0.        0.       -0.810709]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 917. State = [[-0.36807328  0.09073929]]. Action = [[ 0.05280291 -0.0860868   0.         -0.04020178]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 918. State = [[-0.36674803  0.08775492]]. Action = [[ 0.         0.         0.        -0.3131243]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 919. State = [[-0.36675057  0.08293805]]. Action = [[-0.00539885 -0.08641053  0.         -0.19274044]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 920. State = [[-0.3688283   0.08339416]]. Action = [[-0.04635785  0.07061199  0.         -0.084005  ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 921. State = [[-0.3702456   0.08506633]]. Action = [[ 0.          0.          0.         -0.50360954]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 922. State = [[-0.3694211   0.08855956]]. Action = [[0.02573372 0.07320728 0.         0.19213271]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 923. State = [[-0.37007824  0.08899228]]. Action = [[-0.01742995 -0.03159185  0.         -0.7685298 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 924. State = [[-0.3658421   0.08505115]]. Action = [[ 0.09855596 -0.05979334  0.         -0.83836794]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 925. State = [[-0.35887083  0.08607166]]. Action = [[ 0.08489127  0.07069082  0.         -0.8627608 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 926. State = [[-0.3531763   0.08927587]]. Action = [[ 0.06230464  0.03760489  0.         -0.40125072]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 927. State = [[-0.34992182  0.08719316]]. Action = [[ 0.01541126 -0.0598431   0.         -0.30474436]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 928. State = [[-0.3497937   0.08332142]]. Action = [[-0.03415328 -0.04460625  0.         -0.7646643 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 929. State = [[-0.35211194  0.0823027 ]]. Action = [[-0.05831342  0.00335568  0.         -0.73924065]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 930. State = [[-0.34877905  0.07789635]]. Action = [[ 0.07643946 -0.0900886   0.         -0.12447423]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 931. State = [[-0.3425731   0.07105837]]. Action = [[ 0.05321293 -0.07351936  0.         -0.48149657]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 932. State = [[-0.33512372  0.06827069]]. Action = [[ 0.08549155  0.01011561  0.         -0.04074025]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 933. State = [[-0.3283045   0.06924281]]. Action = [[ 0.05888129  0.04208889  0.         -0.62088346]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 934. State = [[-0.32765892  0.06505718]]. Action = [[-0.0599026  -0.0891082   0.         -0.20879304]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 935. State = [[-0.32529888  0.06401501]]. Action = [[ 0.05721845  0.0504986   0.         -0.6464641 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 936. State = [[-0.3265371   0.05985891]]. Action = [[-0.088356   -0.09439993  0.         -0.88038653]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 937. State = [[-0.32780004  0.06074316]]. Action = [[0.0102791  0.08876581 0.         0.14798653]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 938. State = [[-0.3291056   0.06014748]]. Action = [[-0.03553292 -0.05121147  0.         -0.7996005 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 939. State = [[-0.32723138  0.05957368]]. Action = [[ 0.05520485  0.0248213   0.         -0.7346544 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 940. State = [[-0.3295538  0.0582086]]. Action = [[-0.08316762 -0.03405897  0.         -0.9321242 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 941. State = [[-0.32994765  0.06157809]]. Action = [[ 0.05076898  0.09204235  0.         -0.54443043]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 942. State = [[-0.3250235   0.06457149]]. Action = [[0.09057439 0.01138053 0.         0.18964612]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 943. State = [[-0.32335985  0.06079542]]. Action = [[-0.01520038 -0.08262627  0.          0.34866166]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 944. State = [[-0.32632637  0.06173537]]. Action = [[-0.0537979   0.06844508  0.         -0.82016677]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 945. State = [[-0.3271718   0.06316061]]. Action = [[ 0.02040648 -0.01298895  0.         -0.04247928]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 946. State = [[-0.32501537  0.06762671]]. Action = [[0.0437225  0.09253655 0.         0.00258529]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 947. State = [[-0.32672423  0.0695781 ]]. Action = [[-0.04998421 -0.02117235  0.         -0.66969717]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 948. State = [[-0.32547814  0.06548394]]. Action = [[ 0.05804486 -0.0817676   0.         -0.7461456 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 949. State = [[-0.32397947  0.06494946]]. Action = [[-0.00186069  0.03597655  0.         -0.21948433]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 950. State = [[-0.32235533  0.07068628]]. Action = [[ 0.03717411  0.09454099  0.         -0.43322694]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 951. State = [[-0.3262186   0.07758584]]. Action = [[-0.09149549  0.07513096  0.         -0.34380907]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 952. State = [[-0.33323798  0.07919914]]. Action = [[-0.07927436 -0.03286397  0.         -0.46028072]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 953. State = [[-0.33361354  0.07517973]]. Action = [[ 0.05035207 -0.08411525  0.         -0.1695137 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 954. State = [[-0.33089906  0.0732187 ]]. Action = [[ 0.03286447  0.00177801  0.         -0.6541052 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 955. State = [[-0.32697976  0.06969634]]. Action = [[ 0.05900491 -0.0713148   0.         -0.95116913]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 956. State = [[-0.32397005  0.0661999 ]]. Action = [[ 0.01657369 -0.0238563   0.         -0.83490705]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 957. State = [[-0.3180447   0.06495061]]. Action = [[ 0.09702315  0.00242756  0.         -0.8202655 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 958. State = [[-0.3122109   0.06191485]]. Action = [[ 0.04607698 -0.04342197  0.         -0.9248553 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 959. State = [[-0.31351984  0.05579716]]. Action = [[-0.08881769 -0.08275883  0.         -0.6062799 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 960. State = [[-0.31739452  0.04823302]]. Action = [[-0.06114979 -0.08906579  0.         -0.05353242]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 961. State = [[-0.31621057  0.04425659]]. Action = [[ 0.04114852 -0.00314986  0.         -0.12849629]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 962. State = [[-0.31757843  0.04098018]]. Action = [[-0.06831492 -0.03473366  0.         -0.4279927 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 963. State = [[-0.31484786  0.04265631]]. Action = [[ 0.09377553  0.08156719  0.         -0.555861  ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 964. State = [[-0.30817297  0.04378141]]. Action = [[ 0.08317118  0.00150204  0.         -0.6563642 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 965. State = [[-0.3045332  0.0415886]]. Action = [[ 0.01636114 -0.0252152   0.         -0.8434846 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 966. State = [[-0.30469492  0.03964442]]. Action = [[-0.02839112 -0.0072405   0.         -0.7478205 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 967. State = [[-0.30165437  0.03961536]]. Action = [[ 0.06727589  0.01830588  0.         -0.8950813 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 968. State = [[-0.30205393  0.03926252]]. Action = [[-0.05774189 -0.00738622  0.         -0.90749484]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 969. State = [[-0.30555892  0.03888967]]. Action = [[-5.0415695e-02 -1.8729270e-04  0.0000000e+00 -6.7321819e-01]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 970. State = [[-0.3056037  0.0349972]]. Action = [[ 0.02207574 -0.07570542  0.         -0.7181084 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 971. State = [[-0.303438    0.02891743]]. Action = [[ 0.0228977  -0.07002634  0.         -0.5922889 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 972. State = [[-0.29950103  0.0294272 ]]. Action = [[0.05874415 0.0680523  0.         0.53614366]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 973. State = [[-0.2946304   0.03082997]]. Action = [[0.06445774 0.00405017 0.         0.9343672 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 974. State = [[-0.28938538  0.02688988]]. Action = [[ 0.06229403 -0.06990939  0.         -0.4776755 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 975. State = [[-0.2821172   0.02666276]]. Action = [[ 0.09857631  0.05363446  0.         -0.74592084]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 976. State = [[-0.27719298  0.02720919]]. Action = [[ 0.02901449 -0.00128813  0.         -0.12039685]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 977. State = [[-0.27510983  0.02213828]]. Action = [[-1.9272417e-04 -8.9634769e-02  0.0000000e+00 -8.6208951e-01]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 978. State = [[-0.27017114  0.02148797]]. Action = [[ 0.07152278  0.05438051  0.         -0.4287095 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 979. State = [[-0.26218966  0.01826972]]. Action = [[ 0.09464515 -0.07814834  0.          0.28245795]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 980. State = [[-0.25358307  0.01514407]]. Action = [[ 8.4683530e-02  3.9438903e-04  0.0000000e+00 -9.4165111e-01]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 981. State = [[-0.24541317  0.01626487]]. Action = [[0.07972641 0.04546236 0.         0.46372974]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 982. State = [[-0.2426005   0.01324559]]. Action = [[-0.03241742 -0.0701464   0.         -0.8912333 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 983. State = [[-0.23948504  0.01379452]]. Action = [[ 0.03670309  0.06488102  0.         -0.4142176 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 984. State = [[-0.23752786  0.0146127 ]]. Action = [[-0.01605943 -0.01020771  0.         -0.32385367]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 985. State = [[-0.2345022   0.01374673]]. Action = [[ 0.03336138 -0.00868472  0.         -0.5279019 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 986. State = [[-0.22825404  0.01145613]]. Action = [[ 0.07409108 -0.03572518  0.         -0.37352926]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 987. State = [[-0.22335903  0.00682208]]. Action = [[ 0.01949205 -0.06433924  0.         -0.47936094]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 988. State = [[-0.22056866  0.00227248]]. Action = [[ 0.0022532  -0.04203531  0.         -0.6061785 ]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 989. State = [[-0.21404788 -0.00177461]]. Action = [[ 0.09137218 -0.03950295  0.         -0.09960759]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 990. State = [[-0.21341592 -0.00826017]]. Action = [[-0.08945784 -0.08642855  0.         -0.69208765]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 991. State = [[-0.21767269 -0.01624794]]. Action = [[-0.08112399 -0.08855259  0.         -0.5603301 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 992. State = [[-0.21580692 -0.01951694]]. Action = [[ 0.06058339  0.01483072  0.         -0.9762714 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 993. State = [[-0.21368013 -0.02077981]]. Action = [[-0.01145698 -0.00284038  0.         -0.52238595]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 994. State = [[-0.20869915 -0.01831032]]. Action = [[ 0.0938823   0.07590888  0.         -0.563863  ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 995. State = [[-0.2092375  -0.01758034]]. Action = [[-0.07541278 -0.01085065  0.         -0.8190508 ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 996. State = [[-0.20687966 -0.01474577]]. Action = [[ 0.0968607   0.07198738  0.         -0.67657614]]. Reward = [0.]
Curr episode timestep = 102
Current timestep = 997. State = [[-0.20007157 -0.00996082]]. Action = [[ 0.0971758   0.05969024  0.         -0.6084987 ]]. Reward = [0.]
Curr episode timestep = 103
Current timestep = 998. State = [[-0.19761938 -0.00613997]]. Action = [[-1.0124594e-04  3.9330699e-02  0.0000000e+00 -4.8499477e-01]]. Reward = [0.]
Curr episode timestep = 104
Current timestep = 999. State = [[-0.20166345 -0.00151428]]. Action = [[-0.08370622  0.05986673  0.          0.03087354]]. Reward = [0.]
Curr episode timestep = 105
Current timestep = 1000. State = [[-0.2040718   0.00402931]]. Action = [[ 0.00311176  0.05652881  0.         -0.8123309 ]]. Reward = [0.]
Curr episode timestep = 106
Current timestep = 1001. State = [[-0.20066218  0.00452095]]. Action = [[ 0.0797668  -0.04962879  0.         -0.5425595 ]]. Reward = [0.]
Curr episode timestep = 107
Current timestep = 1002. State = [[-0.2018024  -0.00035549]]. Action = [[-0.07591711 -0.09251747  0.         -0.25656635]]. Reward = [0.]
Curr episode timestep = 108
Current timestep = 1003. State = [[-1.9916795e-01  4.0886280e-05]]. Action = [[ 0.09846798  0.04859959  0.         -0.5865755 ]]. Reward = [0.]
Curr episode timestep = 109
Current timestep = 1004. State = [[-0.19965029  0.00120361]]. Action = [[-0.0774143  -0.01484503  0.         -0.55252564]]. Reward = [0.]
Curr episode timestep = 110
Current timestep = 1005. State = [[-0.20464772  0.00138304]]. Action = [[-0.0696655  -0.00266792  0.         -0.6826261 ]]. Reward = [0.]
Curr episode timestep = 111
Current timestep = 1006. State = [[-0.20255119 -0.00227361]]. Action = [[ 0.08491736 -0.08065341  0.         -0.80287415]]. Reward = [0.]
Curr episode timestep = 112
Current timestep = 1007. State = [[-0.20316951 -0.00570148]]. Action = [[-0.0771417  -0.02106602  0.         -0.82951915]]. Reward = [0.]
Curr episode timestep = 113
Current timestep = 1008. State = [[-0.20031644 -0.00755099]]. Action = [[ 0.096844   -0.01779308  0.         -0.35527253]]. Reward = [0.]
Curr episode timestep = 114
Current timestep = 1009. State = [[-0.20003903 -0.00373017]]. Action = [[-0.06146539  0.09733579  0.         -0.22035867]]. Reward = [0.]
Curr episode timestep = 115
Current timestep = 1010. State = [[-0.20171356  0.00275537]]. Action = [[-3.2232702e-04  7.6055415e-02  0.0000000e+00  7.7161586e-01]]. Reward = [0.]
Curr episode timestep = 116
Current timestep = 1011. State = [[-0.20176587  0.00927409]]. Action = [[ 0.012527    0.07714001  0.         -0.4980613 ]]. Reward = [0.]
Curr episode timestep = 117
Current timestep = 1012. State = [[-0.20655967  0.01378662]]. Action = [[-0.09180093  0.02875347  0.         -0.3568691 ]]. Reward = [0.]
Curr episode timestep = 118
Current timestep = 1013. State = [[-0.20847797  0.01440228]]. Action = [[ 0.02790499 -0.02741133  0.          0.10581577]]. Reward = [0.]
Curr episode timestep = 119
Current timestep = 1014. State = [[-0.20663147  0.0130737 ]]. Action = [[ 0.03778303 -0.02951268  0.         -0.79801005]]. Reward = [0.]
Curr episode timestep = 120
Current timestep = 1015. State = [[-0.20959617  0.01491693]]. Action = [[-0.07556798  0.04043985  0.         -0.88781434]]. Reward = [0.]
Curr episode timestep = 121
Current timestep = 1016. State = [[-0.21139583  0.01960458]]. Action = [[ 0.01840051  0.0546638   0.         -0.88968146]]. Reward = [0.]
Curr episode timestep = 122
Current timestep = 1017. State = [[-0.21133609  0.02211538]]. Action = [[ 1.0093428e-02  1.4446676e-05  0.0000000e+00 -9.4446915e-01]]. Reward = [0.]
Curr episode timestep = 123
Current timestep = 1018. State = [[-0.20932128  0.02242159]]. Action = [[ 0.04918528 -0.01060353  0.         -0.14672858]]. Reward = [0.]
Curr episode timestep = 124
Current timestep = 1019. State = [[-0.20745352  0.0188727 ]]. Action = [[ 0.01688757 -0.07501589  0.         -0.96562403]]. Reward = [0.]
Curr episode timestep = 125
Current timestep = 1020. State = [[-0.20770952  0.01885619]]. Action = [[-0.01739235  0.03956144  0.         -0.993633  ]]. Reward = [0.]
Curr episode timestep = 126
Current timestep = 1021. State = [[-0.21180709  0.02277834]]. Action = [[-0.07514013  0.05263465  0.         -0.42405164]]. Reward = [0.]
Curr episode timestep = 127
Current timestep = 1022. State = [[-0.21480776  0.02937668]]. Action = [[-0.01205093  0.09008823  0.         -0.6928792 ]]. Reward = [0.]
Curr episode timestep = 128
Current timestep = 1023. State = [[-0.21459414  0.02862058]]. Action = [[ 0.02486894 -0.084679    0.         -0.9648218 ]]. Reward = [0.]
Curr episode timestep = 129
Current timestep = 1024. State = [[-0.21247928  0.02242729]]. Action = [[ 0.03139157 -0.08610029  0.          0.0367378 ]]. Reward = [0.]
Curr episode timestep = 130
Current timestep = 1025. State = [[-0.2100876   0.02168987]]. Action = [[ 0.02760435  0.03886063  0.         -0.8458462 ]]. Reward = [0.]
Curr episode timestep = 131
Current timestep = 1026. State = [[-0.20971395  0.0225429 ]]. Action = [[-0.01029492 -0.00046685  0.          0.22646034]]. Reward = [0.]
Curr episode timestep = 132
Current timestep = 1027. State = [[-0.20786321  0.02256175]]. Action = [[ 0.04074716  0.00255428  0.         -0.81449544]]. Reward = [0.]
Curr episode timestep = 133
Current timestep = 1028. State = [[-0.20422967  0.02401559]]. Action = [[ 0.05008497  0.03272452  0.         -0.66640675]]. Reward = [0.]
Curr episode timestep = 134
Current timestep = 1029. State = [[-0.20327435  0.02918484]]. Action = [[-0.00929143  0.08791258  0.         -0.92868966]]. Reward = [0.]
Curr episode timestep = 135
Current timestep = 1030. State = [[-0.19988236  0.02898883]]. Action = [[ 0.07460276 -0.05798176  0.          0.2374897 ]]. Reward = [0.]
Curr episode timestep = 136
Current timestep = 1031. State = [[-0.20132568  0.02631553]]. Action = [[-0.08641271 -0.02449789  0.         -0.20948422]]. Reward = [0.]
Curr episode timestep = 137
Current timestep = 1032. State = [[-0.20166984  0.02750968]]. Action = [[ 0.03565184  0.03718138  0.         -0.32992357]]. Reward = [0.]
Curr episode timestep = 138
Current timestep = 1033. State = [[-0.20328887  0.02527237]]. Action = [[-0.05625236 -0.06920069  0.         -0.951028  ]]. Reward = [0.]
Curr episode timestep = 139
Current timestep = 1034. State = [[-0.20282605  0.02608587]]. Action = [[ 0.03390288  0.05360038  0.         -0.20109427]]. Reward = [0.]
Curr episode timestep = 140
Current timestep = 1035. State = [[-0.19790074  0.02541247]]. Action = [[ 0.08171351 -0.04262133  0.         -0.9346143 ]]. Reward = [0.]
Curr episode timestep = 141
Current timestep = 1036. State = [[-0.19847369  0.02542627]]. Action = [[-0.0694674   0.02638302  0.         -0.88114494]]. Reward = [0.]
Curr episode timestep = 142
Current timestep = 1037. State = [[-0.19697115  0.02125704]]. Action = [[ 0.06484292 -0.09639752  0.         -0.8775563 ]]. Reward = [0.]
Curr episode timestep = 143
Current timestep = 1038. State = [[-0.19381316  0.01781166]]. Action = [[ 0.02020843 -0.00468725  0.         -0.5230809 ]]. Reward = [0.]
Curr episode timestep = 144
Current timestep = 1039. State = [[-0.18779081  0.01394468]]. Action = [[ 0.09901661 -0.05739978  0.          0.746495  ]]. Reward = [0.]
Curr episode timestep = 145
Current timestep = 1040. State = [[-0.18086082  0.0101819 ]]. Action = [[ 0.07001222 -0.01961595  0.         -0.12272882]]. Reward = [0.]
Curr episode timestep = 146
Current timestep = 1041. State = [[-0.17541495  0.00619174]]. Action = [[ 0.04765309 -0.04061538  0.         -0.4140749 ]]. Reward = [0.]
Curr episode timestep = 147
Current timestep = 1042. State = [[-0.17527297  0.00792271]]. Action = [[-0.05195519  0.08584433  0.         -0.19321483]]. Reward = [0.]
Curr episode timestep = 148
Current timestep = 1043. State = [[-0.17160621  0.00577822]]. Action = [[ 0.08441661 -0.07133325  0.         -0.6238518 ]]. Reward = [0.]
Curr episode timestep = 149
Current timestep = 1044. State = [[-0.16898897  0.00082235]]. Action = [[-0.02080731 -0.03867009  0.         -0.6749424 ]]. Reward = [0.]
Curr episode timestep = 150
Current timestep = 1045. State = [[-0.16671129 -0.00057094]]. Action = [[ 0.02701978  0.01741117  0.         -0.8221398 ]]. Reward = [0.]
Curr episode timestep = 151
Current timestep = 1046. State = [[-0.16509135  0.00276451]]. Action = [[-0.00401913  0.07449039  0.         -0.65999866]]. Reward = [0.]
Curr episode timestep = 152
Current timestep = 1047. State = [[-0.1685535   0.00300159]]. Action = [[-0.09018426 -0.03285874  0.         -0.8346224 ]]. Reward = [0.]
Curr episode timestep = 153
Current timestep = 1048. State = [[-0.16977409  0.00432037]]. Action = [[ 0.00756557  0.04343925  0.         -0.5293486 ]]. Reward = [0.]
Curr episode timestep = 154
Current timestep = 1049. State = [[-0.17389312  0.01033581]]. Action = [[-0.094379    0.08961067  0.         -0.27603066]]. Reward = [0.]
Curr episode timestep = 155
Current timestep = 1050. State = [[-0.17391834  0.00927745]]. Action = [[ 0.05607773 -0.09040932  0.         -0.52466357]]. Reward = [0.]
Curr episode timestep = 156
Current timestep = 1051. State = [[-0.16804361  0.00235819]]. Action = [[ 0.08743662 -0.09595138  0.         -0.3389864 ]]. Reward = [0.]
Curr episode timestep = 157
Current timestep = 1052. State = [[-0.16210376 -0.00208529]]. Action = [[ 0.06084826 -0.02511448  0.         -0.9062456 ]]. Reward = [0.]
Curr episode timestep = 158
Current timestep = 1053. State = [[-0.15613605 -0.00597957]]. Action = [[ 0.07035432 -0.04753481  0.         -0.9562265 ]]. Reward = [0.]
Curr episode timestep = 159
Current timestep = 1054. State = [[-0.15105408 -0.00437387]]. Action = [[ 0.0459343   0.07942397  0.         -0.80884016]]. Reward = [0.]
Curr episode timestep = 160
Current timestep = 1055. State = [[-0.14801475 -0.0014645 ]]. Action = [[ 0.02135648  0.0291164   0.         -0.6586402 ]]. Reward = [0.]
Curr episode timestep = 161
Current timestep = 1056. State = [[-0.15078431  0.00058345]]. Action = [[-0.08828454  0.03034673  0.         -0.7190448 ]]. Reward = [0.]
Curr episode timestep = 162
Current timestep = 1057. State = [[-0.14911625 -0.00091674]]. Action = [[ 0.07286107 -0.04716719  0.         -0.22618353]]. Reward = [0.]
Curr episode timestep = 163
Current timestep = 1058. State = [[-0.14768898 -0.00295768]]. Action = [[-0.03140769 -0.01087823  0.         -0.6827358 ]]. Reward = [0.]
Curr episode timestep = 164
Current timestep = 1059. State = [[-0.14445795 -0.00254981]]. Action = [[ 0.06128957  0.01846888  0.         -0.7863393 ]]. Reward = [0.]
Curr episode timestep = 165
Current timestep = 1060. State = [[-0.13761027 -0.00212922]]. Action = [[ 0.09067164  0.00150349  0.         -0.8332004 ]]. Reward = [0.]
Curr episode timestep = 166
Current timestep = 1061. State = [[-0.13624448 -0.00607687]]. Action = [[-0.04981264 -0.07607001  0.         -0.7010664 ]]. Reward = [0.]
Curr episode timestep = 167
Current timestep = 1062. State = [[-0.13247608 -0.00923   ]]. Action = [[ 0.08289266 -0.01185222  0.         -0.96247673]]. Reward = [0.]
Curr episode timestep = 168
Current timestep = 1063. State = [[-0.13022512 -0.00583438]]. Action = [[-0.02002226  0.08714259  0.         -0.15855765]]. Reward = [0.]
Curr episode timestep = 169
Current timestep = 1064. State = [[-0.12630302 -0.00087314]]. Action = [[ 0.07644362  0.05189274  0.         -0.46465337]]. Reward = [0.]
Curr episode timestep = 170
Current timestep = 1065. State = [[-0.12259898  0.00514293]]. Action = [[ 0.02398541  0.08510242  0.         -0.32022572]]. Reward = [0.]
Curr episode timestep = 171
Current timestep = 1066. State = [[-0.11960986  0.01187802]]. Action = [[ 0.03987142  0.07288768  0.         -0.9599286 ]]. Reward = [0.]
Curr episode timestep = 172
Current timestep = 1067. State = [[-0.11434235  0.01346169]]. Action = [[ 0.07879054 -0.03004064  0.          0.24348283]]. Reward = [0.]
Curr episode timestep = 173
Current timestep = 1068. State = [[-0.10953374  0.01774375]]. Action = [[ 0.04277781  0.08463905  0.         -0.5812739 ]]. Reward = [0.]
Curr episode timestep = 174
Current timestep = 1069. State = [[-0.10504428  0.01872171]]. Action = [[ 0.05283756 -0.04867608  0.         -0.8626641 ]]. Reward = [0.]
Curr episode timestep = 175
Current timestep = 1070. State = [[-0.09836608  0.023141  ]]. Action = [[ 0.08740757  0.0970925   0.         -0.56880903]]. Reward = [0.]
Curr episode timestep = 176
Current timestep = 1071. State = [[-0.09089419  0.02738385]]. Action = [[ 0.08547733  0.01236516  0.         -0.51627517]]. Reward = [0.]
Curr episode timestep = 177
Current timestep = 1072. State = [[-0.08577777  0.03250585]]. Action = [[ 0.03160063  0.07442959  0.         -0.84847176]]. Reward = [0.]
Curr episode timestep = 178
Current timestep = 1073. State = [[-0.08641022  0.04025556]]. Action = [[-0.06166061  0.08559192  0.          0.50440514]]. Reward = [0.]
Curr episode timestep = 179
Current timestep = 1074. State = [[-0.08521073  0.04574927]]. Action = [[0.03067297 0.02125577 0.         0.84602714]]. Reward = [0.]
Curr episode timestep = 180
Current timestep = 1075. State = [[-0.07935955  0.04853155]]. Action = [[0.0784168  0.00386988 0.         0.28970933]]. Reward = [0.]
Curr episode timestep = 181
Current timestep = 1076. State = [[-0.07124932  0.05380926]]. Action = [[ 0.09707344  0.07161606  0.         -0.32552528]]. Reward = [0.]
Curr episode timestep = 182
Current timestep = 1077. State = [[-0.07069255  0.0591893 ]]. Action = [[-0.08261661  0.03197617  0.         -0.7667238 ]]. Reward = [0.]
Curr episode timestep = 183
Current timestep = 1078. State = [[-0.07109144  0.0586138 ]]. Action = [[ 0.01072937 -0.07146005  0.         -0.7494626 ]]. Reward = [0.]
Curr episode timestep = 184
Current timestep = 1079. State = [[-0.06820111  0.05905127]]. Action = [[ 0.02997143  0.01843339  0.         -0.8852254 ]]. Reward = [0.]
Curr episode timestep = 185
Current timestep = 1080. State = [[-0.0618531  0.0579886]]. Action = [[ 0.08925193 -0.05101476  0.         -0.7050847 ]]. Reward = [0.]
Curr episode timestep = 186
Current timestep = 1081. State = [[-0.05709478  0.05962659]]. Action = [[ 0.02058289  0.05214689  0.         -0.9467949 ]]. Reward = [0.]
Curr episode timestep = 187
Current timestep = 1082. State = [[-0.05085239  0.05687683]]. Action = [[ 0.08623181 -0.09053874  0.         -0.94552726]]. Reward = [0.]
Curr episode timestep = 188
Current timestep = 1083. State = [[-0.05021794  0.04980016]]. Action = [[-0.08556663 -0.09085734  0.         -0.959098  ]]. Reward = [0.]
Curr episode timestep = 189
Current timestep = 1084. State = [[-0.0529502  0.0445649]]. Action = [[-0.05459964 -0.04558533  0.         -0.9268323 ]]. Reward = [0.]
Curr episode timestep = 190
Current timestep = 1085. State = [[-0.05383711  0.04471333]]. Action = [[-0.01935808  0.04078617  0.         -0.9017685 ]]. Reward = [0.]
Curr episode timestep = 191
Current timestep = 1086. State = [[-0.3747022   0.03554082]]. Action = [[ 0.07563784 -0.0730833   0.         -0.61109304]]. Reward = [100.]
Curr episode timestep = 192
Current timestep = 1087. State = [[-0.37087902  0.03402135]]. Action = [[ 0.08910961  0.02881873  0.         -0.68344516]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1088. State = [[-0.3684854   0.03467282]]. Action = [[0.         0.         0.         0.04794145]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1089. State = [[-0.3697357   0.03619526]]. Action = [[-0.03220014  0.02943834  0.         -0.49437344]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1090. State = [[-0.36617276  0.03462283]]. Action = [[ 0.08691979 -0.0487153   0.         -0.04458427]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1091. State = [[-0.3591909   0.02922031]]. Action = [[ 0.07883718 -0.07230935  0.         -0.8888983 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1092. State = [[-0.35119832  0.0296575 ]]. Action = [[0.09556405 0.06934319 0.         0.8869097 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1093. State = [[-0.34325016  0.03569577]]. Action = [[ 0.0902512   0.09471752  0.         -0.811815  ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1094. State = [[-0.33681437  0.03644015]]. Action = [[ 0.05948975 -0.03592917  0.         -0.25400484]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1095. State = [[-0.33764127  0.03809233]]. Action = [[-0.08839098  0.0541159   0.          0.10302997]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1096. State = [[-0.34244767  0.03595546]]. Action = [[-0.08081408 -0.07956354  0.         -0.50393885]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1097. State = [[-0.34559274  0.03261302]]. Action = [[-0.04656666 -0.02841239  0.         -0.22879553]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1098. State = [[-0.3451111   0.03018549]]. Action = [[ 0.01621599 -0.03203875  0.         -0.7060454 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1099. State = [[-0.33936596  0.03134026]]. Action = [[0.09872717 0.0459987  0.         0.3318318 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1100. State = [[-0.3391933   0.03108365]]. Action = [[-0.07130437 -0.02968892  0.         -0.9604736 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1101. State = [[-0.33684313  0.02560578]]. Action = [[ 0.08395705 -0.08875024  0.          0.11105192]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1102. State = [[-0.33293617  0.02028099]]. Action = [[ 0.02285844 -0.03938498  0.         -0.7051062 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1103. State = [[-0.33007053  0.02156916]]. Action = [[ 0.03178889  0.069754    0.         -0.8932403 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1104. State = [[-0.33239168  0.02260711]]. Action = [[-0.07554869 -0.00598414  0.         -0.25971836]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1105. State = [[-0.33065808  0.02618777]]. Action = [[0.08435892 0.08133467 0.         0.4032588 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1106. State = [[-0.32433942  0.02506302]]. Action = [[ 0.08132977 -0.06640139  0.         -0.56001556]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1107. State = [[-0.3166447   0.02027558]]. Action = [[ 0.09387054 -0.04987401  0.          0.2755742 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1108. State = [[-0.30996403  0.02070199]]. Action = [[ 0.05888388  0.05297904  0.         -0.7828734 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1109. State = [[-0.30609626  0.01760798]]. Action = [[ 0.01659028 -0.08085319  0.         -0.5047035 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1110. State = [[-0.30728754  0.01790421]]. Action = [[-0.07118613  0.06270465  0.         -0.9589215 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1111. State = [[-0.30648386  0.01559874]]. Action = [[ 0.02522068 -0.07492156  0.         -0.5496197 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1112. State = [[-0.30665     0.01400463]]. Action = [[-0.04586598  0.01753022  0.         -0.78670305]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1113. State = [[-0.30595183  0.01438038]]. Action = [[0.01562619 0.00448392 0.         0.76071346]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1114. State = [[-0.30110338  0.0126952 ]]. Action = [[ 0.07391647 -0.03241181  0.         -0.7879227 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1115. State = [[-0.2945057   0.01053746]]. Action = [[ 0.07583993 -0.01611968  0.         -0.36858445]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1116. State = [[-0.29193923  0.0113668 ]]. Action = [[-0.01158077  0.03640223  0.         -0.32789356]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1117. State = [[-0.28717044  0.01393685]]. Action = [[ 0.08782605  0.03654882  0.         -0.00750595]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1118. State = [[-0.28651044  0.01209773]]. Action = [[-0.05765907 -0.05607728  0.         -0.5744134 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1119. State = [[-0.2881706   0.01410687]]. Action = [[-0.01774071  0.07434813  0.         -0.80766886]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1120. State = [[-0.2886139  0.0196302]]. Action = [[ 0.00086989  0.06414334  0.         -0.7662682 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1121. State = [[-0.28475598  0.02529316]]. Action = [[ 0.08691002  0.06208291  0.         -0.9037698 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1122. State = [[-0.28164372  0.02507201]]. Action = [[ 0.01986752 -0.05667514  0.         -0.58594334]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1123. State = [[-0.2815876   0.02794269]]. Action = [[-0.01608646  0.07576939  0.         -0.5006377 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1124. State = [[-0.27903008  0.0267092 ]]. Action = [[ 0.05644324 -0.08367697  0.         -0.83658326]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1125. State = [[-0.27411962  0.02312866]]. Action = [[ 0.05366426 -0.0344996   0.          0.3115319 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1126. State = [[-0.26769423  0.02092724]]. Action = [[ 0.07863016 -0.02348253  0.          0.14831376]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1127. State = [[-0.26414508  0.01988674]]. Action = [[ 1.7317384e-04 -2.9581040e-03  0.0000000e+00 -2.6046658e-01]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1128. State = [[-0.25941795  0.01676714]]. Action = [[ 0.06356425 -0.05408769  0.         -0.58902663]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1129. State = [[-0.25719583  0.01341482]]. Action = [[-0.02688074 -0.02457161  0.         -0.39777607]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1130. State = [[-0.25821123  0.01102542]]. Action = [[-0.04593391 -0.02000119  0.          0.04995561]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1131. State = [[-0.25802127  0.00983454]]. Action = [[-0.00261077  0.00089376  0.         -0.8570648 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1132. State = [[-0.25981617  0.01193373]]. Action = [[-0.05867123  0.05259193  0.          0.7396798 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1133. State = [[-0.26391003  0.01412975]]. Action = [[-0.06581208  0.01616333  0.          0.60150635]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1134. State = [[-0.26723593  0.01154056]]. Action = [[-0.03838699 -0.06453848  0.         -0.67595947]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1135. State = [[-0.27208194  0.01073165]]. Action = [[-0.0805193   0.02083616  0.          0.09957933]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1136. State = [[-0.27667445  0.01038487]]. Action = [[-0.04222016 -0.01979031  0.          0.03145373]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1137. State = [[-0.2775448   0.00630024]]. Action = [[ 0.01996504 -0.0711813   0.         -0.7554277 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1138. State = [[-0.27651972  0.00085062]]. Action = [[ 0.02414536 -0.0589155   0.         -0.50254405]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1139. State = [[-0.27421483 -0.0055262 ]]. Action = [[ 0.0435122  -0.07523195  0.          0.24283957]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1140. State = [[-0.2773341  -0.01141834]]. Action = [[-0.08805363 -0.05056372  0.         -0.4246825 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1141. State = [[-0.28235713 -0.01212478]]. Action = [[-0.04277052  0.04136138  0.         -0.27071798]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1142. State = [[-0.28168175 -0.01508594]]. Action = [[ 0.06370931 -0.05910523  0.         -0.95407534]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1143. State = [[-0.2776003  -0.01699718]]. Action = [[ 0.06930771  0.01999028  0.         -0.42230105]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1144. State = [[-0.27478206 -0.01613252]]. Action = [[ 0.0364948   0.03052584  0.         -0.31995046]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1145. State = [[-0.26982632 -0.01795522]]. Action = [[ 0.09689366 -0.03510553  0.         -0.14934069]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1146. State = [[-0.26333985 -0.01689441]]. Action = [[ 0.09018355  0.06008706  0.         -0.32156652]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1147. State = [[-0.25896037 -0.01626884]]. Action = [[ 0.04438379 -0.0058025   0.         -0.5418668 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1148. State = [[-0.25475067 -0.01557143]]. Action = [[ 0.05843166  0.02881039  0.         -0.77147776]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1149. State = [[-0.25601026 -0.01039044]]. Action = [[-0.06719856  0.09600688  0.         -0.8402411 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1150. State = [[-0.25686297 -0.00756658]]. Action = [[ 0.01894311 -0.0025795   0.         -0.6559367 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1151. State = [[-0.25607833 -0.0073066 ]]. Action = [[ 0.00442694 -0.00558531  0.         -0.84849393]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1152. State = [[-0.25753015 -0.00409545]]. Action = [[-0.03883766  0.05492706  0.         -0.54814094]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1153. State = [[-0.25392768 -0.00481705]]. Action = [[ 0.09196497 -0.06313781  0.         -0.34510136]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1154. State = [[-0.25404173 -0.00429477]]. Action = [[-0.0769055   0.03255453  0.          0.13536763]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1155. State = [[-0.25895724 -0.00551584]]. Action = [[-0.07474687 -0.05625856  0.         -0.77141494]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1156. State = [[-0.261224   -0.00357472]]. Action = [[-0.01308867  0.05882419  0.         -0.51677096]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1157. State = [[-0.26030037  0.00184001]]. Action = [[ 0.02998627  0.06139753  0.         -0.34099978]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1158. State = [[-0.25625223  0.00521834]]. Action = [[ 0.07586005  0.01344679  0.         -0.8726202 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1159. State = [[-0.25149012  0.00524448]]. Action = [[ 0.06062063 -0.02290324  0.         -0.82657456]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1160. State = [[-0.25318903  0.00673225]]. Action = [[-0.07747623  0.03238245  0.         -0.7747897 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1161. State = [[-0.2529031   0.01147497]]. Action = [[ 0.05542005  0.06455535  0.         -0.36064774]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1162. State = [[-0.25051403  0.0185608 ]]. Action = [[ 0.03047638  0.08843292  0.         -0.9360397 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1163. State = [[-0.25090098  0.02528747]]. Action = [[-0.01389696  0.06156627  0.          0.11748302]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1164. State = [[-0.2482098   0.02516926]]. Action = [[ 0.07040086 -0.06464605  0.         -0.63373053]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1165. State = [[-0.24151897  0.02110878]]. Action = [[ 0.09343529 -0.06346319  0.          0.02222621]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1166. State = [[-0.2339849  0.0166514]]. Action = [[ 0.0811064  -0.05870501  0.          0.120368  ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1167. State = [[-0.2322506   0.01366238]]. Action = [[-0.0467254  -0.02507788  0.         -0.5717343 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1168. State = [[-0.23197158  0.01529019]]. Action = [[-0.00111895  0.05059781  0.          0.26354015]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1169. State = [[-0.22905786  0.01670397]]. Action = [[ 0.03819861  0.00168631  0.         -0.29717398]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1170. State = [[-0.22775538  0.01456843]]. Action = [[-0.01971319 -0.0428785   0.         -0.05045724]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1171. State = [[-0.22778808  0.00909262]]. Action = [[-0.02029649 -0.08040389  0.         -0.16595447]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1172. State = [[-0.22403572  0.00630597]]. Action = [[ 0.0597486   0.00330622  0.         -0.20601773]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1173. State = [[-0.22163497  0.00351958]]. Action = [[-0.01020242 -0.04040185  0.         -0.5525931 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1174. State = [[-0.21758181  0.00630959]]. Action = [[ 0.06607435  0.09567698  0.         -0.32397997]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1175. State = [[-0.21295938  0.00572762]]. Action = [[ 0.04382259 -0.05433892  0.         -0.73884434]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1176. State = [[-0.21492594  0.00100873]]. Action = [[-0.0927959  -0.05418462  0.         -0.8640511 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1177. State = [[-0.2171305  -0.00332029]]. Action = [[-0.01456203 -0.04209083  0.         -0.68212354]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1178. State = [[-0.2215853 -0.0043048]]. Action = [[-0.09557274  0.0187846   0.         -0.72022516]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 1179. State = [[-0.2226695  -0.00361072]]. Action = [[ 0.02875318  0.01289006  0.         -0.8118487 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1180. State = [[-0.22522393 -0.00202821]]. Action = [[-0.06584819  0.02943713  0.         -0.92067325]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1181. State = [[-0.2241608  -0.00450765]]. Action = [[ 0.06630649 -0.06481365  0.         -0.93636984]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1182. State = [[-0.22660932 -0.00242712]]. Action = [[-0.08503487  0.08414526  0.         -0.94453776]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1183. State = [[-0.23091716 -0.00460481]]. Action = [[-0.02918439 -0.0925874   0.         -0.45331103]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 1184. State = [[-0.23436426 -0.00488896]]. Action = [[-0.03814995  0.04780359  0.          0.3639393 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1185. State = [[-0.23492731  0.0004795 ]]. Action = [[ 0.03463357  0.08099323  0.         -0.9718365 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1186. State = [[-0.23300737  0.00379204]]. Action = [[ 0.05291135  0.0113956   0.         -0.85792106]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 1187. State = [[-0.23464029  0.00530633]]. Action = [[-0.03681906  0.01418366  0.         -0.16761261]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 1188. State = [[-0.23284985  0.01061147]]. Action = [[ 0.08614359  0.08887989  0.         -0.52068114]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 1189. State = [[-0.23615624  0.01595402]]. Action = [[-0.09578051  0.04182693  0.         -0.85026926]]. Reward = [0.]
Curr episode timestep = 102
Current timestep = 1190. State = [[-0.24146302  0.0184246 ]]. Action = [[-0.02393023  0.00332552  0.         -0.8950243 ]]. Reward = [0.]
Curr episode timestep = 103
Current timestep = 1191. State = [[-0.24519213  0.02324653]]. Action = [[-0.0255459   0.07175326  0.         -0.75699145]]. Reward = [0.]
Curr episode timestep = 104
Current timestep = 1192. State = [[-0.25167033  0.0239497 ]]. Action = [[-0.0861593 -0.0532986  0.        -0.6831808]]. Reward = [0.]
Curr episode timestep = 105
Current timestep = 1193. State = [[-0.25354305  0.02726617]]. Action = [[0.040888   0.07148375 0.         0.40601456]]. Reward = [0.]
Curr episode timestep = 106
Current timestep = 1194. State = [[-0.25486222  0.03337839]]. Action = [[-0.01865431  0.05871978  0.          0.17459977]]. Reward = [0.]
Curr episode timestep = 107
Current timestep = 1195. State = [[-0.25201005  0.03532643]]. Action = [[ 0.09585542 -0.02074699  0.          0.4108039 ]]. Reward = [0.]
Curr episode timestep = 108
Current timestep = 1196. State = [[-0.25239584  0.03152841]]. Action = [[-0.05316141 -0.08517747  0.          0.7492341 ]]. Reward = [0.]
Curr episode timestep = 109
Current timestep = 1197. State = [[-0.25528157  0.02897001]]. Action = [[-0.02500962 -0.01528676  0.         -0.7717673 ]]. Reward = [0.]
Curr episode timestep = 110
Current timestep = 1198. State = [[-0.25205252  0.0263547 ]]. Action = [[ 0.08843801 -0.04655284  0.          0.14331818]]. Reward = [0.]
Curr episode timestep = 111
Current timestep = 1199. State = [[-0.24992476  0.0211336 ]]. Action = [[-0.01048853 -0.07308249  0.         -0.28181696]]. Reward = [0.]
Curr episode timestep = 112
Current timestep = 1200. State = [[-0.24536839  0.02109133]]. Action = [[ 0.08892832  0.0553736   0.         -0.53466123]]. Reward = [0.]
Curr episode timestep = 113
Current timestep = 1201. State = [[-0.24256153  0.02652197]]. Action = [[ 0.00286286  0.0937063   0.         -0.73882496]]. Reward = [0.]
Curr episode timestep = 114
Current timestep = 1202. State = [[-0.24483885  0.02971417]]. Action = [[-0.05387378  0.01517753  0.         -0.9205297 ]]. Reward = [0.]
Curr episode timestep = 115
Current timestep = 1203. State = [[-0.24494913  0.02804341]]. Action = [[ 0.02175643 -0.04416047  0.         -0.46015733]]. Reward = [0.]
Curr episode timestep = 116
Current timestep = 1204. State = [[-0.2471415   0.02322462]]. Action = [[-0.06789023 -0.07001761  0.         -0.98214906]]. Reward = [0.]
Curr episode timestep = 117
Current timestep = 1205. State = [[-0.24779418  0.02566164]]. Action = [[ 0.01516593  0.09421716  0.         -0.5068779 ]]. Reward = [0.]
Curr episode timestep = 118
Current timestep = 1206. State = [[-0.24715897  0.02987886]]. Action = [[ 0.00866552  0.02940632  0.         -0.81114304]]. Reward = [0.]
Curr episode timestep = 119
Current timestep = 1207. State = [[-0.24688523  0.03018851]]. Action = [[ 0.00327881 -0.01957802  0.         -0.84666556]]. Reward = [0.]
Curr episode timestep = 120
Current timestep = 1208. State = [[-0.24638169  0.03330952]]. Action = [[ 0.01144729  0.06719071  0.         -0.56256473]]. Reward = [0.]
Curr episode timestep = 121
Current timestep = 1209. State = [[-0.24230912  0.0326697 ]]. Action = [[ 0.08293576 -0.05835613  0.         -0.9375634 ]]. Reward = [0.]
Curr episode timestep = 122
Current timestep = 1210. State = [[-0.24349421  0.03583313]]. Action = [[-0.07432809  0.09097365  0.         -0.8257165 ]]. Reward = [0.]
Curr episode timestep = 123
Current timestep = 1211. State = [[-0.24369366  0.03471739]]. Action = [[ 0.04112881 -0.08439684  0.         -0.64867663]]. Reward = [0.]
Curr episode timestep = 124
Current timestep = 1212. State = [[-0.23922145  0.03046979]]. Action = [[ 0.06862838 -0.0422456   0.         -0.80066454]]. Reward = [0.]
Curr episode timestep = 125
Current timestep = 1213. State = [[-0.23256883  0.02556735]]. Action = [[ 0.08741564 -0.06569484  0.         -0.8158995 ]]. Reward = [0.]
Curr episode timestep = 126
Current timestep = 1214. State = [[-0.22802614  0.02193541]]. Action = [[ 0.02321035 -0.02017921  0.         -0.5056188 ]]. Reward = [0.]
Curr episode timestep = 127
Current timestep = 1215. State = [[-0.2220207   0.02411875]]. Action = [[ 0.08783164  0.07502276  0.         -0.34339416]]. Reward = [0.]
Curr episode timestep = 128
Current timestep = 1216. State = [[-0.21696173  0.02125163]]. Action = [[ 0.03258836 -0.08311687  0.         -0.04897535]]. Reward = [0.]
Curr episode timestep = 129
Current timestep = 1217. State = [[-0.21847601  0.01594151]]. Action = [[-0.08848037 -0.04338192  0.         -0.71661997]]. Reward = [0.]
Curr episode timestep = 130
Current timestep = 1218. State = [[-0.219862    0.01119725]]. Action = [[-0.01424684 -0.05113518  0.         -0.88074344]]. Reward = [0.]
Curr episode timestep = 131
Current timestep = 1219. State = [[-0.22039683  0.01301511]]. Action = [[-0.02535085  0.08564167  0.         -0.72535026]]. Reward = [0.]
Curr episode timestep = 132
Current timestep = 1220. State = [[-0.21605252  0.01791565]]. Action = [[ 0.09715713  0.06158393  0.         -0.8879484 ]]. Reward = [0.]
Curr episode timestep = 133
Current timestep = 1221. State = [[-0.20939963  0.01726886]]. Action = [[ 0.07604036 -0.04313002  0.         -0.881214  ]]. Reward = [0.]
Curr episode timestep = 134
Current timestep = 1222. State = [[-0.20573917  0.01795475]]. Action = [[ 0.01578082  0.04575413  0.         -0.98335123]]. Reward = [0.]
Curr episode timestep = 135
Current timestep = 1223. State = [[-0.20026906  0.01799693]]. Action = [[ 0.08732819 -0.01929958  0.         -0.8300396 ]]. Reward = [0.]
Curr episode timestep = 136
Current timestep = 1224. State = [[-0.19303992  0.01558161]]. Action = [[ 0.0795655  -0.03197093  0.          0.03035843]]. Reward = [0.]
Curr episode timestep = 137
Current timestep = 1225. State = [[-0.1918095   0.01918236]]. Action = [[-0.04783508  0.0975787   0.         -0.62768865]]. Reward = [0.]
Curr episode timestep = 138
Current timestep = 1226. State = [[-0.19136111  0.02530971]]. Action = [[ 0.01883022  0.06354026  0.         -0.956842  ]]. Reward = [0.]
Curr episode timestep = 139
Current timestep = 1227. State = [[-0.18845654  0.02461577]]. Action = [[ 0.03628885 -0.06411439  0.         -0.5196593 ]]. Reward = [0.]
Curr episode timestep = 140
Current timestep = 1228. State = [[-0.18342115  0.02284076]]. Action = [[ 0.06068143 -0.00843155  0.         -0.77604127]]. Reward = [0.]
Curr episode timestep = 141
Current timestep = 1229. State = [[-0.1777924   0.02040038]]. Action = [[ 0.05475629 -0.04656516  0.         -0.8985442 ]]. Reward = [0.]
Curr episode timestep = 142
Current timestep = 1230. State = [[-0.17073397  0.01471908]]. Action = [[ 0.07958143 -0.08497     0.         -0.24298203]]. Reward = [0.]
Curr episode timestep = 143
Current timestep = 1231. State = [[-0.16415474  0.01153946]]. Action = [[ 0.04992426 -0.00246978  0.         -0.19743359]]. Reward = [0.]
Curr episode timestep = 144
Current timestep = 1232. State = [[-0.15654609  0.00756927]]. Action = [[ 0.08478313 -0.05943745  0.         -0.6733772 ]]. Reward = [0.]
Curr episode timestep = 145
Current timestep = 1233. State = [[-0.15563993  0.00211434]]. Action = [[-0.08637001 -0.05469277  0.         -0.42623436]]. Reward = [0.]
Curr episode timestep = 146
Current timestep = 1234. State = [[-0.15811372  0.00133148]]. Action = [[-0.04951741  0.03654604  0.         -0.9240083 ]]. Reward = [0.]
Curr episode timestep = 147
Current timestep = 1235. State = [[-0.16197167 -0.0024623 ]]. Action = [[-0.08425824 -0.08098026  0.         -0.7979698 ]]. Reward = [0.]
Curr episode timestep = 148
Current timestep = 1236. State = [[-0.16099432 -0.00261512]]. Action = [[ 0.04617137  0.05851708  0.         -0.7341627 ]]. Reward = [0.]
Curr episode timestep = 149
Current timestep = 1237. State = [[-0.15712115 -0.00467675]]. Action = [[ 0.03824998 -0.05946981  0.         -0.65409863]]. Reward = [0.]
Curr episode timestep = 150
Current timestep = 1238. State = [[-0.15902126 -0.01134472]]. Action = [[-0.08562725 -0.08539957  0.         -0.35178548]]. Reward = [0.]
Curr episode timestep = 151
Current timestep = 1239. State = [[-0.15783271 -0.01030998]]. Action = [[ 0.06215466  0.090431    0.         -0.78290045]]. Reward = [0.]
Curr episode timestep = 152
Current timestep = 1240. State = [[-0.15172474 -0.00653919]]. Action = [[ 0.09232419  0.0372797   0.         -0.8375932 ]]. Reward = [0.]
Curr episode timestep = 153
Current timestep = 1241. State = [[-0.14483854 -0.00458962]]. Action = [[ 0.09213509  0.0240899   0.         -0.56016195]]. Reward = [0.]
Curr episode timestep = 154
Current timestep = 1242. State = [[-0.13750072 -0.00549006]]. Action = [[ 0.09657145 -0.02395836  0.         -0.94968027]]. Reward = [0.]
Curr episode timestep = 155
Current timestep = 1243. State = [[-0.13005963 -0.00756708]]. Action = [[ 0.08752001 -0.01663981  0.         -0.9028813 ]]. Reward = [0.]
Curr episode timestep = 156
Current timestep = 1244. State = [[-0.12439183 -0.00924619]]. Action = [[ 0.0474278  -0.01011121  0.         -0.6292052 ]]. Reward = [0.]
Curr episode timestep = 157
Current timestep = 1245. State = [[-0.12318923 -0.00862858]]. Action = [[-0.02932087  0.03016647  0.         -0.67637503]]. Reward = [0.]
Curr episode timestep = 158
Current timestep = 1246. State = [[-0.11968011 -0.00694382]]. Action = [[ 0.06070001  0.02222802  0.         -0.5505903 ]]. Reward = [0.]
Curr episode timestep = 159
Current timestep = 1247. State = [[-0.11712447 -0.00360615]]. Action = [[-0.00620744  0.05461665  0.         -0.8520071 ]]. Reward = [0.]
Curr episode timestep = 160
Current timestep = 1248. State = [[-0.11588366  0.00024555]]. Action = [[ 0.00256491  0.0381903   0.         -0.92968225]]. Reward = [0.]
Curr episode timestep = 161
Current timestep = 1249. State = [[-0.11873171  0.00136811]]. Action = [[-0.08785387 -0.0159388   0.         -0.4865278 ]]. Reward = [0.]
Curr episode timestep = 162
Current timestep = 1250. State = [[-0.12407389 -0.00207909]]. Action = [[-0.09559827 -0.08112605  0.          0.1230402 ]]. Reward = [0.]
Curr episode timestep = 163
Current timestep = 1251. State = [[-0.12128296 -0.00190786]]. Action = [[0.09941078 0.03588287 0.         0.5215683 ]]. Reward = [0.]
Curr episode timestep = 164
Current timestep = 1252. State = [[-0.11255135 -0.00401925]]. Action = [[ 0.09819669 -0.06936342  0.         -0.5690774 ]]. Reward = [0.]
Curr episode timestep = 165
Current timestep = 1253. State = [[-0.10820767 -0.00513934]]. Action = [[-0.00447945  0.01713882  0.         -0.9787303 ]]. Reward = [0.]
Curr episode timestep = 166
Current timestep = 1254. State = [[-0.10294873 -0.00727568]]. Action = [[ 0.07324667 -0.04860956  0.         -0.6337875 ]]. Reward = [0.]
Curr episode timestep = 167
Current timestep = 1255. State = [[-0.10315832 -0.01175878]]. Action = [[-0.08944675 -0.05478218  0.          0.32800663]]. Reward = [0.]
Curr episode timestep = 168
Current timestep = 1256. State = [[-0.10462973 -0.01820638]]. Action = [[-0.01707862 -0.08660525  0.         -0.9354211 ]]. Reward = [0.]
Curr episode timestep = 169
Current timestep = 1257. State = [[-0.10662107 -0.02040108]]. Action = [[-0.05841303  0.02330651  0.         -0.9084259 ]]. Reward = [0.]
Curr episode timestep = 170
Current timestep = 1258. State = [[-0.10552194 -0.02321186]]. Action = [[ 0.03620983 -0.05193964  0.         -0.31416726]]. Reward = [0.]
Curr episode timestep = 171
Current timestep = 1259. State = [[-0.09934338 -0.0260799 ]]. Action = [[ 0.0972806  -0.00606503  0.         -0.9448561 ]]. Reward = [0.]
Curr episode timestep = 172
Current timestep = 1260. State = [[-0.09175278 -0.03051693]]. Action = [[ 0.09615634 -0.05639641  0.         -0.9181589 ]]. Reward = [0.]
Curr episode timestep = 173
Current timestep = 1261. State = [[-0.09072312 -0.03524943]]. Action = [[-0.05366988 -0.02986618  0.         -0.8618702 ]]. Reward = [0.]
Curr episode timestep = 174
Current timestep = 1262. State = [[-0.09134501 -0.03468542]]. Action = [[ 0.00691625  0.05910213  0.         -0.85171074]]. Reward = [0.]
Curr episode timestep = 175
Current timestep = 1263. State = [[-0.09464183 -0.0363953 ]]. Action = [[-0.07385752 -0.04634653  0.         -0.94131416]]. Reward = [0.]
Curr episode timestep = 176
Current timestep = 1264. State = [[-0.09413002 -0.03414317]]. Action = [[ 0.05255345  0.0871575   0.         -0.6869533 ]]. Reward = [0.]
Curr episode timestep = 177
Current timestep = 1265. State = [[-0.09077609 -0.03027061]]. Action = [[0.04935243 0.03643721 0.         0.00521529]]. Reward = [0.]
Curr episode timestep = 178
Current timestep = 1266. State = [[-0.08510464 -0.02970345]]. Action = [[ 0.09831584 -0.00858627  0.         -0.92375267]]. Reward = [0.]
Curr episode timestep = 179
Current timestep = 1267. State = [[-0.08565632 -0.02906945]]. Action = [[-0.07214734  0.02045846  0.         -0.9472039 ]]. Reward = [0.]
Curr episode timestep = 180
Current timestep = 1268. State = [[-0.08448469 -0.02665613]]. Action = [[ 0.07603583  0.03306129  0.         -0.69183475]]. Reward = [0.]
Curr episode timestep = 181
Current timestep = 1269. State = [[-0.07837323 -0.03038989]]. Action = [[ 0.09469921 -0.09726688  0.         -0.49031413]]. Reward = [0.]
Curr episode timestep = 182
Current timestep = 1270. State = [[-0.07752687 -0.0336316 ]]. Action = [[-0.04673731 -0.0044442   0.         -0.23163795]]. Reward = [0.]
Curr episode timestep = 183
Current timestep = 1271. State = [[-0.07493845 -0.03161379]]. Action = [[ 0.07694226  0.05039793  0.         -0.44311583]]. Reward = [0.]
Curr episode timestep = 184
Current timestep = 1272. State = [[-0.0747241  -0.02754063]]. Action = [[-0.03979312  0.05439273  0.         -0.84413886]]. Reward = [0.]
Curr episode timestep = 185
Current timestep = 1273. State = [[-0.07669104 -0.0237675 ]]. Action = [[-0.02275301  0.03585046  0.         -0.96883667]]. Reward = [0.]
Curr episode timestep = 186
Current timestep = 1274. State = [[-0.08093937 -0.02394367]]. Action = [[-0.08094515 -0.03883956  0.         -0.75491375]]. Reward = [0.]
Curr episode timestep = 187
Current timestep = 1275. State = [[-0.07916435 -0.02722369]]. Action = [[ 0.07916697 -0.05621488  0.          0.24756241]]. Reward = [0.]
Curr episode timestep = 188
Current timestep = 1276. State = [[-0.07534906 -0.03126711]]. Action = [[ 0.01912205 -0.04902221  0.         -0.9162812 ]]. Reward = [0.]
Curr episode timestep = 189
Current timestep = 1277. State = [[-0.07114431 -0.02824641]]. Action = [[ 0.05651359  0.09464214  0.         -0.4395867 ]]. Reward = [0.]
Curr episode timestep = 190
Current timestep = 1278. State = [[-0.06597058 -0.02861621]]. Action = [[ 0.06198921 -0.05983996  0.         -0.95381486]]. Reward = [0.]
Curr episode timestep = 191
Current timestep = 1279. State = [[-0.06004931 -0.03072052]]. Action = [[ 0.06902225 -0.00326843  0.         -0.9016288 ]]. Reward = [0.]
Curr episode timestep = 192
Current timestep = 1280. State = [[-0.05989292 -0.03415024]]. Action = [[-0.06367379 -0.05582122  0.         -0.8888857 ]]. Reward = [0.]
Curr episode timestep = 193
Current timestep = 1281. State = [[-0.05599407 -0.03380947]]. Action = [[0.09871877 0.04920609 0.         0.47714758]]. Reward = [0.]
Curr episode timestep = 194
Current timestep = 1282. State = [[-0.05037479 -0.03687853]]. Action = [[ 0.03962054 -0.07725555  0.          0.08483493]]. Reward = [0.]
Curr episode timestep = 195
Current timestep = 1283. State = [[-0.1522485  -0.02356259]]. Action = [[ 0.09129442  0.01992826  0.         -0.10967141]]. Reward = [100.]
Curr episode timestep = 196
Current timestep = 1284. State = [[-0.14905718 -0.01997463]]. Action = [[ 0.05583062  0.0481595   0.         -0.6445572 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1285. State = [[-0.15173443 -0.01829568]]. Action = [[-0.07468277 -0.0233977   0.         -0.00526172]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1286. State = [[-0.15145497 -0.02031309]]. Action = [[ 0.06308366 -0.05405015  0.         -0.31660426]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1287. State = [[-0.15248443 -0.02153727]]. Action = [[-0.04903311 -0.00842996  0.         -0.86115223]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1288. State = [[-0.15291555 -0.02021747]]. Action = [[ 0.02297156  0.02149476  0.         -0.7942763 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1289. State = [[-0.14933214 -0.02163463]]. Action = [[ 0.06905147 -0.04822059  0.         -0.33459204]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1290. State = [[-0.14500615 -0.02490392]]. Action = [[ 0.05134857 -0.03654051  0.         -0.5901934 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1291. State = [[-0.14020371 -0.02679164]]. Action = [[ 0.06404687 -0.00572274  0.         -0.61442816]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1292. State = [[-0.13391925 -0.02719513]]. Action = [[0.08495305 0.01091381 0.         0.7311326 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1293. State = [[-0.13382417 -0.03200505]]. Action = [[-0.06784643 -0.08601756  0.         -0.8578407 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1294. State = [[-0.130403   -0.03178877]]. Action = [[ 0.09716428  0.07297089  0.         -0.91511804]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1295. State = [[-0.12547128 -0.02621757]]. Action = [[ 0.03802367  0.08634041  0.         -0.90459275]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1296. State = [[-0.12312668 -0.02110277]]. Action = [[ 0.01481391  0.0550187   0.         -0.45672703]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1297. State = [[-0.11857858 -0.01597817]]. Action = [[0.07490285 0.06401957 0.         0.01946092]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1298. State = [[-0.11812031 -0.01601604]]. Action = [[-0.04767336 -0.05025861  0.         -0.7093903 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1299. State = [[-0.12128632 -0.01636304]]. Action = [[-0.06142694  0.00616457  0.          0.26830828]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1300. State = [[-0.11957817 -0.01480946]]. Action = [[ 0.05416618  0.01244458  0.         -0.54978085]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1301. State = [[-0.11825497 -0.00949206]]. Action = [[-0.01728626  0.08163872  0.          0.07876503]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1302. State = [[-0.1198417  -0.00350538]]. Action = [[-0.03539765  0.04766614  0.         -0.5878593 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1303. State = [[-0.11838603 -0.00047072]]. Action = [[ 0.04203295  0.00218363  0.         -0.9234648 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1304. State = [[-0.11589557 -0.00149151]]. Action = [[ 0.01915038 -0.0484239   0.         -0.18570292]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1305. State = [[-0.11353578  0.00026799]]. Action = [[ 0.02428417  0.04443061  0.         -0.6543772 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1306. State = [[-0.10974003  0.00015157]]. Action = [[ 0.05372036 -0.0427042   0.         -0.4228872 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1307. State = [[-0.11143485 -0.00193944]]. Action = [[-0.08742737 -0.02879996  0.         -0.9425713 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1308. State = [[-0.10980265  0.00036743]]. Action = [[ 0.07575514  0.05613812  0.         -0.5968161 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1309. State = [[-0.1036308   0.00343665]]. Action = [[ 0.07814873  0.02461779  0.         -0.5596857 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1310. State = [[-0.09610424  0.00084751]]. Action = [[ 0.09748615 -0.06880541  0.          0.25463367]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1311. State = [[-0.0945828  -0.00337103]]. Action = [[-0.05487427 -0.03925972  0.         -0.50682867]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1312. State = [[-0.09430106 -0.00810835]]. Action = [[ 0.01075475 -0.06164092  0.         -0.8917816 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1313. State = [[-0.0899744  -0.00891432]]. Action = [[ 0.06424045  0.03580465  0.         -0.73854244]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1314. State = [[-0.08307188 -0.00731581]]. Action = [[ 0.09179973  0.02959276  0.         -0.8733265 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1315. State = [[-0.07788984 -0.00340705]]. Action = [[ 0.04125472  0.07464729  0.         -0.6556598 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1316. State = [[-0.07268207  0.0001401 ]]. Action = [[ 0.06988875  0.03450534  0.         -0.64531636]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1317. State = [[-0.07208714  0.00548988]]. Action = [[-0.04353073  0.08533796  0.         -0.8914035 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1318. State = [[-0.07034879  0.00928951]]. Action = [[ 0.04671853  0.01401464  0.         -0.6556926 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1319. State = [[-0.0684125   0.01439345]]. Action = [[ 0.00379511  0.07469396  0.         -0.11865944]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1320. State = [[-0.06550876  0.02225167]]. Action = [[ 0.04917938  0.08842159  0.         -0.45272684]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1321. State = [[-0.06015744  0.02598001]]. Action = [[ 0.07712748 -0.00848782  0.         -0.84947413]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1322. State = [[-0.05873122  0.02571864]]. Action = [[-0.03152482 -0.03215601  0.         -0.4654079 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1323. State = [[-0.05752836  0.02269951]]. Action = [[ 0.01704544 -0.06906232  0.         -0.793935  ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1324. State = [[-0.05863933  0.02421636]]. Action = [[-0.06030364  0.05071569  0.         -0.92618495]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1325. State = [[-0.05763272  0.0298931 ]]. Action = [[ 0.03101071  0.06300544  0.         -0.36914015]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1326. State = [[-0.05603214  0.03524303]]. Action = [[ 0.00057855  0.04423905  0.         -0.47895038]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1327. State = [[-0.05133002  0.03660355]]. Action = [[ 0.0823879  -0.02376808  0.         -0.17778432]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1328. State = [[-0.25770146 -0.0270739 ]]. Action = [[ 0.0935281 -0.084682   0.        -0.9332698]]. Reward = [100.]
Curr episode timestep = 44
Current timestep = 1329. State = [[-0.2602836 -0.0200331]]. Action = [[-0.07102768  0.07889623  0.         -0.42452478]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1330. State = [[-0.26227695 -0.0134021 ]]. Action = [[ 0.02252412  0.07060657  0.         -0.85172576]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1331. State = [[-0.2615796  -0.01363134]]. Action = [[ 0.0275951  -0.06769626  0.         -0.8900086 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1332. State = [[-0.26132944 -0.01028698]]. Action = [[ 0.00585087  0.08841338  0.         -0.10838038]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1333. State = [[-0.26374286 -0.00267789]]. Action = [[-0.03466512  0.08144204  0.         -0.39595342]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1334. State = [[-0.2661648   0.00121353]]. Action = [[-0.01186733  0.00079374  0.         -0.02165782]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1335. State = [[-0.2660708   0.00644818]]. Action = [[0.02599479 0.07288107 0.         0.3035679 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1336. State = [[-0.26584882  0.01405355]]. Action = [[0.01000478 0.07776789 0.         0.7169527 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1337. State = [[-0.26364458  0.01777295]]. Action = [[ 0.05695233 -0.00703476  0.         -0.38643038]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1338. State = [[-0.26308686  0.02250969]]. Action = [[-0.00990596  0.06458443  0.         -0.6984632 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1339. State = [[-0.25916663  0.02534001]]. Action = [[ 0.09319323 -0.01241112  0.         -0.8679377 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1340. State = [[-0.2518834   0.02752076]]. Action = [[0.09631861 0.02326085 0.         0.57220805]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1341. State = [[-0.24675727  0.02944253]]. Action = [[ 0.03694338  0.0040807   0.         -0.9413392 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1342. State = [[-0.24339402  0.02858829]]. Action = [[ 0.02217378 -0.03908914  0.         -0.90996367]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1343. State = [[-0.23783553  0.02586024]]. Action = [[ 0.06597052 -0.04526669  0.         -0.3541218 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1344. State = [[-0.23756813  0.02264605]]. Action = [[-0.0813356  -0.04413986  0.         -0.28696704]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1345. State = [[-0.23533699  0.018992  ]]. Action = [[ 0.04862096 -0.04787556  0.         -0.8485615 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1346. State = [[-0.23382822  0.01734419]]. Action = [[-0.03628538  0.00219563  0.         -0.8507359 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1347. State = [[-0.23603132  0.01345716]]. Action = [[-0.06796959 -0.07060345  0.          0.14193428]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1348. State = [[-0.23397276  0.01032898]]. Action = [[ 0.04732376 -0.00942277  0.         -0.10762298]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1349. State = [[-0.234314    0.00503135]]. Action = [[-0.06674729 -0.08203728  0.          0.39893222]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1350. State = [[-0.23893455 -0.00317679]]. Action = [[-0.09228562 -0.09675324  0.         -0.82847077]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1351. State = [[-0.2443666  -0.00612511]]. Action = [[-0.08286711  0.023398    0.         -0.88541836]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1352. State = [[-0.24697146 -0.00798673]]. Action = [[-0.01160564 -0.02671394  0.         -0.8451687 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1353. State = [[-0.25192863 -0.00690151]]. Action = [[-0.08672424  0.05575416  0.         -0.8620645 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1354. State = [[-0.25253534 -0.00239751]]. Action = [[ 0.06695113  0.06832729  0.         -0.82560664]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1355. State = [[-2.4958727e-01 -2.3383122e-04]]. Action = [[ 0.05738892  0.00443397  0.         -0.7460525 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1356. State = [[-0.24599703 -0.00261223]]. Action = [[ 0.06801804 -0.04907905  0.         -0.49472427]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1357. State = [[-2.4497098e-01  1.5096735e-04]]. Action = [[ 0.0031579   0.09019163  0.         -0.26765156]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1358. State = [[-0.24476582  0.0014282 ]]. Action = [[ 0.02197709 -0.02536987  0.         -0.92922103]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1359. State = [[-0.24784964  0.00386187]]. Action = [[-0.06070752  0.05912448  0.         -0.95673156]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1360. State = [[-0.25212923  0.00720816]]. Action = [[-0.03794503  0.02530005  0.         -0.5470681 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1361. State = [[-0.25536358  0.01130013]]. Action = [[-0.02491494  0.05191625  0.         -0.43061316]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1362. State = [[-0.25271398  0.00930039]]. Action = [[ 0.09055699 -0.08704942  0.         -0.7967573 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1363. State = [[-0.24834599  0.01125619]]. Action = [[ 0.05250192  0.08104237  0.         -0.5558523 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1364. State = [[-0.24272445  0.0106681 ]]. Action = [[ 0.09079725 -0.06584142  0.          0.5637181 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1365. State = [[-0.24377045  0.00626176]]. Action = [[-0.09379745 -0.0568116   0.          0.72879076]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1366. State = [[-0.24324726  0.00609615]]. Action = [[ 0.06408722  0.03143293  0.         -0.57206655]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1367. State = [[-0.23942228  0.0077553 ]]. Action = [[ 0.04476506  0.01907383  0.         -0.7982435 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1368. State = [[-0.24188985  0.00429028]]. Action = [[-0.09087986 -0.07951073  0.         -0.8253131 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1369. State = [[-0.2403326   0.00395766]]. Action = [[ 0.08176278  0.04451121  0.         -0.8954568 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1370. State = [[-0.23547807  0.00102052]]. Action = [[ 0.0428387  -0.07564411  0.          0.30193114]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1371. State = [[-0.2307599 -0.0019749]]. Action = [[ 0.04850369 -0.00481866  0.          0.04729044]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1372. State = [[-0.22454785 -0.00205489]]. Action = [[0.07760809 0.01874076 0.         0.31111872]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1373. State = [[-2.2281761e-01 -4.9348771e-05]]. Action = [[-0.03194796  0.04313662  0.         -0.83358663]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1374. State = [[-0.22228813 -0.00105265]]. Action = [[ 0.00627544 -0.03802835  0.         -0.63706666]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1375. State = [[-0.22558518 -0.00347122]]. Action = [[-0.09608077 -0.02120493  0.          0.32513535]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1376. State = [[-0.22992815 -0.00445136]]. Action = [[-0.05889101 -0.00284642  0.          0.5156343 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1377. State = [[-0.22865503 -0.00498919]]. Action = [[ 0.05136731 -0.00693079  0.         -0.5745985 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1378. State = [[-0.22881639 -0.00735142]]. Action = [[-0.04226753 -0.04069385  0.         -0.86320126]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1379. State = [[-0.23227488 -0.0098297 ]]. Action = [[-0.06071253 -0.02147396  0.         -0.04505998]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1380. State = [[-0.22991592 -0.01104193]]. Action = [[ 0.0836075  -0.00498664  0.         -0.2951975 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1381. State = [[-0.22628199 -0.0120502 ]]. Action = [[ 0.02400494 -0.00839053  0.         -0.35959542]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1382. State = [[-0.22732532 -0.01126163]]. Action = [[-0.04437976  0.02910518  0.         -0.7792864 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1383. State = [[-0.22512485 -0.01293101]]. Action = [[ 0.06719387 -0.04442405  0.         -0.5497892 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1384. State = [[-0.22657748 -0.01773054]]. Action = [[-0.07965577 -0.06158936  0.         -0.9730045 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1385. State = [[-0.22431111 -0.01613481]]. Action = [[0.09559124 0.08071396 0.         0.3124894 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1386. State = [[-0.22002825 -0.01307162]]. Action = [[ 0.03637809  0.02165861  0.         -0.7424767 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1387. State = [[-0.22038367 -0.01394175]]. Action = [[-0.03525413 -0.02958362  0.         -0.43850696]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1388. State = [[-0.22113921 -0.01844758]]. Action = [[-0.00667365 -0.06922738  0.         -0.15672475]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1389. State = [[-0.22249112 -0.02374504]]. Action = [[-0.037165   -0.05548367  0.         -0.53676325]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1390. State = [[-0.2243081  -0.02161905]]. Action = [[-0.02741745  0.08906854  0.         -0.8373855 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1391. State = [[-0.22648935 -0.02171816]]. Action = [[-0.03197742 -0.04799236  0.         -0.57051176]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1392. State = [[-0.22292754 -0.02060111]]. Action = [[0.09611172 0.05153299 0.         0.1075809 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1393. State = [[-0.22178137 -0.01775637]]. Action = [[-0.02719716  0.02877551  0.          0.21263635]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1394. State = [[-0.22244658 -0.01504174]]. Action = [[ 0.00383115  0.03248792  0.         -0.9027736 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1395. State = [[-0.22400412 -0.01260751]]. Action = [[-0.02720062  0.02144657  0.         -0.12519681]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1396. State = [[-0.22499599 -0.01509856]]. Action = [[-5.0249696e-04 -7.2261259e-02  0.0000000e+00 -5.2873635e-01]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1397. State = [[-0.22132312 -0.01865128]]. Action = [[ 0.07768119 -0.03249041  0.         -0.93375164]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1398. State = [[-0.21573886 -0.0205554 ]]. Action = [[ 0.07021708 -0.01380473  0.          0.21258354]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1399. State = [[-0.20897633 -0.01796496]]. Action = [[ 0.09423793  0.06717906  0.         -0.8685392 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1400. State = [[-0.20182712 -0.02121611]]. Action = [[ 0.08484126 -0.09787747  0.         -0.7151821 ]]. Reward = [0.]
