Current timestep = 0. State = [[-0.32604954 -0.08628346]]. Action = [[0.00435984 0.09830839 0.         0.7271644 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0668, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of None
Current timestep = 1. State = [[-0.33171427 -0.08338373]]. Action = [[-0.08950701 -0.00990611  0.          0.9261981 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0582, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of None
Current timestep = 2. State = [[-0.33283383 -0.08530018]]. Action = [[ 0.05919524 -0.05022868  0.          0.01754153]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0498, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of None
Current timestep = 3. State = [[-0.32827222 -0.08361281]]. Action = [[ 0.07941509  0.0483874   0.         -0.50816584]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0282, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of None
Current timestep = 4. State = [[-0.3254116  -0.08398479]]. Action = [[ 0.02239919 -0.05190226  0.          0.06075954]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0375, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of None
Current timestep = 5. State = [[-0.3279671  -0.08313566]]. Action = [[-0.06503823  0.04295453  0.         -0.71753365]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of None
Current timestep = 6. State = [[-0.33477935 -0.08682702]]. Action = [[-0.09400872 -0.0952624   0.         -0.55266756]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of None
Current timestep = 7. State = [[-0.3423005  -0.08953982]]. Action = [[-0.08573388  0.01543184  0.          0.75047183]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0277, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of None
Current timestep = 8. State = [[-0.34299555 -0.0931111 ]]. Action = [[ 0.06539624 -0.06995545  0.          0.36012435]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0267, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of None
Current timestep = 9. State = [[-0.33896556 -0.09817613]]. Action = [[ 0.05116934 -0.04899728  0.          0.96619225]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0148, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of None
Current timestep = 10. State = [[-0.34125704 -0.09601232]]. Action = [[-0.09370168  0.09744289  0.          0.9564408 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0123, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of None
Current timestep = 11. State = [[-0.34794155 -0.08941568]]. Action = [[-0.06642361  0.08775141  0.         -0.7740712 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of None
Current timestep = 12. State = [[-0.3524674  -0.09067384]]. Action = [[-0.02062605 -0.08017505  0.         -0.9536822 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of None
Current timestep = 13. State = [[-0.35143182 -0.09707875]]. Action = [[ 0.06866229 -0.083746    0.          0.8004637 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of None
Current timestep = 14. State = [[-0.3490534  -0.10334155]]. Action = [[ 0.03622013 -0.07354388  0.          0.9915631 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of None
Current timestep = 15. State = [[-0.34913963 -0.10775554]]. Action = [[-0.00907946 -0.03376415  0.         -0.21082371]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of None
Current timestep = 16. State = [[-0.3515684  -0.11455903]]. Action = [[-0.03354903 -0.0951744   0.         -0.46467394]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of None
Current timestep = 17. State = [[-0.35036102 -0.11521903]]. Action = [[ 0.06343738  0.07837261  0.         -0.04249555]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of None
Current timestep = 18. State = [[-0.3524108  -0.11073043]]. Action = [[-0.07434311  0.06280682  0.         -0.7986902 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 18 of None
Current timestep = 19. State = [[-0.35568303 -0.10865618]]. Action = [[-0.01442628  0.01446489  0.         -0.61173034]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(8.1447e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 19 of None
Current timestep = 20. State = [[-0.3526776  -0.10687124]]. Action = [[0.08623167 0.02300803 0.         0.31270623]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 20 of None
Current timestep = 21. State = [[-0.35496762 -0.10565591]]. Action = [[-0.09344417  0.00754379  0.          0.03627145]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 21 of None
Current timestep = 22. State = [[-0.35567173 -0.1096269 ]]. Action = [[ 0.06830675 -0.09284499  0.         -0.53725266]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 22 of None
Current timestep = 23. State = [[-0.34997484 -0.11670735]]. Action = [[ 0.08822257 -0.09029897  0.          0.20686066]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 23 of None
Current timestep = 24. State = [[-0.34345004 -0.12370998]]. Action = [[ 0.07326324 -0.08367714  0.         -0.62574905]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 24 of None
Current timestep = 25. State = [[-0.33743477 -0.13015684]]. Action = [[ 0.05893933 -0.06729642  0.         -0.7062875 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, True, False, False]
State prediction error at timestep 25 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 25 of None
Current timestep = 26. State = [[-0.3335562 -0.1360468]]. Action = [[ 0.01803955 -0.05434812  0.         -0.49133933]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, True, False, False]
State prediction error at timestep 26 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 26 of None
Current timestep = 27. State = [[-0.3279426  -0.14183766]]. Action = [[ 0.07475772 -0.05114451  0.         -0.83698577]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, True, False, False]
State prediction error at timestep 27 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 27 of None
Current timestep = 28. State = [[-0.32778814 -0.14228137]]. Action = [[-0.07985155  0.07023907  0.          0.716177  ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, True, False, False]
State prediction error at timestep 28 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 28 of None
Current timestep = 29. State = [[-0.3257879  -0.13905415]]. Action = [[ 0.07731897  0.0548152   0.         -0.19668382]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, True, False, False]
State prediction error at timestep 29 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 29 of None
Current timestep = 30. State = [[-0.32135898 -0.13593896]]. Action = [[0.03792714 0.03898365 0.         0.9145137 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, True, False, False]
State prediction error at timestep 30 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 30 of None
Current timestep = 31. State = [[-0.32012093 -0.13237548]]. Action = [[-0.00734532  0.05215795  0.         -0.68890154]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, True, False, False]
State prediction error at timestep 31 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 31 of None
Current timestep = 32. State = [[-0.3156949  -0.13425143]]. Action = [[ 0.09171552 -0.07368196  0.         -0.01193464]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, True, False, False]
State prediction error at timestep 32 is tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 32 of None
Current timestep = 33. State = [[-0.31349254 -0.1335877 ]]. Action = [[-0.03082895  0.06172993  0.         -0.43724132]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, True, False, False]
State prediction error at timestep 33 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 33 of None
Current timestep = 34. State = [[-0.31570745 -0.1293414 ]]. Action = [[-0.03572042  0.05166627  0.         -0.984189  ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, True, False, False]
State prediction error at timestep 34 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 34 of None
Current timestep = 35. State = [[-0.3144213  -0.12887667]]. Action = [[ 0.04760996 -0.03087246  0.          0.73354495]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, True, False, False]
State prediction error at timestep 35 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 35 of None
Current timestep = 36. State = [[-0.3164821 -0.131975 ]]. Action = [[-0.07450292 -0.05111818  0.         -0.9411027 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, True, False, False]
State prediction error at timestep 36 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 36 of None
Current timestep = 37. State = [[-0.31953534 -0.13823329]]. Action = [[-0.02443783 -0.08774377  0.         -0.680884  ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, True, False, False]
State prediction error at timestep 37 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 37 of None
Current timestep = 38. State = [[-0.31880757 -0.14025575]]. Action = [[ 0.02529877  0.02194778  0.         -0.69732094]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, True, False, False]
State prediction error at timestep 38 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 38 of None
Current timestep = 39. State = [[-0.32189897 -0.13661738]]. Action = [[-0.08657759  0.07219615  0.         -0.52458906]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, True, False, False]
State prediction error at timestep 39 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 39 of None
Current timestep = 40. State = [[-0.3218579  -0.13827516]]. Action = [[ 0.0623932 -0.0812559  0.        -0.505362 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, True, False, False]
State prediction error at timestep 40 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 40 of None
Current timestep = 41. State = [[-0.31472388 -0.14271297]]. Action = [[ 0.0969845  -0.04060469  0.         -0.45733178]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, True, False, False]
State prediction error at timestep 41 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 41 of None
Current timestep = 42. State = [[-0.30885214 -0.1393448 ]]. Action = [[ 0.03615708  0.09605473  0.         -0.7085937 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 42 of None
Current timestep = 43. State = [[-0.3075256 -0.1385312]]. Action = [[-0.01379284 -0.03964251  0.          0.885164  ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, True, False, False]
State prediction error at timestep 43 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 43 of None
Current timestep = 44. State = [[-0.30511642 -0.13938357]]. Action = [[ 0.04331518  0.00724858  0.         -0.32532144]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, True, False, False]
State prediction error at timestep 44 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 44 of None
Current timestep = 45. State = [[-0.3004675  -0.14153159]]. Action = [[ 0.05608428 -0.04577896  0.          0.5140588 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, True, False, False]
State prediction error at timestep 45 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 45 of None
Current timestep = 46. State = [[-0.2973581  -0.14006814]]. Action = [[0.01304604 0.0577567  0.         0.5734048 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, True, False, False]
State prediction error at timestep 46 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 46 of None
Current timestep = 47. State = [[-0.29775846 -0.13404134]]. Action = [[-0.03035113  0.08973774  0.         -0.25634837]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, True, False, False]
State prediction error at timestep 47 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 47 of None
Current timestep = 48. State = [[-0.2946827  -0.13356327]]. Action = [[ 0.07656788 -0.05497192  0.         -0.7528407 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, True, False, False]
State prediction error at timestep 48 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 48 of None
Current timestep = 49. State = [[-0.28808215 -0.13106784]]. Action = [[ 0.07046223  0.06966154  0.         -0.49434084]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, True, False, False]
State prediction error at timestep 49 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 49 of None
Current timestep = 50. State = [[-0.28263673 -0.12450146]]. Action = [[ 0.04658066  0.07537553  0.         -0.86270106]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 50 of None
Current timestep = 51. State = [[-0.2849465  -0.12122615]]. Action = [[-0.09492144  0.00337236  0.          0.3534478 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 51 of None
Current timestep = 52. State = [[-0.2884364  -0.12493194]]. Action = [[-0.03289726 -0.0846393   0.         -0.5317908 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 52 of None
Current timestep = 53. State = [[-0.2924004  -0.12905939]]. Action = [[-0.07372832 -0.03210803  0.         -0.3655308 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, True, False, False]
State prediction error at timestep 53 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 53 of None
Current timestep = 54. State = [[-0.29175925 -0.1345024 ]]. Action = [[ 0.04515976 -0.08424617  0.         -0.08591765]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, True, False, False]
State prediction error at timestep 54 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 54 of None
Current timestep = 55. State = [[-0.29224348 -0.14118977]]. Action = [[-0.05234873 -0.07303961  0.          0.24395955]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, True, False, False]
State prediction error at timestep 55 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 55 of None
Current timestep = 56. State = [[-0.2935808  -0.14767027]]. Action = [[-0.01509072 -0.06223612  0.         -0.30916786]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, True, False, False]
State prediction error at timestep 56 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 56 of None
Current timestep = 57. State = [[-0.296698   -0.14820628]]. Action = [[-0.06777793  0.05757075  0.          0.8447294 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, True, False, False]
State prediction error at timestep 57 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 57 of None
Current timestep = 58. State = [[-0.29488915 -0.1444477 ]]. Action = [[0.0731707  0.06383226 0.         0.16354978]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, True, False, False]
State prediction error at timestep 58 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 58 of None
Current timestep = 59. State = [[-0.29312417 -0.13960184]]. Action = [[-0.00756636  0.06271797  0.         -0.6371225 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, True, False, False]
State prediction error at timestep 59 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 59 of None
Current timestep = 60. State = [[-0.29052037 -0.13890968]]. Action = [[ 0.05789507 -0.02778776  0.          0.34296846]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, True, False, False]
State prediction error at timestep 60 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 60 of None
Current timestep = 61. State = [[-0.29315203 -0.14057347]]. Action = [[-0.08712094 -0.01703764  0.         -0.6709282 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, True, False, False]
State prediction error at timestep 61 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 61 of None
Current timestep = 62. State = [[-0.2946062  -0.14345214]]. Action = [[ 0.02928352 -0.04201383  0.          0.7420423 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, True, False, False]
State prediction error at timestep 62 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 62 of None
Current timestep = 63. State = [[-0.2921044  -0.14441872]]. Action = [[ 0.04457492  0.00559046  0.         -0.58027   ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, True, False, False]
State prediction error at timestep 63 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 63 of None
Current timestep = 64. State = [[-0.2934974  -0.14515385]]. Action = [[-0.04956371 -0.01710913  0.          0.08260489]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, True, False, False]
State prediction error at timestep 64 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 64 of None
Current timestep = 65. State = [[-0.2969364  -0.14632499]]. Action = [[-0.03606131 -0.00573687  0.          0.5306635 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, True, False, False]
State prediction error at timestep 65 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 65 of None
Current timestep = 66. State = [[-0.30299938 -0.14961572]]. Action = [[-0.09214138 -0.04942728  0.          0.7834394 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, True, False, False]
State prediction error at timestep 66 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 66 of None
Current timestep = 67. State = [[-0.3087747  -0.15668295]]. Action = [[-0.05005399 -0.09348495  0.         -0.9126737 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, True, False, False]
State prediction error at timestep 67 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 67 of None
Current timestep = 68. State = [[-0.30875462 -0.1625674 ]]. Action = [[ 0.04520542 -0.04233116  0.         -0.11649919]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, True, False, False]
State prediction error at timestep 68 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 68 of None
Current timestep = 69. State = [[-0.3051849  -0.16444488]]. Action = [[ 0.0544023   0.00353122  0.         -0.17321903]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, True, False, False]
State prediction error at timestep 69 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 69 of None
Current timestep = 70. State = [[-0.29955998 -0.16759995]]. Action = [[ 0.08436743 -0.05597566  0.         -0.7629528 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, True, False, False]
State prediction error at timestep 70 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 70 of None
Current timestep = 71. State = [[-0.30078626 -0.17202455]]. Action = [[-0.07863273 -0.0375288   0.          0.44133997]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, True, False, False]
State prediction error at timestep 71 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 71 of None
Current timestep = 72. State = [[-0.30629405 -0.17581381]]. Action = [[-0.05588006 -0.02587659  0.         -0.6986717 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, True, False, False]
State prediction error at timestep 72 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 72 of None
Current timestep = 73. State = [[-0.30621335 -0.17921014]]. Action = [[ 0.05203088 -0.02631958  0.          0.7714429 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, True, False, False]
State prediction error at timestep 73 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 73 of None
Current timestep = 74. State = [[-0.30612662 -0.18541655]]. Action = [[-0.01667716 -0.08764011  0.          0.54999256]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, True, False, False]
State prediction error at timestep 74 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 74 of None
Current timestep = 75. State = [[-0.30335984 -0.19070832]]. Action = [[ 0.07094366 -0.03047304  0.         -0.06648868]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, True, False, False]
State prediction error at timestep 75 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 75 of None
Current timestep = 76. State = [[-0.29732487 -0.19472417]]. Action = [[ 0.08019779 -0.04101266  0.         -0.00120997]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, True, False, False]
State prediction error at timestep 76 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 76 of None
Current timestep = 77. State = [[-0.29738328 -0.1988749 ]]. Action = [[-0.05424083 -0.03388353  0.          0.60991466]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, True, False, False]
State prediction error at timestep 77 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 77 of None
Current timestep = 78. State = [[-0.29488298 -0.19722204]]. Action = [[ 0.08216538  0.08587495  0.         -0.92393196]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, True, False, False]
State prediction error at timestep 78 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 78 of None
Current timestep = 79. State = [[-0.29323092 -0.19543213]]. Action = [[-0.01305099  0.00414294  0.          0.280133  ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, True, False, False]
State prediction error at timestep 79 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 79 of None
Current timestep = 80. State = [[-0.28953698 -0.19895992]]. Action = [[ 0.08208812 -0.06493977  0.         -0.8023042 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, True, False, False]
State prediction error at timestep 80 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 80 of None
Current timestep = 81. State = [[-0.29066485 -0.19782409]]. Action = [[-0.0863136   0.08920132  0.         -0.5418714 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, True, False, False]
State prediction error at timestep 81 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 81 of None
Current timestep = 82. State = [[-0.29197884 -0.19356984]]. Action = [[ 0.03636044  0.03875247  0.         -0.53592783]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, True, False, False]
State prediction error at timestep 82 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 82 of None
Current timestep = 83. State = [[-0.28895164 -0.19397621]]. Action = [[ 0.05714736 -0.04794896  0.          0.54164505]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, True, False, False]
State prediction error at timestep 83 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 83 of None
Current timestep = 84. State = [[-0.28954238 -0.19054149]]. Action = [[-0.04169829  0.08812124  0.          0.16435027]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, True, False, False]
State prediction error at timestep 84 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 84 of None
Current timestep = 85. State = [[-0.29637164 -0.18881868]]. Action = [[-0.09386183 -0.02766778  0.         -0.4471004 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 85 of None
Current timestep = 86. State = [[-0.30292216 -0.18676987]]. Action = [[-0.05021065  0.04696802  0.         -0.31456292]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, True, False, False]
State prediction error at timestep 86 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 86 of None
Current timestep = 87. State = [[-0.30942342 -0.18381153]]. Action = [[-0.07318203  0.02330372  0.         -0.94092625]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, True, False, False]
State prediction error at timestep 87 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 87 of None
Current timestep = 88. State = [[-0.31107888 -0.17933653]]. Action = [[ 0.03608038  0.0597463   0.         -0.1485101 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, True, False, False]
State prediction error at timestep 88 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 88 of None
Current timestep = 89. State = [[-0.3093929  -0.17189798]]. Action = [[0.03588101 0.08424745 0.         0.72728515]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, True, False, False]
State prediction error at timestep 89 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 89 of None
Current timestep = 90. State = [[-0.3074474  -0.16868968]]. Action = [[ 0.03582566 -0.02468487  0.         -0.57426333]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, True, False, False]
State prediction error at timestep 90 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 90 of None
Current timestep = 91. State = [[-0.30842522 -0.17246784]]. Action = [[-0.02372159 -0.09848043  0.          0.65254605]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, True, False, False]
State prediction error at timestep 91 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 91 of None
Current timestep = 92. State = [[-0.31231016 -0.17773314]]. Action = [[-0.04921351 -0.06410162  0.         -0.21959925]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, True, False, False]
State prediction error at timestep 92 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 92 of None
Current timestep = 93. State = [[-0.3121777  -0.17555882]]. Action = [[ 0.04527733  0.08112764  0.         -0.9027257 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, True, False, False]
State prediction error at timestep 93 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 93 of None
Current timestep = 94. State = [[-0.31248632 -0.1751134 ]]. Action = [[-0.0250743  -0.04552994  0.         -0.7957489 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, True, False, False]
State prediction error at timestep 94 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 94 of None
Current timestep = 95. State = [[-0.31284434 -0.17221117]]. Action = [[ 0.01128552  0.07857841  0.         -0.953551  ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, True, False, False]
State prediction error at timestep 95 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 95 of None
Current timestep = 96. State = [[-0.31393522 -0.17263283]]. Action = [[-0.02295548 -0.05828817  0.         -0.79793316]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, True, False, False]
State prediction error at timestep 96 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 96 of None
Current timestep = 97. State = [[-0.31114185 -0.17453703]]. Action = [[ 0.07296499 -0.01049364  0.         -0.41482598]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, True, False, False]
State prediction error at timestep 97 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 97 of None
Current timestep = 98. State = [[-0.30863374 -0.1736454 ]]. Action = [[ 0.00643849  0.0184052   0.         -0.8022589 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, True, False, False]
State prediction error at timestep 98 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 98 of None
Current timestep = 99. State = [[-0.3073743  -0.16883315]]. Action = [[ 0.01777432  0.08199792  0.         -0.24452662]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, True, False, False]
State prediction error at timestep 99 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 99 of None
Current timestep = 100. State = [[-0.30361223 -0.16379371]]. Action = [[ 0.06508752  0.04058089  0.         -0.7943448 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, True, False, False]
State prediction error at timestep 100 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 100 of None
Current timestep = 101. State = [[-0.29713145 -0.15706937]]. Action = [[ 0.09153306  0.08392868  0.         -0.9493565 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, True, False, False]
State prediction error at timestep 101 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 101 of None
Current timestep = 102. State = [[-0.2947682  -0.15097114]]. Action = [[-0.00710889  0.04199185  0.         -0.75457567]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, True, False, False]
State prediction error at timestep 102 is tensor(3.8019e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 102 of None
Current timestep = 103. State = [[-0.29191485 -0.15177433]]. Action = [[ 0.06249606 -0.07246204  0.          0.06477809]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, True, False, False]
State prediction error at timestep 103 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 103 of None
Current timestep = 104. State = [[-0.2865393 -0.1515061]]. Action = [[ 0.06639535  0.0207387   0.         -0.62643766]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, True, False, False]
State prediction error at timestep 104 is tensor(2.2039e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 104 of None
Current timestep = 105. State = [[-0.28035995 -0.15345265]]. Action = [[ 0.0712591  -0.07239728  0.         -0.5898253 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, True, False, False]
State prediction error at timestep 105 is tensor(5.3580e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 105 of None
Current timestep = 106. State = [[-0.27602306 -0.14997862]]. Action = [[ 0.02201173  0.09787326  0.         -0.5130818 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, True, False, False]
State prediction error at timestep 106 is tensor(3.0439e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 106 of None
Current timestep = 107. State = [[-0.27173048 -0.14759202]]. Action = [[ 0.05863035 -0.02467187  0.          0.15600681]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, True, False, False]
State prediction error at timestep 107 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 107 of None
Current timestep = 108. State = [[-0.26544854 -0.14680995]]. Action = [[0.07366601 0.00974835 0.         0.19119322]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, True, False, False]
State prediction error at timestep 108 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 108 of None
Current timestep = 109. State = [[-0.26308972 -0.14471605]]. Action = [[-0.01889743  0.02365838  0.          0.1033498 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, True, False, False]
State prediction error at timestep 109 is tensor(6.3254e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 109 of None
Current timestep = 110. State = [[-0.2635116  -0.13935491]]. Action = [[-0.02226926  0.08669998  0.          0.06672692]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, True, False, False]
State prediction error at timestep 110 is tensor(4.8894e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 110 of None
Current timestep = 111. State = [[-0.2646824  -0.13255192]]. Action = [[-0.02919118  0.07460263  0.         -0.31455213]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, True, False, False]
State prediction error at timestep 111 is tensor(4.1393e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 111 of None
Current timestep = 112. State = [[-0.26440695 -0.13380767]]. Action = [[ 0.00673946 -0.08929274  0.         -0.9165057 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, True, False, False]
State prediction error at timestep 112 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 112 of None
Current timestep = 113. State = [[-0.25923845 -0.13953757]]. Action = [[ 0.07839321 -0.07971199  0.          0.44455087]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, True, False, False]
State prediction error at timestep 113 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 113 of None
Current timestep = 114. State = [[-0.2587347 -0.1418054]]. Action = [[-0.07074497  0.00189976  0.         -0.5383673 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, True, False, False]
State prediction error at timestep 114 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 114 of None
Current timestep = 115. State = [[-0.25698492 -0.13720593]]. Action = [[0.05568183 0.0908032  0.         0.07338595]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, True, False, False]
State prediction error at timestep 115 is tensor(1.6380e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 115 of None
Current timestep = 116. State = [[-0.250266   -0.13818064]]. Action = [[ 0.09744205 -0.0879102   0.         -0.02250725]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, True, False, False]
State prediction error at timestep 116 is tensor(2.8357e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 116 of None
Current timestep = 117. State = [[-0.24637754 -0.1421957 ]]. Action = [[-1.8393248e-04 -3.7044920e-02  0.0000000e+00  6.4866734e-01]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, True, False, False]
State prediction error at timestep 117 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 117 of None
Current timestep = 118. State = [[-0.24472731 -0.13906732]]. Action = [[ 0.00969952  0.09172004  0.         -0.8351518 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, True, False, False]
State prediction error at timestep 118 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 118 of None
Current timestep = 119. State = [[-0.2462078  -0.13319235]]. Action = [[-0.05208472  0.06840052  0.         -0.57194847]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, True, False, False]
State prediction error at timestep 119 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 119 of None
Current timestep = 120. State = [[-0.24492815 -0.13272007]]. Action = [[ 0.04597933 -0.03798363  0.         -0.26840603]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, True, False, False]
State prediction error at timestep 120 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 120 of None
Current timestep = 121. State = [[-0.23838215 -0.1375143 ]]. Action = [[ 0.09177988 -0.08230245  0.         -0.22404808]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, True, False, False]
State prediction error at timestep 121 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 121 of None
Current timestep = 122. State = [[-0.2333303  -0.13902129]]. Action = [[0.02691147 0.01803006 0.         0.29837286]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, True, False, False]
State prediction error at timestep 122 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 122 of None
Current timestep = 123. State = [[-0.23509097 -0.14202282]]. Action = [[-0.07719038 -0.06139922  0.         -0.6976466 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, True, False, False]
State prediction error at timestep 123 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 123 of None
Current timestep = 124. State = [[-0.23358153 -0.14048833]]. Action = [[ 0.05345894  0.07995074  0.         -0.36949706]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, True, False, False]
State prediction error at timestep 124 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 124 of None
Current timestep = 125. State = [[-0.22799757 -0.13519369]]. Action = [[ 0.07012018  0.06067916  0.         -0.89316255]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, True, False, False]
State prediction error at timestep 125 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 125 of None
Current timestep = 126. State = [[-0.2221043  -0.13472365]]. Action = [[ 0.06557637 -0.03788507  0.         -0.49634492]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, True, False, False]
State prediction error at timestep 126 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 126 of None
Current timestep = 127. State = [[-0.21795946 -0.13163573]]. Action = [[0.03025316 0.07409523 0.         0.3022282 ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, True, False, False]
State prediction error at timestep 127 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 127 of None
Current timestep = 128. State = [[-0.21806177 -0.12954257]]. Action = [[-0.03462861 -0.00886088  0.         -0.88619226]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, True, False, False]
State prediction error at timestep 128 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 128 of None
Current timestep = 129. State = [[-0.21671921 -0.131645  ]]. Action = [[ 0.03025208 -0.04577982  0.         -0.9661095 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, True, False, False]
State prediction error at timestep 129 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 129 of None
Current timestep = 130. State = [[-0.21520136 -0.1342449 ]]. Action = [[-0.00448696 -0.02939345  0.          0.74524343]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, True, False, False]
State prediction error at timestep 130 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 130 of None
Current timestep = 131. State = [[-0.21216169 -0.13469827]]. Action = [[ 0.0437078   0.00884126  0.         -0.94852537]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, True, False, False]
State prediction error at timestep 131 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 131 of None
Current timestep = 132. State = [[-0.21421279 -0.1359184 ]]. Action = [[-0.08823476 -0.0264411   0.          0.6057162 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, True, False, False]
State prediction error at timestep 132 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 132 of None
Current timestep = 133. State = [[-0.21313398 -0.13657638]]. Action = [[ 0.05329754  0.00743739  0.         -0.73417974]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, True, False, False]
State prediction error at timestep 133 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 133 of None
Current timestep = 134. State = [[-0.21127485 -0.13194796]]. Action = [[-0.00472779  0.08996452  0.         -0.313873  ]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, True, False, False]
State prediction error at timestep 134 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 134 of None
Current timestep = 135. State = [[-0.20802891 -0.125654  ]]. Action = [[ 0.05927698  0.06546011  0.         -0.48902696]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, True, False, False]
State prediction error at timestep 135 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 135 of None
Current timestep = 136. State = [[-0.20724745 -0.12659886]]. Action = [[-0.02438033 -0.07531552  0.         -0.94854784]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, True, False, False]
State prediction error at timestep 136 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 136 of None
Current timestep = 137. State = [[-0.20845602 -0.12790743]]. Action = [[-0.02210756  0.00905782  0.         -0.4377808 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, True, False, False]
State prediction error at timestep 137 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 137 of None
Current timestep = 138. State = [[-0.20615013 -0.13078499]]. Action = [[ 0.05144646 -0.065789    0.          0.5631981 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, True, False, False]
State prediction error at timestep 138 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 138 of None
Current timestep = 139. State = [[-0.19990681 -0.13703004]]. Action = [[ 0.08778626 -0.0885314   0.         -0.46138155]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, True, False, False]
State prediction error at timestep 139 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 139 of None
Current timestep = 140. State = [[-0.19408539 -0.13939078]]. Action = [[ 0.05194949  0.01271038  0.         -0.7271328 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, True, False, False]
State prediction error at timestep 140 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 140 of None
Current timestep = 141. State = [[-0.19520785 -0.13586338]]. Action = [[-0.07671288  0.08099077  0.         -0.8429633 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, True, False, False]
State prediction error at timestep 141 is tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 141 of None
Current timestep = 142. State = [[-0.19679634 -0.1321382 ]]. Action = [[ 0.0012453   0.03556318  0.         -0.39457023]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, True, False, False]
State prediction error at timestep 142 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 142 of None
Current timestep = 143. State = [[-0.19842954 -0.13578635]]. Action = [[-0.03633756 -0.09418394  0.          0.25695074]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, True, False, False]
State prediction error at timestep 143 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 143 of None
Current timestep = 144. State = [[-0.19494997 -0.13544767]]. Action = [[ 0.08882966  0.06598433  0.         -0.22281373]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, True, False, False]
State prediction error at timestep 144 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 144 of None
Current timestep = 145. State = [[-0.1964832  -0.13661253]]. Action = [[-0.08816331 -0.06302179  0.         -0.05668837]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, True, False, False]
State prediction error at timestep 145 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 145 of None
Current timestep = 146. State = [[-0.20193388 -0.1348661 ]]. Action = [[-0.07111715  0.07659329  0.          0.29248834]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, True, False, False]
State prediction error at timestep 146 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 146 of None
Current timestep = 147. State = [[-0.20099334 -0.13671984]]. Action = [[ 0.07136879 -0.08393641  0.         -0.36177433]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, True, False, False]
State prediction error at timestep 147 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 147 of None
Current timestep = 148. State = [[-0.1961222 -0.1387938]]. Action = [[ 0.04925121  0.00942131  0.         -0.8954313 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, True, False, False]
State prediction error at timestep 148 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 148 of None
Current timestep = 149. State = [[-0.19141138 -0.14296982]]. Action = [[ 0.05104961 -0.08083465  0.         -0.9734423 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, True, False, False]
State prediction error at timestep 149 is tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 149 of None
Current timestep = 150. State = [[-0.18455817 -0.1419974 ]]. Action = [[ 0.09441175  0.0751619   0.         -0.06481206]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, True, False, False]
State prediction error at timestep 150 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 150 of None
Current timestep = 151. State = [[-0.17843871 -0.14099292]]. Action = [[ 0.05671098 -0.01705305  0.         -0.80955017]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, True, False, False]
State prediction error at timestep 151 is tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 151 of None
Current timestep = 152. State = [[-0.17789203 -0.14571768]]. Action = [[-0.04195581 -0.07819287  0.         -0.2532825 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, True, False, False]
State prediction error at timestep 152 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 152 of None
Current timestep = 153. State = [[-0.17704599 -0.1439068 ]]. Action = [[0.01923206 0.0977012  0.         0.40493035]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, True, False, False]
State prediction error at timestep 153 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 153 of None
Current timestep = 154. State = [[-0.17763177 -0.13924597]]. Action = [[-0.0385046   0.04565517  0.         -0.93924767]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, True, False, False]
State prediction error at timestep 154 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 154 of None
Current timestep = 155. State = [[-0.17956078 -0.14191306]]. Action = [[-0.03469849 -0.08091599  0.         -0.66715074]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, True, False, False]
State prediction error at timestep 155 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 155 of None
Current timestep = 156. State = [[-0.18004823 -0.14227691]]. Action = [[-0.00570323  0.04251546  0.         -0.6177148 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, True, False, False]
State prediction error at timestep 156 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 156 of None
Current timestep = 157. State = [[-0.17819144 -0.13927889]]. Action = [[0.03045905 0.03587116 0.         0.1512059 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, True, False, False]
State prediction error at timestep 157 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 157 of None
Current timestep = 158. State = [[-0.17748776 -0.1373541 ]]. Action = [[-0.01113888  0.00889014  0.         -0.81470823]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, True, False, False]
State prediction error at timestep 158 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 158 of None
Current timestep = 159. State = [[-0.17730966 -0.1318122 ]]. Action = [[ 0.00324055  0.09510059  0.         -0.7571435 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, True, False, False]
State prediction error at timestep 159 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 159 of None
Current timestep = 160. State = [[-0.17721584 -0.12892571]]. Action = [[-0.00079836 -0.01528897  0.         -0.19361317]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, True, False, False]
State prediction error at timestep 160 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 160 of None
Current timestep = 161. State = [[-0.17631982 -0.1264225 ]]. Action = [[ 0.01856641  0.03714008  0.         -0.8919489 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, True, False, False]
State prediction error at timestep 161 is tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 161 of None
Current timestep = 162. State = [[-0.17360185 -0.12400767]]. Action = [[ 0.04707891  0.00530438  0.         -0.34793043]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 162 of None
Current timestep = 163. State = [[-0.17217927 -0.12381842]]. Action = [[ 0.00273812 -0.02125858  0.          0.22714484]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 163 of None
Current timestep = 164. State = [[-0.17589608 -0.12160581]]. Action = [[-0.0837265   0.04268847  0.         -0.5530472 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 164 of None
Current timestep = 165. State = [[-0.18008384 -0.1221605 ]]. Action = [[-0.04095819 -0.04755416  0.          0.710667  ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 165 of None
Current timestep = 166. State = [[-0.18015839 -0.12681702]]. Action = [[ 0.02244838 -0.07530906  0.          0.04876173]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, True, False, False]
State prediction error at timestep 166 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 166 of None
Current timestep = 167. State = [[-0.18141024 -0.1258222 ]]. Action = [[-0.03968007  0.0636952   0.         -0.07502997]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, True, False, False]
State prediction error at timestep 167 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 167 of None
Current timestep = 168. State = [[-0.17828114 -0.12835516]]. Action = [[ 0.09731426 -0.09170576  0.          0.10280859]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, True, False, False]
State prediction error at timestep 168 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 168 of None
Current timestep = 169. State = [[-0.17069711 -0.13226652]]. Action = [[ 0.09577601 -0.0221171   0.         -0.96921426]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, True, False, False]
State prediction error at timestep 169 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 169 of None
Current timestep = 170. State = [[-0.16354159 -0.1377996 ]]. Action = [[ 0.07780256 -0.08372957  0.         -0.8996382 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, True, False, False]
State prediction error at timestep 170 is tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 170 of None
Current timestep = 171. State = [[-0.16403943 -0.13912414]]. Action = [[-0.08767621  0.05081899  0.         -0.95316285]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, True, False, False]
State prediction error at timestep 171 is tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 171 of None
Current timestep = 172. State = [[-0.16255262 -0.1416017 ]]. Action = [[ 0.08241279 -0.06146271  0.          0.7341882 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, True, False, False]
State prediction error at timestep 172 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 172 of None
Current timestep = 173. State = [[-0.16130605 -0.14768581]]. Action = [[-0.02979808 -0.06459431  0.          0.68503535]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, True, False, False]
State prediction error at timestep 173 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 173 of None
Current timestep = 174. State = [[-0.16143858 -0.1544118 ]]. Action = [[-0.00428994 -0.06474148  0.         -0.9585862 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, True, False, False]
State prediction error at timestep 174 is tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 174 of None
Current timestep = 175. State = [[-0.16124834 -0.15331414]]. Action = [[-0.00858837  0.09524328  0.         -0.9053191 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, True, False, False]
State prediction error at timestep 175 is tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 175 of None
Current timestep = 176. State = [[-0.15921879 -0.15184827]]. Action = [[ 3.5827689e-02 -4.1664392e-04  0.0000000e+00 -6.7738444e-01]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, True, False, False]
State prediction error at timestep 176 is tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 176 of None
Current timestep = 177. State = [[-0.15620872 -0.15235552]]. Action = [[ 0.03305223  0.00249888  0.         -0.2402196 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, True, False, False]
State prediction error at timestep 177 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 177 of None
Current timestep = 178. State = [[-0.15910229 -0.15572979]]. Action = [[-0.09001112 -0.05458039  0.         -0.16368663]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, True, False, False]
State prediction error at timestep 178 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 178 of None
Current timestep = 179. State = [[-0.15992647 -0.16006799]]. Action = [[ 0.02207436 -0.03756168  0.          0.8393779 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, True, False, False]
State prediction error at timestep 179 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 179 of None
Current timestep = 180. State = [[-0.15688512 -0.15790798]]. Action = [[ 0.0446971   0.08004446  0.         -0.91132325]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, True, False, False]
State prediction error at timestep 180 is tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 180 of None
Current timestep = 181. State = [[-0.15148467 -0.15727104]]. Action = [[ 0.08200116 -0.0299238   0.         -0.91764426]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, True, False, False]
State prediction error at timestep 181 is tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 181 of None
Current timestep = 182. State = [[-0.14807618 -0.15501536]]. Action = [[ 0.01850939  0.06111563  0.         -0.17120665]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, True, False, False]
State prediction error at timestep 182 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 182 of None
Current timestep = 183. State = [[-0.1503008  -0.14841369]]. Action = [[-0.06275274  0.09559447  0.         -0.17459524]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, True, False, False]
State prediction error at timestep 183 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 183 of None
Current timestep = 184. State = [[-0.15614532 -0.14595054]]. Action = [[-0.0894238  -0.01610518  0.          0.38157284]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, True, False, False]
State prediction error at timestep 184 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 184 of None
Current timestep = 185. State = [[-0.16309932 -0.15019141]]. Action = [[-0.09550122 -0.08684864  0.         -0.28130263]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, True, False, False]
State prediction error at timestep 185 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 185 of None
Current timestep = 186. State = [[-0.1617991  -0.15343854]]. Action = [[ 0.08684722 -0.02104642  0.         -0.5656564 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, True, False, False]
State prediction error at timestep 186 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 186 of None
Current timestep = 187. State = [[-0.1626489  -0.15148222]]. Action = [[-0.0736371   0.05216948  0.         -0.32180786]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, True, False, False]
State prediction error at timestep 187 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 187 of None
Current timestep = 188. State = [[-0.16439761 -0.14531045]]. Action = [[0.00998636 0.08479091 0.         0.74505746]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, True, False, False]
State prediction error at timestep 188 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 188 of None
Current timestep = 189. State = [[-0.16687442 -0.13958317]]. Action = [[-0.04081595  0.04607918  0.          0.41192472]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, True, False, False]
State prediction error at timestep 189 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 189 of None
Current timestep = 190. State = [[-0.17196244 -0.14175296]]. Action = [[-0.06706262 -0.09120143  0.          0.31058538]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, True, False, False]
State prediction error at timestep 190 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 190 of None
Current timestep = 191. State = [[-0.1720679  -0.13992967]]. Action = [[ 0.05394008  0.0766312   0.         -0.84216917]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, True, False, False]
State prediction error at timestep 191 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 191 of None
Current timestep = 192. State = [[-0.17007163 -0.13529651]]. Action = [[0.03140029 0.03012384 0.         0.36797857]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, True, False, False]
State prediction error at timestep 192 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 192 of None
Current timestep = 193. State = [[-0.16978692 -0.12848854]]. Action = [[ 0.00656237  0.09354534  0.         -0.36648762]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, True, False, False]
State prediction error at timestep 193 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 193 of None
Current timestep = 194. State = [[-0.16781454 -0.12655236]]. Action = [[ 0.05708151 -0.04371686  0.          0.47513294]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, True, False, False]
State prediction error at timestep 194 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 194 of None
Current timestep = 195. State = [[-0.16782583 -0.12442905]]. Action = [[-0.01779375  0.04235553  0.         -0.8609843 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 195 of None
Current timestep = 196. State = [[-0.16602357 -0.12658134]]. Action = [[ 0.05932758 -0.08730996  0.          0.4440391 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, True, False, False]
State prediction error at timestep 196 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 196 of None
Current timestep = 197. State = [[-0.16438141 -0.12946422]]. Action = [[ 0.00450791 -0.01971347  0.         -0.46942592]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, True, False, False]
State prediction error at timestep 197 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 197 of None
Current timestep = 198. State = [[-0.16320781 -0.13237536]]. Action = [[ 0.01796357 -0.04709416  0.         -0.09800684]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, True, False, False]
State prediction error at timestep 198 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 198 of None
Current timestep = 199. State = [[-0.16528031 -0.13224012]]. Action = [[-0.06066329  0.03592093  0.         -0.92163295]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, True, False, False]
State prediction error at timestep 199 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 199 of None
Current timestep = 200. State = [[-0.16308592 -0.1298561 ]]. Action = [[0.0766081  0.03129532 0.         0.49557257]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, True, False, False]
State prediction error at timestep 200 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 200 of None
Current timestep = 201. State = [[-0.15881953 -0.12540364]]. Action = [[ 0.04170042  0.06818094  0.         -0.9016541 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, True, False, False]
State prediction error at timestep 201 is tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 201 of None
Current timestep = 202. State = [[-0.15348615 -0.12461194]]. Action = [[ 0.07982295 -0.03120009  0.         -0.01796281]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 202 of None
Current timestep = 203. State = [[-0.14709911 -0.12181418]]. Action = [[ 0.07860499  0.06573229  0.         -0.69627917]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 203 of None
Current timestep = 204. State = [[-0.14658117 -0.11799689]]. Action = [[-0.04576306  0.03109819  0.         -0.34049845]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 204 of None
Current timestep = 205. State = [[-0.147107   -0.11763189]]. Action = [[ 0.00085052 -0.0227257   0.          0.06691444]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 205 of None
Current timestep = 206. State = [[-0.1506425  -0.11525325]]. Action = [[-0.08906895  0.05045999  0.         -0.15999377]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 206 of None
Current timestep = 207. State = [[-0.15512384 -0.11562476]]. Action = [[-0.0588933  -0.04783098  0.         -0.07499313]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 207 of None
Current timestep = 208. State = [[-0.16035646 -0.11201043]]. Action = [[-0.08636668  0.08767145  0.         -0.7197887 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 208 of None
Current timestep = 209. State = [[-0.16042021 -0.11334401]]. Action = [[ 0.05153228 -0.09572721  0.         -0.23051631]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 209 of None
Current timestep = 210. State = [[-0.1573447  -0.11462112]]. Action = [[ 0.021734    0.01939516  0.         -0.9660799 ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 210 of None
Current timestep = 211. State = [[-0.1517947 -0.1186257]]. Action = [[ 0.08753536 -0.09383938  0.         -0.7511918 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 211 of None
Current timestep = 212. State = [[-0.14810584 -0.1200159 ]]. Action = [[ 0.01020353  0.03045822  0.         -0.15673643]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 212 of None
Current timestep = 213. State = [[-0.14492624 -0.11856889]]. Action = [[ 0.04492814  0.01952668  0.         -0.0274756 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 213 of None
Current timestep = 214. State = [[-0.14543083 -0.12059633]]. Action = [[-0.04797783 -0.04706627  0.         -0.8874194 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 214 of None
Current timestep = 215. State = [[-0.14275788 -0.12383542]]. Action = [[ 0.06835457 -0.02792741  0.         -0.8747029 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 215 of None
Current timestep = 216. State = [[-0.13692202 -0.12760039]]. Action = [[ 0.06969178 -0.04537316  0.         -0.48320687]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, True, False, False]
State prediction error at timestep 216 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 216 of None
Current timestep = 217. State = [[-0.13793874 -0.12693758]]. Action = [[-0.08729779  0.06075687  0.          0.62777877]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, True, False, False]
State prediction error at timestep 217 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 217 of None
Current timestep = 218. State = [[-0.14167175 -0.12284006]]. Action = [[-0.03395958  0.05820105  0.         -0.56183386]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 218 of None
Current timestep = 219. State = [[-0.13883501 -0.11938652]]. Action = [[ 0.08842202  0.03041712  0.         -0.7791199 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 219 of None
Current timestep = 220. State = [[-0.13241167 -0.12248179]]. Action = [[ 0.08825677 -0.08891974  0.         -0.8828833 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 220 of None
Current timestep = 221. State = [[-0.12967391 -0.12960869]]. Action = [[-3.9216131e-04 -9.0303965e-02  0.0000000e+00 -9.0667701e-01]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, True, False, False]
State prediction error at timestep 221 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 221 of None
Current timestep = 222. State = [[-0.12520452 -0.13366793]]. Action = [[ 0.0783845  -0.01325782  0.         -0.47893226]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, True, False, False]
State prediction error at timestep 222 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 222 of None
Current timestep = 223. State = [[-0.12180389 -0.13056546]]. Action = [[ 0.01376997  0.08796317  0.         -0.98658085]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, True, False, False]
State prediction error at timestep 223 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 223 of None
Current timestep = 224. State = [[-0.11772322 -0.12359157]]. Action = [[ 0.06568214  0.09662368  0.         -0.9814874 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 224 of None
Current timestep = 225. State = [[-0.11175412 -0.12329891]]. Action = [[ 0.07810546 -0.05605414  0.         -0.9116442 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 225 of None
Current timestep = 226. State = [[-0.10770214 -0.12100485]]. Action = [[ 0.02440158  0.07289266  0.         -0.70836556]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 226 of None
Current timestep = 227. State = [[-0.10280323 -0.11375333]]. Action = [[ 0.07269251  0.09512161  0.         -0.8798352 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
State prediction error at timestep 227 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 227 of None
Current timestep = 228. State = [[-0.10175988 -0.11367599]]. Action = [[-0.03612902 -0.07547991  0.         -0.92198867]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 228 of None
Current timestep = 229. State = [[-0.09711868 -0.11605868]]. Action = [[ 0.09056532 -0.019853    0.         -0.66732836]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(0.0085, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 229 of None
Current timestep = 230. State = [[-0.09495109 -0.11965802]]. Action = [[-0.03603417 -0.06595498  0.          0.6482564 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 230 of None
Current timestep = 231. State = [[-0.09536769 -0.1189028 ]]. Action = [[-0.02589919  0.0526622   0.         -0.31883043]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 231 of None
Current timestep = 232. State = [[-0.09344714 -0.12116462]]. Action = [[ 0.02196995 -0.07633873  0.         -0.7025927 ]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
State prediction error at timestep 232 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 232 of None
Current timestep = 233. State = [[-0.09388387 -0.1276313 ]]. Action = [[-0.05558281 -0.08173689  0.         -0.6128802 ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, True, False, False]
State prediction error at timestep 233 is tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 233 of None
Current timestep = 234. State = [[-0.09749436 -0.1301453 ]]. Action = [[-0.08147253  0.01356877  0.         -0.25118566]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, True, False, False]
State prediction error at timestep 234 is tensor(0.0076, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 234 of None
Current timestep = 235. State = [[-0.0959801  -0.12813494]]. Action = [[ 0.05166782  0.04652192  0.         -0.9584807 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, True, False, False]
State prediction error at timestep 235 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 235 of None
Current timestep = 236. State = [[-0.09177445 -0.13159607]]. Action = [[ 0.03860941 -0.08854376  0.          0.77237105]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, True, False, False]
State prediction error at timestep 236 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 236 of None
Current timestep = 237. State = [[-0.08785226 -0.1299776 ]]. Action = [[ 0.04075079  0.09514409  0.         -0.44616377]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, True, False, False]
State prediction error at timestep 237 is tensor(0.0087, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 237 of None
Current timestep = 238. State = [[-0.08617593 -0.12726302]]. Action = [[-0.00191407  0.006726    0.          0.05251479]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, True, False, False]
State prediction error at timestep 238 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 238 of None
Current timestep = 239. State = [[-0.08271077 -0.12484033]]. Action = [[ 0.06249734  0.04261472  0.         -0.39751267]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
State prediction error at timestep 239 is tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 239 of None
Current timestep = 240. State = [[-0.07819421 -0.12309613]]. Action = [[ 0.0534177   0.00535719  0.         -0.18938398]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(0.0094, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 240 of None
Current timestep = 241. State = [[-0.07260642 -0.12459502]]. Action = [[ 0.07849348 -0.03903358  0.          0.11425495]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
State prediction error at timestep 241 is tensor(0.0080, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 241 of None
Current timestep = 242. State = [[-0.07327331 -0.12281477]]. Action = [[-0.07928754  0.0601706   0.         -0.75666744]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
State prediction error at timestep 242 is tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 242 of None
Current timestep = 243. State = [[-0.07621196 -0.12510502]]. Action = [[-0.02245636 -0.08554407  0.         -0.56010056]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, True, False, False]
State prediction error at timestep 243 is tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 243 of None
Current timestep = 244. State = [[-0.07933091 -0.12692016]]. Action = [[-0.05432862  0.00989646  0.         -0.91395444]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, True, False, False]
State prediction error at timestep 244 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 244 of None
Current timestep = 245. State = [[-0.07856935 -0.12784502]]. Action = [[ 0.04419396 -0.02299352  0.         -0.88941836]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, True, False, False]
State prediction error at timestep 245 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 245 of None
Current timestep = 246. State = [[-0.07889704 -0.12513839]]. Action = [[-0.03340079  0.06752204  0.         -0.5340969 ]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, True, False, False]
State prediction error at timestep 246 is tensor(0.0087, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 246 of None
Current timestep = 247. State = [[-0.08156182 -0.11845809]]. Action = [[-0.0372306   0.08939011  0.         -0.5996798 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
State prediction error at timestep 247 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 247 of None
Current timestep = 248. State = [[-0.08103497 -0.11518127]]. Action = [[ 0.03807179 -0.00397983  0.         -0.16885108]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(0.0083, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 248 of None
Current timestep = 249. State = [[-0.07971642 -0.11718803]]. Action = [[ 0.01185431 -0.05619736  0.         -0.9152822 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
State prediction error at timestep 249 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 249 of None
Current timestep = 250. State = [[-0.08151709 -0.12093799]]. Action = [[-0.04437668 -0.04995789  0.         -0.4438148 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, True, False]
State prediction error at timestep 250 is tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 250 of None
Current timestep = 251. State = [[-0.07847848 -0.12282401]]. Action = [[ 0.08941735 -0.00826125  0.          0.79118454]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, True, False]
State prediction error at timestep 251 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 251 of None
Current timestep = 252. State = [[-0.07199185 -0.11867189]]. Action = [[ 0.08880136  0.09011982  0.         -0.8408416 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 252 of None
Current timestep = 253. State = [[-0.07149649 -0.11118452]]. Action = [[-0.04087759  0.09437845  0.         -0.9426048 ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, True, False]
State prediction error at timestep 253 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 253 of None
Current timestep = 254. State = [[-0.0736263  -0.10740632]]. Action = [[-0.01887534  0.00470914  0.         -0.88149875]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
State prediction error at timestep 254 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 254 of None
Current timestep = 255. State = [[-0.06971951 -0.10859825]]. Action = [[ 0.09957046 -0.04670478  0.         -0.9585923 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, True, False]
State prediction error at timestep 255 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 255 of None
Current timestep = 256. State = [[-0.06995042 -0.10597369]]. Action = [[-0.07462709  0.07112799  0.         -0.12121713]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 256 of None
Current timestep = 257. State = [[-0.07457298 -0.10232142]]. Action = [[-0.05257046  0.01552172  0.         -0.6954086 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 257 of None
Current timestep = 258. State = [[-0.07982037 -0.10004878]]. Action = [[-0.07234057  0.01344629  0.         -0.10722435]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
State prediction error at timestep 258 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 258 of None
Current timestep = 259. State = [[-0.08097588 -0.10088588]]. Action = [[ 0.02452465 -0.04589802  0.         -0.43979377]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
State prediction error at timestep 259 is tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 259 of None
Current timestep = 260. State = [[-0.0823664  -0.10477456]]. Action = [[-0.03729273 -0.06448126  0.          0.3881147 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 260 of None
Current timestep = 261. State = [[-0.08415487 -0.10973743]]. Action = [[-0.01674557 -0.06308734  0.          0.35896087]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, True, False]
State prediction error at timestep 261 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 261 of None
Current timestep = 262. State = [[-0.08299139 -0.11441307]]. Action = [[ 0.0338257  -0.04755974  0.          0.27368164]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, True, False]
State prediction error at timestep 262 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 262 of None
Current timestep = 263. State = [[-0.07760362 -0.11291598]]. Action = [[ 0.09825271  0.07306144  0.         -0.8265477 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 263 of None
Current timestep = 264. State = [[-0.07584672 -0.10769747]]. Action = [[-0.01077856  0.07376336  0.         -0.07547539]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, True, False]
State prediction error at timestep 264 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 264 of None
Current timestep = 265. State = [[-0.07700337 -0.1016254 ]]. Action = [[-0.00984494  0.07857186  0.          0.02294111]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 265 of None
Current timestep = 266. State = [[-0.07658838 -0.09775688]]. Action = [[0.02622204 0.02156143 0.         0.24346161]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 266 of None
Current timestep = 267. State = [[-0.07773773 -0.09491994]]. Action = [[-0.02837231  0.02925659  0.         -0.60072744]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
State prediction error at timestep 267 is tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 267 of None
Current timestep = 268. State = [[-0.07759653 -0.09816907]]. Action = [[ 0.02398013 -0.09731371  0.         -0.15434766]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
State prediction error at timestep 268 is tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 268 of None
Current timestep = 269. State = [[-0.08063384 -0.10396546]]. Action = [[-0.0789359  -0.06632335  0.          0.03132367]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 269 of None
Current timestep = 270. State = [[-0.07901564 -0.10323166]]. Action = [[ 0.07775206  0.05648781  0.         -0.80053717]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
State prediction error at timestep 270 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 270 of None
Current timestep = 271. State = [[-0.0789854  -0.09759409]]. Action = [[-0.0418909   0.07929084  0.         -0.29894078]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
State prediction error at timestep 271 is tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 271 of None
Current timestep = 272. State = [[-0.08075091 -0.09698773]]. Action = [[-0.01402382 -0.04455657  0.          0.20696151]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
State prediction error at timestep 272 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 272 of None
Current timestep = 273. State = [[-0.07902152 -0.10130339]]. Action = [[ 0.04417313 -0.06809179  0.          0.85092926]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
State prediction error at timestep 273 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 273 of None
Current timestep = 274. State = [[-0.07481825 -0.10403358]]. Action = [[ 0.06232005 -0.01075452  0.         -0.6231805 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 274 of None
Current timestep = 275. State = [[-0.07250135 -0.10970042]]. Action = [[ 0.00882664 -0.0961043   0.         -0.64135456]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 275 of None
Current timestep = 276. State = [[-0.06880277 -0.11284747]]. Action = [[0.06254917 0.01142285 0.         0.3280499 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 276 of None
Current timestep = 277. State = [[-0.06730445 -0.10909937]]. Action = [[-0.01073998  0.09050272  0.         -0.04665679]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, True, False]
State prediction error at timestep 277 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 277 of None
Current timestep = 278. State = [[-0.0712785  -0.10215452]]. Action = [[-0.08570568  0.09609223  0.         -0.8342241 ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 278 of None
Current timestep = 279. State = [[-0.06953257 -0.10291668]]. Action = [[ 0.09345431 -0.08052297  0.         -0.92570287]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, True, False]
State prediction error at timestep 279 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 279 of None
Current timestep = 280. State = [[-0.06614398 -0.10801713]]. Action = [[-0.0013587  -0.05469605  0.         -0.37102276]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, True, False]
State prediction error at timestep 280 is tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 280 of None
Current timestep = 281. State = [[-0.06785389 -0.10703381]]. Action = [[-0.06680836  0.0620558   0.         -0.8225328 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 281 of None
Current timestep = 282. State = [[-0.06783619 -0.10410515]]. Action = [[0.01259848 0.02437731 0.         0.13708079]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, True, False]
State prediction error at timestep 282 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 282 of None
Current timestep = 283. State = [[-0.06789996 -0.09914204]]. Action = [[-0.0250494   0.07598232  0.         -0.96977854]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 283 of None
Current timestep = 284. State = [[-0.07146994 -0.09756666]]. Action = [[-0.07575284 -0.02814543  0.         -0.8902079 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 284 of None
Current timestep = 285. State = [[-0.06879434 -0.10139219]]. Action = [[ 0.09565008 -0.07804678  0.          0.70493805]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, True, False]
State prediction error at timestep 285 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 285 of None
Current timestep = 286. State = [[-0.06891967 -0.09938551]]. Action = [[-0.08442501  0.0850196   0.          0.706267  ]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, True, False]
State prediction error at timestep 286 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 286 of None
Current timestep = 287. State = [[-0.07444139 -0.09398496]]. Action = [[-0.07648765  0.04874492  0.         -0.36686844]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 287 of None
Current timestep = 288. State = [[-0.07375656 -0.08829901]]. Action = [[ 0.06920148  0.06112327  0.         -0.7576747 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
State prediction error at timestep 288 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 288 of None
Current timestep = 289. State = [[-0.06927942 -0.08963893]]. Action = [[ 0.06573725 -0.08649232  0.          0.0662601 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
State prediction error at timestep 289 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 289 of None
Current timestep = 290. State = [[-0.07090641 -0.09261093]]. Action = [[-0.07681118 -0.02102212  0.         -0.6566087 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 290 of None
Current timestep = 291. State = [[-0.07585315 -0.09605452]]. Action = [[-0.0580017  -0.06023573  0.         -0.6094841 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 291 of None
Current timestep = 292. State = [[-0.07821991 -0.09768245]]. Action = [[-0.00926689  0.00316113  0.         -0.1858362 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 292 of None
Current timestep = 293. State = [[-0.07967516 -0.10134121]]. Action = [[-0.01577621 -0.06921993  0.         -0.9212457 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 293 of None
Current timestep = 294. State = [[-0.07613061 -0.10757962]]. Action = [[ 0.09351457 -0.07241698  0.          0.12498856]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, True, False]
State prediction error at timestep 294 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 294 of None
Current timestep = 295. State = [[-0.07458556 -0.10827214]]. Action = [[-0.0138489   0.05249805  0.         -0.96284735]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, True, False]
State prediction error at timestep 295 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 295 of None
Current timestep = 296. State = [[-0.07139631 -0.11238738]]. Action = [[ 0.08568268 -0.09083103  0.          0.5596614 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, True, False]
State prediction error at timestep 296 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 296 of None
Current timestep = 297. State = [[-0.06979098 -0.11201414]]. Action = [[-0.0094654   0.08573849  0.          0.20355439]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 297 of None
Current timestep = 298. State = [[-0.06790879 -0.10676375]]. Action = [[ 0.05543529  0.07574027  0.         -0.08405113]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 298 of None
Current timestep = 299. State = [[-0.06644528 -0.10853694]]. Action = [[ 0.01275221 -0.07359684  0.         -0.95834327]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 299 of None
Current timestep = 300. State = [[-0.07044248 -0.10771839]]. Action = [[-0.09188253  0.06810571  0.         -0.86397994]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, True, False]
State prediction error at timestep 300 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 300 of None
Current timestep = 301. State = [[-0.06982242 -0.10844053]]. Action = [[ 0.0816297  -0.0525126   0.          0.04302537]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, True, False]
State prediction error at timestep 301 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 301 of None
Current timestep = 302. State = [[-0.07127889 -0.10816055]]. Action = [[-0.08639731  0.04129194  0.         -0.64740026]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 302 of None
Current timestep = 303. State = [[-0.07191429 -0.1048737 ]]. Action = [[0.03486221 0.04113758 0.         0.13628578]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, True, False]
State prediction error at timestep 303 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 303 of None
Current timestep = 304. State = [[-0.07236037 -0.10480254]]. Action = [[-0.02334759 -0.03109773  0.         -0.8594745 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, True, False]
State prediction error at timestep 304 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 304 of None
Current timestep = 305. State = [[-0.07408246 -0.10046863]]. Action = [[-0.02179058  0.09622443  0.         -0.09737033]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 305 of None
Current timestep = 306. State = [[-0.07254257 -0.09809271]]. Action = [[ 0.05113504 -0.02282911  0.          0.322621  ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, True, False]
State prediction error at timestep 306 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 306 of None
Current timestep = 307. State = [[-0.06679458 -0.09527321]]. Action = [[ 0.09913371  0.05133148  0.         -0.6141987 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, True, False]
State prediction error at timestep 307 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 307 of None
Current timestep = 308. State = [[-0.06574693 -0.09701783]]. Action = [[-0.03049731 -0.07833579  0.         -0.85786146]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, True, False]
State prediction error at timestep 308 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 308 of None
Current timestep = 309. State = [[-0.06982157 -0.099775  ]]. Action = [[-0.07578351 -0.01784189  0.          0.96148634]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, True, False]
State prediction error at timestep 309 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 309 of None
Current timestep = 310. State = [[-0.06845433 -0.09628151]]. Action = [[ 0.06788141  0.0775881   0.         -0.24912238]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, True, False]
State prediction error at timestep 310 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 310 of None
Current timestep = 311. State = [[-0.06700291 -0.09331838]]. Action = [[-0.00876398  0.00552112  0.          0.5992118 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 311 of None
Current timestep = 312. State = [[-0.06390388 -0.09397099]]. Action = [[ 0.06263942 -0.02738083  0.         -0.995379  ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(6.5755e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 312 of None
Current timestep = 313. State = [[-0.06196669 -0.09211253]]. Action = [[-1.5832484e-05  4.7849543e-02  0.0000000e+00 -9.8492742e-01]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(4.6140e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 313 of None
Current timestep = 314. State = [[-0.0572894  -0.08704426]]. Action = [[0.09259333 0.06622834 0.         0.5162505 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
State prediction error at timestep 314 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 314 of None
Current timestep = 315. State = [[-0.05044387 -0.08240774]]. Action = [[ 0.09090281  0.04024484  0.         -0.9352383 ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 315 of None
Current timestep = 316. State = [[-0.18604495 -0.03221656]]. Action = [[0.01185153 0.09014822 0.         0.49807823]]. Reward = [100.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
State prediction error at timestep 316 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 316 of None
Current timestep = 317. State = [[-0.1934449  -0.03317354]]. Action = [[-0.07373514  0.035932    0.          0.6331867 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 317 of None
Current timestep = 318. State = [[-0.19408157 -0.02741897]]. Action = [[ 0.06983908  0.09250028  0.         -0.78581196]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 318 of None
Current timestep = 319. State = [[-0.19229497 -0.02905893]]. Action = [[ 0.03759862 -0.0954158   0.         -0.8149729 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 319 is [True, False, False, False, True, False]
State prediction error at timestep 319 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 319 of None
Current timestep = 320. State = [[-0.1925582  -0.03534839]]. Action = [[-0.00234128 -0.06812812  0.         -0.82736313]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 320 is [True, False, False, False, True, False]
State prediction error at timestep 320 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 320 of None
Current timestep = 321. State = [[-0.19059803 -0.03669779]]. Action = [[ 0.06181519  0.02952261  0.         -0.7082916 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 321 is [True, False, False, False, True, False]
State prediction error at timestep 321 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 321 of None
Current timestep = 322. State = [[-0.19285886 -0.03229027]]. Action = [[-0.06012329  0.08797897  0.         -0.0436061 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 322 is [True, False, False, False, True, False]
State prediction error at timestep 322 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 322 of None
Current timestep = 323. State = [[-0.19759028 -0.02529373]]. Action = [[-0.03237077  0.09400576  0.         -0.65993226]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 323 is [True, False, False, False, True, False]
State prediction error at timestep 323 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 323 of None
Current timestep = 324. State = [[-0.19594637 -0.02428262]]. Action = [[ 0.09405703 -0.0414823   0.         -0.3707137 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 324 of None
Current timestep = 325. State = [[-0.19508158 -0.02045687]]. Action = [[-0.00761622  0.09413958  0.         -0.4225403 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 325 is [True, False, False, False, True, False]
State prediction error at timestep 325 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 325 of None
Current timestep = 326. State = [[-0.19825706 -0.02073514]]. Action = [[-0.04113787 -0.06863061  0.         -0.89098907]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 326 of None
Current timestep = 327. State = [[-0.20224468 -0.0188073 ]]. Action = [[-0.04413927  0.06716467  0.          0.3922112 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 327 is [True, False, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 327 of None
Current timestep = 328. State = [[-0.20311321 -0.01704209]]. Action = [[ 0.02435204 -0.01461549  0.         -0.9379036 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 328 is [True, False, False, False, True, False]
State prediction error at timestep 328 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 328 of None
Current timestep = 329. State = [[-0.19946484 -0.01660273]]. Action = [[ 0.07696142  0.00361027  0.         -0.24558246]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 329 is [True, False, False, False, True, False]
State prediction error at timestep 329 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 329 of None
Current timestep = 330. State = [[-0.20178531 -0.01470437]]. Action = [[-0.08968778  0.02715828  0.         -0.97710234]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 330 is [True, False, False, False, True, False]
State prediction error at timestep 330 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 330 of None
Current timestep = 331. State = [[-0.20682015 -0.01248803]]. Action = [[-0.03966068  0.01607104  0.          0.2875929 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 331 of None
Current timestep = 332. State = [[-0.20734   -0.0064551]]. Action = [[ 0.03598507  0.09611126  0.         -0.49144018]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 332 is [True, False, False, False, True, False]
State prediction error at timestep 332 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 332 of None
Current timestep = 333. State = [[-2.0690946e-01 -1.1433615e-04]]. Action = [[ 0.01770635  0.05103337  0.         -0.9859259 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 333 of None
Current timestep = 334. State = [[-0.2091163   0.00491456]]. Action = [[-0.03104451  0.04515808  0.         -0.8364887 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 334 is [True, False, False, False, True, False]
State prediction error at timestep 334 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 334 of None
Current timestep = 335. State = [[-0.21372278  0.00933472]]. Action = [[-0.05569751  0.0340815   0.         -0.8766092 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 335 is [True, False, False, False, True, False]
State prediction error at timestep 335 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 335 of None
Current timestep = 336. State = [[-0.21418758  0.01155927]]. Action = [[ 0.04369695 -0.00549036  0.         -0.4613874 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 336 of None
Current timestep = 337. State = [[-0.21828209  0.01733153]]. Action = [[-0.09220613  0.08986337  0.         -0.72134674]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 337 is [True, False, False, False, True, False]
State prediction error at timestep 337 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 337 of None
Current timestep = 338. State = [[-0.22176231  0.02574595]]. Action = [[ 0.00639872  0.08406541  0.         -0.8329116 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 338 is [True, False, False, False, True, False]
State prediction error at timestep 338 is tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 338 of None
Current timestep = 339. State = [[-0.22698136  0.02950851]]. Action = [[-0.0807749  -0.01257744  0.         -0.95212734]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 339 is [True, False, False, False, True, False]
State prediction error at timestep 339 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 339 of None
Current timestep = 340. State = [[-0.23427822  0.03310697]]. Action = [[-0.08109045  0.03940845  0.         -0.8274143 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 340 is [True, False, False, False, True, False]
State prediction error at timestep 340 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 340 of None
Current timestep = 341. State = [[-0.23360257  0.03118879]]. Action = [[ 0.09262551 -0.09472864  0.         -0.88172936]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 341 of None
Current timestep = 342. State = [[-0.2362441   0.03310738]]. Action = [[-0.09712332  0.07019674  0.         -0.6871707 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 342 of None
Current timestep = 343. State = [[-0.2355367   0.03642889]]. Action = [[ 0.09542974  0.00679877  0.         -0.7911701 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 343 of None
Current timestep = 344. State = [[-0.23396546  0.04065847]]. Action = [[-0.00492574  0.0619627   0.         -0.91450304]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 344 of None
Current timestep = 345. State = [[-0.23055637  0.04747154]]. Action = [[ 0.08516229  0.08394561  0.         -0.86286813]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 345 is [True, False, False, False, True, False]
State prediction error at timestep 345 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 345 of None
Current timestep = 346. State = [[-0.22666588  0.05047208]]. Action = [[ 0.04471678 -0.0057978   0.         -0.99193394]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 346 is [True, False, False, False, True, False]
State prediction error at timestep 346 is tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 346 of None
Current timestep = 347. State = [[-0.22107932  0.05018592]]. Action = [[ 0.0879839  -0.01781151  0.         -0.94075984]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 347 is [True, False, False, False, True, False]
State prediction error at timestep 347 is tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 347 of None
Current timestep = 348. State = [[-0.21762526  0.05446697]]. Action = [[ 0.01353999  0.08718143  0.         -0.4362285 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 348 is [True, False, False, False, True, False]
State prediction error at timestep 348 is tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 348 of None
Current timestep = 349. State = [[-0.2136817   0.05273863]]. Action = [[ 0.05863182 -0.09838398  0.         -0.5915705 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 349 is [True, False, False, False, True, False]
State prediction error at timestep 349 is tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 349 of None
Current timestep = 350. State = [[-0.21158908  0.04845076]]. Action = [[-0.01903856 -0.03954691  0.          0.14760208]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 350 is [True, False, False, False, True, False]
State prediction error at timestep 350 is tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 350 of None
Current timestep = 351. State = [[-0.20626143  0.05196981]]. Action = [[ 0.09104259  0.09367571  0.         -0.7939128 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 351 is [True, False, False, False, True, False]
State prediction error at timestep 351 is tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 351 of None
Current timestep = 352. State = [[-0.1993609   0.05708737]]. Action = [[ 0.06346586  0.04724199  0.         -0.49365246]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 352 is [True, False, False, False, True, False]
State prediction error at timestep 352 is tensor(0.0077, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 352 of None
Current timestep = 353. State = [[-0.20023598  0.06349615]]. Action = [[-0.08867712  0.08914562  0.         -0.3189423 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0095, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 353 of None
Current timestep = 354. State = [[-0.20257032  0.06575321]]. Action = [[-0.02080842 -0.02907237  0.         -0.6844312 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 354 is [True, False, False, False, True, False]
State prediction error at timestep 354 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 354 of None
Current timestep = 355. State = [[-0.19809462  0.07033327]]. Action = [[0.0948169  0.08517284 0.         0.12938285]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(0.0111, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 355 of None
Current timestep = 356. State = [[-0.19693826  0.0753085 ]]. Action = [[-0.04757467  0.02716831  0.         -0.13715506]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 356 is [True, False, False, False, True, False]
State prediction error at timestep 356 is tensor(0.0116, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 356 of None
Current timestep = 357. State = [[-0.19549543  0.07985259]]. Action = [[ 0.04282906  0.04446075  0.         -0.965465  ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 357 is [True, False, False, False, True, False]
State prediction error at timestep 357 is tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 357 of None
Current timestep = 358. State = [[-0.19011408  0.08550338]]. Action = [[ 0.08161964  0.06033658  0.         -0.9477744 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0095, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 358 of None
Current timestep = 359. State = [[-0.18415466  0.08800472]]. Action = [[ 0.06708188 -0.00977646  0.          0.54889464]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 359 is [True, False, False, False, True, False]
State prediction error at timestep 359 is tensor(0.0125, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 359 of None
Current timestep = 360. State = [[-0.17974466  0.09250002]]. Action = [[ 0.03508932  0.07189072  0.         -0.00216669]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 360 is [True, False, False, False, True, False]
State prediction error at timestep 360 is tensor(0.0133, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 360 of None
Current timestep = 361. State = [[-0.1736311   0.09751624]]. Action = [[0.08829207 0.03698904 0.         0.72948015]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0142, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 361 of None
Current timestep = 362. State = [[-0.1681501   0.10172497]]. Action = [[ 0.04163463  0.04022165  0.         -0.7161324 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 362 is [True, False, False, False, True, False]
State prediction error at timestep 362 is tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 362 of None
Current timestep = 363. State = [[-0.17012273  0.10921957]]. Action = [[-0.0919165   0.09788194  0.          0.7330234 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 363 is [True, False, False, False, True, False]
State prediction error at timestep 363 is tensor(0.0144, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 363 of None
Current timestep = 364. State = [[-0.1669867   0.11077838]]. Action = [[ 0.09670176 -0.06280027  0.         -0.85736406]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 364 is [True, False, False, False, True, False]
State prediction error at timestep 364 is tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 364 of None
Current timestep = 365. State = [[-0.15994753  0.10971774]]. Action = [[ 0.0603688  -0.01575383  0.          0.08580399]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 365 is [True, False, False, False, True, False]
State prediction error at timestep 365 is tensor(0.0127, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 365 of None
Current timestep = 366. State = [[-0.15775424  0.11086081]]. Action = [[-0.0298913   0.00985235  0.         -0.87130105]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 366 is [True, False, False, False, True, False]
State prediction error at timestep 366 is tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 366 of None
Current timestep = 367. State = [[-0.15306193  0.11561587]]. Action = [[ 0.07743875  0.06772221  0.         -0.49214935]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 367 is [True, False, False, False, True, False]
State prediction error at timestep 367 is tensor(0.0102, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 367 of None
Current timestep = 368. State = [[-0.15226974  0.12170437]]. Action = [[-0.0621279  0.0587666  0.        -0.7718304]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0099, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 368 of None
Current timestep = 369. State = [[-0.15261117  0.12834458]]. Action = [[ 0.00153668  0.06563554  0.         -0.77121174]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 369 is [True, False, False, False, False, True]
State prediction error at timestep 369 is tensor(0.0104, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 369 of None
Current timestep = 370. State = [[-0.14731982  0.1359743 ]]. Action = [[ 0.09753089  0.08375744  0.         -0.7193367 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 370 is [True, False, False, False, False, True]
State prediction error at timestep 370 is tensor(0.0109, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 370 of None
Current timestep = 371. State = [[-0.14355735  0.14326061]]. Action = [[ 0.01206796  0.06808909  0.         -0.1484226 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 371 is [True, False, False, False, False, True]
State prediction error at timestep 371 is tensor(0.0145, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 371 of None
Current timestep = 372. State = [[-0.13832293  0.14331119]]. Action = [[ 0.08147437 -0.07267924  0.          0.28157055]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 372 is [True, False, False, False, False, True]
State prediction error at timestep 372 is tensor(0.0152, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 372 of None
Current timestep = 373. State = [[-0.13668989  0.14056516]]. Action = [[-0.04483504 -0.04516206  0.         -0.93585247]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 373 is [True, False, False, False, False, True]
State prediction error at timestep 373 is tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 373 of None
Current timestep = 374. State = [[-0.13215403  0.14036626]]. Action = [[8.6417936e-02 1.9238144e-04 0.0000000e+00 2.1290123e-01]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 374 is [True, False, False, False, False, True]
State prediction error at timestep 374 is tensor(0.0142, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 374 of None
Current timestep = 375. State = [[-0.12947431  0.13833345]]. Action = [[-0.03515954 -0.05876913  0.         -0.9239963 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 375 is [True, False, False, False, False, True]
State prediction error at timestep 375 is tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 375 of None
Current timestep = 376. State = [[-0.1312296   0.14043869]]. Action = [[-0.05864388  0.05733129  0.          0.11837578]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 376 is [True, False, False, False, False, True]
State prediction error at timestep 376 is tensor(0.0140, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 376 of None
Current timestep = 377. State = [[-0.12925804  0.14710358]]. Action = [[ 0.05294404  0.08426946  0.         -0.46221733]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 377 is [True, False, False, False, False, True]
State prediction error at timestep 377 is tensor(0.0097, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 377 of None
Current timestep = 378. State = [[-0.1255887   0.15365629]]. Action = [[ 0.03236107  0.06480313  0.         -0.24293602]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 378 is [True, False, False, False, False, True]
State prediction error at timestep 378 is tensor(0.0120, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 378 of None
Current timestep = 379. State = [[-0.126926    0.15997979]]. Action = [[-0.05798434  0.06494939  0.          0.30246925]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 379 is [True, False, False, False, False, True]
State prediction error at timestep 379 is tensor(0.0153, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 379 of None
Current timestep = 380. State = [[-0.13248536  0.16321263]]. Action = [[-0.09123563 -0.0098684   0.         -0.4061725 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 380 is [True, False, False, False, False, True]
State prediction error at timestep 380 is tensor(0.0101, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 380 of None
Current timestep = 381. State = [[-0.13644637  0.1634927 ]]. Action = [[-0.03771004 -0.03331573  0.         -0.8857828 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 381 is [True, False, False, False, False, True]
State prediction error at timestep 381 is tensor(0.0084, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 381 of None
Current timestep = 382. State = [[-0.13661854  0.15953927]]. Action = [[ 0.0098634  -0.09694251  0.          0.15162456]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 382 is [True, False, False, False, False, True]
State prediction error at timestep 382 is tensor(0.0139, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 382 of None
Current timestep = 383. State = [[-0.13662837  0.16246669]]. Action = [[-0.01236192  0.09550025  0.         -0.9462384 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 383 is [True, False, False, False, False, True]
State prediction error at timestep 383 is tensor(0.0080, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 383 of None
Current timestep = 384. State = [[-0.14049107  0.16659607]]. Action = [[-0.06694706  0.00591914  0.          0.3810197 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 384 is [True, False, False, False, False, True]
State prediction error at timestep 384 is tensor(0.0141, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 384 of None
Current timestep = 385. State = [[-0.1426555  0.1649577]]. Action = [[-0.0008333  -0.0654867   0.         -0.63849175]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 385 is [True, False, False, False, False, True]
State prediction error at timestep 385 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 385 of None
Current timestep = 386. State = [[-0.14522536  0.16021651]]. Action = [[-0.05066428 -0.07608692  0.         -0.7467245 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 386 is [True, False, False, False, False, True]
State prediction error at timestep 386 is tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 386 of None
Current timestep = 387. State = [[-0.1501006   0.15916334]]. Action = [[-0.07035406  0.01191242  0.         -0.89853334]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 387 is [True, False, False, False, False, True]
State prediction error at timestep 387 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 387 of None
Current timestep = 388. State = [[-0.15350868  0.16218723]]. Action = [[-0.01628955  0.04753966  0.         -0.31506962]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 388 is [True, False, False, False, False, True]
State prediction error at timestep 388 is tensor(0.0083, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 388 of None
Current timestep = 389. State = [[-0.15160847  0.16075715]]. Action = [[ 0.07088231 -0.05704239  0.          0.32690752]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 389 is [True, False, False, False, False, True]
State prediction error at timestep 389 is tensor(0.0123, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 389 of None
Current timestep = 390. State = [[-0.15150467  0.16015173]]. Action = [[-0.02031896  0.03026891  0.         -0.40286708]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 390 is [True, False, False, False, False, True]
State prediction error at timestep 390 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 390 of None
Current timestep = 391. State = [[-0.14959906  0.15951146]]. Action = [[ 0.06639422 -0.01657367  0.          0.38169467]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 391 is [True, False, False, False, False, True]
State prediction error at timestep 391 is tensor(0.0111, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 391 of None
Current timestep = 392. State = [[-0.15187715  0.16213161]]. Action = [[-0.06622058  0.0770124   0.         -0.8483203 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 392 is [True, False, False, False, False, True]
State prediction error at timestep 392 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 392 of None
Current timestep = 393. State = [[-0.15100949  0.16623692]]. Action = [[ 0.08005103  0.0492112   0.         -0.68046904]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 393 is [True, False, False, False, False, True]
State prediction error at timestep 393 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 393 of None
Current timestep = 394. State = [[-0.1490956   0.16993068]]. Action = [[ 0.018935    0.05533818  0.         -0.05422103]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 394 is [True, False, False, False, False, True]
State prediction error at timestep 394 is tensor(0.0091, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 394 of None
Current timestep = 395. State = [[-0.14805278  0.17152819]]. Action = [[ 0.02647097  0.00491761  0.         -0.25470078]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 395 is [True, False, False, False, False, True]
State prediction error at timestep 395 is tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 395 of None
Current timestep = 396. State = [[-0.1431563  0.1690817]]. Action = [[ 0.09368201 -0.04795967  0.         -0.81853527]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 396 is [True, False, False, False, False, True]
State prediction error at timestep 396 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 396 of None
Current timestep = 397. State = [[-0.1409995   0.16991928]]. Action = [[-0.01036333  0.05367155  0.         -0.57695645]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 397 is [True, False, False, False, False, True]
State prediction error at timestep 397 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 397 of None
Current timestep = 398. State = [[-0.14552015  0.17060286]]. Action = [[-0.09796377 -0.02093121  0.         -0.40949917]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 398 is [True, False, False, False, False, True]
State prediction error at timestep 398 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 398 of None
Current timestep = 399. State = [[-0.148407    0.17064409]]. Action = [[-0.01016746 -0.00275965  0.         -0.51793283]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 399 is [True, False, False, False, False, True]
State prediction error at timestep 399 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 399 of None
Current timestep = 400. State = [[-0.1493581   0.16673926]]. Action = [[-0.01724321 -0.08908166  0.         -0.53926814]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 400 is [True, False, False, False, False, True]
State prediction error at timestep 400 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 400 of None
Current timestep = 401. State = [[-0.14680952  0.16245371]]. Action = [[ 0.05001435 -0.04025743  0.         -0.9934623 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 401 is [True, False, False, False, False, True]
State prediction error at timestep 401 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 401 of None
Current timestep = 402. State = [[-0.1486541  0.1621818]]. Action = [[-0.07908053  0.02167807  0.          0.60197806]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 402 is [True, False, False, False, False, True]
State prediction error at timestep 402 is tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 402 of None
Current timestep = 403. State = [[-0.14797013  0.15871637]]. Action = [[ 0.04718464 -0.08235101  0.         -0.8415303 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 403 is [True, False, False, False, False, True]
State prediction error at timestep 403 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 403 of None
Current timestep = 404. State = [[-0.14277282  0.1564247 ]]. Action = [[ 0.07153029  0.0137526   0.         -0.7283066 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 404 is [True, False, False, False, False, True]
State prediction error at timestep 404 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 404 of None
Current timestep = 405. State = [[-0.14038037  0.15875746]]. Action = [[ 0.00195732  0.06149139  0.         -0.77827895]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 405 is [True, False, False, False, False, True]
State prediction error at timestep 405 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 405 of None
Current timestep = 406. State = [[-0.13841256  0.15806556]]. Action = [[ 0.03399021 -0.03458209  0.         -0.8287349 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 406 is [True, False, False, False, False, True]
State prediction error at timestep 406 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 406 of None
Current timestep = 407. State = [[-0.13981682  0.15972385]]. Action = [[-0.05421577  0.06355592  0.         -0.71773875]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 407 is [True, False, False, False, False, True]
State prediction error at timestep 407 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 407 of None
Current timestep = 408. State = [[-0.13744837  0.16601872]]. Action = [[0.08777755 0.09964433 0.         0.35871136]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 408 is [True, False, False, False, False, True]
State prediction error at timestep 408 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 408 of None
Current timestep = 409. State = [[-0.13727705  0.17258841]]. Action = [[-0.03960049  0.07394571  0.         -0.7673674 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 409 is [True, False, False, False, False, True]
State prediction error at timestep 409 is tensor(6.1613e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 409 of None
Current timestep = 410. State = [[-0.13760334  0.17387408]]. Action = [[ 0.02032544 -0.03052529  0.         -0.7641954 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 410 is [True, False, False, False, False, True]
State prediction error at timestep 410 is tensor(1.8293e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 410 of None
Current timestep = 411. State = [[-0.13722053  0.17363189]]. Action = [[-0.00245807 -0.0033558   0.          0.5297909 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 411 is [True, False, False, False, False, True]
State prediction error at timestep 411 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 411 of None
Current timestep = 412. State = [[-0.13451488  0.16973117]]. Action = [[ 0.05014073 -0.08918399  0.          0.05031383]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 412 is [True, False, False, False, False, True]
State prediction error at timestep 412 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 412 of None
Current timestep = 413. State = [[-0.12910841  0.16467957]]. Action = [[ 0.06561085 -0.05378416  0.         -0.82324415]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 413 is [True, False, False, False, False, True]
State prediction error at timestep 413 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 413 of None
Current timestep = 414. State = [[-0.12416272  0.15916122]]. Action = [[ 0.0383283  -0.07025295  0.          0.73958945]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 414 is [True, False, False, False, False, True]
State prediction error at timestep 414 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 414 of None
Current timestep = 415. State = [[-0.11986996  0.16028081]]. Action = [[ 0.03833384  0.08048587  0.         -0.9651355 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 415 is [True, False, False, False, False, True]
State prediction error at timestep 415 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 415 of None
Current timestep = 416. State = [[-0.11951022  0.16367221]]. Action = [[-0.03444133  0.03630342  0.         -0.70805544]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 416 is [True, False, False, False, False, True]
State prediction error at timestep 416 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 416 of None
Current timestep = 417. State = [[-0.11606888  0.16359347]]. Action = [[ 0.07100191 -0.01936097  0.          0.2970091 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 417 is [True, False, False, False, False, True]
State prediction error at timestep 417 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 417 of None
Current timestep = 418. State = [[-0.11320465  0.15839015]]. Action = [[-0.00459515 -0.08675406  0.         -0.230219  ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 418 is [True, False, False, False, False, True]
State prediction error at timestep 418 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 418 of None
Current timestep = 419. State = [[-0.11272145  0.15878265]]. Action = [[-0.01558597  0.06668124  0.         -0.10705018]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 419 is [True, False, False, False, False, True]
State prediction error at timestep 419 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 419 of None
Current timestep = 420. State = [[-0.10792295  0.15735108]]. Action = [[ 0.08892933 -0.05786657  0.         -0.4370476 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 420 is [True, False, False, False, False, True]
State prediction error at timestep 420 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 420 of None
Current timestep = 421. State = [[-0.10013652  0.1586926 ]]. Action = [[0.09242637 0.07652202 0.         0.09023726]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 421 is [True, False, False, False, False, True]
State prediction error at timestep 421 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 421 of None
Current timestep = 422. State = [[-0.09540709  0.15540074]]. Action = [[ 0.01971923 -0.09312426  0.         -0.60313267]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 422 is [True, False, False, False, False, True]
State prediction error at timestep 422 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 422 of None
Current timestep = 423. State = [[-0.08870494  0.14987797]]. Action = [[ 0.09730012 -0.04001926  0.          0.2828709 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 423 is [True, False, False, False, False, True]
State prediction error at timestep 423 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 423 of None
Current timestep = 424. State = [[-0.08844958  0.14672594]]. Action = [[-0.09181233 -0.01921806  0.         -0.83109766]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 424 is [True, False, False, False, False, True]
State prediction error at timestep 424 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 424 of None
Current timestep = 425. State = [[-0.08628498  0.14776598]]. Action = [[ 0.0812682   0.05395044  0.         -0.868702  ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 425 is [True, False, False, False, False, True]
State prediction error at timestep 425 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 425 of None
Current timestep = 426. State = [[-0.07899759  0.15285262]]. Action = [[0.09091359 0.09235809 0.         0.598106  ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 426 is [True, False, False, False, False, True]
State prediction error at timestep 426 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 426 of None
Current timestep = 427. State = [[-0.07809311  0.15877682]]. Action = [[-0.05220569  0.07412284  0.          0.10936797]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 427 is [True, False, False, False, False, True]
State prediction error at timestep 427 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 427 of None
Current timestep = 428. State = [[-0.07949328  0.16447626]]. Action = [[-0.00645233  0.06305242  0.         -0.0701645 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 428 is [True, False, False, False, False, True]
State prediction error at timestep 428 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 428 of None
Current timestep = 429. State = [[-0.0751892   0.16891903]]. Action = [[0.09607125 0.0394846  0.         0.7274227 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 429 is [True, False, False, False, False, True]
State prediction error at timestep 429 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 429 of None
Current timestep = 430. State = [[-0.07203169  0.17007725]]. Action = [[ 0.00756101 -0.01379795  0.          0.5263125 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 430 is [True, False, False, False, False, True]
State prediction error at timestep 430 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 430 of None
Current timestep = 431. State = [[-0.07302023  0.170069  ]]. Action = [[-0.03942317 -0.01377114  0.         -0.94864035]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 431 is [True, False, False, False, False, True]
State prediction error at timestep 431 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 431 of None
Current timestep = 432. State = [[-0.06917971  0.17504217]]. Action = [[ 0.09904965  0.09146481  0.         -0.12769777]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 432 is [True, False, False, False, False, True]
State prediction error at timestep 432 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 432 of None
Current timestep = 433. State = [[-0.06868783  0.17457855]]. Action = [[-0.08692148 -0.09438004  0.         -0.63977927]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 433 is [True, False, False, False, False, True]
State prediction error at timestep 433 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 433 of None
Current timestep = 434. State = [[-0.06745455  0.17600009]]. Action = [[ 0.06846143  0.0629921   0.         -0.6451024 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 434 is [True, False, False, False, False, True]
State prediction error at timestep 434 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 434 of None
Current timestep = 435. State = [[-0.06068081  0.17619409]]. Action = [[ 0.08961923 -0.04145808  0.          0.19869888]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 435 is [True, False, False, False, False, True]
State prediction error at timestep 435 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 435 of None
Current timestep = 436. State = [[-0.06100394  0.17854577]]. Action = [[-0.08498625  0.05956627  0.         -0.03712451]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 436 is [True, False, False, False, False, True]
State prediction error at timestep 436 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 436 of None
Current timestep = 437. State = [[-0.06247284  0.18278204]]. Action = [[-3.5867095e-04  3.1027384e-02  0.0000000e+00  7.8097582e-01]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 437 is [True, False, False, False, False, True]
State prediction error at timestep 437 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 437 of None
Current timestep = 438. State = [[-0.06070391  0.18812302]]. Action = [[ 0.03165197  0.06762481  0.         -0.96456945]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 438 is [True, False, False, False, False, True]
State prediction error at timestep 438 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 438 of None
Current timestep = 439. State = [[-0.06233489  0.19437684]]. Action = [[-0.05228326  0.06337228  0.         -0.89675266]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 439 is [True, False, False, False, False, True]
State prediction error at timestep 439 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 439 of None
Current timestep = 440. State = [[-0.060377    0.19279067]]. Action = [[ 0.06497986 -0.09891482  0.          0.8566419 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 440 is [True, False, False, False, False, True]
State prediction error at timestep 440 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 440 of None
Current timestep = 441. State = [[-0.05511896  0.19482413]]. Action = [[ 0.06448378  0.08357132  0.         -0.92626995]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 441 is [True, False, False, False, False, True]
State prediction error at timestep 441 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 441 of None
Current timestep = 442. State = [[-0.05516386  0.19508749]]. Action = [[-0.05756077 -0.05872153  0.         -0.81888294]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 442 is [True, False, False, False, False, True]
State prediction error at timestep 442 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 442 of None
Current timestep = 443. State = [[-0.05625702  0.19573896]]. Action = [[0.00074176 0.02995834 0.         0.1802324 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 443 is [True, False, False, False, False, True]
State prediction error at timestep 443 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 443 of None
Current timestep = 444. State = [[-0.05290508  0.19503386]]. Action = [[ 0.0673781  -0.03761851  0.         -0.871663  ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 444 is [True, False, False, False, False, True]
State prediction error at timestep 444 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 444 of None
Current timestep = 445. State = [[-0.05288314  0.19256915]]. Action = [[-0.04886719 -0.03133871  0.         -0.8509049 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 445 is [True, False, False, False, False, True]
State prediction error at timestep 445 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 445 of None
Current timestep = 446. State = [[-0.05782246  0.19497307]]. Action = [[-0.08785035  0.05911205  0.         -0.5092126 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 446 is [True, False, False, False, False, True]
State prediction error at timestep 446 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 446 of None
Current timestep = 447. State = [[-0.05819075  0.19422166]]. Action = [[ 0.03819413 -0.06173578  0.         -0.757402  ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 447 is [True, False, False, False, False, True]
State prediction error at timestep 447 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 447 of None
Current timestep = 448. State = [[-0.05333278  0.1894615 ]]. Action = [[ 0.07419535 -0.06157604  0.         -0.26733863]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 448 is [True, False, False, False, False, True]
State prediction error at timestep 448 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 448 of None
Current timestep = 449. State = [[-0.05509795  0.18809038]]. Action = [[-0.09427935  0.01628031  0.         -0.791532  ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 449 is [True, False, False, False, False, True]
State prediction error at timestep 449 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 449 of None
Current timestep = 450. State = [[-0.05564714  0.18933961]]. Action = [[0.039079   0.02061701 0.         0.6999843 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 450 is [True, False, False, False, False, True]
State prediction error at timestep 450 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 450 of None
Current timestep = 451. State = [[-0.05209192  0.18740498]]. Action = [[ 0.0535842  -0.04322369  0.         -0.65210915]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 451 is [True, False, False, False, False, True]
State prediction error at timestep 451 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 451 of None
Current timestep = 452. State = [[-0.05203281  0.18218006]]. Action = [[-0.03975767 -0.06947058  0.         -0.6796356 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 452 is [True, False, False, False, False, True]
State prediction error at timestep 452 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 452 of None
Current timestep = 453. State = [[-0.05187664  0.17533918]]. Action = [[ 0.00886096 -0.08408941  0.          0.223688  ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 453 is [True, False, False, False, False, True]
State prediction error at timestep 453 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 453 of None
Current timestep = 454. State = [[-0.0481794  0.170285 ]]. Action = [[ 0.05593581 -0.02821187  0.          0.66351104]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 454 is [False, True, False, False, False, True]
State prediction error at timestep 454 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 454 of None
Current timestep = 455. State = [[-0.04835662  0.1715141 ]]. Action = [[-0.04521712  0.07275031  0.          0.38863897]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 455 is [False, True, False, False, False, True]
State prediction error at timestep 455 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 455 of None
Current timestep = 456. State = [[-0.04668597  0.1714712 ]]. Action = [[ 0.05644295 -0.02031497  0.         -0.31971574]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 456 is [False, True, False, False, False, True]
State prediction error at timestep 456 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 456 of None
Current timestep = 457. State = [[-0.04347973  0.17143522]]. Action = [[ 0.03616876  0.03398333  0.         -0.99689347]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 457 is [False, True, False, False, False, True]
State prediction error at timestep 457 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 457 of None
Current timestep = 458. State = [[-0.04211959  0.17489058]]. Action = [[0.0125974  0.07278246 0.         0.6528033 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 458 is [False, True, False, False, False, True]
State prediction error at timestep 458 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 458 of None
Current timestep = 459. State = [[-0.0458126   0.17900833]]. Action = [[-0.07572332  0.04887991  0.         -0.6079391 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 459 is [False, True, False, False, False, True]
State prediction error at timestep 459 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 459 of None
Current timestep = 460. State = [[-0.04716538  0.17721568]]. Action = [[ 0.02187307 -0.07089917  0.          0.76403284]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 460 is [False, True, False, False, False, True]
State prediction error at timestep 460 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 460 of None
Current timestep = 461. State = [[-0.04496385  0.17485182]]. Action = [[ 0.0378954  -0.00775312  0.         -0.9849755 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 461 is [False, True, False, False, False, True]
State prediction error at timestep 461 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 461 of None
Current timestep = 462. State = [[-0.04470884  0.1736782 ]]. Action = [[-0.01505198 -0.01385823  0.          0.28755498]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 462 is [False, True, False, False, False, True]
State prediction error at timestep 462 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 462 of None
Current timestep = 463. State = [[-0.04786699  0.17073081]]. Action = [[-0.06277445 -0.05328531  0.          0.5096462 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 463 is [False, True, False, False, False, True]
State prediction error at timestep 463 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 463 of None
Current timestep = 464. State = [[-0.04522403  0.16950732]]. Action = [[0.09507061 0.01049405 0.         0.29248524]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 464 is [False, True, False, False, False, True]
State prediction error at timestep 464 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 464 of None
Current timestep = 465. State = [[-0.0437723   0.17218788]]. Action = [[-0.0306132   0.05435573  0.          0.77313066]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 465 is [False, True, False, False, False, True]
State prediction error at timestep 465 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 465 of None
Current timestep = 466. State = [[-0.03985877  0.17729934]]. Action = [[0.09692433 0.07409088 0.         0.20028007]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 466 is [False, True, False, False, False, True]
State prediction error at timestep 466 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 466 of None
Current timestep = 467. State = [[-0.03975108  0.18345442]]. Action = [[-0.0560289   0.07659016  0.          0.03656256]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 467 is [False, True, False, False, False, True]
State prediction error at timestep 467 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 467 of None
Current timestep = 468. State = [[-0.04415587  0.18418495]]. Action = [[-0.06241982 -0.04926784  0.         -0.7642311 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 468 is [False, True, False, False, False, True]
State prediction error at timestep 468 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 468 of None
Current timestep = 469. State = [[-0.04550421  0.1835544 ]]. Action = [[ 0.00236021 -0.00979757  0.          0.81110954]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 469 is [False, True, False, False, False, True]
State prediction error at timestep 469 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 469 of None
Current timestep = 470. State = [[-0.04708346  0.18038768]]. Action = [[-0.04060962 -0.07709982  0.         -0.95276743]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 470 is [False, True, False, False, False, True]
State prediction error at timestep 470 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 470 of None
Current timestep = 471. State = [[-0.05067399  0.18051524]]. Action = [[-0.06385813  0.02793127  0.         -0.7864066 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 471 is [True, False, False, False, False, True]
State prediction error at timestep 471 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 471 of None
Current timestep = 472. State = [[-0.05631123  0.17977048]]. Action = [[-0.08771055 -0.05275123  0.         -0.68305767]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 472 is [True, False, False, False, False, True]
State prediction error at timestep 472 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 472 of None
Current timestep = 473. State = [[-0.05796317  0.17539835]]. Action = [[ 0.00722432 -0.07977861  0.          0.68337405]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 473 is [True, False, False, False, False, True]
State prediction error at timestep 473 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 473 of None
Current timestep = 474. State = [[-0.05556286  0.17150684]]. Action = [[ 0.04011638 -0.03509398  0.         -0.8046633 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 474 is [True, False, False, False, False, True]
State prediction error at timestep 474 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 474 of None
Current timestep = 475. State = [[-0.05418949  0.17095986]]. Action = [[ 0.00333098  0.0203289   0.         -0.9433328 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 475 is [True, False, False, False, False, True]
State prediction error at timestep 475 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 475 of None
Current timestep = 476. State = [[-0.05021369  0.1663721 ]]. Action = [[ 0.07958306 -0.08735388  0.         -0.77915967]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 476 is [True, False, False, False, False, True]
State prediction error at timestep 476 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 476 of None
Current timestep = 477. State = [[-0.05069807  0.15911466]]. Action = [[-0.06769274 -0.0740193   0.         -0.7942422 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 477 is [True, False, False, False, False, True]
State prediction error at timestep 477 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 477 of None
Current timestep = 478. State = [[-0.05174647  0.15566257]]. Action = [[ 0.00988408  0.00114918  0.         -0.05230415]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 478 is [True, False, False, False, False, True]
State prediction error at timestep 478 is tensor(9.0406e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 478 of None
Current timestep = 479. State = [[-0.05009476  0.15350291]]. Action = [[ 0.03242589 -0.01064234  0.         -0.74662197]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 479 is [True, False, False, False, False, True]
State prediction error at timestep 479 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 479 of None
Current timestep = 480. State = [[-0.05368069  0.14831145]]. Action = [[-0.09840836 -0.07138821  0.         -0.08760995]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 480 is [True, False, False, False, False, True]
State prediction error at timestep 480 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 480 of None
Current timestep = 481. State = [[-0.05294428  0.14244534]]. Action = [[ 0.08613933 -0.04291958  0.         -0.9798343 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 481 is [True, False, False, False, False, True]
State prediction error at timestep 481 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 481 of None
Current timestep = 482. State = [[-0.05170412  0.13614526]]. Action = [[-0.02255277 -0.06267504  0.         -0.9690105 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 482 is [True, False, False, False, False, True]
State prediction error at timestep 482 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 482 of None
Current timestep = 483. State = [[-0.05641312  0.13511252]]. Action = [[-0.09073891  0.04814532  0.          0.48626256]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 483 is [True, False, False, False, False, True]
State prediction error at timestep 483 is tensor(6.0128e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 483 of None
Current timestep = 484. State = [[-0.05687074  0.13276272]]. Action = [[ 0.04794984 -0.04979857  0.          0.73534036]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 484 is [True, False, False, False, False, True]
State prediction error at timestep 484 is tensor(4.1278e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 484 of None
Current timestep = 485. State = [[-0.05964873  0.13392046]]. Action = [[-0.07268779  0.07521311  0.         -0.42847276]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 485 is [True, False, False, False, False, True]
State prediction error at timestep 485 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 485 of None
Current timestep = 486. State = [[-0.05943348  0.13310604]]. Action = [[ 0.0611145  -0.04367035  0.          0.02552819]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 486 is [True, False, False, False, False, True]
State prediction error at timestep 486 is tensor(1.9417e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 486 of None
Current timestep = 487. State = [[-0.05638402  0.12936972]]. Action = [[ 0.04525454 -0.03095014  0.          0.31174433]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 487 is [True, False, False, False, False, True]
State prediction error at timestep 487 is tensor(7.0397e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 487 of None
Current timestep = 488. State = [[-0.05622638  0.12620072]]. Action = [[-0.01371291 -0.02148351  0.         -0.651771  ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 488 is [True, False, False, False, False, True]
State prediction error at timestep 488 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 488 of None
Current timestep = 489. State = [[-0.05393421  0.12809914]]. Action = [[ 0.06666458  0.07413066  0.         -0.8805777 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 489 is [True, False, False, False, False, True]
State prediction error at timestep 489 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 489 of None
Current timestep = 490. State = [[-0.0492068   0.13093989]]. Action = [[0.07857919 0.03761297 0.         0.8724617 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 490 is [False, True, False, False, False, True]
State prediction error at timestep 490 is tensor(2.2053e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 490 of None
Current timestep = 491. State = [[-0.05175731  0.13594007]]. Action = [[-0.09123781  0.08912777  0.         -0.8861072 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 491 is [True, False, False, False, False, True]
State prediction error at timestep 491 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 491 of None
Current timestep = 492. State = [[-0.05961718  0.13583915]]. Action = [[-0.09934918 -0.06087896  0.         -0.8164157 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 492 is [True, False, False, False, False, True]
State prediction error at timestep 492 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 492 of None
Current timestep = 493. State = [[-0.06691379  0.13699998]]. Action = [[-0.08191719  0.03873125  0.          0.41963613]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 493 is [True, False, False, False, False, True]
State prediction error at timestep 493 is tensor(9.1801e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 493 of None
Current timestep = 494. State = [[-0.07085647  0.14168273]]. Action = [[-0.01028489  0.05232174  0.          0.1006887 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 494 is [True, False, False, False, False, True]
State prediction error at timestep 494 is tensor(8.2519e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 494 of None
Current timestep = 495. State = [[-0.0752199   0.14028607]]. Action = [[-0.05939303 -0.08350846  0.         -0.8734267 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 495 is [True, False, False, False, False, True]
State prediction error at timestep 495 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 495 of None
Current timestep = 496. State = [[-0.0759005  0.1374623]]. Action = [[ 0.03725865 -0.02798805  0.          0.41580582]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 496 is [True, False, False, False, False, True]
State prediction error at timestep 496 is tensor(3.6495e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 496 of None
Current timestep = 497. State = [[-0.07284296  0.14022022]]. Action = [[ 0.06396467  0.07074595  0.         -0.16293776]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 497 is [True, False, False, False, False, True]
State prediction error at timestep 497 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 497 of None
Current timestep = 498. State = [[-0.0720001   0.13793473]]. Action = [[-0.0029472  -0.08542409  0.         -0.9153493 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 498 is [True, False, False, False, False, True]
State prediction error at timestep 498 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 498 of None
Current timestep = 499. State = [[-0.06889968  0.13292156]]. Action = [[ 0.07228012 -0.04513639  0.         -0.47093332]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 499 is [True, False, False, False, False, True]
State prediction error at timestep 499 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 499 of None
Current timestep = 500. State = [[-0.06303347  0.13030735]]. Action = [[ 0.08552033 -0.00121915  0.          0.00141442]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 500 is [True, False, False, False, False, True]
State prediction error at timestep 500 is tensor(4.5360e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 500 of None
Current timestep = 501. State = [[-0.06333228  0.1329061 ]]. Action = [[-0.05575528  0.07898221  0.         -0.2139731 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 501 is [True, False, False, False, False, True]
State prediction error at timestep 501 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 501 of None
Current timestep = 502. State = [[-0.06313476  0.1313838 ]]. Action = [[ 0.03668465 -0.06094838  0.         -0.9359514 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 502 is [True, False, False, False, False, True]
State prediction error at timestep 502 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 502 of None
Current timestep = 503. State = [[-0.06213683  0.12510523]]. Action = [[-0.00345428 -0.07593755  0.         -0.00957137]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 503 is [True, False, False, False, False, True]
State prediction error at timestep 503 is tensor(9.0548e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 503 of None
Current timestep = 504. State = [[-0.06027256  0.11691274]]. Action = [[ 0.02509788 -0.09790611  0.         -0.8804873 ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 504 is [True, False, False, False, True, False]
State prediction error at timestep 504 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 504 of None
Current timestep = 505. State = [[-0.05569888  0.11398189]]. Action = [[0.06378982 0.03102066 0.         0.5671929 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 505 is [True, False, False, False, True, False]
State prediction error at timestep 505 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 505 of None
Current timestep = 506. State = [[-0.05394944  0.11427677]]. Action = [[-0.01186339  0.02355348  0.         -0.585575  ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 506 is [True, False, False, False, True, False]
State prediction error at timestep 506 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 506 of None
Current timestep = 507. State = [[-0.05046522  0.11774746]]. Action = [[ 0.07139004  0.0821235   0.         -0.6152159 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 507 is [True, False, False, False, True, False]
State prediction error at timestep 507 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 507 of None
Current timestep = 508. State = [[-0.05170989  0.11994974]]. Action = [[-0.07755039  0.01138115  0.          0.7337606 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 508 is [True, False, False, False, True, False]
State prediction error at timestep 508 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 508 of None
Current timestep = 509. State = [[-0.05009175  0.12317397]]. Action = [[ 0.08828562  0.06556135  0.         -0.99313545]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 509 is [True, False, False, False, True, False]
State prediction error at timestep 509 is tensor(7.6700e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 509 of None
Current timestep = 510. State = [[-0.15367858  0.07896432]]. Action = [[ 0.07787991 -0.01192182  0.         -0.64921033]]. Reward = [100.]
Curr episode timestep = 193
Scene graph at timestep 510 is [True, False, False, False, True, False]
State prediction error at timestep 510 is tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 510 of None
Current timestep = 511. State = [[-0.15575679  0.08645852]]. Action = [[-0.05455391  0.07491086  0.         -0.7948948 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 511 is [True, False, False, False, True, False]
State prediction error at timestep 511 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 511 of None
Current timestep = 512. State = [[-0.15483223  0.09367724]]. Action = [[ 0.04478926  0.07287122  0.         -0.98999643]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 512 is [True, False, False, False, True, False]
State prediction error at timestep 512 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 512 of None
Current timestep = 513. State = [[-0.15031043  0.09457821]]. Action = [[ 0.06359791 -0.05338441  0.          0.11007965]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 513 is [True, False, False, False, True, False]
State prediction error at timestep 513 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 513 of None
Current timestep = 514. State = [[-0.14812973  0.09142245]]. Action = [[-0.00976153 -0.0582801   0.          0.77830815]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 514 is [True, False, False, False, True, False]
State prediction error at timestep 514 is tensor(1.1635e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 514 of None
Current timestep = 515. State = [[-0.14520621  0.09262457]]. Action = [[ 0.04104842  0.04001445  0.         -0.97895885]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 515 is [True, False, False, False, True, False]
State prediction error at timestep 515 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 515 of None
Current timestep = 516. State = [[-0.1438323   0.09916034]]. Action = [[-0.01113202  0.09273403  0.         -0.8861686 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 516 is [True, False, False, False, True, False]
State prediction error at timestep 516 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 516 of None
Current timestep = 517. State = [[-0.14630696  0.09853563]]. Action = [[-0.0631564  -0.09263217  0.         -0.24746609]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 517 is [True, False, False, False, True, False]
State prediction error at timestep 517 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 517 of None
Current timestep = 518. State = [[-0.14591478  0.09824727]]. Action = [[ 0.02369791  0.02289695  0.         -0.37573653]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 518 is [True, False, False, False, True, False]
State prediction error at timestep 518 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 518 of None
Current timestep = 519. State = [[-0.13985404  0.09693633]]. Action = [[ 0.09472007 -0.04936346  0.         -0.93243814]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 519 is [True, False, False, False, True, False]
State prediction error at timestep 519 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 519 of None
Current timestep = 520. State = [[-0.1378175   0.09536415]]. Action = [[-0.03603081 -0.00729632  0.         -0.8566656 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 520 is [True, False, False, False, True, False]
State prediction error at timestep 520 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 520 of None
Current timestep = 521. State = [[-0.13784993  0.09135944]]. Action = [[-0.00692641 -0.07865517  0.         -0.35375464]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 521 is [True, False, False, False, True, False]
State prediction error at timestep 521 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 521 of None
Current timestep = 522. State = [[-0.14073284  0.09381987]]. Action = [[-0.076865   0.0958504  0.         0.8152894]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 522 is [True, False, False, False, True, False]
State prediction error at timestep 522 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 522 of None
Current timestep = 523. State = [[-0.13827874  0.09558031]]. Action = [[ 0.0885627  -0.02278001  0.          0.5103345 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 523 is [True, False, False, False, True, False]
State prediction error at timestep 523 is tensor(3.4841e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 523 of None
Current timestep = 524. State = [[-0.13288027  0.09320977]]. Action = [[ 0.05100156 -0.03443407  0.         -0.15612322]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 524 is [True, False, False, False, True, False]
State prediction error at timestep 524 is tensor(5.1063e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 524 of None
Current timestep = 525. State = [[-0.1265113   0.09182698]]. Action = [[ 0.08397891  0.00304289  0.         -0.6039998 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 525 is [True, False, False, False, True, False]
State prediction error at timestep 525 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 525 of None
Current timestep = 526. State = [[-0.1246101   0.09242839]]. Action = [[-0.02713408  0.02328877  0.         -0.9920753 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 526 is [True, False, False, False, True, False]
State prediction error at timestep 526 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 526 of None
Current timestep = 527. State = [[-0.12073065  0.09739599]]. Action = [[ 0.08482382  0.0947305   0.         -0.42623782]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 527 is [True, False, False, False, True, False]
State prediction error at timestep 527 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 527 of None
Current timestep = 528. State = [[-0.11567675  0.09883364]]. Action = [[ 0.04709201 -0.02080694  0.         -0.91535544]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 528 is [True, False, False, False, True, False]
State prediction error at timestep 528 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 528 of None
Current timestep = 529. State = [[-0.11683427  0.10083918]]. Action = [[-0.07104699  0.04915578  0.          0.07545269]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 529 is [True, False, False, False, True, False]
State prediction error at timestep 529 is tensor(6.9087e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 529 of None
Current timestep = 530. State = [[-0.11521939  0.09930509]]. Action = [[ 0.0554798  -0.06751062  0.          0.36440635]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 530 is [True, False, False, False, True, False]
State prediction error at timestep 530 is tensor(2.0425e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 530 of None
Current timestep = 531. State = [[-0.10962488  0.09681965]]. Action = [[ 0.06520689 -0.01368806  0.          0.67698765]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 531 is [True, False, False, False, True, False]
State prediction error at timestep 531 is tensor(6.8211e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 531 of None
Current timestep = 532. State = [[-0.10395669  0.09885646]]. Action = [[ 0.05896478  0.05519748  0.         -0.47881126]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 532 is [True, False, False, False, True, False]
State prediction error at timestep 532 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 532 of None
Current timestep = 533. State = [[-0.1057274   0.10438335]]. Action = [[-0.09397481  0.0759637   0.         -0.70420414]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 533 is [True, False, False, False, True, False]
State prediction error at timestep 533 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 533 of None
Current timestep = 534. State = [[-0.1056574   0.10744854]]. Action = [[ 0.04542672  0.00220773  0.         -0.6691308 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 534 is [True, False, False, False, True, False]
State prediction error at timestep 534 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 534 of None
Current timestep = 535. State = [[-0.09960841  0.10830426]]. Action = [[0.09481149 0.00389504 0.         0.56124353]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 535 is [True, False, False, False, True, False]
State prediction error at timestep 535 is tensor(1.4519e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 535 of None
Current timestep = 536. State = [[-0.09743216  0.10876859]]. Action = [[-2.2724733e-02  2.5876611e-04  0.0000000e+00  5.5326223e-01]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 536 is [True, False, False, False, True, False]
State prediction error at timestep 536 is tensor(2.7597e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 536 of None
Current timestep = 537. State = [[-0.09425437  0.1046903 ]]. Action = [[ 0.05843461 -0.08986419  0.         -0.91948223]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 537 is [True, False, False, False, True, False]
State prediction error at timestep 537 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 537 of None
Current timestep = 538. State = [[-0.09475305  0.10487343]]. Action = [[-0.06567232  0.05423158  0.         -0.20949358]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 538 is [True, False, False, False, True, False]
State prediction error at timestep 538 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 538 of None
Current timestep = 539. State = [[-0.0933897   0.11095747]]. Action = [[ 0.05177612  0.08510358  0.         -0.99338025]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(6.3213e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 539 of None
Current timestep = 540. State = [[-0.09588574  0.11413474]]. Action = [[-0.09874459 -0.00430997  0.         -0.885375  ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 540 is [True, False, False, False, True, False]
State prediction error at timestep 540 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 540 of None
Current timestep = 541. State = [[-0.1020548   0.11513714]]. Action = [[-0.08248148 -0.00478257  0.         -0.8303036 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 541 is [True, False, False, False, True, False]
State prediction error at timestep 541 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 541 of None
Current timestep = 542. State = [[-0.10298777  0.11335424]]. Action = [[ 0.02570329 -0.05734837  0.         -0.98716235]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 542 is [True, False, False, False, True, False]
State prediction error at timestep 542 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 542 of None
Current timestep = 543. State = [[-0.10310677  0.10805684]]. Action = [[-0.0224773  -0.08723488  0.          0.3303119 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 543 is [True, False, False, False, True, False]
State prediction error at timestep 543 is tensor(2.2714e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 543 of None
Current timestep = 544. State = [[-0.10618936  0.11050035]]. Action = [[-0.05681566  0.09543245  0.         -0.9356353 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 544 is [True, False, False, False, True, False]
State prediction error at timestep 544 is tensor(7.3022e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 544 of None
Current timestep = 545. State = [[-0.10499322  0.10938659]]. Action = [[ 0.06179453 -0.08549089  0.          0.44329095]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 545 is [True, False, False, False, True, False]
State prediction error at timestep 545 is tensor(1.0139e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 545 of None
Current timestep = 546. State = [[-0.09942818  0.11110695]]. Action = [[ 0.08997076  0.08859264  0.         -0.7499742 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 546 is [True, False, False, False, True, False]
State prediction error at timestep 546 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 546 of None
Current timestep = 547. State = [[-0.09410947  0.1163419 ]]. Action = [[ 0.07378072  0.06791382  0.         -0.4312389 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 547 is [True, False, False, False, True, False]
State prediction error at timestep 547 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 547 of None
Current timestep = 548. State = [[-0.0881773   0.12056421]]. Action = [[ 0.09273645  0.0542526   0.         -0.04003352]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 548 is [True, False, False, False, True, False]
State prediction error at timestep 548 is tensor(3.4637e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 548 of None
Current timestep = 549. State = [[-0.08327189  0.11730503]]. Action = [[ 0.05037384 -0.08866217  0.         -0.5403863 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 549 is [True, False, False, False, True, False]
State prediction error at timestep 549 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 549 of None
Current timestep = 550. State = [[-0.08024796  0.11220176]]. Action = [[ 0.01845197 -0.04167136  0.         -0.61198634]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 550 is [True, False, False, False, True, False]
State prediction error at timestep 550 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 550 of None
Current timestep = 551. State = [[-0.07769644  0.10767371]]. Action = [[ 0.01812482 -0.0509989   0.         -0.57754076]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 551 is [True, False, False, False, True, False]
State prediction error at timestep 551 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 551 of None
Current timestep = 552. State = [[-0.07147727  0.10377191]]. Action = [[ 0.08927567 -0.02868698  0.         -0.97523606]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 552 is [True, False, False, False, True, False]
State prediction error at timestep 552 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 552 of None
Current timestep = 553. State = [[-0.06801116  0.10000532]]. Action = [[-0.0125605  -0.033888    0.         -0.63755894]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 553 is [True, False, False, False, True, False]
State prediction error at timestep 553 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 553 of None
Current timestep = 554. State = [[-0.06795342  0.09486064]]. Action = [[-0.03334014 -0.06313461  0.         -0.6811187 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 554 is [True, False, False, False, True, False]
State prediction error at timestep 554 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 554 of None
Current timestep = 555. State = [[-0.06673533  0.09253994]]. Action = [[ 0.00523585  0.00989383  0.         -0.97257245]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 555 is [True, False, False, False, True, False]
State prediction error at timestep 555 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 555 of None
Current timestep = 556. State = [[-0.06112857  0.0918948 ]]. Action = [[ 0.08488526  0.00471036  0.         -0.828215  ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 556 is [True, False, False, False, True, False]
State prediction error at timestep 556 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 556 of None
Current timestep = 557. State = [[-0.05804756  0.09567707]]. Action = [[-0.00598137  0.09413853  0.         -0.45436877]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 557 is [True, False, False, False, True, False]
State prediction error at timestep 557 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 557 of None
Current timestep = 558. State = [[-0.05302097  0.09429113]]. Action = [[ 0.08738204 -0.07050566  0.         -0.7150705 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 558 is [True, False, False, False, True, False]
State prediction error at timestep 558 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 558 of None
Current timestep = 559. State = [[-0.33844447  0.17249212]]. Action = [[0.08087962 0.09648389 0.         0.4302752 ]]. Reward = [100.]
Curr episode timestep = 48
Scene graph at timestep 559 is [True, False, False, False, False, True]
State prediction error at timestep 559 is tensor(0.0423, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 559 of None
Current timestep = 560. State = [[-0.3458586  0.1760356]]. Action = [[-0.07481678  0.01634103  0.          0.15289104]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 560 is [True, False, False, False, False, True]
State prediction error at timestep 560 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 560 of None
Current timestep = 561. State = [[-0.34932995  0.17450996]]. Action = [[-0.00317583 -0.06989611  0.         -0.9837991 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 561 is [True, False, False, False, False, True]
State prediction error at timestep 561 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 561 of None
Current timestep = 562. State = [[-0.3542831   0.17361999]]. Action = [[-0.07868577  0.00056628  0.          0.25656855]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 562 is [True, False, False, False, False, True]
State prediction error at timestep 562 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 562 of None
Current timestep = 563. State = [[-0.35539147  0.17799692]]. Action = [[ 0.05477179  0.07963195  0.         -0.7304185 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 563 is [True, False, False, False, False, True]
State prediction error at timestep 563 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 563 of None
Current timestep = 564. State = [[-0.35185143  0.1841879 ]]. Action = [[ 0.07264747  0.07957477  0.         -0.57663023]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 564 is [True, False, False, False, False, True]
State prediction error at timestep 564 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 564 of None
Current timestep = 565. State = [[-0.3463381  0.1865168]]. Action = [[ 0.08989149  0.00567225  0.         -0.05512255]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 565 is [True, False, False, False, False, True]
State prediction error at timestep 565 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 565 of None
Current timestep = 566. State = [[-0.34765074  0.18344769]]. Action = [[-0.08559708 -0.07820754  0.         -0.12908006]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 566 is [True, False, False, False, False, True]
State prediction error at timestep 566 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 566 of None
Current timestep = 567. State = [[-0.3481586   0.18181512]]. Action = [[ 0.04937271  0.00928976  0.         -0.81188226]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 567 is [True, False, False, False, False, True]
State prediction error at timestep 567 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 567 of None
Current timestep = 568. State = [[-0.34762514  0.18086207]]. Action = [[-0.00981151 -0.02027389  0.         -0.35185385]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 568 is [True, False, False, False, False, True]
State prediction error at timestep 568 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 568 of None
Current timestep = 569. State = [[-0.34525135  0.18339173]]. Action = [[0.05576172 0.06785766 0.         0.03660917]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 569 is [True, False, False, False, False, True]
State prediction error at timestep 569 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 569 of None
Current timestep = 570. State = [[-0.34812418  0.18700655]]. Action = [[-0.08955034  0.03592894  0.          0.09634864]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 570 is [True, False, False, False, False, True]
State prediction error at timestep 570 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 570 of None
Current timestep = 571. State = [[-0.35495675  0.19240527]]. Action = [[-0.07515022  0.07774586  0.         -0.6502125 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 571 is [True, False, False, False, False, True]
State prediction error at timestep 571 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 571 of None
Current timestep = 572. State = [[-0.36084253  0.19432862]]. Action = [[-0.05063985 -0.02247103  0.         -0.7186309 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 572 is [True, False, False, False, False, True]
State prediction error at timestep 572 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 572 of None
Current timestep = 573. State = [[-0.36751956  0.19812313]]. Action = [[-0.07811721  0.067366    0.         -0.9639782 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 573 is [True, False, False, False, False, True]
State prediction error at timestep 573 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 573 of None
Current timestep = 574. State = [[-0.37476385  0.20563504]]. Action = [[-0.0603688   0.09365318  0.         -0.2743988 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 574 is [True, False, False, False, False, True]
State prediction error at timestep 574 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 574 of None
Current timestep = 575. State = [[-0.3788643   0.20936196]]. Action = [[ 0.          0.          0.         -0.87405586]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 575 is [True, False, False, False, False, True]
State prediction error at timestep 575 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 575 of None
Current timestep = 576. State = [[-0.37703496  0.21087621]]. Action = [[ 0.07806211  0.01362626  0.         -0.16845089]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 576 is [True, False, False, False, False, True]
State prediction error at timestep 576 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 576 of None
Current timestep = 577. State = [[-0.37617663  0.21170555]]. Action = [[ 0.          0.          0.         -0.85999095]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 577 is [True, False, False, False, False, True]
State prediction error at timestep 577 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 577 of None
Current timestep = 578. State = [[-0.37278074  0.2156576 ]]. Action = [[0.08709898 0.0723651  0.         0.0897727 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 578 is [True, False, False, False, False, True]
State prediction error at timestep 578 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 578 of None
Current timestep = 579. State = [[-0.36747506  0.21575299]]. Action = [[ 0.06355289 -0.0436518   0.         -0.49836302]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 579 is [True, False, False, False, False, True]
State prediction error at timestep 579 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 579 of None
Current timestep = 580. State = [[-0.36303723  0.2111607 ]]. Action = [[ 0.03971662 -0.07985608  0.         -0.7669274 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 580 is [True, False, False, False, False, True]
State prediction error at timestep 580 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 580 of None
Current timestep = 581. State = [[-0.36442727  0.20743191]]. Action = [[-0.07682313 -0.04622988  0.          0.36436534]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 581 is [True, False, False, False, False, True]
State prediction error at timestep 581 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 581 of None
Current timestep = 582. State = [[-0.36106488  0.20649098]]. Action = [[ 0.09146688 -0.00230528  0.         -0.927411  ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 582 is [True, False, False, False, False, True]
State prediction error at timestep 582 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 582 of None
Current timestep = 583. State = [[-0.3530516   0.20584914]]. Action = [[ 0.07888923 -0.00925609  0.         -0.80217665]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 583 is [True, False, False, False, False, True]
State prediction error at timestep 583 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 583 of None
Current timestep = 584. State = [[-0.344487    0.20308888]]. Action = [[ 0.08377207 -0.04175016  0.         -0.37514126]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 584 is [True, False, False, False, False, True]
State prediction error at timestep 584 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 584 of None
Current timestep = 585. State = [[-0.3439175   0.20038676]]. Action = [[-0.08788439 -0.02625225  0.         -0.5847831 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 585 is [True, False, False, False, False, True]
State prediction error at timestep 585 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 585 of None
Current timestep = 586. State = [[-0.34329563  0.1997745 ]]. Action = [[ 0.02353217  0.00202602  0.         -0.30630726]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 586 is [True, False, False, False, False, True]
State prediction error at timestep 586 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 586 of None
Current timestep = 587. State = [[-0.34538332  0.19631079]]. Action = [[-0.08647256 -0.07209126  0.         -0.2562604 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 587 is [True, False, False, False, False, True]
State prediction error at timestep 587 is tensor(6.0967e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 587 of None
Current timestep = 588. State = [[-0.34827614  0.19776052]]. Action = [[-0.02586249  0.07099742  0.          0.3371222 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 588 is [True, False, False, False, False, True]
State prediction error at timestep 588 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 588 of None
Current timestep = 589. State = [[-0.34401923  0.19872531]]. Action = [[ 0.09739711 -0.01241609  0.          0.6788218 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 589 is [True, False, False, False, False, True]
State prediction error at timestep 589 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 589 of None
Current timestep = 590. State = [[-0.336202    0.19464464]]. Action = [[ 0.09153926 -0.05628297  0.         -0.8765905 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 590 is [True, False, False, False, False, True]
State prediction error at timestep 590 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 590 of None
Current timestep = 591. State = [[-0.3322671  0.1955339]]. Action = [[ 0.00659671  0.07650862  0.         -0.6527873 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 591 is [True, False, False, False, False, True]
State prediction error at timestep 591 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 591 of None
Current timestep = 592. State = [[-0.32978603  0.19998127]]. Action = [[ 0.03550252  0.06602284  0.         -0.40060747]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 592 is [True, False, False, False, False, True]
State prediction error at timestep 592 is tensor(8.6803e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 592 of None
Current timestep = 593. State = [[-0.33204243  0.20024012]]. Action = [[-0.07770424 -0.03056848  0.         -0.9990724 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 593 is [True, False, False, False, False, True]
State prediction error at timestep 593 is tensor(4.1995e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 593 of None
Current timestep = 594. State = [[-0.33484262  0.19839446]]. Action = [[-0.01453017 -0.02524221  0.         -0.7067156 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 594 is [True, False, False, False, False, True]
State prediction error at timestep 594 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 594 of None
Current timestep = 595. State = [[-0.33134812  0.20205903]]. Action = [[ 0.09095774  0.09863707  0.         -0.9078261 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 595 is [True, False, False, False, False, True]
State prediction error at timestep 595 is tensor(7.0647e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 595 of None
Current timestep = 596. State = [[-0.33228338  0.2083962 ]]. Action = [[-0.0695235   0.07551303  0.         -0.81383604]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 596 is [True, False, False, False, False, True]
State prediction error at timestep 596 is tensor(5.2380e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 596 of None
Current timestep = 597. State = [[-0.3307293   0.21238564]]. Action = [[ 0.07826222  0.0297216   0.         -0.8877925 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 597 is [True, False, False, False, False, True]
State prediction error at timestep 597 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 597 of None
Current timestep = 598. State = [[-0.3270834  0.210499 ]]. Action = [[ 0.03120924 -0.06269739  0.         -0.7965759 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 598 is [True, False, False, False, False, True]
State prediction error at timestep 598 is tensor(5.6415e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 598 of None
Current timestep = 599. State = [[-0.3284245   0.20662265]]. Action = [[-0.05861041 -0.0594977   0.         -0.98397595]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 599 is [True, False, False, False, False, True]
State prediction error at timestep 599 is tensor(5.8665e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 599 of None
Current timestep = 600. State = [[-0.32567942  0.20988268]]. Action = [[ 0.08838887  0.09594963  0.         -0.9379067 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 600 is [True, False, False, False, False, True]
State prediction error at timestep 600 is tensor(3.0896e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 600 of None
Current timestep = 601. State = [[-0.3252917   0.20959538]]. Action = [[-0.07537161 -0.08886627  0.         -0.94796556]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 601 is [True, False, False, False, False, True]
State prediction error at timestep 601 is tensor(6.5184e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 601 of None
Current timestep = 602. State = [[-0.3306284   0.20742747]]. Action = [[-0.08210637 -0.02011269  0.         -0.8482857 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 602 is [True, False, False, False, False, True]
State prediction error at timestep 602 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 602 of None
Current timestep = 603. State = [[-0.32980156  0.20787887]]. Action = [[ 0.06664736  0.01035118  0.         -0.9645829 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 603 is [True, False, False, False, False, True]
State prediction error at timestep 603 is tensor(5.8743e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 603 of None
Current timestep = 604. State = [[-0.3238282  0.2111914]]. Action = [[ 0.08297012  0.06404174  0.         -0.0879786 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 604 is [True, False, False, False, False, True]
State prediction error at timestep 604 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 604 of None
Current timestep = 605. State = [[-0.31799603  0.20781752]]. Action = [[ 0.0574734  -0.0988246   0.         -0.49383008]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 605 is [True, False, False, False, False, True]
State prediction error at timestep 605 is tensor(7.5170e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 605 of None
Current timestep = 606. State = [[-0.31135646  0.20808455]]. Action = [[0.08355542 0.07536071 0.         0.41712475]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 606 is [True, False, False, False, False, True]
State prediction error at timestep 606 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 606 of None
Current timestep = 607. State = [[-0.30623037  0.2054954 ]]. Action = [[ 0.02873773 -0.08177964  0.         -0.8690293 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 607 is [True, False, False, False, False, True]
State prediction error at timestep 607 is tensor(4.6446e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 607 of None
Current timestep = 608. State = [[-0.3006578   0.20699179]]. Action = [[ 0.07176108  0.09299018  0.         -0.924362  ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 608 is [True, False, False, False, False, True]
State prediction error at timestep 608 is tensor(1.4691e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 608 of None
Current timestep = 609. State = [[-0.2942203   0.20742954]]. Action = [[ 0.06093001 -0.02519904  0.         -0.42758965]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 609 is [True, False, False, False, False, True]
State prediction error at timestep 609 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 609 of None
Current timestep = 610. State = [[-0.28627712  0.21053626]]. Action = [[ 0.09187192  0.09225894  0.         -0.00030965]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 610 is [True, False, False, False, False, True]
State prediction error at timestep 610 is tensor(4.2138e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 610 of None
Current timestep = 611. State = [[-0.280709    0.20956472]]. Action = [[ 0.01869567 -0.06181797  0.         -0.4008453 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 611 is [True, False, False, False, False, True]
State prediction error at timestep 611 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 611 of None
Current timestep = 612. State = [[-0.2771763   0.21229905]]. Action = [[ 0.0244455   0.09626503  0.         -0.79703677]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 612 is [True, False, False, False, False, True]
State prediction error at timestep 612 is tensor(2.8848e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 612 of None
Current timestep = 613. State = [[-0.270359   0.2106447]]. Action = [[ 0.09206829 -0.08294318  0.         -0.7330631 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 613 is [True, False, False, False, False, True]
State prediction error at timestep 613 is tensor(5.4591e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 613 of None
Current timestep = 614. State = [[-0.2608962   0.20572136]]. Action = [[ 0.09375852 -0.04730086  0.         -0.9370782 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 614 is [True, False, False, False, False, True]
State prediction error at timestep 614 is tensor(3.9763e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 614 of None
Current timestep = 615. State = [[-0.25827858  0.20487481]]. Action = [[-0.05406648  0.0191981   0.          0.70596504]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 615 is [True, False, False, False, False, True]
State prediction error at timestep 615 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 615 of None
Current timestep = 616. State = [[-0.25601152  0.2046565 ]]. Action = [[ 0.02852095 -0.01786149  0.         -0.86152446]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 616 is [True, False, False, False, False, True]
State prediction error at timestep 616 is tensor(4.0906e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 616 of None
Current timestep = 617. State = [[-0.25404423  0.19966401]]. Action = [[-0.02037843 -0.0960578   0.         -0.9229336 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 617 is [True, False, False, False, False, True]
State prediction error at timestep 617 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 617 of None
Current timestep = 618. State = [[-0.25629044  0.19951037]]. Action = [[-0.07899954  0.0446322   0.          0.44405615]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 618 is [True, False, False, False, False, True]
State prediction error at timestep 618 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 618 of None
Current timestep = 619. State = [[-0.2617758   0.19927931]]. Action = [[-0.09445044 -0.04428436  0.         -0.9880715 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 619 is [True, False, False, False, False, True]
State prediction error at timestep 619 is tensor(3.8390e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 619 of None
Current timestep = 620. State = [[-0.26123792  0.20100665]]. Action = [[ 0.06096568  0.05138946  0.         -0.12642717]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 620 is [True, False, False, False, False, True]
State prediction error at timestep 620 is tensor(1.8525e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 620 of None
Current timestep = 621. State = [[-0.25405297  0.19970492]]. Action = [[ 0.09848437 -0.05780016  0.         -0.84140456]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 621 is [True, False, False, False, False, True]
State prediction error at timestep 621 is tensor(4.1408e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 621 of None
Current timestep = 622. State = [[-0.2477542  0.1988828]]. Action = [[ 0.04674495  0.02137645  0.         -0.84299153]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 622 is [True, False, False, False, False, True]
State prediction error at timestep 622 is tensor(7.0107e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 622 of None
Current timestep = 623. State = [[-0.24308144  0.20271993]]. Action = [[ 0.04561526  0.07599802  0.         -0.23985577]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 623 is [True, False, False, False, False, True]
State prediction error at timestep 623 is tensor(6.3059e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 623 of None
Current timestep = 624. State = [[-0.24334936  0.20543383]]. Action = [[-0.04872105  0.01334139  0.          0.37829137]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 624 is [True, False, False, False, False, True]
State prediction error at timestep 624 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 624 of None
Current timestep = 625. State = [[-0.2485784  0.2044489]]. Action = [[-0.0926365  -0.04114735  0.         -0.6823578 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 625 is [True, False, False, False, False, True]
State prediction error at timestep 625 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 625 of None
Current timestep = 626. State = [[-0.25044072  0.20326263]]. Action = [[ 0.00790635 -0.01815805  0.          0.22903311]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 626 is [True, False, False, False, False, True]
State prediction error at timestep 626 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 626 of None
Current timestep = 627. State = [[-0.25430372  0.20194854]]. Action = [[-0.08576979 -0.02905196  0.         -0.9786542 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 627 is [True, False, False, False, False, True]
State prediction error at timestep 627 is tensor(4.9680e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 627 of None
Current timestep = 628. State = [[-0.25527307  0.20070933]]. Action = [[ 0.02931156 -0.02134444  0.         -0.7460973 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 628 is [True, False, False, False, False, True]
State prediction error at timestep 628 is tensor(5.9621e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 628 of None
Current timestep = 629. State = [[-0.25080964  0.19854061]]. Action = [[ 0.07759208 -0.03019757  0.         -0.44757903]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 629 is [True, False, False, False, False, True]
State prediction error at timestep 629 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 629 of None
Current timestep = 630. State = [[-0.24378805  0.19850208]]. Action = [[0.09749503 0.033984   0.         0.06271482]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 630 is [True, False, False, False, False, True]
State prediction error at timestep 630 is tensor(1.7690e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 630 of None
Current timestep = 631. State = [[-0.2362498  0.194881 ]]. Action = [[ 0.08864775 -0.06692711  0.          0.6907854 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 631 is [True, False, False, False, False, True]
State prediction error at timestep 631 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 631 of None
Current timestep = 632. State = [[-0.22894543  0.19586058]]. Action = [[ 0.07890714  0.08785111  0.         -0.99468154]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 632 is [True, False, False, False, False, True]
State prediction error at timestep 632 is tensor(3.8429e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 632 of None
Current timestep = 633. State = [[-0.22108641  0.19453633]]. Action = [[ 0.09399367 -0.04399243  0.         -0.8239386 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 633 is [True, False, False, False, False, True]
State prediction error at timestep 633 is tensor(8.8242e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 633 of None
Current timestep = 634. State = [[-0.2129394   0.19574732]]. Action = [[ 0.0828483  0.0791489  0.        -0.7261174]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 634 is [True, False, False, False, False, True]
State prediction error at timestep 634 is tensor(6.9344e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 634 of None
Current timestep = 635. State = [[-0.20812994  0.20107488]]. Action = [[ 0.02235992  0.09180338  0.         -0.97722065]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 635 is [True, False, False, False, False, True]
State prediction error at timestep 635 is tensor(6.6083e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 635 of None
Current timestep = 636. State = [[-0.20793952  0.19889694]]. Action = [[-0.04709195 -0.09352652  0.         -0.02950972]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 636 is [True, False, False, False, False, True]
State prediction error at timestep 636 is tensor(1.6742e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 636 of None
Current timestep = 637. State = [[-0.20816725  0.19712277]]. Action = [[-0.00825642  0.01305436  0.          0.08223927]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 637 is [True, False, False, False, False, True]
State prediction error at timestep 637 is tensor(6.2521e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 637 of None
Current timestep = 638. State = [[-0.20549515  0.20118617]]. Action = [[ 0.04187209  0.07768012  0.         -0.880095  ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 638 is [True, False, False, False, False, True]
State prediction error at timestep 638 is tensor(6.2109e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 638 of None
Current timestep = 639. State = [[-0.19896314  0.20674562]]. Action = [[ 0.09859139  0.07118354  0.         -0.4972914 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 639 is [True, False, False, False, False, True]
State prediction error at timestep 639 is tensor(8.3432e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 639 of None
Current timestep = 640. State = [[-0.19364324  0.20427632]]. Action = [[ 0.02874706 -0.09478816  0.          0.770751  ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 640 is [True, False, False, False, False, True]
State prediction error at timestep 640 is tensor(8.6524e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 640 of None
Current timestep = 641. State = [[-0.19544445  0.19756849]]. Action = [[-0.08874512 -0.09548942  0.         -0.9475864 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 641 is [True, False, False, False, False, True]
State prediction error at timestep 641 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 641 of None
Current timestep = 642. State = [[-0.19555365  0.19437739]]. Action = [[ 0.01915906 -0.01939958  0.         -0.8775045 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 642 is [True, False, False, False, False, True]
State prediction error at timestep 642 is tensor(6.6183e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 642 of None
Current timestep = 643. State = [[-0.19754036  0.19342935]]. Action = [[-0.07604833 -0.01415092  0.         -0.44371533]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 643 is [True, False, False, False, False, True]
State prediction error at timestep 643 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 643 of None
Current timestep = 644. State = [[-0.20239103  0.19569404]]. Action = [[-0.07567173  0.03949643  0.         -0.5922185 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 644 is [True, False, False, False, False, True]
State prediction error at timestep 644 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 644 of None
Current timestep = 645. State = [[-0.20704725  0.20124236]]. Action = [[-0.05119219  0.07225261  0.         -0.9836193 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 645 is [True, False, False, False, False, True]
State prediction error at timestep 645 is tensor(2.4185e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 645 of None
Current timestep = 646. State = [[-0.21397711  0.20747992]]. Action = [[-0.09477343  0.06180889  0.          0.53159714]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 646 is [True, False, False, False, False, True]
State prediction error at timestep 646 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 646 of None
Current timestep = 647. State = [[-0.21623029  0.21106291]]. Action = [[ 0.03586108  0.0099887   0.         -0.74267554]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 647 is [True, False, False, False, False, True]
State prediction error at timestep 647 is tensor(4.3794e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 647 of None
Current timestep = 648. State = [[-0.22020105  0.21345362]]. Action = [[-0.07099564  0.02001761  0.         -0.9844585 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 648 is [True, False, False, False, False, True]
State prediction error at timestep 648 is tensor(2.2287e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 648 of None
Current timestep = 649. State = [[-0.22659343  0.21577294]]. Action = [[-0.06025081  0.00900584  0.         -0.9542409 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 649 is [True, False, False, False, False, True]
State prediction error at timestep 649 is tensor(2.2477e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 649 of None
Current timestep = 650. State = [[-0.22596888  0.22075202]]. Action = [[0.09271664 0.07988923 0.         0.3213768 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 650 is [True, False, False, False, False, True]
State prediction error at timestep 650 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 650 of None
Current timestep = 651. State = [[-0.22743413  0.22507882]]. Action = [[-0.05365675  0.02400132  0.          0.39675796]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 651 is [True, False, False, False, False, True]
State prediction error at timestep 651 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 651 of None
Current timestep = 652. State = [[-0.23367293  0.22359927]]. Action = [[-0.07413442 -0.07145747  0.         -0.9704362 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 652 is [True, False, False, False, False, True]
State prediction error at timestep 652 is tensor(3.0375e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 652 of None
Current timestep = 653. State = [[-0.24033731  0.22104669]]. Action = [[-0.07228331 -0.04150636  0.         -0.9649498 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 653 is [True, False, False, False, False, True]
State prediction error at timestep 653 is tensor(3.3351e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 653 of None
Current timestep = 654. State = [[-0.24386132  0.21762724]]. Action = [[-0.00897346 -0.0668564   0.          0.09964621]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 654 is [True, False, False, False, False, True]
State prediction error at timestep 654 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 654 of None
Current timestep = 655. State = [[-0.24581861  0.22056587]]. Action = [[-0.01096582  0.09117777  0.         -0.33086097]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 655 is [True, False, False, False, False, True]
State prediction error at timestep 655 is tensor(5.4003e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 655 of None
Current timestep = 656. State = [[-0.24543907  0.22115862]]. Action = [[ 0.04098373 -0.03921248  0.         -0.66430426]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 656 is [True, False, False, False, False, True]
State prediction error at timestep 656 is tensor(7.4150e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 656 of None
Current timestep = 657. State = [[-0.24513121  0.21727437]]. Action = [[-0.00133132 -0.0548162   0.         -0.8661055 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 657 is [True, False, False, False, False, True]
State prediction error at timestep 657 is tensor(6.0004e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 657 of None
Current timestep = 658. State = [[-0.24783719  0.21987088]]. Action = [[-0.04181717  0.09233918  0.         -0.72249055]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 658 is [True, False, False, False, False, True]
State prediction error at timestep 658 is tensor(2.1778e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 658 of None
Current timestep = 659. State = [[-0.24596918  0.22274065]]. Action = [[ 0.08431611  0.01609199  0.         -0.63080114]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 659 is [True, False, False, False, False, True]
State prediction error at timestep 659 is tensor(3.3415e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 659 of None
Current timestep = 660. State = [[-0.24122916  0.22021829]]. Action = [[ 0.06275976 -0.04571314  0.         -0.10829455]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 660 is [True, False, False, False, False, True]
State prediction error at timestep 660 is tensor(2.4730e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 660 of None
Current timestep = 661. State = [[-0.23675181  0.2182385 ]]. Action = [[ 0.0531683   0.0049869   0.         -0.35500515]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 661 is [True, False, False, False, False, True]
State prediction error at timestep 661 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 661 of None
Current timestep = 662. State = [[-0.23268823  0.21688741]]. Action = [[ 0.04372063 -0.00857604  0.          0.735559  ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 662 is [True, False, False, False, False, True]
State prediction error at timestep 662 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 662 of None
Current timestep = 663. State = [[-0.23488975  0.2182345 ]]. Action = [[-0.08275677  0.04598222  0.         -0.80637765]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 663 is [True, False, False, False, False, True]
State prediction error at timestep 663 is tensor(7.4028e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 663 of None
Current timestep = 664. State = [[-0.23348771  0.21932101]]. Action = [[0.0682798  0.00106657 0.         0.8464353 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 664 is [True, False, False, False, False, True]
State prediction error at timestep 664 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 664 of None
Current timestep = 665. State = [[-0.2319701   0.22119457]]. Action = [[-0.01305398  0.04409815  0.         -0.5842229 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 665 is [True, False, False, False, False, True]
State prediction error at timestep 665 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 665 of None
Current timestep = 666. State = [[-0.23272476  0.22088721]]. Action = [[-0.01639304 -0.03170468  0.         -0.98920864]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 666 is [True, False, False, False, False, True]
State prediction error at timestep 666 is tensor(2.7800e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 666 of None
Current timestep = 667. State = [[-0.23687616  0.218717  ]]. Action = [[-0.08640352 -0.03727583  0.         -0.9855602 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 667 is [True, False, False, False, False, True]
State prediction error at timestep 667 is tensor(4.8712e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 667 of None
Current timestep = 668. State = [[-0.2372967   0.21882173]]. Action = [[ 0.03600959  0.01339464  0.         -0.39592373]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 668 is [True, False, False, False, False, True]
State prediction error at timestep 668 is tensor(9.1486e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 668 of None
Current timestep = 669. State = [[-0.23653045  0.22088313]]. Action = [[-0.00793909  0.02860726  0.         -0.33672547]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 669 is [True, False, False, False, False, True]
State prediction error at timestep 669 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 669 of None
Current timestep = 670. State = [[-0.23631418  0.22267702]]. Action = [[ 0.00597535  0.01255739  0.         -0.92794293]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 670 is [True, False, False, False, False, True]
State prediction error at timestep 670 is tensor(7.3727e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 670 of None
Current timestep = 671. State = [[-0.23157983  0.21946958]]. Action = [[ 0.08839173 -0.07600591  0.         -0.88182104]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 671 is [True, False, False, False, False, True]
State prediction error at timestep 671 is tensor(3.7220e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 671 of None
Current timestep = 672. State = [[-0.22725192  0.21624812]]. Action = [[ 0.02246039 -0.01670019  0.         -0.5258692 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 672 is [True, False, False, False, False, True]
State prediction error at timestep 672 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 672 of None
Current timestep = 673. State = [[-0.22102132  0.21579637]]. Action = [[ 0.09420461  0.01364545  0.         -0.01952273]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 673 is [True, False, False, False, False, True]
State prediction error at timestep 673 is tensor(1.3700e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 673 of None
Current timestep = 674. State = [[-0.21821406  0.21434234]]. Action = [[-0.01926376 -0.02296376  0.          0.26151466]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 674 is [True, False, False, False, False, True]
State prediction error at timestep 674 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 674 of None
Current timestep = 675. State = [[-0.22174418  0.21157195]]. Action = [[-0.08713161 -0.04018925  0.         -0.79504156]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 675 is [True, False, False, False, False, True]
State prediction error at timestep 675 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 675 of None
Current timestep = 676. State = [[-0.22413974  0.21365859]]. Action = [[-0.01256095  0.06444701  0.          0.3670429 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 676 is [True, False, False, False, False, True]
State prediction error at timestep 676 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 676 of None
Current timestep = 677. State = [[-0.22349495  0.21255602]]. Action = [[ 0.01439345 -0.05946186  0.         -0.8416075 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 677 is [True, False, False, False, False, True]
State prediction error at timestep 677 is tensor(7.7336e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 677 of None
Current timestep = 678. State = [[-0.2268349   0.21035635]]. Action = [[-0.08731909 -0.01552775  0.         -0.04377836]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 678 is [True, False, False, False, False, True]
State prediction error at timestep 678 is tensor(1.7997e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 678 of None
Current timestep = 679. State = [[-0.2315199   0.20713422]]. Action = [[-0.05243193 -0.06518163  0.         -0.9369584 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 679 is [True, False, False, False, False, True]
State prediction error at timestep 679 is tensor(5.1295e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 679 of None
Current timestep = 680. State = [[-0.23124807  0.20133068]]. Action = [[ 0.03355675 -0.08498718  0.         -0.8815718 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 680 is [True, False, False, False, False, True]
State prediction error at timestep 680 is tensor(4.2613e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 680 of None
Current timestep = 681. State = [[-0.22786607  0.19482899]]. Action = [[ 0.0440359  -0.07082949  0.         -0.44301546]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 681 is [True, False, False, False, False, True]
State prediction error at timestep 681 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 681 of None
Current timestep = 682. State = [[-0.22191216  0.18945542]]. Action = [[ 0.08519159 -0.0403239   0.         -0.15979469]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 682 is [True, False, False, False, False, True]
State prediction error at timestep 682 is tensor(8.9868e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 682 of None
Current timestep = 683. State = [[-0.21743594  0.18174064]]. Action = [[ 0.02591098 -0.09581873  0.         -0.8748261 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 683 is [True, False, False, False, False, True]
State prediction error at timestep 683 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 683 of None
Current timestep = 684. State = [[-0.21333481  0.17792825]]. Action = [[ 0.04686034  0.01823232  0.         -0.5184479 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 684 is [True, False, False, False, False, True]
State prediction error at timestep 684 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 684 of None
Current timestep = 685. State = [[-0.20845804  0.1756235 ]]. Action = [[ 0.05714538 -0.00822493  0.         -0.28413397]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 685 is [True, False, False, False, False, True]
State prediction error at timestep 685 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 685 of None
Current timestep = 686. State = [[-0.20911844  0.1689854 ]]. Action = [[-0.07160991 -0.09235428  0.         -0.1055249 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 686 is [True, False, False, False, False, True]
State prediction error at timestep 686 is tensor(4.2307e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 686 of None
Current timestep = 687. State = [[-0.2094627   0.16681692]]. Action = [[ 0.02530081  0.04818561  0.         -0.36090094]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 687 is [True, False, False, False, False, True]
State prediction error at timestep 687 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 687 of None
Current timestep = 688. State = [[-0.20827118  0.16166879]]. Action = [[ 0.00448124 -0.09167729  0.         -0.54868484]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 688 is [True, False, False, False, False, True]
State prediction error at timestep 688 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 688 of None
Current timestep = 689. State = [[-0.20437616  0.16136105]]. Action = [[ 0.07229792  0.0858844   0.         -0.8707937 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 689 is [True, False, False, False, False, True]
State prediction error at timestep 689 is tensor(3.9873e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 689 of None
Current timestep = 690. State = [[-0.19930463  0.16044246]]. Action = [[ 0.05800996 -0.02692471  0.         -0.13413543]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 690 is [True, False, False, False, False, True]
State prediction error at timestep 690 is tensor(6.6281e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 690 of None
Current timestep = 691. State = [[-0.19410199  0.16203056]]. Action = [[ 0.06374992  0.0805077   0.         -0.37404597]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 691 is [True, False, False, False, False, True]
State prediction error at timestep 691 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 691 of None
Current timestep = 692. State = [[-0.19543074  0.16577092]]. Action = [[-0.07240522  0.05286013  0.         -0.7750932 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 692 is [True, False, False, False, False, True]
State prediction error at timestep 692 is tensor(7.8195e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 692 of None
Current timestep = 693. State = [[-0.1942847   0.16265224]]. Action = [[ 0.06205503 -0.08588717  0.         -0.6878755 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 693 is [True, False, False, False, False, True]
State prediction error at timestep 693 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 693 of None
Current timestep = 694. State = [[-0.19275907  0.15518062]]. Action = [[-0.01352839 -0.09315361  0.         -0.41945982]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 694 is [True, False, False, False, False, True]
State prediction error at timestep 694 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 694 of None
Current timestep = 695. State = [[-0.19382747  0.15250129]]. Action = [[-0.03344549  0.00944833  0.          0.08609581]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 695 is [True, False, False, False, False, True]
State prediction error at timestep 695 is tensor(7.7194e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 695 of None
Current timestep = 696. State = [[-0.19156834  0.14759408]]. Action = [[ 0.04995548 -0.0933864   0.         -0.9164132 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 696 is [True, False, False, False, False, True]
State prediction error at timestep 696 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 696 of None
Current timestep = 697. State = [[-0.18710549  0.14569545]]. Action = [[ 0.04605327  0.03356037  0.         -0.89172107]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 697 is [True, False, False, False, False, True]
State prediction error at timestep 697 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 697 of None
Current timestep = 698. State = [[-0.1833998   0.14602661]]. Action = [[0.03498278 0.01111311 0.         0.21686244]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 698 is [True, False, False, False, False, True]
State prediction error at timestep 698 is tensor(1.0201e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 698 of None
Current timestep = 699. State = [[-0.17708772  0.14903232]]. Action = [[ 0.09845764  0.07619444  0.         -0.6925963 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 699 is [True, False, False, False, False, True]
State prediction error at timestep 699 is tensor(6.2244e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 699 of None
Current timestep = 700. State = [[-0.17027943  0.15030925]]. Action = [[ 0.07389463  0.00485037  0.         -0.7112819 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 700 is [True, False, False, False, False, True]
State prediction error at timestep 700 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 700 of None
Current timestep = 701. State = [[-0.16498777  0.154265  ]]. Action = [[0.05006295 0.09374186 0.         0.4150772 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 701 is [True, False, False, False, False, True]
State prediction error at timestep 701 is tensor(7.8123e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 701 of None
Current timestep = 702. State = [[-0.16528334  0.15710425]]. Action = [[-0.05336875  0.00870063  0.         -0.54655534]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 702 is [True, False, False, False, False, True]
State prediction error at timestep 702 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 702 of None
Current timestep = 703. State = [[-0.16228144  0.16069123]]. Action = [[0.08303607 0.06308196 0.         0.28545046]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 703 is [True, False, False, False, False, True]
State prediction error at timestep 703 is tensor(3.0838e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 703 of None
Current timestep = 704. State = [[-0.16253714  0.16673383]]. Action = [[-0.06455845  0.07499553  0.         -0.09456444]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 704 is [True, False, False, False, False, True]
State prediction error at timestep 704 is tensor(1.6618e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 704 of None
Current timestep = 705. State = [[-0.15969037  0.1657738 ]]. Action = [[ 0.08099215 -0.0860699   0.         -0.8776778 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 705 is [True, False, False, False, False, True]
State prediction error at timestep 705 is tensor(3.4134e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 705 of None
Current timestep = 706. State = [[-0.15767622  0.16764551]]. Action = [[-0.02161039  0.06947843  0.         -0.45366234]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 706 is [True, False, False, False, False, True]
State prediction error at timestep 706 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 706 of None
Current timestep = 707. State = [[-0.15686238  0.17432913]]. Action = [[0.01543687 0.073479   0.         0.00144446]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 707 is [True, False, False, False, False, True]
State prediction error at timestep 707 is tensor(2.5519e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 707 of None
Current timestep = 708. State = [[-0.1591824   0.18165414]]. Action = [[-0.06125847  0.07362426  0.         -0.95208585]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 708 is [True, False, False, False, False, True]
State prediction error at timestep 708 is tensor(1.7218e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 708 of None
Current timestep = 709. State = [[-0.159296    0.18560125]]. Action = [[ 0.02910168 -0.00277322  0.         -0.6312248 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 709 is [True, False, False, False, False, True]
State prediction error at timestep 709 is tensor(5.3474e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 709 of None
Current timestep = 710. State = [[-0.16219701  0.18404758]]. Action = [[-0.08801895 -0.07487424  0.         -0.7066295 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 710 is [True, False, False, False, False, True]
State prediction error at timestep 710 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 710 of None
Current timestep = 711. State = [[-0.16722609  0.18351777]]. Action = [[-0.06638209 -0.01224699  0.         -0.79488623]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 711 is [True, False, False, False, False, True]
State prediction error at timestep 711 is tensor(5.7990e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 711 of None
Current timestep = 712. State = [[-0.1719394   0.18600741]]. Action = [[-0.06004999  0.01945768  0.          0.03730249]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 712 is [True, False, False, False, False, True]
State prediction error at timestep 712 is tensor(3.1941e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 712 of None
Current timestep = 713. State = [[-0.17528525  0.18663351]]. Action = [[-0.02921057 -0.03419071  0.         -0.82578933]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 713 is [True, False, False, False, False, True]
State prediction error at timestep 713 is tensor(4.6658e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 713 of None
Current timestep = 714. State = [[-0.18024524  0.19012125]]. Action = [[-0.07490948  0.05958103  0.         -0.9929851 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 714 is [True, False, False, False, False, True]
State prediction error at timestep 714 is tensor(3.8945e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 714 of None
Current timestep = 715. State = [[-0.18629068  0.19312108]]. Action = [[-0.06125618 -0.00299867  0.          0.11581945]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 715 is [True, False, False, False, False, True]
State prediction error at timestep 715 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 715 of None
Current timestep = 716. State = [[-0.18765023  0.19246374]]. Action = [[ 0.03033621 -0.03789983  0.         -0.33604884]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 716 is [True, False, False, False, False, True]
State prediction error at timestep 716 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 716 of None
Current timestep = 717. State = [[-0.19209035  0.19276838]]. Action = [[-0.08551618  0.01362847  0.         -0.54537874]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 717 is [True, False, False, False, False, True]
State prediction error at timestep 717 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 717 of None
Current timestep = 718. State = [[-0.19360991  0.19325513]]. Action = [[ 0.042078   -0.00858474  0.         -0.8512873 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 718 is [True, False, False, False, False, True]
State prediction error at timestep 718 is tensor(1.8070e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 718 of None
Current timestep = 719. State = [[-0.19686791  0.1956259 ]]. Action = [[-0.06211795  0.04945264  0.          0.28851616]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 719 is [True, False, False, False, False, True]
State prediction error at timestep 719 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 719 of None
Current timestep = 720. State = [[-0.2005901   0.20089844]]. Action = [[-0.0086655   0.07402278  0.         -0.71818745]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 720 is [True, False, False, False, False, True]
State prediction error at timestep 720 is tensor(1.2731e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 720 of None
Current timestep = 721. State = [[-0.19780618  0.2000557 ]]. Action = [[ 0.09895653 -0.06011579  0.         -0.8354593 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 721 is [True, False, False, False, False, True]
State prediction error at timestep 721 is tensor(2.8276e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 721 of None
Current timestep = 722. State = [[-0.20009048  0.20056912]]. Action = [[-0.07921505  0.05199658  0.         -0.02548915]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 722 is [True, False, False, False, False, True]
State prediction error at timestep 722 is tensor(1.6862e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 722 of None
Current timestep = 723. State = [[-0.20355757  0.20633519]]. Action = [[ 0.00144859  0.08666148  0.         -0.7722773 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 723 is [True, False, False, False, False, True]
State prediction error at timestep 723 is tensor(1.9469e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 723 of None
Current timestep = 724. State = [[-0.20255353  0.2114253 ]]. Action = [[ 0.05666017  0.05177224  0.         -0.57580626]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 724 is [True, False, False, False, False, True]
State prediction error at timestep 724 is tensor(2.2671e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 724 of None
Current timestep = 725. State = [[-0.19952844  0.21255091]]. Action = [[ 0.05685186 -0.00686871  0.          0.35779536]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 725 is [True, False, False, False, False, True]
State prediction error at timestep 725 is tensor(6.5647e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 725 of None
Current timestep = 726. State = [[-0.19419904  0.20770468]]. Action = [[ 0.08492806 -0.09248821  0.         -0.36755216]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 726 is [True, False, False, False, False, True]
State prediction error at timestep 726 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 726 of None
Current timestep = 727. State = [[-0.1868536   0.20086703]]. Action = [[ 0.09021271 -0.07614396  0.         -0.4142195 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 727 is [True, False, False, False, False, True]
State prediction error at timestep 727 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 727 of None
Current timestep = 728. State = [[-0.18472053  0.19750474]]. Action = [[-0.03364623 -0.00803602  0.          0.83901906]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 728 is [True, False, False, False, False, True]
State prediction error at timestep 728 is tensor(1.0281e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 728 of None
Current timestep = 729. State = [[-0.18090275  0.19411837]]. Action = [[ 0.06637654 -0.04978693  0.         -0.38389885]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 729 is [True, False, False, False, False, True]
State prediction error at timestep 729 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 729 of None
Current timestep = 730. State = [[-0.18137434  0.1913752 ]]. Action = [[-0.08145909 -0.0131226   0.          0.19101381]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 730 is [True, False, False, False, False, True]
State prediction error at timestep 730 is tensor(6.1725e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 730 of None
Current timestep = 731. State = [[-0.1854289   0.19373879]]. Action = [[-0.06649695  0.061144    0.         -0.7846142 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 731 is [True, False, False, False, False, True]
State prediction error at timestep 731 is tensor(2.9802e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 731 of None
Current timestep = 732. State = [[-0.184528    0.19096729]]. Action = [[ 0.04259891 -0.0893927   0.          0.54204977]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 732 is [True, False, False, False, False, True]
State prediction error at timestep 732 is tensor(6.0834e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 732 of None
Current timestep = 733. State = [[-0.18725148  0.19147645]]. Action = [[-0.09312073  0.0679263   0.         -0.9412025 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 733 is [True, False, False, False, False, True]
State prediction error at timestep 733 is tensor(7.8132e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 733 of None
Current timestep = 734. State = [[-0.18650334  0.19381897]]. Action = [[ 0.06845092  0.01049823  0.         -0.74334   ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 734 is [True, False, False, False, False, True]
State prediction error at timestep 734 is tensor(3.9975e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 734 of None
Current timestep = 735. State = [[-0.1879877   0.19111441]]. Action = [[-0.07435589 -0.06626314  0.         -0.6629184 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 735 is [True, False, False, False, False, True]
State prediction error at timestep 735 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 735 of None
Current timestep = 736. State = [[-0.18686388  0.18450111]]. Action = [[ 0.06213946 -0.09496504  0.         -0.0999583 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 736 is [True, False, False, False, False, True]
State prediction error at timestep 736 is tensor(3.0541e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 736 of None
Current timestep = 737. State = [[-0.18671843  0.17912717]]. Action = [[-0.03638103 -0.04028258  0.         -0.93581367]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 737 is [True, False, False, False, False, True]
State prediction error at timestep 737 is tensor(6.1743e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 737 of None
Current timestep = 738. State = [[-0.18642357  0.17824845]]. Action = [[ 0.02146723  0.02236868  0.         -0.81616735]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 738 is [True, False, False, False, False, True]
State prediction error at timestep 738 is tensor(3.9414e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 738 of None
Current timestep = 739. State = [[-0.18938038  0.18159968]]. Action = [[-0.06649239  0.07195691  0.         -0.7614515 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 739 is [True, False, False, False, False, True]
State prediction error at timestep 739 is tensor(2.8924e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 739 of None
Current timestep = 740. State = [[-0.19602308  0.18546943]]. Action = [[-0.08288784  0.04066379  0.         -0.60546315]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 740 is [True, False, False, False, False, True]
State prediction error at timestep 740 is tensor(2.4765e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 740 of None
Current timestep = 741. State = [[-0.19552056  0.18568431]]. Action = [[ 0.08350687 -0.01938571  0.          0.49074173]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 741 is [True, False, False, False, False, True]
State prediction error at timestep 741 is tensor(2.2341e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 741 of None
Current timestep = 742. State = [[-0.19389246  0.1879456 ]]. Action = [[0.00914097 0.0648182  0.         0.45006108]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 742 is [True, False, False, False, False, True]
State prediction error at timestep 742 is tensor(5.8743e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 742 of None
Current timestep = 743. State = [[-0.19159642  0.19212002]]. Action = [[ 0.06239968  0.05615527  0.         -0.89179873]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 743 is [True, False, False, False, False, True]
State prediction error at timestep 743 is tensor(5.8746e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 743 of None
Current timestep = 744. State = [[-0.190715    0.19598654]]. Action = [[0.00363304 0.05123831 0.         0.9591639 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 744 is [True, False, False, False, False, True]
State prediction error at timestep 744 is tensor(4.3267e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 744 of None
Current timestep = 745. State = [[-0.19484313  0.19433399]]. Action = [[-0.07911585 -0.07326297  0.         -0.7796792 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 745 is [True, False, False, False, False, True]
State prediction error at timestep 745 is tensor(6.3736e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 745 of None
Current timestep = 746. State = [[-0.19324672  0.19098064]]. Action = [[ 0.09206713 -0.03250124  0.         -0.89114326]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 746 is [True, False, False, False, False, True]
State prediction error at timestep 746 is tensor(1.8391e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 746 of None
Current timestep = 747. State = [[-0.19386102  0.19281854]]. Action = [[-0.06245117  0.05651661  0.          0.32625747]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 747 is [True, False, False, False, False, True]
State prediction error at timestep 747 is tensor(3.9956e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 747 of None
Current timestep = 748. State = [[-0.19103707  0.1949288 ]]. Action = [[0.09414282 0.0051381  0.         0.17089558]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 748 is [True, False, False, False, False, True]
State prediction error at timestep 748 is tensor(1.1111e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 748 of None
Current timestep = 749. State = [[-0.18999241  0.19206989]]. Action = [[-0.04240106 -0.06889234  0.          0.5379673 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 749 is [True, False, False, False, False, True]
State prediction error at timestep 749 is tensor(2.7201e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 749 of None
Current timestep = 750. State = [[-0.19053848  0.186194  ]]. Action = [[-0.00203671 -0.08575591  0.         -0.16753787]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 750 is [True, False, False, False, False, True]
State prediction error at timestep 750 is tensor(4.6952e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 750 of None
Current timestep = 751. State = [[-0.19310011  0.18264452]]. Action = [[-0.06616767 -0.0233991   0.         -0.01850593]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 751 is [True, False, False, False, False, True]
State prediction error at timestep 751 is tensor(4.4908e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 751 of None
Current timestep = 752. State = [[-0.19566026  0.18603365]]. Action = [[-0.02124311  0.0830744   0.         -0.9954066 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 752 is [True, False, False, False, False, True]
State prediction error at timestep 752 is tensor(1.5876e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 752 of None
Current timestep = 753. State = [[-0.1917676   0.18338764]]. Action = [[ 0.09484438 -0.09882001  0.         -0.00982201]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 753 is [True, False, False, False, False, True]
State prediction error at timestep 753 is tensor(1.3498e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 753 of None
Current timestep = 754. State = [[-0.18385343  0.17566226]]. Action = [[ 0.09822422 -0.08116858  0.         -0.00965673]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 754 is [True, False, False, False, False, True]
State prediction error at timestep 754 is tensor(3.8672e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 754 of None
Current timestep = 755. State = [[-0.17701465  0.17502117]]. Action = [[ 0.06485412  0.07072622  0.         -0.4990071 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 755 is [True, False, False, False, False, True]
State prediction error at timestep 755 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 755 of None
Current timestep = 756. State = [[-0.17790227  0.17222404]]. Action = [[-0.08729248 -0.07455187  0.         -0.951071  ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 756 is [True, False, False, False, False, True]
State prediction error at timestep 756 is tensor(4.8058e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 756 of None
Current timestep = 757. State = [[-0.17537342  0.16708261]]. Action = [[ 0.09766882 -0.03400389  0.         -0.53300124]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 757 is [True, False, False, False, False, True]
State prediction error at timestep 757 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 757 of None
Current timestep = 758. State = [[-0.1755212   0.16589896]]. Action = [[-0.06972717  0.02744371  0.         -0.26083332]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 758 is [True, False, False, False, False, True]
State prediction error at timestep 758 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 758 of None
Current timestep = 759. State = [[-0.17434442  0.16861539]]. Action = [[ 0.05975468  0.0647266   0.         -0.52785385]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 759 is [True, False, False, False, False, True]
State prediction error at timestep 759 is tensor(4.5526e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 759 of None
Current timestep = 760. State = [[-0.17414704  0.16870168]]. Action = [[-0.03111105 -0.01751219  0.          0.36060333]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 760 is [True, False, False, False, False, True]
State prediction error at timestep 760 is tensor(1.1353e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 760 of None
Current timestep = 761. State = [[-0.17867404  0.1717898 ]]. Action = [[-0.07638167  0.0779795   0.          0.5446844 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 761 is [True, False, False, False, False, True]
State prediction error at timestep 761 is tensor(1.5320e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 761 of None
Current timestep = 762. State = [[-0.17946213  0.17363118]]. Action = [[ 0.03780248 -0.0087795   0.         -0.5450827 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 762 is [True, False, False, False, False, True]
State prediction error at timestep 762 is tensor(4.3937e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 762 of None
Current timestep = 763. State = [[-0.18201646  0.17742221]]. Action = [[-0.05967756  0.07582349  0.         -0.98712957]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 763 is [True, False, False, False, False, True]
State prediction error at timestep 763 is tensor(9.2117e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 763 of None
Current timestep = 764. State = [[-0.18393613  0.17723499]]. Action = [[ 0.00795119 -0.05990616  0.         -0.01541817]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 764 is [True, False, False, False, False, True]
State prediction error at timestep 764 is tensor(8.5345e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 764 of None
Current timestep = 765. State = [[-0.18164843  0.17211229]]. Action = [[ 0.04944909 -0.08039597  0.         -0.51731336]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 765 is [True, False, False, False, False, True]
State prediction error at timestep 765 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 765 of None
Current timestep = 766. State = [[-0.18169984  0.16927576]]. Action = [[-0.0314877  -0.01182826  0.         -0.3450958 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 766 is [True, False, False, False, False, True]
State prediction error at timestep 766 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 766 of None
Current timestep = 767. State = [[-0.18482043  0.17089827]]. Action = [[-0.04778857  0.03722338  0.         -0.761178  ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 767 is [True, False, False, False, False, True]
State prediction error at timestep 767 is tensor(5.3105e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 767 of None
Current timestep = 768. State = [[-0.19001585  0.17550106]]. Action = [[-0.07015808  0.06272685  0.         -0.2930509 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 768 is [True, False, False, False, False, True]
State prediction error at timestep 768 is tensor(4.0836e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 768 of None
Current timestep = 769. State = [[-0.19278331  0.18214957]]. Action = [[ 0.00498321  0.08471084  0.         -0.97801226]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 769 is [True, False, False, False, False, True]
State prediction error at timestep 769 is tensor(2.9799e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 769 of None
Current timestep = 770. State = [[-0.19303966  0.18982819]]. Action = [[ 0.02555949  0.09421455  0.         -0.7884393 ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 770 is [True, False, False, False, False, True]
State prediction error at timestep 770 is tensor(2.1809e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 770 of None
Current timestep = 771. State = [[-0.19706428  0.1902237 ]]. Action = [[-0.07542247 -0.0694467   0.         -0.82025856]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 771 is [True, False, False, False, False, True]
State prediction error at timestep 771 is tensor(3.9852e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 771 of None
Current timestep = 772. State = [[-0.20487505  0.18543203]]. Action = [[-0.09350871 -0.08489807  0.          0.1664561 ]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 772 is [True, False, False, False, False, True]
State prediction error at timestep 772 is tensor(8.4050e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 772 of None
Current timestep = 773. State = [[-0.20668301  0.17992477]]. Action = [[ 0.04065578 -0.07902218  0.         -0.6890601 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 773 is [True, False, False, False, False, True]
State prediction error at timestep 773 is tensor(5.4331e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 773 of None
Current timestep = 774. State = [[-0.20382467  0.17896521]]. Action = [[ 0.05450998  0.02899978  0.         -0.11727142]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 774 is [True, False, False, False, False, True]
State prediction error at timestep 774 is tensor(4.1107e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 774 of None
Current timestep = 775. State = [[-0.20208383  0.18302657]]. Action = [[ 0.02272024  0.07983773  0.         -0.9178512 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 775 is [True, False, False, False, False, True]
State prediction error at timestep 775 is tensor(2.5486e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 775 of None
Current timestep = 776. State = [[-0.20379758  0.18163835]]. Action = [[-0.03530105 -0.0683025   0.         -0.7157455 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 776 is [True, False, False, False, False, True]
State prediction error at timestep 776 is tensor(7.8881e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 776 of None
Current timestep = 777. State = [[-0.20194182  0.17606503]]. Action = [[ 0.06669145 -0.06740217  0.         -0.8612726 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 777 is [True, False, False, False, False, True]
State prediction error at timestep 777 is tensor(4.8588e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 777 of None
Current timestep = 778. State = [[-0.1977369   0.17376553]]. Action = [[ 0.05178187  0.0151819   0.         -0.02401131]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 778 is [True, False, False, False, False, True]
State prediction error at timestep 778 is tensor(2.0023e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 778 of None
Current timestep = 779. State = [[-0.19924554  0.17152181]]. Action = [[-0.06659424 -0.03325915  0.         -0.79453534]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 779 is [True, False, False, False, False, True]
State prediction error at timestep 779 is tensor(8.3636e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 779 of None
Current timestep = 780. State = [[-0.20477085  0.1656378 ]]. Action = [[-0.08136597 -0.09014343  0.          0.33865452]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 780 is [True, False, False, False, False, True]
State prediction error at timestep 780 is tensor(1.8309e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 780 of None
Current timestep = 781. State = [[-0.20291376  0.16290477]]. Action = [[ 0.09623004  0.0187216   0.         -0.9742958 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 781 is [True, False, False, False, False, True]
State prediction error at timestep 781 is tensor(2.4899e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 781 of None
Current timestep = 782. State = [[-0.1950584   0.15948184]]. Action = [[ 0.09563532 -0.05115573  0.          0.19943571]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 782 is [True, False, False, False, False, True]
State prediction error at timestep 782 is tensor(2.6911e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 782 of None
Current timestep = 783. State = [[-0.18808073  0.15660761]]. Action = [[ 0.06319001  0.0029721   0.         -0.9405144 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 783 is [True, False, False, False, False, True]
State prediction error at timestep 783 is tensor(9.6946e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 783 of None
Current timestep = 784. State = [[-0.18112078  0.15607288]]. Action = [[0.07991765 0.02307348 0.         0.7791686 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 784 is [True, False, False, False, False, True]
State prediction error at timestep 784 is tensor(6.8498e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 784 of None
Current timestep = 785. State = [[-0.18014075  0.15609987]]. Action = [[-0.05479734  0.01517282  0.          0.29862905]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 785 is [True, False, False, False, False, True]
State prediction error at timestep 785 is tensor(2.1225e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 785 of None
Current timestep = 786. State = [[-0.17841743  0.15977378]]. Action = [[ 0.04913033  0.08503402  0.         -0.37618297]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 786 is [True, False, False, False, False, True]
State prediction error at timestep 786 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 786 of None
Current timestep = 787. State = [[-0.1788852   0.15793566]]. Action = [[-0.05649011 -0.07784439  0.         -0.4015621 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 787 is [True, False, False, False, False, True]
State prediction error at timestep 787 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 787 of None
Current timestep = 788. State = [[-0.17960013  0.15990649]]. Action = [[ 0.00803082  0.08802976  0.         -0.7093947 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 788 is [True, False, False, False, False, True]
State prediction error at timestep 788 is tensor(3.7687e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 788 of None
Current timestep = 789. State = [[-0.1771965   0.16354458]]. Action = [[ 0.04630091  0.02762879  0.         -0.06905138]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 789 is [True, False, False, False, False, True]
State prediction error at timestep 789 is tensor(9.3241e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 789 of None
Current timestep = 790. State = [[-0.17971955  0.16294879]]. Action = [[-0.08638468 -0.03741048  0.          0.83164144]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 790 is [True, False, False, False, False, True]
State prediction error at timestep 790 is tensor(7.9247e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 790 of None
Current timestep = 791. State = [[-0.18581548  0.1598439 ]]. Action = [[-0.08123295 -0.05831217  0.         -0.97657406]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 791 is [True, False, False, False, False, True]
State prediction error at timestep 791 is tensor(1.9225e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 791 of None
Current timestep = 792. State = [[-0.19252323  0.16197816]]. Action = [[-0.08639898  0.05900807  0.         -0.8389355 ]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 792 is [True, False, False, False, False, True]
State prediction error at timestep 792 is tensor(7.1960e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 792 of None
Current timestep = 793. State = [[-0.19154832  0.16470109]]. Action = [[0.09414358 0.00993919 0.         0.24174225]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 793 is [True, False, False, False, False, True]
State prediction error at timestep 793 is tensor(8.7087e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 793 of None
Current timestep = 794. State = [[-0.19231847  0.16503559]]. Action = [[-0.05716166 -0.01107396  0.         -0.8720465 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 794 is [True, False, False, False, False, True]
State prediction error at timestep 794 is tensor(4.3350e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 794 of None
Current timestep = 795. State = [[-0.19177277  0.16444689]]. Action = [[ 0.05318899 -0.01439317  0.         -0.9853858 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 795 is [True, False, False, False, False, True]
State prediction error at timestep 795 is tensor(5.7856e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 795 of None
Current timestep = 796. State = [[-0.19145918  0.1613872 ]]. Action = [[-0.01473095 -0.05496125  0.         -0.04101241]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 796 is [True, False, False, False, False, True]
State prediction error at timestep 796 is tensor(1.6144e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 796 of None
Current timestep = 797. State = [[-0.19285472  0.15883732]]. Action = [[-0.0198672  -0.02039111  0.         -0.7592256 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 797 is [True, False, False, False, False, True]
State prediction error at timestep 797 is tensor(7.7905e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 797 of None
Current timestep = 798. State = [[-0.19454546  0.15604301]]. Action = [[-0.0218766  -0.04218298  0.         -0.7852765 ]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 798 is [True, False, False, False, False, True]
State prediction error at timestep 798 is tensor(7.9742e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 798 of None
Current timestep = 799. State = [[-0.1952285   0.15547429]]. Action = [[ 0.00078012  0.01820868  0.         -0.6721428 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 799 is [True, False, False, False, False, True]
State prediction error at timestep 799 is tensor(7.4490e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 799 of None
Current timestep = 800. State = [[-0.19296008  0.15643573]]. Action = [[ 0.05318733  0.02104495  0.         -0.9283992 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 800 is [True, False, False, False, False, True]
State prediction error at timestep 800 is tensor(3.2419e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 800 of None
Current timestep = 801. State = [[-0.18724671  0.15404256]]. Action = [[ 0.09341408 -0.04232084  0.         -0.43570817]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 801 is [True, False, False, False, False, True]
State prediction error at timestep 801 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 801 of None
Current timestep = 802. State = [[-0.18346453  0.14725874]]. Action = [[ 0.01723769 -0.09009178  0.          0.80735683]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 802 is [True, False, False, False, False, True]
State prediction error at timestep 802 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 802 of None
Current timestep = 803. State = [[-0.18448077  0.14466867]]. Action = [[-0.04701221  0.02222691  0.         -0.8684809 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 803 is [True, False, False, False, False, True]
State prediction error at timestep 803 is tensor(5.5534e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 803 of None
Current timestep = 804. State = [[-0.18229365  0.14644657]]. Action = [[ 0.06520989  0.04654033  0.         -0.21148145]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 804 is [True, False, False, False, False, True]
State prediction error at timestep 804 is tensor(6.4421e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 804 of None
Current timestep = 805. State = [[-0.17599805  0.14574836]]. Action = [[ 0.08770282 -0.01661402  0.         -0.17325044]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 805 is [True, False, False, False, False, True]
State prediction error at timestep 805 is tensor(8.0568e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 805 of None
Current timestep = 806. State = [[-0.169079    0.13979802]]. Action = [[ 0.07502035 -0.0842034   0.         -0.4269899 ]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 806 is [True, False, False, False, False, True]
State prediction error at timestep 806 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 806 of None
Current timestep = 807. State = [[-0.16223837  0.13937537]]. Action = [[ 0.07280185  0.07264955  0.         -0.07721984]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 807 is [True, False, False, False, False, True]
State prediction error at timestep 807 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 807 of None
Current timestep = 808. State = [[-0.15734512  0.14474335]]. Action = [[0.03995537 0.09839117 0.         0.15999722]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 808 is [True, False, False, False, False, True]
State prediction error at timestep 808 is tensor(5.4691e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 808 of None
Current timestep = 809. State = [[-0.15686202  0.14468586]]. Action = [[-0.03611841 -0.04542102  0.          0.5728071 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 809 is [True, False, False, False, False, True]
State prediction error at timestep 809 is tensor(3.3411e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 809 of None
Current timestep = 810. State = [[-0.15348883  0.14078014]]. Action = [[ 0.06449436 -0.04853399  0.         -0.5285346 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 810 is [True, False, False, False, False, True]
State prediction error at timestep 810 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 810 of None
Current timestep = 811. State = [[-0.15199581  0.14349656]]. Action = [[-0.02817736  0.09283025  0.         -0.301669  ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 811 is [True, False, False, False, False, True]
State prediction error at timestep 811 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 811 of None
Current timestep = 812. State = [[-0.14937373  0.14421788]]. Action = [[ 0.04984509 -0.03862877  0.         -0.6406912 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 812 is [True, False, False, False, False, True]
State prediction error at timestep 812 is tensor(9.8742e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 812 of None
Current timestep = 813. State = [[-0.14621268  0.14184316]]. Action = [[ 0.01328947 -0.02961161  0.         -0.84497255]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 813 is [True, False, False, False, False, True]
State prediction error at timestep 813 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 813 of None
Current timestep = 814. State = [[-0.14795142  0.14021719]]. Action = [[-0.07316679 -0.02063358  0.         -0.8969563 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 814 is [True, False, False, False, False, True]
State prediction error at timestep 814 is tensor(6.1882e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 814 of None
Current timestep = 815. State = [[-0.14641204  0.13713911]]. Action = [[ 0.0444978  -0.05875156  0.         -0.9042825 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 815 is [True, False, False, False, False, True]
State prediction error at timestep 815 is tensor(5.3147e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 815 of None
Current timestep = 816. State = [[-0.13986456  0.14006256]]. Action = [[ 0.09090043  0.09690524  0.         -0.9060341 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 816 is [True, False, False, False, False, True]
State prediction error at timestep 816 is tensor(4.9169e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 816 of None
Current timestep = 817. State = [[-0.13367622  0.1418546 ]]. Action = [[ 0.05868029 -0.01219621  0.         -0.5054985 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 817 is [True, False, False, False, False, True]
State prediction error at timestep 817 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 817 of None
Current timestep = 818. State = [[-0.12687269  0.13722406]]. Action = [[ 0.07802653 -0.08281392  0.         -0.90674835]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 818 is [True, False, False, False, False, True]
State prediction error at timestep 818 is tensor(9.9182e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 818 of None
Current timestep = 819. State = [[-0.12199892  0.13697645]]. Action = [[ 0.02250241  0.05541243  0.         -0.74435097]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 819 is [True, False, False, False, False, True]
State prediction error at timestep 819 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 819 of None
Current timestep = 820. State = [[-0.12111092  0.14031115]]. Action = [[-0.02385489  0.04344568  0.         -0.6893322 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 820 is [True, False, False, False, False, True]
State prediction error at timestep 820 is tensor(8.9144e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 820 of None
Current timestep = 821. State = [[-0.12058946  0.14465453]]. Action = [[ 0.00105224  0.05748702  0.         -0.7532946 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 821 is [True, False, False, False, False, True]
State prediction error at timestep 821 is tensor(4.8506e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 821 of None
Current timestep = 822. State = [[-0.12370177  0.14701755]]. Action = [[-8.3768308e-02 -2.7845055e-04  0.0000000e+00 -3.2602310e-01]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 822 is [True, False, False, False, False, True]
State prediction error at timestep 822 is tensor(8.6634e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 822 of None
Current timestep = 823. State = [[-0.1238711   0.14867082]]. Action = [[0.02856057 0.01160879 0.         0.6682048 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 823 is [True, False, False, False, False, True]
State prediction error at timestep 823 is tensor(1.7407e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 823 of None
Current timestep = 824. State = [[-0.12460189  0.14798395]]. Action = [[-0.04379533 -0.04085473  0.         -0.8989877 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 824 is [True, False, False, False, False, True]
State prediction error at timestep 824 is tensor(4.1602e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 824 of None
Current timestep = 825. State = [[-0.12253784  0.15134344]]. Action = [[ 0.05869288  0.07370894  0.         -0.82746816]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 825 is [True, False, False, False, False, True]
State prediction error at timestep 825 is tensor(3.0416e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 825 of None
Current timestep = 826. State = [[-0.11672271  0.1521524 ]]. Action = [[ 0.07892118 -0.03736067  0.         -0.8511992 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 826 is [True, False, False, False, False, True]
State prediction error at timestep 826 is tensor(5.5267e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 826 of None
Current timestep = 827. State = [[-0.11538351  0.14982833]]. Action = [[-0.03853248 -0.03683665  0.          0.58830893]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 827 is [True, False, False, False, False, True]
State prediction error at timestep 827 is tensor(6.8600e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 827 of None
Current timestep = 828. State = [[-0.11316169  0.15137492]]. Action = [[ 0.04888571  0.04592545  0.         -0.9445583 ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 828 is [True, False, False, False, False, True]
State prediction error at timestep 828 is tensor(1.8103e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 828 of None
Current timestep = 829. State = [[-0.11409966  0.15411086]]. Action = [[-0.06114295  0.02110831  0.         -0.267821  ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 829 is [True, False, False, False, False, True]
State prediction error at timestep 829 is tensor(9.2322e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 829 of None
Current timestep = 830. State = [[-0.11499421  0.15608087]]. Action = [[ 0.00409018  0.01170845  0.         -0.40159595]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 830 is [True, False, False, False, False, True]
State prediction error at timestep 830 is tensor(7.7186e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 830 of None
Current timestep = 831. State = [[-0.11851908  0.15505446]]. Action = [[-0.08485473 -0.04531053  0.         -0.42926586]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 831 is [True, False, False, False, False, True]
State prediction error at timestep 831 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 831 of None
Current timestep = 832. State = [[-0.11876421  0.15885918]]. Action = [[ 0.04640882  0.08958895  0.         -0.952247  ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 832 is [True, False, False, False, False, True]
State prediction error at timestep 832 is tensor(4.4158e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 832 of None
Current timestep = 833. State = [[-0.11421316  0.15808877]]. Action = [[ 0.06463823 -0.0776719   0.         -0.7500523 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 833 is [True, False, False, False, False, True]
State prediction error at timestep 833 is tensor(6.4979e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 833 of None
Current timestep = 834. State = [[-0.11059     0.16032977]]. Action = [[ 0.0254031   0.08487298  0.         -0.9707994 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 834 is [True, False, False, False, False, True]
State prediction error at timestep 834 is tensor(9.5935e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 834 of None
Current timestep = 835. State = [[-0.10706402  0.16705056]]. Action = [[ 0.05439211  0.08461481  0.         -0.10137272]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 835 is [True, False, False, False, False, True]
State prediction error at timestep 835 is tensor(6.7327e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 835 of None
Current timestep = 836. State = [[-0.10161368  0.17386071]]. Action = [[ 0.08472612  0.08274011  0.         -0.68601465]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 836 is [True, False, False, False, False, True]
State prediction error at timestep 836 is tensor(5.5577e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 836 of None
Current timestep = 837. State = [[-0.09607015  0.17918642]]. Action = [[ 0.06780443  0.05397529  0.         -0.5958774 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 837 is [True, False, False, False, False, True]
State prediction error at timestep 837 is tensor(7.6568e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 837 of None
Current timestep = 838. State = [[-0.08974023  0.17738132]]. Action = [[ 0.08159382 -0.07596143  0.         -0.9453525 ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 838 is [True, False, False, False, False, True]
State prediction error at timestep 838 is tensor(5.5817e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 838 of None
Current timestep = 839. State = [[-0.08323535  0.17367321]]. Action = [[ 0.06138403 -0.03627026  0.         -0.82736075]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 839 is [True, False, False, False, False, True]
State prediction error at timestep 839 is tensor(8.6612e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 839 of None
Current timestep = 840. State = [[-0.07649596  0.17382854]]. Action = [[ 0.06973734  0.02835866  0.         -0.8155819 ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 840 is [True, False, False, False, False, True]
State prediction error at timestep 840 is tensor(8.6993e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 840 of None
Current timestep = 841. State = [[-0.07703693  0.17882343]]. Action = [[-0.09003887  0.08160429  0.          0.37636757]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 841 is [True, False, False, False, False, True]
State prediction error at timestep 841 is tensor(1.6048e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 841 of None
Current timestep = 842. State = [[-0.07499424  0.18376917]]. Action = [[ 0.06702308  0.03801759  0.         -0.50322247]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 842 is [True, False, False, False, False, True]
State prediction error at timestep 842 is tensor(4.7452e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 842 of None
Current timestep = 843. State = [[-0.07432514  0.18718126]]. Action = [[-0.04830685  0.02592341  0.         -0.66850996]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 843 is [True, False, False, False, False, True]
State prediction error at timestep 843 is tensor(7.9231e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 843 of None
Current timestep = 844. State = [[-0.07072003  0.18722238]]. Action = [[ 0.0727281  -0.03921443  0.          0.1903255 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 844 is [True, False, False, False, False, True]
State prediction error at timestep 844 is tensor(7.2959e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 844 of None
Current timestep = 845. State = [[-0.0676621   0.18731199]]. Action = [[-0.00767609  0.00541653  0.         -0.75277686]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 845 is [True, False, False, False, False, True]
State prediction error at timestep 845 is tensor(7.9021e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 845 of None
Current timestep = 846. State = [[-0.06159602  0.18794732]]. Action = [[ 0.09753457 -0.00417178  0.         -0.73881674]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 846 is [True, False, False, False, False, True]
State prediction error at timestep 846 is tensor(6.8350e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 846 of None
Current timestep = 847. State = [[-0.05720254  0.18523808]]. Action = [[-0.00128826 -0.06201497  0.          0.15117264]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 847 is [True, False, False, False, False, True]
State prediction error at timestep 847 is tensor(4.8895e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 847 of None
Current timestep = 848. State = [[-0.05552093  0.17928529]]. Action = [[-0.01206763 -0.09314666  0.         -0.9650483 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 848 is [True, False, False, False, False, True]
State prediction error at timestep 848 is tensor(7.9964e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 848 of None
Current timestep = 849. State = [[-0.05674069  0.1762072 ]]. Action = [[-0.06923755 -0.01380563  0.         -0.45466882]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 849 is [True, False, False, False, False, True]
State prediction error at timestep 849 is tensor(5.3302e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 849 of None
Current timestep = 850. State = [[-0.05369406  0.1712636 ]]. Action = [[ 0.05615849 -0.09306554  0.         -0.9394925 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 850 is [True, False, False, False, False, True]
State prediction error at timestep 850 is tensor(3.2114e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 850 of None
Current timestep = 851. State = [[-0.0489368   0.16936362]]. Action = [[0.02638597 0.02666112 0.         0.03922749]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 851 is [False, True, False, False, False, True]
State prediction error at timestep 851 is tensor(8.0480e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 851 of None
Current timestep = 852. State = [[-0.04816472  0.17113498]]. Action = [[-0.03131233  0.03443254  0.         -0.45389378]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 852 is [False, True, False, False, False, True]
State prediction error at timestep 852 is tensor(4.6035e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 852 of None
Current timestep = 853. State = [[-0.0443237   0.17474337]]. Action = [[ 0.080655    0.06163654  0.         -0.9429861 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 853 is [False, True, False, False, False, True]
State prediction error at timestep 853 is tensor(4.2214e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 853 of None
Current timestep = 854. State = [[-0.04440028  0.17918709]]. Action = [[-0.06034715  0.05686305  0.         -0.9860719 ]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 854 is [False, True, False, False, False, True]
State prediction error at timestep 854 is tensor(3.7286e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 854 of None
Current timestep = 855. State = [[-0.04547225  0.17804046]]. Action = [[ 0.00214121 -0.06443019  0.         -0.98794264]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 855 is [False, True, False, False, False, True]
State prediction error at timestep 855 is tensor(1.7638e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 855 of None
Current timestep = 856. State = [[-0.04527114  0.17348659]]. Action = [[-0.00923176 -0.0615316   0.          0.06395268]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 856 is [False, True, False, False, False, True]
State prediction error at timestep 856 is tensor(1.2072e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 856 of None
Current timestep = 857. State = [[-0.04154351  0.16723348]]. Action = [[ 0.0654515  -0.08580338  0.         -0.7366477 ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 857 is [False, True, False, False, False, True]
State prediction error at timestep 857 is tensor(8.7004e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 857 of None
Current timestep = 858. State = [[-0.04169757  0.16592833]]. Action = [[-0.05692024  0.0384692   0.         -0.5745712 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 858 is [False, True, False, False, False, True]
State prediction error at timestep 858 is tensor(5.1386e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 858 of None
Current timestep = 859. State = [[-0.04682346  0.16331224]]. Action = [[-0.0921061  -0.07225795  0.         -0.14371043]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 859 is [False, True, False, False, False, True]
State prediction error at timestep 859 is tensor(2.4226e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 859 of None
Current timestep = 860. State = [[-0.05298929  0.16166012]]. Action = [[-9.073537e-02  7.869601e-04  0.000000e+00 -9.509663e-01]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 860 is [True, False, False, False, False, True]
State prediction error at timestep 860 is tensor(1.5778e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 860 of None
Current timestep = 861. State = [[-0.05919439  0.16246848]]. Action = [[-0.07854428  0.00769665  0.         -0.31575406]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 861 is [True, False, False, False, False, True]
State prediction error at timestep 861 is tensor(4.5799e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 861 of None
Current timestep = 862. State = [[-0.06246668  0.16404177]]. Action = [[-0.01238865  0.01660591  0.          0.5029113 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 862 is [True, False, False, False, False, True]
State prediction error at timestep 862 is tensor(1.5590e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 862 of None
Current timestep = 863. State = [[-0.06421455  0.16562745]]. Action = [[-0.00688659  0.01587838  0.         -0.5727068 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 863 is [True, False, False, False, False, True]
State prediction error at timestep 863 is tensor(1.8553e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 863 of None
Current timestep = 864. State = [[-0.06977747  0.17062795]]. Action = [[-0.08118087  0.08493636  0.         -0.75866765]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 864 is [True, False, False, False, False, True]
State prediction error at timestep 864 is tensor(1.0283e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 864 of None
Current timestep = 865. State = [[-0.07198377  0.17686872]]. Action = [[ 0.0454281   0.06619022  0.         -0.6172999 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 865 is [True, False, False, False, False, True]
State prediction error at timestep 865 is tensor(1.0670e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 865 of None
Current timestep = 866. State = [[-0.07593336  0.17715935]]. Action = [[-0.06789801 -0.04891085  0.         -0.70940125]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 866 is [True, False, False, False, False, True]
State prediction error at timestep 866 is tensor(1.8486e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 866 of None
Current timestep = 867. State = [[-0.07699606  0.18029834]]. Action = [[0.06982625 0.08606017 0.         0.44941294]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 867 is [True, False, False, False, False, True]
State prediction error at timestep 867 is tensor(2.7226e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 867 of None
Current timestep = 868. State = [[-0.07966451  0.18449065]]. Action = [[-0.0518306   0.0294896   0.         -0.55441666]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 868 is [True, False, False, False, False, True]
State prediction error at timestep 868 is tensor(2.4390e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 868 of None
Current timestep = 869. State = [[-0.08646439  0.18877497]]. Action = [[-0.07073341  0.0530088   0.         -0.7865992 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 869 is [True, False, False, False, False, True]
State prediction error at timestep 869 is tensor(1.6792e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 869 of None
Current timestep = 870. State = [[-0.08620988  0.19235493]]. Action = [[ 0.09657706  0.02842025  0.         -0.3785575 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 870 is [True, False, False, False, False, True]
State prediction error at timestep 870 is tensor(6.7634e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 870 of None
Current timestep = 871. State = [[-0.08323582  0.18976273]]. Action = [[ 0.04121806 -0.07608715  0.         -0.6798469 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 871 is [True, False, False, False, False, True]
State prediction error at timestep 871 is tensor(5.6377e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 871 of None
Current timestep = 872. State = [[-0.07828485  0.18863113]]. Action = [[ 0.09301568  0.02311287  0.         -0.8230147 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 872 is [True, False, False, False, False, True]
State prediction error at timestep 872 is tensor(4.6730e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 872 of None
Current timestep = 873. State = [[-0.07499794  0.18516277]]. Action = [[ 0.01930475 -0.07161572  0.         -0.78354394]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 873 is [True, False, False, False, False, True]
State prediction error at timestep 873 is tensor(6.2158e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 873 of None
Current timestep = 874. State = [[-0.07693833  0.18475679]]. Action = [[-0.05795069  0.04157702  0.         -0.72488606]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 874 is [True, False, False, False, False, True]
State prediction error at timestep 874 is tensor(3.4829e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 874 of None
Current timestep = 875. State = [[-0.07376758  0.18084338]]. Action = [[ 0.0899455  -0.09503918  0.         -0.6805855 ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 875 is [True, False, False, False, False, True]
State prediction error at timestep 875 is tensor(4.3390e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 875 of None
Current timestep = 876. State = [[-0.07398156  0.17333408]]. Action = [[-0.08141106 -0.08817455  0.         -0.5531831 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 876 is [True, False, False, False, False, True]
State prediction error at timestep 876 is tensor(7.6496e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 876 of None
Current timestep = 877. State = [[-0.07857022  0.17423879]]. Action = [[-0.06996746  0.08161021  0.         -0.85892665]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 877 is [True, False, False, False, False, True]
State prediction error at timestep 877 is tensor(2.6687e-07, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 877 of None
Current timestep = 878. State = [[-0.07636388  0.17182982]]. Action = [[ 0.08338077 -0.08491488  0.         -0.5486183 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 878 is [True, False, False, False, False, True]
State prediction error at timestep 878 is tensor(2.7141e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 878 of None
Current timestep = 879. State = [[-0.06963731  0.16667683]]. Action = [[ 0.08689015 -0.03187789  0.         -0.5567169 ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 879 is [True, False, False, False, False, True]
State prediction error at timestep 879 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 879 of None
Current timestep = 880. State = [[-0.0622835   0.16172567]]. Action = [[ 0.09005759 -0.04199257  0.          0.21169436]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 880 is [True, False, False, False, False, True]
State prediction error at timestep 880 is tensor(6.1150e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 880 of None
Current timestep = 881. State = [[-0.05513646  0.16048656]]. Action = [[ 0.08128626  0.04509116  0.         -0.48183787]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 881 is [True, False, False, False, False, True]
State prediction error at timestep 881 is tensor(9.2127e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 881 of None
Current timestep = 882. State = [[-0.05129732  0.15928876]]. Action = [[ 0.01784156 -0.00626888  0.         -0.6464832 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 882 is [True, False, False, False, False, True]
State prediction error at timestep 882 is tensor(7.7135e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 882 of None
Current timestep = 883. State = [[-0.04544553  0.15722056]]. Action = [[ 0.09152757 -0.0014891   0.         -0.1368342 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 883 is [False, True, False, False, False, True]
State prediction error at timestep 883 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 883 of None
Current timestep = 884. State = [[-0.03731196  0.15254843]]. Action = [[ 0.09536042 -0.05225141  0.          0.52518487]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 884 is [False, True, False, False, False, True]
State prediction error at timestep 884 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 884 of None
Current timestep = 885. State = [[-0.03131837  0.14989221]]. Action = [[ 0.04113264  0.01882945  0.         -0.90335095]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 885 is [False, True, False, False, False, True]
State prediction error at timestep 885 is tensor(5.1127e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 885 of None
Current timestep = 886. State = [[-0.03110533  0.1467367 ]]. Action = [[-0.05803394 -0.0425142   0.         -0.84024   ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 886 is [False, True, False, False, False, True]
State prediction error at timestep 886 is tensor(1.8044e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 886 of None
Current timestep = 887. State = [[-0.03420101  0.14342175]]. Action = [[-0.07415251 -0.02701037  0.         -0.6873187 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 887 is [False, True, False, False, False, True]
State prediction error at timestep 887 is tensor(3.1221e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 887 of None
Current timestep = 888. State = [[-0.0325626   0.14389652]]. Action = [[ 0.05142046  0.03761484  0.         -0.9643406 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 888 is [False, True, False, False, False, True]
State prediction error at timestep 888 is tensor(9.4241e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 888 of None
Current timestep = 889. State = [[-0.02988888  0.1427215 ]]. Action = [[ 1.7291307e-04 -3.8742136e-02  0.0000000e+00  3.0637443e-01]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 889 is [False, True, False, False, False, True]
State prediction error at timestep 889 is tensor(5.8241e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 889 of None
Current timestep = 890. State = [[-0.0243742   0.13650207]]. Action = [[ 0.07970456 -0.09739367  0.         -0.6277181 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 890 is [False, True, False, False, False, True]
State prediction error at timestep 890 is tensor(6.6380e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 890 of None
Current timestep = 891. State = [[-0.01798324  0.13223289]]. Action = [[ 0.05343916 -0.00995098  0.         -0.6912418 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 891 is [False, True, False, False, False, True]
State prediction error at timestep 891 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 891 of None
Current timestep = 892. State = [[-0.01021268  0.12732269]]. Action = [[ 0.09637154 -0.06407495  0.         -0.3368503 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 892 is [False, True, False, False, False, True]
State prediction error at timestep 892 is tensor(9.0483e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 892 of None
Current timestep = 893. State = [[-0.35961157  0.12521824]]. Action = [[ 0.08677707 -0.03397493  0.         -0.12736076]]. Reward = [100.]
Curr episode timestep = 333
Scene graph at timestep 893 is [True, False, False, False, False, True]
State prediction error at timestep 893 is tensor(0.0594, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 893 of None
Current timestep = 894. State = [[-0.36283404  0.1256434 ]]. Action = [[-0.04919813 -0.09688291  0.         -0.52008146]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 894 is [True, False, False, False, False, True]
State prediction error at timestep 894 is tensor(6.4863e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 894 of None
Current timestep = 895. State = [[-0.35971552  0.12227283]]. Action = [[ 0.09033526 -0.05415378  0.         -0.7808078 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 896. State = [[-0.35915488  0.11989598]]. Action = [[-0.05932966 -0.04600285  0.         -0.3601765 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 897. State = [[-0.3586689   0.12112746]]. Action = [[ 0.02152586  0.02463274  0.         -0.512477  ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 898. State = [[-0.3560947   0.12398622]]. Action = [[ 0.02585328  0.02614724  0.         -0.7618888 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 899. State = [[-0.35679895  0.12263398]]. Action = [[-0.04826049 -0.06019225  0.         -0.62658507]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 900. State = [[-0.3540604   0.11940464]]. Action = [[ 0.06243967 -0.04277776  0.         -0.74622726]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 901. State = [[-0.35010764  0.11946958]]. Action = [[ 0.02470702  0.02931387  0.         -0.8490958 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 902. State = [[-0.34975073  0.115549  ]]. Action = [[-0.03173082 -0.09302239  0.         -0.8194065 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 903. State = [[-0.35027686  0.11157005]]. Action = [[-0.01794145 -0.0246997   0.         -0.96572214]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 904. State = [[-0.35321367  0.10656044]]. Action = [[-0.07012568 -0.07884294  0.         -0.76188505]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 905. State = [[-0.35551965  0.09958321]]. Action = [[-0.02628541 -0.08701055  0.         -0.7699176 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 906. State = [[-0.35295498  0.09721673]]. Action = [[ 0.06008095  0.02615469  0.         -0.1736061 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 907. State = [[-0.3547671   0.09719221]]. Action = [[-0.07718369  0.01013073  0.         -0.28709215]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 908. State = [[-0.35585356  0.09909347]]. Action = [[ 0.02996496  0.05134962  0.         -0.51156276]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 909. State = [[-0.35868517  0.09564725]]. Action = [[-0.06172827 -0.08391473  0.         -0.26524407]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 910. State = [[-0.36565313  0.0943883 ]]. Action = [[-0.08977943  0.03508741  0.         -0.67101526]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 911. State = [[-0.3682798   0.09332757]]. Action = [[ 0.02852464 -0.02850483  0.         -0.33169246]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 912. State = [[-0.3691172   0.09176164]]. Action = [[ 0.          0.          0.         -0.55806446]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 913. State = [[-0.36684278  0.09374686]]. Action = [[ 0.08088965  0.06032831  0.         -0.5034304 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 914. State = [[-0.36586243  0.09354278]]. Action = [[ 0.0061457  -0.01951186  0.         -0.2474854 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 915. State = [[-0.36803183  0.09445977]]. Action = [[-0.02520682  0.04147715  0.         -0.58140236]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 916. State = [[-0.36987835  0.09527333]]. Action = [[ 0.        0.        0.       -0.810709]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 917. State = [[-0.36807328  0.09073929]]. Action = [[ 0.05280291 -0.0860868   0.         -0.04020178]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 918. State = [[-0.36674803  0.08775492]]. Action = [[ 0.         0.         0.        -0.3131243]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 919. State = [[-0.36675057  0.08293805]]. Action = [[-0.00539885 -0.08641053  0.         -0.19274044]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 920. State = [[-0.3688283   0.08339416]]. Action = [[-0.04635785  0.07061199  0.         -0.084005  ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 921. State = [[-0.3702456   0.08506633]]. Action = [[ 0.          0.          0.         -0.50360954]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 922. State = [[-0.3694211   0.08855956]]. Action = [[0.02573372 0.07320728 0.         0.19213271]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 923. State = [[-0.37007824  0.08899228]]. Action = [[-0.01742995 -0.03159185  0.         -0.7685298 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 924. State = [[-0.3658421   0.08505115]]. Action = [[ 0.09855596 -0.05979334  0.         -0.83836794]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 925. State = [[-0.35887083  0.08607166]]. Action = [[ 0.08489127  0.07069082  0.         -0.8627608 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 926. State = [[-0.3531763   0.08927587]]. Action = [[ 0.06230464  0.03760489  0.         -0.40125072]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 927. State = [[-0.34992182  0.08719316]]. Action = [[ 0.01541126 -0.0598431   0.         -0.30474436]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 928. State = [[-0.3497937   0.08332142]]. Action = [[-0.03415328 -0.04460625  0.         -0.7646643 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 929. State = [[-0.35211194  0.0823027 ]]. Action = [[-0.05831342  0.00335568  0.         -0.73924065]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 930. State = [[-0.34877905  0.07789635]]. Action = [[ 0.07643946 -0.0900886   0.         -0.12447423]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 931. State = [[-0.3425731   0.07105837]]. Action = [[ 0.05321293 -0.07351936  0.         -0.48149657]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 932. State = [[-0.33512372  0.06827069]]. Action = [[ 0.08549155  0.01011561  0.         -0.04074025]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 933. State = [[-0.3283045   0.06924281]]. Action = [[ 0.05888129  0.04208889  0.         -0.62088346]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 934. State = [[-0.32765892  0.06505718]]. Action = [[-0.0599026  -0.0891082   0.         -0.20879304]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 935. State = [[-0.32529888  0.06401501]]. Action = [[ 0.05721845  0.0504986   0.         -0.6464641 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 936. State = [[-0.3265371   0.05985891]]. Action = [[-0.088356   -0.09439993  0.         -0.88038653]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 937. State = [[-0.32780004  0.06074316]]. Action = [[0.0102791  0.08876581 0.         0.14798653]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 938. State = [[-0.3291056   0.06014748]]. Action = [[-0.03553292 -0.05121147  0.         -0.7996005 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 939. State = [[-0.32723138  0.05957368]]. Action = [[ 0.05520485  0.0248213   0.         -0.7346544 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 940. State = [[-0.3295538  0.0582086]]. Action = [[-0.08316762 -0.03405897  0.         -0.9321242 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 941. State = [[-0.32994765  0.06157809]]. Action = [[ 0.05076898  0.09204235  0.         -0.54443043]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 942. State = [[-0.3250235   0.06457149]]. Action = [[0.09057439 0.01138053 0.         0.18964612]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 943. State = [[-0.32335985  0.06079542]]. Action = [[-0.01520038 -0.08262627  0.          0.34866166]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 944. State = [[-0.32632637  0.06173537]]. Action = [[-0.0537979   0.06844508  0.         -0.82016677]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 945. State = [[-0.3271718   0.06316061]]. Action = [[ 0.02040648 -0.01298895  0.         -0.04247928]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 946. State = [[-0.32501537  0.06762671]]. Action = [[0.0437225  0.09253655 0.         0.00258529]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 947. State = [[-0.32672423  0.0695781 ]]. Action = [[-0.04998421 -0.02117235  0.         -0.66969717]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 948. State = [[-0.32547814  0.06548394]]. Action = [[ 0.05804486 -0.0817676   0.         -0.7461456 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 949. State = [[-0.32397947  0.06494946]]. Action = [[-0.00186069  0.03597655  0.         -0.21948433]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 950. State = [[-0.32235533  0.07068628]]. Action = [[ 0.03717411  0.09454099  0.         -0.43322694]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 951. State = [[-0.3262186   0.07758584]]. Action = [[-0.09149549  0.07513096  0.         -0.34380907]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 952. State = [[-0.33323798  0.07919914]]. Action = [[-0.07927436 -0.03286397  0.         -0.46028072]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 953. State = [[-0.33361354  0.07517973]]. Action = [[ 0.05035207 -0.08411525  0.         -0.1695137 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 954. State = [[-0.33089906  0.0732187 ]]. Action = [[ 0.03286447  0.00177801  0.         -0.6541052 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 955. State = [[-0.32697976  0.06969634]]. Action = [[ 0.05900491 -0.0713148   0.         -0.95116913]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 956. State = [[-0.32397005  0.0661999 ]]. Action = [[ 0.01657369 -0.0238563   0.         -0.83490705]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 957. State = [[-0.3180447   0.06495061]]. Action = [[ 0.09702315  0.00242756  0.         -0.8202655 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 958. State = [[-0.3122109   0.06191485]]. Action = [[ 0.04607698 -0.04342197  0.         -0.9248553 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 959. State = [[-0.31351984  0.05579716]]. Action = [[-0.08881769 -0.08275883  0.         -0.6062799 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 960. State = [[-0.31739452  0.04823302]]. Action = [[-0.06114979 -0.08906579  0.         -0.05353242]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 961. State = [[-0.31621057  0.04425659]]. Action = [[ 0.04114852 -0.00314986  0.         -0.12849629]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 962. State = [[-0.31757843  0.04098018]]. Action = [[-0.06831492 -0.03473366  0.         -0.4279927 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 963. State = [[-0.31484786  0.04265631]]. Action = [[ 0.09377553  0.08156719  0.         -0.555861  ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 964. State = [[-0.30817297  0.04378141]]. Action = [[ 0.08317118  0.00150204  0.         -0.6563642 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 965. State = [[-0.3045332  0.0415886]]. Action = [[ 0.01636114 -0.0252152   0.         -0.8434846 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 966. State = [[-0.30469492  0.03964442]]. Action = [[-0.02839112 -0.0072405   0.         -0.7478205 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 967. State = [[-0.30165437  0.03961536]]. Action = [[ 0.06727589  0.01830588  0.         -0.8950813 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 968. State = [[-0.30205393  0.03926252]]. Action = [[-0.05774189 -0.00738622  0.         -0.90749484]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 969. State = [[-0.30555892  0.03888967]]. Action = [[-5.0415695e-02 -1.8729270e-04  0.0000000e+00 -6.7321819e-01]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 970. State = [[-0.3056037  0.0349972]]. Action = [[ 0.02207574 -0.07570542  0.         -0.7181084 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 971. State = [[-0.303438    0.02891743]]. Action = [[ 0.0228977  -0.07002634  0.         -0.5922889 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 972. State = [[-0.29950103  0.0294272 ]]. Action = [[0.05874415 0.0680523  0.         0.53614366]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 973. State = [[-0.2946304   0.03082997]]. Action = [[0.06445774 0.00405017 0.         0.9343672 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 974. State = [[-0.28938538  0.02688988]]. Action = [[ 0.06229403 -0.06990939  0.         -0.4776755 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 975. State = [[-0.2821172   0.02666276]]. Action = [[ 0.09857631  0.05363446  0.         -0.74592084]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 976. State = [[-0.27719298  0.02720919]]. Action = [[ 0.02901449 -0.00128813  0.         -0.12039685]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 977. State = [[-0.27510983  0.02213828]]. Action = [[-1.9272417e-04 -8.9634769e-02  0.0000000e+00 -8.6208951e-01]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 978. State = [[-0.27017114  0.02148797]]. Action = [[ 0.07152278  0.05438051  0.         -0.4287095 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 979. State = [[-0.26218966  0.01826972]]. Action = [[ 0.09464515 -0.07814834  0.          0.28245795]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 980. State = [[-0.25358307  0.01514407]]. Action = [[ 8.4683530e-02  3.9438903e-04  0.0000000e+00 -9.4165111e-01]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 981. State = [[-0.24541317  0.01626487]]. Action = [[0.07972641 0.04546236 0.         0.46372974]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 982. State = [[-0.2426005   0.01324559]]. Action = [[-0.03241742 -0.0701464   0.         -0.8912333 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 983. State = [[-0.23948504  0.01379452]]. Action = [[ 0.03670309  0.06488102  0.         -0.4142176 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 984. State = [[-0.23752786  0.0146127 ]]. Action = [[-0.01605943 -0.01020771  0.         -0.32385367]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 985. State = [[-0.2345022   0.01374673]]. Action = [[ 0.03336138 -0.00868472  0.         -0.5279019 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 986. State = [[-0.22825404  0.01145613]]. Action = [[ 0.07409108 -0.03572518  0.         -0.37352926]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 987. State = [[-0.22335903  0.00682208]]. Action = [[ 0.01949205 -0.06433924  0.         -0.47936094]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 988. State = [[-0.22056866  0.00227248]]. Action = [[ 0.0022532  -0.04203531  0.         -0.6061785 ]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 989. State = [[-0.21404788 -0.00177461]]. Action = [[ 0.09137218 -0.03950295  0.         -0.09960759]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 990. State = [[-0.21341592 -0.00826017]]. Action = [[-0.08945784 -0.08642855  0.         -0.69208765]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 991. State = [[-0.21767269 -0.01624794]]. Action = [[-0.08112399 -0.08855259  0.         -0.5603301 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 992. State = [[-0.21580692 -0.01951694]]. Action = [[ 0.06058339  0.01483072  0.         -0.9762714 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 993. State = [[-0.21368013 -0.02077981]]. Action = [[-0.01145698 -0.00284038  0.         -0.52238595]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 994. State = [[-0.20869915 -0.01831032]]. Action = [[ 0.0938823   0.07590888  0.         -0.563863  ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 995. State = [[-0.2092375  -0.01758034]]. Action = [[-0.07541278 -0.01085065  0.         -0.8190508 ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 996. State = [[-0.20687966 -0.01474577]]. Action = [[ 0.0968607   0.07198738  0.         -0.67657614]]. Reward = [0.]
Curr episode timestep = 102
Current timestep = 997. State = [[-0.20007157 -0.00996082]]. Action = [[ 0.0971758   0.05969024  0.         -0.6084987 ]]. Reward = [0.]
Curr episode timestep = 103
Current timestep = 998. State = [[-0.19761938 -0.00613997]]. Action = [[-1.0124594e-04  3.9330699e-02  0.0000000e+00 -4.8499477e-01]]. Reward = [0.]
Curr episode timestep = 104
Current timestep = 999. State = [[-0.20166345 -0.00151428]]. Action = [[-0.08370622  0.05986673  0.          0.03087354]]. Reward = [0.]
Curr episode timestep = 105
Current timestep = 1000. State = [[-0.2040718   0.00402931]]. Action = [[ 0.00311176  0.05652881  0.         -0.8123309 ]]. Reward = [0.]
Curr episode timestep = 106
Current timestep = 1001. State = [[-0.20066218  0.00452095]]. Action = [[ 0.0797668  -0.04962879  0.         -0.5425595 ]]. Reward = [0.]
Curr episode timestep = 107
Current timestep = 1002. State = [[-0.2018024  -0.00035549]]. Action = [[-0.07591711 -0.09251747  0.         -0.25656635]]. Reward = [0.]
Curr episode timestep = 108
Current timestep = 1003. State = [[-1.9916795e-01  4.0886280e-05]]. Action = [[ 0.09846798  0.04859959  0.         -0.5865755 ]]. Reward = [0.]
Curr episode timestep = 109
Current timestep = 1004. State = [[-0.19965029  0.00120361]]. Action = [[-0.0774143  -0.01484503  0.         -0.55252564]]. Reward = [0.]
Curr episode timestep = 110
Current timestep = 1005. State = [[-0.20464772  0.00138304]]. Action = [[-0.0696655  -0.00266792  0.         -0.6826261 ]]. Reward = [0.]
Curr episode timestep = 111
Current timestep = 1006. State = [[-0.20255119 -0.00227361]]. Action = [[ 0.08491736 -0.08065341  0.         -0.80287415]]. Reward = [0.]
Curr episode timestep = 112
Current timestep = 1007. State = [[-0.20316951 -0.00570148]]. Action = [[-0.0771417  -0.02106602  0.         -0.82951915]]. Reward = [0.]
Curr episode timestep = 113
Current timestep = 1008. State = [[-0.20031644 -0.00755099]]. Action = [[ 0.096844   -0.01779308  0.         -0.35527253]]. Reward = [0.]
Curr episode timestep = 114
Current timestep = 1009. State = [[-0.20003903 -0.00373017]]. Action = [[-0.06146539  0.09733579  0.         -0.22035867]]. Reward = [0.]
Curr episode timestep = 115
Current timestep = 1010. State = [[-0.20171356  0.00275537]]. Action = [[-3.2232702e-04  7.6055415e-02  0.0000000e+00  7.7161586e-01]]. Reward = [0.]
Curr episode timestep = 116
Current timestep = 1011. State = [[-0.20176587  0.00927409]]. Action = [[ 0.012527    0.07714001  0.         -0.4980613 ]]. Reward = [0.]
Curr episode timestep = 117
Current timestep = 1012. State = [[-0.20655967  0.01378662]]. Action = [[-0.09180093  0.02875347  0.         -0.3568691 ]]. Reward = [0.]
Curr episode timestep = 118
Current timestep = 1013. State = [[-0.20847797  0.01440228]]. Action = [[ 0.02790499 -0.02741133  0.          0.10581577]]. Reward = [0.]
Curr episode timestep = 119
Current timestep = 1014. State = [[-0.20663147  0.0130737 ]]. Action = [[ 0.03778303 -0.02951268  0.         -0.79801005]]. Reward = [0.]
Curr episode timestep = 120
Current timestep = 1015. State = [[-0.20959617  0.01491693]]. Action = [[-0.07556798  0.04043985  0.         -0.88781434]]. Reward = [0.]
Curr episode timestep = 121
Current timestep = 1016. State = [[-0.21139583  0.01960458]]. Action = [[ 0.01840051  0.0546638   0.         -0.88968146]]. Reward = [0.]
Curr episode timestep = 122
Current timestep = 1017. State = [[-0.21133609  0.02211538]]. Action = [[ 1.0093428e-02  1.4446676e-05  0.0000000e+00 -9.4446915e-01]]. Reward = [0.]
Curr episode timestep = 123
Current timestep = 1018. State = [[-0.20932128  0.02242159]]. Action = [[ 0.04918528 -0.01060353  0.         -0.14672858]]. Reward = [0.]
Curr episode timestep = 124
Current timestep = 1019. State = [[-0.20745352  0.0188727 ]]. Action = [[ 0.01688757 -0.07501589  0.         -0.96562403]]. Reward = [0.]
Curr episode timestep = 125
Current timestep = 1020. State = [[-0.20770952  0.01885619]]. Action = [[-0.01739235  0.03956144  0.         -0.993633  ]]. Reward = [0.]
Curr episode timestep = 126
Current timestep = 1021. State = [[-0.21180709  0.02277834]]. Action = [[-0.07514013  0.05263465  0.         -0.42405164]]. Reward = [0.]
Curr episode timestep = 127
Current timestep = 1022. State = [[-0.21480776  0.02937668]]. Action = [[-0.01205093  0.09008823  0.         -0.6928792 ]]. Reward = [0.]
Curr episode timestep = 128
Current timestep = 1023. State = [[-0.21459414  0.02862058]]. Action = [[ 0.02486894 -0.084679    0.         -0.9648218 ]]. Reward = [0.]
Curr episode timestep = 129
Current timestep = 1024. State = [[-0.21247928  0.02242729]]. Action = [[ 0.03139157 -0.08610029  0.          0.0367378 ]]. Reward = [0.]
Curr episode timestep = 130
Current timestep = 1025. State = [[-0.2100876   0.02168987]]. Action = [[ 0.02760435  0.03886063  0.         -0.8458462 ]]. Reward = [0.]
Curr episode timestep = 131
Current timestep = 1026. State = [[-0.20971395  0.0225429 ]]. Action = [[-0.01029492 -0.00046685  0.          0.22646034]]. Reward = [0.]
Curr episode timestep = 132
Current timestep = 1027. State = [[-0.20786321  0.02256175]]. Action = [[ 0.04074716  0.00255428  0.         -0.81449544]]. Reward = [0.]
Curr episode timestep = 133
Current timestep = 1028. State = [[-0.20422967  0.02401559]]. Action = [[ 0.05008497  0.03272452  0.         -0.66640675]]. Reward = [0.]
Curr episode timestep = 134
Current timestep = 1029. State = [[-0.20327435  0.02918484]]. Action = [[-0.00929143  0.08791258  0.         -0.92868966]]. Reward = [0.]
Curr episode timestep = 135
Current timestep = 1030. State = [[-0.19988236  0.02898883]]. Action = [[ 0.07460276 -0.05798176  0.          0.2374897 ]]. Reward = [0.]
Curr episode timestep = 136
Current timestep = 1031. State = [[-0.20132568  0.02631553]]. Action = [[-0.08641271 -0.02449789  0.         -0.20948422]]. Reward = [0.]
Curr episode timestep = 137
Current timestep = 1032. State = [[-0.20166984  0.02750968]]. Action = [[ 0.03565184  0.03718138  0.         -0.32992357]]. Reward = [0.]
Curr episode timestep = 138
Current timestep = 1033. State = [[-0.20328887  0.02527237]]. Action = [[-0.05625236 -0.06920069  0.         -0.951028  ]]. Reward = [0.]
Curr episode timestep = 139
Current timestep = 1034. State = [[-0.20282605  0.02608587]]. Action = [[ 0.03390288  0.05360038  0.         -0.20109427]]. Reward = [0.]
Curr episode timestep = 140
Current timestep = 1035. State = [[-0.19790074  0.02541247]]. Action = [[ 0.08171351 -0.04262133  0.         -0.9346143 ]]. Reward = [0.]
Curr episode timestep = 141
Current timestep = 1036. State = [[-0.19847369  0.02542627]]. Action = [[-0.0694674   0.02638302  0.         -0.88114494]]. Reward = [0.]
Curr episode timestep = 142
Current timestep = 1037. State = [[-0.19697115  0.02125704]]. Action = [[ 0.06484292 -0.09639752  0.         -0.8775563 ]]. Reward = [0.]
Curr episode timestep = 143
Current timestep = 1038. State = [[-0.19381316  0.01781166]]. Action = [[ 0.02020843 -0.00468725  0.         -0.5230809 ]]. Reward = [0.]
Curr episode timestep = 144
Current timestep = 1039. State = [[-0.18779081  0.01394468]]. Action = [[ 0.09901661 -0.05739978  0.          0.746495  ]]. Reward = [0.]
Curr episode timestep = 145
Current timestep = 1040. State = [[-0.18086082  0.0101819 ]]. Action = [[ 0.07001222 -0.01961595  0.         -0.12272882]]. Reward = [0.]
Curr episode timestep = 146
Current timestep = 1041. State = [[-0.17541495  0.00619174]]. Action = [[ 0.04765309 -0.04061538  0.         -0.4140749 ]]. Reward = [0.]
Curr episode timestep = 147
Current timestep = 1042. State = [[-0.17527297  0.00792271]]. Action = [[-0.05195519  0.08584433  0.         -0.19321483]]. Reward = [0.]
Curr episode timestep = 148
Current timestep = 1043. State = [[-0.17160621  0.00577822]]. Action = [[ 0.08441661 -0.07133325  0.         -0.6238518 ]]. Reward = [0.]
Curr episode timestep = 149
Current timestep = 1044. State = [[-0.16898897  0.00082235]]. Action = [[-0.02080731 -0.03867009  0.         -0.6749424 ]]. Reward = [0.]
Curr episode timestep = 150
Current timestep = 1045. State = [[-0.16671129 -0.00057094]]. Action = [[ 0.02701978  0.01741117  0.         -0.8221398 ]]. Reward = [0.]
Curr episode timestep = 151
Current timestep = 1046. State = [[-0.16509135  0.00276451]]. Action = [[-0.00401913  0.07449039  0.         -0.65999866]]. Reward = [0.]
Curr episode timestep = 152
Current timestep = 1047. State = [[-0.1685535   0.00300159]]. Action = [[-0.09018426 -0.03285874  0.         -0.8346224 ]]. Reward = [0.]
Curr episode timestep = 153
Current timestep = 1048. State = [[-0.16977409  0.00432037]]. Action = [[ 0.00756557  0.04343925  0.         -0.5293486 ]]. Reward = [0.]
Curr episode timestep = 154
Current timestep = 1049. State = [[-0.17389312  0.01033581]]. Action = [[-0.094379    0.08961067  0.         -0.27603066]]. Reward = [0.]
Curr episode timestep = 155
Current timestep = 1050. State = [[-0.17391834  0.00927745]]. Action = [[ 0.05607773 -0.09040932  0.         -0.52466357]]. Reward = [0.]
Curr episode timestep = 156
Current timestep = 1051. State = [[-0.16804361  0.00235819]]. Action = [[ 0.08743662 -0.09595138  0.         -0.3389864 ]]. Reward = [0.]
Curr episode timestep = 157
Current timestep = 1052. State = [[-0.16210376 -0.00208529]]. Action = [[ 0.06084826 -0.02511448  0.         -0.9062456 ]]. Reward = [0.]
Curr episode timestep = 158
Current timestep = 1053. State = [[-0.15613605 -0.00597957]]. Action = [[ 0.07035432 -0.04753481  0.         -0.9562265 ]]. Reward = [0.]
Curr episode timestep = 159
Current timestep = 1054. State = [[-0.15105408 -0.00437387]]. Action = [[ 0.0459343   0.07942397  0.         -0.80884016]]. Reward = [0.]
Curr episode timestep = 160
Current timestep = 1055. State = [[-0.14801475 -0.0014645 ]]. Action = [[ 0.02135648  0.0291164   0.         -0.6586402 ]]. Reward = [0.]
Curr episode timestep = 161
Current timestep = 1056. State = [[-0.15078431  0.00058345]]. Action = [[-0.08828454  0.03034673  0.         -0.7190448 ]]. Reward = [0.]
Curr episode timestep = 162
Current timestep = 1057. State = [[-0.14911625 -0.00091674]]. Action = [[ 0.07286107 -0.04716719  0.         -0.22618353]]. Reward = [0.]
Curr episode timestep = 163
Current timestep = 1058. State = [[-0.14768898 -0.00295768]]. Action = [[-0.03140769 -0.01087823  0.         -0.6827358 ]]. Reward = [0.]
Curr episode timestep = 164
Current timestep = 1059. State = [[-0.14445795 -0.00254981]]. Action = [[ 0.06128957  0.01846888  0.         -0.7863393 ]]. Reward = [0.]
Curr episode timestep = 165
Current timestep = 1060. State = [[-0.13761027 -0.00212922]]. Action = [[ 0.09067164  0.00150349  0.         -0.8332004 ]]. Reward = [0.]
Curr episode timestep = 166
Current timestep = 1061. State = [[-0.13624448 -0.00607687]]. Action = [[-0.04981264 -0.07607001  0.         -0.7010664 ]]. Reward = [0.]
Curr episode timestep = 167
Current timestep = 1062. State = [[-0.13247608 -0.00923   ]]. Action = [[ 0.08289266 -0.01185222  0.         -0.96247673]]. Reward = [0.]
Curr episode timestep = 168
Current timestep = 1063. State = [[-0.13022512 -0.00583438]]. Action = [[-0.02002226  0.08714259  0.         -0.15855765]]. Reward = [0.]
Curr episode timestep = 169
Current timestep = 1064. State = [[-0.12630302 -0.00087314]]. Action = [[ 0.07644362  0.05189274  0.         -0.46465337]]. Reward = [0.]
Curr episode timestep = 170
Current timestep = 1065. State = [[-0.12259898  0.00514293]]. Action = [[ 0.02398541  0.08510242  0.         -0.32022572]]. Reward = [0.]
Curr episode timestep = 171
Current timestep = 1066. State = [[-0.11960986  0.01187802]]. Action = [[ 0.03987142  0.07288768  0.         -0.9599286 ]]. Reward = [0.]
Curr episode timestep = 172
Current timestep = 1067. State = [[-0.11434235  0.01346169]]. Action = [[ 0.07879054 -0.03004064  0.          0.24348283]]. Reward = [0.]
Curr episode timestep = 173
Current timestep = 1068. State = [[-0.10953374  0.01774375]]. Action = [[ 0.04277781  0.08463905  0.         -0.5812739 ]]. Reward = [0.]
Curr episode timestep = 174
Current timestep = 1069. State = [[-0.10504428  0.01872171]]. Action = [[ 0.05283756 -0.04867608  0.         -0.8626641 ]]. Reward = [0.]
Curr episode timestep = 175
Current timestep = 1070. State = [[-0.09836608  0.023141  ]]. Action = [[ 0.08740757  0.0970925   0.         -0.56880903]]. Reward = [0.]
Curr episode timestep = 176
Current timestep = 1071. State = [[-0.09089419  0.02738385]]. Action = [[ 0.08547733  0.01236516  0.         -0.51627517]]. Reward = [0.]
Curr episode timestep = 177
Current timestep = 1072. State = [[-0.08577777  0.03250585]]. Action = [[ 0.03160063  0.07442959  0.         -0.84847176]]. Reward = [0.]
Curr episode timestep = 178
Current timestep = 1073. State = [[-0.08641022  0.04025556]]. Action = [[-0.06166061  0.08559192  0.          0.50440514]]. Reward = [0.]
Curr episode timestep = 179
Current timestep = 1074. State = [[-0.08521073  0.04574927]]. Action = [[0.03067297 0.02125577 0.         0.84602714]]. Reward = [0.]
Curr episode timestep = 180
Current timestep = 1075. State = [[-0.07935955  0.04853155]]. Action = [[0.0784168  0.00386988 0.         0.28970933]]. Reward = [0.]
Curr episode timestep = 181
Current timestep = 1076. State = [[-0.07124932  0.05380926]]. Action = [[ 0.09707344  0.07161606  0.         -0.32552528]]. Reward = [0.]
Curr episode timestep = 182
Current timestep = 1077. State = [[-0.07069255  0.0591893 ]]. Action = [[-0.08261661  0.03197617  0.         -0.7667238 ]]. Reward = [0.]
Curr episode timestep = 183
Current timestep = 1078. State = [[-0.07109144  0.0586138 ]]. Action = [[ 0.01072937 -0.07146005  0.         -0.7494626 ]]. Reward = [0.]
Curr episode timestep = 184
Current timestep = 1079. State = [[-0.06820111  0.05905127]]. Action = [[ 0.02997143  0.01843339  0.         -0.8852254 ]]. Reward = [0.]
Curr episode timestep = 185
Current timestep = 1080. State = [[-0.0618531  0.0579886]]. Action = [[ 0.08925193 -0.05101476  0.         -0.7050847 ]]. Reward = [0.]
Curr episode timestep = 186
Current timestep = 1081. State = [[-0.05709478  0.05962659]]. Action = [[ 0.02058289  0.05214689  0.         -0.9467949 ]]. Reward = [0.]
Curr episode timestep = 187
Current timestep = 1082. State = [[-0.05085239  0.05687683]]. Action = [[ 0.08623181 -0.09053874  0.         -0.94552726]]. Reward = [0.]
Curr episode timestep = 188
Current timestep = 1083. State = [[-0.05021794  0.04980016]]. Action = [[-0.08556663 -0.09085734  0.         -0.959098  ]]. Reward = [0.]
Curr episode timestep = 189
Current timestep = 1084. State = [[-0.0529502  0.0445649]]. Action = [[-0.05459964 -0.04558533  0.         -0.9268323 ]]. Reward = [0.]
Curr episode timestep = 190
Current timestep = 1085. State = [[-0.05383711  0.04471333]]. Action = [[-0.01935808  0.04078617  0.         -0.9017685 ]]. Reward = [0.]
Curr episode timestep = 191
Current timestep = 1086. State = [[-0.3747022   0.03554082]]. Action = [[ 0.07563784 -0.0730833   0.         -0.61109304]]. Reward = [100.]
Curr episode timestep = 192
Current timestep = 1087. State = [[-0.37087902  0.03402135]]. Action = [[ 0.08910961  0.02881873  0.         -0.68344516]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1088. State = [[-0.3684854   0.03467282]]. Action = [[0.         0.         0.         0.04794145]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1089. State = [[-0.3697357   0.03619526]]. Action = [[-0.03220014  0.02943834  0.         -0.49437344]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1090. State = [[-0.36617276  0.03462283]]. Action = [[ 0.08691979 -0.0487153   0.         -0.04458427]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1091. State = [[-0.3591909   0.02922031]]. Action = [[ 0.07883718 -0.07230935  0.         -0.8888983 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1092. State = [[-0.35119832  0.0296575 ]]. Action = [[0.09556405 0.06934319 0.         0.8869097 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1093. State = [[-0.34325016  0.03569577]]. Action = [[ 0.0902512   0.09471752  0.         -0.811815  ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1094. State = [[-0.33681437  0.03644015]]. Action = [[ 0.05948975 -0.03592917  0.         -0.25400484]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1095. State = [[-0.33764127  0.03809233]]. Action = [[-0.08839098  0.0541159   0.          0.10302997]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1096. State = [[-0.34244767  0.03595546]]. Action = [[-0.08081408 -0.07956354  0.         -0.50393885]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1097. State = [[-0.34559274  0.03261302]]. Action = [[-0.04656666 -0.02841239  0.         -0.22879553]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1098. State = [[-0.3451111   0.03018549]]. Action = [[ 0.01621599 -0.03203875  0.         -0.7060454 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1099. State = [[-0.33936596  0.03134026]]. Action = [[0.09872717 0.0459987  0.         0.3318318 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1100. State = [[-0.3391933   0.03108365]]. Action = [[-0.07130437 -0.02968892  0.         -0.9604736 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1101. State = [[-0.33684313  0.02560578]]. Action = [[ 0.08395705 -0.08875024  0.          0.11105192]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1102. State = [[-0.33293617  0.02028099]]. Action = [[ 0.02285844 -0.03938498  0.         -0.7051062 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1103. State = [[-0.33007053  0.02156916]]. Action = [[ 0.03178889  0.069754    0.         -0.8932403 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1104. State = [[-0.33239168  0.02260711]]. Action = [[-0.07554869 -0.00598414  0.         -0.25971836]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1105. State = [[-0.33065808  0.02618777]]. Action = [[0.08435892 0.08133467 0.         0.4032588 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1106. State = [[-0.32433942  0.02506302]]. Action = [[ 0.08132977 -0.06640139  0.         -0.56001556]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1107. State = [[-0.3166447   0.02027558]]. Action = [[ 0.09387054 -0.04987401  0.          0.2755742 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1108. State = [[-0.30996403  0.02070199]]. Action = [[ 0.05888388  0.05297904  0.         -0.7828734 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1109. State = [[-0.30609626  0.01760798]]. Action = [[ 0.01659028 -0.08085319  0.         -0.5047035 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1110. State = [[-0.30728754  0.01790421]]. Action = [[-0.07118613  0.06270465  0.         -0.9589215 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1111. State = [[-0.30648386  0.01559874]]. Action = [[ 0.02522068 -0.07492156  0.         -0.5496197 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1112. State = [[-0.30665     0.01400463]]. Action = [[-0.04586598  0.01753022  0.         -0.78670305]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1113. State = [[-0.30595183  0.01438038]]. Action = [[0.01562619 0.00448392 0.         0.76071346]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1114. State = [[-0.30110338  0.0126952 ]]. Action = [[ 0.07391647 -0.03241181  0.         -0.7879227 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1115. State = [[-0.2945057   0.01053746]]. Action = [[ 0.07583993 -0.01611968  0.         -0.36858445]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1116. State = [[-0.29193923  0.0113668 ]]. Action = [[-0.01158077  0.03640223  0.         -0.32789356]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1117. State = [[-0.28717044  0.01393685]]. Action = [[ 0.08782605  0.03654882  0.         -0.00750595]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1118. State = [[-0.28651044  0.01209773]]. Action = [[-0.05765907 -0.05607728  0.         -0.5744134 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1119. State = [[-0.2881706   0.01410687]]. Action = [[-0.01774071  0.07434813  0.         -0.80766886]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1120. State = [[-0.2886139  0.0196302]]. Action = [[ 0.00086989  0.06414334  0.         -0.7662682 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1121. State = [[-0.28475598  0.02529316]]. Action = [[ 0.08691002  0.06208291  0.         -0.9037698 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1122. State = [[-0.28164372  0.02507201]]. Action = [[ 0.01986752 -0.05667514  0.         -0.58594334]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1123. State = [[-0.2815876   0.02794269]]. Action = [[-0.01608646  0.07576939  0.         -0.5006377 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1124. State = [[-0.27903008  0.0267092 ]]. Action = [[ 0.05644324 -0.08367697  0.         -0.83658326]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1125. State = [[-0.27411962  0.02312866]]. Action = [[ 0.05366426 -0.0344996   0.          0.3115319 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1126. State = [[-0.26769423  0.02092724]]. Action = [[ 0.07863016 -0.02348253  0.          0.14831376]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1127. State = [[-0.26414508  0.01988674]]. Action = [[ 1.7317384e-04 -2.9581040e-03  0.0000000e+00 -2.6046658e-01]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1128. State = [[-0.25941795  0.01676714]]. Action = [[ 0.06356425 -0.05408769  0.         -0.58902663]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1129. State = [[-0.25719583  0.01341482]]. Action = [[-0.02688074 -0.02457161  0.         -0.39777607]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1130. State = [[-0.25821123  0.01102542]]. Action = [[-0.04593391 -0.02000119  0.          0.04995561]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1131. State = [[-0.25802127  0.00983454]]. Action = [[-0.00261077  0.00089376  0.         -0.8570648 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1132. State = [[-0.25981617  0.01193373]]. Action = [[-0.05867123  0.05259193  0.          0.7396798 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1133. State = [[-0.26391003  0.01412975]]. Action = [[-0.06581208  0.01616333  0.          0.60150635]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1134. State = [[-0.26723593  0.01154056]]. Action = [[-0.03838699 -0.06453848  0.         -0.67595947]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1135. State = [[-0.27208194  0.01073165]]. Action = [[-0.0805193   0.02083616  0.          0.09957933]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1136. State = [[-0.27667445  0.01038487]]. Action = [[-0.04222016 -0.01979031  0.          0.03145373]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1137. State = [[-0.2775448   0.00630024]]. Action = [[ 0.01996504 -0.0711813   0.         -0.7554277 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1138. State = [[-0.27651972  0.00085062]]. Action = [[ 0.02414536 -0.0589155   0.         -0.50254405]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1139. State = [[-0.27421483 -0.0055262 ]]. Action = [[ 0.0435122  -0.07523195  0.          0.24283957]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1140. State = [[-0.2773341  -0.01141834]]. Action = [[-0.08805363 -0.05056372  0.         -0.4246825 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1141. State = [[-0.28235713 -0.01212478]]. Action = [[-0.04277052  0.04136138  0.         -0.27071798]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1142. State = [[-0.28168175 -0.01508594]]. Action = [[ 0.06370931 -0.05910523  0.         -0.95407534]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1143. State = [[-0.2776003  -0.01699718]]. Action = [[ 0.06930771  0.01999028  0.         -0.42230105]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1144. State = [[-0.27478206 -0.01613252]]. Action = [[ 0.0364948   0.03052584  0.         -0.31995046]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1145. State = [[-0.26982632 -0.01795522]]. Action = [[ 0.09689366 -0.03510553  0.         -0.14934069]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1146. State = [[-0.26333985 -0.01689441]]. Action = [[ 0.09018355  0.06008706  0.         -0.32156652]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1147. State = [[-0.25896037 -0.01626884]]. Action = [[ 0.04438379 -0.0058025   0.         -0.5418668 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1148. State = [[-0.25475067 -0.01557143]]. Action = [[ 0.05843166  0.02881039  0.         -0.77147776]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1149. State = [[-0.25601026 -0.01039044]]. Action = [[-0.06719856  0.09600688  0.         -0.8402411 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1150. State = [[-0.25686297 -0.00756658]]. Action = [[ 0.01894311 -0.0025795   0.         -0.6559367 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1151. State = [[-0.25607833 -0.0073066 ]]. Action = [[ 0.00442694 -0.00558531  0.         -0.84849393]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1152. State = [[-0.25753015 -0.00409545]]. Action = [[-0.03883766  0.05492706  0.         -0.54814094]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1153. State = [[-0.25392768 -0.00481705]]. Action = [[ 0.09196497 -0.06313781  0.         -0.34510136]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1154. State = [[-0.25404173 -0.00429477]]. Action = [[-0.0769055   0.03255453  0.          0.13536763]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1155. State = [[-0.25895724 -0.00551584]]. Action = [[-0.07474687 -0.05625856  0.         -0.77141494]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1156. State = [[-0.261224   -0.00357472]]. Action = [[-0.01308867  0.05882419  0.         -0.51677096]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1157. State = [[-0.26030037  0.00184001]]. Action = [[ 0.02998627  0.06139753  0.         -0.34099978]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1158. State = [[-0.25625223  0.00521834]]. Action = [[ 0.07586005  0.01344679  0.         -0.8726202 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1159. State = [[-0.25149012  0.00524448]]. Action = [[ 0.06062063 -0.02290324  0.         -0.82657456]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1160. State = [[-0.25318903  0.00673225]]. Action = [[-0.07747623  0.03238245  0.         -0.7747897 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1161. State = [[-0.2529031   0.01147497]]. Action = [[ 0.05542005  0.06455535  0.         -0.36064774]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1162. State = [[-0.25051403  0.0185608 ]]. Action = [[ 0.03047638  0.08843292  0.         -0.9360397 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1163. State = [[-0.25090098  0.02528747]]. Action = [[-0.01389696  0.06156627  0.          0.11748302]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1164. State = [[-0.2482098   0.02516926]]. Action = [[ 0.07040086 -0.06464605  0.         -0.63373053]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1165. State = [[-0.24151897  0.02110878]]. Action = [[ 0.09343529 -0.06346319  0.          0.02222621]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1166. State = [[-0.2339849  0.0166514]]. Action = [[ 0.0811064  -0.05870501  0.          0.120368  ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1167. State = [[-0.2322506   0.01366238]]. Action = [[-0.0467254  -0.02507788  0.         -0.5717343 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1168. State = [[-0.23197158  0.01529019]]. Action = [[-0.00111895  0.05059781  0.          0.26354015]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1169. State = [[-0.22905786  0.01670397]]. Action = [[ 0.03819861  0.00168631  0.         -0.29717398]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1170. State = [[-0.22775538  0.01456843]]. Action = [[-0.01971319 -0.0428785   0.         -0.05045724]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1171. State = [[-0.22778808  0.00909262]]. Action = [[-0.02029649 -0.08040389  0.         -0.16595447]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1172. State = [[-0.22403572  0.00630597]]. Action = [[ 0.0597486   0.00330622  0.         -0.20601773]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1173. State = [[-0.22163497  0.00351958]]. Action = [[-0.01020242 -0.04040185  0.         -0.5525931 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1174. State = [[-0.21758181  0.00630959]]. Action = [[ 0.06607435  0.09567698  0.         -0.32397997]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1175. State = [[-0.21295938  0.00572762]]. Action = [[ 0.04382259 -0.05433892  0.         -0.73884434]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1176. State = [[-0.21492594  0.00100873]]. Action = [[-0.0927959  -0.05418462  0.         -0.8640511 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1177. State = [[-0.2171305  -0.00332029]]. Action = [[-0.01456203 -0.04209083  0.         -0.68212354]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1178. State = [[-0.2215853 -0.0043048]]. Action = [[-0.09557274  0.0187846   0.         -0.72022516]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 1179. State = [[-0.2226695  -0.00361072]]. Action = [[ 0.02875318  0.01289006  0.         -0.8118487 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1180. State = [[-0.22522393 -0.00202821]]. Action = [[-0.06584819  0.02943713  0.         -0.92067325]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1181. State = [[-0.2241608  -0.00450765]]. Action = [[ 0.06630649 -0.06481365  0.         -0.93636984]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1182. State = [[-0.22660932 -0.00242712]]. Action = [[-0.08503487  0.08414526  0.         -0.94453776]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1183. State = [[-0.23091716 -0.00460481]]. Action = [[-0.02918439 -0.0925874   0.         -0.45331103]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 1184. State = [[-0.23436426 -0.00488896]]. Action = [[-0.03814995  0.04780359  0.          0.3639393 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1185. State = [[-0.23492731  0.0004795 ]]. Action = [[ 0.03463357  0.08099323  0.         -0.9718365 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1186. State = [[-0.23300737  0.00379204]]. Action = [[ 0.05291135  0.0113956   0.         -0.85792106]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 1187. State = [[-0.23464029  0.00530633]]. Action = [[-0.03681906  0.01418366  0.         -0.16761261]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 1188. State = [[-0.23284985  0.01061147]]. Action = [[ 0.08614359  0.08887989  0.         -0.52068114]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 1189. State = [[-0.23615624  0.01595402]]. Action = [[-0.09578051  0.04182693  0.         -0.85026926]]. Reward = [0.]
Curr episode timestep = 102
Current timestep = 1190. State = [[-0.24146302  0.0184246 ]]. Action = [[-0.02393023  0.00332552  0.         -0.8950243 ]]. Reward = [0.]
Curr episode timestep = 103
Current timestep = 1191. State = [[-0.24519213  0.02324653]]. Action = [[-0.0255459   0.07175326  0.         -0.75699145]]. Reward = [0.]
Curr episode timestep = 104
Current timestep = 1192. State = [[-0.25167033  0.0239497 ]]. Action = [[-0.0861593 -0.0532986  0.        -0.6831808]]. Reward = [0.]
Curr episode timestep = 105
Current timestep = 1193. State = [[-0.25354305  0.02726617]]. Action = [[0.040888   0.07148375 0.         0.40601456]]. Reward = [0.]
Curr episode timestep = 106
Current timestep = 1194. State = [[-0.25486222  0.03337839]]. Action = [[-0.01865431  0.05871978  0.          0.17459977]]. Reward = [0.]
Curr episode timestep = 107
Current timestep = 1195. State = [[-0.25201005  0.03532643]]. Action = [[ 0.09585542 -0.02074699  0.          0.4108039 ]]. Reward = [0.]
Curr episode timestep = 108
Current timestep = 1196. State = [[-0.25239584  0.03152841]]. Action = [[-0.05316141 -0.08517747  0.          0.7492341 ]]. Reward = [0.]
Curr episode timestep = 109
Current timestep = 1197. State = [[-0.25528157  0.02897001]]. Action = [[-0.02500962 -0.01528676  0.         -0.7717673 ]]. Reward = [0.]
Curr episode timestep = 110
Current timestep = 1198. State = [[-0.25205252  0.0263547 ]]. Action = [[ 0.08843801 -0.04655284  0.          0.14331818]]. Reward = [0.]
Curr episode timestep = 111
Current timestep = 1199. State = [[-0.24992476  0.0211336 ]]. Action = [[-0.01048853 -0.07308249  0.         -0.28181696]]. Reward = [0.]
Curr episode timestep = 112
Current timestep = 1200. State = [[-0.24536839  0.02109133]]. Action = [[ 0.08892832  0.0553736   0.         -0.53466123]]. Reward = [0.]
Curr episode timestep = 113
Current timestep = 1201. State = [[-0.24256153  0.02652197]]. Action = [[ 0.00286286  0.0937063   0.         -0.73882496]]. Reward = [0.]
Curr episode timestep = 114
Current timestep = 1202. State = [[-0.24483885  0.02971417]]. Action = [[-0.05387378  0.01517753  0.         -0.9205297 ]]. Reward = [0.]
Curr episode timestep = 115
Current timestep = 1203. State = [[-0.24494913  0.02804341]]. Action = [[ 0.02175643 -0.04416047  0.         -0.46015733]]. Reward = [0.]
Curr episode timestep = 116
Current timestep = 1204. State = [[-0.2471415   0.02322462]]. Action = [[-0.06789023 -0.07001761  0.         -0.98214906]]. Reward = [0.]
Curr episode timestep = 117
Current timestep = 1205. State = [[-0.24779418  0.02566164]]. Action = [[ 0.01516593  0.09421716  0.         -0.5068779 ]]. Reward = [0.]
Curr episode timestep = 118
Current timestep = 1206. State = [[-0.24715897  0.02987886]]. Action = [[ 0.00866552  0.02940632  0.         -0.81114304]]. Reward = [0.]
Curr episode timestep = 119
Current timestep = 1207. State = [[-0.24688523  0.03018851]]. Action = [[ 0.00327881 -0.01957802  0.         -0.84666556]]. Reward = [0.]
Curr episode timestep = 120
Current timestep = 1208. State = [[-0.24638169  0.03330952]]. Action = [[ 0.01144729  0.06719071  0.         -0.56256473]]. Reward = [0.]
Curr episode timestep = 121
Current timestep = 1209. State = [[-0.24230912  0.0326697 ]]. Action = [[ 0.08293576 -0.05835613  0.         -0.9375634 ]]. Reward = [0.]
Curr episode timestep = 122
Current timestep = 1210. State = [[-0.24349421  0.03583313]]. Action = [[-0.07432809  0.09097365  0.         -0.8257165 ]]. Reward = [0.]
Curr episode timestep = 123
Current timestep = 1211. State = [[-0.24369366  0.03471739]]. Action = [[ 0.04112881 -0.08439684  0.         -0.64867663]]. Reward = [0.]
Curr episode timestep = 124
Current timestep = 1212. State = [[-0.23922145  0.03046979]]. Action = [[ 0.06862838 -0.0422456   0.         -0.80066454]]. Reward = [0.]
Curr episode timestep = 125
Current timestep = 1213. State = [[-0.23256883  0.02556735]]. Action = [[ 0.08741564 -0.06569484  0.         -0.8158995 ]]. Reward = [0.]
Curr episode timestep = 126
Current timestep = 1214. State = [[-0.22802614  0.02193541]]. Action = [[ 0.02321035 -0.02017921  0.         -0.5056188 ]]. Reward = [0.]
Curr episode timestep = 127
Current timestep = 1215. State = [[-0.2220207   0.02411875]]. Action = [[ 0.08783164  0.07502276  0.         -0.34339416]]. Reward = [0.]
Curr episode timestep = 128
Current timestep = 1216. State = [[-0.21696173  0.02125163]]. Action = [[ 0.03258836 -0.08311687  0.         -0.04897535]]. Reward = [0.]
Curr episode timestep = 129
Current timestep = 1217. State = [[-0.21847601  0.01594151]]. Action = [[-0.08848037 -0.04338192  0.         -0.71661997]]. Reward = [0.]
Curr episode timestep = 130
Current timestep = 1218. State = [[-0.219862    0.01119725]]. Action = [[-0.01424684 -0.05113518  0.         -0.88074344]]. Reward = [0.]
Curr episode timestep = 131
Current timestep = 1219. State = [[-0.22039683  0.01301511]]. Action = [[-0.02535085  0.08564167  0.         -0.72535026]]. Reward = [0.]
Curr episode timestep = 132
Current timestep = 1220. State = [[-0.21605252  0.01791565]]. Action = [[ 0.09715713  0.06158393  0.         -0.8879484 ]]. Reward = [0.]
Curr episode timestep = 133
Current timestep = 1221. State = [[-0.20939963  0.01726886]]. Action = [[ 0.07604036 -0.04313002  0.         -0.881214  ]]. Reward = [0.]
Curr episode timestep = 134
Current timestep = 1222. State = [[-0.20573917  0.01795475]]. Action = [[ 0.01578082  0.04575413  0.         -0.98335123]]. Reward = [0.]
Curr episode timestep = 135
Current timestep = 1223. State = [[-0.20026906  0.01799693]]. Action = [[ 0.08732819 -0.01929958  0.         -0.8300396 ]]. Reward = [0.]
Curr episode timestep = 136
Current timestep = 1224. State = [[-0.19303992  0.01558161]]. Action = [[ 0.0795655  -0.03197093  0.          0.03035843]]. Reward = [0.]
Curr episode timestep = 137
Current timestep = 1225. State = [[-0.1918095   0.01918236]]. Action = [[-0.04783508  0.0975787   0.         -0.62768865]]. Reward = [0.]
Curr episode timestep = 138
Current timestep = 1226. State = [[-0.19136111  0.02530971]]. Action = [[ 0.01883022  0.06354026  0.         -0.956842  ]]. Reward = [0.]
Curr episode timestep = 139
Current timestep = 1227. State = [[-0.18845654  0.02461577]]. Action = [[ 0.03628885 -0.06411439  0.         -0.5196593 ]]. Reward = [0.]
Curr episode timestep = 140
Current timestep = 1228. State = [[-0.18342115  0.02284076]]. Action = [[ 0.06068143 -0.00843155  0.         -0.77604127]]. Reward = [0.]
Curr episode timestep = 141
Current timestep = 1229. State = [[-0.1777924   0.02040038]]. Action = [[ 0.05475629 -0.04656516  0.         -0.8985442 ]]. Reward = [0.]
Curr episode timestep = 142
Current timestep = 1230. State = [[-0.17073397  0.01471908]]. Action = [[ 0.07958143 -0.08497     0.         -0.24298203]]. Reward = [0.]
Curr episode timestep = 143
Current timestep = 1231. State = [[-0.16415474  0.01153946]]. Action = [[ 0.04992426 -0.00246978  0.         -0.19743359]]. Reward = [0.]
Curr episode timestep = 144
Current timestep = 1232. State = [[-0.15654609  0.00756927]]. Action = [[ 0.08478313 -0.05943745  0.         -0.6733772 ]]. Reward = [0.]
Curr episode timestep = 145
Current timestep = 1233. State = [[-0.15563993  0.00211434]]. Action = [[-0.08637001 -0.05469277  0.         -0.42623436]]. Reward = [0.]
Curr episode timestep = 146
Current timestep = 1234. State = [[-0.15811372  0.00133148]]. Action = [[-0.04951741  0.03654604  0.         -0.9240083 ]]. Reward = [0.]
Curr episode timestep = 147
Current timestep = 1235. State = [[-0.16197167 -0.0024623 ]]. Action = [[-0.08425824 -0.08098026  0.         -0.7979698 ]]. Reward = [0.]
Curr episode timestep = 148
Current timestep = 1236. State = [[-0.16099432 -0.00261512]]. Action = [[ 0.04617137  0.05851708  0.         -0.7341627 ]]. Reward = [0.]
Curr episode timestep = 149
Current timestep = 1237. State = [[-0.15712115 -0.00467675]]. Action = [[ 0.03824998 -0.05946981  0.         -0.65409863]]. Reward = [0.]
Curr episode timestep = 150
Current timestep = 1238. State = [[-0.15902126 -0.01134472]]. Action = [[-0.08562725 -0.08539957  0.         -0.35178548]]. Reward = [0.]
Curr episode timestep = 151
Current timestep = 1239. State = [[-0.15783271 -0.01030998]]. Action = [[ 0.06215466  0.090431    0.         -0.78290045]]. Reward = [0.]
Curr episode timestep = 152
Current timestep = 1240. State = [[-0.15172474 -0.00653919]]. Action = [[ 0.09232419  0.0372797   0.         -0.8375932 ]]. Reward = [0.]
Curr episode timestep = 153
Current timestep = 1241. State = [[-0.14483854 -0.00458962]]. Action = [[ 0.09213509  0.0240899   0.         -0.56016195]]. Reward = [0.]
Curr episode timestep = 154
Current timestep = 1242. State = [[-0.13750072 -0.00549006]]. Action = [[ 0.09657145 -0.02395836  0.         -0.94968027]]. Reward = [0.]
Curr episode timestep = 155
Current timestep = 1243. State = [[-0.13005963 -0.00756708]]. Action = [[ 0.08752001 -0.01663981  0.         -0.9028813 ]]. Reward = [0.]
Curr episode timestep = 156
Current timestep = 1244. State = [[-0.12439183 -0.00924619]]. Action = [[ 0.0474278  -0.01011121  0.         -0.6292052 ]]. Reward = [0.]
Curr episode timestep = 157
Current timestep = 1245. State = [[-0.12318923 -0.00862858]]. Action = [[-0.02932087  0.03016647  0.         -0.67637503]]. Reward = [0.]
Curr episode timestep = 158
Current timestep = 1246. State = [[-0.11968011 -0.00694382]]. Action = [[ 0.06070001  0.02222802  0.         -0.5505903 ]]. Reward = [0.]
Curr episode timestep = 159
Current timestep = 1247. State = [[-0.11712447 -0.00360615]]. Action = [[-0.00620744  0.05461665  0.         -0.8520071 ]]. Reward = [0.]
Curr episode timestep = 160
Current timestep = 1248. State = [[-0.11588366  0.00024555]]. Action = [[ 0.00256491  0.0381903   0.         -0.92968225]]. Reward = [0.]
Curr episode timestep = 161
Current timestep = 1249. State = [[-0.11873171  0.00136811]]. Action = [[-0.08785387 -0.0159388   0.         -0.4865278 ]]. Reward = [0.]
Curr episode timestep = 162
Current timestep = 1250. State = [[-0.12407389 -0.00207909]]. Action = [[-0.09559827 -0.08112605  0.          0.1230402 ]]. Reward = [0.]
Curr episode timestep = 163
Current timestep = 1251. State = [[-0.12128296 -0.00190786]]. Action = [[0.09941078 0.03588287 0.         0.5215683 ]]. Reward = [0.]
Curr episode timestep = 164
Current timestep = 1252. State = [[-0.11255135 -0.00401925]]. Action = [[ 0.09819669 -0.06936342  0.         -0.5690774 ]]. Reward = [0.]
Curr episode timestep = 165
Current timestep = 1253. State = [[-0.10820767 -0.00513934]]. Action = [[-0.00447945  0.01713882  0.         -0.9787303 ]]. Reward = [0.]
Curr episode timestep = 166
Current timestep = 1254. State = [[-0.10294873 -0.00727568]]. Action = [[ 0.07324667 -0.04860956  0.         -0.6337875 ]]. Reward = [0.]
Curr episode timestep = 167
Current timestep = 1255. State = [[-0.10315832 -0.01175878]]. Action = [[-0.08944675 -0.05478218  0.          0.32800663]]. Reward = [0.]
Curr episode timestep = 168
Current timestep = 1256. State = [[-0.10462973 -0.01820638]]. Action = [[-0.01707862 -0.08660525  0.         -0.9354211 ]]. Reward = [0.]
Curr episode timestep = 169
Current timestep = 1257. State = [[-0.10662107 -0.02040108]]. Action = [[-0.05841303  0.02330651  0.         -0.9084259 ]]. Reward = [0.]
Curr episode timestep = 170
Current timestep = 1258. State = [[-0.10552194 -0.02321186]]. Action = [[ 0.03620983 -0.05193964  0.         -0.31416726]]. Reward = [0.]
Curr episode timestep = 171
Current timestep = 1259. State = [[-0.09934338 -0.0260799 ]]. Action = [[ 0.0972806  -0.00606503  0.         -0.9448561 ]]. Reward = [0.]
Curr episode timestep = 172
Current timestep = 1260. State = [[-0.09175278 -0.03051693]]. Action = [[ 0.09615634 -0.05639641  0.         -0.9181589 ]]. Reward = [0.]
Curr episode timestep = 173
Current timestep = 1261. State = [[-0.09072312 -0.03524943]]. Action = [[-0.05366988 -0.02986618  0.         -0.8618702 ]]. Reward = [0.]
Curr episode timestep = 174
Current timestep = 1262. State = [[-0.09134501 -0.03468542]]. Action = [[ 0.00691625  0.05910213  0.         -0.85171074]]. Reward = [0.]
Curr episode timestep = 175
Current timestep = 1263. State = [[-0.09464183 -0.0363953 ]]. Action = [[-0.07385752 -0.04634653  0.         -0.94131416]]. Reward = [0.]
Curr episode timestep = 176
Current timestep = 1264. State = [[-0.09413002 -0.03414317]]. Action = [[ 0.05255345  0.0871575   0.         -0.6869533 ]]. Reward = [0.]
Curr episode timestep = 177
Current timestep = 1265. State = [[-0.09077609 -0.03027061]]. Action = [[0.04935243 0.03643721 0.         0.00521529]]. Reward = [0.]
Curr episode timestep = 178
Current timestep = 1266. State = [[-0.08510464 -0.02970345]]. Action = [[ 0.09831584 -0.00858627  0.         -0.92375267]]. Reward = [0.]
Curr episode timestep = 179
Current timestep = 1267. State = [[-0.08565632 -0.02906945]]. Action = [[-0.07214734  0.02045846  0.         -0.9472039 ]]. Reward = [0.]
Curr episode timestep = 180
Current timestep = 1268. State = [[-0.08448469 -0.02665613]]. Action = [[ 0.07603583  0.03306129  0.         -0.69183475]]. Reward = [0.]
Curr episode timestep = 181
Current timestep = 1269. State = [[-0.07837323 -0.03038989]]. Action = [[ 0.09469921 -0.09726688  0.         -0.49031413]]. Reward = [0.]
Curr episode timestep = 182
Current timestep = 1270. State = [[-0.07752687 -0.0336316 ]]. Action = [[-0.04673731 -0.0044442   0.         -0.23163795]]. Reward = [0.]
Curr episode timestep = 183
Current timestep = 1271. State = [[-0.07493845 -0.03161379]]. Action = [[ 0.07694226  0.05039793  0.         -0.44311583]]. Reward = [0.]
Curr episode timestep = 184
Current timestep = 1272. State = [[-0.0747241  -0.02754063]]. Action = [[-0.03979312  0.05439273  0.         -0.84413886]]. Reward = [0.]
Curr episode timestep = 185
Current timestep = 1273. State = [[-0.07669104 -0.0237675 ]]. Action = [[-0.02275301  0.03585046  0.         -0.96883667]]. Reward = [0.]
Curr episode timestep = 186
Current timestep = 1274. State = [[-0.08093937 -0.02394367]]. Action = [[-0.08094515 -0.03883956  0.         -0.75491375]]. Reward = [0.]
Curr episode timestep = 187
Current timestep = 1275. State = [[-0.07916435 -0.02722369]]. Action = [[ 0.07916697 -0.05621488  0.          0.24756241]]. Reward = [0.]
Curr episode timestep = 188
Current timestep = 1276. State = [[-0.07534906 -0.03126711]]. Action = [[ 0.01912205 -0.04902221  0.         -0.9162812 ]]. Reward = [0.]
Curr episode timestep = 189
Current timestep = 1277. State = [[-0.07114431 -0.02824641]]. Action = [[ 0.05651359  0.09464214  0.         -0.4395867 ]]. Reward = [0.]
Curr episode timestep = 190
Current timestep = 1278. State = [[-0.06597058 -0.02861621]]. Action = [[ 0.06198921 -0.05983996  0.         -0.95381486]]. Reward = [0.]
Curr episode timestep = 191
Current timestep = 1279. State = [[-0.06004931 -0.03072052]]. Action = [[ 0.06902225 -0.00326843  0.         -0.9016288 ]]. Reward = [0.]
Curr episode timestep = 192
Current timestep = 1280. State = [[-0.05989292 -0.03415024]]. Action = [[-0.06367379 -0.05582122  0.         -0.8888857 ]]. Reward = [0.]
Curr episode timestep = 193
Current timestep = 1281. State = [[-0.05599407 -0.03380947]]. Action = [[0.09871877 0.04920609 0.         0.47714758]]. Reward = [0.]
Curr episode timestep = 194
Current timestep = 1282. State = [[-0.05037479 -0.03687853]]. Action = [[ 0.03962054 -0.07725555  0.          0.08483493]]. Reward = [0.]
Curr episode timestep = 195
Current timestep = 1283. State = [[-0.1522485  -0.02356259]]. Action = [[ 0.09129442  0.01992826  0.         -0.10967141]]. Reward = [100.]
Curr episode timestep = 196
Current timestep = 1284. State = [[-0.14905718 -0.01997463]]. Action = [[ 0.05583062  0.0481595   0.         -0.6445572 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1285. State = [[-0.15173443 -0.01829568]]. Action = [[-0.07468277 -0.0233977   0.         -0.00526172]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1286. State = [[-0.15145497 -0.02031309]]. Action = [[ 0.06308366 -0.05405015  0.         -0.31660426]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1287. State = [[-0.15248443 -0.02153727]]. Action = [[-0.04903311 -0.00842996  0.         -0.86115223]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1288. State = [[-0.15291555 -0.02021747]]. Action = [[ 0.02297156  0.02149476  0.         -0.7942763 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1289. State = [[-0.14933214 -0.02163463]]. Action = [[ 0.06905147 -0.04822059  0.         -0.33459204]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1290. State = [[-0.14500615 -0.02490392]]. Action = [[ 0.05134857 -0.03654051  0.         -0.5901934 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1291. State = [[-0.14020371 -0.02679164]]. Action = [[ 0.06404687 -0.00572274  0.         -0.61442816]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1292. State = [[-0.13391925 -0.02719513]]. Action = [[0.08495305 0.01091381 0.         0.7311326 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1293. State = [[-0.13382417 -0.03200505]]. Action = [[-0.06784643 -0.08601756  0.         -0.8578407 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1294. State = [[-0.130403   -0.03178877]]. Action = [[ 0.09716428  0.07297089  0.         -0.91511804]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1295. State = [[-0.12547128 -0.02621757]]. Action = [[ 0.03802367  0.08634041  0.         -0.90459275]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1296. State = [[-0.12312668 -0.02110277]]. Action = [[ 0.01481391  0.0550187   0.         -0.45672703]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1297. State = [[-0.11857858 -0.01597817]]. Action = [[0.07490285 0.06401957 0.         0.01946092]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1298. State = [[-0.11812031 -0.01601604]]. Action = [[-0.04767336 -0.05025861  0.         -0.7093903 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1299. State = [[-0.12128632 -0.01636304]]. Action = [[-0.06142694  0.00616457  0.          0.26830828]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1300. State = [[-0.11957817 -0.01480946]]. Action = [[ 0.05416618  0.01244458  0.         -0.54978085]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1301. State = [[-0.11825497 -0.00949206]]. Action = [[-0.01728626  0.08163872  0.          0.07876503]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1302. State = [[-0.1198417  -0.00350538]]. Action = [[-0.03539765  0.04766614  0.         -0.5878593 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1303. State = [[-0.11838603 -0.00047072]]. Action = [[ 0.04203295  0.00218363  0.         -0.9234648 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1304. State = [[-0.11589557 -0.00149151]]. Action = [[ 0.01915038 -0.0484239   0.         -0.18570292]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1305. State = [[-0.11353578  0.00026799]]. Action = [[ 0.02428417  0.04443061  0.         -0.6543772 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1306. State = [[-0.10974003  0.00015157]]. Action = [[ 0.05372036 -0.0427042   0.         -0.4228872 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1307. State = [[-0.11143485 -0.00193944]]. Action = [[-0.08742737 -0.02879996  0.         -0.9425713 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1308. State = [[-0.10980265  0.00036743]]. Action = [[ 0.07575514  0.05613812  0.         -0.5968161 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1309. State = [[-0.1036308   0.00343665]]. Action = [[ 0.07814873  0.02461779  0.         -0.5596857 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1310. State = [[-0.09610424  0.00084751]]. Action = [[ 0.09748615 -0.06880541  0.          0.25463367]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1311. State = [[-0.0945828  -0.00337103]]. Action = [[-0.05487427 -0.03925972  0.         -0.50682867]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1312. State = [[-0.09430106 -0.00810835]]. Action = [[ 0.01075475 -0.06164092  0.         -0.8917816 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1313. State = [[-0.0899744  -0.00891432]]. Action = [[ 0.06424045  0.03580465  0.         -0.73854244]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1314. State = [[-0.08307188 -0.00731581]]. Action = [[ 0.09179973  0.02959276  0.         -0.8733265 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1315. State = [[-0.07788984 -0.00340705]]. Action = [[ 0.04125472  0.07464729  0.         -0.6556598 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1316. State = [[-0.07268207  0.0001401 ]]. Action = [[ 0.06988875  0.03450534  0.         -0.64531636]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1317. State = [[-0.07208714  0.00548988]]. Action = [[-0.04353073  0.08533796  0.         -0.8914035 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1318. State = [[-0.07034879  0.00928951]]. Action = [[ 0.04671853  0.01401464  0.         -0.6556926 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1319. State = [[-0.0684125   0.01439345]]. Action = [[ 0.00379511  0.07469396  0.         -0.11865944]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1320. State = [[-0.06550876  0.02225167]]. Action = [[ 0.04917938  0.08842159  0.         -0.45272684]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1321. State = [[-0.06015744  0.02598001]]. Action = [[ 0.07712748 -0.00848782  0.         -0.84947413]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1322. State = [[-0.05873122  0.02571864]]. Action = [[-0.03152482 -0.03215601  0.         -0.4654079 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1323. State = [[-0.05752836  0.02269951]]. Action = [[ 0.01704544 -0.06906232  0.         -0.793935  ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1324. State = [[-0.05863933  0.02421636]]. Action = [[-0.06030364  0.05071569  0.         -0.92618495]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1325. State = [[-0.05763272  0.0298931 ]]. Action = [[ 0.03101071  0.06300544  0.         -0.36914015]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1326. State = [[-0.05603214  0.03524303]]. Action = [[ 0.00057855  0.04423905  0.         -0.47895038]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1327. State = [[-0.05133002  0.03660355]]. Action = [[ 0.0823879  -0.02376808  0.         -0.17778432]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1328. State = [[-0.25770146 -0.0270739 ]]. Action = [[ 0.0935281 -0.084682   0.        -0.9332698]]. Reward = [100.]
Curr episode timestep = 44
Current timestep = 1329. State = [[-0.2602836 -0.0200331]]. Action = [[-0.07102768  0.07889623  0.         -0.42452478]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1330. State = [[-0.26227695 -0.0134021 ]]. Action = [[ 0.02252412  0.07060657  0.         -0.85172576]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1331. State = [[-0.2615796  -0.01363134]]. Action = [[ 0.0275951  -0.06769626  0.         -0.8900086 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1332. State = [[-0.26132944 -0.01028698]]. Action = [[ 0.00585087  0.08841338  0.         -0.10838038]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1333. State = [[-0.26374286 -0.00267789]]. Action = [[-0.03466512  0.08144204  0.         -0.39595342]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1334. State = [[-0.2661648   0.00121353]]. Action = [[-0.01186733  0.00079374  0.         -0.02165782]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1335. State = [[-0.2660708   0.00644818]]. Action = [[0.02599479 0.07288107 0.         0.3035679 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1336. State = [[-0.26584882  0.01405355]]. Action = [[0.01000478 0.07776789 0.         0.7169527 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1337. State = [[-0.26364458  0.01777295]]. Action = [[ 0.05695233 -0.00703476  0.         -0.38643038]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1338. State = [[-0.26308686  0.02250969]]. Action = [[-0.00990596  0.06458443  0.         -0.6984632 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1339. State = [[-0.25916663  0.02534001]]. Action = [[ 0.09319323 -0.01241112  0.         -0.8679377 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1340. State = [[-0.2518834   0.02752076]]. Action = [[0.09631861 0.02326085 0.         0.57220805]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1341. State = [[-0.24675727  0.02944253]]. Action = [[ 0.03694338  0.0040807   0.         -0.9413392 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1342. State = [[-0.24339402  0.02858829]]. Action = [[ 0.02217378 -0.03908914  0.         -0.90996367]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1343. State = [[-0.23783553  0.02586024]]. Action = [[ 0.06597052 -0.04526669  0.         -0.3541218 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1344. State = [[-0.23756813  0.02264605]]. Action = [[-0.0813356  -0.04413986  0.         -0.28696704]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1345. State = [[-0.23533699  0.018992  ]]. Action = [[ 0.04862096 -0.04787556  0.         -0.8485615 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1346. State = [[-0.23382822  0.01734419]]. Action = [[-0.03628538  0.00219563  0.         -0.8507359 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1347. State = [[-0.23603132  0.01345716]]. Action = [[-0.06796959 -0.07060345  0.          0.14193428]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1348. State = [[-0.23397276  0.01032898]]. Action = [[ 0.04732376 -0.00942277  0.         -0.10762298]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1349. State = [[-0.234314    0.00503135]]. Action = [[-0.06674729 -0.08203728  0.          0.39893222]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1350. State = [[-0.23893455 -0.00317679]]. Action = [[-0.09228562 -0.09675324  0.         -0.82847077]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1351. State = [[-0.2443666  -0.00612511]]. Action = [[-0.08286711  0.023398    0.         -0.88541836]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1352. State = [[-0.24697146 -0.00798673]]. Action = [[-0.01160564 -0.02671394  0.         -0.8451687 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1353. State = [[-0.25192863 -0.00690151]]. Action = [[-0.08672424  0.05575416  0.         -0.8620645 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1354. State = [[-0.25253534 -0.00239751]]. Action = [[ 0.06695113  0.06832729  0.         -0.82560664]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1355. State = [[-2.4958727e-01 -2.3383122e-04]]. Action = [[ 0.05738892  0.00443397  0.         -0.7460525 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1356. State = [[-0.24599703 -0.00261223]]. Action = [[ 0.06801804 -0.04907905  0.         -0.49472427]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1357. State = [[-2.4497098e-01  1.5096735e-04]]. Action = [[ 0.0031579   0.09019163  0.         -0.26765156]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1358. State = [[-0.24476582  0.0014282 ]]. Action = [[ 0.02197709 -0.02536987  0.         -0.92922103]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1359. State = [[-0.24784964  0.00386187]]. Action = [[-0.06070752  0.05912448  0.         -0.95673156]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1360. State = [[-0.25212923  0.00720816]]. Action = [[-0.03794503  0.02530005  0.         -0.5470681 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1361. State = [[-0.25536358  0.01130013]]. Action = [[-0.02491494  0.05191625  0.         -0.43061316]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1362. State = [[-0.25271398  0.00930039]]. Action = [[ 0.09055699 -0.08704942  0.         -0.7967573 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1363. State = [[-0.24834599  0.01125619]]. Action = [[ 0.05250192  0.08104237  0.         -0.5558523 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1364. State = [[-0.24272445  0.0106681 ]]. Action = [[ 0.09079725 -0.06584142  0.          0.5637181 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1365. State = [[-0.24377045  0.00626176]]. Action = [[-0.09379745 -0.0568116   0.          0.72879076]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1366. State = [[-0.24324726  0.00609615]]. Action = [[ 0.06408722  0.03143293  0.         -0.57206655]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1367. State = [[-0.23942228  0.0077553 ]]. Action = [[ 0.04476506  0.01907383  0.         -0.7982435 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1368. State = [[-0.24188985  0.00429028]]. Action = [[-0.09087986 -0.07951073  0.         -0.8253131 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1369. State = [[-0.2403326   0.00395766]]. Action = [[ 0.08176278  0.04451121  0.         -0.8954568 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1370. State = [[-0.23547807  0.00102052]]. Action = [[ 0.0428387  -0.07564411  0.          0.30193114]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1371. State = [[-0.2307599 -0.0019749]]. Action = [[ 0.04850369 -0.00481866  0.          0.04729044]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1372. State = [[-0.22454785 -0.00205489]]. Action = [[0.07760809 0.01874076 0.         0.31111872]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1373. State = [[-2.2281761e-01 -4.9348771e-05]]. Action = [[-0.03194796  0.04313662  0.         -0.83358663]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1374. State = [[-0.22228813 -0.00105265]]. Action = [[ 0.00627544 -0.03802835  0.         -0.63706666]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1375. State = [[-0.22558518 -0.00347122]]. Action = [[-0.09608077 -0.02120493  0.          0.32513535]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1376. State = [[-0.22992815 -0.00445136]]. Action = [[-0.05889101 -0.00284642  0.          0.5156343 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1377. State = [[-0.22865503 -0.00498919]]. Action = [[ 0.05136731 -0.00693079  0.         -0.5745985 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1378. State = [[-0.22881639 -0.00735142]]. Action = [[-0.04226753 -0.04069385  0.         -0.86320126]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1379. State = [[-0.23227488 -0.0098297 ]]. Action = [[-0.06071253 -0.02147396  0.         -0.04505998]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1380. State = [[-0.22991592 -0.01104193]]. Action = [[ 0.0836075  -0.00498664  0.         -0.2951975 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1381. State = [[-0.22628199 -0.0120502 ]]. Action = [[ 0.02400494 -0.00839053  0.         -0.35959542]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1382. State = [[-0.22732532 -0.01126163]]. Action = [[-0.04437976  0.02910518  0.         -0.7792864 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1383. State = [[-0.22512485 -0.01293101]]. Action = [[ 0.06719387 -0.04442405  0.         -0.5497892 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1384. State = [[-0.22657748 -0.01773054]]. Action = [[-0.07965577 -0.06158936  0.         -0.9730045 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1385. State = [[-0.22431111 -0.01613481]]. Action = [[0.09559124 0.08071396 0.         0.3124894 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1386. State = [[-0.22002825 -0.01307162]]. Action = [[ 0.03637809  0.02165861  0.         -0.7424767 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1387. State = [[-0.22038367 -0.01394175]]. Action = [[-0.03525413 -0.02958362  0.         -0.43850696]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1388. State = [[-0.22113921 -0.01844758]]. Action = [[-0.00667365 -0.06922738  0.         -0.15672475]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1389. State = [[-0.22249112 -0.02374504]]. Action = [[-0.037165   -0.05548367  0.         -0.53676325]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1390. State = [[-0.2243081  -0.02161905]]. Action = [[-0.02741745  0.08906854  0.         -0.8373855 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1391. State = [[-0.22648935 -0.02171816]]. Action = [[-0.03197742 -0.04799236  0.         -0.57051176]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1392. State = [[-0.22292754 -0.02060111]]. Action = [[0.09611172 0.05153299 0.         0.1075809 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1393. State = [[-0.22178137 -0.01775637]]. Action = [[-0.02719716  0.02877551  0.          0.21263635]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1394. State = [[-0.22244658 -0.01504174]]. Action = [[ 0.00383115  0.03248792  0.         -0.9027736 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1395. State = [[-0.22400412 -0.01260751]]. Action = [[-0.02720062  0.02144657  0.         -0.12519681]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1396. State = [[-0.22499599 -0.01509856]]. Action = [[-5.0249696e-04 -7.2261259e-02  0.0000000e+00 -5.2873635e-01]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1397. State = [[-0.22132312 -0.01865128]]. Action = [[ 0.07768119 -0.03249041  0.         -0.93375164]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1398. State = [[-0.21573886 -0.0205554 ]]. Action = [[ 0.07021708 -0.01380473  0.          0.21258354]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1399. State = [[-0.20897633 -0.01796496]]. Action = [[ 0.09423793  0.06717906  0.         -0.8685392 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1400. State = [[-0.20182712 -0.02121611]]. Action = [[ 0.08484126 -0.09787747  0.         -0.7151821 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1401. State = [[-0.19936192 -0.02870403]]. Action = [[-0.0240493  -0.08127199  0.         -0.72271025]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1402. State = [[-0.19391629 -0.03334727]]. Action = [[ 0.09656275 -0.02163319  0.         -0.9861875 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1403. State = [[-0.18511987 -0.03949488]]. Action = [[ 0.09513991 -0.08043478  0.         -0.49228537]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1404. State = [[-0.17772208 -0.04603684]]. Action = [[ 0.05692431 -0.04900298  0.         -0.9290459 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1405. State = [[-0.17432962 -0.04800353]]. Action = [[-0.00808031  0.03046001  0.         -0.4982332 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1406. State = [[-0.17589755 -0.05223063]]. Action = [[-0.07586658 -0.06438908  0.         -0.643402  ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1407. State = [[-0.17199127 -0.05451298]]. Action = [[ 0.08687796  0.02440177  0.         -0.9857513 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1408. State = [[-0.16317694 -0.0594161 ]]. Action = [[ 0.09451217 -0.07995342  0.         -0.10615593]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1409. State = [[-0.156853 -0.066855]]. Action = [[ 0.03061391 -0.06978998  0.         -0.60147   ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1410. State = [[-0.14996023 -0.07470338]]. Action = [[ 0.07541627 -0.07666234  0.         -0.59989375]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1411. State = [[-0.14142312 -0.08336915]]. Action = [[ 0.08581967 -0.08450985  0.         -0.8544345 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1412. State = [[-0.13746102 -0.08643728]]. Action = [[-0.01783876  0.03687415  0.          0.20941734]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1413. State = [[-0.13376716 -0.08696231]]. Action = [[ 0.04371072  0.01147113  0.         -0.01388192]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1414. State = [[-0.12644307 -0.08364332]]. Action = [[ 0.0942628   0.08940043  0.         -0.9340676 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1415. State = [[-0.12492106 -0.07903528]]. Action = [[-0.05272783  0.05715612  0.         -0.9848827 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1416. State = [[-0.12679856 -0.08173241]]. Action = [[-0.0366386  -0.08395372  0.         -0.8472825 ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1417. State = [[-0.12963824 -0.08143397]]. Action = [[-0.06248078  0.05751277  0.         -0.59496117]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1418. State = [[-0.1306376  -0.07886109]]. Action = [[-0.0047381   0.01629122  0.          0.37712288]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1419. State = [[-0.12631652 -0.08049916]]. Action = [[ 0.08221377 -0.0538489   0.         -0.91572833]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1420. State = [[-0.12086993 -0.07914203]]. Action = [[ 0.05615488  0.05162197  0.         -0.09989268]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 1421. State = [[-0.12135101 -0.08120544]]. Action = [[-0.0567014  -0.07588543  0.         -0.2944346 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1422. State = [[-0.12114663 -0.0811055 ]]. Action = [[ 0.01982699  0.04392276  0.         -0.5677164 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1423. State = [[-0.11566932 -0.07699449]]. Action = [[ 0.09620049  0.0538997   0.         -0.40518767]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1424. State = [[-0.11639211 -0.07204889]]. Action = [[-0.08282454  0.05860957  0.         -0.7431952 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1425. State = [[-0.1175174  -0.07383718]]. Action = [[ 0.02449743 -0.08617399  0.          0.08641613]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 1426. State = [[-0.12137856 -0.07161687]]. Action = [[-0.09654924  0.08460768  0.         -0.77633345]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1427. State = [[-0.12261327 -0.0653681 ]]. Action = [[0.04110933 0.0616735  0.         0.10355914]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1428. State = [[-0.12552896 -0.06593533]]. Action = [[-0.07226574 -0.06995155  0.         -0.72593296]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 1429. State = [[-0.12713131 -0.07069825]]. Action = [[ 0.00929821 -0.06918834  0.         -0.40264726]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 1430. State = [[-0.12296238 -0.06776502]]. Action = [[ 0.09083953  0.09660221  0.         -0.9631904 ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 1431. State = [[-0.12405334 -0.06731902]]. Action = [[-0.06822899 -0.05432311  0.         -0.58437645]]. Reward = [0.]
Curr episode timestep = 102
Current timestep = 1432. State = [[-0.12195805 -0.07074597]]. Action = [[ 0.09166036 -0.04410654  0.         -0.6080539 ]]. Reward = [0.]
Curr episode timestep = 103
Current timestep = 1433. State = [[-0.1179122  -0.07483654]]. Action = [[ 0.03502323 -0.04878286  0.         -0.62324816]]. Reward = [0.]
Curr episode timestep = 104
Current timestep = 1434. State = [[-0.11369018 -0.08066125]]. Action = [[ 0.05891813 -0.07254463  0.         -0.54218745]]. Reward = [0.]
Curr episode timestep = 105
Current timestep = 1435. State = [[-0.1105909  -0.08496129]]. Action = [[ 0.0185198  -0.01959777  0.         -0.96652734]]. Reward = [0.]
Curr episode timestep = 106
Current timestep = 1436. State = [[-0.10589302 -0.09094124]]. Action = [[ 0.07065064 -0.07792757  0.         -0.83092546]]. Reward = [0.]
Curr episode timestep = 107
Current timestep = 1437. State = [[-0.09892205 -0.09296858]]. Action = [[ 0.08744996  0.03981446  0.         -0.86218244]]. Reward = [0.]
Curr episode timestep = 108
Current timestep = 1438. State = [[-0.09499133 -0.09095837]]. Action = [[ 0.01487903  0.05118824  0.         -0.37954926]]. Reward = [0.]
Curr episode timestep = 109
Current timestep = 1439. State = [[-0.08928584 -0.09301129]]. Action = [[ 0.09033228 -0.04734677  0.         -0.71339154]]. Reward = [0.]
Curr episode timestep = 110
Current timestep = 1440. State = [[-0.08264512 -0.09123922]]. Action = [[ 0.06607539  0.08388341  0.         -0.788694  ]]. Reward = [0.]
Curr episode timestep = 111
Current timestep = 1441. State = [[-0.08098609 -0.08742772]]. Action = [[-0.02626863  0.04180864  0.         -0.6952411 ]]. Reward = [0.]
Curr episode timestep = 112
Current timestep = 1442. State = [[-0.08438741 -0.0828853 ]]. Action = [[-0.08560359  0.06642545  0.         -0.9087661 ]]. Reward = [0.]
Curr episode timestep = 113
Current timestep = 1443. State = [[-0.08600434 -0.08424555]]. Action = [[-0.01236403 -0.07659662  0.         -0.9857056 ]]. Reward = [0.]
Curr episode timestep = 114
Current timestep = 1444. State = [[-0.08278608 -0.08294806]]. Action = [[ 0.05065069  0.05841909  0.         -0.34742987]]. Reward = [0.]
Curr episode timestep = 115
Current timestep = 1445. State = [[-0.07627141 -0.08554046]]. Action = [[ 0.08731585 -0.09738152  0.         -0.602675  ]]. Reward = [0.]
Curr episode timestep = 116
Current timestep = 1446. State = [[-0.07498841 -0.09012981]]. Action = [[-0.05394376 -0.03969904  0.         -0.62047297]]. Reward = [0.]
Curr episode timestep = 117
Current timestep = 1447. State = [[-0.07351331 -0.09142265]]. Action = [[ 0.03680969  0.00179847  0.         -0.68435884]]. Reward = [0.]
Curr episode timestep = 118
Current timestep = 1448. State = [[-0.07176133 -0.0917694 ]]. Action = [[-4.3115020e-04 -3.1125322e-03  0.0000000e+00 -9.2364490e-01]]. Reward = [0.]
Curr episode timestep = 119
Current timestep = 1449. State = [[-0.06679325 -0.09178592]]. Action = [[ 0.08777624  0.00583518  0.         -0.7184318 ]]. Reward = [0.]
Curr episode timestep = 120
Current timestep = 1450. State = [[-0.06435309 -0.08720627]]. Action = [[-0.0120842   0.09219176  0.         -0.6141856 ]]. Reward = [0.]
Curr episode timestep = 121
Current timestep = 1451. State = [[-0.06174633 -0.08581583]]. Action = [[ 0.04913086 -0.02952482  0.         -0.3777145 ]]. Reward = [0.]
Curr episode timestep = 122
Current timestep = 1452. State = [[-0.05562989 -0.08311352]]. Action = [[0.09115822 0.06495898 0.         0.03635311]]. Reward = [0.]
Curr episode timestep = 123
Current timestep = 1453. State = [[-0.05413343 -0.07626005]]. Action = [[-0.03057396  0.0919949   0.         -0.91094124]]. Reward = [0.]
Curr episode timestep = 124
Current timestep = 1454. State = [[-0.05213837 -0.07282891]]. Action = [[ 0.05149119 -0.00469659  0.         -0.35185474]]. Reward = [0.]
Curr episode timestep = 125
Current timestep = 1455. State = [[-0.0544092 -0.0709084]]. Action = [[-0.09009673  0.01936356  0.         -0.9512702 ]]. Reward = [0.]
Curr episode timestep = 126
Current timestep = 1456. State = [[-0.05280643 -0.0672047 ]]. Action = [[ 0.08326019  0.03759692  0.         -0.9475701 ]]. Reward = [0.]
Curr episode timestep = 127
Current timestep = 1457. State = [[-0.11977291 -0.16167407]]. Action = [[ 0.0128345  -0.00222494  0.          0.13097155]]. Reward = [100.]
Curr episode timestep = 128
Scene graph at timestep 1457 is [True, False, False, True, False, False]
State prediction error at timestep 1457 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1457 of None
Current timestep = 1458. State = [[-0.12035389 -0.1562364 ]]. Action = [[ 0.00671816  0.05910485  0.         -0.778942  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1458 is [True, False, False, True, False, False]
State prediction error at timestep 1458 is tensor(6.3739e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1458 of None
Current timestep = 1459. State = [[-0.11533095 -0.15110257]]. Action = [[ 0.08077418  0.05396464  0.         -0.95872766]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1459 is [True, False, False, True, False, False]
State prediction error at timestep 1459 is tensor(2.6748e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1459 of None
Current timestep = 1460. State = [[-0.10777248 -0.152371  ]]. Action = [[ 0.09462882 -0.07842757  0.          0.03079844]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1460 is [True, False, False, True, False, False]
State prediction error at timestep 1460 is tensor(7.7317e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1460 of None
Current timestep = 1461. State = [[-0.1019112  -0.15578626]]. Action = [[ 0.04644995 -0.03657296  0.          0.48086238]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1461 is [True, False, False, True, False, False]
State prediction error at timestep 1461 is tensor(3.4903e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1461 of None
Current timestep = 1462. State = [[-0.09782592 -0.15263441]]. Action = [[ 0.03318185  0.0823609   0.         -0.6967939 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1462 is [True, False, False, True, False, False]
State prediction error at timestep 1462 is tensor(1.1407e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1462 of None
Current timestep = 1463. State = [[-0.09791818 -0.14520779]]. Action = [[-0.04330913  0.09359742  0.         -0.3913722 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1463 is [True, False, False, True, False, False]
State prediction error at timestep 1463 is tensor(5.0132e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1463 of None
Current timestep = 1464. State = [[-0.09780114 -0.14047582]]. Action = [[ 0.00530905  0.02120475  0.         -0.5341937 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1464 is [True, False, False, True, False, False]
State prediction error at timestep 1464 is tensor(2.9739e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1464 of None
Current timestep = 1465. State = [[-0.10048369 -0.13969953]]. Action = [[-0.07798598 -0.0199139   0.         -0.54581344]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1465 is [True, False, False, True, False, False]
State prediction error at timestep 1465 is tensor(2.5136e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1465 of None
Current timestep = 1466. State = [[-0.10276059 -0.1347935 ]]. Action = [[-0.0244987   0.08981109  0.         -0.9281361 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1466 is [True, False, False, True, False, False]
State prediction error at timestep 1466 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1466 of None
Current timestep = 1467. State = [[-0.09854396 -0.13231908]]. Action = [[ 0.09693819 -0.03258184  0.         -0.6413753 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1467 is [True, False, False, True, False, False]
State prediction error at timestep 1467 is tensor(1.4060e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1467 of None
Current timestep = 1468. State = [[-0.09428138 -0.1281499 ]]. Action = [[ 0.01738431  0.07449133  0.         -0.76430964]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1468 is [True, False, False, True, False, False]
State prediction error at timestep 1468 is tensor(1.7943e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1468 of None
Current timestep = 1469. State = [[-0.09512608 -0.12804411]]. Action = [[-0.0474808  -0.06430553  0.          0.6588993 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1469 is [True, False, False, True, False, False]
State prediction error at timestep 1469 is tensor(6.9015e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1469 of None
Current timestep = 1470. State = [[-0.0912383  -0.12880035]]. Action = [[ 0.09099322  0.00326665  0.         -0.9045988 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1471. State = [[-0.08767343 -0.12610084]]. Action = [[ 0.00741734  0.04053516  0.         -0.9907661 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1472. State = [[-0.08870599 -0.11963506]]. Action = [[-0.04378975  0.09150291  0.         -0.7463996 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1472 is [True, False, False, False, True, False]
State prediction error at timestep 1472 is tensor(6.2122e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1472 of None
Current timestep = 1473. State = [[-0.09195343 -0.11492476]]. Action = [[-0.05769574  0.02006108  0.         -0.34674048]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1474. State = [[-0.09334356 -0.11018013]]. Action = [[-0.00715125  0.05675112  0.         -0.81397474]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1475. State = [[-0.08915309 -0.10804808]]. Action = [[ 0.09046824 -0.0206999   0.         -0.83892137]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1476. State = [[-0.0824546 -0.1042038]]. Action = [[ 0.08680476  0.06065553  0.         -0.7363639 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1477. State = [[-0.08294597 -0.10070478]]. Action = [[-0.07169628  0.01188739  0.         -0.37613386]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1478. State = [[-0.08114468 -0.10356468]]. Action = [[ 0.07852257 -0.08982591  0.         -0.74363494]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1479. State = [[-0.07722954 -0.10907622]]. Action = [[ 0.0266793  -0.06735675  0.         -0.83700347]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1480. State = [[-0.07527886 -0.11346737]]. Action = [[ 0.00490166 -0.04191548  0.          0.1415534 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1481. State = [[-0.07102048 -0.11149344]]. Action = [[ 0.06619184  0.07645143  0.         -0.7499461 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1482. State = [[-0.06699486 -0.10668141]]. Action = [[ 0.0318751   0.05845921  0.         -0.80807984]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1483. State = [[-0.06370759 -0.10614852]]. Action = [[ 0.03412185 -0.02519017  0.         -0.05667877]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1484. State = [[-0.05776478 -0.10999449]]. Action = [[ 0.08599053 -0.06124054  0.         -0.632855  ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1485. State = [[-0.2501804  -0.08372994]]. Action = [[ 0.09539647 -0.0100802   0.         -0.65541565]]. Reward = [100.]
Curr episode timestep = 27
Current timestep = 1486. State = [[-0.25239098 -0.07801943]]. Action = [[-0.0014848   0.07111961  0.         -0.8334789 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1487. State = [[-0.2553753  -0.07873936]]. Action = [[-0.02859082 -0.05617578  0.         -0.5886624 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1488. State = [[-0.26189288 -0.08210694]]. Action = [[-0.0954387  -0.03096884  0.         -0.5460791 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1489. State = [[-0.2646835  -0.07919642]]. Action = [[ 0.01963351  0.0841948   0.         -0.30236   ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1490. State = [[-0.268955   -0.07507633]]. Action = [[-0.07040227  0.02984572  0.         -0.70506966]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1491. State = [[-0.27664292 -0.0763253 ]]. Action = [[-0.09433573 -0.05057153  0.         -0.03277051]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1492. State = [[-0.2777423  -0.08114894]]. Action = [[ 0.06118575 -0.07269202  0.         -0.43854052]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1493. State = [[-0.27326325 -0.07878905]]. Action = [[ 0.07907815  0.08953076  0.         -0.41745186]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1494. State = [[-0.27634597 -0.07228971]]. Action = [[-0.09920449  0.07401615  0.         -0.2700991 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1495. State = [[-0.27741018 -0.06577144]]. Action = [[0.067401   0.06819949 0.         0.7055862 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1496. State = [[-0.27347165 -0.06091855]]. Action = [[ 0.07583823  0.02932357  0.         -0.4934023 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1497. State = [[-0.27279738 -0.0616243 ]]. Action = [[-0.00811058 -0.05867286  0.         -0.8055042 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1498. State = [[-0.27746883 -0.06200361]]. Action = [[-0.08357412  0.01106107  0.          0.14664567]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1499. State = [[-0.28228432 -0.05825321]]. Action = [[-0.04118193  0.05885751  0.         -0.23560941]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1500. State = [[-0.2844008  -0.05874954]]. Action = [[-0.00569813 -0.05964631  0.         -0.62484527]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1501. State = [[-0.28150088 -0.05702863]]. Action = [[ 0.07452891  0.05414707  0.         -0.7705256 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1502. State = [[-0.28349674 -0.05497533]]. Action = [[-0.0769044  -0.00309323  0.         -0.01822728]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1503. State = [[-0.28666833 -0.05454555]]. Action = [[-0.01312912 -0.00224477  0.          0.655386  ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1504. State = [[-0.28734884 -0.05002709]]. Action = [[0.00925561 0.08058836 0.         0.23173463]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1505. State = [[-0.28852946 -0.04873215]]. Action = [[-0.013942   -0.03603192  0.          0.6267941 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1506. State = [[-0.2931511  -0.05330867]]. Action = [[-0.07960703 -0.08182622  0.         -0.40618837]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1507. State = [[-0.29652715 -0.05551049]]. Action = [[-0.01999862  0.00500128  0.          0.06579971]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1508. State = [[-0.3018062  -0.06048911]]. Action = [[-0.09027922 -0.09622157  0.         -0.7519706 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1509. State = [[-0.30162865 -0.06059134]]. Action = [[ 0.06925809  0.07097902  0.         -0.9216045 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1510. State = [[-0.29597676 -0.05829113]]. Action = [[0.09251124 0.00586635 0.         0.03337741]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1511. State = [[-0.29451132 -0.05351124]]. Action = [[-0.01354481  0.08597838  0.         -0.3867718 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1512. State = [[-0.29508993 -0.05084408]]. Action = [[ 0.01004708 -0.00371146  0.         -0.29784268]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1513. State = [[-0.2931182  -0.04595945]]. Action = [[ 0.05245972  0.08788631  0.         -0.8314375 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1514. State = [[-0.29087478 -0.04512901]]. Action = [[ 0.0331929  -0.04974591  0.         -0.82347226]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1515. State = [[-0.29202458 -0.04644307]]. Action = [[-0.03663681 -0.01007302  0.         -0.7691358 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1516. State = [[-0.29698512 -0.04329739]]. Action = [[-0.07972001  0.06448295  0.         -0.89794374]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1517. State = [[-0.2992867 -0.042725 ]]. Action = [[ 0.00388579 -0.03514022  0.          0.81179225]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1518. State = [[-0.30204782 -0.04535219]]. Action = [[-0.05567006 -0.04166453  0.         -0.7970403 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1519. State = [[-0.30428904 -0.04286348]]. Action = [[-0.0131343   0.07120473  0.         -0.8730808 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1520. State = [[-0.30328324 -0.0394036 ]]. Action = [[0.03823418 0.01816893 0.         0.00119543]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1521. State = [[-0.30452055 -0.03903906]]. Action = [[-0.03816923 -0.0176454   0.         -0.42132598]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1522. State = [[-0.30216822 -0.03460768]]. Action = [[0.08017311 0.08755357 0.         0.05910873]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1523. State = [[-0.29717433 -0.0279533 ]]. Action = [[0.07616738 0.06307509 0.         0.12899208]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 1524. State = [[-0.2926003  -0.02432335]]. Action = [[ 0.06454217  0.01105626  0.         -0.87272865]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1525. State = [[-0.29093146 -0.0181901 ]]. Action = [[ 0.00664172  0.09361453  0.         -0.90523285]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1526. State = [[-0.28869724 -0.01032615]]. Action = [[ 0.05363715  0.07814912  0.         -0.5921285 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1527. State = [[-0.2868688  -0.00236852]]. Action = [[ 0.02055822  0.08136161  0.         -0.8816242 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1528. State = [[-0.28252724 -0.00093328]]. Action = [[ 0.08374164 -0.05484043  0.          0.09698594]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1529. State = [[-0.28303435  0.00267468]]. Action = [[-0.06942946  0.07036174  0.         -0.7146828 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1530. State = [[-0.2840728   0.00153422]]. Action = [[ 0.01183518 -0.09538969  0.         -0.25955397]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1531. State = [[-0.28005072 -0.00163923]]. Action = [[ 0.06270545 -0.0359964   0.         -0.6298454 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1532. State = [[-0.28123257 -0.0013014 ]]. Action = [[-0.08612017  0.01339924  0.         -0.46543682]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1533. State = [[-0.2832318   0.00148679]]. Action = [[-0.0092588   0.03456566  0.         -0.3389811 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1534. State = [[-0.28586534  0.00351496]]. Action = [[-0.05496929  0.00628366  0.         -0.95188713]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1535. State = [[-0.2898116   0.00398181]]. Action = [[-0.05765033 -0.00805624  0.         -0.56206346]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1536. State = [[-0.289728    0.00212312]]. Action = [[ 0.03078809 -0.04204764  0.         -0.8115002 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1537. State = [[-0.2921244   0.00390244]]. Action = [[-0.06779827  0.05619053  0.          0.5451282 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1538. State = [[-0.29870078  0.00656859]]. Action = [[-0.09271366  0.01501857  0.         -0.35671854]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1539. State = [[-0.30628875  0.00603381]]. Action = [[-0.0920426  -0.03021375  0.          0.26402497]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1540. State = [[-0.30771756  0.00809538]]. Action = [[ 0.04905026  0.05131935  0.         -0.85885286]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1541. State = [[-0.30563474  0.013878  ]]. Action = [[0.04619978 0.07747991 0.         0.42990136]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1542. State = [[-0.30121592  0.01317729]]. Action = [[ 0.09437884 -0.07147484  0.          0.79860854]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1543. State = [[-0.29960284  0.01568774]]. Action = [[ 3.535971e-04  8.627727e-02  0.000000e+00 -8.420012e-01]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1544. State = [[-0.30305174  0.0227237 ]]. Action = [[-0.05127773  0.08487553  0.          0.01145029]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1545. State = [[-0.30339846  0.02335455]]. Action = [[ 0.04331636 -0.05386633  0.         -0.5754731 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1546. State = [[-0.30096695  0.02138112]]. Action = [[ 0.03789184 -0.02252293  0.         -0.69879466]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1547. State = [[-0.30143622  0.02105543]]. Action = [[-0.02916824 -0.00015496  0.         -0.01255256]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1548. State = [[-0.29790974  0.02324988]]. Action = [[ 0.09085194  0.0385183   0.         -0.04643619]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1549. State = [[-0.29776207  0.02121552]]. Action = [[-0.05850717 -0.06899383  0.          0.3339398 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1550. State = [[-0.30059558  0.01634669]]. Action = [[-0.03919547 -0.06183019  0.         -0.07829368]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1551. State = [[-0.29807183  0.01058133]]. Action = [[ 0.06284099 -0.07277145  0.          0.5000489 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1552. State = [[-0.29648766  0.00957172]]. Action = [[-0.01780739  0.03758819  0.         -0.8418845 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1553. State = [[-0.29429674  0.01358615]]. Action = [[0.04521038 0.07324832 0.         0.05120242]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1554. State = [[-0.29548937  0.01072495]]. Action = [[-0.0577051  -0.09702787  0.         -0.21011847]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1555. State = [[-0.2943202   0.01011073]]. Action = [[ 0.04922204  0.05403294  0.         -0.65044796]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1556. State = [[-0.29042554  0.01315657]]. Action = [[0.053806   0.04100548 0.         0.6516051 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1557. State = [[-0.292668    0.01071793]]. Action = [[-0.08482218 -0.06994464  0.         -0.8150141 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 1558. State = [[-0.29132855  0.00646718]]. Action = [[ 0.07521207 -0.03969733  0.         -0.44696945]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1559. State = [[-0.28622967  0.00792594]]. Action = [[ 0.05910491  0.06491684  0.         -0.4526962 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1560. State = [[-0.28025457  0.00780559]]. Action = [[ 0.08287541 -0.03104655  0.         -0.7746923 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1561. State = [[-0.27764022  0.00781953]]. Action = [[-0.00594199  0.02516455  0.          0.20891786]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1562. State = [[-0.27986974  0.00428939]]. Action = [[-0.06141827 -0.07883508  0.         -0.219948  ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1563. State = [[-0.27828127 -0.00281466]]. Action = [[ 0.04426449 -0.08887958  0.          0.2837968 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1564. State = [[-0.27972564 -0.00939805]]. Action = [[-0.08386627 -0.06233699  0.         -0.6670883 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1565. State = [[-0.27814013 -0.01500776]]. Action = [[ 0.05587929 -0.05289934  0.         -0.74295056]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1566. State = [[-0.2760261  -0.01894616]]. Action = [[-0.0116903  -0.01947887  0.         -0.02807897]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 1567. State = [[-0.2771427  -0.01974769]]. Action = [[-0.03970634  0.02437848  0.          0.43375492]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 1568. State = [[-0.2775931  -0.02183177]]. Action = [[-3.5883486e-04 -3.1174682e-02  0.0000000e+00 -6.7636305e-01]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1569. State = [[-0.27422395 -0.02618038]]. Action = [[ 0.0615479  -0.04582976  0.          0.34745896]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1570. State = [[-0.27325943 -0.03171659]]. Action = [[-0.0241155  -0.05784726  0.         -0.8667907 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1571. State = [[-0.27043378 -0.03868657]]. Action = [[ 0.05924504 -0.0763516   0.         -0.15415764]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1572. State = [[-0.2708202  -0.04628924]]. Action = [[-0.05547191 -0.07375775  0.         -0.26007605]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1573. State = [[-0.27531105 -0.04572513]]. Action = [[-0.07307055  0.09149792  0.         -0.5913486 ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1574. State = [[-0.27412713 -0.04335898]]. Action = [[0.07809163 0.01946723 0.         0.03755224]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1575. State = [[-0.27327827 -0.04377627]]. Action = [[-0.02323012 -0.00591803  0.          0.23650897]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1576. State = [[-0.27848542 -0.04013933]]. Action = [[-0.09531068  0.08861544  0.         -0.96945715]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1577. State = [[-0.28523648 -0.0394992 ]]. Action = [[-0.07575732 -0.03534362  0.         -0.28228617]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 1578. State = [[-0.28784752 -0.04454652]]. Action = [[ 0.00354181 -0.08211058  0.         -0.3965038 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1579. State = [[-0.28416428 -0.04367012]]. Action = [[ 0.09577609  0.07039886  0.         -0.84295374]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1580. State = [[-0.28513053 -0.04384286]]. Action = [[-0.05709425 -0.0470788   0.         -0.1723063 ]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1581. State = [[-0.29192975 -0.04891079]]. Action = [[-0.0961678  -0.07561974  0.         -0.7028887 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1582. State = [[-0.29889342 -0.05197706]]. Action = [[-0.07483822 -0.00835744  0.         -0.31763077]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 1583. State = [[-0.3061919  -0.04902012]]. Action = [[-0.08493127  0.07494118  0.         -0.04673046]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1584. State = [[-0.3130934 -0.0499801]]. Action = [[-0.06244012 -0.0600528   0.          0.4104793 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 1585. State = [[-0.320737   -0.05239047]]. Action = [[-0.08779948 -0.01120935  0.         -0.6042731 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 1586. State = [[-0.32356545 -0.04999503]]. Action = [[ 0.03410957  0.05866916  0.         -0.9294712 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 1587. State = [[-0.32743213 -0.04950881]]. Action = [[-0.05146357 -0.02909555  0.         -0.81468344]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 1588. State = [[-0.33457547 -0.04580877]]. Action = [[-0.07330117  0.08648028  0.         -0.78909004]]. Reward = [0.]
Curr episode timestep = 102
Current timestep = 1589. State = [[-0.34129557 -0.04475509]]. Action = [[-0.04509416 -0.03578644  0.         -0.6202811 ]]. Reward = [0.]
Curr episode timestep = 103
Current timestep = 1590. State = [[-0.34819815 -0.05010489]]. Action = [[-0.06704682 -0.09300647  0.          0.58761716]]. Reward = [0.]
Curr episode timestep = 104
Current timestep = 1591. State = [[-0.3530374  -0.04929511]]. Action = [[-0.00918768  0.07945526  0.         -0.09600019]]. Reward = [0.]
Curr episode timestep = 105
Current timestep = 1592. State = [[-0.35849378 -0.04827321]]. Action = [[-0.05527245 -0.02874704  0.         -0.54686236]]. Reward = [0.]
Curr episode timestep = 106
Current timestep = 1593. State = [[-0.36693478 -0.04584252]]. Action = [[-0.09338867  0.05937863  0.         -0.2303251 ]]. Reward = [0.]
Curr episode timestep = 107
Current timestep = 1594. State = [[-0.3705846  -0.04607675]]. Action = [[ 0.03656063 -0.04687667  0.         -0.4388423 ]]. Reward = [0.]
Curr episode timestep = 108
Current timestep = 1595. State = [[-0.37285045 -0.04887149]]. Action = [[-0.02272673 -0.03500976  0.          0.31046677]]. Reward = [0.]
Curr episode timestep = 109
Current timestep = 1596. State = [[-0.3764238  -0.04982166]]. Action = [[-0.02577835  0.00357028  0.         -0.75429326]]. Reward = [0.]
Curr episode timestep = 110
Current timestep = 1597. State = [[-0.3756183  -0.04507289]]. Action = [[ 0.06900217  0.09048409  0.         -0.5105627 ]]. Reward = [0.]
Curr episode timestep = 111
Current timestep = 1598. State = [[-0.37555164 -0.04163192]]. Action = [[ 0.          0.          0.         -0.36934125]]. Reward = [0.]
Curr episode timestep = 112
Current timestep = 1599. State = [[-0.37251684 -0.04483879]]. Action = [[ 0.09210318 -0.07920663  0.         -0.5043645 ]]. Reward = [0.]
Curr episode timestep = 113
Current timestep = 1600. State = [[-0.3707371  -0.04694582]]. Action = [[ 0.          0.          0.         -0.38563222]]. Reward = [0.]
Curr episode timestep = 114
Current timestep = 1601. State = [[-0.36911762 -0.04865241]]. Action = [[ 0.04225106 -0.03144604  0.         -0.6641774 ]]. Reward = [0.]
Curr episode timestep = 115
Current timestep = 1602. State = [[-0.36893487 -0.05046689]]. Action = [[-0.01704824 -0.01293785  0.         -0.16836905]]. Reward = [0.]
Curr episode timestep = 116
Current timestep = 1603. State = [[-0.36459306 -0.0515076 ]]. Action = [[ 0.09722698 -0.00564251  0.          0.3399446 ]]. Reward = [0.]
Curr episode timestep = 117
Current timestep = 1604. State = [[-0.3627048  -0.04960776]]. Action = [[-0.02763394  0.04761875  0.          0.26053762]]. Reward = [0.]
Curr episode timestep = 118
Current timestep = 1605. State = [[-0.36440036 -0.04775681]]. Action = [[-0.02563754  0.01469143  0.         -0.35810226]]. Reward = [0.]
Curr episode timestep = 119
Current timestep = 1606. State = [[-0.36368757 -0.04300961]]. Action = [[ 0.03018472  0.08522331  0.         -0.57925755]]. Reward = [0.]
Curr episode timestep = 120
Current timestep = 1607. State = [[-0.36784816 -0.03740451]]. Action = [[-0.09938984  0.05394711  0.         -0.33720922]]. Reward = [0.]
Curr episode timestep = 121
Current timestep = 1608. State = [[-0.37440535 -0.03916602]]. Action = [[-0.07302873 -0.07759796  0.          0.37822986]]. Reward = [0.]
Curr episode timestep = 122
Current timestep = 1609. State = [[-0.37690124 -0.04124412]]. Action = [[ 0.         0.         0.        -0.6685914]]. Reward = [0.]
Curr episode timestep = 123
Current timestep = 1610. State = [[-0.3774165  -0.04105311]]. Action = [[ 0.         0.         0.        -0.6372591]]. Reward = [0.]
Curr episode timestep = 124
Current timestep = 1611. State = [[-0.37663737 -0.03633852]]. Action = [[ 0.02784099  0.08361579  0.         -0.5453124 ]]. Reward = [0.]
Curr episode timestep = 125
Current timestep = 1612. State = [[-0.3736886  -0.02818261]]. Action = [[ 0.06519022  0.09146995  0.         -0.64137506]]. Reward = [0.]
Curr episode timestep = 126
Current timestep = 1613. State = [[-0.37352538 -0.0257468 ]]. Action = [[-0.01265133 -0.03751103  0.         -0.52040356]]. Reward = [0.]
Curr episode timestep = 127
Current timestep = 1614. State = [[-0.37532663 -0.02593136]]. Action = [[-0.01621141 -0.00228808  0.         -0.6818179 ]]. Reward = [0.]
Curr episode timestep = 128
Current timestep = 1615. State = [[-0.3763406 -0.025381 ]]. Action = [[ 0.          0.          0.         -0.08509254]]. Reward = [0.]
Curr episode timestep = 129
Current timestep = 1616. State = [[-0.37475845 -0.02099444]]. Action = [[0.04417267 0.07271297 0.         0.0353992 ]]. Reward = [0.]
Curr episode timestep = 130
Current timestep = 1617. State = [[-0.3716728  -0.01345197]]. Action = [[ 0.0549472   0.08556915  0.         -0.6879628 ]]. Reward = [0.]
Curr episode timestep = 131
Current timestep = 1618. State = [[-0.37090933 -0.00953937]]. Action = [[ 0.          0.          0.         -0.64151776]]. Reward = [0.]
Curr episode timestep = 132
Current timestep = 1619. State = [[-0.3712583 -0.0082855]]. Action = [[0.         0.         0.         0.40474868]]. Reward = [0.]
Curr episode timestep = 133
Current timestep = 1620. State = [[-0.36789238 -0.00869046]]. Action = [[ 0.07041078 -0.02831696  0.         -0.8073086 ]]. Reward = [0.]
Curr episode timestep = 134
Current timestep = 1621. State = [[-0.3628852  -0.00439315]]. Action = [[ 0.05980764  0.08439136  0.         -0.17255116]]. Reward = [0.]
Curr episode timestep = 135
Current timestep = 1622. State = [[-0.35921422  0.00365706]]. Action = [[ 0.04038992  0.09157848  0.         -0.7409115 ]]. Reward = [0.]
Curr episode timestep = 136
Current timestep = 1623. State = [[-0.35777804  0.00833022]]. Action = [[ 0.00594804  0.01347752  0.         -0.81115603]]. Reward = [0.]
Curr episode timestep = 137
Current timestep = 1624. State = [[-0.35850674  0.00510913]]. Action = [[-0.03123219 -0.09894244  0.         -0.61486685]]. Reward = [0.]
Curr episode timestep = 138
Current timestep = 1625. State = [[-0.35899872  0.0037889 ]]. Action = [[-0.01673709  0.01477722  0.          0.06808817]]. Reward = [0.]
Curr episode timestep = 139
Current timestep = 1626. State = [[-0.35931495  0.00311486]]. Action = [[-0.02127291 -0.03287693  0.         -0.84773064]]. Reward = [0.]
Curr episode timestep = 140
Current timestep = 1627. State = [[-0.358539    0.00216357]]. Action = [[ 0.00227796 -0.00917168  0.          0.5276669 ]]. Reward = [0.]
Curr episode timestep = 141
Current timestep = 1628. State = [[-0.35577172 -0.00226745]]. Action = [[ 0.02974122 -0.08532384  0.         -0.757484  ]]. Reward = [0.]
Curr episode timestep = 142
Current timestep = 1629. State = [[-0.35441056 -0.00547115]]. Action = [[-0.0179108  -0.00770316  0.         -0.00438464]]. Reward = [0.]
Curr episode timestep = 143
Current timestep = 1630. State = [[-0.35754773 -0.00829245]]. Action = [[-0.082884   -0.0384826   0.          0.04994214]]. Reward = [0.]
Curr episode timestep = 144
Current timestep = 1631. State = [[-0.35518962 -0.01227149]]. Action = [[ 0.08098231 -0.04158579  0.         -0.09086525]]. Reward = [0.]
Curr episode timestep = 145
Current timestep = 1632. State = [[-0.35483944 -0.01590332]]. Action = [[-0.06633583 -0.02637529  0.         -0.3386382 ]]. Reward = [0.]
Curr episode timestep = 146
Current timestep = 1633. State = [[-0.3517851  -0.01435317]]. Action = [[ 0.0864618   0.06736682  0.         -0.8658402 ]]. Reward = [0.]
Curr episode timestep = 147
Current timestep = 1634. State = [[-0.35271046 -0.00829838]]. Action = [[-0.0785218   0.09315831  0.         -0.29897475]]. Reward = [0.]
Curr episode timestep = 148
Current timestep = 1635. State = [[-0.3508489  -0.00831523]]. Action = [[ 0.0944056  -0.05655188  0.         -0.4027865 ]]. Reward = [0.]
Curr episode timestep = 149
Current timestep = 1636. State = [[-0.35069275 -0.01396232]]. Action = [[-0.05928292 -0.07723536  0.          0.35667586]]. Reward = [0.]
Curr episode timestep = 150
Current timestep = 1637. State = [[-0.34979624 -0.01314566]]. Action = [[0.04775351 0.07604519 0.         0.16828775]]. Reward = [0.]
Curr episode timestep = 151
Current timestep = 1638. State = [[-0.3525042  -0.00838957]]. Action = [[-0.07686291  0.05284403  0.          0.12026954]]. Reward = [0.]
Curr episode timestep = 152
Current timestep = 1639. State = [[-0.35059997 -0.01010243]]. Action = [[ 0.09971196 -0.07341529  0.          0.00225914]]. Reward = [0.]
Curr episode timestep = 153
Current timestep = 1640. State = [[-0.3455288 -0.0106836]]. Action = [[0.04657937 0.0286346  0.         0.23552191]]. Reward = [0.]
Curr episode timestep = 154
Current timestep = 1641. State = [[-0.34047413 -0.00892488]]. Action = [[ 0.06790961  0.0185446   0.         -0.35053992]]. Reward = [0.]
Curr episode timestep = 155
Current timestep = 1642. State = [[-0.33479205 -0.00511929]]. Action = [[ 0.07038677  0.06034348  0.         -0.6750494 ]]. Reward = [0.]
Curr episode timestep = 156
Current timestep = 1643. State = [[-0.32878023 -0.00052961]]. Action = [[ 0.07679103  0.04813737  0.         -0.78884315]]. Reward = [0.]
Curr episode timestep = 157
Current timestep = 1644. State = [[-0.3247824   0.00047892]]. Action = [[ 0.02645583 -0.01962567  0.         -0.6731316 ]]. Reward = [0.]
Curr episode timestep = 158
Current timestep = 1645. State = [[-0.3181909  -0.00068949]]. Action = [[ 0.09725823 -0.02251901  0.         -0.5173993 ]]. Reward = [0.]
Curr episode timestep = 159
Current timestep = 1646. State = [[-3.1650937e-01  2.5728237e-04]]. Action = [[-0.05476753  0.02571318  0.         -0.76056767]]. Reward = [0.]
Curr episode timestep = 160
Current timestep = 1647. State = [[-3.1435263e-01 -9.6629083e-05]]. Action = [[ 0.04744483 -0.02979723  0.          0.04181588]]. Reward = [0.]
Curr episode timestep = 161
Current timestep = 1648. State = [[-0.3116603  0.0028205]]. Action = [[ 0.00464852  0.06771059  0.         -0.75036657]]. Reward = [0.]
Curr episode timestep = 162
Current timestep = 1649. State = [[-0.3121398   0.00900951]]. Action = [[-0.03263299  0.07400622  0.         -0.7501515 ]]. Reward = [0.]
Curr episode timestep = 163
Current timestep = 1650. State = [[-0.31634903  0.01201526]]. Action = [[-0.08482873 -0.0029332   0.         -0.60618174]]. Reward = [0.]
Curr episode timestep = 164
Current timestep = 1651. State = [[-0.3167766  0.0156086]]. Action = [[ 0.02811275  0.04945102  0.         -0.8089827 ]]. Reward = [0.]
Curr episode timestep = 165
Current timestep = 1652. State = [[-0.3114074   0.02127929]]. Action = [[ 0.09440542  0.05995699  0.         -0.916399  ]]. Reward = [0.]
Curr episode timestep = 166
Current timestep = 1653. State = [[-0.31240585  0.02430078]]. Action = [[-0.08426337 -0.00207608  0.         -0.76492745]]. Reward = [0.]
Curr episode timestep = 167
Current timestep = 1654. State = [[-0.3171237   0.02847229]]. Action = [[-0.04586307  0.05601677  0.          0.05936587]]. Reward = [0.]
Curr episode timestep = 168
Current timestep = 1655. State = [[-0.3200828  0.0291224]]. Action = [[-0.02182174 -0.04809841  0.         -0.6430491 ]]. Reward = [0.]
Curr episode timestep = 169
Current timestep = 1656. State = [[-0.3198618   0.02663999]]. Action = [[ 0.02365525 -0.04645517  0.         -0.6185206 ]]. Reward = [0.]
Curr episode timestep = 170
Current timestep = 1657. State = [[-0.3197026   0.02159452]]. Action = [[-0.01124106 -0.086383    0.         -0.49971724]]. Reward = [0.]
Curr episode timestep = 171
Current timestep = 1658. State = [[-0.3176127   0.01769436]]. Action = [[ 0.04057736 -0.02940457  0.         -0.27127254]]. Reward = [0.]
Curr episode timestep = 172
Current timestep = 1659. State = [[-0.3119993   0.02106193]]. Action = [[0.09020967 0.0938772  0.         0.35685933]]. Reward = [0.]
Curr episode timestep = 173
Current timestep = 1660. State = [[-0.30587387  0.01888222]]. Action = [[ 0.07217912 -0.0951012   0.         -0.719156  ]]. Reward = [0.]
Curr episode timestep = 174
Current timestep = 1661. State = [[-0.30155247  0.01900495]]. Action = [[ 0.03477461  0.06963415  0.         -0.8167366 ]]. Reward = [0.]
Curr episode timestep = 175
Current timestep = 1662. State = [[-0.30236816  0.02199953]]. Action = [[-0.04764004  0.03382666  0.         -0.9090018 ]]. Reward = [0.]
Curr episode timestep = 176
Current timestep = 1663. State = [[-0.29879838  0.01842735]]. Action = [[ 0.08985155 -0.08563455  0.         -0.44749582]]. Reward = [0.]
Curr episode timestep = 177
Current timestep = 1664. State = [[-0.29320598  0.01386789]]. Action = [[ 0.04270997 -0.02996545  0.         -0.8418878 ]]. Reward = [0.]
Curr episode timestep = 178
Current timestep = 1665. State = [[-0.29054067  0.01688601]]. Action = [[ 0.0049666   0.09458854  0.         -0.7729504 ]]. Reward = [0.]
Curr episode timestep = 179
Current timestep = 1666. State = [[-0.28839275  0.02084931]]. Action = [[ 0.02512357  0.03270974  0.         -0.7045094 ]]. Reward = [0.]
Curr episode timestep = 180
Current timestep = 1667. State = [[-0.28302595  0.0189654 ]]. Action = [[ 0.0792169  -0.05640632  0.         -0.5203648 ]]. Reward = [0.]
Curr episode timestep = 181
Current timestep = 1668. State = [[-0.2814687   0.01979156]]. Action = [[-0.03690573  0.05253118  0.         -0.70093966]]. Reward = [0.]
Curr episode timestep = 182
Current timestep = 1669. State = [[-0.28444225  0.01923455]]. Action = [[-0.06452069 -0.04213261  0.         -0.9637791 ]]. Reward = [0.]
Curr episode timestep = 183
Current timestep = 1670. State = [[-0.28845915  0.01453686]]. Action = [[-0.07187215 -0.07552563  0.         -0.64485073]]. Reward = [0.]
Curr episode timestep = 184
Current timestep = 1671. State = [[-0.2864529   0.01078051]]. Action = [[ 0.06382909 -0.02919034  0.         -0.6642051 ]]. Reward = [0.]
Curr episode timestep = 185
Current timestep = 1672. State = [[-0.2863693   0.00643746]]. Action = [[-0.05957657 -0.06096404  0.         -0.32623416]]. Reward = [0.]
Curr episode timestep = 186
Current timestep = 1673. State = [[-0.29089814  0.00177914]]. Action = [[-0.08772766 -0.04768595  0.         -0.76678663]]. Reward = [0.]
Curr episode timestep = 187
Current timestep = 1674. State = [[-0.2942196   0.00460037]]. Action = [[-0.03085604  0.09804738  0.         -0.57154   ]]. Reward = [0.]
Curr episode timestep = 188
Current timestep = 1675. State = [[-0.2963344   0.01137267]]. Action = [[-0.01548494  0.08290415  0.         -0.72050047]]. Reward = [0.]
Curr episode timestep = 189
Current timestep = 1676. State = [[-0.30198336  0.01755737]]. Action = [[-0.0840231   0.06580458  0.         -0.22722733]]. Reward = [0.]
Curr episode timestep = 190
Current timestep = 1677. State = [[-0.30235636  0.01863697]]. Action = [[ 0.07478554 -0.03239559  0.          0.28916872]]. Reward = [0.]
Curr episode timestep = 191
Current timestep = 1678. State = [[-0.29940748  0.01455168]]. Action = [[ 0.04470327 -0.07348426  0.         -0.8905079 ]]. Reward = [0.]
Curr episode timestep = 192
Current timestep = 1679. State = [[-0.2954398   0.00870181]]. Action = [[ 0.06455899 -0.07337461  0.         -0.385552  ]]. Reward = [0.]
Curr episode timestep = 193
Current timestep = 1680. State = [[-0.29124224  0.00594016]]. Action = [[ 0.0497591  -0.00203435  0.         -0.98370427]]. Reward = [0.]
Curr episode timestep = 194
Current timestep = 1681. State = [[-0.29414183  0.00749005]]. Action = [[-0.09612807  0.04518709  0.         -0.6377829 ]]. Reward = [0.]
Curr episode timestep = 195
Current timestep = 1682. State = [[-0.29801446  0.01205167]]. Action = [[-0.01660454  0.0712541   0.         -0.4545353 ]]. Reward = [0.]
Curr episode timestep = 196
Current timestep = 1683. State = [[-0.29783     0.01695279]]. Action = [[ 0.0381112   0.05486796  0.         -0.72870374]]. Reward = [0.]
Curr episode timestep = 197
Current timestep = 1684. State = [[-0.2982636   0.02238123]]. Action = [[-0.00394379  0.06851611  0.         -0.6702828 ]]. Reward = [0.]
Curr episode timestep = 198
Current timestep = 1685. State = [[-0.2993871   0.02117948]]. Action = [[ 0.00025542 -0.07727094  0.         -0.10672736]]. Reward = [0.]
Curr episode timestep = 199
Current timestep = 1686. State = [[-0.30418575  0.02161252]]. Action = [[-0.08721206  0.04109684  0.         -0.71789074]]. Reward = [0.]
Curr episode timestep = 200
Current timestep = 1687. State = [[-0.30977124  0.02628171]]. Action = [[-0.04997636  0.05774745  0.         -0.98205024]]. Reward = [0.]
Curr episode timestep = 201
Current timestep = 1688. State = [[-0.3172585   0.03368806]]. Action = [[-0.09797231  0.09428281  0.         -0.3865577 ]]. Reward = [0.]
Curr episode timestep = 202
Current timestep = 1689. State = [[-0.3225411  0.0342325]]. Action = [[-0.02285348 -0.0734404   0.         -0.5496073 ]]. Reward = [0.]
Curr episode timestep = 203
Current timestep = 1690. State = [[-0.32624692  0.03454851]]. Action = [[-0.03548001  0.02442626  0.         -0.21428984]]. Reward = [0.]
Curr episode timestep = 204
Current timestep = 1691. State = [[-0.33224297  0.03877406]]. Action = [[-0.07209797  0.05238267  0.         -0.5432062 ]]. Reward = [0.]
Curr episode timestep = 205
Current timestep = 1692. State = [[-0.33771497  0.0458013 ]]. Action = [[-0.03139828  0.08784737  0.         -0.17937177]]. Reward = [0.]
Curr episode timestep = 206
Current timestep = 1693. State = [[-0.34220174  0.05041325]]. Action = [[-0.02663602  0.01539379  0.          0.15412986]]. Reward = [0.]
Curr episode timestep = 207
Current timestep = 1694. State = [[-0.34864214  0.05504316]]. Action = [[-0.07110807  0.05628889  0.         -0.09461403]]. Reward = [0.]
Curr episode timestep = 208
Current timestep = 1695. State = [[-0.34931558  0.05687803]]. Action = [[ 0.07616735 -0.02028839  0.          0.31284285]]. Reward = [0.]
Curr episode timestep = 209
Current timestep = 1696. State = [[-0.34535688  0.05416176]]. Action = [[ 0.07665361 -0.0595452   0.          0.07268643]]. Reward = [0.]
Curr episode timestep = 210
Current timestep = 1697. State = [[-0.3407022   0.05574584]]. Action = [[ 0.07250089  0.06208663  0.         -0.3592435 ]]. Reward = [0.]
Curr episode timestep = 211
Current timestep = 1698. State = [[-0.3353935   0.05770715]]. Action = [[ 0.08049544  0.0022267   0.         -0.8252313 ]]. Reward = [0.]
Curr episode timestep = 212
Current timestep = 1699. State = [[-0.333048    0.05374967]]. Action = [[-0.00244153 -0.08517607  0.          0.5061765 ]]. Reward = [0.]
Curr episode timestep = 213
Current timestep = 1700. State = [[-0.33403844  0.04797851]]. Action = [[-0.03936709 -0.06861923  0.          0.20176315]]. Reward = [0.]
Curr episode timestep = 214
Current timestep = 1701. State = [[-0.33621553  0.04816071]]. Action = [[-0.04499582  0.04867343  0.         -0.44373554]]. Reward = [0.]
Curr episode timestep = 215
Current timestep = 1702. State = [[-0.33432612  0.04726573]]. Action = [[ 0.05095942 -0.0392141   0.         -0.5438709 ]]. Reward = [0.]
Curr episode timestep = 216
Current timestep = 1703. State = [[-0.33260342  0.05021316]]. Action = [[-0.00284532  0.09027795  0.         -0.16024542]]. Reward = [0.]
Curr episode timestep = 217
Current timestep = 1704. State = [[-0.33583084  0.05664928]]. Action = [[-0.06588805  0.08011294  0.         -0.04396552]]. Reward = [0.]
Curr episode timestep = 218
Current timestep = 1705. State = [[-0.33770764  0.05715688]]. Action = [[ 0.00335406 -0.04637869  0.         -0.50647527]]. Reward = [0.]
Curr episode timestep = 219
Current timestep = 1706. State = [[-0.33750758  0.05551328]]. Action = [[ 0.00501965 -0.01461551  0.          0.38883448]]. Reward = [0.]
Curr episode timestep = 220
Current timestep = 1707. State = [[-0.33709767  0.058845  ]]. Action = [[ 0.00973818  0.07201593  0.         -0.53719884]]. Reward = [0.]
Curr episode timestep = 221
Current timestep = 1708. State = [[-0.33978224  0.06008314]]. Action = [[-0.05280012 -0.02553953  0.         -0.6922512 ]]. Reward = [0.]
Curr episode timestep = 222
Current timestep = 1709. State = [[-0.34004626  0.06423021]]. Action = [[0.03486722 0.08539135 0.         0.56461716]]. Reward = [0.]
Curr episode timestep = 223
Current timestep = 1710. State = [[-0.3413896   0.06357661]]. Action = [[-0.03627546 -0.07589702  0.         -0.83322513]]. Reward = [0.]
Curr episode timestep = 224
Current timestep = 1711. State = [[-0.34428734  0.05844036]]. Action = [[-0.04043506 -0.07385001  0.         -0.51632893]]. Reward = [0.]
Curr episode timestep = 225
Current timestep = 1712. State = [[-0.34478343  0.05406844]]. Action = [[ 0.00813943 -0.04816669  0.         -0.7001749 ]]. Reward = [0.]
Curr episode timestep = 226
Current timestep = 1713. State = [[-0.34822306  0.05629777]]. Action = [[-0.07501581  0.07691465  0.          0.4720583 ]]. Reward = [0.]
Curr episode timestep = 227
Current timestep = 1714. State = [[-0.35524952  0.05436269]]. Action = [[-0.09719171 -0.08961635  0.         -0.46861255]]. Reward = [0.]
Curr episode timestep = 228
Current timestep = 1715. State = [[-0.35811862  0.04736092]]. Action = [[ 0.00047569 -0.09155124  0.         -0.2116648 ]]. Reward = [0.]
Curr episode timestep = 229
Current timestep = 1716. State = [[-0.3591139   0.04055338]]. Action = [[-0.0161052  -0.06893545  0.         -0.23295897]]. Reward = [0.]
Curr episode timestep = 230
Current timestep = 1717. State = [[-0.3591469  0.0411927]]. Action = [[ 0.0183598   0.07730935  0.         -0.15718198]]. Reward = [0.]
Curr episode timestep = 231
Current timestep = 1718. State = [[-0.3593493   0.04288903]]. Action = [[ 0.00859545  0.00871581  0.         -0.30224383]]. Reward = [0.]
Curr episode timestep = 232
Current timestep = 1719. State = [[-0.36149746  0.04273135]]. Action = [[-0.02524458  0.00405465  0.         -0.7103222 ]]. Reward = [0.]
Curr episode timestep = 233
Current timestep = 1720. State = [[-0.36737245  0.0390615 ]]. Action = [[-0.08564395 -0.06619486  0.         -0.7936083 ]]. Reward = [0.]
Curr episode timestep = 234
Current timestep = 1721. State = [[-0.37427095  0.04104491]]. Action = [[-0.06774304  0.08911417  0.         -0.6922697 ]]. Reward = [0.]
Curr episode timestep = 235
Current timestep = 1722. State = [[-0.38001817  0.04784301]]. Action = [[-0.03377371  0.0884157   0.         -0.7307304 ]]. Reward = [0.]
Curr episode timestep = 236
Current timestep = 1723. State = [[-0.3816479   0.05348703]]. Action = [[0.04465561 0.05403178 0.         0.41362643]]. Reward = [0.]
Curr episode timestep = 237
Current timestep = 1724. State = [[-0.3813883   0.05216582]]. Action = [[ 0.03484706 -0.06695084  0.          0.6803751 ]]. Reward = [0.]
Curr episode timestep = 238
Current timestep = 1725. State = [[-0.37830424  0.05401802]]. Action = [[ 0.08559079  0.07495687  0.         -0.21178609]]. Reward = [0.]
Curr episode timestep = 239
Current timestep = 1726. State = [[-0.3731157   0.05703459]]. Action = [[ 0.09627392  0.01905147  0.         -0.70995593]]. Reward = [0.]
Curr episode timestep = 240
Current timestep = 1727. State = [[-0.37140483  0.05778138]]. Action = [[0.         0.         0.         0.43357956]]. Reward = [0.]
Curr episode timestep = 241
Current timestep = 1728. State = [[-0.3671824   0.06030411]]. Action = [[ 0.09718361  0.04618718  0.         -0.5162835 ]]. Reward = [0.]
Curr episode timestep = 242
Current timestep = 1729. State = [[-0.3657519   0.05763178]]. Action = [[-0.03234743 -0.0878972   0.         -0.05149102]]. Reward = [0.]
Curr episode timestep = 243
Current timestep = 1730. State = [[-0.3686456   0.05405698]]. Action = [[-0.05525095 -0.03158513  0.         -0.6610596 ]]. Reward = [0.]
Curr episode timestep = 244
Current timestep = 1731. State = [[-0.36916476  0.05229303]]. Action = [[ 0.00794852 -0.02083287  0.         -0.90907675]]. Reward = [0.]
Curr episode timestep = 245
Current timestep = 1732. State = [[-0.36840656  0.05166499]]. Action = [[ 0.         0.         0.        -0.6515471]]. Reward = [0.]
Curr episode timestep = 246
Current timestep = 1733. State = [[-0.3710399   0.05082271]]. Action = [[-0.06805551 -0.01618327  0.         -0.53942955]]. Reward = [0.]
Curr episode timestep = 247
Current timestep = 1734. State = [[-0.374862    0.05110054]]. Action = [[-0.05176505  0.01343932  0.         -0.9260501 ]]. Reward = [0.]
Curr episode timestep = 248
Current timestep = 1735. State = [[-0.37240198  0.05008892]]. Action = [[ 0.08064223 -0.02610268  0.         -0.928653  ]]. Reward = [0.]
Curr episode timestep = 249
Current timestep = 1736. State = [[-0.37025324  0.04571346]]. Action = [[-0.00917025 -0.06585751  0.         -0.91222614]]. Reward = [0.]
Curr episode timestep = 250
Current timestep = 1737. State = [[-0.36970395  0.04328893]]. Action = [[ 0.         0.         0.        -0.8345805]]. Reward = [0.]
Curr episode timestep = 251
Current timestep = 1738. State = [[-0.36618233  0.0435316 ]]. Action = [[ 0.06395171  0.01963955  0.         -0.7837227 ]]. Reward = [0.]
Curr episode timestep = 252
Current timestep = 1739. State = [[-0.36567166  0.04084105]]. Action = [[-0.03471237 -0.05228699  0.         -0.44907862]]. Reward = [0.]
Curr episode timestep = 253
Current timestep = 1740. State = [[-0.36807975  0.03698265]]. Action = [[-0.04310706 -0.03578867  0.          0.01144516]]. Reward = [0.]
Curr episode timestep = 254
Current timestep = 1741. State = [[-0.3700999   0.03423889]]. Action = [[-0.02560143 -0.01990421  0.          0.3907702 ]]. Reward = [0.]
Curr episode timestep = 255
Current timestep = 1742. State = [[-0.37170845  0.03371397]]. Action = [[-0.02009339  0.01583743  0.          0.09194791]]. Reward = [0.]
Curr episode timestep = 256
Current timestep = 1743. State = [[-0.37430438  0.03323767]]. Action = [[-0.03686947 -0.00686339  0.         -0.6227709 ]]. Reward = [0.]
Curr episode timestep = 257
Current timestep = 1744. State = [[-0.37414733  0.02903424]]. Action = [[ 0.02980309 -0.06987314  0.          0.01148605]]. Reward = [0.]
Curr episode timestep = 258
Current timestep = 1745. State = [[-0.37352097  0.02622555]]. Action = [[0.        0.        0.        0.7987907]]. Reward = [0.]
Curr episode timestep = 259
Current timestep = 1746. State = [[-0.37202996  0.02523282]]. Action = [[ 0.03554203 -0.0021164   0.          0.21034539]]. Reward = [0.]
Curr episode timestep = 260
Current timestep = 1747. State = [[-0.37138683  0.02441379]]. Action = [[0.         0.         0.         0.28341222]]. Reward = [0.]
Curr episode timestep = 261
Current timestep = 1748. State = [[-0.3710716   0.02270634]]. Action = [[ 0.01134314 -0.02050908  0.          0.13429892]]. Reward = [0.]
Curr episode timestep = 262
Current timestep = 1749. State = [[-0.3690226   0.01991406]]. Action = [[ 0.03871589 -0.02973465  0.         -0.44692862]]. Reward = [0.]
Curr episode timestep = 263
Current timestep = 1750. State = [[-0.3647988   0.01651916]]. Action = [[ 0.06367701 -0.03354086  0.          0.00517488]]. Reward = [0.]
Curr episode timestep = 264
Current timestep = 1751. State = [[-0.36242914  0.01684636]]. Action = [[ 0.0080725   0.04475049  0.         -0.5616341 ]]. Reward = [0.]
Curr episode timestep = 265
Current timestep = 1752. State = [[-0.3664314  0.0181675]]. Action = [[-0.09252269  0.01416941  0.         -0.35369962]]. Reward = [0.]
Curr episode timestep = 266
Current timestep = 1753. State = [[-0.3689308   0.01460302]]. Action = [[-0.00179669 -0.07330354  0.          0.90068924]]. Reward = [0.]
Curr episode timestep = 267
Current timestep = 1754. State = [[-0.36894473  0.01199068]]. Action = [[ 0.          0.          0.         -0.38074803]]. Reward = [0.]
Curr episode timestep = 268
Current timestep = 1755. State = [[-0.36527368  0.00876633]]. Action = [[ 0.07421906 -0.05100947  0.         -0.87678343]]. Reward = [0.]
Curr episode timestep = 269
Current timestep = 1756. State = [[-0.36101058  0.00558874]]. Action = [[ 0.04049755 -0.01879103  0.         -0.27341723]]. Reward = [0.]
Curr episode timestep = 270
Current timestep = 1757. State = [[-3.5711223e-01 -2.8057385e-04]]. Action = [[ 0.04334823 -0.0876012   0.         -0.20592105]]. Reward = [0.]
Curr episode timestep = 271
Current timestep = 1758. State = [[-0.3584706  -0.00428155]]. Action = [[-0.0763083  -0.00692591  0.         -0.05139738]]. Reward = [0.]
Curr episode timestep = 272
Current timestep = 1759. State = [[-0.36134872 -0.00722494]]. Action = [[-0.03353848 -0.0295536   0.         -0.8805631 ]]. Reward = [0.]
Curr episode timestep = 273
Current timestep = 1760. State = [[-0.36517635 -0.00798757]]. Action = [[-0.06647439  0.02471106  0.         -0.6523289 ]]. Reward = [0.]
Curr episode timestep = 274
Current timestep = 1761. State = [[-0.36572117 -0.0087132 ]]. Action = [[ 0.02895191 -0.0100832   0.          0.5100858 ]]. Reward = [0.]
Curr episode timestep = 275
Current timestep = 1762. State = [[-0.36180058 -0.00921558]]. Action = [[ 0.07375441  0.00950782  0.         -0.42327577]]. Reward = [0.]
Curr episode timestep = 276
Current timestep = 1763. State = [[-0.3598559  -0.01314384]]. Action = [[ 0.00429124 -0.0715874   0.         -0.8373938 ]]. Reward = [0.]
Curr episode timestep = 277
Current timestep = 1764. State = [[-0.36318997 -0.01817638]]. Action = [[-0.07699858 -0.04307882  0.         -0.9302562 ]]. Reward = [0.]
Curr episode timestep = 278
Current timestep = 1765. State = [[-0.36673036 -0.02089033]]. Action = [[-0.03227402 -0.00813507  0.         -0.63241816]]. Reward = [0.]
Curr episode timestep = 279
Current timestep = 1766. State = [[-0.3661757  -0.02662752]]. Action = [[ 0.03267349 -0.08954247  0.          0.5434457 ]]. Reward = [0.]
Curr episode timestep = 280
Current timestep = 1767. State = [[-0.3675654  -0.03516428]]. Action = [[-0.04993425 -0.0949921   0.         -0.5939168 ]]. Reward = [0.]
Curr episode timestep = 281
Current timestep = 1768. State = [[-0.3688747  -0.03560796]]. Action = [[0.0036147  0.08609594 0.         0.06518126]]. Reward = [0.]
Curr episode timestep = 282
Current timestep = 1769. State = [[-0.36757988 -0.03260112]]. Action = [[ 0.03593411  0.02963931  0.         -0.8012544 ]]. Reward = [0.]
Curr episode timestep = 283
Current timestep = 1770. State = [[-0.36937094 -0.0332373 ]]. Action = [[-0.04358191 -0.02323024  0.         -0.4204749 ]]. Reward = [0.]
Curr episode timestep = 284
Current timestep = 1771. State = [[-0.3711489  -0.03439081]]. Action = [[ 0.         0.         0.        -0.8726037]]. Reward = [0.]
Curr episode timestep = 285
Current timestep = 1772. State = [[-0.37188107 -0.03480308]]. Action = [[ 0.        0.        0.       -0.683045]]. Reward = [0.]
Curr episode timestep = 286
Current timestep = 1773. State = [[-0.37261268 -0.03397388]]. Action = [[-2.9896200e-04  2.1186151e-02  0.0000000e+00 -9.2018044e-01]]. Reward = [0.]
Curr episode timestep = 287
Current timestep = 1774. State = [[-0.37067208 -0.03540023]]. Action = [[ 0.05461556 -0.03990852  0.          0.68152237]]. Reward = [0.]
Curr episode timestep = 288
Current timestep = 1775. State = [[-0.36848348 -0.03999815]]. Action = [[ 0.02186455 -0.06596477  0.         -0.28017974]]. Reward = [0.]
Curr episode timestep = 289
Current timestep = 1776. State = [[-0.37077367 -0.04462427]]. Action = [[-0.06303904 -0.04151063  0.          0.01816845]]. Reward = [0.]
Curr episode timestep = 290
Current timestep = 1777. State = [[-0.37035272 -0.04737761]]. Action = [[ 0.04171295 -0.01273184  0.         -0.9214306 ]]. Reward = [0.]
Curr episode timestep = 291
Current timestep = 1778. State = [[-0.36914307 -0.04855109]]. Action = [[0.         0.         0.         0.37476325]]. Reward = [0.]
Curr episode timestep = 292
Current timestep = 1779. State = [[-0.3689366  -0.04929737]]. Action = [[ 0.         0.         0.        -0.5351393]]. Reward = [0.]
Curr episode timestep = 293
Current timestep = 1780. State = [[-0.36423907 -0.04568959]]. Action = [[ 0.09811009  0.08044364  0.         -0.38305193]]. Reward = [0.]
Curr episode timestep = 294
Current timestep = 1781. State = [[-0.3641083  -0.03852675]]. Action = [[-0.04568451  0.09409881  0.         -0.36282158]]. Reward = [0.]
Curr episode timestep = 295
Current timestep = 1782. State = [[-0.36911187 -0.03093249]]. Action = [[-0.05722264  0.08679899  0.         -0.02688533]]. Reward = [0.]
Curr episode timestep = 296
Current timestep = 1783. State = [[-0.37232545 -0.02442267]]. Action = [[-0.00182457  0.059411    0.          0.5385606 ]]. Reward = [0.]
Curr episode timestep = 297
Current timestep = 1784. State = [[-0.37074876 -0.02163586]]. Action = [[ 0.06837691 -0.00881115  0.         -0.21300054]]. Reward = [0.]
Curr episode timestep = 298
Current timestep = 1785. State = [[-0.37015784 -0.02040373]]. Action = [[ 0.         0.         0.        -0.8979424]]. Reward = [0.]
Curr episode timestep = 299
Current timestep = 1786. State = [[-0.37391606 -0.01994547]]. Action = [[-0.06278472 -0.01377744  0.         -0.6402882 ]]. Reward = [0.]
Curr episode timestep = 300
Current timestep = 1787. State = [[-0.3761273  -0.01937798]]. Action = [[ 0.          0.          0.         -0.27864182]]. Reward = [0.]
Curr episode timestep = 301
Current timestep = 1788. State = [[-0.37231115 -0.01415873]]. Action = [[ 0.09268583  0.08378942  0.         -0.9579589 ]]. Reward = [0.]
Curr episode timestep = 302
Current timestep = 1789. State = [[-0.37289122 -0.00633467]]. Action = [[-0.04973025  0.0835451   0.         -0.12082803]]. Reward = [0.]
Curr episode timestep = 303
Current timestep = 1790. State = [[-0.37070975 -0.00543672]]. Action = [[ 0.09278116 -0.05961223  0.         -0.6342629 ]]. Reward = [0.]
Curr episode timestep = 304
Current timestep = 1791. State = [[-0.3641109  -0.01007502]]. Action = [[ 0.08347739 -0.08164601  0.         -0.3313986 ]]. Reward = [0.]
Curr episode timestep = 305
Current timestep = 1792. State = [[-0.36364838 -0.01634727]]. Action = [[-0.06783859 -0.08681686  0.          0.6716306 ]]. Reward = [0.]
Curr episode timestep = 306
Current timestep = 1793. State = [[-0.36866635 -0.01559889]]. Action = [[-0.09057115  0.06895355  0.         -0.04141819]]. Reward = [0.]
Curr episode timestep = 307
Current timestep = 1794. State = [[-0.36975574 -0.01656871]]. Action = [[ 0.01920934 -0.05598747  0.         -0.6697142 ]]. Reward = [0.]
Curr episode timestep = 308
Current timestep = 1795. State = [[-0.37107483 -0.01508311]]. Action = [[-0.047173   0.0640303  0.        -0.6386341]]. Reward = [0.]
Curr episode timestep = 309
Current timestep = 1796. State = [[-0.37138608 -0.01431399]]. Action = [[ 0.01534746 -0.01947586  0.          0.1960988 ]]. Reward = [0.]
Curr episode timestep = 310
Current timestep = 1797. State = [[-0.37224758 -0.01208388]]. Action = [[-0.02640293  0.05414789  0.          0.03947294]]. Reward = [0.]
Curr episode timestep = 311
Current timestep = 1798. State = [[-0.3748063  -0.01456984]]. Action = [[-0.03705765 -0.08442966  0.         -0.58550036]]. Reward = [0.]
Curr episode timestep = 312
Current timestep = 1799. State = [[-0.37468952 -0.01655962]]. Action = [[ 0.02297922  0.01198349  0.         -0.63899696]]. Reward = [0.]
Curr episode timestep = 313
Current timestep = 1800. State = [[-0.37423286 -0.0165173 ]]. Action = [[0.        0.        0.        0.6876457]]. Reward = [0.]
Curr episode timestep = 314
Current timestep = 1801. State = [[-0.37438563 -0.01666661]]. Action = [[0.         0.         0.         0.19162273]]. Reward = [0.]
Curr episode timestep = 315
Current timestep = 1802. State = [[-0.37459713 -0.01680606]]. Action = [[0.         0.         0.         0.43454313]]. Reward = [0.]
Curr episode timestep = 316
Current timestep = 1803. State = [[-0.37482005 -0.01692691]]. Action = [[ 0.          0.          0.         -0.28919637]]. Reward = [0.]
Curr episode timestep = 317
Current timestep = 1804. State = [[-0.37503454 -0.01702412]]. Action = [[ 0.         0.         0.        -0.7810545]]. Reward = [0.]
Curr episode timestep = 318
Current timestep = 1805. State = [[-0.3729544  -0.01642183]]. Action = [[0.04794324 0.01300361 0.         0.04796815]]. Reward = [0.]
Curr episode timestep = 319
Current timestep = 1806. State = [[-0.37189752 -0.01603241]]. Action = [[0.         0.         0.         0.11913776]]. Reward = [0.]
Curr episode timestep = 320
Current timestep = 1807. State = [[-0.36933118 -0.01578663]]. Action = [[ 0.05473461  0.00477532  0.         -0.41154242]]. Reward = [0.]
Curr episode timestep = 321
Current timestep = 1808. State = [[-0.3678645  -0.01564711]]. Action = [[ 0.          0.          0.         -0.44687647]]. Reward = [0.]
Curr episode timestep = 322
Current timestep = 1809. State = [[-0.36982086 -0.01135841]]. Action = [[-0.04053993  0.08557551  0.          0.25486302]]. Reward = [0.]
Curr episode timestep = 323
Current timestep = 1810. State = [[-0.37130165 -0.00855899]]. Action = [[ 0.          0.          0.         -0.25339758]]. Reward = [0.]
Curr episode timestep = 324
Current timestep = 1811. State = [[-0.37014884 -0.00846984]]. Action = [[ 0.03035174 -0.01023027  0.         -0.34772003]]. Reward = [0.]
Curr episode timestep = 325
Current timestep = 1812. State = [[-0.36939558 -0.00832098]]. Action = [[0.        0.        0.        0.7638111]]. Reward = [0.]
Curr episode timestep = 326
Current timestep = 1813. State = [[-0.36604357 -0.01030715]]. Action = [[ 0.06568819 -0.04666297  0.          0.08701169]]. Reward = [0.]
Curr episode timestep = 327
Current timestep = 1814. State = [[-0.36619303 -0.00874588]]. Action = [[-0.0489763   0.05509079  0.         -0.6632192 ]]. Reward = [0.]
Curr episode timestep = 328
Current timestep = 1815. State = [[-0.36783782 -0.00936345]]. Action = [[-0.01125777 -0.04746503  0.         -0.48690605]]. Reward = [0.]
Curr episode timestep = 329
Current timestep = 1816. State = [[-0.36413038 -0.00756133]]. Action = [[ 0.0812917   0.05935729  0.         -0.28497982]]. Reward = [0.]
Curr episode timestep = 330
Current timestep = 1817. State = [[-0.36056966 -0.01013546]]. Action = [[ 0.02264404 -0.09323837  0.         -0.24828964]]. Reward = [0.]
Curr episode timestep = 331
Current timestep = 1818. State = [[-0.36326775 -0.01395092]]. Action = [[-0.09058534 -0.02097568  0.         -0.04013085]]. Reward = [0.]
Curr episode timestep = 332
Current timestep = 1819. State = [[-0.3699621  -0.01012459]]. Action = [[-0.09974383  0.09829696  0.          0.7582177 ]]. Reward = [0.]
Curr episode timestep = 333
Current timestep = 1820. State = [[-0.3713781  -0.00996911]]. Action = [[ 0.03739763 -0.05454776  0.          0.45176125]]. Reward = [0.]
Curr episode timestep = 334
Current timestep = 1821. State = [[-0.37038106 -0.01143774]]. Action = [[ 0.         0.         0.        -0.5567332]]. Reward = [0.]
Curr episode timestep = 335
Current timestep = 1822. State = [[-0.3659827 -0.0156466]]. Action = [[ 0.08687148 -0.08020864  0.          0.30628455]]. Reward = [0.]
Curr episode timestep = 336
Current timestep = 1823. State = [[-0.35859415 -0.02067978]]. Action = [[ 0.09419148 -0.04526601  0.         -0.76661646]]. Reward = [0.]
Curr episode timestep = 337
Current timestep = 1824. State = [[-0.35225582 -0.0221342 ]]. Action = [[ 0.05898411  0.01341887  0.         -0.87182206]]. Reward = [0.]
Curr episode timestep = 338
Current timestep = 1825. State = [[-0.34554613 -0.02640496]]. Action = [[ 0.08178284 -0.07535093  0.         -0.939181  ]]. Reward = [0.]
Curr episode timestep = 339
Current timestep = 1826. State = [[-0.33893427 -0.02679499]]. Action = [[ 0.06224114  0.05721354  0.         -0.56899893]]. Reward = [0.]
Curr episode timestep = 340
Current timestep = 1827. State = [[-0.3352466 -0.0215954]]. Action = [[0.01995261 0.09254708 0.         0.74496925]]. Reward = [0.]
Curr episode timestep = 341
Current timestep = 1828. State = [[-0.3321955  -0.02005043]]. Action = [[ 0.03742332 -0.01247811  0.         -0.02077657]]. Reward = [0.]
Curr episode timestep = 342
Current timestep = 1829. State = [[-0.3342867  -0.02299497]]. Action = [[-0.08549445 -0.04493586  0.         -0.32383013]]. Reward = [0.]
Curr episode timestep = 343
Current timestep = 1830. State = [[-0.33247143 -0.02291209]]. Action = [[ 0.06750567  0.03609271  0.         -0.2138204 ]]. Reward = [0.]
Curr episode timestep = 344
Current timestep = 1831. State = [[-0.3295394  -0.02355445]]. Action = [[ 0.00483322 -0.02946747  0.          0.8036026 ]]. Reward = [0.]
Curr episode timestep = 345
Current timestep = 1832. State = [[-0.32714996 -0.02448187]]. Action = [[0.02159586 0.00037955 0.         0.24226022]]. Reward = [0.]
Curr episode timestep = 346
Current timestep = 1833. State = [[-0.32297152 -0.02365544]]. Action = [[ 0.05227626  0.0190924   0.         -0.79299295]]. Reward = [0.]
Curr episode timestep = 347
Current timestep = 1834. State = [[-0.3158133  -0.02794744]]. Action = [[ 0.09488108 -0.09617236  0.         -0.38006604]]. Reward = [0.]
Curr episode timestep = 348
Current timestep = 1835. State = [[-0.3120539  -0.03030437]]. Action = [[-0.00974866  0.0151066   0.         -0.8800391 ]]. Reward = [0.]
Curr episode timestep = 349
Current timestep = 1836. State = [[-0.30906406 -0.03048879]]. Action = [[ 0.03589935 -0.00119161  0.         -0.4114099 ]]. Reward = [0.]
Curr episode timestep = 350
Current timestep = 1837. State = [[-0.30422604 -0.02994751]]. Action = [[ 0.05254827  0.01868938  0.         -0.22961771]]. Reward = [0.]
Curr episode timestep = 351
Current timestep = 1838. State = [[-0.30279166 -0.03270195]]. Action = [[-0.02902909 -0.05942419  0.         -0.41471928]]. Reward = [0.]
Curr episode timestep = 352
Current timestep = 1839. State = [[-0.30256492 -0.03942797]]. Action = [[-0.01399304 -0.09004325  0.         -0.4303229 ]]. Reward = [0.]
Curr episode timestep = 353
Current timestep = 1840. State = [[-0.29869518 -0.03874265]]. Action = [[ 0.05865612  0.08805285  0.         -0.8913467 ]]. Reward = [0.]
Curr episode timestep = 354
Current timestep = 1841. State = [[-0.2962518  -0.04112059]]. Action = [[-0.00922327 -0.09197042  0.          0.02510762]]. Reward = [0.]
Curr episode timestep = 355
Current timestep = 1842. State = [[-0.29866612 -0.04167246]]. Action = [[-0.07407372  0.0540207   0.         -0.8432117 ]]. Reward = [0.]
Curr episode timestep = 356
Current timestep = 1843. State = [[-0.29597875 -0.04027654]]. Action = [[0.08601902 0.00813854 0.         0.6723939 ]]. Reward = [0.]
Curr episode timestep = 357
Current timestep = 1844. State = [[-0.2953401  -0.04424493]]. Action = [[-0.05663273 -0.07754312  0.         -0.53706735]]. Reward = [0.]
Curr episode timestep = 358
Current timestep = 1845. State = [[-0.29297635 -0.04773301]]. Action = [[ 0.05413093 -0.01257354  0.         -0.73700833]]. Reward = [0.]
Curr episode timestep = 359
Current timestep = 1846. State = [[-0.2872491  -0.04547331]]. Action = [[0.07139661 0.06534868 0.         0.46806085]]. Reward = [0.]
Curr episode timestep = 360
Current timestep = 1847. State = [[-0.2824137  -0.04332082]]. Action = [[ 0.04734311  0.01012983  0.         -0.7383136 ]]. Reward = [0.]
Curr episode timestep = 361
Current timestep = 1848. State = [[-0.28463072 -0.04039442]]. Action = [[-0.08782469  0.05500656  0.         -0.084023  ]]. Reward = [0.]
Curr episode timestep = 362
Current timestep = 1849. State = [[-0.28773212 -0.04043527]]. Action = [[-0.01899133 -0.03433729  0.         -0.94000787]]. Reward = [0.]
Curr episode timestep = 363
Current timestep = 1850. State = [[-0.28956938 -0.04310969]]. Action = [[-0.02829187 -0.03731277  0.         -0.81746626]]. Reward = [0.]
Curr episode timestep = 364
Current timestep = 1851. State = [[-0.29345676 -0.03974643]]. Action = [[-0.06432907  0.08874884  0.          0.15613163]]. Reward = [0.]
Curr episode timestep = 365
Current timestep = 1852. State = [[-0.29396042 -0.03234794]]. Action = [[0.0394985  0.08804537 0.         0.5728749 ]]. Reward = [0.]
Curr episode timestep = 366
Current timestep = 1853. State = [[-0.2951112  -0.03045097]]. Action = [[-0.02770398 -0.03407191  0.         -0.6007985 ]]. Reward = [0.]
Curr episode timestep = 367
Current timestep = 1854. State = [[-0.2923772  -0.03328606]]. Action = [[ 0.08351869 -0.05574907  0.         -0.6353874 ]]. Reward = [0.]
Curr episode timestep = 368
Current timestep = 1855. State = [[-0.29374737 -0.03198338]]. Action = [[-0.07357807  0.05019195  0.         -0.48966783]]. Reward = [0.]
Curr episode timestep = 369
Current timestep = 1856. State = [[-0.29208034 -0.02908209]]. Action = [[ 0.09552062  0.01819693  0.         -0.06482202]]. Reward = [0.]
Curr episode timestep = 370
Current timestep = 1857. State = [[-0.28831586 -0.02962576]]. Action = [[ 0.03503751 -0.03433721  0.         -0.8001301 ]]. Reward = [0.]
Curr episode timestep = 371
Current timestep = 1858. State = [[-0.28253424 -0.02808549]]. Action = [[ 0.09810942  0.04341798  0.         -0.6907295 ]]. Reward = [0.]
Curr episode timestep = 372
Current timestep = 1859. State = [[-0.27829126 -0.0251472 ]]. Action = [[ 0.030761    0.02886612  0.         -0.73591536]]. Reward = [0.]
Curr episode timestep = 373
Current timestep = 1860. State = [[-0.28133225 -0.02033744]]. Action = [[-0.08923983  0.07293717  0.         -0.89939946]]. Reward = [0.]
Curr episode timestep = 374
Current timestep = 1861. State = [[-0.28237382 -0.01686798]]. Action = [[ 0.03007569  0.01491554  0.         -0.8764101 ]]. Reward = [0.]
Curr episode timestep = 375
Current timestep = 1862. State = [[-0.2801118  -0.01774551]]. Action = [[ 0.02970164 -0.04352315  0.         -0.6638983 ]]. Reward = [0.]
Curr episode timestep = 376
Current timestep = 1863. State = [[-0.28122878 -0.0143654 ]]. Action = [[-0.04698329  0.07910889  0.         -0.37554562]]. Reward = [0.]
Curr episode timestep = 377
Current timestep = 1864. State = [[-0.2828638  -0.01595121]]. Action = [[-0.01369003 -0.09406772  0.          0.45354986]]. Reward = [0.]
Curr episode timestep = 378
Current timestep = 1865. State = [[-0.2846604  -0.01410651]]. Action = [[-0.03719187  0.07801066  0.         -0.44707716]]. Reward = [0.]
Curr episode timestep = 379
Current timestep = 1866. State = [[-0.28964844 -0.00834749]]. Action = [[-0.08358327  0.05805545  0.         -0.69800925]]. Reward = [0.]
Curr episode timestep = 380
Current timestep = 1867. State = [[-0.29081795 -0.00792719]]. Action = [[ 0.02766792 -0.04682132  0.          0.26254344]]. Reward = [0.]
Curr episode timestep = 381
Current timestep = 1868. State = [[-0.2867268  -0.00736796]]. Action = [[ 0.07348097  0.02117071  0.         -0.51950085]]. Reward = [0.]
Curr episode timestep = 382
Current timestep = 1869. State = [[-0.28734207 -0.00646149]]. Action = [[-0.05472515 -0.00534048  0.         -0.11191177]]. Reward = [0.]
Curr episode timestep = 383
Current timestep = 1870. State = [[-0.29272318 -0.00890959]]. Action = [[-0.0827739  -0.0574981   0.          0.30773675]]. Reward = [0.]
Curr episode timestep = 384
Current timestep = 1871. State = [[-0.29908007 -0.01438403]]. Action = [[-0.08735937 -0.08264575  0.         -0.6606169 ]]. Reward = [0.]
Curr episode timestep = 385
Current timestep = 1872. State = [[-0.30007708 -0.01606673]]. Action = [[ 0.03186864  0.01756699  0.         -0.66912353]]. Reward = [0.]
Curr episode timestep = 386
Current timestep = 1873. State = [[-0.30298105 -0.01176251]]. Action = [[-0.06719397  0.08204652  0.         -0.8601054 ]]. Reward = [0.]
Curr episode timestep = 387
Current timestep = 1874. State = [[-0.3047927  -0.00904037]]. Action = [[ 0.01884316  0.0051443   0.         -0.64042854]]. Reward = [0.]
Curr episode timestep = 388
Current timestep = 1875. State = [[-0.30132687 -0.00626464]]. Action = [[ 0.08862946  0.04704525  0.         -0.8212647 ]]. Reward = [0.]
Curr episode timestep = 389
Current timestep = 1876. State = [[-0.30008632 -0.0081711 ]]. Action = [[-3.7930906e-04 -6.9347709e-02  0.0000000e+00 -4.7000855e-01]]. Reward = [0.]
Curr episode timestep = 390
Current timestep = 1877. State = [[-0.30354658 -0.00710389]]. Action = [[-0.05899962  0.06073395  0.         -0.21208704]]. Reward = [0.]
Curr episode timestep = 391
Current timestep = 1878. State = [[-0.30791944 -0.00681147]]. Action = [[-0.0403588  -0.0294071   0.         -0.76015955]]. Reward = [0.]
Curr episode timestep = 392
Current timestep = 1879. State = [[-0.30668026 -0.00661025]]. Action = [[0.0666318  0.01747517 0.         0.23039842]]. Reward = [0.]
Curr episode timestep = 393
Current timestep = 1880. State = [[-0.30626056 -0.00257998]]. Action = [[-0.0100316  0.0693614  0.        -0.9401617]]. Reward = [0.]
Curr episode timestep = 394
Current timestep = 1881. State = [[-0.3085835 -0.0018001]]. Action = [[-0.02713218 -0.03090367  0.         -0.82575667]]. Reward = [0.]
Curr episode timestep = 395
Current timestep = 1882. State = [[-0.31101128 -0.00101965]]. Action = [[-0.02267227  0.02549244  0.         -0.7521431 ]]. Reward = [0.]
Curr episode timestep = 396
Current timestep = 1883. State = [[-0.31649923 -0.00279715]]. Action = [[-0.0898399  -0.05686289  0.         -0.23336923]]. Reward = [0.]
Curr episode timestep = 397
Current timestep = 1884. State = [[-0.31529695 -0.00110223]]. Action = [[ 0.09381761  0.0611408   0.         -0.7043046 ]]. Reward = [0.]
Curr episode timestep = 398
Current timestep = 1885. State = [[-3.1116802e-01 -2.0438523e-05]]. Action = [[ 0.03958138 -0.01855416  0.         -0.5767487 ]]. Reward = [0.]
Curr episode timestep = 399
Current timestep = 1886. State = [[-0.3143352   0.00102332]]. Action = [[-0.09702841  0.0251398   0.          0.11322975]]. Reward = [0.]
Curr episode timestep = 400
Current timestep = 1887. State = [[-0.3210233   0.00152564]]. Action = [[-0.0829093  -0.01116083  0.         -0.1031363 ]]. Reward = [0.]
Curr episode timestep = 401
Current timestep = 1888. State = [[-0.32624993  0.00578721]]. Action = [[-0.04781394  0.08272318  0.         -0.9053291 ]]. Reward = [0.]
Curr episode timestep = 402
Current timestep = 1889. State = [[-0.33331972  0.00993758]]. Action = [[-0.0940439  0.0216874  0.        -0.719277 ]]. Reward = [0.]
Curr episode timestep = 403
Current timestep = 1890. State = [[-0.34101558  0.0154737 ]]. Action = [[-0.07448343  0.0780145   0.          0.82349634]]. Reward = [0.]
Curr episode timestep = 404
Current timestep = 1891. State = [[-0.3473322   0.02044562]]. Action = [[-0.04478703  0.03104014  0.         -0.28881466]]. Reward = [0.]
Curr episode timestep = 405
Current timestep = 1892. State = [[-0.34644604  0.0226954 ]]. Action = [[ 0.09723371  0.00142749  0.         -0.38389945]]. Reward = [0.]
Curr episode timestep = 406
Current timestep = 1893. State = [[-0.34758562  0.02451856]]. Action = [[-0.03678767  0.01506189  0.         -0.6723447 ]]. Reward = [0.]
Curr episode timestep = 407
Current timestep = 1894. State = [[-0.35243568  0.02914125]]. Action = [[-0.04308783  0.06519171  0.         -0.8510752 ]]. Reward = [0.]
Curr episode timestep = 408
Current timestep = 1895. State = [[-0.35674265  0.03603145]]. Action = [[-0.01971154  0.07967613  0.         -0.27122414]]. Reward = [0.]
Curr episode timestep = 409
Current timestep = 1896. State = [[-0.35765347  0.03840448]]. Action = [[ 0.03711853 -0.02315511  0.          0.4890598 ]]. Reward = [0.]
Curr episode timestep = 410
Current timestep = 1897. State = [[-0.3627446   0.04231457]]. Action = [[-0.0907995   0.06782115  0.         -0.6111249 ]]. Reward = [0.]
Curr episode timestep = 411
Current timestep = 1898. State = [[-0.37094086  0.0462662 ]]. Action = [[-0.08066156  0.01638994  0.          0.21285343]]. Reward = [0.]
Curr episode timestep = 412
Current timestep = 1899. State = [[-0.37151006  0.04863679]]. Action = [[0.07884369 0.01230452 0.         0.14426005]]. Reward = [0.]
Curr episode timestep = 413
Current timestep = 1900. State = [[-0.36618412  0.05032454]]. Action = [[ 0.09667871  0.0080142   0.         -0.36523044]]. Reward = [0.]
Curr episode timestep = 414
Current timestep = 1901. State = [[-0.36602983  0.05142707]]. Action = [[-0.04087327  0.00152083  0.         -0.23838663]]. Reward = [0.]
Curr episode timestep = 415
Current timestep = 1902. State = [[-0.367624    0.05226183]]. Action = [[0.         0.         0.         0.28494167]]. Reward = [0.]
Curr episode timestep = 416
Current timestep = 1903. State = [[-0.3636715   0.05698796]]. Action = [[ 0.09555592  0.08377428  0.         -0.55557853]]. Reward = [0.]
Curr episode timestep = 417
Current timestep = 1904. State = [[-0.36422867  0.05562105]]. Action = [[-0.07148052 -0.09215083  0.         -0.54646355]]. Reward = [0.]
Curr episode timestep = 418
Current timestep = 1905. State = [[-0.36299723  0.05075113]]. Action = [[ 0.06836965 -0.05483609  0.         -0.07589161]]. Reward = [0.]
Curr episode timestep = 419
Current timestep = 1906. State = [[-0.3631361   0.04954341]]. Action = [[-0.04357831  0.0104216   0.         -0.4013586 ]]. Reward = [0.]
Curr episode timestep = 420
Current timestep = 1907. State = [[-0.36017016  0.05329634]]. Action = [[ 0.08483683  0.07430816  0.         -0.09290648]]. Reward = [0.]
Curr episode timestep = 421
Current timestep = 1908. State = [[-0.35310996  0.05071316]]. Action = [[ 0.09104688 -0.09326489  0.         -0.85827476]]. Reward = [0.]
Curr episode timestep = 422
Current timestep = 1909. State = [[-0.3449814   0.04270338]]. Action = [[ 0.08704894 -0.09805208  0.         -0.59116423]]. Reward = [0.]
Curr episode timestep = 423
Current timestep = 1910. State = [[-0.3395114   0.04254305]]. Action = [[ 0.02764957  0.07853592  0.         -0.5406497 ]]. Reward = [0.]
Curr episode timestep = 424
Current timestep = 1911. State = [[-0.33770636  0.04425977]]. Action = [[-0.01088757  0.00784792  0.         -0.11712688]]. Reward = [0.]
Curr episode timestep = 425
Current timestep = 1912. State = [[-0.33606875  0.04313306]]. Action = [[ 0.00882716 -0.01780459  0.         -0.7651454 ]]. Reward = [0.]
Curr episode timestep = 426
Current timestep = 1913. State = [[-0.33613685  0.04586199]]. Action = [[-0.03252544  0.07417915  0.         -0.6365448 ]]. Reward = [0.]
Curr episode timestep = 427
Current timestep = 1914. State = [[-0.333609    0.04553083]]. Action = [[ 0.05085044 -0.04542691  0.          0.0421319 ]]. Reward = [0.]
Curr episode timestep = 428
Current timestep = 1915. State = [[-0.32870653  0.04127484]]. Action = [[ 0.04866005 -0.05554738  0.         -0.19010204]]. Reward = [0.]
Curr episode timestep = 429
Current timestep = 1916. State = [[-0.32272303  0.03671928]]. Action = [[ 0.06512391 -0.04832826  0.         -0.11975908]]. Reward = [0.]
Curr episode timestep = 430
Current timestep = 1917. State = [[-0.31469297  0.0330369 ]]. Action = [[ 0.09429278 -0.02914365  0.         -0.9153661 ]]. Reward = [0.]
Curr episode timestep = 431
Current timestep = 1918. State = [[-0.30666658  0.03530882]]. Action = [[ 0.07755215  0.08351877  0.         -0.73757017]]. Reward = [0.]
Curr episode timestep = 432
Current timestep = 1919. State = [[-0.29943132  0.03952739]]. Action = [[0.07503515 0.05449117 0.         0.07556903]]. Reward = [0.]
Curr episode timestep = 433
Current timestep = 1920. State = [[-0.2976148   0.04058545]]. Action = [[-0.03709646 -0.0020887   0.         -0.9261602 ]]. Reward = [0.]
Curr episode timestep = 434
Current timestep = 1921. State = [[-0.2939926   0.04335444]]. Action = [[ 0.0667646   0.05917326  0.         -0.14843053]]. Reward = [0.]
Curr episode timestep = 435
Current timestep = 1922. State = [[-0.2903177   0.04762676]]. Action = [[0.01654873 0.05094995 0.         0.4332807 ]]. Reward = [0.]
Curr episode timestep = 436
Current timestep = 1923. State = [[-0.29077974  0.0474689 ]]. Action = [[-0.04474915 -0.04381068  0.         -0.70317256]]. Reward = [0.]
Curr episode timestep = 437
Current timestep = 1924. State = [[-0.28760815  0.0505564 ]]. Action = [[ 0.06621004  0.07247739  0.         -0.04929733]]. Reward = [0.]
Curr episode timestep = 438
Current timestep = 1925. State = [[-0.28230083  0.05765892]]. Action = [[0.05410237 0.0864488  0.         0.48172307]]. Reward = [0.]
Curr episode timestep = 439
Current timestep = 1926. State = [[-0.2760755   0.06522465]]. Action = [[ 0.08329297  0.0812906   0.         -0.79122555]]. Reward = [0.]
Curr episode timestep = 440
Current timestep = 1927. State = [[-0.27248022  0.07327426]]. Action = [[ 0.01549534  0.08951614  0.         -0.08846253]]. Reward = [0.]
Curr episode timestep = 441
Current timestep = 1928. State = [[-0.27139512  0.07908357]]. Action = [[ 0.00155588  0.03127306  0.         -0.13572562]]. Reward = [0.]
Curr episode timestep = 442
Current timestep = 1929. State = [[-0.2676648   0.08540304]]. Action = [[ 0.06340074  0.0688431   0.         -0.18933928]]. Reward = [0.]
Curr episode timestep = 443
Current timestep = 1930. State = [[-0.2633803   0.09055321]]. Action = [[ 0.03900916  0.02365403  0.         -0.19500262]]. Reward = [0.]
Curr episode timestep = 444
Current timestep = 1931. State = [[-0.2644252   0.09018965]]. Action = [[-0.07024659 -0.06943402  0.         -0.567122  ]]. Reward = [0.]
Curr episode timestep = 445
Current timestep = 1932. State = [[-0.26682416  0.09053382]]. Action = [[-0.03818736 -0.00278257  0.         -0.2881598 ]]. Reward = [0.]
Curr episode timestep = 446
Current timestep = 1933. State = [[-0.2639429   0.09112435]]. Action = [[ 0.05887809 -0.02575862  0.         -0.6022923 ]]. Reward = [0.]
Curr episode timestep = 447
Current timestep = 1934. State = [[-0.259575    0.09434104]]. Action = [[ 0.03331888  0.05102182  0.         -0.43718755]]. Reward = [0.]
Curr episode timestep = 448
Current timestep = 1935. State = [[-0.25374073  0.09960812]]. Action = [[ 0.080186    0.05627728  0.         -0.41436112]]. Reward = [0.]
Curr episode timestep = 449
Current timestep = 1936. State = [[-0.24746044  0.10223584]]. Action = [[ 0.06219413  0.00360028  0.         -0.15157926]]. Reward = [0.]
Curr episode timestep = 450
Current timestep = 1937. State = [[-0.24387445  0.10643174]]. Action = [[ 0.01191608  0.06721351  0.         -0.66932154]]. Reward = [0.]
Curr episode timestep = 451
Current timestep = 1938. State = [[-0.24525602  0.10848054]]. Action = [[-0.06152119 -0.01704944  0.         -0.56427675]]. Reward = [0.]
Curr episode timestep = 452
Current timestep = 1939. State = [[-0.24151906  0.10863018]]. Action = [[ 0.08827644 -0.00923365  0.          0.24331391]]. Reward = [0.]
Curr episode timestep = 453
Current timestep = 1940. State = [[-0.23416813  0.11189998]]. Action = [[ 0.07358991  0.05949069  0.         -0.8275112 ]]. Reward = [0.]
Curr episode timestep = 454
Current timestep = 1941. State = [[-0.23169589  0.11145359]]. Action = [[-0.02726213 -0.0565463   0.         -0.5545579 ]]. Reward = [0.]
Curr episode timestep = 455
Current timestep = 1942. State = [[-0.23499331  0.11469214]]. Action = [[-0.08515538  0.07877571  0.          0.84892917]]. Reward = [0.]
Curr episode timestep = 456
Current timestep = 1943. State = [[-0.23720701  0.11477242]]. Action = [[-0.017853   -0.0662832   0.         -0.73318195]]. Reward = [0.]
Curr episode timestep = 457
Current timestep = 1944. State = [[-0.23327406  0.11814279]]. Action = [[ 0.08138693  0.08919998  0.         -0.7391461 ]]. Reward = [0.]
Curr episode timestep = 458
Current timestep = 1945. State = [[-0.22682133  0.12072548]]. Action = [[ 0.07269324 -0.00797749  0.         -0.69693583]]. Reward = [0.]
Curr episode timestep = 459
Current timestep = 1946. State = [[-0.22071667  0.11832052]]. Action = [[ 0.05787637 -0.05401411  0.          0.45371485]]. Reward = [0.]
Curr episode timestep = 460
Current timestep = 1947. State = [[-0.21360041  0.12021747]]. Action = [[ 0.08303051  0.06815589  0.         -0.20218867]]. Reward = [0.]
Curr episode timestep = 461
Current timestep = 1948. State = [[-0.20789246  0.12196225]]. Action = [[ 0.04153395 -0.0022669   0.         -0.01874614]]. Reward = [0.]
Curr episode timestep = 462
Current timestep = 1949. State = [[-0.20051739  0.11957508]]. Action = [[ 0.09166486 -0.04880449  0.         -0.7722704 ]]. Reward = [0.]
Curr episode timestep = 463
Current timestep = 1950. State = [[-0.19211581  0.11421169]]. Action = [[ 0.07613439 -0.07379766  0.         -0.16476876]]. Reward = [0.]
Curr episode timestep = 464
Current timestep = 1951. State = [[-0.18516509  0.10729619]]. Action = [[ 0.04415876 -0.08362406  0.         -0.43069208]]. Reward = [0.]
Curr episode timestep = 465
Current timestep = 1952. State = [[-0.17662562  0.10009882]]. Action = [[ 0.08812832 -0.07514964  0.         -0.7061663 ]]. Reward = [0.]
Curr episode timestep = 466
Current timestep = 1953. State = [[-0.1672797   0.09669868]]. Action = [[ 0.07920497  0.00819119  0.         -0.226848  ]]. Reward = [0.]
Curr episode timestep = 467
Current timestep = 1954. State = [[-0.15730366  0.09058709]]. Action = [[ 0.09664311 -0.08816508  0.         -0.9029618 ]]. Reward = [0.]
Curr episode timestep = 468
Current timestep = 1955. State = [[-0.15155175  0.08836871]]. Action = [[-0.0017588   0.04694853  0.         -0.5190367 ]]. Reward = [0.]
Curr episode timestep = 469
Current timestep = 1956. State = [[-0.14540079  0.08796724]]. Action = [[ 0.06823207  0.00323848  0.         -0.90004945]]. Reward = [0.]
Curr episode timestep = 470
Current timestep = 1957. State = [[-0.1382465   0.08388931]]. Action = [[ 0.0554877  -0.05077296  0.         -0.54741955]]. Reward = [0.]
Curr episode timestep = 471
Current timestep = 1958. State = [[-0.13363956  0.07654882]]. Action = [[ 0.00535931 -0.08635293  0.         -0.8974375 ]]. Reward = [0.]
Curr episode timestep = 472
Current timestep = 1959. State = [[-0.13324441  0.07010573]]. Action = [[-0.05717406 -0.05031105  0.         -0.8502954 ]]. Reward = [0.]
Curr episode timestep = 473
Current timestep = 1960. State = [[-0.12987033  0.06497375]]. Action = [[ 0.047275   -0.04491679  0.         -0.8026792 ]]. Reward = [0.]
Curr episode timestep = 474
Current timestep = 1961. State = [[-0.12578763  0.06391151]]. Action = [[ 0.01282968  0.03569532  0.         -0.32994884]]. Reward = [0.]
Curr episode timestep = 475
Current timestep = 1962. State = [[-0.12192037  0.06639784]]. Action = [[ 0.03847552  0.05490116  0.         -0.2411462 ]]. Reward = [0.]
Curr episode timestep = 476
Current timestep = 1963. State = [[-0.11642163  0.0656874 ]]. Action = [[ 0.0677122  -0.0284148   0.         -0.28180254]]. Reward = [0.]
Curr episode timestep = 477
Current timestep = 1964. State = [[-0.114404    0.05960813]]. Action = [[-0.02537394 -0.09068283  0.         -0.80683446]]. Reward = [0.]
Curr episode timestep = 478
Current timestep = 1965. State = [[-0.11121736  0.05809902]]. Action = [[ 0.05137608  0.04023045  0.         -0.03287172]]. Reward = [0.]
Curr episode timestep = 479
Current timestep = 1966. State = [[-0.10470449  0.061455  ]]. Action = [[ 0.0898791   0.06416856  0.         -0.8977246 ]]. Reward = [0.]
Curr episode timestep = 480
Current timestep = 1967. State = [[-0.0992116   0.06482956]]. Action = [[ 0.05099633  0.04340228  0.         -0.34574372]]. Reward = [0.]
Curr episode timestep = 481
Current timestep = 1968. State = [[-0.09325566  0.0658844 ]]. Action = [[ 0.07873047  0.00324517  0.         -0.7011492 ]]. Reward = [0.]
Curr episode timestep = 482
Current timestep = 1969. State = [[-0.09184507  0.06655939]]. Action = [[-0.03581107  0.01516203  0.         -0.77740103]]. Reward = [0.]
Curr episode timestep = 483
Current timestep = 1970. State = [[-0.09423658  0.0640754 ]]. Action = [[-0.05511192 -0.06394425  0.         -0.9432835 ]]. Reward = [0.]
Curr episode timestep = 484
Current timestep = 1971. State = [[-0.09088832  0.0582618 ]]. Action = [[ 0.07516479 -0.08441724  0.         -0.8594797 ]]. Reward = [0.]
Curr episode timestep = 485
Current timestep = 1972. State = [[-0.08673088  0.05833721]]. Action = [[ 0.01492801  0.05796628  0.         -0.49026108]]. Reward = [0.]
Curr episode timestep = 486
Current timestep = 1973. State = [[-0.08821152  0.05948107]]. Action = [[-0.06849311 -0.00839943  0.         -0.4930666 ]]. Reward = [0.]
Curr episode timestep = 487
Current timestep = 1974. State = [[-0.08906694  0.05837766]]. Action = [[-0.00664552 -0.02550259  0.         -0.40657878]]. Reward = [0.]
Curr episode timestep = 488
Current timestep = 1975. State = [[-0.08420116  0.0540178 ]]. Action = [[ 0.0838424  -0.07308252  0.         -0.59382594]]. Reward = [0.]
Curr episode timestep = 489
Current timestep = 1976. State = [[-0.07749139  0.05078834]]. Action = [[ 0.07052971 -0.00931916  0.         -0.83229506]]. Reward = [0.]
Curr episode timestep = 490
Current timestep = 1977. State = [[-0.07027985  0.05068073]]. Action = [[ 0.08801835  0.02474475  0.         -0.8422588 ]]. Reward = [0.]
Curr episode timestep = 491
Current timestep = 1978. State = [[-0.06821594  0.04988187]]. Action = [[-0.03164297 -0.01183973  0.          0.4427905 ]]. Reward = [0.]
Curr episode timestep = 492
Current timestep = 1979. State = [[-0.06328053  0.045826  ]]. Action = [[ 0.09843684 -0.05829756  0.         -0.83267975]]. Reward = [0.]
Curr episode timestep = 493
Current timestep = 1980. State = [[-0.05472888  0.04051219]]. Action = [[ 0.0983919  -0.04580348  0.         -0.9119789 ]]. Reward = [0.]
Curr episode timestep = 494
Current timestep = 1981. State = [[-0.21268757  0.03510943]]. Action = [[ 0.07149342  0.07312312  0.         -0.10813183]]. Reward = [100.]
Curr episode timestep = 495
Current timestep = 1982. State = [[-0.21339759  0.02790006]]. Action = [[ 0.00474168 -0.07400922  0.          0.3449874 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1983. State = [[-0.20967649  0.02331377]]. Action = [[ 0.07954181 -0.02478715  0.         -0.2635851 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1984. State = [[-0.2093932   0.02058336]]. Action = [[-0.0351903  -0.01067545  0.          0.10974765]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1985. State = [[-0.20808026  0.02205323]]. Action = [[ 0.05370467  0.06389635  0.         -0.3292017 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1986. State = [[-0.20467271  0.02017505]]. Action = [[ 0.05374957 -0.04819106  0.         -0.16164863]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1987. State = [[-0.20074683  0.01843026]]. Action = [[ 0.05692407  0.0185385   0.         -0.96541154]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1988. State = [[-0.19559155  0.01487507]]. Action = [[ 0.07642629 -0.05363593  0.         -0.8698064 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1989. State = [[-0.1886822   0.00904898]]. Action = [[ 0.09198152 -0.05651042  0.         -0.45117474]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1990. State = [[-0.1827069   0.00911058]]. Action = [[0.05671216 0.068045   0.         0.581532  ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1991. State = [[-0.17818643  0.01111818]]. Action = [[ 0.04604674  0.02953009  0.         -0.66261697]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1992. State = [[-0.1730611   0.00906259]]. Action = [[ 0.0597165  -0.03977977  0.         -0.7577721 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1993. State = [[-0.16650654  0.01054285]]. Action = [[ 0.07614722  0.070416    0.         -0.3182251 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1994. State = [[-0.16581552  0.01452366]]. Action = [[-0.05810541  0.05008414  0.         -0.624089  ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1995. State = [[-0.16798212  0.01232317]]. Action = [[-0.0415656  -0.07780986  0.         -0.4239471 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1996. State = [[-0.16330056  0.00904374]]. Action = [[ 0.09599779 -0.02476986  0.         -0.8737401 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1997. State = [[-0.15435268  0.00609124]]. Action = [[ 0.09732335 -0.03910286  0.          0.04159689]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1998. State = [[-0.15067697  0.0038989 ]]. Action = [[-0.02390949 -0.01357575  0.         -0.62383723]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1999. State = [[-0.14459704  0.0002024 ]]. Action = [[ 0.09660814 -0.05711458  0.          0.24129677]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2000. State = [[-0.13670844  0.00227596]]. Action = [[ 0.06842899  0.08933035  0.         -0.8192898 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2001. State = [[-0.13163213  0.00290645]]. Action = [[ 0.02661601 -0.02846573  0.         -0.8539495 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2002. State = [[-0.12918712  0.00475404]]. Action = [[-0.00622524  0.05680089  0.         -0.08254498]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2003. State = [[-0.12310048  0.00561592]]. Action = [[ 0.09257109 -0.01410975  0.          0.34508777]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2004. State = [[-0.11401492  0.00367141]]. Action = [[ 0.09804156 -0.02930421  0.         -0.5121419 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2005. State = [[-0.10844908  0.00132658]]. Action = [[ 0.01531229 -0.02358739  0.         -0.56097394]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2006. State = [[-0.10277767 -0.00146043]]. Action = [[ 0.05945349 -0.0345877   0.         -0.871516  ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2007. State = [[-0.09586515 -0.00219092]]. Action = [[ 0.06227366  0.01650288  0.         -0.96495306]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2008. State = [[-0.08794274 -0.00133   ]]. Action = [[ 0.08299599  0.01899789  0.         -0.46144187]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2009. State = [[-0.08031459 -0.00077061]]. Action = [[ 0.06641258  0.00867756  0.         -0.6307075 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2010. State = [[-7.5005449e-02  2.8414593e-06]]. Action = [[ 0.02608993  0.01673518  0.         -0.72558165]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2011. State = [[-0.07209677 -0.00278273]]. Action = [[-0.00487556 -0.06250788  0.         -0.9401067 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2012. State = [[-0.06699523 -0.00194462]]. Action = [[ 0.05728797  0.05655582  0.         -0.76816916]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 2013. State = [[-0.05874113  0.00310704]]. Action = [[ 0.09843177  0.07089115  0.         -0.80445457]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2014. State = [[-0.05081332  0.00790428]]. Action = [[ 0.07393814  0.04931445  0.         -0.75937676]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2015. State = [[-0.19555058  0.07691517]]. Action = [[ 0.09697167 -0.05774579  0.         -0.8083861 ]]. Reward = [100.]
Curr episode timestep = 33
Current timestep = 2016. State = [[-0.19304019  0.07806472]]. Action = [[ 0.08704033  0.08183462  0.         -0.9564584 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2017. State = [[-0.18426718  0.07647929]]. Action = [[ 0.09358528 -0.07959392  0.         -0.40087128]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2018. State = [[-0.17769724  0.07262895]]. Action = [[ 0.02793851 -0.0292329   0.         -0.50071627]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2019. State = [[-0.17038079  0.06715491]]. Action = [[ 0.07644231 -0.08106501  0.         -0.32608604]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2020. State = [[-0.16266558  0.06087447]]. Action = [[ 0.055489   -0.05848398  0.         -0.5439459 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2021. State = [[-0.15398164  0.0529227 ]]. Action = [[ 0.08388101 -0.0948127   0.         -0.46778548]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2022. State = [[-0.1439917   0.04910501]]. Action = [[ 0.09636164  0.01936579  0.         -0.9498415 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2023. State = [[-0.14205505  0.04337312]]. Action = [[-0.08934721 -0.08734778  0.         -0.62450933]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2024. State = [[-0.1428807   0.03748146]]. Action = [[-0.01860682 -0.03438609  0.         -0.7238475 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2025. State = [[-0.14031236  0.03321801]]. Action = [[ 0.03080738 -0.02704062  0.         -0.80995506]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2026. State = [[-0.1337689   0.03430939]]. Action = [[ 0.09564885  0.07628674  0.         -0.88118154]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2027. State = [[-0.12702113  0.03593602]]. Action = [[ 0.07253399  0.02148573  0.         -0.9645353 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2028. State = [[-0.12563676  0.0393145 ]]. Action = [[-0.02714261  0.07576118  0.         -0.36220914]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2029. State = [[-0.12194578  0.03910031]]. Action = [[ 0.08087415 -0.03833449  0.         -0.74978006]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2030. State = [[-0.11484918  0.04177976]]. Action = [[ 0.09235597  0.08448563  0.         -0.33952343]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2031. State = [[-0.10992764  0.0470986 ]]. Action = [[ 0.04060299  0.06268876  0.         -0.781095  ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2032. State = [[-0.10365142  0.04849574]]. Action = [[ 0.0931459  -0.01412347  0.         -0.75235623]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2033. State = [[-0.09683225  0.04743413]]. Action = [[ 0.06759364 -0.01827266  0.         -0.7487727 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2034. State = [[-0.09262662  0.0456794 ]]. Action = [[ 0.0169763  -0.02776696  0.         -0.8752679 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2035. State = [[-0.08580779  0.0421962 ]]. Action = [[ 0.09095966 -0.05523116  0.         -0.61121845]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2036. State = [[-0.0801226   0.04182259]]. Action = [[ 0.02256064  0.02950586  0.         -0.9053217 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2037. State = [[-0.07497074  0.04465048]]. Action = [[ 0.04812305  0.04275983  0.         -0.96582574]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2038. State = [[-0.0670986   0.04510523]]. Action = [[ 0.09241138 -0.01651427  0.         -0.9163174 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2039. State = [[-0.05838038  0.04396355]]. Action = [[ 0.08154815 -0.01110549  0.          0.00395429]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2040. State = [[-0.05206026  0.04484235]]. Action = [[ 0.03499318  0.02942387  0.         -0.65041435]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2041. State = [[-0.30838075 -0.00526309]]. Action = [[ 0.02594315 -0.01030435  0.         -0.57071525]]. Reward = [100.]
Curr episode timestep = 25
Current timestep = 2042. State = [[-0.31235737 -0.00972143]]. Action = [[-0.08333163 -0.03215551  0.         -0.42546344]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2043. State = [[-0.31821302 -0.00886087]]. Action = [[-0.0486463   0.04397079  0.         -0.8439918 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2044. State = [[-0.32571888 -0.0055548 ]]. Action = [[-0.0902699   0.04301489  0.         -0.08272928]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2045. State = [[-0.33464685 -0.00262397]]. Action = [[-0.09065565  0.02854563  0.         -0.790256  ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2046. State = [[-3.356149e-01  6.961828e-06]]. Action = [[0.09396911 0.02522979 0.         0.64081407]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2047. State = [[-0.33296522 -0.00322897]]. Action = [[ 0.04140437 -0.08887102  0.         -0.45000076]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2048. State = [[-0.32852304 -0.00285816]]. Action = [[ 0.09156602  0.0599687   0.         -0.02994996]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2049. State = [[-0.32822174 -0.00240746]]. Action = [[-0.02990635 -0.02613562  0.          0.2086184 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2050. State = [[-0.3300813  -0.00338708]]. Action = [[-0.01016469 -0.00658089  0.         -0.4007733 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2051. State = [[-0.3307152  -0.00786783]]. Action = [[ 0.00130796 -0.08257528  0.         -0.660914  ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2052. State = [[-0.33028105 -0.01505395]]. Action = [[ 0.00679599 -0.0854392   0.         -0.1691978 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2053. State = [[-0.33251035 -0.0225052 ]]. Action = [[-0.05949718 -0.07808778  0.         -0.7135664 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2054. State = [[-0.33641905 -0.02938847]]. Action = [[-0.06019194 -0.0629781   0.         -0.24775738]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2055. State = [[-0.3423223  -0.03535682]]. Action = [[-0.09609974 -0.04763859  0.          0.7097405 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2056. State = [[-0.34765837 -0.03891737]]. Action = [[-0.05538331 -0.00642602  0.         -0.26690054]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2057. State = [[-0.34755158 -0.03615464]]. Action = [[ 0.05197544  0.08835954  0.         -0.5621754 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2058. State = [[-0.34751323 -0.03801746]]. Action = [[-0.00857654 -0.0817966   0.          0.00853264]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2059. State = [[-0.35325706 -0.03638995]]. Action = [[-0.09564808  0.0963623   0.         -0.6585667 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2060. State = [[-0.35607335 -0.03169527]]. Action = [[0.03700864 0.04391333 0.         0.02175379]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2061. State = [[-0.35642788 -0.03216469]]. Action = [[ 0.00943863 -0.04341732  0.         -0.34355927]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2062. State = [[-0.355301   -0.02952415]]. Action = [[ 0.04903462  0.07166792  0.         -0.84535635]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2063. State = [[-0.3578686  -0.03094183]]. Action = [[-0.05567748 -0.08750472  0.         -0.6116202 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2064. State = [[-0.36484015 -0.0364417 ]]. Action = [[-0.09466551 -0.0582087   0.         -0.9341209 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2065. State = [[-0.3700154  -0.03892343]]. Action = [[-0.03461207 -0.00202107  0.         -0.7904419 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2066. State = [[-0.37163454 -0.03705387]]. Action = [[ 0.01134173  0.04693092  0.         -0.8033769 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2067. State = [[-0.37310606 -0.03312669]]. Action = [[-0.00801547  0.0475131   0.         -0.41190708]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2068. State = [[-0.3702237  -0.02761653]]. Action = [[ 0.09778366  0.06666268  0.         -0.12542427]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2069. State = [[-0.36996776 -0.02039555]]. Action = [[-0.01858991  0.08177508  0.         -0.27536047]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2070. State = [[-0.370539   -0.01298869]]. Action = [[ 0.03271928  0.07258434  0.         -0.5688268 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2071. State = [[-0.37136  -0.009261]]. Action = [[0.         0.         0.         0.45392787]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2072. State = [[-0.37255654 -0.00781841]]. Action = [[ 0.          0.          0.         -0.46190226]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 2073. State = [[-0.37340578 -0.00661931]]. Action = [[0.         0.         0.         0.00304675]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2074. State = [[-0.37282583 -0.00721981]]. Action = [[ 0.0209228  -0.03299328  0.         -0.96018535]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2075. State = [[-0.3727037  -0.01103252]]. Action = [[-0.01124298 -0.0704205   0.         -0.5578598 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 2076. State = [[-0.3725062  -0.01295897]]. Action = [[ 0.         0.         0.        -0.6081295]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 2077. State = [[-0.3685618  -0.00869907]]. Action = [[0.07538293 0.08441906 0.         0.5762247 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2078. State = [[-0.36580852 -0.00183873]]. Action = [[ 0.01593413  0.07715113  0.         -0.18549371]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2079. State = [[-0.369012   -0.00239385]]. Action = [[-0.08314571 -0.07841785  0.         -0.27034616]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2080. State = [[-0.3677437  -0.00781989]]. Action = [[ 0.06657743 -0.06693192  0.          0.6963084 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 2081. State = [[-0.36408618 -0.01361753]]. Action = [[ 0.01855625 -0.06660041  0.         -0.33575553]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2082. State = [[-0.35948455 -0.02056778]]. Action = [[ 0.05495956 -0.08195041  0.         -0.06176311]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 2083. State = [[-0.35241485 -0.02307754]]. Action = [[ 0.08554877  0.02268082  0.         -0.64580894]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2084. State = [[-0.3526818  -0.02527895]]. Action = [[-0.09388033 -0.03070018  0.         -0.7355809 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 2085. State = [[-0.35616317 -0.02489896]]. Action = [[-0.03590818  0.0491088   0.         -0.8998849 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 2086. State = [[-0.3535882  -0.02784434]]. Action = [[ 0.07825663 -0.06914785  0.          0.6961415 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 2087. State = [[-0.35368675 -0.03262726]]. Action = [[-0.05832564 -0.03514913  0.         -0.15349346]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 2088. State = [[-0.35123667 -0.03118266]]. Action = [[ 0.08223387  0.07120503  0.         -0.04410321]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 2089. State = [[-0.3521353  -0.02984889]]. Action = [[-0.06592716 -0.0047406   0.         -0.5063269 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 2090. State = [[-0.35130098 -0.03481479]]. Action = [[ 0.05691052 -0.08933846  0.         -0.32881427]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 2091. State = [[-0.34577164 -0.04049141]]. Action = [[ 0.07893843 -0.04705436  0.         -0.19168991]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 2092. State = [[-0.34079307 -0.04328819]]. Action = [[ 0.04564067 -0.00978002  0.         -0.6148023 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 2093. State = [[-0.34044597 -0.03978773]]. Action = [[-0.03332794  0.09366708  0.          0.0015049 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 2094. State = [[-0.33855537 -0.03236649]]. Action = [[ 0.05690833  0.0995729   0.         -0.863506  ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 2095. State = [[-0.33996654 -0.03277694]]. Action = [[-0.06404178 -0.09547475  0.         -0.22768068]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 2096. State = [[-0.3378332  -0.03207473]]. Action = [[ 0.08238723  0.07435053  0.         -0.47196126]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 2097. State = [[-0.33135906 -0.03128572]]. Action = [[ 0.08897097 -0.03285163  0.          0.0036335 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 2098. State = [[-0.3277448  -0.03514167]]. Action = [[ 0.01130559 -0.06579517  0.         -0.4506091 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 2099. State = [[-0.3306154  -0.03478057]]. Action = [[-0.09094784  0.05014274  0.         -0.5300109 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 2100. State = [[-0.33017316 -0.03344809]]. Action = [[ 0.05194245 -0.00111864  0.         -0.80875045]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 2101. State = [[-0.32706368 -0.0367577 ]]. Action = [[ 0.01900513 -0.06801595  0.          0.11114693]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 2102. State = [[-0.32115105 -0.03686734]]. Action = [[ 0.08872277  0.04054656  0.         -0.5680791 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 2103. State = [[-0.32213378 -0.03581771]]. Action = [[-0.09950702  0.00175421  0.         -0.46176076]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 2104. State = [[-0.3203762  -0.03969356]]. Action = [[ 0.08804592 -0.07901103  0.         -0.26884496]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 2105. State = [[-0.31827193 -0.03878652]]. Action = [[-0.02539973  0.07165013  0.         -0.33187592]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 2106. State = [[-0.32040372 -0.03424012]]. Action = [[-0.04543104  0.05405822  0.         -0.82278675]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 2107. State = [[-0.32591677 -0.03034105]]. Action = [[-0.09523154  0.04128069  0.         -0.8497395 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 2108. State = [[-0.3298627  -0.02787575]]. Action = [[-0.02729769  0.01509938  0.         -0.9421727 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 2109. State = [[-0.3344314  -0.02527074]]. Action = [[-0.06992213  0.02901874  0.          0.13065529]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 2110. State = [[-0.3346203  -0.02347463]]. Action = [[ 0.05222123  0.00141104  0.         -0.25414932]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 2111. State = [[-0.33456296 -0.02161014]]. Action = [[-0.01235824  0.01750562  0.         -0.09551358]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 2112. State = [[-0.33238712 -0.01905409]]. Action = [[0.06534822 0.02286557 0.         0.11469626]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 2113. State = [[-0.3324195  -0.01811823]]. Action = [[-0.02308487 -0.01244704  0.         -0.14909446]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 2114. State = [[-0.32884288 -0.01961099]]. Action = [[ 0.09533017 -0.03592153  0.         -0.88992965]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 2115. State = [[-0.3296884  -0.01806958]]. Action = [[-0.07693734  0.04620562  0.         -0.46810108]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 2116. State = [[-0.33202013 -0.02010597]]. Action = [[ 0.00347662 -0.07255737  0.          0.07065272]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 2117. State = [[-0.33186284 -0.01785475]]. Action = [[0.01467972 0.08567863 0.         0.35720456]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 2118. State = [[-0.3316311  -0.01960625]]. Action = [[ 0.00585578 -0.09286403  0.         -0.04784703]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 2119. State = [[-0.33593473 -0.02389015]]. Action = [[-0.09477379 -0.03124996  0.         -0.55404705]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 2120. State = [[-0.33553803 -0.02092371]]. Action = [[ 0.07323035  0.08718615  0.         -0.9304992 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 2121. State = [[-0.33077154 -0.02078989]]. Action = [[ 0.06367987 -0.04908993  0.          0.78206086]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 2122. State = [[-0.32470238 -0.02127124]]. Action = [[0.08441461 0.01780629 0.         0.22480106]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 2123. State = [[-0.31780323 -0.02066281]]. Action = [[ 0.08699032  0.00478058  0.         -0.33193886]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 2124. State = [[-0.3141866  -0.02502955]]. Action = [[ 0.00821754 -0.08636449  0.         -0.23874408]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 2125. State = [[-0.30802283 -0.03011032]]. Action = [[ 0.09372688 -0.03962754  0.         -0.913983  ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 2126. State = [[-0.30138454 -0.02850672]]. Action = [[ 0.05452322  0.0729954   0.         -0.5638829 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 2127. State = [[-0.30097902 -0.0282872 ]]. Action = [[-0.05427487 -0.0240159   0.         -0.9374522 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 2128. State = [[-0.30259272 -0.02551619]]. Action = [[-0.03319732  0.07911891  0.         -0.6538142 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 2129. State = [[-0.30618268 -0.02573477]]. Action = [[-0.07599311 -0.04579332  0.         -0.02388436]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 2130. State = [[-0.30967638 -0.02328223]]. Action = [[-0.04745347  0.07517277  0.         -0.63625777]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 2131. State = [[-0.31222376 -0.01931248]]. Action = [[-0.03066739  0.02869876  0.         -0.676008  ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 2132. State = [[-0.3125348 -0.0154497]]. Action = [[ 0.01332468  0.04298723  0.         -0.3606305 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 2133. State = [[-0.30890998 -0.01380475]]. Action = [[ 0.07746249 -0.01293214  0.          0.0382117 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 2134. State = [[-0.3102977  -0.01371745]]. Action = [[-0.07080575 -0.00880837  0.         -0.69494534]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 2135. State = [[-0.30848128 -0.01341505]]. Action = [[ 0.08996973 -0.00314257  0.          0.19839525]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 2136. State = [[-0.30206078 -0.00853893]]. Action = [[ 0.09279763  0.08647626  0.         -0.71106434]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 2137. State = [[-0.30173925 -0.00680621]]. Action = [[-0.04521244 -0.02993499  0.         -0.18622994]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 2138. State = [[-0.30706072 -0.00411968]]. Action = [[-0.08439156  0.05644303  0.          0.5085895 ]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 2139. State = [[-0.31043234 -0.00236792]]. Action = [[-0.0152363  -0.0118679   0.         -0.11124778]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 2140. State = [[-0.3087471  0.0013932]]. Action = [[ 0.0550847   0.06416243  0.         -0.6278511 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 2141. State = [[-0.30969256  0.00408329]]. Action = [[-0.03819413 -0.00144166  0.         -0.36425745]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 2142. State = [[-0.30734742  0.00228276]]. Action = [[ 0.07577965 -0.0532153   0.         -0.93130213]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 2143. State = [[-0.3069976   0.00174866]]. Action = [[-0.03742772  0.01066178  0.          0.06770241]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 2144. State = [[-0.31122395  0.00715067]]. Action = [[-0.0678165   0.09497831  0.         -0.82151425]]. Reward = [0.]
Curr episode timestep = 102
Current timestep = 2145. State = [[-0.31732488  0.00665616]]. Action = [[-0.08397196 -0.08333803  0.         -0.39699107]]. Reward = [0.]
Curr episode timestep = 103
Current timestep = 2146. State = [[-0.31795147  0.00675143]]. Action = [[ 0.04243893  0.038813    0.         -0.34543985]]. Reward = [0.]
Curr episode timestep = 104
Current timestep = 2147. State = [[-0.3124573   0.00420322]]. Action = [[ 0.09711378 -0.07790867  0.         -0.37889725]]. Reward = [0.]
Curr episode timestep = 105
Current timestep = 2148. State = [[-0.30738732 -0.0014243 ]]. Action = [[ 0.0435502  -0.06578729  0.         -0.56967   ]]. Reward = [0.]
Curr episode timestep = 106
Current timestep = 2149. State = [[-0.30103046 -0.00606485]]. Action = [[ 0.08820394 -0.04084895  0.         -0.21761596]]. Reward = [0.]
Curr episode timestep = 107
Current timestep = 2150. State = [[-0.29553187 -0.01284086]]. Action = [[ 0.03928607 -0.09099867  0.         -0.5422611 ]]. Reward = [0.]
Curr episode timestep = 108
Current timestep = 2151. State = [[-0.29057884 -0.01881862]]. Action = [[ 0.04673193 -0.03848821  0.          0.29524255]]. Reward = [0.]
Curr episode timestep = 109
Current timestep = 2152. State = [[-0.2875068  -0.01883879]]. Action = [[ 0.00589919  0.05777342  0.         -0.64473385]]. Reward = [0.]
Curr episode timestep = 110
Current timestep = 2153. State = [[-0.28566748 -0.02214832]]. Action = [[ 0.00753613 -0.0675486   0.         -0.55045956]]. Reward = [0.]
Curr episode timestep = 111
Current timestep = 2154. State = [[-0.2857331  -0.02164568]]. Action = [[-0.03049253  0.07846478  0.         -0.7634292 ]]. Reward = [0.]
Curr episode timestep = 112
Current timestep = 2155. State = [[-0.28753164 -0.0176693 ]]. Action = [[-0.03484587  0.05667765  0.          0.23177171]]. Reward = [0.]
Curr episode timestep = 113
Current timestep = 2156. State = [[-0.2884883 -0.0168315]]. Action = [[-0.0062729  -0.00933894  0.         -0.60255   ]]. Reward = [0.]
Curr episode timestep = 114
Current timestep = 2157. State = [[-0.29285124 -0.02112743]]. Action = [[-0.09311861 -0.07686237  0.          0.7634542 ]]. Reward = [0.]
Curr episode timestep = 115
Current timestep = 2158. State = [[-0.29419187 -0.02188709]]. Action = [[ 0.01948453  0.03672133  0.         -0.08524883]]. Reward = [0.]
Curr episode timestep = 116
Current timestep = 2159. State = [[-0.29530945 -0.01688908]]. Action = [[-0.02758101  0.08033981  0.         -0.8140987 ]]. Reward = [0.]
Curr episode timestep = 117
Current timestep = 2160. State = [[-0.2954539  -0.00934859]]. Action = [[0.02772719 0.09241121 0.         0.14963341]]. Reward = [0.]
Curr episode timestep = 118
Current timestep = 2161. State = [[-0.2960768  -0.00991322]]. Action = [[-0.01058169 -0.09105128  0.          0.38549876]]. Reward = [0.]
Curr episode timestep = 119
Current timestep = 2162. State = [[-0.29279727 -0.01621439]]. Action = [[ 0.0822253  -0.08581891  0.         -0.16717207]]. Reward = [0.]
Curr episode timestep = 120
Current timestep = 2163. State = [[-0.2882573  -0.02206397]]. Action = [[ 0.04742987 -0.0637106   0.         -0.93576914]]. Reward = [0.]
Curr episode timestep = 121
Current timestep = 2164. State = [[-0.2818077 -0.0234995]]. Action = [[0.0961991  0.01968386 0.         0.17942631]]. Reward = [0.]
Curr episode timestep = 122
Current timestep = 2165. State = [[-0.28004155 -0.02033796]]. Action = [[-0.02930433  0.06485318  0.         -0.70179856]]. Reward = [0.]
Curr episode timestep = 123
Current timestep = 2166. State = [[-0.28023875 -0.02101083]]. Action = [[ 0.00570136 -0.04403832  0.         -0.8633449 ]]. Reward = [0.]
Curr episode timestep = 124
Current timestep = 2167. State = [[-0.27592444 -0.01859519]]. Action = [[ 0.08184064  0.079652    0.         -0.38276267]]. Reward = [0.]
Curr episode timestep = 125
Current timestep = 2168. State = [[-0.27173775 -0.01323122]]. Action = [[ 0.04035539  0.06385357  0.         -0.77685463]]. Reward = [0.]
Curr episode timestep = 126
Current timestep = 2169. State = [[-0.26722068 -0.01005667]]. Action = [[0.06764794 0.01996763 0.         0.07829618]]. Reward = [0.]
Curr episode timestep = 127
Current timestep = 2170. State = [[-0.2674576  -0.01045018]]. Action = [[-0.05388566 -0.0282542   0.         -0.36675018]]. Reward = [0.]
Curr episode timestep = 128
Current timestep = 2171. State = [[-0.2699607  -0.00921023]]. Action = [[-0.03471743  0.03194966  0.         -0.52636886]]. Reward = [0.]
Curr episode timestep = 129
Current timestep = 2172. State = [[-0.26975513 -0.0091201 ]]. Action = [[ 0.01492736 -0.02802916  0.         -0.56943285]]. Reward = [0.]
Curr episode timestep = 130
Current timestep = 2173. State = [[-0.26449683 -0.0098856 ]]. Action = [[ 0.08839222 -0.01039593  0.         -0.60230196]]. Reward = [0.]
Curr episode timestep = 131
Current timestep = 2174. State = [[-0.2624152  -0.00884901]]. Action = [[-0.02309817  0.02050972  0.         -0.81782746]]. Reward = [0.]
Curr episode timestep = 132
Current timestep = 2175. State = [[-0.26095867 -0.0063531 ]]. Action = [[ 0.02722877  0.03099421  0.         -0.5167768 ]]. Reward = [0.]
Curr episode timestep = 133
Current timestep = 2176. State = [[-0.25795934 -0.00841488]]. Action = [[ 0.03201694 -0.06736086  0.         -0.37892544]]. Reward = [0.]
Curr episode timestep = 134
Current timestep = 2177. State = [[-0.25657544 -0.01312799]]. Action = [[-0.01276568 -0.05783356  0.         -0.31364453]]. Reward = [0.]
Curr episode timestep = 135
Current timestep = 2178. State = [[-0.2516498  -0.01941901]]. Action = [[ 0.08111537 -0.08447304  0.         -0.26838726]]. Reward = [0.]
Curr episode timestep = 136
Current timestep = 2179. State = [[-0.24324316 -0.02457426]]. Action = [[ 0.0968452  -0.03635852  0.         -0.81297296]]. Reward = [0.]
Curr episode timestep = 137
Current timestep = 2180. State = [[-0.23520672 -0.02321837]]. Action = [[ 0.07742671  0.07333503  0.         -0.87400013]]. Reward = [0.]
Curr episode timestep = 138
Current timestep = 2181. State = [[-0.23416376 -0.02338288]]. Action = [[-0.0569014  -0.02361341  0.         -0.604442  ]]. Reward = [0.]
Curr episode timestep = 139
Current timestep = 2182. State = [[-0.23253788 -0.02705864]]. Action = [[ 0.03416222 -0.04253149  0.         -0.61439455]]. Reward = [0.]
Curr episode timestep = 140
Current timestep = 2183. State = [[-0.22920689 -0.02916777]]. Action = [[0.02348487 0.00349327 0.         0.4293295 ]]. Reward = [0.]
Curr episode timestep = 141
Current timestep = 2184. State = [[-0.2284444  -0.03021723]]. Action = [[-0.02366051 -0.00374608  0.         -0.16746372]]. Reward = [0.]
Curr episode timestep = 142
Current timestep = 2185. State = [[-0.22451288 -0.03032615]]. Action = [[ 0.07193484  0.01483536  0.         -0.35115135]]. Reward = [0.]
Curr episode timestep = 143
Current timestep = 2186. State = [[-0.21937785 -0.02619163]]. Action = [[ 0.04933598  0.08449572  0.         -0.10660696]]. Reward = [0.]
Curr episode timestep = 144
Current timestep = 2187. State = [[-0.21311115 -0.0258587 ]]. Action = [[ 0.08626503 -0.03951899  0.         -0.76060754]]. Reward = [0.]
Curr episode timestep = 145
Current timestep = 2188. State = [[-0.20514755 -0.02398565]]. Action = [[ 0.09819057  0.06138488  0.         -0.67590255]]. Reward = [0.]
Curr episode timestep = 146
Current timestep = 2189. State = [[-0.20329548 -0.01942271]]. Action = [[-0.03873888  0.05637778  0.         -0.46382678]]. Reward = [0.]
Curr episode timestep = 147
Current timestep = 2190. State = [[-0.2038946  -0.01290186]]. Action = [[-0.00449599  0.08641192  0.         -0.66079605]]. Reward = [0.]
Curr episode timestep = 148
Current timestep = 2191. State = [[-0.20252383 -0.00579724]]. Action = [[ 0.0271632  0.0704144  0.        -0.5667987]]. Reward = [0.]
Curr episode timestep = 149
Current timestep = 2192. State = [[-0.2035245  -0.00170453]]. Action = [[-0.04001564  0.01040085  0.         -0.63517505]]. Reward = [0.]
Curr episode timestep = 150
Current timestep = 2193. State = [[-0.20256574  0.00198001]]. Action = [[ 0.03451387  0.03298593  0.         -0.799512  ]]. Reward = [0.]
Curr episode timestep = 151
Current timestep = 2194. State = [[-0.2056808   0.00204372]]. Action = [[-0.09474287 -0.05321267  0.          0.31335926]]. Reward = [0.]
Curr episode timestep = 152
Current timestep = 2195. State = [[-0.20382349  0.00241251]]. Action = [[ 0.0867132   0.00530169  0.         -0.7979322 ]]. Reward = [0.]
Curr episode timestep = 153
Current timestep = 2196. State = [[-0.19636874  0.0002884 ]]. Action = [[ 0.08880232 -0.06709364  0.         -0.48236233]]. Reward = [0.]
Curr episode timestep = 154
Current timestep = 2197. State = [[-0.19262196 -0.00533449]]. Action = [[-0.0063969  -0.08572721  0.         -0.01263493]]. Reward = [0.]
Curr episode timestep = 155
Current timestep = 2198. State = [[-0.19421211 -0.0029516 ]]. Action = [[-0.06694296  0.09640493  0.         -0.13135654]]. Reward = [0.]
Curr episode timestep = 156
Current timestep = 2199. State = [[-0.19233435 -0.00384386]]. Action = [[ 0.04962897 -0.07718568  0.         -0.75834894]]. Reward = [0.]
Curr episode timestep = 157
Current timestep = 2200. State = [[-0.18824199 -0.00954423]]. Action = [[ 0.02626524 -0.0693932   0.         -0.4521984 ]]. Reward = [0.]
Curr episode timestep = 158
Current timestep = 2201. State = [[-0.18244494 -0.00831229]]. Action = [[ 0.0748674   0.07902432  0.         -0.92652905]]. Reward = [0.]
Curr episode timestep = 159
Current timestep = 2202. State = [[-0.18303925 -0.00424603]]. Action = [[-0.08076999  0.04558543  0.         -0.6970633 ]]. Reward = [0.]
Curr episode timestep = 160
Current timestep = 2203. State = [[-0.18068416  0.0003469 ]]. Action = [[ 0.0890977  0.0634682  0.        -0.6134145]]. Reward = [0.]
Curr episode timestep = 161
Current timestep = 2204. State = [[-0.1732777 -0.0005924]]. Action = [[ 0.09571633 -0.05827775  0.         -0.7096316 ]]. Reward = [0.]
Curr episode timestep = 162
Current timestep = 2205. State = [[-0.1730474   0.00208538]]. Action = [[-0.07528076  0.08678365  0.         -0.9571754 ]]. Reward = [0.]
Curr episode timestep = 163
Current timestep = 2206. State = [[-0.17281766  0.00227268]]. Action = [[ 0.03845853 -0.05155584  0.         -0.78219724]]. Reward = [0.]
Curr episode timestep = 164
Current timestep = 2207. State = [[-0.16770299 -0.00121126]]. Action = [[ 0.07693308 -0.04565417  0.         -0.52985597]]. Reward = [0.]
Curr episode timestep = 165
Current timestep = 2208. State = [[-0.16141501 -0.00079749]]. Action = [[ 0.07380272  0.03956323  0.         -0.62751937]]. Reward = [0.]
Curr episode timestep = 166
Current timestep = 2209. State = [[-0.15435871  0.00328398]]. Action = [[ 0.09112491  0.06420947  0.         -0.434031  ]]. Reward = [0.]
Curr episode timestep = 167
Current timestep = 2210. State = [[-0.14933923  0.0042477 ]]. Action = [[ 0.03609248 -0.01709572  0.         -0.6719795 ]]. Reward = [0.]
Curr episode timestep = 168
Current timestep = 2211. State = [[-0.1495828   0.00853839]]. Action = [[-0.04930481  0.09279876  0.         -0.5671414 ]]. Reward = [0.]
Curr episode timestep = 169
Current timestep = 2212. State = [[-0.14626752  0.01234092]]. Action = [[ 0.07870101  0.01299632  0.         -0.3354249 ]]. Reward = [0.]
Curr episode timestep = 170
Current timestep = 2213. State = [[-0.14263144  0.0158808 ]]. Action = [[ 0.01095639  0.04810704  0.         -0.73684263]]. Reward = [0.]
Curr episode timestep = 171
Current timestep = 2214. State = [[-0.14306927  0.01585696]]. Action = [[-0.04266327 -0.04738275  0.         -0.97073025]]. Reward = [0.]
Curr episode timestep = 172
Current timestep = 2215. State = [[-0.14251435  0.01806519]]. Action = [[ 0.00579424  0.05007877  0.         -0.6113547 ]]. Reward = [0.]
Curr episode timestep = 173
Current timestep = 2216. State = [[-0.13770035  0.01668009]]. Action = [[ 0.07134337 -0.07519837  0.         -0.40066302]]. Reward = [0.]
Curr episode timestep = 174
Current timestep = 2217. State = [[-0.13600232  0.0125629 ]]. Action = [[-0.03802239 -0.05236115  0.         -0.406664  ]]. Reward = [0.]
Curr episode timestep = 175
Current timestep = 2218. State = [[-0.13198572  0.01221119]]. Action = [[ 0.07169092  0.01979531  0.         -0.9319174 ]]. Reward = [0.]
Curr episode timestep = 176
Current timestep = 2219. State = [[-0.12510747  0.01382205]]. Action = [[ 0.07559571  0.02214216  0.         -0.8566496 ]]. Reward = [0.]
Curr episode timestep = 177
Current timestep = 2220. State = [[-0.11693097  0.01137128]]. Action = [[ 0.09724427 -0.05885322  0.         -0.82310045]]. Reward = [0.]
Curr episode timestep = 178
Current timestep = 2221. State = [[-0.11086749  0.01326279]]. Action = [[ 0.03794719  0.08154089  0.         -0.7531559 ]]. Reward = [0.]
Curr episode timestep = 179
Current timestep = 2222. State = [[-0.10746652  0.01088477]]. Action = [[ 0.01453773 -0.08712104  0.         -0.37763035]]. Reward = [0.]
Curr episode timestep = 180
Current timestep = 2223. State = [[-0.10317575  0.0092597 ]]. Action = [[ 0.04136372  0.02472296  0.         -0.5770055 ]]. Reward = [0.]
Curr episode timestep = 181
Current timestep = 2224. State = [[-0.09640225  0.00947591]]. Action = [[ 0.08102138  0.00267966  0.         -0.8572103 ]]. Reward = [0.]
Curr episode timestep = 182
Current timestep = 2225. State = [[-0.08872628  0.00541377]]. Action = [[ 0.07545086 -0.07176943  0.         -0.84207517]]. Reward = [0.]
Curr episode timestep = 183
Current timestep = 2226. State = [[-0.08021294 -0.00107034]]. Action = [[ 0.08893973 -0.06972466  0.         -0.63687897]]. Reward = [0.]
Curr episode timestep = 184
Current timestep = 2227. State = [[-0.07135892 -0.0062153 ]]. Action = [[ 0.08509371 -0.03332506  0.         -0.61413276]]. Reward = [0.]
Curr episode timestep = 185
Current timestep = 2228. State = [[-0.06215947 -0.00616506]]. Action = [[ 0.09488861  0.05441821  0.         -0.31447124]]. Reward = [0.]
Curr episode timestep = 186
Current timestep = 2229. State = [[-0.05281655 -0.00497312]]. Action = [[ 0.09626823  0.02278931  0.         -0.88841796]]. Reward = [0.]
Curr episode timestep = 187
Current timestep = 2230. State = [[-0.23136944 -0.04203728]]. Action = [[ 0.08029462 -0.08210517  0.         -0.23511183]]. Reward = [100.]
Curr episode timestep = 188
Current timestep = 2231. State = [[-0.22599545 -0.03927947]]. Action = [[ 0.09415727  0.04367609  0.         -0.75159234]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2232. State = [[-0.2216603  -0.03688956]]. Action = [[ 0.02778485 -0.00218058  0.         -0.29428726]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2233. State = [[-0.21702239 -0.03305637]]. Action = [[ 0.0654371   0.05532639  0.         -0.6386783 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2234. State = [[-0.21071275 -0.02718231]]. Action = [[ 0.08269496  0.06386106  0.         -0.63412666]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2235. State = [[-0.20877306 -0.0197276 ]]. Action = [[-0.0182956   0.08767661  0.          0.31011868]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2236. State = [[-0.20821577 -0.01106222]]. Action = [[ 0.01370072  0.09239855  0.         -0.8658083 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2237. State = [[-0.2032815  -0.00454739]]. Action = [[ 0.09164404  0.03924241  0.         -0.25840986]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2238. State = [[-0.19729085  0.00054365]]. Action = [[ 0.06479644  0.040272    0.         -0.5313117 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2239. State = [[-0.1913596  0.0035805]]. Action = [[0.06959077 0.00098623 0.         0.11455548]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2240. State = [[-0.18916541  0.0044395 ]]. Action = [[-0.01974358 -0.01792559  0.         -0.8163457 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2241. State = [[-0.18489863  0.00957244]]. Action = [[ 0.0690992   0.08170106  0.         -0.6793896 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2242. State = [[-0.17722155  0.01689533]]. Action = [[ 0.09588421  0.0691141   0.         -0.84847677]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2243. State = [[-0.17499138  0.01907143]]. Action = [[-0.04233394 -0.03002443  0.          0.0937531 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2244. State = [[-0.17029288  0.02435302]]. Action = [[ 0.09261458  0.09112703  0.         -0.8404304 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2245. State = [[-0.1631178   0.03038972]]. Action = [[ 0.0696478   0.03964     0.         -0.05116868]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2246. State = [[-0.16025037  0.03307594]]. Action = [[-0.01564831 -0.00140378  0.         -0.8003663 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2247. State = [[-0.1553732   0.03440239]]. Action = [[ 0.06881686 -0.0029619   0.         -0.62221336]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2248. State = [[-0.1474868   0.03492779]]. Action = [[ 0.08273686 -0.01043172  0.         -0.4844203 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2249. State = [[-0.14214954  0.03468994]]. Action = [[ 0.01577423 -0.01591448  0.          0.31303453]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2250. State = [[-0.1409753   0.03700763]]. Action = [[-0.03704178  0.03990368  0.         -0.9491517 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2251. State = [[-0.14210859  0.04023757]]. Action = [[-0.05076834  0.02155203  0.         -0.49980783]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2252. State = [[-0.1453781  0.0460594]]. Action = [[-0.07774691  0.07715289  0.         -0.8104626 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2253. State = [[-0.14173232  0.04646959]]. Action = [[ 0.09881867 -0.06539515  0.         -0.2605313 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2254. State = [[-0.13398066  0.04790892]]. Action = [[0.07479676 0.04722729 0.         0.03486896]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2255. State = [[-0.12915723  0.04575607]]. Action = [[ 0.02041759 -0.08215322  0.         -0.8173077 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2256. State = [[-0.12218435  0.04400947]]. Action = [[ 0.09183014  0.00671641  0.         -0.9532897 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2257. State = [[-0.11934635  0.04741289]]. Action = [[-0.02901634  0.06615668  0.         -0.28833473]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2258. State = [[-0.11424629  0.04947632]]. Action = [[ 0.09221601 -0.00180379  0.         -0.06938541]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2259. State = [[-0.10585751  0.05030548]]. Action = [[ 0.094294    0.01649328  0.         -0.39611828]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2260. State = [[-0.09786659  0.04792908]]. Action = [[ 0.07762311 -0.0524709   0.         -0.88103926]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2261. State = [[-0.09250909  0.04964269]]. Action = [[ 0.02822176  0.07230801  0.         -0.58069664]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 2262. State = [[-0.08646763  0.04798704]]. Action = [[ 0.07029558 -0.06966968  0.         -0.00830215]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2263. State = [[-0.07893189  0.04588527]]. Action = [[ 0.07289504  0.00495616  0.         -0.6240325 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2264. State = [[-0.07396075  0.04455411]]. Action = [[ 0.01596592 -0.01770069  0.          0.03805971]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 2265. State = [[-0.06930257  0.0413995 ]]. Action = [[ 0.03695977 -0.04492107  0.         -0.02576834]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 2266. State = [[-0.06823329  0.03556858]]. Action = [[-0.0545441  -0.08126249  0.         -0.676842  ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2267. State = [[-0.0627023  0.0335593]]. Action = [[ 0.09850902  0.02204856  0.         -0.86985517]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2268. State = [[-0.0535048   0.03040052]]. Action = [[ 0.08290984 -0.05523536  0.         -0.45329714]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2269. State = [[-0.33899963 -0.0138809 ]]. Action = [[ 0.0994109  -0.00227538  0.         -0.9763799 ]]. Reward = [100.]
Curr episode timestep = 38
Current timestep = 2270. State = [[-0.34134132 -0.01383131]]. Action = [[ 0.01410556  0.09103037  0.         -0.7757794 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2271. State = [[-0.34466898 -0.00972928]]. Action = [[-0.05801738  0.04278546  0.         -0.5994886 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2272. State = [[-0.3474004  -0.01214363]]. Action = [[-0.00628956 -0.07021089  0.         -0.21505708]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2273. State = [[-0.3481465  -0.01853504]]. Action = [[ 0.00297103 -0.0754832   0.          0.32564497]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2274. State = [[-0.35315827 -0.02141341]]. Action = [[-0.09612027  0.00768501  0.         -0.04278272]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2275. State = [[-0.35808903 -0.01762827]]. Action = [[-0.02660601  0.08924753  0.         -0.85430324]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2276. State = [[-0.36111775 -0.01066914]]. Action = [[-0.0083481   0.08738346  0.          0.24594069]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2277. State = [[-0.36239934 -0.00675529]]. Action = [[ 0.02247308  0.01269891  0.         -0.05457503]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2278. State = [[-0.36621642 -0.0050979 ]]. Action = [[-0.05068541  0.00993148  0.          0.86225224]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2279. State = [[-0.3727142 -0.0065687]]. Action = [[-0.07124217 -0.04571505  0.         -0.64216775]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2280. State = [[-0.37361935 -0.00910778]]. Action = [[ 0.05707768 -0.02932498  0.         -0.6990408 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2281. State = [[-0.37329307 -0.00991219]]. Action = [[ 0.          0.          0.         -0.62309337]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2282. State = [[-0.37116832 -0.0087472 ]]. Action = [[ 0.06225272  0.02160163  0.         -0.8875168 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2283. State = [[-0.37188447 -0.00867707]]. Action = [[-0.03460099 -0.01442973  0.          0.08375132]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2284. State = [[-0.36928016 -0.00647046]]. Action = [[ 0.08973289  0.04929746  0.         -0.7652278 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2285. State = [[-0.36976802 -0.00834259]]. Action = [[-0.06136573 -0.07119788  0.          0.05013633]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2286. State = [[-0.3745348 -0.0093642]]. Action = [[-0.06040379  0.02438276  0.         -0.87880677]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2287. State = [[-0.3765584  -0.01099053]]. Action = [[ 0.00270865 -0.04012391  0.         -0.47616464]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2288. State = [[-0.3769585  -0.01243725]]. Action = [[ 0.          0.          0.         -0.36909026]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2289. State = [[-0.37428683 -0.0123144 ]]. Action = [[ 0.06444476  0.00793048  0.         -0.38867807]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2290. State = [[-0.37296137 -0.01218628]]. Action = [[0.         0.         0.         0.32121682]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2291. State = [[-0.37289852 -0.01427911]]. Action = [[ 0.00308955 -0.03779243  0.         -0.54573333]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2292. State = [[-0.3727913  -0.01579119]]. Action = [[ 0.          0.          0.         -0.84841084]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2293. State = [[-0.37276536 -0.01622368]]. Action = [[ 0.          0.          0.         -0.46661937]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2294. State = [[-0.37275043 -0.01653742]]. Action = [[0.         0.         0.         0.53197074]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2295. State = [[-0.368564   -0.01492911]]. Action = [[0.08902154 0.03440017 0.         0.77521634]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2296. State = [[-0.3675766 -0.0166502]]. Action = [[-0.03361191 -0.05474144  0.          0.58424497]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2297. State = [[-0.36820102 -0.01853982]]. Action = [[0.         0.         0.         0.26128113]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2298. State = [[-0.36762127 -0.01654332]]. Action = [[ 0.01163129  0.04623412  0.         -0.41441393]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2299. State = [[-0.36386004 -0.01126087]]. Action = [[ 0.07608514  0.07336137  0.         -0.5921367 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2300. State = [[-0.36360657 -0.00646816]]. Action = [[-0.03056808  0.0383748   0.         -0.82064545]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 2301. State = [[-0.3648301  -0.00919725]]. Action = [[-0.00550728 -0.09146963  0.         -0.811875  ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2302. State = [[-0.36825246 -0.01526051]]. Action = [[-0.07345645 -0.06916615  0.         -0.18173826]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2303. State = [[-0.3689752  -0.01951707]]. Action = [[ 0.01677048 -0.03342652  0.          0.1608839 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 2304. State = [[-0.36784598 -0.02085993]]. Action = [[ 0.00777827  0.00536303  0.         -0.1879111 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 2305. State = [[-0.365206   -0.02323842]]. Action = [[ 0.04351041 -0.03943569  0.         -0.5502806 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2306. State = [[-0.36521265 -0.02437892]]. Action = [[-0.0322964   0.01304495  0.         -0.639076  ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2307. State = [[-0.36822343 -0.0284745 ]]. Action = [[-0.05214697 -0.07280025  0.         -0.82862324]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2308. State = [[-0.36785343 -0.02716019]]. Action = [[ 0.04051556  0.09570695  0.         -0.72629   ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 2309. State = [[-0.37005535 -0.02829855]]. Action = [[-0.0704753  -0.08461643  0.         -0.92484856]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2310. State = [[-0.36843005 -0.03206264]]. Action = [[ 0.07897706 -0.01729524  0.         -0.7521101 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 2311. State = [[-0.3654089  -0.03478631]]. Action = [[ 0.01638776 -0.02905853  0.         -0.57089525]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2312. State = [[-0.36352083 -0.0376185 ]]. Action = [[ 0.02052221 -0.02385982  0.          0.10509408]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 2313. State = [[-0.362098 -0.040771]]. Action = [[ 0.00901804 -0.03109013  0.          0.49572992]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 2314. State = [[-0.3654062  -0.04500229]]. Action = [[-0.08560427 -0.04347905  0.         -0.54808986]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 2315. State = [[-0.3705025  -0.04550783]]. Action = [[-0.06405987  0.04221808  0.         -0.92956924]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 2316. State = [[-0.37130222 -0.04262847]]. Action = [[ 0.02738602  0.04830743  0.         -0.9165145 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 2317. State = [[-0.3692454  -0.04021227]]. Action = [[ 0.03892755  0.01742387  0.         -0.84081554]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 2318. State = [[-0.3642439  -0.04308616]]. Action = [[ 0.09332512 -0.07514696  0.         -0.41633928]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 2319. State = [[-0.36217657 -0.04580394]]. Action = [[-0.00766344 -0.00926432  0.         -0.40284765]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 2320. State = [[-0.36062536 -0.05126015]]. Action = [[ 0.03548928 -0.09385687  0.          0.6503804 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 2321. State = [[-0.35579798 -0.05877718]]. Action = [[ 0.073894   -0.08013549  0.         -0.720328  ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 2322. State = [[-0.35338476 -0.0652756 ]]. Action = [[-0.00740223 -0.05815387  0.          0.32071388]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 2323. State = [[-0.35087472 -0.07321101]]. Action = [[ 0.03598156 -0.09138462  0.         -0.8505332 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 2324. State = [[-0.34725657 -0.07896464]]. Action = [[ 0.03306486 -0.02346404  0.         -0.8243519 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 2325. State = [[-0.34855646 -0.08656405]]. Action = [[-0.06818371 -0.09400275  0.         -0.54385686]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 2326. State = [[-0.3517179  -0.09573223]]. Action = [[-0.04436773 -0.07624488  0.          0.13606298]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 2327. State = [[-0.3568608  -0.09692089]]. Action = [[-0.09293516  0.08041554  0.         -0.7869849 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 2328. State = [[-0.3556487  -0.09360154]]. Action = [[ 0.08766188  0.05258077  0.         -0.54055214]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 2329. State = [[-0.35166284 -0.08763774]]. Action = [[ 0.02002716  0.09865946  0.         -0.13828295]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 2330. State = [[-0.34639418 -0.07967658]]. Action = [[ 0.08257265  0.09293395  0.         -0.5057536 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 2331. State = [[-0.34765372 -0.07331301]]. Action = [[-0.08131681  0.05359843  0.         -0.6841321 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 2332. State = [[-0.35113916 -0.07151569]]. Action = [[-0.01675119 -0.01339612  0.         -0.7564262 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 2333. State = [[-0.35195524 -0.06786481]]. Action = [[ 0.00955038  0.06001926  0.         -0.67011607]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 2334. State = [[-0.35205984 -0.06529125]]. Action = [[ 0.00864711 -0.01060097  0.          0.06545329]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 2335. State = [[-0.34835175 -0.06664519]]. Action = [[ 0.08565743 -0.05140425  0.          0.0462476 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 2336. State = [[-0.34205362 -0.06650955]]. Action = [[ 0.08736289  0.00531743  0.         -0.10299075]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 2337. State = [[-0.33627334 -0.06160367]]. Action = [[ 0.06604267  0.06959257  0.         -0.30737817]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 2338. State = [[-0.3313978  -0.05412075]]. Action = [[ 0.05891455  0.08087378  0.         -0.9760944 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 2339. State = [[-0.33048314 -0.04781424]]. Action = [[-0.01763408  0.04912526  0.         -0.65477896]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 2340. State = [[-0.33093482 -0.04362793]]. Action = [[-0.00408422  0.0272728   0.         -0.89848393]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 2341. State = [[-0.3316341 -0.0394149]]. Action = [[-0.01678984  0.04061308  0.         -0.7955574 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 2342. State = [[-0.33272597 -0.03326122]]. Action = [[-0.01740588  0.06981725  0.         -0.89494336]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 2343. State = [[-0.33427858 -0.02609206]]. Action = [[-0.02251762  0.0681104   0.         -0.7604127 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 2344. State = [[-0.33336115 -0.02664676]]. Action = [[ 0.02953327 -0.08892491  0.         -0.9553566 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 2345. State = [[-0.3315916  -0.02949858]]. Action = [[ 0.00790455 -0.03221963  0.         -0.7168029 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 2346. State = [[-0.3282795  -0.02782637]]. Action = [[ 0.04824949  0.03681997  0.         -0.7647171 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 2347. State = [[-0.32819808 -0.02876971]]. Action = [[-0.04083774 -0.05449833  0.         -0.92736095]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 2348. State = [[-0.33014813 -0.03390581]]. Action = [[-0.03916724 -0.07540504  0.          0.14626098]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 2349. State = [[-0.3258825  -0.03616384]]. Action = [[0.09432865 0.00605072 0.         0.32071447]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 2350. State = [[-0.32227707 -0.03391094]]. Action = [[ 0.00121544  0.04898331  0.         -0.9198848 ]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 2351. State = [[-0.3181713  -0.03697072]]. Action = [[ 0.06194336 -0.0845685   0.         -0.51055396]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 2352. State = [[-0.31681922 -0.03646633]]. Action = [[-0.03000182  0.07001751  0.         -0.6486139 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 2353. State = [[-0.3199201  -0.03123021]]. Action = [[-0.06131732  0.07375102  0.         -0.36546797]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 2354. State = [[-0.3171881  -0.02728903]]. Action = [[ 0.09743988  0.0285375   0.         -0.69315815]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 2355. State = [[-0.31603333 -0.02166406]]. Action = [[-0.03839556  0.0827236   0.         -0.10027593]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 2356. State = [[-0.3125012 -0.0229683]]. Action = [[ 0.09190335 -0.0915665   0.         -0.7756838 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 2357. State = [[-0.3102442  -0.02706687]]. Action = [[-0.02326908 -0.0382515   0.         -0.19646955]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 2358. State = [[-0.3057085  -0.03189985]]. Action = [[ 0.08312155 -0.07199074  0.         -0.12996376]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 2359. State = [[-0.29802263 -0.03318051]]. Action = [[ 0.0860701   0.02279341  0.         -0.53330564]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 2360. State = [[-0.29492563 -0.03770225]]. Action = [[-0.01752403 -0.09391221  0.         -0.09123856]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 2361. State = [[-0.29080084 -0.04024501]]. Action = [[ 0.05812255  0.02017076  0.         -0.8974875 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 2362. State = [[-0.28857055 -0.04416412]]. Action = [[-0.01986011 -0.06897159  0.         -0.36437863]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 2363. State = [[-0.2872781  -0.04530906]]. Action = [[ 0.00264372  0.03970755  0.         -0.5071967 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 2364. State = [[-0.28687027 -0.04299052]]. Action = [[-0.01844524  0.04328952  0.         -0.53490436]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 2365. State = [[-0.28349066 -0.04642893]]. Action = [[ 0.05992366 -0.0831527   0.          0.8281605 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 2366. State = [[-0.28329682 -0.04982875]]. Action = [[-0.0557037  -0.00434983  0.         -0.8496115 ]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 2367. State = [[-0.27989185 -0.0531654 ]]. Action = [[ 0.08519199 -0.04670095  0.         -0.01747131]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 2368. State = [[-0.27641115 -0.05130459]]. Action = [[ 0.00392774  0.07942715  0.         -0.9694013 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 2369. State = [[-0.27185926 -0.04760344]]. Action = [[ 0.07550708  0.03489507  0.         -0.8803603 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 2370. State = [[-0.27181983 -0.04927831]]. Action = [[-0.05646276 -0.05411511  0.         -0.6810389 ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 2371. State = [[-0.26807958 -0.04810959]]. Action = [[ 0.09779749  0.05756777  0.         -0.08303338]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 2372. State = [[-0.2674926  -0.04658325]]. Action = [[-0.05659292 -0.00336397  0.         -0.8927173 ]]. Reward = [0.]
Curr episode timestep = 102
Current timestep = 2373. State = [[-0.26392028 -0.04923594]]. Action = [[ 0.09587104 -0.05622898  0.         -0.7152331 ]]. Reward = [0.]
Curr episode timestep = 103
Current timestep = 2374. State = [[-0.25643775 -0.04920189]]. Action = [[0.08238555 0.03366634 0.         0.51715374]]. Reward = [0.]
Curr episode timestep = 104
Current timestep = 2375. State = [[-0.25595772 -0.04469109]]. Action = [[-0.06420784  0.07251113  0.         -0.38185358]]. Reward = [0.]
Curr episode timestep = 105
Current timestep = 2376. State = [[-0.25338626 -0.03844142]]. Action = [[ 0.08224738  0.07272189  0.         -0.4814633 ]]. Reward = [0.]
Curr episode timestep = 106
Current timestep = 2377. State = [[-0.24916865 -0.03833191]]. Action = [[ 0.03208675 -0.05767141  0.         -0.04241395]]. Reward = [0.]
Curr episode timestep = 107
Current timestep = 2378. State = [[-0.24884395 -0.03607874]]. Action = [[-0.03096788  0.06463767  0.          0.2932464 ]]. Reward = [0.]
Curr episode timestep = 108
Current timestep = 2379. State = [[-0.25243676 -0.03661512]]. Action = [[-0.07534326 -0.0602828   0.         -0.45584542]]. Reward = [0.]
Curr episode timestep = 109
Current timestep = 2380. State = [[-0.252052   -0.03897557]]. Action = [[ 0.03088058 -0.02411814  0.          0.31154966]]. Reward = [0.]
Curr episode timestep = 110
Current timestep = 2381. State = [[-0.24879688 -0.04252539]]. Action = [[ 0.03071255 -0.06116852  0.         -0.9689281 ]]. Reward = [0.]
Curr episode timestep = 111
Current timestep = 2382. State = [[-0.24314065 -0.04281683]]. Action = [[ 0.07767122  0.03073262  0.         -0.7725012 ]]. Reward = [0.]
Curr episode timestep = 112
Current timestep = 2383. State = [[-0.24139704 -0.04231007]]. Action = [[-0.02778804 -0.00482688  0.         -0.4825536 ]]. Reward = [0.]
Curr episode timestep = 113
Current timestep = 2384. State = [[-0.23678286 -0.04141176]]. Action = [[ 0.09320075  0.02179625  0.         -0.37899846]]. Reward = [0.]
Curr episode timestep = 114
Current timestep = 2385. State = [[-0.23369394 -0.03675369]]. Action = [[-0.00348969  0.08091039  0.         -0.2611624 ]]. Reward = [0.]
Curr episode timestep = 115
Current timestep = 2386. State = [[-0.22938798 -0.03555581]]. Action = [[ 0.0762665  -0.02831486  0.         -0.58747965]]. Reward = [0.]
Curr episode timestep = 116
Current timestep = 2387. State = [[-0.22771373 -0.03355641]]. Action = [[-0.02528165  0.05002782  0.         -0.8328055 ]]. Reward = [0.]
Curr episode timestep = 117
Current timestep = 2388. State = [[-0.22676605 -0.03201214]]. Action = [[ 0.01777856 -0.00461811  0.         -0.81584626]]. Reward = [0.]
Curr episode timestep = 118
Current timestep = 2389. State = [[-0.22796395 -0.02885315]]. Action = [[-0.04772967  0.05518719  0.          0.21180844]]. Reward = [0.]
Curr episode timestep = 119
Current timestep = 2390. State = [[-0.22515413 -0.03101435]]. Action = [[ 0.07432523 -0.08807953  0.         -0.4850024 ]]. Reward = [0.]
Curr episode timestep = 120
Current timestep = 2391. State = [[-0.22055265 -0.0312617 ]]. Action = [[0.03304674 0.03857849 0.         0.03726864]]. Reward = [0.]
Curr episode timestep = 121
Current timestep = 2392. State = [[-0.22141315 -0.02802068]]. Action = [[-0.05906048  0.03827994  0.         -0.93018144]]. Reward = [0.]
Curr episode timestep = 122
Current timestep = 2393. State = [[-0.22273372 -0.02275477]]. Action = [[-0.00974721  0.07080738  0.         -0.12034518]]. Reward = [0.]
Curr episode timestep = 123
Current timestep = 2394. State = [[-0.22172603 -0.01518534]]. Action = [[0.02347181 0.09003442 0.         0.01520598]]. Reward = [0.]
Curr episode timestep = 124
Current timestep = 2395. State = [[-0.22157475 -0.01522514]]. Action = [[-0.01209021 -0.08113098  0.          0.25332403]]. Reward = [0.]
Curr episode timestep = 125
Current timestep = 2396. State = [[-0.21715488 -0.01680449]]. Action = [[ 0.09081664 -0.00492149  0.         -0.69754386]]. Reward = [0.]
Curr episode timestep = 126
Current timestep = 2397. State = [[-0.211337   -0.01416272]]. Action = [[ 0.06261156  0.04544977  0.         -0.8774814 ]]. Reward = [0.]
Curr episode timestep = 127
Current timestep = 2398. State = [[-0.2056452  -0.01536338]]. Action = [[ 0.06871233 -0.06109579  0.         -0.21801871]]. Reward = [0.]
Curr episode timestep = 128
Current timestep = 2399. State = [[-0.19981071 -0.01216268]]. Action = [[ 0.06296647  0.09405916  0.         -0.35632908]]. Reward = [0.]
Curr episode timestep = 129
Current timestep = 2400. State = [[-0.19838095 -0.0091736 ]]. Action = [[-0.02424848 -0.00024895  0.         -0.19289243]]. Reward = [0.]
Curr episode timestep = 130
Current timestep = 2401. State = [[-0.19458447 -0.00460391]]. Action = [[ 0.0719182   0.07921951  0.         -0.539685  ]]. Reward = [0.]
Curr episode timestep = 131
Current timestep = 2402. State = [[-0.18915875 -0.00040818]]. Action = [[0.05601638 0.02467678 0.         0.12891972]]. Reward = [0.]
Curr episode timestep = 132
Current timestep = 2403. State = [[-0.18447077  0.00055413]]. Action = [[ 0.04254539 -0.01169115  0.         -0.39941084]]. Reward = [0.]
Curr episode timestep = 133
Current timestep = 2404. State = [[-0.17758708  0.00316691]]. Action = [[ 0.09125442  0.04506584  0.         -0.8644877 ]]. Reward = [0.]
Curr episode timestep = 134
Current timestep = 2405. State = [[-0.17465904  0.00905746]]. Action = [[-0.01769851  0.077146    0.         -0.2894553 ]]. Reward = [0.]
Curr episode timestep = 135
Current timestep = 2406. State = [[-0.1716349   0.01270671]]. Action = [[ 0.0458489   0.00581509  0.         -0.6289343 ]]. Reward = [0.]
Curr episode timestep = 136
Current timestep = 2407. State = [[-0.16924943  0.01837943]]. Action = [[-0.00086335  0.08430693  0.         -0.22696018]]. Reward = [0.]
Curr episode timestep = 137
Current timestep = 2408. State = [[-0.16769429  0.01962357]]. Action = [[ 0.0061699  -0.05321695  0.         -0.83811975]]. Reward = [0.]
Curr episode timestep = 138
Current timestep = 2409. State = [[-0.16786204  0.02218651]]. Action = [[-0.0376247   0.05155792  0.         -0.3193981 ]]. Reward = [0.]
Curr episode timestep = 139
Current timestep = 2410. State = [[-0.17174634  0.03012301]]. Action = [[-0.08438095  0.09745727  0.         -0.07983446]]. Reward = [0.]
Curr episode timestep = 140
Current timestep = 2411. State = [[-0.16937615  0.03583809]]. Action = [[ 0.09107948  0.01792333  0.         -0.778642  ]]. Reward = [0.]
Curr episode timestep = 141
Current timestep = 2412. State = [[-0.1669105  0.0397981]]. Action = [[-0.01858848  0.02916511  0.         -0.49070513]]. Reward = [0.]
Curr episode timestep = 142
Current timestep = 2413. State = [[-0.16217496  0.0431556 ]]. Action = [[ 0.08650158  0.01446418  0.         -0.70347893]]. Reward = [0.]
Curr episode timestep = 143
Current timestep = 2414. State = [[-0.15762065  0.04280067]]. Action = [[ 0.02287839 -0.045378    0.         -0.8088317 ]]. Reward = [0.]
Curr episode timestep = 144
Current timestep = 2415. State = [[-0.15219536  0.03788856]]. Action = [[ 0.0644057  -0.09293615  0.         -0.7444245 ]]. Reward = [0.]
Curr episode timestep = 145
Current timestep = 2416. State = [[-0.14394954  0.03393502]]. Action = [[ 0.09594173 -0.02883969  0.         -0.23202854]]. Reward = [0.]
Curr episode timestep = 146
Current timestep = 2417. State = [[-0.13678847  0.03174954]]. Action = [[ 0.05233973 -0.0177089   0.         -0.19145876]]. Reward = [0.]
Curr episode timestep = 147
Current timestep = 2418. State = [[-0.13238548  0.02854411]]. Action = [[ 0.01547615 -0.0410482   0.         -0.857772  ]]. Reward = [0.]
Curr episode timestep = 148
Current timestep = 2419. State = [[-0.12833817  0.025279  ]]. Action = [[ 0.0273102  -0.02441023  0.         -0.83656096]]. Reward = [0.]
Curr episode timestep = 149
Current timestep = 2420. State = [[-0.12558827  0.02252639]]. Action = [[-0.00445865 -0.02004746  0.         -0.7951344 ]]. Reward = [0.]
Curr episode timestep = 150
Current timestep = 2421. State = [[-0.11969605  0.01990786]]. Action = [[ 0.08118599 -0.01818216  0.         -0.92049617]]. Reward = [0.]
Curr episode timestep = 151
Current timestep = 2422. State = [[-0.11110016  0.01517085]]. Action = [[ 0.09245255 -0.05627574  0.         -0.5938642 ]]. Reward = [0.]
Curr episode timestep = 152
Current timestep = 2423. State = [[-0.10522764  0.01222944]]. Action = [[ 0.02621954  0.00577551  0.         -0.02757478]]. Reward = [0.]
Curr episode timestep = 153
Current timestep = 2424. State = [[-0.09964034  0.00992597]]. Action = [[ 0.0580451  -0.01845854  0.         -0.78280365]]. Reward = [0.]
Curr episode timestep = 154
Current timestep = 2425. State = [[-0.09331323  0.00525403]]. Action = [[ 0.05713695 -0.05339447  0.         -0.73964787]]. Reward = [0.]
Curr episode timestep = 155
Current timestep = 2426. State = [[-0.08522692  0.00186024]]. Action = [[ 0.09305235 -0.00423636  0.         -0.7886085 ]]. Reward = [0.]
Curr episode timestep = 156
Current timestep = 2427. State = [[-0.08087792 -0.00257832]]. Action = [[-0.00499105 -0.05325074  0.         -0.6190355 ]]. Reward = [0.]
Curr episode timestep = 157
Current timestep = 2428. State = [[-0.07536727 -0.01037048]]. Action = [[ 0.06959745 -0.09357576  0.         -0.76911855]]. Reward = [0.]
Curr episode timestep = 158
Current timestep = 2429. State = [[-0.06696328 -0.01915717]]. Action = [[ 0.08890406 -0.08086489  0.         -0.4821174 ]]. Reward = [0.]
Curr episode timestep = 159
Current timestep = 2430. State = [[-0.0578539  -0.02808415]]. Action = [[ 0.09012956 -0.08174874  0.         -0.49990726]]. Reward = [0.]
Curr episode timestep = 160
Current timestep = 2431. State = [[-0.05113218 -0.03593096]]. Action = [[ 0.0379753  -0.05408623  0.         -0.85219634]]. Reward = [0.]
Curr episode timestep = 161
Current timestep = 2432. State = [[-0.08354107  0.01836156]]. Action = [[ 0.0682388  -0.0423107   0.         -0.42408657]]. Reward = [100.]
Curr episode timestep = 162
Current timestep = 2433. State = [[-0.08359913  0.02525769]]. Action = [[ 0.06094494  0.06755017  0.         -0.34489286]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2434. State = [[-0.08013493  0.02618178]]. Action = [[ 0.09066414 -0.03523336  0.         -0.5636934 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2435. State = [[-0.07522226  0.02505089]]. Action = [[ 0.09191788 -0.00776997  0.         -0.74917483]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2436. State = [[-0.07584361  0.02190544]]. Action = [[-0.04788416 -0.05745038  0.         -0.74327993]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2437. State = [[-0.07377749  0.01919617]]. Action = [[ 0.0916222  -0.01645974  0.         -0.68125224]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2438. State = [[-0.06772108  0.01627629]]. Action = [[ 0.0882875  -0.03400131  0.         -0.24974275]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2439. State = [[-0.06136763  0.01782845]]. Action = [[ 0.08457286  0.07096451  0.         -0.6855364 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2440. State = [[-0.05437966  0.02302015]]. Action = [[ 0.09773887  0.08202114  0.         -0.8780309 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2441. State = [[-0.23571718  0.06169115]]. Action = [[ 0.07046061 -0.0512846   0.         -0.8045676 ]]. Reward = [100.]
Curr episode timestep = 8
Current timestep = 2442. State = [[-0.23493826  0.06909753]]. Action = [[-0.06233058  0.06840197  0.         -0.6607357 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2443. State = [[-0.23604466  0.06878987]]. Action = [[-0.01491799 -0.08201935  0.          0.7303065 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2444. State = [[-0.23113017  0.06982022]]. Action = [[ 0.08705702  0.04032657  0.         -0.82773256]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2445. State = [[-0.22455436  0.06956846]]. Action = [[ 0.05779035 -0.04510882  0.         -0.64388466]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2446. State = [[-0.21799485  0.06551951]]. Action = [[ 0.06202116 -0.06777191  0.         -0.59185266]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2447. State = [[-0.2102684   0.06517471]]. Action = [[ 0.08135348  0.03263576  0.         -0.86630577]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2448. State = [[-0.20418447  0.06685255]]. Action = [[ 0.03909204  0.02036615  0.         -0.23693293]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2449. State = [[-0.1974339   0.06749486]]. Action = [[ 0.07588456  0.00462435  0.         -0.24218553]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2450. State = [[-0.18901713  0.06882516]]. Action = [[ 0.0903414   0.03101071  0.         -0.8254647 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2451. State = [[-0.18035123  0.06961628]]. Action = [[ 0.08594335  0.00631038  0.         -0.8627847 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2452. State = [[-0.17101955  0.06606912]]. Action = [[ 0.09586781 -0.06671047  0.         -0.39732575]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2453. State = [[-0.16205     0.06137621]]. Action = [[ 0.07416733 -0.04235541  0.         -0.88335925]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2454. State = [[-0.15272315  0.05719357]]. Action = [[ 0.08795232 -0.0393222   0.         -0.5597601 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2455. State = [[-0.14660753  0.05715353]]. Action = [[ 0.01194236  0.04371393  0.         -0.18411058]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2456. State = [[-0.141254    0.05818782]]. Action = [[ 0.04112297  0.011471    0.         -0.8650466 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2457. State = [[-0.13561027  0.05400905]]. Action = [[ 0.03284717 -0.08030231  0.         -0.88872075]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2458. State = [[-0.13015583  0.05252801]]. Action = [[ 0.03126676  0.02930111  0.         -0.6486082 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2459. State = [[-0.12941733  0.05086156]]. Action = [[-0.06276974 -0.04072523  0.         -0.6420007 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2460. State = [[-0.12456144  0.0484033 ]]. Action = [[ 0.08410721 -0.02129015  0.         -0.7815336 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2461. State = [[-0.11599996  0.04686322]]. Action = [[ 0.07789359 -0.00695246  0.         -0.614887  ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2462. State = [[-0.10686257  0.04886148]]. Action = [[ 0.09602932  0.05765565  0.         -0.85394365]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2463. State = [[-0.09781878  0.04776724]]. Action = [[ 0.0881644  -0.04368     0.         -0.23818624]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2464. State = [[-0.08897468  0.04455336]]. Action = [[ 0.08341046 -0.0267388   0.         -0.7730489 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2465. State = [[-0.08489686  0.04072753]]. Action = [[-0.01751009 -0.04521064  0.         -0.54369736]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2466. State = [[-0.07803912  0.03589911]]. Action = [[ 0.09686632 -0.05401089  0.         -0.69859135]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2467. State = [[-0.06923103  0.03058056]]. Action = [[ 0.07276253 -0.05033395  0.         -0.30403244]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2468. State = [[-0.0608994   0.03077936]]. Action = [[ 0.07545681  0.06275063  0.         -0.7126205 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2469. State = [[-0.05213175  0.03172277]]. Action = [[ 0.09029246  0.00878914  0.         -0.7830462 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2470. State = [[-0.3449861  -0.00477332]]. Action = [[-0.0336677   0.0417117   0.         -0.67254174]]. Reward = [100.]
Curr episode timestep = 28
Current timestep = 2471. State = [[-0.34546533 -0.0084189 ]]. Action = [[ 0.01326845 -0.02592273  0.         -0.31253505]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2472. State = [[-0.34010613 -0.00525635]]. Action = [[ 0.07907427  0.07444883  0.         -0.8647223 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2473. State = [[-0.3375437  -0.00332843]]. Action = [[-0.01242029 -0.01431172  0.          0.40393865]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2474. State = [[-0.33374727 -0.00096125]]. Action = [[ 0.06285479  0.0459859   0.         -0.6953691 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2475. State = [[-0.33488196 -0.00304639]]. Action = [[-0.08333267 -0.07538588  0.         -0.6294981 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2476. State = [[-0.33469817 -0.0070239 ]]. Action = [[ 0.03087262 -0.03640106  0.         -0.10838169]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2477. State = [[-0.3321064  -0.00836216]]. Action = [[ 0.02193979  0.00264531  0.         -0.16179812]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2478. State = [[-0.3280436 -0.0109931]]. Action = [[ 0.05574869 -0.0449686   0.         -0.51513195]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2479. State = [[-0.32913327 -0.01383413]]. Action = [[-0.07600909 -0.0179726   0.         -0.58992857]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2480. State = [[-0.33176672 -0.0123424 ]]. Action = [[-0.02290408  0.05491444  0.         -0.611395  ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2481. State = [[-0.33391696 -0.01075189]]. Action = [[-0.02783792  0.00790243  0.         -0.06666017]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2482. State = [[-0.33541653 -0.01143684]]. Action = [[-0.00959101 -0.01597033  0.         -0.7322614 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2483. State = [[-0.33264884 -0.01322985]]. Action = [[ 0.07024097 -0.02317359  0.          0.558012  ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2484. State = [[-0.330425   -0.01020088]]. Action = [[ 0.01580243  0.07746942  0.         -0.34751117]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2485. State = [[-0.33203644 -0.00746002]]. Action = [[-0.03232787  0.00800999  0.         -0.75104046]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2486. State = [[-0.33646232 -0.00814587]]. Action = [[-0.06524219 -0.02406512  0.          0.82469845]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2487. State = [[-0.33865607 -0.00964735]]. Action = [[-0.00161217 -0.01829739  0.         -0.5591698 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2488. State = [[-0.3420517  -0.01059897]]. Action = [[-0.06118345 -0.00862704  0.         -0.8767854 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2489. State = [[-0.34069556 -0.01279484]]. Action = [[ 0.0735598  -0.03842885  0.          0.23449802]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2490. State = [[-0.34082615 -0.01637636]]. Action = [[-0.04393819 -0.04422308  0.          0.22974038]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2491. State = [[-0.34592134 -0.0227319 ]]. Action = [[-0.08700206 -0.08962491  0.         -0.5610746 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2492. State = [[-0.34466198 -0.02243241]]. Action = [[0.09136584 0.08778996 0.         0.50120115]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2493. State = [[-0.3387374  -0.02395943]]. Action = [[ 0.07102401 -0.0731966   0.         -0.7363945 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2494. State = [[-0.33392668 -0.02462298]]. Action = [[ 0.04747871  0.03707946  0.         -0.84204626]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2495. State = [[-0.32730794 -0.0255497 ]]. Action = [[ 0.09822699 -0.02907421  0.         -0.66141975]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2496. State = [[-0.32435796 -0.02334043]]. Action = [[-0.00933197  0.07006819  0.         -0.4066211 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2497. State = [[-0.32160884 -0.01718645]]. Action = [[ 0.05673826  0.08671787  0.         -0.4081149 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2498. State = [[-0.32279983 -0.01731548]]. Action = [[-0.06010218 -0.06318226  0.          0.18442106]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2499. State = [[-0.32124016 -0.02195205]]. Action = [[ 0.05488395 -0.05764525  0.         -0.77935576]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2500. State = [[-0.3164808 -0.0239508]]. Action = [[ 5.2143492e-02 -6.4179301e-05  0.0000000e+00 -6.3843703e-01]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2501. State = [[-0.31716388 -0.02424799]]. Action = [[-6.621541e-02  8.172244e-04  0.000000e+00 -9.743082e-01]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 2502. State = [[-0.31588298 -0.02863319]]. Action = [[ 0.04474515 -0.08235715  0.         -0.9709409 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2503. State = [[-0.31070048 -0.02856139]]. Action = [[ 0.06369288  0.05955584  0.         -0.38796532]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2504. State = [[-0.30482674 -0.02250351]]. Action = [[ 0.07078805  0.09132773  0.         -0.14681852]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 2505. State = [[-0.2986695 -0.020433 ]]. Action = [[ 0.07582625 -0.01767399  0.         -0.28326118]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 2506. State = [[-0.29678085 -0.01803554]]. Action = [[-0.0214      0.05207244  0.         -0.6607676 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2507. State = [[-0.2992078  -0.01323362]]. Action = [[-0.05193866  0.05888992  0.          0.1404016 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2508. State = [[-0.30334985 -0.01451376]]. Action = [[-0.06785136 -0.07449666  0.         -0.26880658]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2509. State = [[-0.3036715  -0.02017459]]. Action = [[ 0.01504    -0.08074592  0.         -0.6874683 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 2510. State = [[-0.29988012 -0.0196618 ]]. Action = [[0.056184   0.05716241 0.         0.03404081]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2511. State = [[-0.2949009  -0.01923474]]. Action = [[ 0.05928976 -0.02845933  0.         -0.10497606]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 2512. State = [[-0.2892594  -0.01619042]]. Action = [[ 0.06933335  0.07211671  0.         -0.8555839 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2513. State = [[-0.28260025 -0.01539069]]. Action = [[ 0.08603955 -0.03002985  0.         -0.81282884]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 2514. State = [[-0.27505794 -0.01362132]]. Action = [[ 0.08898783  0.04914118  0.         -0.57698727]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 2515. State = [[-0.27436686 -0.0081059 ]]. Action = [[-0.05918503  0.08046938  0.         -0.6800685 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 2516. State = [[-0.27214822 -0.00795284]]. Action = [[ 0.06761193 -0.0539715   0.         -0.80053234]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 2517. State = [[-0.26823756 -0.00426819]]. Action = [[ 0.02819265  0.09421434  0.         -0.67439616]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 2518. State = [[-0.2641826   0.00242381]]. Action = [[ 0.05533626  0.06638724  0.         -0.38715768]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 2519. State = [[-0.2586425  0.0027342]]. Action = [[ 0.06807863 -0.05368245  0.         -0.28778505]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 2520. State = [[-0.25111973  0.00308268]]. Action = [[ 0.0907805   0.02083655  0.         -0.26425695]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 2521. State = [[-2.5117522e-01 -2.5515059e-05]]. Action = [[-0.09618136 -0.08682286  0.         -0.5363164 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 2522. State = [[-0.25045672 -0.00279619]]. Action = [[ 0.03850167 -0.01674851  0.         -0.0928157 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 2523. State = [[-0.2485805  -0.00687675]]. Action = [[-0.00963613 -0.07395602  0.          0.37579155]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 2524. State = [[-0.24518299 -0.00648143]]. Action = [[ 0.04418097  0.0545618   0.         -0.7846061 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 2525. State = [[-0.2449229 -0.0058126]]. Action = [[-0.04447566 -0.01322776  0.         -0.18407202]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 2526. State = [[-0.24171063 -0.00471449]]. Action = [[ 0.06875861  0.03102297  0.         -0.1656422 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 2527. State = [[-0.2363195  -0.00329593]]. Action = [[ 0.05495336  0.01349571  0.         -0.743513  ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 2528. State = [[-0.23443112 -0.00254236]]. Action = [[-0.01300818  0.0084857   0.         -0.82163566]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 2529. State = [[-0.23262942 -0.00249815]]. Action = [[ 0.02334522 -0.00367032  0.          0.19020414]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 2530. State = [[-0.22812779 -0.00057433]]. Action = [[0.06277017 0.04095372 0.         0.570158  ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 2531. State = [[-0.22377248  0.00532205]]. Action = [[ 0.04224934  0.09208324  0.         -0.09968907]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 2532. State = [[-0.22289032  0.00998861]]. Action = [[-0.01479255  0.02931643  0.         -0.7088709 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 2533. State = [[-0.22669493  0.0153301 ]]. Action = [[-0.07976121  0.07050741  0.         -0.84611535]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 2534. State = [[-0.2249467   0.01596525]]. Action = [[ 0.07944102 -0.05304246  0.         -0.30487978]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 2535. State = [[-0.21945822  0.01297197]]. Action = [[ 0.05267755 -0.05062695  0.         -0.6192228 ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 2536. State = [[-0.21600127  0.00788542]]. Action = [[ 0.01115004 -0.08318369  0.         -0.1465019 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 2537. State = [[-0.2098096   0.00985191]]. Action = [[ 0.09129121  0.08672439  0.         -0.74262017]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 2538. State = [[-0.20712729  0.00809155]]. Action = [[-0.02603479 -0.08885991  0.         -0.80103457]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 2539. State = [[-0.20422544  0.00731336]]. Action = [[ 0.04064877  0.03426433  0.         -0.04306477]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 2540. State = [[-0.19762647  0.00858433]]. Action = [[ 0.08787993  0.0116506   0.         -0.8259083 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 2541. State = [[-0.18922418  0.0047648 ]]. Action = [[ 0.09511764 -0.0772173   0.         -0.38697207]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 2542. State = [[-0.18122303  0.00507915]]. Action = [[ 0.07566216  0.06485226  0.         -0.83847827]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 2543. State = [[-0.17337205  0.00435309]]. Action = [[ 0.08318204 -0.03665652  0.         -0.05390501]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 2544. State = [[-1.7110871e-01  9.0122892e-05]]. Action = [[-0.04727397 -0.05267622  0.         -0.67888355]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 2545. State = [[-0.16677815 -0.00381519]]. Action = [[ 0.07120814 -0.03208817  0.         -0.8698651 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 2546. State = [[-0.16101648 -0.00154439]]. Action = [[ 0.03968183  0.08295686  0.         -0.44364905]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 2547. State = [[-0.15554065  0.00068074]]. Action = [[ 0.0540979  0.0086218  0.        -0.509183 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 2548. State = [[-0.1512078   0.00365986]]. Action = [[ 0.02436198  0.05908161  0.         -0.3450663 ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 2549. State = [[-0.14522439  0.00494071]]. Action = [[ 0.07677732 -0.00800706  0.         -0.9073451 ]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 2550. State = [[-0.13837531  0.00795467]]. Action = [[ 0.06611376  0.06257439  0.         -0.45733202]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 2551. State = [[-0.13079931  0.00848373]]. Action = [[ 0.08627696 -0.02970733  0.         -0.45405775]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 2552. State = [[-0.12638794  0.00852535]]. Action = [[ 0.00400784  0.01210024  0.         -0.64742017]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 2553. State = [[-0.11954767  0.01019223]]. Action = [[ 0.09809711  0.02140816  0.         -0.29702878]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 2554. State = [[-0.11667255  0.00938973]]. Action = [[-0.0418269  -0.03574622  0.         -0.72962874]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 2555. State = [[-0.11559338  0.0072224 ]]. Action = [[-0.00262932 -0.03196887  0.         -0.6110991 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 2556. State = [[-0.10928495  0.00923035]]. Action = [[ 0.09311599  0.05453596  0.         -0.43742347]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 2557. State = [[-0.10047799  0.01370496]]. Action = [[ 0.09674015  0.05515636  0.         -0.6686191 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 2558. State = [[-0.09169374  0.01782802]]. Action = [[ 0.09425641  0.04444719  0.         -0.6082742 ]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 2559. State = [[-0.08318551  0.01785918]]. Action = [[ 0.0869692  -0.03147771  0.         -0.821003  ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 2560. State = [[-0.07397152  0.0122661 ]]. Action = [[ 0.09643244 -0.09591288  0.         -0.70168483]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 2561. State = [[-0.06436266  0.00576876]]. Action = [[ 0.08985723 -0.06468699  0.         -0.4554146 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 2562. State = [[-0.05547446  0.00507666]]. Action = [[ 0.07512931  0.04469163  0.         -0.46802646]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 2563. State = [[-0.25515416 -0.01524487]]. Action = [[ 0.09016751 -0.0671059   0.         -0.7907348 ]]. Reward = [100.]
Curr episode timestep = 92
Current timestep = 2564. State = [[-0.25423706 -0.00849377]]. Action = [[ 0.07769511  0.0348681   0.         -0.36126816]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2565. State = [[-0.2553501  -0.00282993]]. Action = [[-0.07526813  0.04970645  0.         -0.14790726]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2566. State = [[-0.25818524 -0.00333716]]. Action = [[-0.02839131 -0.08140846  0.         -0.08570284]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2567. State = [[-0.26118225 -0.00688727]]. Action = [[-0.06266043 -0.0590745   0.         -0.86718047]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2568. State = [[-0.25819716 -0.00330685]]. Action = [[ 0.08659009  0.08563075  0.         -0.98024464]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2569. State = [[-0.25349241  0.00383068]]. Action = [[ 0.03803989  0.07128764  0.         -0.9090315 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2570. State = [[-0.24813665  0.00303087]]. Action = [[ 0.07232382 -0.08205707  0.         -0.32675695]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2571. State = [[-0.2452148   0.00421169]]. Action = [[-0.00361668  0.05664348  0.         -0.85390544]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2572. State = [[-0.24254422  0.00851413]]. Action = [[ 0.03651414  0.04344193  0.         -0.27245402]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2573. State = [[-0.2359084   0.00584264]]. Action = [[ 0.09633634 -0.09115485  0.          0.6203475 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2574. State = [[-0.23452988  0.00292922]]. Action = [[-0.06183109 -0.0096188   0.         -0.6858454 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2575. State = [[-0.23810269  0.00152924]]. Action = [[-0.06912006 -0.02150331  0.          0.2696854 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2576. State = [[-0.23740353  0.0023669 ]]. Action = [[ 0.03560587  0.0293378   0.         -0.2324568 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2577. State = [[-0.23576318  0.00426729]]. Action = [[ 0.00209919  0.02127896  0.         -0.29781836]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2578. State = [[-0.2339899   0.00878831]]. Action = [[ 0.02710364  0.07489654  0.         -0.65315557]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2579. State = [[-0.23212487  0.01121144]]. Action = [[ 0.02149367 -0.0022094   0.         -0.5364617 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2580. State = [[-0.22803152  0.01023906]]. Action = [[ 0.06724586 -0.02654837  0.         -0.8208511 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2581. State = [[-0.22340065  0.00580331]]. Action = [[ 0.04609074 -0.07480002  0.         -0.74319327]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2582. State = [[-0.21876308  0.00348096]]. Action = [[0.04883387 0.00307157 0.         0.21071315]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2583. State = [[-0.21726476  0.00621689]]. Action = [[-0.01584673  0.06208562  0.         -0.4971894 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2584. State = [[-0.21405225  0.0041341 ]]. Action = [[ 0.0559238  -0.0744568   0.         -0.91129977]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2585. State = [[-0.20857777  0.00572657]]. Action = [[ 0.06164397  0.08119188  0.         -0.8222739 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2586. State = [[-0.20308842  0.0075524 ]]. Action = [[ 0.0597694  -0.00396542  0.         -0.9410716 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2587. State = [[-0.19786532  0.00589857]]. Action = [[ 0.04982405 -0.02916475  0.          0.7701857 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2588. State = [[-0.19897655  0.00217057]]. Action = [[-0.08701845 -0.05356866  0.         -0.4676234 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2589. State = [[-0.1964382   0.00306429]]. Action = [[ 0.07812833  0.0540104   0.         -0.5292583 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2590. State = [[-1.9111131e-01  2.1753209e-05]]. Action = [[ 0.03861497 -0.08681556  0.         -0.37438858]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2591. State = [[-0.18866569 -0.00301107]]. Action = [[-0.00935264 -0.00373653  0.         -0.81135684]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2592. State = [[-0.18597431 -0.00212331]]. Action = [[ 0.02655289  0.03087143  0.         -0.5751369 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2593. State = [[-0.18526565 -0.0059844 ]]. Action = [[-0.0311539  -0.08872323  0.         -0.21708739]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2594. State = [[-0.18334891 -0.00846481]]. Action = [[ 0.02376075  0.01110417  0.         -0.9123997 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 2595. State = [[-0.18270254 -0.00905402]]. Action = [[-0.02565151 -0.0054221   0.         -0.70897895]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2596. State = [[-0.17779465 -0.01159147]]. Action = [[ 0.09537778 -0.03777714  0.         -0.5994031 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2597. State = [[-0.17139871 -0.00970591]]. Action = [[ 0.0591914   0.07362034  0.         -0.05931813]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 2598. State = [[-0.16556272 -0.0110465 ]]. Action = [[ 0.06615389 -0.0581773   0.         -0.78229564]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 2599. State = [[-0.16161904 -0.01276641]]. Action = [[ 0.01898658  0.00984547  0.         -0.916021  ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2600. State = [[-0.15615119 -0.01445095]]. Action = [[ 0.07557803 -0.02595138  0.         -0.7310524 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2601. State = [[-0.1485621  -0.01192613]]. Action = [[ 0.09024995  0.07811943  0.         -0.7734513 ]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2602. State = [[-0.14011511 -0.01425348]]. Action = [[ 0.0981861  -0.08118887  0.         -0.5879052 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 2603. State = [[-0.13842808 -0.01627783]]. Action = [[-0.06394276  0.01664992  0.         -0.8725798 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2604. State = [[-0.13641591 -0.01998565]]. Action = [[ 0.04577065 -0.07328393  0.         -0.8649839 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 2605. State = [[-0.13211444 -0.02360916]]. Action = [[ 0.03403515 -0.01799954  0.         -0.79059255]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2606. State = [[-0.12540886 -0.02859608]]. Action = [[ 0.08560228 -0.07089964  0.         -0.23122853]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 2607. State = [[-0.11662796 -0.03250527]]. Action = [[ 0.09800983 -0.01308535  0.         -0.54927254]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 2608. State = [[-0.1104627 -0.0320168]]. Action = [[ 0.03577844  0.04564879  0.         -0.79963195]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 2609. State = [[-0.10388351 -0.03286108]]. Action = [[ 0.07881289 -0.02066342  0.         -0.9104859 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 2610. State = [[-0.09549429 -0.03844986]]. Action = [[ 0.08978026 -0.07869194  0.         -0.7928667 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 2611. State = [[-0.08643878 -0.03855628]]. Action = [[ 0.09421415  0.07095218  0.         -0.7438648 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 2612. State = [[-0.07778907 -0.03393854]]. Action = [[ 0.08753894  0.07469308  0.         -0.9437706 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 2613. State = [[-0.06874631 -0.03327405]]. Action = [[ 0.09891248 -0.01885737  0.         -0.8893257 ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 2614. State = [[-0.06726872 -0.03539668]]. Action = [[-0.08494108 -0.02403536  0.         -0.2039625 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 2615. State = [[-0.06321131 -0.03695786]]. Action = [[ 0.09219075 -0.0144283   0.         -0.58083916]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 2616. State = [[-0.06110193 -0.04165711]]. Action = [[-0.05808729 -0.08072822  0.         -0.8549037 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 2617. State = [[-0.05591357 -0.04493798]]. Action = [[ 0.09090007 -0.01193906  0.         -0.65605855]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 2618. State = [[-0.05241443 -0.04485545]]. Action = [[-0.02905661  0.02080671  0.         -0.91948676]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 2619. State = [[-0.16520588 -0.05789783]]. Action = [[ 0.06455851 -0.00430948  0.         -0.8763829 ]]. Reward = [100.]
Curr episode timestep = 55
Current timestep = 2620. State = [[-0.15940157 -0.05561487]]. Action = [[ 0.05460518  0.04111017  0.         -0.8260444 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2621. State = [[-0.1550681 -0.0599194]]. Action = [[ 0.02569967 -0.08605596  0.          0.25289726]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2622. State = [[-0.14849517 -0.05984759]]. Action = [[ 0.08376623  0.07660144  0.         -0.6819926 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2623. State = [[-0.14107172 -0.05653344]]. Action = [[ 0.07271225  0.04219035  0.         -0.5117861 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2624. State = [[-0.13580576 -0.05679833]]. Action = [[ 0.03052036 -0.02152979  0.         -0.8306186 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2625. State = [[-0.13433848 -0.06094722]]. Action = [[-0.03242483 -0.06376767  0.         -0.5904864 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2626. State = [[-0.12842906 -0.06526538]]. Action = [[ 0.09296336 -0.03980509  0.         -0.6188444 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2627. State = [[-0.11998659 -0.06433842]]. Action = [[ 0.07470108  0.05462243  0.         -0.34692734]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2628. State = [[-0.11120908 -0.06547892]]. Action = [[ 0.09070892 -0.04555951  0.         -0.72208935]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2629. State = [[-0.10321014 -0.07016706]]. Action = [[ 0.06240088 -0.05802696  0.         -0.21784759]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2630. State = [[-0.09469826 -0.07513831]]. Action = [[ 0.08491559 -0.04798439  0.         -0.90056527]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2631. State = [[-0.08565062 -0.07837406]]. Action = [[ 0.08315865 -0.01355903  0.         -0.72347486]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2632. State = [[-0.07729462 -0.07899994]]. Action = [[ 0.06986421  0.01964314  0.         -0.7622429 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2633. State = [[-0.06797268 -0.08306672]]. Action = [[ 0.09647617 -0.07123558  0.         -0.78075045]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2634. State = [[-0.059148   -0.08532229]]. Action = [[ 0.06987946  0.02023964  0.         -0.6123159 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2635. State = [[-0.05731581 -0.08710748]]. Action = [[-0.06888451 -0.02318745  0.         -0.91227436]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2636. State = [[-0.05374126 -0.08838441]]. Action = [[ 0.05002499  0.00395668  0.         -0.26570642]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2637. State = [[-0.29730806  0.1372869 ]]. Action = [[ 0.09709836  0.01881906  0.         -0.39205086]]. Reward = [100.]
Curr episode timestep = 17
Scene graph at timestep 2637 is [True, False, False, False, False, True]
State prediction error at timestep 2637 is tensor(0.0559, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2637 of None
Current timestep = 2638. State = [[-0.29227024  0.1306207 ]]. Action = [[ 0.0818379  -0.07647647  0.         -0.566373  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 2638 is [True, False, False, False, False, True]
State prediction error at timestep 2638 is tensor(5.0849e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2638 of None
Current timestep = 2639. State = [[-0.2838234   0.12375912]]. Action = [[ 0.08425788 -0.07500146  0.         -0.83398986]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2640. State = [[-0.27900258  0.11564633]]. Action = [[-0.00212525 -0.09481215  0.         -0.07786649]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2641. State = [[-0.2753887   0.11371969]]. Action = [[ 0.02397515  0.04486661  0.         -0.17872399]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2642. State = [[-0.27252394  0.11227606]]. Action = [[ 0.00681929 -0.02497406  0.         -0.34667343]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2643. State = [[-0.26688477  0.10596383]]. Action = [[ 0.07352323 -0.08462366  0.          0.7482755 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2644. State = [[-0.2615891   0.10178731]]. Action = [[ 0.02845544  0.00133151  0.         -0.20495105]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2645. State = [[-0.25722873  0.09727313]]. Action = [[ 0.03643645 -0.05453587  0.         -0.33220482]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2646. State = [[-0.25076428  0.09667025]]. Action = [[ 0.08015772  0.05620667  0.         -0.6490255 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2647. State = [[-0.24330229  0.0999903 ]]. Action = [[ 0.08628871  0.07191186  0.         -0.57011044]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2648. State = [[-0.23559932  0.09870259]]. Action = [[ 0.08715267 -0.03912286  0.         -0.68657863]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2649. State = [[-0.22775029  0.09851775]]. Action = [[ 0.08211667  0.04385839  0.         -0.2847618 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2650. State = [[-0.22044562  0.0991495 ]]. Action = [[ 0.07223897  0.0096485   0.         -0.9314587 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2651. State = [[-0.2128459   0.10241058]]. Action = [[ 0.08171318  0.07448877  0.         -0.7265193 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2652. State = [[-0.2065144   0.10563036]]. Action = [[ 0.05043263  0.03005504  0.         -0.29946357]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2653. State = [[-0.20285968  0.10812916]]. Action = [[ 0.00812922  0.02917839  0.         -0.5239638 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2654. State = [[-0.19997258  0.11072805]]. Action = [[ 0.01461855  0.02231455  0.         -0.39189512]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2655. State = [[-0.19308001  0.10979369]]. Action = [[ 0.09132231 -0.04857656  0.         -0.2121557 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2656. State = [[-0.18423495  0.10868755]]. Action = [[ 0.08106663 -0.00520574  0.         -0.51417655]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2657. State = [[-0.17748216  0.10598276]]. Action = [[ 0.03635802 -0.05802586  0.         -0.42858648]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2658. State = [[-0.16863959  0.10330998]]. Action = [[ 0.09850346 -0.023422    0.         -0.851187  ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2659. State = [[-0.1590071   0.10281049]]. Action = [[ 0.07997257  0.01057734  0.         -0.6153792 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2660. State = [[-0.15136659  0.10491148]]. Action = [[ 0.04982797  0.04406101  0.         -0.64641273]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2661. State = [[-0.14322487  0.10496336]]. Action = [[ 0.07915743 -0.02010111  0.         -0.7142817 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2662. State = [[-0.13410732  0.10108531]]. Action = [[ 0.07920181 -0.06026497  0.         -0.74312997]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2663. State = [[-0.12399321  0.09606303]]. Action = [[ 0.09379195 -0.05161935  0.         -0.6912271 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2664. State = [[-0.11467317  0.09733801]]. Action = [[0.07072151 0.07749707 0.         0.05487418]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2665. State = [[-0.10500742  0.09585957]]. Action = [[ 0.09508633 -0.05596454  0.         -0.6922238 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2666. State = [[-0.09480587  0.08925863]]. Action = [[ 0.08661839 -0.08214492  0.         -0.6067726 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2667. State = [[-0.08440739  0.08145472]]. Action = [[ 0.0891666  -0.07991858  0.         -0.6491877 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2668. State = [[-0.07644176  0.07599109]]. Action = [[ 0.03496218 -0.02745575  0.         -0.38618422]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 2669. State = [[-0.06887081  0.07482838]]. Action = [[ 0.06033546  0.0286397   0.         -0.5929432 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2670. State = [[-0.06035371  0.07188484]]. Action = [[ 0.07329553 -0.04329425  0.         -0.48765206]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2671. State = [[-0.05228656  0.06561325]]. Action = [[ 0.05557657 -0.06950332  0.         -0.7557063 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 2672. State = [[-0.17448913  0.02491559]]. Action = [[ 0.09922674 -0.08195879  0.         -0.21040195]]. Reward = [100.]
Curr episode timestep = 34
Current timestep = 2673. State = [[-0.17292446  0.01918196]]. Action = [[ 0.07551635 -0.04874284  0.         -0.33814967]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2674. State = [[-0.16618039  0.01374692]]. Action = [[ 0.08022859 -0.05370754  0.         -0.01004106]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2675. State = [[-0.1618682   0.01493743]]. Action = [[ 0.02240261  0.08457284  0.         -0.77464634]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2676. State = [[-0.1614075  0.015512 ]]. Action = [[-0.02071806 -0.01637849  0.         -0.6344385 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2677. State = [[-0.161558    0.01854634]]. Action = [[-0.00591226  0.08055265  0.         -0.4939103 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2678. State = [[-0.15782665  0.01898733]]. Action = [[ 0.07340956 -0.03226624  0.         -0.86617875]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2679. State = [[-0.15072861  0.01391986]]. Action = [[ 0.09439302 -0.07682507  0.         -0.41813743]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2680. State = [[-0.14374994  0.01123846]]. Action = [[ 0.07072467  0.00683035  0.         -0.8669542 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2681. State = [[-0.13611732  0.00993086]]. Action = [[ 0.09312608 -0.01111903  0.         -0.6195502 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2682. State = [[-0.12832794  0.00544137]]. Action = [[ 0.07817728 -0.06480041  0.         -0.6342277 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2683. State = [[-0.12286954  0.00312712]]. Action = [[ 0.03085556  0.0139847   0.         -0.12239802]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2684. State = [[-0.11595559  0.00303934]]. Action = [[ 0.08619807  0.01353456  0.         -0.84090096]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2685. State = [[-0.10950338  0.00062675]]. Action = [[ 0.0445862  -0.03692282  0.         -0.1924836 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2686. State = [[-0.10215897 -0.00066614]]. Action = [[ 0.08069076  0.01466278  0.         -0.66828996]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2687. State = [[-0.09819202 -0.00290588]]. Action = [[-0.01132447 -0.03578798  0.         -0.63950443]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2688. State = [[-0.09123785 -0.00662419]]. Action = [[ 0.09858488 -0.03914515  0.          0.19312036]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2689. State = [[-0.08172219 -0.00753463]]. Action = [[ 0.09127348  0.02474216  0.         -0.8558552 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2690. State = [[-0.07412901 -0.01000653]]. Action = [[ 0.0529881  -0.04530247  0.         -0.81784666]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2691. State = [[-0.06527329 -0.01316259]]. Action = [[ 0.09733508 -0.01888664  0.         -0.31430805]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2692. State = [[-0.05653622 -0.01616021]]. Action = [[ 0.07074412 -0.02685925  0.         -0.593253  ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2693. State = [[-0.05359573 -0.01895553]]. Action = [[-0.04269588 -0.02034663  0.         -0.5593287 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2694. State = [[-0.3569594  -0.06084351]]. Action = [[ 0.09142011 -0.01326506  0.         -0.7212211 ]]. Reward = [100.]
Curr episode timestep = 21
Current timestep = 2695. State = [[-0.35499576 -0.06048557]]. Action = [[-0.02663425 -0.09463653  0.         -0.6455251 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2696. State = [[-0.35847262 -0.05955043]]. Action = [[-0.06587426  0.0726608   0.          0.92293537]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2697. State = [[-0.35779122 -0.05212966]]. Action = [[ 0.0582877   0.09945006  0.         -0.96134746]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2698. State = [[-0.35722962 -0.05175249]]. Action = [[-0.02395392 -0.0928529   0.         -0.19893086]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2699. State = [[-0.35839257 -0.05034336]]. Action = [[-0.00841536  0.07020263  0.          0.7731644 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2700. State = [[-0.35749835 -0.05139636]]. Action = [[ 0.02710379 -0.07362606  0.         -0.09826124]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2701. State = [[-0.3561475  -0.05752043]]. Action = [[ 0.01215322 -0.0869984   0.         -0.24271369]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2702. State = [[-0.3550421  -0.05779331]]. Action = [[ 0.01234247  0.05118158  0.         -0.7413528 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2703. State = [[-0.3587057  -0.05263963]]. Action = [[-0.08777037  0.07444651  0.         -0.72870797]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2704. State = [[-0.3581537  -0.05181162]]. Action = [[ 0.07506999 -0.03882594  0.         -0.63505167]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2705. State = [[-0.3538636  -0.05409763]]. Action = [[ 0.03739554 -0.03091867  0.          0.39664268]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2706. State = [[-0.35589883 -0.05274617]]. Action = [[-0.08746072  0.04921285  0.         -0.47914362]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2707. State = [[-0.35408494 -0.0547456 ]]. Action = [[ 0.09372475 -0.07201159  0.          0.5640402 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2708. State = [[-0.34793422 -0.05502021]]. Action = [[ 0.05542842  0.03643397  0.         -0.11966944]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2709. State = [[-0.34206203 -0.05183581]]. Action = [[ 0.0625065   0.03923104  0.         -0.63251764]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2710. State = [[-0.34265682 -0.04735273]]. Action = [[-0.07732783  0.06054226  0.         -0.19674677]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2711. State = [[-0.34394968 -0.04249296]]. Action = [[ 0.00533415  0.05119016  0.         -0.92741364]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2712. State = [[-0.34708259 -0.03521676]]. Action = [[-0.06704868  0.09586763  0.         -0.83610076]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2713. State = [[-0.34802553 -0.02665724]]. Action = [[ 0.02657016  0.08545492  0.         -0.6263491 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2714. State = [[-0.34700313 -0.02429539]]. Action = [[ 0.01909193 -0.04250065  0.          0.107939  ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2715. State = [[-0.34473103 -0.0248198 ]]. Action = [[ 0.04198372 -0.01773386  0.          0.30076575]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2716. State = [[-0.340672   -0.02576868]]. Action = [[ 0.06072363 -0.03097974  0.         -0.7348565 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2717. State = [[-0.34031844 -0.02471576]]. Action = [[-0.03465039  0.02335372  0.         -0.15023178]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2718. State = [[-0.33793053 -0.02421317]]. Action = [[ 0.0625656  -0.01572542  0.         -0.34420228]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2719. State = [[-0.33354566 -0.02026101]]. Action = [[ 0.04864871  0.07532746  0.         -0.21292663]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2720. State = [[-0.33509183 -0.0184586 ]]. Action = [[-0.07175638 -0.02041848  0.         -0.35468686]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2721. State = [[-0.33410087 -0.02183351]]. Action = [[ 0.05017746 -0.06433868  0.          0.7720592 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2722. State = [[-0.3341633  -0.02585377]]. Action = [[-0.04630225 -0.04041321  0.         -0.73735714]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2723. State = [[-0.33187604 -0.0238903 ]]. Action = [[0.0572909  0.07052719 0.         0.43605602]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2724. State = [[-0.3320276  -0.02124024]]. Action = [[-0.04539023  0.01099727  0.         -0.8589045 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2725. State = [[-0.33646223 -0.02032103]]. Action = [[-0.07849053  0.00874905  0.          0.41274548]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 2726. State = [[-0.34150594 -0.02475171]]. Action = [[-0.06791738 -0.09208962  0.         -0.71561825]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2727. State = [[-0.3439768  -0.02343397]]. Action = [[-0.00767846  0.09580249  0.         -0.36824405]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2728. State = [[-0.3443393  -0.01617191]]. Action = [[ 0.0110116   0.08184996  0.         -0.57554877]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 2729. State = [[-0.3457366  -0.01304606]]. Action = [[-0.01611858 -0.00797553  0.         -0.82499623]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 2730. State = [[-0.34693298 -0.00939389]]. Action = [[0.00322243 0.05666962 0.         0.6056707 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2731. State = [[-0.346011   -0.00261807]]. Action = [[ 0.04363623  0.07873435  0.         -0.78363156]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2732. State = [[-0.34755352  0.00424603]]. Action = [[-0.02607621  0.06051701  0.         -0.23899353]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2733. State = [[-0.34652758  0.00494382]]. Action = [[ 0.06139893 -0.05281594  0.         -0.68063366]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 2734. State = [[-0.34220058  0.00345085]]. Action = [[ 0.07122459 -0.02245026  0.         -0.6157694 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2735. State = [[-0.34203216 -0.00090194]]. Action = [[-0.03954867 -0.08549001  0.          0.51184726]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 2736. State = [[-0.34108657 -0.00834239]]. Action = [[ 0.03167132 -0.09918015  0.         -0.8362532 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2737. State = [[-0.3359775  -0.01563059]]. Action = [[ 0.07228566 -0.07271548  0.         -0.52970344]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 2738. State = [[-0.3337489  -0.01587793]]. Action = [[-0.01546882  0.0627374   0.          0.1652844 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 2739. State = [[-0.33258075 -0.01475415]]. Action = [[ 0.01583715  0.00762486  0.         -0.94999456]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 2740. State = [[-0.32944083 -0.01818793]]. Action = [[ 0.04273293 -0.05622045  0.         -0.6291626 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 2741. State = [[-0.3237494  -0.01813667]]. Action = [[ 0.07805277  0.05380065  0.         -0.5195782 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 2742. State = [[-0.3203861  -0.01601442]]. Action = [[0.01239512 0.02569904 0.         0.1207279 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 2743. State = [[-0.3160545  -0.01118323]]. Action = [[0.07113192 0.08653878 0.         0.18751824]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 2744. State = [[-0.31651634 -0.00832599]]. Action = [[-0.05925588  0.0041345   0.         -0.24306446]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 2745. State = [[-0.31577885 -0.00411817]]. Action = [[ 0.04353148  0.07216936  0.         -0.05232942]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 2746. State = [[-0.31718713  0.0008732 ]]. Action = [[-0.05073094  0.0418523   0.         -0.9812196 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 2747. State = [[-0.31504643  0.00113829]]. Action = [[ 0.07350681 -0.04153972  0.         -0.5766355 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 2748. State = [[-0.30847928 -0.00310598]]. Action = [[ 0.0838924  -0.07795366  0.          0.08096087]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 2749. State = [[-0.30633226 -0.00138619]]. Action = [[-0.02645503  0.07328189  0.         -0.8175459 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 2750. State = [[-0.30294102  0.00145197]]. Action = [[ 0.06674964  0.00465606  0.         -0.84390163]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 2751. State = [[-2.9959235e-01  1.9572934e-04]]. Action = [[ 0.01115561 -0.04027137  0.         -0.63384503]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 2752. State = [[-0.29440057  0.0034688 ]]. Action = [[ 0.07588602  0.08280661  0.         -0.46006095]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 2753. State = [[-0.2887224   0.00441412]]. Action = [[ 0.05242544 -0.03702885  0.         -0.12600452]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 2754. State = [[-0.28534943  0.00291893]]. Action = [[ 0.00796577 -0.01623495  0.          0.2765323 ]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 2755. State = [[-0.2834629   0.00470203]]. Action = [[ 0.00090214  0.04301756  0.         -0.8919301 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 2756. State = [[-0.28274834  0.00464353]]. Action = [[-0.01653291 -0.03104216  0.         -0.25978386]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 2757. State = [[-0.27901432  0.00461312]]. Action = [[ 0.05490989  0.01265605  0.         -0.53621244]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 2758. State = [[-0.27281392  0.00751296]]. Action = [[0.07131738 0.04839144 0.         0.41048932]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 2759. State = [[-0.26818383  0.01140677]]. Action = [[ 0.03153176  0.0434807   0.         -0.00442404]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 2760. State = [[-0.26221377  0.00904927]]. Action = [[ 0.07807905 -0.0810912   0.         -0.8864881 ]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 2761. State = [[-0.25686964  0.0054302 ]]. Action = [[ 0.03089    -0.02711134  0.         -0.60116357]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 2762. State = [[-2.5746599e-01 -2.3432811e-04]]. Action = [[-0.07488507 -0.09213387  0.         -0.07107437]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 2763. State = [[-0.253821   -0.00755972]]. Action = [[ 0.07741714 -0.08146243  0.         -0.88480365]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 2764. State = [[-0.24861619 -0.00702847]]. Action = [[ 0.02127554  0.0816158   0.         -0.31140006]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 2765. State = [[-0.24263117 -0.00115474]]. Action = [[ 0.07608325  0.08564987  0.         -0.9156561 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 2766. State = [[-0.23804104 -0.00024924]]. Action = [[ 0.0239915  -0.0281448   0.         -0.25479555]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 2767. State = [[-0.23324043 -0.00255315]]. Action = [[ 0.05224646 -0.0260791   0.         -0.8296462 ]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 2768. State = [[-0.22694434 -0.00293304]]. Action = [[ 0.06835296  0.01627149  0.         -0.73267317]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 2769. State = [[-0.2250262   0.00196067]]. Action = [[-0.02777795  0.0936516   0.          0.14615059]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 2770. State = [[-0.22277401  0.00330552]]. Action = [[ 0.03800733 -0.03266405  0.         -0.3227408 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 2771. State = [[-0.21665987  0.00400684]]. Action = [[ 0.08176393  0.02533077  0.         -0.902104  ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 2772. State = [[-0.20845677  0.00146055]]. Action = [[ 0.09720988 -0.0686428   0.         -0.63978624]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 2773. State = [[-0.20083211 -0.00359946]]. Action = [[ 0.06453449 -0.05935358  0.         -0.83205605]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 2774. State = [[-0.1929007  -0.00594933]]. Action = [[ 0.0822042  -0.00089501  0.         -0.26794434]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 2775. State = [[-0.1855394  -0.00367723]]. Action = [[ 0.06173231  0.05985936  0.         -0.11022222]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 2776. State = [[-0.17695023 -0.00336034]]. Action = [[ 0.09851558 -0.01887371  0.         -0.52904004]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 2777. State = [[-0.16749339 -0.00240697]]. Action = [[ 0.09303933  0.03847774  0.         -0.96064645]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 2778. State = [[-1.5817814e-01  1.2107620e-04]]. Action = [[ 0.09199692  0.03694881  0.         -0.7091979 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 2779. State = [[-0.14909941  0.00395132]]. Action = [[ 0.08719427  0.05867264  0.         -0.8479409 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 2780. State = [[-0.1443997   0.00614945]]. Action = [[-0.0038525   0.00724687  0.         -0.5875906 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 2781. State = [[-0.14172707  0.00705326]]. Action = [[-0.00092427  0.0034778   0.         -0.74549556]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 2782. State = [[-0.13444676  0.00980962]]. Action = [[ 0.09857423  0.04013444  0.         -0.61010957]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 2783. State = [[-0.12545949  0.01427501]]. Action = [[ 0.07999707  0.05150371  0.         -0.7370833 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 2784. State = [[-0.11610008  0.01474692]]. Action = [[ 0.09490348 -0.03746429  0.         -0.4516245 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 2785. State = [[-0.10608584  0.01290552]]. Action = [[ 0.09460426 -0.02818391  0.         -0.66823506]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 2786. State = [[-0.0964989   0.01497886]]. Action = [[ 0.08343949  0.05285663  0.         -0.36708963]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 2787. State = [[-0.08743726  0.01629944]]. Action = [[ 0.07934567 -0.00911562  0.         -0.6397231 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 2788. State = [[-0.07736164  0.01577448]]. Action = [[ 0.09907886 -0.00983343  0.         -0.7624343 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 2789. State = [[-0.06787362  0.01355338]]. Action = [[ 0.07247702 -0.03807594  0.         -0.62155986]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 2790. State = [[-0.05761619  0.01090993]]. Action = [[ 0.09866957 -0.02574823  0.         -0.01197588]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 2791. State = [[-0.05078376  0.00642192]]. Action = [[ 0.00912765 -0.06502417  0.         -0.34135735]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 2792. State = [[-0.19285174 -0.15612595]]. Action = [[ 0.08638389 -0.06541848  0.         -0.27604628]]. Reward = [100.]
Curr episode timestep = 97
Scene graph at timestep 2792 is [True, False, False, True, False, False]
State prediction error at timestep 2792 is tensor(0.0239, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2792 of None
Current timestep = 2793. State = [[-0.19439155 -0.1564134 ]]. Action = [[ 0.03499327 -0.0702137   0.         -0.9194397 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 2794. State = [[-0.19003408 -0.15885651]]. Action = [[ 0.07983784 -0.01683462  0.         -0.8766438 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 2795. State = [[-0.19077519 -0.15695879]]. Action = [[-0.06119584  0.05052798  0.         -0.8968538 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 2796. State = [[-0.18928511 -0.15252751]]. Action = [[ 0.07977255  0.05273398  0.         -0.63838995]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 2797. State = [[-0.18323089 -0.15230595]]. Action = [[ 0.09811222 -0.0436132   0.         -0.7450921 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 2798. State = [[-0.17811705 -0.15557519]]. Action = [[ 0.05842597 -0.0537679   0.         -0.89284605]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 2799. State = [[-0.17176059 -0.15249228]]. Action = [[ 0.09948388  0.08929688  0.         -0.9529303 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 2800. State = [[-0.16547623 -0.15052938]]. Action = [[ 0.07366569 -0.01874626  0.         -0.46096885]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 2801. State = [[-0.15844756 -0.15493746]]. Action = [[ 0.09364075 -0.08476842  0.         -0.74517906]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 2802. State = [[-0.15179099 -0.15760316]]. Action = [[ 6.5508582e-02  3.4682453e-04  0.0000000e+00 -8.1839043e-01]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 2803. State = [[-0.14748006 -0.15784849]]. Action = [[ 0.02616813  0.00727161  0.         -0.8542152 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 2804. State = [[-0.1407312  -0.15813233]]. Action = [[ 0.09380401  0.0009747   0.         -0.55584943]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 2805. State = [[-0.13509688 -0.15766482]]. Action = [[ 0.02952481  0.01815493  0.         -0.76675093]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 2806. State = [[-0.12946856 -0.15347669]]. Action = [[ 0.06082176  0.08071404  0.         -0.44537908]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 2807. State = [[-0.12642232 -0.15257043]]. Action = [[-0.01009925 -0.02890354  0.         -0.30115986]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 2808. State = [[-0.12633117 -0.15407877]]. Action = [[-0.03873433 -0.01395182  0.         -0.6261802 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 2809. State = [[-0.12864038 -0.15223897]]. Action = [[-0.07317124  0.04819293  0.         -0.74856174]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 2810. State = [[-0.12922747 -0.14941008]]. Action = [[-0.0126904   0.02529863  0.         -0.60318893]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 2811. State = [[-0.12432186 -0.15160456]]. Action = [[ 0.07803405 -0.07244915  0.         -0.87697816]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 2812. State = [[-0.11897057 -0.15328425]]. Action = [[ 0.03124235  0.00181653  0.         -0.9545965 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 2813. State = [[-0.11773208 -0.15185328]]. Action = [[-0.02834983  0.02805346  0.         -0.85992813]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 2814. State = [[-0.11566199 -0.1544914 ]]. Action = [[ 0.02718101 -0.06995029  0.         -0.32587898]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 2815. State = [[-0.11413383 -0.15451606]]. Action = [[-0.01111906  0.04347778  0.         -0.71340036]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 2816. State = [[-0.11220005 -0.14906296]]. Action = [[ 0.02300151  0.08604182  0.         -0.71845067]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 2817. State = [[-0.10870522 -0.14166312]]. Action = [[0.04495756 0.08531702 0.         0.5941386 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 2818. State = [[-0.10516827 -0.1351734 ]]. Action = [[ 0.03861492  0.05509255  0.         -0.6097279 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 2819. State = [[-0.10212858 -0.12852415]]. Action = [[ 0.03456242  0.06937555  0.         -0.7884381 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 2820. State = [[-0.10222879 -0.12871166]]. Action = [[-0.02851237 -0.0774738   0.         -0.8351023 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 2821. State = [[-0.09970109 -0.13293582]]. Action = [[ 0.05672758 -0.06484818  0.         -0.45847642]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 2822. State = [[-0.09999491 -0.13246113]]. Action = [[-0.05441282  0.03744189  0.         -0.27442378]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 2823. State = [[-0.09659251 -0.13026015]]. Action = [[ 0.09597952  0.00986757  0.         -0.71538365]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 2824. State = [[-0.09666724 -0.1327364 ]]. Action = [[-0.07001811 -0.06398594  0.         -0.7300713 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 2825. State = [[-0.0950711 -0.137851 ]]. Action = [[ 0.05646852 -0.06530966  0.          0.05299687]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 2826. State = [[-0.094126   -0.13645983]]. Action = [[-0.02765577  0.07646377  0.         -0.88109785]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 2827. State = [[-0.08994792 -0.13104187]]. Action = [[ 0.09355999  0.06713554  0.         -0.9376567 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 2828. State = [[-0.08811966 -0.1321108 ]]. Action = [[-0.02257018 -0.06627351  0.         -0.28371406]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 2829. State = [[-0.08350964 -0.1353664 ]]. Action = [[ 0.09408546 -0.0259837   0.         -0.8991755 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 2830. State = [[-0.07605323 -0.14067332]]. Action = [[ 0.08927944 -0.08201293  0.         -0.94001585]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 2831. State = [[-0.07310718 -0.14589633]]. Action = [[-0.01215757 -0.03893539  0.         -0.930464  ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 2832. State = [[-0.06757894 -0.146133  ]]. Action = [[ 0.09880985  0.04145283  0.         -0.7023141 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 2833. State = [[-0.06157029 -0.14371246]]. Action = [[ 0.051544    0.04197509  0.         -0.80544525]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 2834. State = [[-0.0554601  -0.14086874]]. Action = [[ 0.07655711  0.04189251  0.         -0.8867513 ]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 2835. State = [[-0.0493545  -0.14437728]]. Action = [[ 0.0605421  -0.08948401  0.         -0.94393295]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 2835 is [False, True, False, True, False, False]
State prediction error at timestep 2835 is tensor(1.7017e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2835 of None
Current timestep = 2836. State = [[-0.04184286 -0.15184408]]. Action = [[ 0.09165695 -0.08635063  0.         -0.8088642 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 2836 is [False, True, False, True, False, False]
State prediction error at timestep 2836 is tensor(8.5353e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2836 of None
Current timestep = 2837. State = [[-0.03372311 -0.152372  ]]. Action = [[ 0.08465598  0.06180529  0.         -0.8903917 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 2837 is [False, True, False, True, False, False]
State prediction error at timestep 2837 is tensor(2.3904e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2837 of None
Current timestep = 2838. State = [[-0.02541247 -0.15110026]]. Action = [[ 0.09118169  0.00791256  0.         -0.93086755]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 2838 is [False, True, False, True, False, False]
State prediction error at timestep 2838 is tensor(1.4597e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2838 of None
Current timestep = 2839. State = [[-0.02333441 -0.15449378]]. Action = [[-0.05163744 -0.05897927  0.         -0.4636941 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 2839 is [False, True, False, True, False, False]
State prediction error at timestep 2839 is tensor(3.4011e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2839 of None
Current timestep = 2840. State = [[-0.01866536 -0.15615653]]. Action = [[ 0.07983043  0.01671282  0.         -0.08174604]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 2840 is [False, True, False, True, False, False]
State prediction error at timestep 2840 is tensor(5.4159e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2840 of None
Current timestep = 2841. State = [[-0.00989624 -0.15255858]]. Action = [[ 0.09871048  0.07517197  0.         -0.92758125]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 2841 is [False, True, False, True, False, False]
State prediction error at timestep 2841 is tensor(2.8737e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2841 of None
Current timestep = 2842. State = [[-0.00117062 -0.15084957]]. Action = [[ 0.08642382 -0.00732033  0.         -0.9397592 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 2842 is [False, True, False, True, False, False]
State prediction error at timestep 2842 is tensor(1.7669e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2842 of None
Current timestep = 2843. State = [[-0.00058238 -0.15493284]]. Action = [[-0.0916857  -0.07418921  0.         -0.8552369 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 2843 is [False, True, False, True, False, False]
State prediction error at timestep 2843 is tensor(3.2266e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2843 of None
Current timestep = 2844. State = [[-0.00040436 -0.15541555]]. Action = [[ 0.00452606  0.03864456  0.         -0.7776333 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 2844 is [False, True, False, True, False, False]
State prediction error at timestep 2844 is tensor(1.1927e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2844 of None
Current timestep = 2845. State = [[ 0.00614123 -0.1532929 ]]. Action = [[ 0.09906323  0.02014937  0.         -0.93672156]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 2845 is [False, True, False, True, False, False]
State prediction error at timestep 2845 is tensor(1.4629e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2845 of None
Current timestep = 2846. State = [[ 0.01500893 -0.15181287]]. Action = [[ 0.0962759   0.01280525  0.         -0.6013249 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 2846 is [False, True, False, True, False, False]
State prediction error at timestep 2846 is tensor(1.8608e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2846 of None
Current timestep = 2847. State = [[ 0.02167931 -0.14906485]]. Action = [[ 0.05101229  0.04245264  0.         -0.8442608 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 2847 is [False, True, False, True, False, False]
State prediction error at timestep 2847 is tensor(5.5231e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2847 of None
Current timestep = 2848. State = [[ 0.02208373 -0.15146753]]. Action = [[-0.0599126  -0.07815483  0.         -0.8223237 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 2848 is [False, True, False, True, False, False]
State prediction error at timestep 2848 is tensor(2.5048e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2848 of None
Current timestep = 2849. State = [[ 0.02707188 -0.15223031]]. Action = [[ 0.09915669  0.02820358  0.         -0.8554483 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 2849 is [False, True, False, True, False, False]
State prediction error at timestep 2849 is tensor(8.2598e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2849 of None
Current timestep = 2850. State = [[ 0.03506701 -0.14677225]]. Action = [[ 0.07607312  0.09228974  0.         -0.92098963]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 2850 is [False, True, False, True, False, False]
State prediction error at timestep 2850 is tensor(3.3140e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2850 of None
Current timestep = 2851. State = [[ 0.04293618 -0.14146057]]. Action = [[ 0.08650476  0.04025792  0.         -0.8832901 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 2851 is [False, True, False, True, False, False]
State prediction error at timestep 2851 is tensor(1.9491e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2851 of None
Current timestep = 2852. State = [[ 0.05064596 -0.13607389]]. Action = [[ 0.08111744  0.06492568  0.         -0.9037149 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 2852 is [False, False, True, True, False, False]
State prediction error at timestep 2852 is tensor(2.3707e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2852 of None
Current timestep = 2853. State = [[ 0.05873745 -0.13461375]]. Action = [[ 0.09343558 -0.02945124  0.         -0.8328784 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 2853 is [False, False, True, True, False, False]
State prediction error at timestep 2853 is tensor(2.7992e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2853 of None
Current timestep = 2854. State = [[ 0.06743391 -0.13592182]]. Action = [[ 0.09879056 -0.02499878  0.         -0.63078177]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 2854 is [False, False, True, True, False, False]
State prediction error at timestep 2854 is tensor(3.0779e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2854 of None
Current timestep = 2855. State = [[ 0.07611784 -0.13526607]]. Action = [[ 0.09451846  0.02096228  0.         -0.84595937]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 2855 is [False, False, True, True, False, False]
State prediction error at timestep 2855 is tensor(3.4473e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2855 of None
Current timestep = 2856. State = [[ 0.08435176 -0.13199194]]. Action = [[ 0.08716578  0.04794026  0.         -0.6610124 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 2856 is [False, False, True, True, False, False]
State prediction error at timestep 2856 is tensor(1.4275e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2856 of None
Current timestep = 2857. State = [[ 0.09299145 -0.13113911]]. Action = [[ 0.09901739 -0.01802023  0.         -0.8787474 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 2857 is [False, False, True, True, False, False]
State prediction error at timestep 2857 is tensor(3.2732e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2857 of None
Current timestep = 2858. State = [[ 0.10186357 -0.12960587]]. Action = [[ 0.09665599  0.03654482  0.         -0.72520745]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 2858 is [False, False, True, True, False, False]
State prediction error at timestep 2858 is tensor(2.8296e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2858 of None
Current timestep = 2859. State = [[ 0.10713656 -0.1309199 ]]. Action = [[ 0.01816294 -0.0497555   0.         -0.8480501 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 2859 is [False, False, True, True, False, False]
State prediction error at timestep 2859 is tensor(4.0585e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2859 of None
Current timestep = 2860. State = [[ 0.11453229 -0.13375105]]. Action = [[ 0.0989574  -0.02623798  0.         -0.7806461 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 2860 is [False, False, True, True, False, False]
State prediction error at timestep 2860 is tensor(3.5856e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2860 of None
Current timestep = 2861. State = [[ 0.12356368 -0.1359055 ]]. Action = [[ 0.09324663 -0.01670162  0.         -0.7041868 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 2861 is [False, False, True, True, False, False]
State prediction error at timestep 2861 is tensor(4.3741e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2861 of None
Current timestep = 2862. State = [[ 0.13010642 -0.13706517]]. Action = [[ 4.2625852e-02  7.0543587e-04  0.0000000e+00 -8.3909118e-01]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 2862 is [False, False, True, True, False, False]
State prediction error at timestep 2862 is tensor(7.7666e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2862 of None
Current timestep = 2863. State = [[ 0.13807645 -0.13736686]]. Action = [[ 0.09929086  0.00788323  0.         -0.7350905 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 2863 is [False, False, True, True, False, False]
State prediction error at timestep 2863 is tensor(5.3136e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2863 of None
Current timestep = 2864. State = [[ 0.1471002  -0.13725843]]. Action = [[ 0.09786145  0.01130267  0.         -0.8853493 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 2864 is [False, False, True, True, False, False]
State prediction error at timestep 2864 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2864 of None
Current timestep = 2865. State = [[ 0.15577424 -0.13668828]]. Action = [[ 0.0953437   0.01714727  0.         -0.8913499 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 2865 is [False, False, True, True, False, False]
State prediction error at timestep 2865 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2865 of None
Current timestep = 2866. State = [[ 0.1639536  -0.13773946]]. Action = [[ 0.09046759 -0.01977681  0.         -0.8489671 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 2866 is [False, False, True, True, False, False]
State prediction error at timestep 2866 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2866 of None
Current timestep = 2867. State = [[ 0.172215   -0.13960359]]. Action = [[ 0.09890986 -0.0122257   0.         -0.8860739 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 2867 is [False, False, True, True, False, False]
State prediction error at timestep 2867 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2867 of None
Current timestep = 2868. State = [[ 0.1793184  -0.14225507]]. Action = [[ 0.07437999 -0.02833112  0.         -0.87026274]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 2868 is [False, False, True, True, False, False]
State prediction error at timestep 2868 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2868 of None
Current timestep = 2869. State = [[ 0.18685524 -0.145692  ]]. Action = [[ 0.09988601 -0.03069717  0.         -0.8642599 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 2869 is [False, False, True, True, False, False]
State prediction error at timestep 2869 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2869 of None
Current timestep = 2870. State = [[ 0.1943727  -0.14880136]]. Action = [[ 0.09401441 -0.01766883  0.         -0.86822563]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 2870 is [False, False, True, True, False, False]
State prediction error at timestep 2870 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2870 of None
Current timestep = 2871. State = [[ 0.20141643 -0.1479895 ]]. Action = [[ 0.09481921  0.05396218  0.         -0.9064683 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 2871 is [False, False, True, True, False, False]
State prediction error at timestep 2871 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2871 of None
Current timestep = 2872. State = [[ 0.20827752 -0.14768627]]. Action = [[ 0.0998451  -0.00491128  0.         -0.929639  ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 2872 is [False, False, True, True, False, False]
State prediction error at timestep 2872 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2872 of None
Current timestep = 2873. State = [[ 0.21474533 -0.14982335]]. Action = [[ 0.09787195 -0.02298523  0.         -0.95628697]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 2873 is [False, False, True, True, False, False]
State prediction error at timestep 2873 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2873 of None
Current timestep = 2874. State = [[ 0.22078507 -0.1519523 ]]. Action = [[ 0.09986832 -0.00776386  0.         -0.91388655]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 2874 is [False, False, True, True, False, False]
State prediction error at timestep 2874 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2874 of None
Current timestep = 2875. State = [[ 0.22566265 -0.15453424]]. Action = [[ 0.08425611 -0.02325419  0.         -0.87402207]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 2875 is [False, False, True, True, False, False]
State prediction error at timestep 2875 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2875 of None
Current timestep = 2876. State = [[ 0.23027359 -0.15535451]]. Action = [[ 0.09648336  0.02170776  0.         -0.85952365]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 2876 is [False, False, True, True, False, False]
State prediction error at timestep 2876 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2876 of None
Current timestep = 2877. State = [[ 0.23467995 -0.15615538]]. Action = [[ 0.0991235  -0.00732442  0.         -0.8031973 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 2877 is [False, False, True, True, False, False]
State prediction error at timestep 2877 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2877 of None
Current timestep = 2878. State = [[ 0.23860161 -0.15901563]]. Action = [[ 0.09971613 -0.03355592  0.         -0.89005893]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 2878 is [False, False, True, True, False, False]
State prediction error at timestep 2878 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2878 of None
Current timestep = 2879. State = [[ 0.24194102 -0.16225815]]. Action = [[ 0.09806644 -0.02272695  0.         -0.8401958 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 2879 is [False, False, True, True, False, False]
State prediction error at timestep 2879 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2879 of None
Current timestep = 2880. State = [[ 0.24476403 -0.16442241]]. Action = [[ 0.09517472 -0.0111423   0.         -0.8923329 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 2880 is [False, False, True, True, False, False]
State prediction error at timestep 2880 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2880 of None
Current timestep = 2881. State = [[ 0.24737562 -0.16302747]]. Action = [[ 0.09556963  0.03276684  0.         -0.8437711 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 2881 is [False, False, True, True, False, False]
State prediction error at timestep 2881 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2881 of None
Current timestep = 2882. State = [[ 0.25004527 -0.16102876]]. Action = [[ 0.09954052 -0.01210631  0.         -0.87805295]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 2882 is [False, False, True, True, False, False]
State prediction error at timestep 2882 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2882 of None
Current timestep = 2883. State = [[ 0.25240055 -0.1575261 ]]. Action = [[ 0.09449805  0.02004038  0.         -0.901717  ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 2883 is [False, False, True, True, False, False]
State prediction error at timestep 2883 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2883 of None
Current timestep = 2884. State = [[ 0.25455713 -0.15519099]]. Action = [[ 0.09647474 -0.03608389  0.         -0.92876065]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 2884 is [False, False, True, True, False, False]
State prediction error at timestep 2884 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2884 of None
Current timestep = 2885. State = [[ 0.25646174 -0.15331767]]. Action = [[ 0.09357973 -0.01403946  0.         -0.8107056 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 2885 is [False, False, True, True, False, False]
State prediction error at timestep 2885 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2885 of None
Current timestep = 2886. State = [[ 0.2583659  -0.15121204]]. Action = [[ 0.09786158 -0.01580046  0.         -0.88114566]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 2886 is [False, False, True, True, False, False]
State prediction error at timestep 2886 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2886 of None
Current timestep = 2887. State = [[ 0.26032442 -0.150207  ]]. Action = [[ 0.09876914 -0.02908237  0.         -0.8607315 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 2887 is [False, False, True, True, False, False]
State prediction error at timestep 2887 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2887 of None
Current timestep = 2888. State = [[ 0.26227015 -0.15221103]]. Action = [[ 0.0981716  -0.06755158  0.         -0.830043  ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 2888 is [False, False, True, True, False, False]
State prediction error at timestep 2888 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2888 of None
Current timestep = 2889. State = [[ 0.26395616 -0.15482935]]. Action = [[ 0.09276027 -0.03957002  0.         -0.9036817 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 2889 is [False, False, True, True, False, False]
State prediction error at timestep 2889 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2889 of None
Current timestep = 2890. State = [[ 0.26528892 -0.15530884]]. Action = [[ 8.663412e-02  7.050410e-04  0.000000e+00 -9.228774e-01]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 2890 is [False, False, True, True, False, False]
State prediction error at timestep 2890 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2890 of None
Current timestep = 2891. State = [[ 0.26705402 -0.15606278]]. Action = [[ 0.09779815 -0.02366634  0.         -0.8411865 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 2891 is [False, False, True, True, False, False]
State prediction error at timestep 2891 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2891 of None
Current timestep = 2892. State = [[ 0.26907885 -0.15901104]]. Action = [[ 0.09801538 -0.04960941  0.         -0.8788959 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 2892 is [False, False, True, True, False, False]
State prediction error at timestep 2892 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2892 of None
Current timestep = 2893. State = [[ 0.2709044  -0.16124438]]. Action = [[ 0.09315459 -0.01347575  0.         -0.9603886 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 2893 is [False, False, True, True, False, False]
State prediction error at timestep 2893 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2893 of None
Current timestep = 2894. State = [[ 0.27287254 -0.16415066]]. Action = [[ 0.09765127 -0.04553856  0.         -0.91977936]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 2894 is [False, False, True, True, False, False]
State prediction error at timestep 2894 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2894 of None
Current timestep = 2895. State = [[ 0.27493727 -0.16702132]]. Action = [[ 0.09687092 -0.02513252  0.         -0.82115245]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 2895 is [False, False, True, True, False, False]
State prediction error at timestep 2895 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2895 of None
Current timestep = 2896. State = [[ 0.27620468 -0.16861376]]. Action = [[ 0.09760768 -0.0364431   0.         -0.7583663 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 2896 is [False, False, True, True, False, False]
State prediction error at timestep 2896 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2896 of None
Current timestep = 2897. State = [[ 0.2767123  -0.16973445]]. Action = [[ 0.06961153 -0.040031    0.         -0.828356  ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 2897 is [False, False, True, True, False, False]
State prediction error at timestep 2897 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2897 of None
Current timestep = 2898. State = [[ 0.2777154  -0.17043155]]. Action = [[ 0.06792117 -0.02611016  0.         -0.77480865]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 2898 is [False, False, True, True, False, False]
State prediction error at timestep 2898 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2898 of None
Current timestep = 2899. State = [[ 0.27926686 -0.17027125]]. Action = [[ 0.07482577 -0.00601885  0.         -0.87317985]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 2899 is [False, False, True, True, False, False]
State prediction error at timestep 2899 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2899 of None
Current timestep = 2900. State = [[ 0.28151026 -0.17152046]]. Action = [[ 0.09733766 -0.05355255  0.         -0.82813835]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 2900 is [False, False, True, True, False, False]
State prediction error at timestep 2900 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2900 of None
Current timestep = 2901. State = [[ 0.28448507 -0.17399931]]. Action = [[ 0.097812   -0.05436627  0.         -0.7725414 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 2901 is [False, False, True, True, False, False]
State prediction error at timestep 2901 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2901 of None
Current timestep = 2902. State = [[ 0.28774586 -0.1751927 ]]. Action = [[ 0.0958646  -0.01217096  0.         -0.9181272 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 2902 is [False, False, True, True, False, False]
State prediction error at timestep 2902 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2902 of None
Current timestep = 2903. State = [[ 0.29089653 -0.17347819]]. Action = [[ 0.09136451  0.03800691  0.         -0.81685954]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 2903 is [False, False, True, True, False, False]
State prediction error at timestep 2903 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2903 of None
Current timestep = 2904. State = [[ 0.2936517  -0.17471094]]. Action = [[ 0.09476373 -0.06186253  0.         -0.82573235]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 2904 is [False, False, True, True, False, False]
State prediction error at timestep 2904 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2904 of None
Current timestep = 2905. State = [[ 0.29660752 -0.1783237 ]]. Action = [[ 0.09305022 -0.06155098  0.         -0.6803757 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 2905 is [False, False, True, True, False, False]
State prediction error at timestep 2905 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2905 of None
Current timestep = 2906. State = [[ 0.29977357 -0.18251862]]. Action = [[ 0.09892855 -0.06821813  0.         -0.7548443 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 2906 is [False, False, True, True, False, False]
State prediction error at timestep 2906 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2906 of None
Current timestep = 2907. State = [[ 0.30324644 -0.1820952 ]]. Action = [[ 0.09775359  0.04661674  0.         -0.8954485 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 2907 is [False, False, True, True, False, False]
State prediction error at timestep 2907 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2907 of None
Current timestep = 2908. State = [[ 0.3060562  -0.18327162]]. Action = [[ 0.0977139  -0.06277446  0.         -0.835239  ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 2908 is [False, False, True, True, False, False]
State prediction error at timestep 2908 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2908 of None
Current timestep = 2909. State = [[ 0.30870634 -0.18692818]]. Action = [[ 0.09930981 -0.0614838   0.         -0.8841685 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 2909 is [False, False, True, True, False, False]
State prediction error at timestep 2909 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2909 of None
Current timestep = 2910. State = [[ 0.31153345 -0.18893513]]. Action = [[ 0.0963785  -0.01679142  0.         -0.8089334 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 2910 is [False, False, True, True, False, False]
State prediction error at timestep 2910 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2910 of None
Current timestep = 2911. State = [[ 0.3140093  -0.19142826]]. Action = [[ 0.09365409 -0.05738571  0.         -0.78614426]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 2911 is [False, False, True, True, False, False]
State prediction error at timestep 2911 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2911 of None
Current timestep = 2912. State = [[ 0.31647587 -0.19283785]]. Action = [[ 0.09882193 -0.01056258  0.         -0.7274953 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 2912 is [False, False, True, True, False, False]
State prediction error at timestep 2912 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2912 of None
Current timestep = 2913. State = [[ 0.3185465  -0.19431385]]. Action = [[ 0.09830108 -0.04592584  0.         -0.8564729 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 2913 is [False, False, True, True, False, False]
State prediction error at timestep 2913 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2913 of None
Current timestep = 2914. State = [[ 0.32028905 -0.19796918]]. Action = [[ 0.0922252  -0.08209811  0.         -0.9150217 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 2914 is [False, False, True, True, False, False]
State prediction error at timestep 2914 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2914 of None
Current timestep = 2915. State = [[ 0.32225    -0.20071311]]. Action = [[ 0.09950592 -0.03757212  0.         -0.81572664]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 2915 is [False, False, True, True, False, False]
State prediction error at timestep 2915 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2915 of None
Current timestep = 2916. State = [[ 0.3245125  -0.19781232]]. Action = [[ 0.09704111  0.073144    0.         -0.73558986]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 2916 is [False, False, True, True, False, False]
State prediction error at timestep 2916 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2916 of None
Current timestep = 2917. State = [[ 0.3257459  -0.19748735]]. Action = [[ 0.09448916 -0.06115413  0.         -0.91200465]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 2917 is [False, False, True, True, False, False]
State prediction error at timestep 2917 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2917 of None
Current timestep = 2918. State = [[ 0.32689324 -0.19830367]]. Action = [[ 0.0768581  -0.01311506  0.         -0.9255485 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 2918 is [False, False, True, True, False, False]
State prediction error at timestep 2918 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2918 of None
Current timestep = 2919. State = [[ 0.32508078 -0.20052846]]. Action = [[-0.04521393 -0.03373938  0.         -0.81426704]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 2919 is [False, False, True, True, False, False]
State prediction error at timestep 2919 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2919 of None
Current timestep = 2920. State = [[ 0.32411364 -0.20347558]]. Action = [[ 0.09712277 -0.05699015  0.         -0.89123446]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 2920 is [False, False, True, True, False, False]
State prediction error at timestep 2920 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2920 of None
Current timestep = 2921. State = [[ 0.3257878  -0.20581283]]. Action = [[ 0.08772136 -0.04439838  0.         -0.94712615]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 2921 is [False, False, True, True, False, False]
State prediction error at timestep 2921 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2921 of None
Current timestep = 2922. State = [[ 0.32794842 -0.20590502]]. Action = [[ 0.09961773  0.00387583  0.         -0.9354842 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 2922 is [False, False, True, True, False, False]
State prediction error at timestep 2922 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2922 of None
Current timestep = 2923. State = [[ 0.32931513 -0.20683934]]. Action = [[ 0.09166355 -0.05736761  0.         -0.9377899 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 2923 is [False, False, True, True, False, False]
State prediction error at timestep 2923 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2923 of None
Current timestep = 2924. State = [[ 0.3302619 -0.2089064]]. Action = [[ 0.09683924 -0.06187432  0.         -0.79184693]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 2924 is [False, False, True, True, False, False]
State prediction error at timestep 2924 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2924 of None
Current timestep = 2925. State = [[ 0.33145562 -0.20861632]]. Action = [[ 0.09546755  0.01262067  0.         -0.89520615]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 2925 is [False, False, True, True, False, False]
State prediction error at timestep 2925 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2925 of None
Current timestep = 2926. State = [[ 0.33263457 -0.21024537]]. Action = [[ 0.09628116 -0.06912982  0.         -0.8786641 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 2926 is [False, False, True, True, False, False]
State prediction error at timestep 2926 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2926 of None
Current timestep = 2927. State = [[ 0.3337785  -0.21070422]]. Action = [[ 0.09442199 -0.0177378   0.         -0.71579134]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 2927 is [False, False, True, True, False, False]
State prediction error at timestep 2927 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2927 of None
Current timestep = 2928. State = [[ 0.33470312 -0.20955563]]. Action = [[ 9.1916524e-02  3.1598657e-04  0.0000000e+00 -8.6567700e-01]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 2928 is [False, False, True, True, False, False]
State prediction error at timestep 2928 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2928 of None
Current timestep = 2929. State = [[ 0.3357128  -0.20858763]]. Action = [[ 0.09906655 -0.00510427  0.         -0.9043927 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 2929 is [False, False, True, True, False, False]
State prediction error at timestep 2929 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2929 of None
Current timestep = 2930. State = [[ 0.33649334 -0.20720851]]. Action = [[ 0.09916393 -0.00912405  0.         -0.8621846 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 2930 is [False, False, True, True, False, False]
State prediction error at timestep 2930 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2930 of None
Current timestep = 2931. State = [[ 0.33718213 -0.20599505]]. Action = [[ 0.09813256 -0.00216679  0.         -0.8993927 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 2931 is [False, False, True, True, False, False]
State prediction error at timestep 2931 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2931 of None
Current timestep = 2932. State = [[ 0.3372904  -0.20758489]]. Action = [[ 0.03990317 -0.06083823  0.         -0.96124303]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 2932 is [False, False, True, True, False, False]
State prediction error at timestep 2932 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2932 of None
Current timestep = 2933. State = [[ 0.33771852 -0.20813209]]. Action = [[ 0.09829431 -0.02081513  0.         -0.7325798 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 2933 is [False, False, True, True, False, False]
State prediction error at timestep 2933 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2933 of None
Current timestep = 2934. State = [[ 0.33830968 -0.20589939]]. Action = [[ 0.08985164  0.02828611  0.         -0.8920004 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 2934 is [False, False, True, True, False, False]
State prediction error at timestep 2934 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2934 of None
Current timestep = 2935. State = [[ 0.3388835 -0.2037374]]. Action = [[ 0.09236499  0.01936072  0.         -0.7663349 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 2935 is [False, False, True, True, False, False]
State prediction error at timestep 2935 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2935 of None
Current timestep = 2936. State = [[ 0.33911318 -0.2041723 ]]. Action = [[ 0.09825934 -0.06405872  0.         -0.85383797]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 2936 is [False, False, True, True, False, False]
State prediction error at timestep 2936 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2936 of None
Current timestep = 2937. State = [[ 0.33952832 -0.20626454]]. Action = [[ 0.0986999  -0.08501398  0.         -0.88898623]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 2937 is [False, False, True, True, False, False]
State prediction error at timestep 2937 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2937 of None
Current timestep = 2938. State = [[ 0.33981073 -0.20651259]]. Action = [[ 0.09981725  0.01373805  0.         -0.90574175]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 2938 is [False, False, True, True, False, False]
State prediction error at timestep 2938 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2938 of None
Current timestep = 2939. State = [[ 0.34004495 -0.20602883]]. Action = [[ 0.09874875 -0.01167121  0.         -0.95510715]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 2939 is [False, False, True, True, False, False]
State prediction error at timestep 2939 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2939 of None
Current timestep = 2940. State = [[ 0.34021348 -0.20580006]]. Action = [[ 0.09826713 -0.01654962  0.         -0.8927189 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 2940 is [False, False, True, True, False, False]
State prediction error at timestep 2940 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2940 of None
Current timestep = 2941. State = [[ 0.3402724  -0.20595509]]. Action = [[ 0.09984367 -0.04006004  0.         -0.9176139 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 2941 is [False, False, True, True, False, False]
State prediction error at timestep 2941 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2941 of None
Current timestep = 2942. State = [[ 0.34027565 -0.20701836]]. Action = [[ 0.08034397 -0.05741298  0.         -0.8939164 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 2942 is [False, False, True, True, False, False]
State prediction error at timestep 2942 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2942 of None
Current timestep = 2943. State = [[ 0.34028676 -0.20744433]]. Action = [[ 0.09817437 -0.03808947  0.         -0.8212203 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 2943 is [False, False, True, True, False, False]
State prediction error at timestep 2943 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2943 of None
Current timestep = 2944. State = [[ 0.34029225 -0.20895767]]. Action = [[ 0.05948246 -0.07046102  0.         -0.86153674]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 2944 is [False, False, True, True, False, False]
State prediction error at timestep 2944 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2944 of None
Current timestep = 2945. State = [[ 0.3403163  -0.21012393]]. Action = [[ 0.0994622  -0.06593718  0.         -0.8725382 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 2945 is [False, False, True, True, False, False]
State prediction error at timestep 2945 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2945 of None
Current timestep = 2946. State = [[ 0.3403163  -0.21085727]]. Action = [[ 0.09976543 -0.06870545  0.         -0.92800903]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 2946 is [False, False, True, True, False, False]
State prediction error at timestep 2946 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2946 of None
Current timestep = 2947. State = [[ 0.34030458 -0.21164936]]. Action = [[ 0.09710761 -0.07207848  0.         -0.8970021 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 2947 is [False, False, True, True, False, False]
State prediction error at timestep 2947 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2947 of None
Current timestep = 2948. State = [[ 0.34029162 -0.21227859]]. Action = [[ 0.09341107 -0.06621505  0.         -0.93541205]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 2948 is [False, False, True, True, False, False]
State prediction error at timestep 2948 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2948 of None
Current timestep = 2949. State = [[ 0.34045345 -0.20955308]]. Action = [[ 0.09945799  0.05137124  0.         -0.8640991 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 2949 is [False, False, True, True, False, False]
State prediction error at timestep 2949 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2949 of None
Current timestep = 2950. State = [[ 0.34050038 -0.20916055]]. Action = [[ 0.08807278 -0.07551221  0.         -0.6902697 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 2950 is [False, False, True, True, False, False]
State prediction error at timestep 2950 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2950 of None
Current timestep = 2951. State = [[ 0.3405147  -0.20973454]]. Action = [[ 0.09978477 -0.05547893  0.         -0.84899956]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 2951 is [False, False, True, True, False, False]
State prediction error at timestep 2951 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2951 of None
Current timestep = 2952. State = [[ 0.34052828 -0.20998955]]. Action = [[ 0.09913952 -0.05507355  0.         -0.85643446]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 2952 is [False, False, True, True, False, False]
State prediction error at timestep 2952 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2952 of None
Current timestep = 2953. State = [[ 0.33912987 -0.2120294 ]]. Action = [[-0.00445735 -0.05763955  0.         -0.78951216]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 2953 is [False, False, True, True, False, False]
State prediction error at timestep 2953 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2953 of None
Current timestep = 2954. State = [[ 0.33853176 -0.21444246]]. Action = [[ 0.09931151 -0.06922118  0.         -0.9092039 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 2954 is [False, False, True, True, False, False]
State prediction error at timestep 2954 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2954 of None
Current timestep = 2955. State = [[ 0.33856842 -0.21642499]]. Action = [[ 0.09684468 -0.08441388  0.         -0.8979756 ]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 2955 is [False, False, True, True, False, False]
State prediction error at timestep 2955 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2955 of None
Current timestep = 2956. State = [[ 0.33858848 -0.21626781]]. Action = [[ 0.09939154 -0.01385272  0.         -0.8879806 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 2956 is [False, False, True, True, False, False]
State prediction error at timestep 2956 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2956 of None
Current timestep = 2957. State = [[ 0.33857942 -0.21603775]]. Action = [[ 0.09927607 -0.05097311  0.         -0.6834101 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 2957 is [False, False, True, True, False, False]
State prediction error at timestep 2957 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2957 of None
Current timestep = 2958. State = [[ 0.33856276 -0.21686342]]. Action = [[ 0.09289537 -0.07883096  0.         -0.93201244]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 2958 is [False, False, True, True, False, False]
State prediction error at timestep 2958 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2958 of None
Current timestep = 2959. State = [[ 0.3385749  -0.21653672]]. Action = [[ 0.09593844 -0.01963142  0.         -0.5220117 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 2959 is [False, False, True, True, False, False]
State prediction error at timestep 2959 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2959 of None
Current timestep = 2960. State = [[ 0.33857945 -0.2165624 ]]. Action = [[ 0.09306326 -0.06545033  0.         -0.9511539 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 2960 is [False, False, True, True, False, False]
State prediction error at timestep 2960 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2960 of None
Current timestep = 2961. State = [[ 0.3385987 -0.2163879]]. Action = [[ 0.09685577 -0.03355484  0.         -0.8622947 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 2961 is [False, False, True, True, False, False]
State prediction error at timestep 2961 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2961 of None
Current timestep = 2962. State = [[ 0.33862862 -0.21512227]]. Action = [[ 8.796989e-02 -2.719313e-04  0.000000e+00 -9.401473e-01]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 2962 is [False, False, True, True, False, False]
State prediction error at timestep 2962 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2962 of None
Current timestep = 2963. State = [[ 0.33864975 -0.21472324]]. Action = [[ 0.09809656 -0.05015267  0.         -0.96319   ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 2963 is [False, False, True, True, False, False]
State prediction error at timestep 2963 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2963 of None
Current timestep = 2964. State = [[ 0.3386708 -0.2157415]]. Action = [[ 0.09639219 -0.08333497  0.         -0.8980233 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 2964 is [False, False, True, True, False, False]
State prediction error at timestep 2964 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2964 of None
Current timestep = 2965. State = [[ 0.33870363 -0.2159121 ]]. Action = [[ 0.09527319 -0.02958287  0.         -0.8467746 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 2965 is [False, False, True, True, False, False]
State prediction error at timestep 2965 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2965 of None
Current timestep = 2966. State = [[ 0.3387221  -0.21620575]]. Action = [[ 0.09211016 -0.06261583  0.         -0.9617204 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 2966 is [False, False, True, True, False, False]
State prediction error at timestep 2966 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2966 of None
Current timestep = 2967. State = [[ 0.33873686 -0.21725029]]. Action = [[ 0.08907526 -0.07879896  0.         -0.576745  ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 2967 is [False, False, True, True, False, False]
State prediction error at timestep 2967 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2967 of None
Current timestep = 2968. State = [[ 0.3387562  -0.21739468]]. Action = [[ 0.09839534 -0.03990154  0.         -0.8009241 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 2968 is [False, False, True, True, False, False]
State prediction error at timestep 2968 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2968 of None
Current timestep = 2969. State = [[ 0.3387714  -0.21708259]]. Action = [[ 0.09763692 -0.03858339  0.         -0.8729689 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 2969 is [False, False, True, True, False, False]
State prediction error at timestep 2969 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2969 of None
Current timestep = 2970. State = [[ 0.3387823  -0.21723418]]. Action = [[ 0.09831124 -0.06704515  0.         -0.9635081 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 2970 is [False, False, True, True, False, False]
State prediction error at timestep 2970 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2970 of None
Current timestep = 2971. State = [[ 0.3388011  -0.21730372]]. Action = [[ 0.07969404 -0.04108608  0.         -0.8462329 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 2971 is [False, False, True, True, False, False]
State prediction error at timestep 2971 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2971 of None
Current timestep = 2972. State = [[ 0.33882976 -0.21655078]]. Action = [[ 0.09468352 -0.02135292  0.         -0.8649967 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 2972 is [False, False, True, True, False, False]
State prediction error at timestep 2972 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2972 of None
Current timestep = 2973. State = [[ 0.33885413 -0.21641365]]. Action = [[ 0.09527598 -0.06117379  0.         -0.879869  ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 2973 is [False, False, True, True, False, False]
State prediction error at timestep 2973 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2973 of None
Current timestep = 2974. State = [[ 0.33887473 -0.21646176]]. Action = [[ 0.09951336 -0.05137866  0.         -0.9209062 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 2974 is [False, False, True, True, False, False]
State prediction error at timestep 2974 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2974 of None
Current timestep = 2975. State = [[ 0.33889025 -0.21647751]]. Action = [[ 0.09580327 -0.04946078  0.         -0.9029801 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 2975 is [False, False, True, True, False, False]
State prediction error at timestep 2975 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2975 of None
Current timestep = 2976. State = [[ 0.3389088  -0.21605872]]. Action = [[ 0.09407706 -0.02607837  0.         -0.86717844]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 2976 is [False, False, True, True, False, False]
State prediction error at timestep 2976 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2976 of None
Current timestep = 2977. State = [[ 0.33893132 -0.2165291 ]]. Action = [[ 0.09850124 -0.07877474  0.         -0.91158485]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 2977 is [False, False, True, True, False, False]
State prediction error at timestep 2977 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2977 of None
Current timestep = 2978. State = [[ 0.33896178 -0.21770757]]. Action = [[ 0.09957973 -0.08645009  0.         -0.8950267 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 2978 is [False, False, True, True, False, False]
State prediction error at timestep 2978 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2978 of None
Current timestep = 2979. State = [[ 0.33898425 -0.21828529]]. Action = [[ 0.09958375 -0.08648493  0.         -0.87805676]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 2979 is [False, False, True, True, False, False]
State prediction error at timestep 2979 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2979 of None
Current timestep = 2980. State = [[ 0.33900273 -0.2182483 ]]. Action = [[ 0.09031347 -0.06398001  0.         -0.92750436]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 2980 is [False, False, True, True, False, False]
State prediction error at timestep 2980 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2980 of None
Current timestep = 2981. State = [[ 0.3390234  -0.21828587]]. Action = [[ 0.07483897 -0.07943726  0.         -0.90080714]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 2981 is [False, False, True, True, False, False]
State prediction error at timestep 2981 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2981 of None
Current timestep = 2982. State = [[ 0.3390525  -0.21641773]]. Action = [[ 0.09181658  0.01576754  0.         -0.96015704]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 2982 is [False, False, True, True, False, False]
State prediction error at timestep 2982 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2982 of None
Current timestep = 2983. State = [[ 0.33907723 -0.21463734]]. Action = [[ 0.09796784 -0.01633859  0.         -0.9468771 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 2983 is [False, False, True, True, False, False]
State prediction error at timestep 2983 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2983 of None
Current timestep = 2984. State = [[ 0.33911315 -0.21400599]]. Action = [[ 0.09939492 -0.03280406  0.         -0.86382717]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 2984 is [False, False, True, True, False, False]
State prediction error at timestep 2984 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2984 of None
Current timestep = 2985. State = [[ 0.33915725 -0.21239935]]. Action = [[ 0.09834216  0.01988757  0.         -0.722541  ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 2985 is [False, False, True, True, False, False]
State prediction error at timestep 2985 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2985 of None
Current timestep = 2986. State = [[ 0.33919716 -0.21103369]]. Action = [[ 0.09731557 -0.0026485   0.         -0.8237385 ]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 2986 is [False, False, True, True, False, False]
State prediction error at timestep 2986 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2986 of None
Current timestep = 2987. State = [[ 0.33924073 -0.21017809]]. Action = [[ 9.791467e-02 -5.702898e-04  0.000000e+00 -8.980146e-01]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 2987 is [False, False, True, True, False, False]
State prediction error at timestep 2987 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2987 of None
Current timestep = 2988. State = [[ 0.33927673 -0.21092801]]. Action = [[ 0.09749777 -0.06167394  0.         -0.92731124]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 2988 is [False, False, True, True, False, False]
State prediction error at timestep 2988 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2988 of None
Current timestep = 2989. State = [[ 0.3393129  -0.21312913]]. Action = [[ 0.09552015 -0.08457995  0.         -0.8746757 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 2989 is [False, False, True, True, False, False]
State prediction error at timestep 2989 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2989 of None
Current timestep = 2990. State = [[ 0.33934632 -0.21631967]]. Action = [[ 0.09261876 -0.09717348  0.         -0.891381  ]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 2990 is [False, False, True, True, False, False]
State prediction error at timestep 2990 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2990 of None
Current timestep = 2991. State = [[ 0.33945996 -0.21869084]]. Action = [[ 0.0908627 -0.0805424  0.        -0.9272212]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 2991 is [False, False, True, True, False, False]
State prediction error at timestep 2991 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2991 of None
Current timestep = 2992. State = [[ 0.3395283 -0.2184324]]. Action = [[ 0.07623019 -0.01543163  0.         -0.6520422 ]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 2992 is [False, False, True, True, False, False]
State prediction error at timestep 2992 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2992 of None
Current timestep = 2993. State = [[ 0.3395708  -0.21851218]]. Action = [[ 0.0963171  -0.08025678  0.         -0.8954134 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 2993 is [False, False, True, True, False, False]
State prediction error at timestep 2993 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2993 of None
Current timestep = 2994. State = [[ 0.33959803 -0.21687132]]. Action = [[ 0.09651969  0.02084128  0.         -0.9313637 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 2994 is [False, False, True, True, False, False]
State prediction error at timestep 2994 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2994 of None
Current timestep = 2995. State = [[ 0.3396183  -0.21701631]]. Action = [[ 0.07428863 -0.06937395  0.         -0.8678365 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 2995 is [False, False, True, True, False, False]
State prediction error at timestep 2995 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2995 of None
Current timestep = 2996. State = [[ 0.3396439  -0.21761487]]. Action = [[ 0.09895182 -0.05800375  0.         -0.9961479 ]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 2996 is [False, False, True, True, False, False]
State prediction error at timestep 2996 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2996 of None
Current timestep = 2997. State = [[ 0.33970162 -0.21544172]]. Action = [[ 0.09468097  0.04368878  0.         -0.9474562 ]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 2997 is [False, False, True, True, False, False]
State prediction error at timestep 2997 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2997 of None
Current timestep = 2998. State = [[ 0.3397303  -0.21496043]]. Action = [[ 0.0984465  -0.06070744  0.         -0.9702716 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 2998 is [False, False, True, True, False, False]
State prediction error at timestep 2998 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2998 of None
Current timestep = 2999. State = [[ 0.33975518 -0.21538429]]. Action = [[ 0.09681148 -0.04489622  0.         -0.8940739 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 2999 is [False, False, True, True, False, False]
State prediction error at timestep 2999 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2999 of None
Current timestep = 3000. State = [[ 0.33978    -0.21667548]]. Action = [[ 0.09151743 -0.07767384  0.         -0.9752536 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 3000 is [False, False, True, True, False, False]
State prediction error at timestep 3000 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3000 of None
Current timestep = 3001. State = [[ 0.3398     -0.21717408]]. Action = [[ 0.09586192 -0.03808277  0.         -0.8768494 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 3001 is [False, False, True, True, False, False]
State prediction error at timestep 3001 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3001 of None
Current timestep = 3002. State = [[ 0.3398176  -0.21681316]]. Action = [[ 0.08440652 -0.01491041  0.         -0.94473314]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 3002 is [False, False, True, True, False, False]
State prediction error at timestep 3002 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3002 of None
Current timestep = 3003. State = [[ 0.34005943 -0.21393973]]. Action = [[ 0.09248631  0.06116479  0.         -0.86796105]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 3003 is [False, False, True, True, False, False]
State prediction error at timestep 3003 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3003 of None
Current timestep = 3004. State = [[ 0.3403797  -0.21164928]]. Action = [[ 0.08522076  0.0129359   0.         -0.683881  ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 3004 is [False, False, True, True, False, False]
State prediction error at timestep 3004 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3004 of None
Current timestep = 3005. State = [[ 0.34042576 -0.21251191]]. Action = [[ 0.09240618 -0.08623369  0.         -0.9165333 ]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 3005 is [False, False, True, True, False, False]
State prediction error at timestep 3005 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3005 of None
Current timestep = 3006. State = [[ 0.34046078 -0.21273328]]. Action = [[ 0.09707075 -0.01955172  0.         -0.9519706 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 3006 is [False, False, True, True, False, False]
State prediction error at timestep 3006 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3006 of None
Current timestep = 3007. State = [[ 0.34048137 -0.21322703]]. Action = [[ 0.0937831  -0.0522619   0.         -0.92691404]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 3007 is [False, False, True, True, False, False]
State prediction error at timestep 3007 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3007 of None
Current timestep = 3008. State = [[ 0.34048906 -0.21522835]]. Action = [[ 0.08535869 -0.08685482  0.         -0.93491894]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 3008 is [False, False, True, True, False, False]
State prediction error at timestep 3008 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3008 of None
Current timestep = 3009. State = [[ 0.34050006 -0.21761169]]. Action = [[ 0.09731369 -0.09773521  0.         -0.7912014 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 3009 is [False, False, True, True, False, False]
State prediction error at timestep 3009 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3009 of None
Current timestep = 3010. State = [[ 0.3405928 -0.2189881]]. Action = [[ 0.08590088 -0.0861725   0.         -0.89806503]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 3010 is [False, False, True, True, False, False]
State prediction error at timestep 3010 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3010 of None
Current timestep = 3011. State = [[ 0.34063923 -0.2169388 ]]. Action = [[ 0.08715452  0.03573411  0.         -0.9380059 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 3011 is [False, False, True, True, False, False]
State prediction error at timestep 3011 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3011 of None
Current timestep = 3012. State = [[ 0.34066033 -0.214829  ]]. Action = [[ 0.09550656 -0.00719848  0.         -0.96817297]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 3012 is [False, False, True, True, False, False]
State prediction error at timestep 3012 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3012 of None
Current timestep = 3013. State = [[ 0.34067914 -0.21274701]]. Action = [[ 0.0983002   0.01734922  0.         -0.81496024]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 3013 is [False, False, True, True, False, False]
State prediction error at timestep 3013 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3013 of None
Current timestep = 3014. State = [[ 0.34075487 -0.21117285]]. Action = [[0.0904064  0.00415959 0.         0.13875663]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 3014 is [False, False, True, True, False, False]
State prediction error at timestep 3014 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3014 of None
Current timestep = 3015. State = [[ 0.34081697 -0.21172044]]. Action = [[ 0.09425654 -0.06350536  0.         -0.30638343]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 3015 is [False, False, True, True, False, False]
State prediction error at timestep 3015 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3015 of None
Current timestep = 3016. State = [[ 0.34083593 -0.21329151]]. Action = [[ 0.09849154 -0.07078894  0.         -0.88857865]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 3016 is [False, False, True, True, False, False]
State prediction error at timestep 3016 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3016 of None
Current timestep = 3017. State = [[ 0.340843   -0.21576165]]. Action = [[ 0.09775691 -0.09993572  0.         -0.9425538 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 3017 is [False, False, True, True, False, False]
State prediction error at timestep 3017 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3017 of None
Current timestep = 3018. State = [[ 0.34084678 -0.21739142]]. Action = [[ 0.0992968  -0.06821994  0.         -0.96766037]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 3018 is [False, False, True, True, False, False]
State prediction error at timestep 3018 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3018 of None
Current timestep = 3019. State = [[ 0.34084857 -0.21846633]]. Action = [[ 0.09690583 -0.07419933  0.         -0.8672341 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 3019 is [False, False, True, True, False, False]
State prediction error at timestep 3019 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3019 of None
Current timestep = 3020. State = [[ 0.3408503  -0.21785814]]. Action = [[ 0.09609831 -0.01126814  0.         -0.7492411 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 3020 is [False, False, True, True, False, False]
State prediction error at timestep 3020 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3020 of None
Current timestep = 3021. State = [[ 0.34085253 -0.21863404]]. Action = [[ 0.08510242 -0.09415931  0.         -0.92346406]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 3021 is [False, False, True, True, False, False]
State prediction error at timestep 3021 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3021 of None
Current timestep = 3022. State = [[ 0.34086323 -0.21802643]]. Action = [[ 0.09056769 -0.00623084  0.         -0.07050741]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 3022 is [False, False, True, True, False, False]
State prediction error at timestep 3022 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3022 of None
Current timestep = 3023. State = [[ 0.34086734 -0.21853893]]. Action = [[ 0.09928337 -0.09939777  0.         -0.9793878 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 3023 is [False, False, True, True, False, False]
State prediction error at timestep 3023 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3023 of None
Current timestep = 3024. State = [[ 0.34088567 -0.2181476 ]]. Action = [[ 0.06757327 -0.00372694  0.         -0.8924956 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 3024 is [False, False, True, True, False, False]
State prediction error at timestep 3024 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3024 of None
Current timestep = 3025. State = [[ 0.34090543 -0.21732236]]. Action = [[ 0.06073936 -0.02200874  0.         -0.71818435]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 3025 is [False, False, True, True, False, False]
State prediction error at timestep 3025 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3025 of None
Current timestep = 3026. State = [[ 0.34135392 -0.21283111]]. Action = [[ 0.09602808  0.09690902  0.         -0.60865545]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 3026 is [False, False, True, True, False, False]
State prediction error at timestep 3026 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3026 of None
Current timestep = 3027. State = [[ 0.34146297 -0.21215022]]. Action = [[ 0.09857818 -0.09992956  0.         -0.85862786]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 3027 is [False, False, True, True, False, False]
State prediction error at timestep 3027 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3027 of None
Current timestep = 3028. State = [[ 0.3417798  -0.20858416]]. Action = [[ 0.09293327  0.09857235  0.         -0.98282814]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 3028 is [False, False, True, True, False, False]
State prediction error at timestep 3028 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3028 of None
Current timestep = 3029. State = [[ 0.34236312 -0.20247324]]. Action = [[ 0.09747472  0.09238464  0.         -0.3054052 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 3029 is [False, False, True, True, False, False]
State prediction error at timestep 3029 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3029 of None
Current timestep = 3030. State = [[ 0.34244895 -0.2028506 ]]. Action = [[ 0.09432756 -0.09511688  0.         -0.94909656]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 3030 is [False, False, True, True, False, False]
State prediction error at timestep 3030 is tensor(6.7037e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3030 of None
Current timestep = 3031. State = [[ 0.34244043 -0.20736812]]. Action = [[ 0.09860731 -0.09954625  0.         -0.86441827]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 3031 is [False, False, True, True, False, False]
State prediction error at timestep 3031 is tensor(8.8537e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3031 of None
Current timestep = 3032. State = [[ 0.3425197 -0.2050119]]. Action = [[ 0.08299949  0.09981637  0.         -0.99766254]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 3032 is [False, False, True, True, False, False]
State prediction error at timestep 3032 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3032 of None
Current timestep = 3033. State = [[ 0.34256503 -0.20588088]]. Action = [[ 0.08720858 -0.09450526  0.         -0.4085325 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 3033 is [False, False, True, True, False, False]
State prediction error at timestep 3033 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3033 of None
Current timestep = 3034. State = [[ 0.3425942  -0.20664427]]. Action = [[ 0.09255414  0.00356732  0.         -0.41430986]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 3034 is [False, False, True, True, False, False]
State prediction error at timestep 3034 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3034 of None
Current timestep = 3035. State = [[ 0.34260792 -0.20854093]]. Action = [[ 0.08909567 -0.08425127  0.         -0.5006611 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 3035 is [False, False, True, True, False, False]
State prediction error at timestep 3035 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3035 of None
Current timestep = 3036. State = [[ 0.34282842 -0.2061555 ]]. Action = [[0.09148299 0.09961953 0.         0.40329516]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 3036 is [False, False, True, True, False, False]
State prediction error at timestep 3036 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3036 of None
Current timestep = 3037. State = [[ 0.34329197 -0.20109186]]. Action = [[ 0.0809866   0.09954531  0.         -0.3163954 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 3037 is [False, False, True, True, False, False]
State prediction error at timestep 3037 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3037 of None
Current timestep = 3038. State = [[ 0.34338632 -0.20136447]]. Action = [[ 0.08776889 -0.07291125  0.         -0.77871346]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 3038 is [False, False, True, True, False, False]
State prediction error at timestep 3038 is tensor(9.6506e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3038 of None
Current timestep = 3039. State = [[ 0.34339106 -0.20475896]]. Action = [[ 0.09828422 -0.07959224  0.          0.08273506]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 3039 is [False, False, True, True, False, False]
State prediction error at timestep 3039 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3039 of None
Current timestep = 3040. State = [[ 0.34350792 -0.20198356]]. Action = [[ 0.08814619  0.09287847  0.         -0.2630409 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 3040 is [False, False, True, True, False, False]
State prediction error at timestep 3040 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3040 of None
Current timestep = 3041. State = [[ 0.3439156  -0.19772205]]. Action = [[0.08611617 0.08735996 0.         0.01146734]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 3041 is [False, False, True, True, False, False]
State prediction error at timestep 3041 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3041 of None
Current timestep = 3042. State = [[ 0.34448695 -0.19188029]]. Action = [[ 0.09358842  0.09999856  0.         -0.99948746]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 3042 is [False, False, True, True, False, False]
State prediction error at timestep 3042 is tensor(7.4510e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3042 of None
Current timestep = 3043. State = [[ 0.34463736 -0.19284022]]. Action = [[ 0.09792135 -0.07977812  0.         -0.99784434]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 3043 is [False, False, True, True, False, False]
State prediction error at timestep 3043 is tensor(2.7819e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3043 of None
Current timestep = 3044. State = [[ 0.34464747 -0.19818352]]. Action = [[ 0.07895628 -0.08169594  0.         -0.9979453 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 3044 is [False, False, True, True, False, False]
State prediction error at timestep 3044 is tensor(6.2778e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3044 of None
Current timestep = 3045. State = [[ 0.3446834  -0.19937155]]. Action = [[ 0.07726271  0.03221904  0.         -0.9895591 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 3045 is [False, False, True, True, False, False]
State prediction error at timestep 3045 is tensor(9.2888e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3045 of None
Current timestep = 3046. State = [[ 0.3447303  -0.19905216]]. Action = [[ 9.0802245e-02  9.8039210e-04  0.0000000e+00 -9.9207175e-01]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 3046 is [False, False, True, True, False, False]
State prediction error at timestep 3046 is tensor(7.2144e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3046 of None
Current timestep = 3047. State = [[ 0.34475243 -0.20164824]]. Action = [[ 0.09762178 -0.07321297  0.         -0.99495   ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 3047 is [False, False, True, True, False, False]
State prediction error at timestep 3047 is tensor(4.3844e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3047 of None
Current timestep = 3048. State = [[ 0.34482566 -0.20011589]]. Action = [[ 0.07787309  0.09068816  0.         -0.99933296]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 3048 is [False, False, True, True, False, False]
State prediction error at timestep 3048 is tensor(9.1898e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3048 of None
Current timestep = 3049. State = [[ 0.34486616 -0.20095225]]. Action = [[ 0.0900678  -0.0682577   0.          0.17391717]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 3049 is [False, False, True, True, False, False]
State prediction error at timestep 3049 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3049 of None
Current timestep = 3050. State = [[ 0.34488052 -0.20298119]]. Action = [[ 0.09901569 -0.0502957   0.         -0.5329155 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 3050 is [False, False, True, True, False, False]
State prediction error at timestep 3050 is tensor(8.6555e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3050 of None
Current timestep = 3051. State = [[ 0.344887   -0.20628071]]. Action = [[ 0.0960914 -0.0999646  0.         0.5479411]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 3051 is [False, False, True, True, False, False]
State prediction error at timestep 3051 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3051 of None
Current timestep = 3052. State = [[ 0.3448798  -0.21066572]]. Action = [[ 0.09596207 -0.09753849  0.         -0.96597016]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 3052 is [False, False, True, True, False, False]
State prediction error at timestep 3052 is tensor(9.5041e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3052 of None
Current timestep = 3053. State = [[ 0.3449272  -0.20897758]]. Action = [[ 0.09634886  0.0855585   0.         -0.8362476 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 3053 is [False, False, True, True, False, False]
State prediction error at timestep 3053 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3053 of None
Current timestep = 3054. State = [[ 0.34494984 -0.20600398]]. Action = [[ 0.09856302  0.03282601  0.         -0.9726799 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 3054 is [False, False, True, True, False, False]
State prediction error at timestep 3054 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3054 of None
Current timestep = 3055. State = [[ 0.34549356 -0.20203073]]. Action = [[ 0.09133401  0.09944857  0.         -0.8829969 ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 3055 is [False, False, True, True, False, False]
State prediction error at timestep 3055 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3055 of None
Current timestep = 3056. State = [[ 0.34633777 -0.19543307]]. Action = [[0.09521239 0.09942146 0.         0.6522329 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 3056 is [False, False, True, True, False, False]
State prediction error at timestep 3056 is tensor(6.5432e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3056 of None
Current timestep = 3057. State = [[ 0.34669864 -0.19122572]]. Action = [[ 0.09389689  0.0261428   0.         -0.9996904 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 3057 is [False, False, True, True, False, False]
State prediction error at timestep 3057 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3057 of None
Current timestep = 3058. State = [[ 0.34677407 -0.19120727]]. Action = [[ 0.09679998 -0.02552049  0.          0.89735365]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 3058 is [False, False, True, True, False, False]
State prediction error at timestep 3058 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3058 of None
Current timestep = 3059. State = [[ 0.34698328 -0.18771502]]. Action = [[0.08219463 0.09975279 0.         0.99849355]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 3059 is [False, False, True, True, False, False]
State prediction error at timestep 3059 is tensor(5.7256e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3059 of None
Current timestep = 3060. State = [[ 0.34723443 -0.18311359]]. Action = [[ 0.05822643  0.0949182   0.         -0.9537914 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 3060 is [False, False, True, True, False, False]
State prediction error at timestep 3060 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3060 of None
Current timestep = 3061. State = [[ 0.34758317 -0.18274495]]. Action = [[ 0.05293062  0.03789931  0.         -0.97498524]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 3061 is [False, False, True, True, False, False]
State prediction error at timestep 3061 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3061 of None
Current timestep = 3062. State = [[ 0.347606   -0.18447515]]. Action = [[ 0.0958525  -0.03027876  0.          0.358616  ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 3062 is [False, False, True, True, False, False]
State prediction error at timestep 3062 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3062 of None
Current timestep = 3063. State = [[ 0.34811178 -0.18247359]]. Action = [[0.09359062 0.09068211 0.         0.7240282 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 3063 is [False, False, True, True, False, False]
State prediction error at timestep 3063 is tensor(6.7913e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3063 of None
Current timestep = 3064. State = [[ 0.3497646  -0.17938398]]. Action = [[ 0.09633977  0.09040072  0.         -0.99829024]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 3064 is [False, False, True, True, False, False]
State prediction error at timestep 3064 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3064 of None
Current timestep = 3065. State = [[ 0.35011315 -0.18257697]]. Action = [[ 0.09639645 -0.0997662   0.         -0.6939382 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 3065 is [False, False, True, True, False, False]
State prediction error at timestep 3065 is tensor(9.6466e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3065 of None
Current timestep = 3066. State = [[ 0.35041294 -0.18215184]]. Action = [[0.09489312 0.09496809 0.         0.9844104 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 3066 is [False, False, True, True, False, False]
State prediction error at timestep 3066 is tensor(7.7571e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3066 of None
Current timestep = 3067. State = [[ 0.35052946 -0.1839932 ]]. Action = [[ 0.09155255 -0.0763029   0.         -0.9225892 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 3067 is [False, False, True, True, False, False]
State prediction error at timestep 3067 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3067 of None
Current timestep = 3068. State = [[ 0.35079393 -0.18345027]]. Action = [[ 0.09533235  0.08500197  0.         -0.6227732 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 3068 is [False, False, True, True, False, False]
State prediction error at timestep 3068 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3068 of None
Current timestep = 3069. State = [[ 0.35088265 -0.1857219 ]]. Action = [[ 0.09504821 -0.08160202  0.          0.68467295]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 3069 is [False, False, True, True, False, False]
State prediction error at timestep 3069 is tensor(5.9895e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3069 of None
Current timestep = 3070. State = [[ 0.3508868  -0.18727748]]. Action = [[0.06684213 0.01910827 0.         0.89335823]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 3070 is [False, False, True, True, False, False]
State prediction error at timestep 3070 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3070 of None
Current timestep = 3071. State = [[ 0.3508901 -0.1887965]]. Action = [[ 0.09918135 -0.05396326  0.         -0.696837  ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 3071 is [False, False, True, True, False, False]
State prediction error at timestep 3071 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3071 of None
Current timestep = 3072. State = [[ 0.3510759  -0.18646571]]. Action = [[ 0.09262306  0.09807218  0.         -0.9837562 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 3072 is [False, False, True, True, False, False]
State prediction error at timestep 3072 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3072 of None
Current timestep = 3073. State = [[ 0.35173956 -0.18177561]]. Action = [[0.09813233 0.09489962 0.         0.22408557]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 3073 is [False, False, True, True, False, False]
State prediction error at timestep 3073 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3073 of None
Current timestep = 3074. State = [[ 0.35262316 -0.17732531]]. Action = [[ 0.08702049  0.08891455  0.         -0.5863775 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 3074 is [False, False, True, True, False, False]
State prediction error at timestep 3074 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3074 of None
Current timestep = 3075. State = [[ 0.3532801  -0.17770806]]. Action = [[ 0.09127171 -0.03006341  0.         -0.96647376]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 3075 is [False, False, True, True, False, False]
State prediction error at timestep 3075 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3075 of None
Current timestep = 3076. State = [[ 0.35371625 -0.1768951 ]]. Action = [[0.09586615 0.05448941 0.         0.8054867 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 3076 is [False, False, True, True, False, False]
State prediction error at timestep 3076 is tensor(4.2516e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3076 of None
Current timestep = 3077. State = [[ 0.35381287 -0.17917347]]. Action = [[ 0.09829745 -0.08214013  0.          0.61064494]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 3077 is [False, False, True, True, False, False]
State prediction error at timestep 3077 is tensor(4.1569e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3077 of None
Current timestep = 3078. State = [[ 0.35383064 -0.17910345]]. Action = [[0.0863666  0.04209969 0.         0.99759555]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 3078 is [False, False, True, True, False, False]
State prediction error at timestep 3078 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3078 of None
Current timestep = 3079. State = [[ 0.3539348  -0.17687173]]. Action = [[0.09636059 0.03084969 0.         0.46105313]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 3079 is [False, False, True, True, False, False]
State prediction error at timestep 3079 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3079 of None
Current timestep = 3080. State = [[ 0.35403988 -0.17681351]]. Action = [[ 0.0942596  -0.01383928  0.          0.99786234]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 3080 is [False, False, True, True, False, False]
State prediction error at timestep 3080 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3080 of None
Current timestep = 3081. State = [[ 0.35480335 -0.17462835]]. Action = [[0.09670337 0.08606378 0.         0.6376717 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 3081 is [False, False, True, True, False, False]
State prediction error at timestep 3081 is tensor(2.8729e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3081 of None
Current timestep = 3082. State = [[ 0.3555461 -0.1745361]]. Action = [[0.09035576 0.00679835 0.         0.95566773]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 3082 is [False, False, True, True, False, False]
State prediction error at timestep 3082 is tensor(7.6413e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3082 of None
Current timestep = 3083. State = [[ 0.3563427  -0.17361721]]. Action = [[ 0.08253456  0.09610409  0.         -0.02637571]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 3083 is [False, False, True, True, False, False]
State prediction error at timestep 3083 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3083 of None
Current timestep = 3084. State = [[ 0.3568068 -0.1733553]]. Action = [[ 0.07888461  0.03257146  0.         -0.796429  ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 3084 is [False, False, True, True, False, False]
State prediction error at timestep 3084 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3084 of None
Current timestep = 3085. State = [[ 0.35754412 -0.1728865 ]]. Action = [[ 0.09472216  0.07086007  0.         -0.1970694 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 3085 is [False, False, True, True, False, False]
State prediction error at timestep 3085 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3085 of None
Current timestep = 3086. State = [[ 0.35838822 -0.17218012]]. Action = [[0.08296052 0.07094412 0.         0.9422586 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 3086 is [False, False, True, True, False, False]
State prediction error at timestep 3086 is tensor(4.2732e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3086 of None
Current timestep = 3087. State = [[ 0.35963863 -0.17084529]]. Action = [[0.09314658 0.099724   0.         0.41334105]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 3087 is [False, False, True, True, False, False]
State prediction error at timestep 3087 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3087 of None
Current timestep = 3088. State = [[ 0.36071822 -0.17008753]]. Action = [[ 0.09920805  0.04547124  0.         -0.21159834]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 3088 is [False, False, True, True, False, False]
State prediction error at timestep 3088 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3088 of None
Current timestep = 3089. State = [[ 0.3617006  -0.16933043]]. Action = [[0.09538222 0.08444846 0.         0.2980113 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 3089 is [False, False, True, True, False, False]
State prediction error at timestep 3089 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3089 of None
Current timestep = 3090. State = [[ 0.36303124 -0.16807538]]. Action = [[0.09664918 0.09756666 0.         0.99432254]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 3090 is [False, False, True, True, False, False]
State prediction error at timestep 3090 is tensor(1.9247e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3090 of None
Current timestep = 3091. State = [[ 0.36374956 -0.16704866]]. Action = [[0.06917764 0.09108882 0.         0.7064452 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 3091 is [False, False, True, True, False, False]
State prediction error at timestep 3091 is tensor(5.2822e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3091 of None
Current timestep = 3092. State = [[ 0.36409575 -0.16650587]]. Action = [[ 0.07214271  0.09036607  0.         -0.1351918 ]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 3092 is [False, False, True, True, False, False]
State prediction error at timestep 3092 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3092 of None
Current timestep = 3093. State = [[ 0.36482126 -0.16581339]]. Action = [[0.09170756 0.09203405 0.         0.9555268 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 3093 is [False, False, True, True, False, False]
State prediction error at timestep 3093 is tensor(2.0389e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3093 of None
Current timestep = 3094. State = [[ 0.36572516 -0.16491117]]. Action = [[0.09047095 0.09374834 0.         0.80608904]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 3094 is [False, False, True, True, False, False]
State prediction error at timestep 3094 is tensor(1.2720e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3094 of None
Current timestep = 3095. State = [[ 0.3654428  -0.16526744]]. Action = [[0.         0.         0.         0.33289862]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 3095 is [False, False, True, True, False, False]
State prediction error at timestep 3095 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3095 of None
Current timestep = 3096. State = [[ 0.36347964 -0.16519542]]. Action = [[0.         0.         0.         0.96280766]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 3096 is [False, False, True, True, False, False]
State prediction error at timestep 3096 is tensor(3.4584e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3096 of None
Current timestep = 3097. State = [[ 0.363565   -0.16347139]]. Action = [[ 0.09596803  0.09828291  0.         -0.11181176]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 3097 is [False, False, True, True, False, False]
State prediction error at timestep 3097 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3097 of None
Current timestep = 3098. State = [[ 0.36501923 -0.16207293]]. Action = [[ 0.09632831  0.09906457  0.         -0.2815764 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 3098 is [False, False, True, True, False, False]
State prediction error at timestep 3098 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3098 of None
Current timestep = 3099. State = [[ 0.36629525 -0.16097103]]. Action = [[0.09726796 0.09015263 0.         0.9319196 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 3099 is [False, False, True, True, False, False]
State prediction error at timestep 3099 is tensor(8.1099e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3099 of None
Current timestep = 3100. State = [[ 0.3663784  -0.16131654]]. Action = [[0.        0.        0.        0.9509779]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 3100 is [False, False, True, True, False, False]
State prediction error at timestep 3100 is tensor(2.3166e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3100 of None
Current timestep = 3101. State = [[ 0.36566713 -0.1614152 ]]. Action = [[0.         0.         0.         0.71109116]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 3101 is [False, False, True, True, False, False]
State prediction error at timestep 3101 is tensor(3.5853e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3101 of None
Current timestep = 3102. State = [[ 0.3631172  -0.16099475]]. Action = [[0.         0.         0.         0.91746414]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 3102 is [False, False, True, True, False, False]
State prediction error at timestep 3102 is tensor(2.7122e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3102 of None
Current timestep = 3103. State = [[ 0.36290854 -0.15880094]]. Action = [[0.09629872 0.09799338 0.         0.9506599 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 3103 is [False, False, True, True, False, False]
State prediction error at timestep 3103 is tensor(5.7305e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3103 of None
Current timestep = 3104. State = [[ 0.36439517 -0.15705691]]. Action = [[0.09854253 0.09905531 0.         0.8560815 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 3104 is [False, False, True, True, False, False]
State prediction error at timestep 3104 is tensor(2.8430e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3104 of None
Current timestep = 3105. State = [[ 0.3656178  -0.15605119]]. Action = [[0.09915093 0.07971042 0.         0.8213805 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 3105 is [False, False, True, True, False, False]
State prediction error at timestep 3105 is tensor(5.7263e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3105 of None
Current timestep = 3106. State = [[ 0.36558726 -0.15647702]]. Action = [[0.         0.         0.         0.92687106]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 3106 is [False, False, True, True, False, False]
State prediction error at timestep 3106 is tensor(2.2104e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3106 of None
Current timestep = 3107. State = [[ 0.36409613 -0.1564759 ]]. Action = [[0.        0.        0.        0.9775827]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 3107 is [False, False, True, True, False, False]
State prediction error at timestep 3107 is tensor(1.5379e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3107 of None
Current timestep = 3108. State = [[ 0.3641101  -0.15501815]]. Action = [[0.09423131 0.09275169 0.         0.8079952 ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 3108 is [False, False, True, True, False, False]
State prediction error at timestep 3108 is tensor(3.7731e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3108 of None
Current timestep = 3109. State = [[ 0.36547595 -0.15375282]]. Action = [[0.09932826 0.09652895 0.         0.51027143]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 3109 is [False, False, True, True, False, False]
State prediction error at timestep 3109 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3109 of None
Current timestep = 3110. State = [[ 0.36670935 -0.15259796]]. Action = [[0.09370788 0.09864654 0.         0.925302  ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 3110 is [False, False, True, True, False, False]
State prediction error at timestep 3110 is tensor(5.6750e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3110 of None
Current timestep = 3111. State = [[ 0.36672193 -0.15299858]]. Action = [[0.         0.         0.         0.88332963]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 3111 is [False, False, True, True, False, False]
State prediction error at timestep 3111 is tensor(2.1963e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3111 of None
Current timestep = 3112. State = [[ 0.36558574 -0.15315174]]. Action = [[0.        0.        0.        0.8850131]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 3112 is [False, False, True, True, False, False]
State prediction error at timestep 3112 is tensor(2.2255e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3112 of None
Current timestep = 3113. State = [[ 0.36277518 -0.15244938]]. Action = [[0.         0.         0.         0.69736314]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 3113 is [False, False, True, True, False, False]
State prediction error at timestep 3113 is tensor(6.2854e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3113 of None
Current timestep = 3114. State = [[ 0.36261645 -0.15082885]]. Action = [[0.09810735 0.09066752 0.         0.8840617 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 3114 is [False, False, True, True, False, False]
State prediction error at timestep 3114 is tensor(8.7799e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3114 of None
Current timestep = 3115. State = [[ 0.3640675  -0.14952011]]. Action = [[0.0986644  0.09619305 0.         0.89999163]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 3115 is [False, False, True, True, False, False]
State prediction error at timestep 3115 is tensor(1.2481e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3115 of None
Current timestep = 3116. State = [[ 0.36546406 -0.1482984 ]]. Action = [[0.09804616 0.09716397 0.         0.8215089 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 3116 is [False, False, True, True, False, False]
State prediction error at timestep 3116 is tensor(2.1287e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3116 of None
Current timestep = 3117. State = [[ 0.36662638 -0.14726374]]. Action = [[0.09544768 0.09600963 0.         0.89180315]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 3117 is [False, False, True, True, False, False]
State prediction error at timestep 3117 is tensor(1.7230e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3117 of None
Current timestep = 3118. State = [[ 0.36647895 -0.14764434]]. Action = [[0.        0.        0.        0.9092076]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 3118 is [False, False, True, True, False, False]
State prediction error at timestep 3118 is tensor(2.5764e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3118 of None
Current timestep = 3119. State = [[ 0.36422238 -0.14758098]]. Action = [[0.        0.        0.        0.9499707]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 3119 is [False, False, True, True, False, False]
State prediction error at timestep 3119 is tensor(1.3174e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3119 of None
Current timestep = 3120. State = [[ 0.36423528 -0.14585987]]. Action = [[0.09674507 0.09810727 0.         0.6766468 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 3120 is [False, False, True, True, False, False]
State prediction error at timestep 3120 is tensor(6.3238e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3120 of None
Current timestep = 3121. State = [[ 0.36559266 -0.14461583]]. Action = [[0.09937931 0.08821135 0.         0.76754904]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 3121 is [False, False, True, True, False, False]
State prediction error at timestep 3121 is tensor(3.4881e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3121 of None
Current timestep = 3122. State = [[ 0.3655158  -0.14494608]]. Action = [[0.       0.       0.       0.618184]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 3122 is [False, False, True, True, False, False]
State prediction error at timestep 3122 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3122 of None
Current timestep = 3123. State = [[ 0.3640522  -0.14509861]]. Action = [[0.        0.        0.        0.8452425]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 3123 is [False, False, True, True, False, False]
State prediction error at timestep 3123 is tensor(1.9235e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3123 of None
Current timestep = 3124. State = [[ 0.36431053 -0.14365493]]. Action = [[0.09909909 0.09776113 0.         0.4566772 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 3124 is [False, False, True, True, False, False]
State prediction error at timestep 3124 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3124 of None
Current timestep = 3125. State = [[ 0.36549994 -0.14228685]]. Action = [[0.08670408 0.09774279 0.         0.83235836]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 3125 is [False, False, True, True, False, False]
State prediction error at timestep 3125 is tensor(2.7398e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3125 of None
Current timestep = 3126. State = [[ 0.36517942 -0.14256053]]. Action = [[0.        0.        0.        0.8895354]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 3126 is [False, False, True, True, False, False]
State prediction error at timestep 3126 is tensor(2.6542e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3126 of None
Current timestep = 3127. State = [[ 0.36336842 -0.14264625]]. Action = [[0.         0.         0.         0.55420756]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 3127 is [False, False, True, True, False, False]
State prediction error at timestep 3127 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3127 of None
Current timestep = 3128. State = [[ 0.363553   -0.14157538]]. Action = [[0.0984767  0.08573114 0.         0.78683877]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 3128 is [False, False, True, True, False, False]
State prediction error at timestep 3128 is tensor(2.3464e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3128 of None
Current timestep = 3129. State = [[ 0.36475053 -0.14040573]]. Action = [[0.09357703 0.0961042  0.         0.8207499 ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 3129 is [False, False, True, True, False, False]
State prediction error at timestep 3129 is tensor(2.7654e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3129 of None
Current timestep = 3130. State = [[ 0.36602977 -0.139299  ]]. Action = [[0.09901441 0.09598831 0.         0.82281303]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 3130 is [False, False, True, True, False, False]
State prediction error at timestep 3130 is tensor(1.9030e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3130 of None
Current timestep = 3131. State = [[ 0.36613092 -0.13971154]]. Action = [[0.         0.         0.         0.89846563]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 3131 is [False, False, True, True, False, False]
State prediction error at timestep 3131 is tensor(2.5163e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3131 of None
Current timestep = 3132. State = [[ 0.36554453 -0.13991825]]. Action = [[0.        0.        0.        0.8713839]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 3132 is [False, False, True, True, False, False]
State prediction error at timestep 3132 is tensor(2.5494e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3132 of None
Current timestep = 3133. State = [[ 0.36361533 -0.13968162]]. Action = [[0.         0.         0.         0.91212034]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 3133 is [False, False, True, True, False, False]
State prediction error at timestep 3133 is tensor(1.7228e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3133 of None
Current timestep = 3134. State = [[ 0.36342862 -0.13782173]]. Action = [[0.09592932 0.09602977 0.         0.8903847 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 3134 is [False, False, True, True, False, False]
State prediction error at timestep 3134 is tensor(6.6405e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3134 of None
Current timestep = 3135. State = [[ 0.36470357 -0.13654046]]. Action = [[0.09421735 0.09494322 0.         0.27912164]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 3135 is [False, False, True, True, False, False]
State prediction error at timestep 3135 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3135 of None
Current timestep = 3136. State = [[ 0.36584947 -0.13557969]]. Action = [[0.09728593 0.09210386 0.         0.8653759 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 3136 is [False, False, True, True, False, False]
State prediction error at timestep 3136 is tensor(1.8255e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3136 of None
Current timestep = 3137. State = [[ 0.36587027 -0.13603866]]. Action = [[0.        0.        0.        0.8708106]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 3137 is [False, False, True, True, False, False]
State prediction error at timestep 3137 is tensor(2.3564e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3137 of None
Current timestep = 3138. State = [[ 0.36472207 -0.13618362]]. Action = [[0.        0.        0.        0.7794764]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 3138 is [False, False, True, True, False, False]
State prediction error at timestep 3138 is tensor(3.1419e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3138 of None
Current timestep = 3139. State = [[ 0.3647739  -0.13476932]]. Action = [[0.0982128 0.0981641 0.        0.8693061]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 3139 is [False, False, True, True, False, False]
State prediction error at timestep 3139 is tensor(8.6574e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3139 of None
Current timestep = 3140. State = [[ 0.3660686  -0.13357751]]. Action = [[0.09886522 0.09290928 0.         0.85537946]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 3140 is [False, False, True, True, False, False]
State prediction error at timestep 3140 is tensor(1.0919e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3140 of None
Current timestep = 3141. State = [[ 0.3659269  -0.13395867]]. Action = [[0.        0.        0.        0.9263165]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 3141 is [False, False, True, True, False, False]
State prediction error at timestep 3141 is tensor(1.1989e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3141 of None
Current timestep = 3142. State = [[ 0.36425388 -0.13410053]]. Action = [[0.         0.         0.         0.90636194]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 3142 is [False, False, True, True, False, False]
State prediction error at timestep 3142 is tensor(1.0954e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3142 of None
Current timestep = 3143. State = [[ 0.364327   -0.13280943]]. Action = [[0.09632354 0.09629538 0.         0.78421926]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 3143 is [False, False, True, True, False, False]
State prediction error at timestep 3143 is tensor(1.7675e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3143 of None
Current timestep = 3144. State = [[ 0.36555424 -0.13158822]]. Action = [[0.09436128 0.09666925 0.         0.6464827 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 3144 is [False, False, True, True, False, False]
State prediction error at timestep 3144 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3144 of None
Current timestep = 3145. State = [[ 0.36527267 -0.13191497]]. Action = [[0.        0.        0.        0.8898288]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 3145 is [False, False, True, True, False, False]
State prediction error at timestep 3145 is tensor(1.6692e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3145 of None
Current timestep = 3146. State = [[ 0.36296165 -0.1319587 ]]. Action = [[0.        0.        0.        0.7998309]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 3146 is [False, False, True, True, False, False]
State prediction error at timestep 3146 is tensor(2.7286e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3146 of None
Current timestep = 3147. State = [[ 0.36291617 -0.1307625 ]]. Action = [[0.09851597 0.09647229 0.         0.9000187 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 3147 is [False, False, True, True, False, False]
State prediction error at timestep 3147 is tensor(1.2363e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3147 of None
Current timestep = 3148. State = [[ 0.36402753 -0.12957263]]. Action = [[0.08689118 0.09369797 0.         0.9047338 ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 3148 is [False, False, True, True, False, False]
State prediction error at timestep 3148 is tensor(9.6759e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3148 of None
Current timestep = 3149. State = [[ 0.36510754 -0.12862612]]. Action = [[0.09589835 0.09156878 0.         0.8771918 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 3149 is [False, False, True, True, False, False]
State prediction error at timestep 3149 is tensor(1.0571e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3149 of None
Current timestep = 3150. State = [[ 0.36615127 -0.1277172 ]]. Action = [[0.09403574 0.09523974 0.         0.856663  ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 3150 is [False, False, True, True, False, False]
State prediction error at timestep 3150 is tensor(1.6858e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3150 of None
Current timestep = 3151. State = [[ 0.36617443 -0.1281701 ]]. Action = [[0.        0.        0.        0.9532858]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 3151 is [False, False, True, True, False, False]
State prediction error at timestep 3151 is tensor(1.5755e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3151 of None
Current timestep = 3152. State = [[ 0.3650398  -0.12831117]]. Action = [[0.         0.         0.         0.60499394]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 3152 is [False, False, True, True, False, False]
State prediction error at timestep 3152 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3152 of None
Current timestep = 3153. State = [[ 0.36480764 -0.12701064]]. Action = [[0.08356907 0.09639359 0.         0.59130454]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 3153 is [False, False, True, True, False, False]
State prediction error at timestep 3153 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3153 of None
Current timestep = 3154. State = [[ 0.36595657 -0.12593172]]. Action = [[0.0996662  0.09376401 0.         0.47560596]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 3154 is [False, False, True, True, False, False]
State prediction error at timestep 3154 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3154 of None
Current timestep = 3155. State = [[ 0.36591265 -0.12629591]]. Action = [[0.         0.         0.         0.82596076]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 3155 is [False, False, True, True, False, False]
State prediction error at timestep 3155 is tensor(3.0008e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3155 of None
Current timestep = 3156. State = [[ 0.36459294 -0.12647472]]. Action = [[0.        0.        0.        0.7972852]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 3156 is [False, False, True, True, False, False]
State prediction error at timestep 3156 is tensor(3.3825e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3156 of None
Current timestep = 3157. State = [[ 0.36469084 -0.12539111]]. Action = [[0.09757014 0.0927432  0.         0.7894459 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 3157 is [False, False, True, True, False, False]
State prediction error at timestep 3157 is tensor(1.9909e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3157 of None
Current timestep = 3158. State = [[ 0.36588305 -0.12423757]]. Action = [[0.09442628 0.09443904 0.         0.7523289 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 3158 is [False, False, True, False, True, False]
State prediction error at timestep 3158 is tensor(4.3173e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3158 of None
Current timestep = 3159. State = [[ 0.365683   -0.12462398]]. Action = [[0.        0.        0.        0.8354783]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 3159 is [False, False, True, False, True, False]
State prediction error at timestep 3159 is tensor(3.8182e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3159 of None
Current timestep = 3160. State = [[ 0.3658745  -0.12378861]]. Action = [[0.09503312 0.09504081 0.         0.66594064]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 3160 is [False, False, True, False, True, False]
State prediction error at timestep 3160 is tensor(8.3785e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3160 of None
Current timestep = 3161. State = [[ 0.36576635 -0.12407443]]. Action = [[0.        0.        0.        0.5596297]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 3161 is [False, False, True, False, True, False]
State prediction error at timestep 3161 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3161 of None
Current timestep = 3162. State = [[ 0.36363226 -0.12384175]]. Action = [[0.         0.         0.         0.73893094]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 3162 is [False, False, True, False, True, False]
State prediction error at timestep 3162 is tensor(3.4693e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3162 of None
Current timestep = 3163. State = [[ 0.36348298 -0.12227844]]. Action = [[0.0989889  0.09697055 0.         0.5798615 ]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 3163 is [False, False, True, False, True, False]
State prediction error at timestep 3163 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3163 of None
Current timestep = 3164. State = [[ 0.36455908 -0.12121284]]. Action = [[0.08920855 0.09474566 0.         0.6939266 ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 3164 is [False, False, True, False, True, False]
State prediction error at timestep 3164 is tensor(6.8838e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3164 of None
Current timestep = 3165. State = [[ 0.36567044 -0.12023527]]. Action = [[0.09604368 0.09703352 0.         0.72892964]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 3165 is [False, False, True, False, True, False]
State prediction error at timestep 3165 is tensor(4.7743e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3165 of None
Current timestep = 3166. State = [[ 0.3656358  -0.12065137]]. Action = [[0.        0.        0.        0.8256053]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 3166 is [False, False, True, False, True, False]
State prediction error at timestep 3166 is tensor(2.5742e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3166 of None
Current timestep = 3167. State = [[ 0.3640617  -0.12078267]]. Action = [[0.         0.         0.         0.60499406]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 3167 is [False, False, True, False, True, False]
State prediction error at timestep 3167 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3167 of None
Current timestep = 3168. State = [[ 0.3639119  -0.11943575]]. Action = [[0.09808717 0.09820626 0.         0.6242354 ]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 3168 is [False, False, True, False, True, False]
State prediction error at timestep 3168 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3168 of None
Current timestep = 3169. State = [[ 0.3652356  -0.11812757]]. Action = [[0.0971544  0.09443472 0.         0.7270298 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 3169 is [False, False, True, False, True, False]
State prediction error at timestep 3169 is tensor(4.9214e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3169 of None
Current timestep = 3170. State = [[ 0.36652717 -0.11696056]]. Action = [[0.09754462 0.09303658 0.         0.80429864]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 3170 is [False, False, True, False, True, False]
State prediction error at timestep 3170 is tensor(2.3080e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3170 of None
Current timestep = 3171. State = [[ 0.36708504 -0.11645174]]. Action = [[0.08284862 0.08898718 0.         0.78035057]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 3171 is [False, False, True, False, True, False]
State prediction error at timestep 3171 is tensor(3.1797e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3171 of None
Current timestep = 3172. State = [[ 0.3668127  -0.11705255]]. Action = [[0.         0.         0.         0.85486126]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 3172 is [False, False, True, False, True, False]
State prediction error at timestep 3172 is tensor(2.7942e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3172 of None
Current timestep = 3173. State = [[ 0.36478427 -0.11711127]]. Action = [[0.        0.        0.        0.8909079]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 3173 is [False, False, True, False, True, False]
State prediction error at timestep 3173 is tensor(2.0871e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3173 of None
Current timestep = 3174. State = [[ 0.3645193 -0.1157127]]. Action = [[0.09611184 0.09337118 0.         0.76912117]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 3174 is [False, False, True, False, True, False]
State prediction error at timestep 3174 is tensor(1.7877e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3174 of None
Current timestep = 3175. State = [[ 0.36577296 -0.11447947]]. Action = [[0.09735895 0.09643238 0.         0.74463964]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 3175 is [False, False, True, False, True, False]
State prediction error at timestep 3175 is tensor(3.4412e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3175 of None
Current timestep = 3176. State = [[ 0.36576623 -0.11485061]]. Action = [[0.         0.         0.         0.51888156]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 3176 is [False, False, True, False, True, False]
State prediction error at timestep 3176 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3176 of None
Current timestep = 3177. State = [[ 0.3646191 -0.115013 ]]. Action = [[0.        0.        0.        0.7646959]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 3177 is [False, False, True, False, True, False]
State prediction error at timestep 3177 is tensor(4.1956e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3177 of None
Current timestep = 3178. State = [[ 0.3645513  -0.11411549]]. Action = [[0.09023445 0.08359303 0.         0.43003428]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 3178 is [False, False, True, False, True, False]
State prediction error at timestep 3178 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3178 of None
Current timestep = 3179. State = [[ 0.36525866 -0.11349693]]. Action = [[0.09176455 0.08582685 0.         0.82348275]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 3179 is [False, False, True, False, True, False]
State prediction error at timestep 3179 is tensor(1.0451e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3179 of None
Current timestep = 3180. State = [[ 0.36614522 -0.11271568]]. Action = [[0.09208853 0.09755898 0.         0.8597224 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 3180 is [False, False, True, False, True, False]
State prediction error at timestep 3180 is tensor(1.6672e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3180 of None
Current timestep = 3181. State = [[ 0.36693132 -0.11198539]]. Action = [[0.0883879  0.09778968 0.         0.8110689 ]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 3181 is [False, False, True, False, True, False]
State prediction error at timestep 3181 is tensor(2.4342e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3181 of None
Current timestep = 3182. State = [[ 0.36678576 -0.11251285]]. Action = [[0.        0.        0.        0.7405257]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 3182 is [False, False, True, False, True, False]
State prediction error at timestep 3182 is tensor(4.9308e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3182 of None
Current timestep = 3183. State = [[ 0.36663666 -0.11202738]]. Action = [[0.06943267 0.08239614 0.         0.5505444 ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 3183 is [False, False, True, False, True, False]
State prediction error at timestep 3183 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3183 of None
Current timestep = 3184. State = [[ 0.36605185 -0.11243663]]. Action = [[0.        0.        0.        0.6238769]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 3184 is [False, False, True, False, True, False]
State prediction error at timestep 3184 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3184 of None
Current timestep = 3185. State = [[ 0.36315328 -0.1123371 ]]. Action = [[0.         0.         0.         0.81556165]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 3185 is [False, False, True, False, True, False]
State prediction error at timestep 3185 is tensor(1.8529e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3185 of None
Current timestep = 3186. State = [[ 0.36259645 -0.11099324]]. Action = [[0.08870674 0.09560487 0.         0.3982675 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 3186 is [False, False, True, False, True, False]
State prediction error at timestep 3186 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3186 of None
Current timestep = 3187. State = [[ 0.3637537  -0.10984882]]. Action = [[0.09745909 0.09791908 0.         0.56868863]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 3187 is [False, False, True, False, True, False]
State prediction error at timestep 3187 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3187 of None
Current timestep = 3188. State = [[ 0.36520988 -0.10850441]]. Action = [[0.09759975 0.09697933 0.         0.6259935 ]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 3188 is [False, False, True, False, True, False]
State prediction error at timestep 3188 is tensor(8.3159e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3188 of None
Current timestep = 3189. State = [[ 0.36636472 -0.1074701 ]]. Action = [[0.09524984 0.09567147 0.         0.2646718 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 3189 is [False, False, True, False, True, False]
State prediction error at timestep 3189 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3189 of None
Current timestep = 3190. State = [[ 0.36629024 -0.10795241]]. Action = [[0.         0.         0.         0.76804054]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 3190 is [False, False, True, False, True, False]
State prediction error at timestep 3190 is tensor(3.8973e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3190 of None
Current timestep = 3191. State = [[ 0.3646924  -0.10812457]]. Action = [[0.         0.         0.         0.48856914]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 3191 is [False, False, True, False, True, False]
State prediction error at timestep 3191 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3191 of None
Current timestep = 3192. State = [[ 0.36459666 -0.10685524]]. Action = [[0.09792321 0.09121073 0.         0.5711334 ]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 3192 is [False, False, True, False, True, False]
State prediction error at timestep 3192 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3192 of None
Current timestep = 3193. State = [[ 0.3657452 -0.1057231]]. Action = [[0.09582924 0.09445082 0.         0.7367892 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 3193 is [False, False, True, False, True, False]
State prediction error at timestep 3193 is tensor(2.9561e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3193 of None
Current timestep = 3194. State = [[ 0.3657     -0.10611721]]. Action = [[0.         0.         0.         0.43345284]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 3194 is [False, False, True, False, True, False]
State prediction error at timestep 3194 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3194 of None
Current timestep = 3195. State = [[ 0.36458996 -0.10626607]]. Action = [[0.        0.        0.        0.6129259]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 3195 is [False, False, True, False, True, False]
State prediction error at timestep 3195 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3195 of None
Current timestep = 3196. State = [[ 0.3646904  -0.10499614]]. Action = [[0.0977302  0.09582121 0.         0.30671144]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 3196 is [False, False, True, False, True, False]
State prediction error at timestep 3196 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3196 of None
Current timestep = 3197. State = [[ 0.36600426 -0.10380931]]. Action = [[0.09962582 0.09689467 0.         0.71580124]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 3197 is [False, False, True, False, True, False]
State prediction error at timestep 3197 is tensor(2.4510e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3197 of None
Current timestep = 3198. State = [[ 0.36591744 -0.10422011]]. Action = [[0.        0.        0.        0.6236285]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 3198 is [False, False, True, False, True, False]
State prediction error at timestep 3198 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3198 of None
Current timestep = 3199. State = [[ 0.36447588 -0.10447781]]. Action = [[0.         0.         0.         0.26688313]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 3199 is [False, False, True, False, True, False]
State prediction error at timestep 3199 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3199 of None
Current timestep = 3200. State = [[ 0.3646367  -0.10340872]]. Action = [[0.09709281 0.09386999 0.         0.531868  ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 3200 is [False, False, True, False, True, False]
State prediction error at timestep 3200 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3200 of None
Current timestep = 3201. State = [[ 0.36568373 -0.10239498]]. Action = [[0.09383874 0.09227107 0.         0.49933124]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 3201 is [False, False, True, False, True, False]
State prediction error at timestep 3201 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3201 of None
Current timestep = 3202. State = [[ 0.36619535 -0.10194957]]. Action = [[0.08350647 0.08732874 0.         0.6829945 ]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 3202 is [False, False, True, False, True, False]
State prediction error at timestep 3202 is tensor(4.4668e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3202 of None
Current timestep = 3203. State = [[ 0.36598924 -0.1025751 ]]. Action = [[0.        0.        0.        0.6433846]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 3203 is [False, False, True, False, True, False]
State prediction error at timestep 3203 is tensor(9.1446e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3203 of None
Current timestep = 3204. State = [[ 0.36507395 -0.10282264]]. Action = [[0.       0.       0.       0.658777]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 3204 is [False, False, True, False, True, False]
State prediction error at timestep 3204 is tensor(7.4127e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3204 of None
Current timestep = 3205. State = [[ 0.362869   -0.10267103]]. Action = [[0.         0.         0.         0.49923122]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 3205 is [False, False, True, False, True, False]
State prediction error at timestep 3205 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3205 of None
Current timestep = 3206. State = [[ 0.36254522 -0.10094771]]. Action = [[0.09488914 0.09312154 0.         0.7727041 ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 3206 is [False, False, True, False, True, False]
State prediction error at timestep 3206 is tensor(6.4267e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3206 of None
Current timestep = 3207. State = [[ 0.36366802 -0.09989747]]. Action = [[0.0950358  0.09040929 0.         0.5681982 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 3207 is [False, False, True, False, True, False]
State prediction error at timestep 3207 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3207 of None
Current timestep = 3208. State = [[ 0.36484623 -0.09892434]]. Action = [[0.09847174 0.0962869  0.         0.7427896 ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 3208 is [False, False, True, False, True, False]
State prediction error at timestep 3208 is tensor(1.1066e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3208 of None
Current timestep = 3209. State = [[ 0.36586028 -0.09813153]]. Action = [[0.09713263 0.09228332 0.         0.17101288]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 3209 is [False, False, True, False, True, False]
State prediction error at timestep 3209 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3209 of None
Current timestep = 3210. State = [[ 0.3665656  -0.09751015]]. Action = [[0.08903799 0.09841197 0.         0.6038171 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 3210 is [False, False, True, False, True, False]
State prediction error at timestep 3210 is tensor(7.8374e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3210 of None
Current timestep = 3211. State = [[ 0.3662004  -0.09805068]]. Action = [[0.        0.        0.        0.5508847]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 3211 is [False, False, True, False, True, False]
State prediction error at timestep 3211 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3211 of None
Current timestep = 3212. State = [[ 0.36352155 -0.09813914]]. Action = [[0.        0.        0.        0.5825983]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 3212 is [False, False, True, False, True, False]
State prediction error at timestep 3212 is tensor(7.9455e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3212 of None
Current timestep = 3213. State = [[ 0.36317936 -0.09670602]]. Action = [[0.09394    0.09471721 0.         0.2710917 ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 3213 is [False, False, True, False, True, False]
State prediction error at timestep 3213 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3213 of None
Current timestep = 3214. State = [[ 0.36417204 -0.09570935]]. Action = [[0.09208889 0.09455667 0.         0.350425  ]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 3214 is [False, False, True, False, True, False]
State prediction error at timestep 3214 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3214 of None
Current timestep = 3215. State = [[ 0.3649783  -0.09494541]]. Action = [[0.08734953 0.09624513 0.         0.44399   ]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 3215 is [False, False, True, False, True, False]
State prediction error at timestep 3215 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3215 of None
Current timestep = 3216. State = [[ 0.3659067  -0.09411561]]. Action = [[0.09482529 0.09797233 0.         0.5024102 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 3216 is [False, False, True, False, True, False]
State prediction error at timestep 3216 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3216 of None
Current timestep = 3217. State = [[ 0.3658593  -0.09460007]]. Action = [[0.         0.         0.         0.57708645]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 3217 is [False, False, True, False, True, False]
State prediction error at timestep 3217 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3217 of None
Current timestep = 3218. State = [[ 0.3644046  -0.09479957]]. Action = [[0.         0.         0.         0.66136336]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 3218 is [False, False, True, False, True, False]
State prediction error at timestep 3218 is tensor(3.2680e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3218 of None
Current timestep = 3219. State = [[ 0.36423263 -0.09383786]]. Action = [[0.08931953 0.08641546 0.         0.71207166]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 3219 is [False, False, True, False, True, False]
State prediction error at timestep 3219 is tensor(5.6567e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3219 of None
Current timestep = 3220. State = [[ 0.3647612 -0.0932127]]. Action = [[0.08114488 0.09376421 0.         0.7232857 ]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 3220 is [False, False, True, False, True, False]
State prediction error at timestep 3220 is tensor(6.3650e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3220 of None
Current timestep = 3221. State = [[ 0.3656801  -0.09247456]]. Action = [[0.09799253 0.09210312 0.         0.5187926 ]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 3221 is [False, False, True, False, True, False]
State prediction error at timestep 3221 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3221 of None
Current timestep = 3222. State = [[ 0.36638132 -0.09178049]]. Action = [[0.08355352 0.0994531  0.         0.30857038]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 3222 is [False, False, True, False, True, False]
State prediction error at timestep 3222 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3222 of None
Current timestep = 3223. State = [[ 0.3662011  -0.09237492]]. Action = [[0.         0.         0.         0.61203134]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 3223 is [False, False, True, False, True, False]
State prediction error at timestep 3223 is tensor(6.6118e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3223 of None
Current timestep = 3224. State = [[ 0.3647916  -0.09246456]]. Action = [[0.         0.         0.         0.31309402]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 3224 is [False, False, True, False, True, False]
State prediction error at timestep 3224 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3224 of None
Current timestep = 3225. State = [[ 0.3645159  -0.09141518]]. Action = [[0.08874365 0.0861432  0.         0.3727417 ]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 3225 is [False, False, True, False, True, False]
State prediction error at timestep 3225 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3225 of None
Current timestep = 3226. State = [[ 0.36538273 -0.09065956]]. Action = [[0.09864051 0.0865318  0.         0.38270676]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 3226 is [False, False, True, False, True, False]
State prediction error at timestep 3226 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3226 of None
Current timestep = 3227. State = [[ 0.36627033 -0.08999728]]. Action = [[0.09694656 0.08518637 0.         0.14164853]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 3227 is [False, False, True, False, True, False]
State prediction error at timestep 3227 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3227 of None
Current timestep = 3228. State = [[ 0.36689374 -0.08950457]]. Action = [[0.09130514 0.09120465 0.         0.7641287 ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 3228 is [False, False, True, False, True, False]
State prediction error at timestep 3228 is tensor(6.4533e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3228 of None
Current timestep = 3229. State = [[ 0.36669487 -0.09011037]]. Action = [[0.         0.         0.         0.49043512]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 3229 is [False, False, True, False, True, False]
State prediction error at timestep 3229 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3229 of None
Current timestep = 3230. State = [[ 0.36491603 -0.09030422]]. Action = [[0.         0.         0.         0.33551073]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 3230 is [False, False, True, False, True, False]
State prediction error at timestep 3230 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3230 of None
Current timestep = 3231. State = [[ 0.3645557  -0.08921614]]. Action = [[0.09546519 0.09506498 0.         0.4722631 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 3231 is [False, False, True, False, True, False]
State prediction error at timestep 3231 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3231 of None
Current timestep = 3232. State = [[ 0.36526603 -0.08854753]]. Action = [[0.09065694 0.08195668 0.         0.0811919 ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 3232 is [False, False, True, False, True, False]
State prediction error at timestep 3232 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3232 of None
Current timestep = 3233. State = [[ 0.36609372 -0.08787516]]. Action = [[0.09558023 0.09515453 0.         0.36775875]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 3233 is [False, False, True, False, True, False]
State prediction error at timestep 3233 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3233 of None
Current timestep = 3234. State = [[ 0.3660159  -0.08840477]]. Action = [[0.        0.        0.        0.7029872]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 3234 is [False, False, True, False, True, False]
State prediction error at timestep 3234 is tensor(6.2954e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3234 of None
Current timestep = 3235. State = [[ 0.364848   -0.08865348]]. Action = [[0.        0.        0.        0.7105448]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 3235 is [False, False, True, False, True, False]
State prediction error at timestep 3235 is tensor(7.3471e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3235 of None
Current timestep = 3236. State = [[ 0.36466748 -0.08749747]]. Action = [[0.09810763 0.0812388  0.         0.552845  ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 3236 is [False, False, True, False, True, False]
State prediction error at timestep 3236 is tensor(7.5669e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3236 of None
Current timestep = 3237. State = [[ 0.3654475  -0.08665351]]. Action = [[0.08661509 0.09867687 0.         0.36364353]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 3237 is [False, False, True, False, True, False]
State prediction error at timestep 3237 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3237 of None
Current timestep = 3238. State = [[ 0.36631134 -0.08584955]]. Action = [[0.09190249 0.09591692 0.         0.5559139 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 3238 is [False, False, True, False, True, False]
State prediction error at timestep 3238 is tensor(9.4238e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3238 of None
Current timestep = 3239. State = [[ 0.36707526 -0.08518477]]. Action = [[0.09098747 0.09790016 0.         0.32255578]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 3239 is [False, False, True, False, True, False]
State prediction error at timestep 3239 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3239 of None
Current timestep = 3240. State = [[ 0.366922   -0.08576894]]. Action = [[0.        0.        0.        0.4002962]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 3240 is [False, False, True, False, True, False]
State prediction error at timestep 3240 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3240 of None
Current timestep = 3241. State = [[ 0.36512077 -0.08604166]]. Action = [[0.         0.         0.         0.55175626]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 3241 is [False, False, True, False, True, False]
State prediction error at timestep 3241 is tensor(8.1752e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3241 of None
Current timestep = 3242. State = [[ 0.36466765 -0.08507413]]. Action = [[0.09229746 0.08761621 0.         0.34852123]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 3242 is [False, False, True, False, True, False]
State prediction error at timestep 3242 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3242 of None
Current timestep = 3243. State = [[ 0.36556074 -0.08427601]]. Action = [[0.09819608 0.08819094 0.         0.6620467 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 3243 is [False, False, True, False, True, False]
State prediction error at timestep 3243 is tensor(1.8324e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3243 of None
Current timestep = 3244. State = [[ 0.36629477 -0.08374066]]. Action = [[0.0946397  0.08097906 0.         0.56978154]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 3244 is [False, False, True, False, True, False]
State prediction error at timestep 3244 is tensor(7.4667e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3244 of None
Current timestep = 3245. State = [[ 0.36616692 -0.08434321]]. Action = [[0.         0.         0.         0.18233871]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 3245 is [False, False, True, False, True, False]
State prediction error at timestep 3245 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3245 of None
Current timestep = 3246. State = [[ 0.36504236 -0.08457252]]. Action = [[0.         0.         0.         0.59010935]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 3246 is [False, False, True, False, True, False]
State prediction error at timestep 3246 is tensor(4.9758e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3246 of None
Current timestep = 3247. State = [[ 0.36504903 -0.08331046]]. Action = [[0.09772771 0.09910119 0.         0.21586299]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 3247 is [False, False, True, False, True, False]
State prediction error at timestep 3247 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3247 of None
Current timestep = 3248. State = [[ 0.36614636 -0.08219972]]. Action = [[0.09587889 0.09742536 0.         0.41392672]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 3248 is [False, False, True, False, True, False]
State prediction error at timestep 3248 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3248 of None
Current timestep = 3249. State = [[ 0.366092   -0.08263857]]. Action = [[0.         0.         0.         0.47376513]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 3249 is [False, False, True, False, True, False]
State prediction error at timestep 3249 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3249 of None
Current timestep = 3250. State = [[ 0.36500052 -0.08293962]]. Action = [[0.        0.        0.        0.7348306]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 3250 is [False, False, True, False, True, False]
State prediction error at timestep 3250 is tensor(7.2258e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3250 of None
Current timestep = 3251. State = [[ 0.36517352 -0.08190041]]. Action = [[0.09726455 0.09133328 0.         0.4340378 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 3251 is [False, False, True, False, True, False]
State prediction error at timestep 3251 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3251 of None
Current timestep = 3252. State = [[ 0.366035   -0.08111305]]. Action = [[0.09467501 0.09184306 0.         0.23846698]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 3252 is [False, False, True, False, True, False]
State prediction error at timestep 3252 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3252 of None
Current timestep = 3253. State = [[ 0.36563027 -0.08162333]]. Action = [[0.        0.        0.        0.3934635]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 3253 is [False, False, True, False, True, False]
State prediction error at timestep 3253 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3253 of None
Current timestep = 3254. State = [[ 0.36557338 -0.08054069]]. Action = [[0.09258322 0.09741808 0.         0.30858982]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 3254 is [False, False, True, False, True, False]
State prediction error at timestep 3254 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3254 of None
Current timestep = 3255. State = [[ 0.36546287 -0.0809579 ]]. Action = [[0.        0.        0.        0.3241024]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 3255 is [False, False, True, False, True, False]
State prediction error at timestep 3255 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3255 of None
Current timestep = 3256. State = [[ 0.36540428 -0.08038113]]. Action = [[0.08262265 0.09063218 0.         0.08490622]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 3256 is [False, False, True, False, True, False]
State prediction error at timestep 3256 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3256 of None
Current timestep = 3257. State = [[ 0.3660422  -0.07978928]]. Action = [[ 0.09446139  0.09104944  0.         -0.02130437]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 3257 is [False, False, True, False, True, False]
State prediction error at timestep 3257 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3257 of None
Current timestep = 3258. State = [[ 0.3658813  -0.08044233]]. Action = [[ 0.         0.         0.        -0.2798826]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 3258 is [False, False, True, False, True, False]
State prediction error at timestep 3258 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3258 of None
Current timestep = 3259. State = [[ 0.36456123 -0.08074871]]. Action = [[0.         0.         0.         0.30939662]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 3259 is [False, False, True, False, True, False]
State prediction error at timestep 3259 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3259 of None
Current timestep = 3260. State = [[ 0.36431608 -0.07974142]]. Action = [[0.09685124 0.08025623 0.         0.65824556]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 3260 is [False, False, True, False, True, False]
State prediction error at timestep 3260 is tensor(5.0272e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3260 of None
Current timestep = 3261. State = [[ 0.36509165 -0.0789432 ]]. Action = [[0.09094722 0.09542976 0.         0.29266477]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 3261 is [False, False, True, False, True, False]
State prediction error at timestep 3261 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3261 of None
Current timestep = 3262. State = [[ 0.3658856  -0.07837195]]. Action = [[0.09709468 0.0813714  0.         0.21580744]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 3262 is [False, False, True, False, True, False]
State prediction error at timestep 3262 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3262 of None
Current timestep = 3263. State = [[ 0.3658397  -0.07896514]]. Action = [[0.         0.         0.         0.41999388]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 3263 is [False, False, True, False, True, False]
State prediction error at timestep 3263 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3263 of None
Current timestep = 3264. State = [[ 0.36578226 -0.07852399]]. Action = [[0.09037786 0.06555379 0.         0.573594  ]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 3264 is [False, False, True, False, True, False]
State prediction error at timestep 3264 is tensor(3.7716e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3264 of None
Current timestep = 3265. State = [[ 0.36616465 -0.07814111]]. Action = [[0.09120435 0.08984945 0.         0.23041785]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 3265 is [False, False, True, False, True, False]
State prediction error at timestep 3265 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3265 of None
Current timestep = 3266. State = [[ 0.36554393 -0.07859475]]. Action = [[0.         0.         0.         0.09677327]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 3266 is [False, False, True, False, True, False]
State prediction error at timestep 3266 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3266 of None
Current timestep = 3267. State = [[ 0.36519676 -0.07765659]]. Action = [[0.09517095 0.09382854 0.         0.17450666]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 3267 is [False, False, True, False, True, False]
State prediction error at timestep 3267 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3267 of None
Current timestep = 3268. State = [[ 0.3654014  -0.07739045]]. Action = [[0.06844605 0.08716107 0.         0.46158898]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 3268 is [False, False, True, False, True, False]
State prediction error at timestep 3268 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3268 of None
Current timestep = 3269. State = [[ 0.36473686 -0.07790726]]. Action = [[0.         0.         0.         0.68935573]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 3269 is [False, False, True, False, True, False]
State prediction error at timestep 3269 is tensor(3.5198e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3269 of None
Current timestep = 3270. State = [[ 0.3644146  -0.07673285]]. Action = [[0.0940947  0.09609129 0.         0.09383845]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 3270 is [False, False, True, False, True, False]
State prediction error at timestep 3270 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3270 of None
Current timestep = 3271. State = [[ 0.36506322 -0.07605566]]. Action = [[0.08991364 0.08603496 0.         0.69195676]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 3271 is [False, False, True, False, True, False]
State prediction error at timestep 3271 is tensor(1.8539e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3271 of None
Current timestep = 3272. State = [[ 0.3654412 -0.0758521]]. Action = [[0.09659828 0.06445282 0.         0.65234065]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 3272 is [False, False, True, False, True, False]
State prediction error at timestep 3272 is tensor(3.9393e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3272 of None
Current timestep = 3273. State = [[ 0.36501628 -0.0763762 ]]. Action = [[0.         0.         0.         0.47246146]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 3273 is [False, False, True, False, True, False]
State prediction error at timestep 3273 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3273 of None
Current timestep = 3274. State = [[ 0.36503872 -0.0755863 ]]. Action = [[0.08824449 0.09760911 0.         0.51247704]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 3274 is [False, False, True, False, True, False]
State prediction error at timestep 3274 is tensor(9.3586e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3274 of None
Current timestep = 3275. State = [[ 0.36569116 -0.07483066]]. Action = [[0.08776353 0.09843732 0.         0.17809725]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 3275 is [False, False, True, False, True, False]
State prediction error at timestep 3275 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3275 of None
Current timestep = 3276. State = [[ 0.36535347 -0.07538527]]. Action = [[ 0.         0.         0.        -0.2878667]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 3276 is [False, False, True, False, True, False]
State prediction error at timestep 3276 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3276 of None
Current timestep = 3277. State = [[ 0.36542234 -0.07461479]]. Action = [[0.09507235 0.09744696 0.         0.7019005 ]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 3277 is [False, False, True, False, True, False]
State prediction error at timestep 3277 is tensor(7.8213e-07, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3277 of None
Current timestep = 3278. State = [[ 0.36626074 -0.07378475]]. Action = [[0.09587092 0.09285022 0.         0.42176414]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 3278 is [False, False, True, False, True, False]
State prediction error at timestep 3278 is tensor(9.7875e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3278 of None
Current timestep = 3279. State = [[ 0.36681312 -0.07329365]]. Action = [[0.08672441 0.09155797 0.         0.47687626]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 3279 is [False, False, True, False, True, False]
State prediction error at timestep 3279 is tensor(9.3040e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3279 of None
Current timestep = 3280. State = [[ 0.3665229  -0.07395044]]. Action = [[0.        0.        0.        0.3262925]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 3280 is [False, False, True, False, True, False]
State prediction error at timestep 3280 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3280 of None
Current timestep = 3281. State = [[ 0.36503428 -0.07422881]]. Action = [[0.        0.        0.        0.2705505]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 3281 is [False, False, True, False, True, False]
State prediction error at timestep 3281 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3281 of None
Current timestep = 3282. State = [[ 0.36484537 -0.07324298]]. Action = [[0.08724562 0.09785109 0.         0.31069398]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 3282 is [False, False, True, False, True, False]
State prediction error at timestep 3282 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3282 of None
Current timestep = 3283. State = [[ 0.36541128 -0.07268412]]. Action = [[0.09086598 0.08369709 0.         0.24721098]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 3283 is [False, False, True, False, True, False]
State prediction error at timestep 3283 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3283 of None
Current timestep = 3284. State = [[ 0.3656402  -0.07263382]]. Action = [[0.09391142 0.05889916 0.         0.44888985]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 3284 is [False, False, True, False, True, False]
State prediction error at timestep 3284 is tensor(8.5243e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3284 of None
Current timestep = 3285. State = [[ 0.36587888 -0.07248248]]. Action = [[0.08750954 0.08726133 0.         0.4751891 ]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 3285 is [False, False, True, False, True, False]
State prediction error at timestep 3285 is tensor(7.8325e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3285 of None
Current timestep = 3286. State = [[ 0.36608383 -0.07233718]]. Action = [[0.08447071 0.08804796 0.         0.45302057]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 3286 is [False, False, True, False, True, False]
State prediction error at timestep 3286 is tensor(9.1331e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3286 of None
Current timestep = 3287. State = [[ 0.3655059  -0.07285053]]. Action = [[0.       0.       0.       0.363567]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 3287 is [False, False, True, False, True, False]
State prediction error at timestep 3287 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3287 of None
Current timestep = 3288. State = [[ 0.3650945  -0.07228541]]. Action = [[0.09322564 0.0522142  0.         0.30388713]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 3288 is [False, False, True, False, True, False]
State prediction error at timestep 3288 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3288 of None
Current timestep = 3289. State = [[ 0.36564043 -0.07168867]]. Action = [[0.09652162 0.09459826 0.         0.55990624]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 3289 is [False, False, True, False, True, False]
State prediction error at timestep 3289 is tensor(2.0240e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3289 of None
Current timestep = 3290. State = [[ 0.366217   -0.07097989]]. Action = [[0.08382394 0.09806278 0.         0.33408248]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 3290 is [False, False, True, False, True, False]
State prediction error at timestep 3290 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3290 of None
Current timestep = 3291. State = [[ 0.36593336 -0.07154488]]. Action = [[0.        0.        0.        0.1759944]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 3291 is [False, False, True, False, True, False]
State prediction error at timestep 3291 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3291 of None
Current timestep = 3292. State = [[ 0.36426124 -0.07177155]]. Action = [[0.         0.         0.         0.01427639]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 3292 is [False, False, True, False, True, False]
State prediction error at timestep 3292 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3292 of None
Current timestep = 3293. State = [[ 0.36412    -0.07067861]]. Action = [[0.09432781 0.09810884 0.         0.6452472 ]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 3293 is [False, False, True, False, True, False]
State prediction error at timestep 3293 is tensor(4.2901e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3293 of None
Current timestep = 3294. State = [[ 0.36513254 -0.06973679]]. Action = [[0.09781644 0.08893158 0.         0.20084286]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 3294 is [False, False, True, False, True, False]
State prediction error at timestep 3294 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3294 of None
Current timestep = 3295. State = [[ 0.3662596  -0.06876756]]. Action = [[0.09867992 0.09839056 0.         0.43948352]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 3295 is [False, False, True, False, True, False]
State prediction error at timestep 3295 is tensor(6.8302e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3295 of None
Current timestep = 3296. State = [[ 0.36676344 -0.06831703]]. Action = [[0.08217186 0.09293551 0.         0.34686565]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 3296 is [False, False, True, False, True, False]
State prediction error at timestep 3296 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3296 of None
Current timestep = 3297. State = [[ 0.36682573 -0.06828851]]. Action = [[0.0646775  0.09381597 0.         0.633651  ]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 3297 is [False, False, True, False, True, False]
State prediction error at timestep 3297 is tensor(3.5062e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3297 of None
Current timestep = 3298. State = [[ 0.36619595 -0.06887621]]. Action = [[0.        0.        0.        0.4533744]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 3298 is [False, False, True, False, True, False]
State prediction error at timestep 3298 is tensor(8.9123e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3298 of None
Current timestep = 3299. State = [[ 0.3658562  -0.06792308]]. Action = [[0.08686831 0.09515209 0.         0.7038475 ]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 3299 is [False, False, True, False, True, False]
State prediction error at timestep 3299 is tensor(1.4068e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3299 of None
Current timestep = 3300. State = [[ 0.3659499  -0.06775797]]. Action = [[0.07242193 0.08519465 0.         0.65686715]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 3300 is [False, False, True, False, True, False]
State prediction error at timestep 3300 is tensor(1.2541e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3300 of None
Current timestep = 3301. State = [[ 0.3654366 -0.0682956]]. Action = [[0.         0.         0.         0.61557186]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 3301 is [False, False, True, False, True, False]
State prediction error at timestep 3301 is tensor(2.3457e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3301 of None
Current timestep = 3302. State = [[ 0.3652376  -0.06787892]]. Action = [[0.0860754  0.06289897 0.         0.7882216 ]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 3302 is [False, False, True, False, True, False]
State prediction error at timestep 3302 is tensor(4.3431e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3302 of None
Current timestep = 3303. State = [[ 0.3657394  -0.06730511]]. Action = [[0.09350147 0.09794838 0.         0.6671481 ]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 3303 is [False, False, True, False, True, False]
State prediction error at timestep 3303 is tensor(1.6224e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3303 of None
Current timestep = 3304. State = [[ 0.36620983 -0.06683216]]. Action = [[0.08758881 0.08635979 0.         0.32161713]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 3304 is [False, False, True, False, True, False]
State prediction error at timestep 3304 is tensor(8.0247e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3304 of None
Current timestep = 3305. State = [[ 0.36646706 -0.06664414]]. Action = [[0.08665813 0.08949671 0.         0.43143153]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 3305 is [False, False, True, False, True, False]
State prediction error at timestep 3305 is tensor(2.4827e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3305 of None
Current timestep = 3306. State = [[ 0.3661133  -0.06720376]]. Action = [[0.         0.         0.         0.59595156]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 3306 is [False, False, True, False, True, False]
State prediction error at timestep 3306 is tensor(3.2730e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3306 of None
Current timestep = 3307. State = [[ 0.36613834 -0.06645608]]. Action = [[0.08888186 0.09968647 0.         0.67189217]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 3307 is [False, False, True, False, True, False]
State prediction error at timestep 3307 is tensor(4.3738e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3307 of None
Current timestep = 3308. State = [[ 0.36585173 -0.06690714]]. Action = [[0.        0.        0.        0.5541239]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 3308 is [False, False, True, False, True, False]
State prediction error at timestep 3308 is tensor(1.6161e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3308 of None
Current timestep = 3309. State = [[ 0.36409572 -0.06444768]]. Action = [[0.0139911  0.09836552 0.         0.7643447 ]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 3309 is [False, False, True, False, True, False]
State prediction error at timestep 3309 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3309 of None
Current timestep = 3310. State = [[ 0.36411437 -0.06199074]]. Action = [[0.08357299 0.09664323 0.         0.37305105]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 3310 is [False, False, True, False, True, False]
State prediction error at timestep 3310 is tensor(5.2578e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3310 of None
Current timestep = 3311. State = [[ 0.364855   -0.06109772]]. Action = [[0.08716872 0.09738108 0.         0.5724282 ]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 3311 is [False, False, True, False, True, False]
State prediction error at timestep 3311 is tensor(2.1733e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3311 of None
Current timestep = 3312. State = [[ 0.3650681  -0.06093103]]. Action = [[0.08081133 0.07751549 0.         0.6331177 ]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 3312 is [False, False, True, False, True, False]
State prediction error at timestep 3312 is tensor(6.4554e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3312 of None
Current timestep = 3313. State = [[ 0.36526248 -0.06076149]]. Action = [[0.08166078 0.09594179 0.         0.73549294]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 3313 is [False, False, True, False, True, False]
State prediction error at timestep 3313 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3313 of None
Current timestep = 3314. State = [[ 0.36548114 -0.06055051]]. Action = [[0.07729126 0.09878226 0.         0.596984  ]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 3314 is [False, False, True, False, True, False]
State prediction error at timestep 3314 is tensor(3.5952e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3314 of None
Current timestep = 3315. State = [[ 0.36551982 -0.0605518 ]]. Action = [[0.05217984 0.09338953 0.         0.7916882 ]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 3315 is [False, False, True, False, True, False]
State prediction error at timestep 3315 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3315 of None
Current timestep = 3316. State = [[ 0.36576355 -0.0603368 ]]. Action = [[0.08725835 0.09361149 0.         0.82137764]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 3316 is [False, False, True, False, True, False]
State prediction error at timestep 3316 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3316 of None
Current timestep = 3317. State = [[ 0.36592606 -0.06015949]]. Action = [[0.07547288 0.09853163 0.         0.62615037]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 3317 is [False, False, True, False, True, False]
State prediction error at timestep 3317 is tensor(9.4419e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3317 of None
Current timestep = 3318. State = [[ 0.36579105 -0.05982582]]. Action = [[0.00957958 0.0961375  0.         0.5882876 ]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 3318 is [False, False, True, False, True, False]
State prediction error at timestep 3318 is tensor(4.6688e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3318 of None
Current timestep = 3319. State = [[ 0.36491048 -0.05787964]]. Action = [[0.02294246 0.09800649 0.         0.83948255]]. Reward = [0.]
Curr episode timestep = 526
