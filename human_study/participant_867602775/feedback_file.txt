Current timestep = 0. State = [[-0.32604954 -0.08628346]]. Action = [[0.00435984 0.09830839 0.         0.7271644 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0668, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.33171427 -0.08338373]]. Action = [[-0.08950701 -0.00990611  0.          0.9261981 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0582, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of 0
Current timestep = 2. State = [[-0.33283383 -0.08530018]]. Action = [[ 0.0591953  -0.05022874  0.          0.01753402]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0498, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of 0
Current timestep = 3. State = [[-0.32827213 -0.08361286]]. Action = [[ 0.07941724  0.04838648  0.         -0.50821066]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0282, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of 1
Current timestep = 4. State = [[-0.3254112  -0.08398439]]. Action = [[ 0.02240376 -0.05189327  0.          0.06061864]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0375, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of 1
Current timestep = 5. State = [[-0.3279658  -0.08313346]]. Action = [[-0.06502104  0.04298217  0.         -0.71768785]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of 1
Current timestep = 6. State = [[-0.33477715 -0.08682345]]. Action = [[-0.094002   -0.09525916  0.         -0.55308723]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of 1
Current timestep = 7. State = [[-0.34229717 -0.08953279]]. Action = [[-0.08571782  0.01549295  0.          0.7501428 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0277, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of 1
Current timestep = 8. State = [[-0.34299064 -0.09310049]]. Action = [[ 0.06541953 -0.06993314  0.          0.35913312]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0267, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of 1
Current timestep = 9. State = [[-0.3389573  -0.09816226]]. Action = [[ 0.05121744 -0.04896273  0.          0.96607983]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0148, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of -1
Current timestep = 10. State = [[-0.34124613 -0.0959961 ]]. Action = [[-0.09368613  0.09745029  0.          0.9562595 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of -1
Current timestep = 11. State = [[-0.34792623 -0.08939695]]. Action = [[-0.06634983  0.08777509  0.         -0.77486765]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of -1
Current timestep = 12. State = [[-0.35244313 -0.0906543 ]]. Action = [[-0.02049319 -0.08020046  0.         -0.95386964]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.35139966 -0.09706095]]. Action = [[ 0.06871281 -0.08378123  0.          0.7991195 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.34901246 -0.10332912]]. Action = [[ 0.03632901 -0.07363381  0.          0.9914689 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
