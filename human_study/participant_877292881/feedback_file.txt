Current timestep = 0. State = [[-0.35738286 -0.05547921]]. Action = [[-0.06292832 -0.00783072  0.21038541 -0.03610903]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Current timestep = 1. State = [[-0.35752153 -0.05436338]]. Action = [[ 0.02804708  0.09211323 -0.17953753 -0.24135602]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0705, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.35789382 -0.05175612]]. Action = [[-0.15283313  0.10317436  0.08250085  0.97395396]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
Current timestep = 3. State = [[-0.35933426 -0.04734477]]. Action = [[-0.15658152  0.18879378 -0.05679892 -0.21903908]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0716, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.36281514 -0.03972324]]. Action = [[-0.06515831 -0.21350987 -0.2270246   0.91687274]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
Current timestep = 5. State = [[-0.36582583 -0.0377627 ]]. Action = [[-0.19404647  0.20411986 -0.19760707 -0.8448607 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
Current timestep = 6. State = [[-0.3699175  -0.03269004]]. Action = [[ 0.24527463 -0.10609299 -0.19202721 -0.2252906 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
Current timestep = 7. State = [[-0.37113813 -0.03064646]]. Action = [[-0.13173239  0.19834805 -0.12744653  0.7123525 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0877, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.37409267 -0.02281862]]. Action = [[ 0.18874025  0.10747921 -0.21657972 -0.08161724]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
Current timestep = 9. State = [[-0.37460154 -0.01884201]]. Action = [[-0.17556798 -0.14458303  0.2087945   0.24431825]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0774, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.37529853 -0.01829862]]. Action = [[ 0.16702765 -0.20503464 -0.198931   -0.67114556]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0614, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.37366644 -0.02138027]]. Action = [[-0.02317014 -0.1613437   0.12703657 -0.9684069 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
Current timestep = 12. State = [[-0.3719916  -0.02715165]]. Action = [[-0.02471386 -0.24597779  0.20099166 -0.614913  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0500, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.3698276  -0.03649179]]. Action = [[ 0.22643995 -0.13567218  0.19493079  0.24568582]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0757, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.3656791  -0.04620034]]. Action = [[ 0.18065628  0.03531045 -0.23489961 -0.4160719 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
Current timestep = 15. State = [[-0.36108837 -0.05389963]]. Action = [[-0.1477273  -0.12928894 -0.13149224  0.7692232 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
Current timestep = 16. State = [[-0.35885984 -0.06011984]]. Action = [[-0.22118598  0.18397707 -0.03226592  0.7894316 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0810, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.36084595 -0.06094344]]. Action = [[ 0.01967832  0.2151779  -0.18030106 -0.33679414]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
Current timestep = 18. State = [[-0.36199817 -0.05751732]]. Action = [[-0.08254196  0.20234978 -0.12120283 -0.731575  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
Current timestep = 19. State = [[-0.36468887 -0.05141538]]. Action = [[ 0.00782257 -0.09399566  0.12785363 -0.25821555]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0615, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.3662421 -0.0480793]]. Action = [[ 0.15812397  0.14410222 -0.09557876  0.49787724]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
Human Feedback received at timestep 20 of 1
Current timestep = 21. State = [[-0.36648637 -0.04391291]]. Action = [[ 0.14182323  0.01168245 -0.18392065  0.501621  ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
Current timestep = 22. State = [[-0.36485928 -0.04027814]]. Action = [[ 0.1688351   0.24548686  0.09852752 -0.9514527 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
Current timestep = 23. State = [[-0.36222863 -0.03329502]]. Action = [[-0.05702102  0.17219979  0.14208743  0.03624845]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0763, grad_fn=<MseLossBackward0>)
Current timestep = 24. State = [[-0.36156392 -0.02275302]]. Action = [[ 0.08372721  0.24889523  0.13376862 -0.682824  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
Current timestep = 25. State = [[-0.36097702 -0.0121189 ]]. Action = [[-0.11555925 -0.20196092  0.24720162  0.63734305]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.3608499  -0.00812096]]. Action = [[ 0.11335847  0.04647794 -0.01763617 -0.01341397]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0748, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.3602276  -0.00494486]]. Action = [[-0.1318131  -0.15813036  0.01346835 -0.01878482]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0675, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.3599242 -0.004656 ]]. Action = [[-0.18267702  0.03736448 -0.0018138  -0.91120285]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
Human Feedback received at timestep 28 of 1
Current timestep = 29. State = [[-0.36111015 -0.00399468]]. Action = [[ 0.12679619  0.13841623 -0.04986307 -0.4208697 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
Current timestep = 30. State = [[-0.36190858 -0.00108058]]. Action = [[-0.0342588   0.1584205  -0.23644787 -0.95380545]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
Current timestep = 31. State = [[-0.36379474  0.00442437]]. Action = [[-0.05987707  0.03214163 -0.05190513  0.84736323]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
Current timestep = 32. State = [[-0.3657156   0.00870674]]. Action = [[ 0.00109294 -0.17708966 -0.18974707 -0.4909749 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
Current timestep = 33. State = [[-0.36616796  0.00843914]]. Action = [[-0.19922154 -0.19781938  0.21593627 -0.9374181 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
Current timestep = 34. State = [[-0.36838073  0.00573797]]. Action = [[ 0.05743226  0.03883946 -0.23645777  0.09854746]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
Current timestep = 35. State = [[-0.36888453  0.00407917]]. Action = [[-0.0594888  -0.08989099  0.08439064  0.6699455 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
Current timestep = 36. State = [[-0.36969838  0.00136609]]. Action = [[ 0.23280835 -0.02098373  0.16651341  0.02484822]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
Current timestep = 37. State = [[-0.36853665 -0.00074451]]. Action = [[ 0.16575176 -0.11865516  0.24713773  0.314659  ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
Current timestep = 38. State = [[-0.36527053 -0.00536476]]. Action = [[-0.08253792 -0.17259337 -0.16319866 -0.56067777]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
Current timestep = 39. State = [[-0.36242428 -0.01163517]]. Action = [[-0.0535952  -0.0970277  -0.09475388  0.5556381 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
Current timestep = 40. State = [[-0.36057755 -0.0176387 ]]. Action = [[-0.22650304  0.23190928 -0.19892998  0.32077074]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
Current timestep = 41. State = [[-0.36216298 -0.0178243 ]]. Action = [[ 0.10872632 -0.09703131  0.20763269 -0.17703742]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0628, grad_fn=<MseLossBackward0>)
Current timestep = 42. State = [[-0.36255303 -0.01943528]]. Action = [[-0.09357411  0.01792291  0.154369    0.46081853]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0789, grad_fn=<MseLossBackward0>)
Current timestep = 43. State = [[-0.3635079  -0.02003236]]. Action = [[ 0.15594012  0.13645342  0.0622842  -0.7390872 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
Current timestep = 44. State = [[-0.3640064  -0.01876288]]. Action = [[-0.02267635  0.18309161  0.1395728  -0.0730992 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0751, grad_fn=<MseLossBackward0>)
Current timestep = 45. State = [[-0.36495376 -0.01434432]]. Action = [[ 0.11342704 -0.04019237  0.05631548 -0.51758724]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0591, grad_fn=<MseLossBackward0>)
Current timestep = 46. State = [[-0.36436415 -0.01251049]]. Action = [[0.09529731 0.10571244 0.22975743 0.02757192]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0745, grad_fn=<MseLossBackward0>)
Current timestep = 47. State = [[-0.36299372 -0.00970916]]. Action = [[-0.03294587 -0.06545065  0.09615448  0.755185  ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
Current timestep = 48. State = [[-0.36271375 -0.00934877]]. Action = [[-0.1157887   0.05567047 -0.20120142  0.37288344]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
Current timestep = 49. State = [[-0.36342418 -0.00710871]]. Action = [[ 0.1134553   0.07748425 -0.20218235  0.8847685 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
Current timestep = 50. State = [[-0.3635654  -0.00495614]]. Action = [[-0.07827386 -0.20051333 -0.12017211 -0.02700591]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
Current timestep = 51. State = [[-0.36295563 -0.00516351]]. Action = [[ 0.2068848   0.22652376 -0.03586359  0.1242373 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0856, grad_fn=<MseLossBackward0>)
Current timestep = 52. State = [[-0.3608478  -0.00333752]]. Action = [[ 0.18189603 -0.20764373  0.18816039  0.5712166 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
Current timestep = 53. State = [[-0.35649914 -0.00473455]]. Action = [[-0.11441588 -0.05674538 -0.12370075 -0.8974389 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
Current timestep = 54. State = [[-0.3545093  -0.00669584]]. Action = [[-0.12237573 -0.2230823   0.20830828  0.16367233]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
Current timestep = 55. State = [[-0.3529927  -0.01195073]]. Action = [[-0.08687785 -0.17729017 -0.08210889  0.7207265 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
Current timestep = 56. State = [[-0.35275063 -0.01990576]]. Action = [[ 0.05953512  0.11114797 -0.14817959  0.91396666]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0727, grad_fn=<MseLossBackward0>)
Current timestep = 57. State = [[-0.35180742 -0.02331837]]. Action = [[ 0.23208168 -0.15677391  0.2330029  -0.27012765]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
Current timestep = 58. State = [[-0.34918478 -0.02900378]]. Action = [[-0.01688263 -0.17942126 -0.09049878  0.02913284]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0681, grad_fn=<MseLossBackward0>)
Current timestep = 59. State = [[-0.34628984 -0.03695577]]. Action = [[-0.22462757  0.22966427  0.03413746 -0.01006943]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
Current timestep = 60. State = [[-0.34729588 -0.03788082]]. Action = [[ 0.08144125  0.15033007 -0.04957311  0.36233544]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
Current timestep = 61. State = [[-0.3481275 -0.0362539]]. Action = [[ 0.21871018 -0.00888601  0.11972737  0.6086904 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
