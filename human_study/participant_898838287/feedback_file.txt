Current timestep = 0. State = [[-0.32469195 -0.09222239]]. Action = [[ 0.02229474 -0.03176285  0.          0.6945392 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0657, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.32169    -0.08889792]]. Action = [[0.07273958 0.08031679 0.         0.5653672 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0604, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.31575733 -0.09026128]]. Action = [[ 0.09720076 -0.09275587  0.          0.741688  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0470, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.31539977 -0.09703942]]. Action = [[-0.05017178 -0.08716325  0.         -0.943911  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0102, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of -1
Current timestep = 4. State = [[-0.3189274  -0.09997799]]. Action = [[-0.03822239  0.00587551  0.          0.9410522 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0348, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.319256   -0.10158595]]. Action = [[ 0.02518066 -0.01758945  0.         -0.7861205 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0068, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.32323048 -0.1033365 ]]. Action = [[-0.08941065 -0.00644071  0.          0.78589296]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0282, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.32839137 -0.10746875]]. Action = [[-0.04469977 -0.05429917  0.          0.822989  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0238, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.33124748 -0.10866003]]. Action = [[-0.0169806   0.03194303  0.         -0.36141574]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0114, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.33664912 -0.10637154]]. Action = [[-0.084472    0.04737439  0.         -0.9839954 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0009, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of -1
Current timestep = 10. State = [[-0.3383301  -0.10440187]]. Action = [[ 0.03704789  0.01940899  0.         -0.12052411]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0116, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of -1
Current timestep = 11. State = [[-0.33424765 -0.10810404]]. Action = [[ 0.08422182 -0.09165921  0.          0.7452457 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0113, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of -1
Current timestep = 12. State = [[-0.33626384 -0.10849103]]. Action = [[-0.09670849  0.0582609   0.          0.8224919 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0082, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.3442533  -0.10348226]]. Action = [[-0.07302345  0.0688956   0.         -0.41426474]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0019, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.34546924 -0.09611364]]. Action = [[0.07074752 0.08803747 0.         0.86513233]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0053, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of -1
Current timestep = 15. State = [[-0.3429427  -0.09644101]]. Action = [[ 0.05255061 -0.09625831  0.         -0.39340043]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0010, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of -1
Current timestep = 16. State = [[-0.3449617  -0.10219696]]. Action = [[-0.04108945 -0.08022957  0.         -0.4702078 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0003, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of -1
Current timestep = 17. State = [[-0.3439915  -0.10916795]]. Action = [[ 0.0697669  -0.0945739   0.          0.17672777]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0064, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of -1
Current timestep = 18. State = [[-0.338468   -0.10819985]]. Action = [[ 0.09107513  0.07852144  0.         -0.7053374 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0009, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 18 of -1
Current timestep = 19. State = [[-0.3325802  -0.10451122]]. Action = [[ 0.06973153  0.01888754  0.         -0.07623309]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0035, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 19 of -1
Current timestep = 20. State = [[-0.33048522 -0.10368551]]. Action = [[ 0.00031517 -0.00476803  0.         -0.11825311]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0032, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 20 of -1
Current timestep = 21. State = [[-0.3282604  -0.10699683]]. Action = [[ 0.03999973 -0.06214224  0.         -0.17243189]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0028, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 21 of -1
Current timestep = 22. State = [[-0.3254193  -0.11202058]]. Action = [[ 0.02460663 -0.05546839  0.          0.79706144]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0013, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.3195051  -0.11386391]]. Action = [[0.08764987 0.00939891 0.         0.59987974]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0029, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 23 of -1
Current timestep = 24. State = [[-0.3130215  -0.11283623]]. Action = [[ 0.05803346  0.02370642  0.         -0.98559666]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0037, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 24 of -1
Current timestep = 25. State = [[-0.31026343 -0.11614295]]. Action = [[-0.00427695 -0.07005113  0.         -0.17642128]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0031, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 25 of -1
Current timestep = 26. State = [[-0.30905437 -0.12296655]]. Action = [[-0.00110683 -0.07423087  0.         -0.38064265]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0024, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 26 of -1
Current timestep = 27. State = [[-0.3116558  -0.13168783]]. Action = [[-0.08033629 -0.09625693  0.          0.7704078 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, True, False, False]
State prediction error at timestep 27 is tensor(0.0017, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 27 of -1
Current timestep = 28. State = [[-0.3119894  -0.14081657]]. Action = [[ 0.0133271 -0.0819366  0.         0.653463 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, True, False, False]
State prediction error at timestep 28 is tensor(0.0030, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 28 of -1
Current timestep = 29. State = [[-0.3113246  -0.14839438]]. Action = [[-0.01809908 -0.05650404  0.         -0.481125  ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, True, False, False]
State prediction error at timestep 29 is tensor(0.0034, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 29 of -1
Current timestep = 30. State = [[-0.30851093 -0.15366091]]. Action = [[ 0.04211315 -0.02406849  0.          0.3846681 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, True, False, False]
State prediction error at timestep 30 is tensor(0.0062, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 30 of -1
Current timestep = 31. State = [[-0.30811593 -0.15841435]]. Action = [[-0.03829063 -0.03513519  0.          0.64907014]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, True, False, False]
State prediction error at timestep 31 is tensor(0.0036, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 31 of -1
Current timestep = 32. State = [[-0.31274343 -0.15792434]]. Action = [[-0.09085851  0.08262824  0.          0.72516227]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, True, False, False]
State prediction error at timestep 32 is tensor(0.0031, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 32 of -1
Current timestep = 33. State = [[-0.31540653 -0.15697634]]. Action = [[-0.00333395  0.01159233  0.          0.6386298 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, True, False, False]
State prediction error at timestep 33 is tensor(0.0039, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 33 of -1
Current timestep = 34. State = [[-0.31119606 -0.15850823]]. Action = [[ 0.09106226 -0.02086967  0.         -0.9665107 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, True, False, False]
State prediction error at timestep 34 is tensor(0.0006, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.31082422 -0.16397797]]. Action = [[-0.04780367 -0.0867682   0.         -0.17254794]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, True, False, False]
State prediction error at timestep 35 is tensor(0.0065, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 35 of -1
Current timestep = 36. State = [[-0.30916497 -0.16936769]]. Action = [[ 0.05861879 -0.03684181  0.         -0.3982991 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, True, False, False]
State prediction error at timestep 36 is tensor(0.0057, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 36 of -1
Current timestep = 37. State = [[-0.3035498  -0.17141597]]. Action = [[ 0.07700456 -0.00436697  0.          0.44656897]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, True, False, False]
State prediction error at timestep 37 is tensor(0.0051, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 37 of -1
Current timestep = 38. State = [[-0.2966221  -0.17550388]]. Action = [[ 0.08552761 -0.07198684  0.         -0.8596128 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, True, False, False]
State prediction error at timestep 38 is tensor(0.0012, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 38 of -1
Current timestep = 39. State = [[-0.2913479  -0.17953016]]. Action = [[ 0.04288609 -0.02878256  0.         -0.36984372]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, True, False, False]
State prediction error at timestep 39 is tensor(0.0048, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 39 of -1
Current timestep = 40. State = [[-0.29208663 -0.17805833]]. Action = [[-0.05706665  0.07269437  0.         -0.6842392 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, True, False, False]
State prediction error at timestep 40 is tensor(0.0019, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[-0.29274294 -0.17631985]]. Action = [[ 0.0191896   0.01138022  0.         -0.54820037]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, True, False, False]
State prediction error at timestep 41 is tensor(0.0029, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 41 of -1
Current timestep = 42. State = [[-0.2922257  -0.17346865]]. Action = [[0.0026096  0.05610836 0.         0.47698927]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0036, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 42 of -1
Current timestep = 43. State = [[-0.29445422 -0.16769941]]. Action = [[-0.04514809  0.08550171  0.         -0.21809268]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, True, False, False]
State prediction error at timestep 43 is tensor(0.0043, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 43 of -1
Current timestep = 44. State = [[-0.29444453 -0.16050972]]. Action = [[ 0.03160175  0.08614082  0.         -0.9579701 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, True, False, False]
State prediction error at timestep 44 is tensor(0.0006, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 44 of -1
Current timestep = 45. State = [[-0.2949645 -0.15788  ]]. Action = [[-0.02169093 -0.02061351  0.         -0.4111904 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, True, False, False]
State prediction error at timestep 45 is tensor(0.0027, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 45 of -1
Current timestep = 46. State = [[-0.29231668 -0.15351862]]. Action = [[ 0.0719865   0.07168538  0.         -0.6066385 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, True, False, False]
State prediction error at timestep 46 is tensor(0.0008, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 46 of -1
Current timestep = 47. State = [[-0.29430494 -0.15297568]]. Action = [[-0.08150573 -0.07112136  0.          0.40817535]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, True, False, False]
State prediction error at timestep 47 is tensor(0.0021, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 47 of -1
Current timestep = 48. State = [[-0.29933125 -0.15419313]]. Action = [[-4.9396206e-02  2.1739304e-04  0.0000000e+00 -9.3491089e-01]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, True, False, False]
State prediction error at timestep 48 is tensor(0.0003, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 48 of -1
Current timestep = 49. State = [[-0.3050113  -0.15718442]]. Action = [[-0.07617895 -0.06437383  0.          0.70311904]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, True, False, False]
State prediction error at timestep 49 is tensor(0.0011, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 49 of -1
Current timestep = 50. State = [[-0.3099134 -0.156857 ]]. Action = [[-0.04557757  0.04409928  0.         -0.76318455]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, True, False, False]
State prediction error at timestep 50 is tensor(0.0003, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 50 of -1
Current timestep = 51. State = [[-0.30974585 -0.1595553 ]]. Action = [[ 0.0452776  -0.08658009  0.          0.4095379 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, True, False, False]
State prediction error at timestep 51 is tensor(0.0027, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 51 of -1
Current timestep = 52. State = [[-0.31141752 -0.16646521]]. Action = [[-0.05890924 -0.08696353  0.         -0.92718446]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, True, False, False]
State prediction error at timestep 52 is tensor(0.0003, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 52 of -1
Current timestep = 53. State = [[-0.30831498 -0.1712182 ]]. Action = [[ 0.09445583 -0.03007958  0.          0.17810309]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, True, False, False]
State prediction error at timestep 53 is tensor(0.0048, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 53 of -1
Current timestep = 54. State = [[-0.3011731  -0.17609875]]. Action = [[ 0.08287285 -0.0715467   0.         -0.6075592 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, True, False, False]
State prediction error at timestep 54 is tensor(0.0017, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 54 of -1
Current timestep = 55. State = [[-0.29899597 -0.17859   ]]. Action = [[-0.02224252  0.0101346   0.         -0.25031316]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, True, False, False]
State prediction error at timestep 55 is tensor(0.0037, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 55 of -1
Current timestep = 56. State = [[-0.30162537 -0.182576  ]]. Action = [[-0.04588234 -0.05900852  0.          0.9318757 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, True, False, False]
State prediction error at timestep 56 is tensor(0.0024, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 56 of -1
Current timestep = 57. State = [[-0.30438098 -0.18239228]]. Action = [[-0.02625312  0.06958126  0.          0.15340912]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, True, False, False]
State prediction error at timestep 57 is tensor(0.0053, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 57 of -1
Current timestep = 58. State = [[-0.30488998 -0.18339987]]. Action = [[ 0.01181759 -0.03393628  0.          0.3219366 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, True, False, False]
State prediction error at timestep 58 is tensor(0.0048, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 58 of -1
Current timestep = 59. State = [[-0.30536205 -0.18187076]]. Action = [[-0.01233193  0.06897721  0.          0.21443868]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, True, False, False]
State prediction error at timestep 59 is tensor(0.0054, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 59 of -1
Current timestep = 60. State = [[-0.30530748 -0.17846863]]. Action = [[ 0.01278992  0.04108963  0.         -0.4113915 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, True, False, False]
State prediction error at timestep 60 is tensor(0.0038, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.30199042 -0.18089293]]. Action = [[ 0.0674668  -0.07771618  0.         -0.8957509 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, True, False, False]
State prediction error at timestep 61 is tensor(0.0007, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 61 of -1
Current timestep = 62. State = [[-0.2959621  -0.18111567]]. Action = [[0.08030897 0.03409124 0.         0.78993654]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, True, False, False]
State prediction error at timestep 62 is tensor(0.0017, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.2933922  -0.17973754]]. Action = [[-0.00172371  0.00337388  0.         -0.46406585]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, True, False, False]
State prediction error at timestep 63 is tensor(0.0025, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 63 of -1
Current timestep = 64. State = [[-0.29437748 -0.17883107]]. Action = [[-0.02298178  0.0148866   0.         -0.7963629 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, True, False, False]
State prediction error at timestep 64 is tensor(0.0005, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 64 of -1
Current timestep = 65. State = [[-0.29891434 -0.1768308 ]]. Action = [[-0.07734887  0.03622632  0.         -0.10169584]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, True, False, False]
State prediction error at timestep 65 is tensor(0.0034, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 65 of -1
Current timestep = 66. State = [[-0.3007113  -0.17623599]]. Action = [[ 0.01301273 -0.00621377  0.         -0.03167582]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, True, False, False]
State prediction error at timestep 66 is tensor(0.0037, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 66 of -1
Current timestep = 67. State = [[-0.30096358 -0.17282012]]. Action = [[-0.00967857  0.06674638  0.         -0.39525443]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, True, False, False]
State prediction error at timestep 67 is tensor(0.0026, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.3054693  -0.17496377]]. Action = [[-0.0861788  -0.09277916  0.          0.08084881]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, True, False, False]
State prediction error at timestep 68 is tensor(0.0039, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 68 of -1
Current timestep = 69. State = [[-0.305591   -0.17883386]]. Action = [[ 0.05408353 -0.02603523  0.         -0.7169528 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, True, False, False]
State prediction error at timestep 69 is tensor(0.0008, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 69 of -1
Current timestep = 70. State = [[-0.30640963 -0.17622232]]. Action = [[-0.04754664  0.07041099  0.          0.92369175]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, True, False, False]
State prediction error at timestep 70 is tensor(0.0009, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 70 of -1
Current timestep = 71. State = [[-0.31162542 -0.17750674]]. Action = [[-0.07075991 -0.06759391  0.         -0.63767916]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, True, False, False]
State prediction error at timestep 71 is tensor(0.0011, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 71 of -1
Current timestep = 72. State = [[-0.31921476 -0.1811895 ]]. Action = [[-0.09649763 -0.02990003  0.          0.6374754 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, True, False, False]
State prediction error at timestep 72 is tensor(0.0026, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 72 of -1
Current timestep = 73. State = [[-0.32562843 -0.17879322]]. Action = [[-0.05332059  0.07959092  0.         -0.8077766 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, True, False, False]
State prediction error at timestep 73 is tensor(0.0006, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-0.3304822 -0.1779438]]. Action = [[-0.04278005 -0.02353501  0.         -0.78991413]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, True, False, False]
State prediction error at timestep 74 is tensor(0.0008, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.32842436 -0.17785683]]. Action = [[ 0.09015434  0.00714462  0.         -0.396618  ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, True, False, False]
State prediction error at timestep 75 is tensor(0.0039, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.32439357 -0.18007547]]. Action = [[ 0.04532213 -0.06792535  0.          0.89667726]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, True, False, False]
State prediction error at timestep 76 is tensor(0.0013, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 76 of -1
Current timestep = 77. State = [[-0.322785   -0.18040259]]. Action = [[0.0144313  0.01986194 0.         0.8663541 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, True, False, False]
State prediction error at timestep 77 is tensor(0.0013, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 77 of -1
Current timestep = 78. State = [[-0.32380325 -0.17500784]]. Action = [[-0.0174998   0.09199872  0.          0.62704897]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, True, False, False]
State prediction error at timestep 78 is tensor(0.0025, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 78 of -1
Current timestep = 79. State = [[-0.326195   -0.16945836]]. Action = [[-0.02059372  0.04968453  0.         -0.31556755]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, True, False, False]
State prediction error at timestep 79 is tensor(0.0033, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.32439566 -0.16264041]]. Action = [[ 0.06770291  0.08787084  0.         -0.57098824]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, True, False, False]
State prediction error at timestep 80 is tensor(0.0015, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 80 of -1
Current timestep = 81. State = [[-0.32486728 -0.15401882]]. Action = [[-0.03180308  0.09222756  0.          0.29493105]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, True, False, False]
State prediction error at timestep 81 is tensor(0.0038, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 81 of -1
Current timestep = 82. State = [[-0.33084825 -0.14660369]]. Action = [[-0.08131857  0.06720298  0.         -0.49331212]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, True, False, False]
State prediction error at timestep 82 is tensor(0.0019, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 82 of -1
Current timestep = 83. State = [[-0.3298373  -0.13907571]]. Action = [[ 0.09885048  0.07256389  0.         -0.23900324]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, True, False, False]
State prediction error at timestep 83 is tensor(0.0032, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 83 of -1
Current timestep = 84. State = [[-0.3241477  -0.13171129]]. Action = [[ 0.0783428   0.04987625  0.         -0.8073817 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, True, False, False]
State prediction error at timestep 84 is tensor(0.0004, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 84 of -1
Current timestep = 85. State = [[-0.32117027 -0.12430743]]. Action = [[ 0.02806234  0.06121648  0.         -0.6994178 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(0.0007, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 85 of -1
Current timestep = 86. State = [[-0.3235761  -0.12180687]]. Action = [[-0.04912203 -0.03526851  0.          0.49409175]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
State prediction error at timestep 86 is tensor(0.0020, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 86 of -1
Current timestep = 87. State = [[-0.32396764 -0.11743826]]. Action = [[ 0.03738386  0.06852175  0.         -0.68570817]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
State prediction error at timestep 87 is tensor(0.0010, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 87 of -1
Current timestep = 88. State = [[-0.3219859  -0.11553261]]. Action = [[ 0.03191466 -0.04235819  0.         -0.30382538]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0023, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 88 of -1
Current timestep = 89. State = [[-0.322478   -0.11428098]]. Action = [[-0.02531275  0.01371982  0.          0.78721476]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0007, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 89 of -1
Current timestep = 90. State = [[-0.32478637 -0.11554446]]. Action = [[-0.0322518  -0.05507109  0.          0.07380342]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(0.0035, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 90 of -1
Current timestep = 91. State = [[-0.33025023 -0.11364338]]. Action = [[-0.09766518  0.06129413  0.          0.8318279 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
State prediction error at timestep 91 is tensor(0.0008, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 91 of -1
Current timestep = 92. State = [[-0.33320326 -0.10848869]]. Action = [[-0.00106319  0.06018581  0.         -0.23641717]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
State prediction error at timestep 92 is tensor(0.0034, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 92 of -1
Current timestep = 93. State = [[-0.3359174 -0.1103379]]. Action = [[-0.04836354 -0.09204544  0.          0.08409059]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
State prediction error at timestep 93 is tensor(0.0041, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 93 of -1
Current timestep = 94. State = [[-0.34022844 -0.10809161]]. Action = [[-0.05479487  0.09291636  0.         -0.9445427 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0008, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 94 of -1
Current timestep = 95. State = [[-0.33964223 -0.10098972]]. Action = [[0.05732603 0.07582965 0.         0.2917838 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
State prediction error at timestep 95 is tensor(0.0044, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 95 of -1
Current timestep = 96. State = [[-0.33977476 -0.09375861]]. Action = [[-0.02218153  0.07064603  0.         -0.57730496]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
State prediction error at timestep 96 is tensor(0.0024, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 96 of -1
Current timestep = 97. State = [[-0.3383496  -0.09133133]]. Action = [[ 0.05626769 -0.0250882   0.          0.5408845 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
State prediction error at timestep 97 is tensor(0.0026, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 97 of -1
Current timestep = 98. State = [[-0.33646536 -0.0916632 ]]. Action = [[ 0.01868503 -0.02183725  0.         -0.71648425]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
State prediction error at timestep 98 is tensor(0.0017, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 98 of -1
Current timestep = 99. State = [[-0.3359994  -0.09030539]]. Action = [[0.00511487 0.02036681 0.         0.19708955]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
State prediction error at timestep 99 is tensor(0.0043, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 99 of 0
Current timestep = 100. State = [[-0.3329895  -0.09264398]]. Action = [[ 0.0630377  -0.07574342  0.          0.6112182 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(0.0017, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 100 of 0
Current timestep = 101. State = [[-0.32820272 -0.09041592]]. Action = [[0.05940504 0.07454019 0.         0.97926426]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
State prediction error at timestep 101 is tensor(0.0008, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 101 of 0
Current timestep = 102. State = [[-0.32305813 -0.08363894]]. Action = [[ 0.06661493  0.07583756  0.         -0.2787708 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
State prediction error at timestep 102 is tensor(0.0029, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 102 of 0
Current timestep = 103. State = [[-0.31968507 -0.08185661]]. Action = [[ 0.02593525 -0.03127158  0.          0.30743098]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
State prediction error at timestep 103 is tensor(0.0026, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 103 of 0
Current timestep = 104. State = [[-0.3227355  -0.08593898]]. Action = [[-0.08915933 -0.0733037   0.          0.6588118 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
State prediction error at timestep 104 is tensor(0.0012, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 104 of 0
Current timestep = 105. State = [[-0.3219089 -0.0899047]]. Action = [[ 0.05561071 -0.03178696  0.         -0.22884381]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
State prediction error at timestep 105 is tensor(0.0024, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 105 of 0
Current timestep = 106. State = [[-0.31546003 -0.09491181]]. Action = [[ 0.08294279 -0.07640102  0.          0.9343016 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0004, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 106 of 0
Current timestep = 107. State = [[-0.3134993  -0.09480538]]. Action = [[-0.04085342  0.05986597  0.         -0.85541826]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(0.0010, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 107 of 0
Current timestep = 108. State = [[-0.3110364  -0.09316641]]. Action = [[ 0.05902144  0.00661679  0.         -0.24565953]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(0.0017, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 108 of 0
Current timestep = 109. State = [[-0.30447257 -0.09516155]]. Action = [[ 0.08849073 -0.0441862   0.         -0.5065092 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
State prediction error at timestep 109 is tensor(0.0009, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 109 of 0
Current timestep = 110. State = [[-0.29925457 -0.09690818]]. Action = [[ 0.03628241 -0.00527938  0.         -0.42174804]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
State prediction error at timestep 110 is tensor(0.0009, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 110 of 1
Current timestep = 111. State = [[-0.29742634 -0.09757391]]. Action = [[-0.00595077  0.00078088  0.          0.07519639]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
State prediction error at timestep 111 is tensor(0.0013, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 111 of 1
Current timestep = 112. State = [[-0.29563275 -0.09401564]]. Action = [[0.02048339 0.08135069 0.         0.8626785 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
State prediction error at timestep 112 is tensor(0.0004, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 112 of 1
Current timestep = 113. State = [[-0.2946586  -0.09298743]]. Action = [[-0.0068268  -0.02358243  0.         -0.5329988 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0008, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.29818162 -0.09492822]]. Action = [[-0.08594918 -0.02052941  0.          0.21986246]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
State prediction error at timestep 114 is tensor(0.0012, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 114 of 1
Current timestep = 115. State = [[-0.30319273 -0.09269623]]. Action = [[-0.0694647   0.06858616  0.         -0.6625464 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
State prediction error at timestep 115 is tensor(0.0008, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 115 of 1
Current timestep = 116. State = [[-0.30095068 -0.09464598]]. Action = [[ 0.09189246 -0.08349197  0.          0.9245734 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(0.0003, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 116 of 1
Current timestep = 117. State = [[-0.300322   -0.09496597]]. Action = [[-0.06986924  0.04877398  0.          0.1330936 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
State prediction error at timestep 117 is tensor(0.0014, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 117 of 0
Current timestep = 118. State = [[-0.3002539  -0.09401998]]. Action = [[ 0.02443395 -0.00480992  0.         -0.3916331 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
State prediction error at timestep 118 is tensor(0.0011, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 118 of 0
Current timestep = 119. State = [[-0.29886326 -0.09152164]]. Action = [[0.00779893 0.04789857 0.         0.5018847 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0008, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 119 of 0
Current timestep = 120. State = [[-0.297884   -0.08912376]]. Action = [[0.0106625  0.01161315 0.         0.5792811 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
State prediction error at timestep 120 is tensor(0.0006, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 120 of 0
Current timestep = 121. State = [[-0.30160716 -0.08642749]]. Action = [[-0.08496416  0.03845976  0.         -0.20725137]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(0.0016, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 121 of 0
Current timestep = 122. State = [[-0.30648306 -0.08741448]]. Action = [[-0.04856151 -0.04782175  0.         -0.29985034]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
State prediction error at timestep 122 is tensor(0.0015, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 122 of 0
Current timestep = 123. State = [[-0.30736998 -0.09108166]]. Action = [[ 0.01720443 -0.04970339  0.          0.30438805]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(0.0012, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 123 of 0
Current timestep = 124. State = [[-0.31122088 -0.09112551]]. Action = [[-0.08375593  0.03026371  0.         -0.07092559]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0017, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 124 of 0
Current timestep = 125. State = [[-0.31169683 -0.09149507]]. Action = [[ 0.0544026  -0.02729323  0.          0.31060517]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
State prediction error at timestep 125 is tensor(0.0013, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 125 of -1
Current timestep = 126. State = [[-0.3143472  -0.08999418]]. Action = [[-0.07763249  0.04547795  0.          0.8979795 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(0.0003, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 126 of -1
Current timestep = 127. State = [[-0.3207847  -0.09240597]]. Action = [[-0.07220902 -0.07283074  0.         -0.417961  ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(0.0013, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 127 of -1
Current timestep = 128. State = [[-0.32601443 -0.09153568]]. Action = [[-0.04258609  0.0642304   0.          0.88312364]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
State prediction error at timestep 128 is tensor(0.0005, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 128 of -1
Current timestep = 129. State = [[-0.32818177 -0.08904193]]. Action = [[0.0081283  0.01229447 0.         0.29454088]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
State prediction error at timestep 129 is tensor(0.0023, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 129 of -1
Current timestep = 130. State = [[-0.32949597 -0.08392878]]. Action = [[-0.00109445  0.0825225   0.          0.8936825 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0008, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 130 of -1
Current timestep = 131. State = [[-0.32734072 -0.07744604]]. Action = [[0.0782055  0.05879841 0.         0.67834985]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0014, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 131 of -1
Current timestep = 132. State = [[-0.33128998 -0.06960944]]. Action = [[-0.09871564  0.09688552  0.          0.7894137 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0017, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 132 of -1
Current timestep = 133. State = [[-0.3324783  -0.06650182]]. Action = [[ 0.09160829 -0.03177218  0.         -0.18349248]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0035, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 133 of -1
Current timestep = 134. State = [[-0.33034384 -0.06971651]]. Action = [[ 0.02629872 -0.07830282  0.         -0.05204403]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0031, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 134 of -1
Current timestep = 135. State = [[-0.32870328 -0.07182936]]. Action = [[ 0.03127342 -0.01270537  0.          0.1026479 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(0.0029, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 135 of -1
Current timestep = 136. State = [[-0.33205768 -0.06999394]]. Action = [[-0.0835173   0.03953498  0.         -0.91675675]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0015, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 136 of -1
Current timestep = 137. State = [[-0.33860216 -0.06603378]]. Action = [[-0.07653449  0.05306821  0.         -0.6050016 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0021, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 137 of -1
Current timestep = 138. State = [[-0.33987364 -0.06102473]]. Action = [[ 0.03886751  0.05673245  0.         -0.2852735 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(0.0036, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 138 of -1
Current timestep = 139. State = [[-0.34297293 -0.05568555]]. Action = [[-0.06635037  0.05314224  0.          0.4618944 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0036, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 139 of -1
Current timestep = 140. State = [[-0.34352437 -0.05327674]]. Action = [[ 0.04669011 -0.00611889  0.         -0.88003045]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(0.0025, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 140 of -1
Current timestep = 141. State = [[-0.34481192 -0.05312083]]. Action = [[-0.03759444 -0.01562236  0.         -0.13379872]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(0.0045, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 141 of -1
Current timestep = 142. State = [[-0.34288427 -0.0500745 ]]. Action = [[ 0.07169325  0.05210995  0.         -0.5472567 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0035, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 142 of -1
Current timestep = 143. State = [[-0.34384593 -0.04961199]]. Action = [[-0.04978251 -0.04120649  0.          0.9500675 ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
State prediction error at timestep 143 is tensor(0.0021, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 143 of -1
Current timestep = 144. State = [[-0.34431452 -0.05460453]]. Action = [[ 0.02055343 -0.08673393  0.          0.7486868 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0023, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 144 of -1
Current timestep = 145. State = [[-0.3467608 -0.0619523]]. Action = [[-0.0647448  -0.09297115  0.          0.01438403]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0039, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.35040557 -0.07018836]]. Action = [[-0.04671069 -0.09222371  0.         -0.16274047]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(0.0034, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 146 of -1
Current timestep = 147. State = [[-0.35084495 -0.07056002]]. Action = [[ 0.01887115  0.08035434  0.         -0.2624923 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0034, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 147 of -1
Current timestep = 148. State = [[-0.34815112 -0.06420945]]. Action = [[0.04668181 0.09266447 0.         0.01457393]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0039, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.349562   -0.05753547]]. Action = [[-0.04688416  0.07305533  0.         -0.5135097 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(0.0029, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 149 of 0
Current timestep = 150. State = [[-0.35556662 -0.05464541]]. Action = [[-0.08296059  0.00737866  0.          0.7211828 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0025, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 150 of 0
Current timestep = 151. State = [[-0.3632372  -0.05851167]]. Action = [[-0.09134612 -0.08391027  0.         -0.08604568]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, True, False]
State prediction error at timestep 151 is tensor(0.0045, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 151 of 0
Current timestep = 152. State = [[-0.36419678 -0.06109624]]. Action = [[ 6.0191400e-02  5.8543682e-04  0.0000000e+00 -9.6244717e-01]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(0.0018, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 152 of 0
Current timestep = 153. State = [[-0.3641673 -0.0639019]]. Action = [[-0.01502562 -0.05594554  0.          0.01299942]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(0.0042, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 153 of 1
Current timestep = 154. State = [[-0.360826   -0.06974173]]. Action = [[ 0.08860145 -0.08137304  0.          0.50528336]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0024, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 154 of 1
Current timestep = 155. State = [[-0.35452047 -0.06821475]]. Action = [[0.08992396 0.08714022 0.         0.39312553]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0029, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 155 of 1
Current timestep = 156. State = [[-0.3477195  -0.06367396]]. Action = [[ 0.0903535   0.03300207  0.         -0.7041444 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0019, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 156 of 1
Current timestep = 157. State = [[-0.34818926 -0.06237032]]. Action = [[-6.5976739e-02 -3.3856928e-04  0.0000000e+00  9.0554595e-01]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(0.0012, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 157 of 1
Current timestep = 158. State = [[-0.35458827 -0.0579385 ]]. Action = [[-0.08692121  0.09067341  0.         -0.32047403]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0033, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 158 of 0
Current timestep = 159. State = [[-0.35771742 -0.05984129]]. Action = [[ 0.00576949 -0.09529742  0.         -0.11741412]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0037, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 159 of 0
Current timestep = 160. State = [[-0.36143163 -0.06337505]]. Action = [[-0.07113503 -0.0128359   0.          0.6224133 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0022, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 160 of 0
Current timestep = 161. State = [[-0.36390907 -0.06005477]]. Action = [[-0.00285707  0.08156051  0.          0.48955405]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0033, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 161 of 1
Current timestep = 162. State = [[-0.36061233 -0.05372891]]. Action = [[0.08669863 0.06521256 0.         0.27684474]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0040, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.35902858 -0.05211241]]. Action = [[-0.00282224 -0.03124424  0.          0.6592709 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0023, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 163 of -1
Current timestep = 164. State = [[-0.3618204  -0.05709125]]. Action = [[-0.05039235 -0.09293339  0.          0.72515655]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0020, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 164 of 0
Current timestep = 165. State = [[-0.3652826  -0.06085214]]. Action = [[-0.04006151 -0.01519392  0.         -0.6009569 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0022, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.36407858 -0.06601188]]. Action = [[ 0.051544   -0.08402209  0.         -0.01210278]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0032, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 166 of -1
Current timestep = 167. State = [[-0.36563718 -0.06505343]]. Action = [[-0.07024594  0.08333761  0.         -0.5979093 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0021, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 167 of 0
Current timestep = 168. State = [[-0.37148187 -0.06702006]]. Action = [[-0.07590076 -0.08042757  0.          0.32451594]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0031, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 168 of -1
Current timestep = 169. State = [[-0.3745018  -0.06830028]]. Action = [[-0.00186737  0.03350597  0.         -0.89892364]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0015, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 169 of 0
Current timestep = 170. State = [[-0.37613612 -0.0676586 ]]. Action = [[-0.01286002  0.00259416  0.         -0.85719454]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(0.0016, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 170 of -1
Current timestep = 171. State = [[-0.38036156 -0.07268251]]. Action = [[-0.06101495 -0.09562536  0.          0.8109982 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(0.0019, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 171 of 1
Current timestep = 172. State = [[-0.38306054 -0.07629158]]. Action = [[0.         0.         0.         0.47648907]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0031, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 172 of 1
Current timestep = 173. State = [[-0.3807492  -0.07846367]]. Action = [[ 0.06801138 -0.03097645  0.          0.10027742]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(0.0036, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 173 of -1
Current timestep = 174. State = [[-0.37703863 -0.0780682 ]]. Action = [[0.05120049 0.02843565 0.         0.9101021 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0012, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 174 of 0
Current timestep = 175. State = [[-0.37331152 -0.07573605]]. Action = [[0.05605986 0.0272652  0.         0.8359958 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0012, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 175 of -1
Current timestep = 176. State = [[-0.36833858 -0.07047604]]. Action = [[ 0.07885195  0.0777267   0.         -0.7460258 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0016, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 176 of 1
Current timestep = 177. State = [[-0.37102237 -0.06654219]]. Action = [[-0.09544706  0.0226173   0.          0.18920386]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0032, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 177 of -1
Current timestep = 178. State = [[-0.3762095  -0.06318828]]. Action = [[-0.02954201  0.04699645  0.         -0.68954355]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0019, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 178 of 1
Current timestep = 179. State = [[-0.38087484 -0.056475  ]]. Action = [[-0.04679541  0.0947767   0.          0.22640336]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0041, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 179 of 0
Current timestep = 180. State = [[-0.38373166 -0.05261432]]. Action = [[0.         0.         0.         0.53774524]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0032, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 180 of 0
Current timestep = 181. State = [[-0.38515392 -0.05129538]]. Action = [[0.         0.         0.         0.23093021]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0044, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 181 of 0
Current timestep = 182. State = [[-0.3862952  -0.05019179]]. Action = [[ 0.        0.        0.       -0.646108]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
State prediction error at timestep 182 is tensor(0.0029, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 182 of -1
Current timestep = 183. State = [[-0.3872392  -0.04919043]]. Action = [[0.         0.         0.         0.15861022]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0047, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.38795346 -0.04831331]]. Action = [[ 0.        0.        0.       -0.291314]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0045, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 184 of -1
Current timestep = 185. State = [[-0.3884752  -0.04757116]]. Action = [[0.        0.        0.        0.3056004]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(0.0045, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 185 of -1
Current timestep = 186. State = [[-0.38884494 -0.04696895]]. Action = [[0.        0.        0.        0.0194416]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(0.0049, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 186 of 0
Current timestep = 187. State = [[-0.38908628 -0.04650631]]. Action = [[0.         0.         0.         0.26369452]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0046, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 187 of 0
Current timestep = 188. State = [[-0.3892341  -0.04617351]]. Action = [[0.         0.         0.         0.46563077]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0038, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 188 of 0
Current timestep = 189. State = [[-0.389338   -0.04595357]]. Action = [[ 0.          0.          0.         -0.52062166]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0036, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 189 of 1
Current timestep = 190. State = [[-0.38941547 -0.04579538]]. Action = [[ 0.         0.         0.        -0.7342685]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0027, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 190 of 1
Current timestep = 191. State = [[-0.38947758 -0.04567752]]. Action = [[0.        0.        0.        0.4369576]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(0.0037, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 191 of 1
Current timestep = 192. State = [[-0.38952404 -0.04559266]]. Action = [[ 0.         0.         0.        -0.6706212]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(0.0028, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 192 of 1
Current timestep = 193. State = [[-0.38954538 -0.04553426]]. Action = [[0.         0.         0.         0.34899688]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
State prediction error at timestep 193 is tensor(0.0039, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 193 of 0
Current timestep = 194. State = [[-0.38955462 -0.04549191]]. Action = [[ 0.         0.         0.        -0.5584884]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
State prediction error at timestep 194 is tensor(0.0031, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 194 of 0
Current timestep = 195. State = [[-0.38955873 -0.04545852]]. Action = [[0.        0.        0.        0.8499336]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0022, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 195 of 0
Current timestep = 196. State = [[-0.38956138 -0.04542912]]. Action = [[0.         0.         0.         0.71903753]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(0.0025, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 196 of -1
Current timestep = 197. State = [[-0.38956463 -0.04540032]]. Action = [[ 0.         0.         0.        -0.7060147]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0026, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 197 of -1
Current timestep = 198. State = [[-0.3895687  -0.04537067]]. Action = [[ 0.          0.          0.         -0.85028934]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(0.0022, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 198 of -1
Current timestep = 199. State = [[-0.3895714  -0.04534202]]. Action = [[0.         0.         0.         0.15653312]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0041, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 199 of -1
Current timestep = 200. State = [[-0.389573   -0.04531556]]. Action = [[0.         0.         0.         0.48286426]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0032, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 200 of -1
Current timestep = 201. State = [[-0.38957307 -0.04529344]]. Action = [[0.        0.        0.        0.2427795]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
State prediction error at timestep 201 is tensor(0.0038, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 201 of -1
Current timestep = 202. State = [[-0.3895719  -0.04527475]]. Action = [[ 0.          0.          0.         -0.15772128]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(0.0038, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 202 of 0
Current timestep = 203. State = [[-0.38956997 -0.04525851]]. Action = [[ 0.          0.          0.         -0.59045196]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0026, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 203 of 0
Current timestep = 204. State = [[-0.38956764 -0.04524427]]. Action = [[0.         0.         0.         0.79353154]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(0.0018, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 204 of 0
Current timestep = 205. State = [[-0.3895656  -0.04523159]]. Action = [[ 0.         0.         0.        -0.9564872]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0016, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 205 of 1
Current timestep = 206. State = [[-0.38956395 -0.04522025]]. Action = [[0.        0.        0.        0.9818708]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0014, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 206 of 1
Current timestep = 207. State = [[-0.3895627  -0.04521011]]. Action = [[0.         0.         0.         0.73170483]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0018, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 207 of 1
Current timestep = 208. State = [[-0.38956174 -0.04520104]]. Action = [[0.        0.        0.        0.4435954]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0027, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 208 of -1
Current timestep = 209. State = [[-0.3895611  -0.04519293]]. Action = [[0.         0.         0.         0.83466864]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0016, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 209 of 0
Current timestep = 210. State = [[-0.38956067 -0.04518567]]. Action = [[ 0.          0.          0.         -0.37628376]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(0.0031, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 210 of 0
Current timestep = 211. State = [[-0.3895605  -0.04517917]]. Action = [[ 0.          0.          0.         -0.39259344]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(0.0031, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 211 of 0
Current timestep = 212. State = [[-0.38956052 -0.04517335]]. Action = [[ 0.         0.         0.        -0.7756133]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(0.0020, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[-0.3895607  -0.04516815]]. Action = [[0.         0.         0.         0.74268603]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0018, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 213 of 0
Current timestep = 214. State = [[-0.38956106 -0.04516348]]. Action = [[ 0.          0.          0.         -0.92190415]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0017, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 214 of 1
Current timestep = 215. State = [[-0.38956153 -0.0451593 ]]. Action = [[ 0.         0.         0.        -0.8142111]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(0.0018, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 215 of 1
Current timestep = 216. State = [[-0.38956216 -0.04515556]]. Action = [[ 0.          0.          0.         -0.29854155]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(0.0029, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 216 of 1
Current timestep = 217. State = [[-0.38956288 -0.0451522 ]]. Action = [[0.         0.         0.         0.37857318]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(0.0026, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 217 of 1
Current timestep = 218. State = [[-0.38956368 -0.04514919]]. Action = [[0.        0.        0.        0.1365565]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0030, grad_fn=<MseLossBackward>)
Human Feedback received at timestep 218 of 0
