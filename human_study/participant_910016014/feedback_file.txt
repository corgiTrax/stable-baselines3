Current timestep = 0. State = [[-0.25684452  0.0064347 ]]. Action = [[ 0.02368204 -0.03158806  0.04000955  0.6916206 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0426, grad_fn=<MseLossBackward0>)
Current timestep = 1. State = [[-0.2563732   0.00543398]]. Action = [[0.07372709 0.08091127 0.06755883 0.5634155 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0388, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.2550453   0.00607737]]. Action = [[ 0.09737053 -0.09297557 -0.07288113  0.7404978 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0304, grad_fn=<MseLossBackward0>)
Current timestep = 3. State = [[-0.2527731  0.0048584]]. Action = [[-0.04975199 -0.08747798  0.00550811 -0.94290835]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.2524203   0.00214955]]. Action = [[-0.03770322  0.00626417 -0.02686705  0.9408605 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0200, grad_fn=<MseLossBackward0>)
Current timestep = 5. State = [[-0.25252056  0.00082599]]. Action = [[ 0.0264079  -0.01754155 -0.08055081 -0.78541875]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 6. State = [[-0.25263497 -0.00049786]]. Action = [[-0.08958    -0.00627899  0.08312207  0.7870858 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0129, grad_fn=<MseLossBackward0>)
Current timestep = 7. State = [[-0.25281012 -0.00195093]]. Action = [[-0.04437431 -0.05474773  0.03463931  0.82439756]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0098, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.25309762 -0.00398525]]. Action = [[-0.01612686  0.03258201  0.08612583 -0.36151648]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 9. State = [[-0.2532503  -0.00452941]]. Action = [[-0.08463106  0.04812131  0.07206846 -0.9843373 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.25398463 -0.00384359]]. Action = [[ 0.03852875  0.01973824  0.02015036 -0.12029588]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.2541206  -0.00363022]]. Action = [[ 0.08500952 -0.09207744  0.08364307  0.748476  ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Current timestep = 12. State = [[-0.25423998 -0.00500547]]. Action = [[-0.09679315  0.05888116 -0.00307482  0.8256942 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.25461    -0.00491819]]. Action = [[-0.07298912  0.06943428  0.05571485 -0.41490388]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.25613493 -0.00272441]]. Action = [[ 0.0716477   0.08837617 -0.05515114  0.8674722 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 15. State = [[-2.5707766e-01  2.2795943e-04]]. Action = [[ 0.0535768  -0.09649874  0.07312288 -0.3943286 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 16. State = [[-0.25710666 -0.00029127]]. Action = [[-0.04066142 -0.08102992  0.0700909  -0.47141904]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 17. State = [[-0.25728303 -0.00269021]]. Action = [[ 0.07058901 -0.09491611 -0.00953929  0.17939961]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Current timestep = 18. State = [[-0.25718832 -0.00563428]]. Action = [[ 0.09144909  0.07856334  0.07934453 -0.70691156]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 19. State = [[-0.25651538 -0.00569251]]. Action = [[ 0.07053     0.01781343  0.09338956 -0.07401174]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.25495186 -0.00560285]]. Action = [[ 0.00106911 -0.00629579  0.04827236 -0.11622131]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Current timestep = 21. State = [[-0.25390455 -0.00571609]]. Action = [[ 0.04093339 -0.06368546 -0.07072036 -0.17125499]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 22. State = [[-0.25245827 -0.0070358 ]]. Action = [[ 0.02552957 -0.05726073  0.05538089  0.8009608 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 23. State = [[-0.2506719  -0.00878877]]. Action = [[0.08808479 0.00759892 0.07233057 0.6050503 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 24. State = [[-0.24727479 -0.00978127]]. Action = [[ 0.05888828  0.0220466   0.04530472 -0.98616046]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 25. State = [[-0.24280035 -0.01001376]]. Action = [[-0.00356411 -0.07182424 -0.0495479  -0.17610985]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 26. State = [[-0.23964517 -0.01182458]]. Action = [[-3.5955757e-04 -7.5967625e-02  7.3605627e-03 -3.8232166e-01]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.23730242 -0.01558847]]. Action = [[-0.08044159 -0.09667316  0.03134448  0.7762878 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.2370166  -0.02042618]]. Action = [[ 0.0142375  -0.08352172  0.0148647   0.6608591 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 29. State = [[-0.23689204 -0.02594862]]. Action = [[-0.01755945 -0.05940769  0.0527937  -0.48497146]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 30. State = [[-0.23703259 -0.03057864]]. Action = [[ 0.04317857 -0.02766459 -0.07992885  0.3924024 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 31. State = [[-0.23703629 -0.03444476]]. Action = [[-0.03815528 -0.03884655 -0.02426586  0.65801084]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 31 of -1
Current timestep = 32. State = [[-0.23726939 -0.03774974]]. Action = [[-0.09111487  0.08218194  0.00126481  0.7339995 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 33. State = [[-0.23747337 -0.03862222]]. Action = [[-0.00272809  0.00779043  0.04854294  0.6480886 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 33 of 1
Current timestep = 34. State = [[-0.23753986 -0.03873631]]. Action = [[ 0.09157007 -0.02538396 -0.0835847  -0.96899426]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(7.4755e-05, grad_fn=<MseLossBackward0>)
Current timestep = 35. State = [[-0.23758833 -0.03932966]]. Action = [[-0.04808431 -0.08846297 -0.0415693  -0.17534375]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.23796308 -0.04232089]]. Action = [[ 0.0596968  -0.04148551 -0.01789393 -0.4060768 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 36 of 1
Current timestep = 37. State = [[-0.23798314 -0.0444833 ]]. Action = [[ 0.07794461 -0.00919861  0.04911012  0.4553492 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 37 of 1
Current timestep = 38. State = [[-0.23641208 -0.04633094]]. Action = [[ 0.08627702 -0.07516858 -0.01840575 -0.8679474 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 39. State = [[-0.23300831 -0.04928191]]. Action = [[ 0.04376142 -0.03385548  0.06208353 -0.3800292 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 39 of 1
Current timestep = 40. State = [[-0.2292634  -0.05221765]]. Action = [[-0.0580999   0.07131732  0.04074175 -0.6974145 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 41. State = [[-0.22890924 -0.05230904]]. Action = [[ 0.01957117  0.00641217 -0.00797312 -0.5624331 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 42. State = [[-0.22907102 -0.05226894]]. Action = [[0.00253178 0.05338109 0.02738933 0.4845363 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 43. State = [[-0.22902621 -0.05176754]]. Action = [[-0.0463446   0.08482645  0.03201141 -0.22952038]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
State prediction error at timestep 43 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.22935349 -0.04940125]]. Action = [[ 0.03219081  0.08545627  0.05265596 -0.9628025 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 44 of 1
Current timestep = 45. State = [[-0.22957428 -0.04623104]]. Action = [[-0.0225781  -0.02672642  0.05756877 -0.42824757]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 45 of 1
Current timestep = 46. State = [[-0.22970647 -0.04507129]]. Action = [[ 0.07308126  0.06977309 -0.09093001 -0.6254249 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 47. State = [[-0.2291943  -0.04254365]]. Action = [[-0.08276216 -0.07506286  0.08407638  0.40943694]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 48. State = [[-0.22909144 -0.04271017]]. Action = [[-0.05099618 -0.00615175 -0.0478387  -0.94244945]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 48 of 1
Current timestep = 49. State = [[-0.2291437  -0.04285642]]. Action = [[-0.0776914  -0.06917848  0.02142533  0.7089217 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 50. State = [[-0.22957125 -0.04401907]]. Action = [[-0.04718298  0.03970618  0.09873927 -0.7823677 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 51. State = [[-0.23075514 -0.04402874]]. Action = [[ 0.0464995  -0.08906421 -0.01477522  0.40511298]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 51 of 1
Current timestep = 52. State = [[-0.23081675 -0.0459163 ]]. Action = [[-0.06065894 -0.08942977 -0.01070233 -0.93673134]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 53. State = [[-0.23156787 -0.04931308]]. Action = [[ 0.09516779 -0.03718332 -0.08420265  0.16100049]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
State prediction error at timestep 53 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 54. State = [[-0.23174879 -0.05199682]]. Action = [[ 0.0843695  -0.07611795 -0.06115966 -0.6377149 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 54 of 1
Current timestep = 55. State = [[-0.23118086 -0.05584377]]. Action = [[-0.02291851  0.00368758 -0.04583663 -0.28496504]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
State prediction error at timestep 55 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 56. State = [[-0.23104501 -0.05799842]]. Action = [[-0.04720552 -0.06471954  0.04962227  0.9349146 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of 1
Current timestep = 57. State = [[-0.2313362  -0.06085849]]. Action = [[-0.02672839  0.06771857  0.02768653  0.1240865 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
State prediction error at timestep 57 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 57 of 1
Current timestep = 58. State = [[-0.23142204 -0.06137537]]. Action = [[ 0.01325951 -0.04095054 -0.03155052  0.29739237]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 58 of 1
Current timestep = 59. State = [[-0.23164143 -0.06228659]]. Action = [[-0.01172426  0.06721833 -0.09583673  0.18140364]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
State prediction error at timestep 59 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 59 of 1
Current timestep = 60. State = [[-0.23168054 -0.06209335]]. Action = [[ 0.0147803   0.03700051 -0.08491783 -0.45961934]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
State prediction error at timestep 60 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 61. State = [[-0.23169996 -0.06156208]]. Action = [[ 0.07031176 -0.08167462  0.05517114 -0.9130548 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
State prediction error at timestep 61 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 62. State = [[-0.23154625 -0.06202196]]. Action = [[ 0.08263592  0.02972975 -0.03081655  0.7839956 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
State prediction error at timestep 62 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 62 of 1
Current timestep = 63. State = [[-0.23047695 -0.06206816]]. Action = [[ 3.0349195e-04 -2.6692897e-03  9.0552293e-02 -5.1661462e-01]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
State prediction error at timestep 63 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 64. State = [[-0.23009512 -0.06198744]]. Action = [[-0.02174591  0.00955226  0.06184907 -0.82691085]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
State prediction error at timestep 64 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 64 of 1
Current timestep = 65. State = [[-0.23010729 -0.06194096]]. Action = [[-0.07810257  0.03243817 -0.05533085 -0.16131526]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
State prediction error at timestep 65 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 66. State = [[-0.23025857 -0.06189864]]. Action = [[ 0.01636297 -0.01233651  0.04911233 -0.09181666]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 66 of 1
Current timestep = 67. State = [[-0.23030847 -0.06177505]]. Action = [[-0.00700933  0.06553296 -0.05320513 -0.45532095]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 68. State = [[-0.23059113 -0.06073007]]. Action = [[-0.08670973 -0.09438849  0.06063212  0.02165365]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
State prediction error at timestep 68 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 69. State = [[-0.23065478 -0.06114041]]. Action = [[ 0.05823655 -0.03222515  0.0324818  -0.75763965]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
State prediction error at timestep 69 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of 1
Current timestep = 70. State = [[-0.23054408 -0.06188738]]. Action = [[-0.04629425  0.06972212 -0.09189488  0.9215472 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of 1
Current timestep = 71. State = [[-0.23059955 -0.06178271]]. Action = [[-0.07057427 -0.07216007 -0.03279528 -0.68519044]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
State prediction error at timestep 71 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 72. State = [[-0.23143823 -0.06249133]]. Action = [[-0.09675477 -0.03565735  0.0822809   0.61300206]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
State prediction error at timestep 72 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 72 of 1
Current timestep = 73. State = [[-0.23358741 -0.06380876]]. Action = [[-0.0519658   0.07967544  0.09091193 -0.8379722 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
State prediction error at timestep 73 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of 1
Current timestep = 74. State = [[-0.23535584 -0.06342488]]. Action = [[-0.04063268 -0.0289557   0.0691594  -0.8217074 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
State prediction error at timestep 74 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 74 of 1
Current timestep = 75. State = [[-0.23699033 -0.06354375]]. Action = [[ 0.09186079  0.00300489 -0.08409944 -0.4550333 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 76. State = [[-0.23704273 -0.06367414]]. Action = [[ 0.05015952 -0.07201422  0.09563444  0.89318   ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
State prediction error at timestep 76 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 77. State = [[-0.2370144  -0.06508677]]. Action = [[0.01931413 0.01705798 0.04800432 0.8612355 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
State prediction error at timestep 77 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 78. State = [[-0.23695993 -0.06516485]]. Action = [[-0.0136621   0.09254866 -0.09063172  0.60538554]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
State prediction error at timestep 78 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 78 of 1
Current timestep = 79. State = [[-0.23711753 -0.06429312]]. Action = [[-0.01686791  0.04906126 -0.07595475 -0.37084365]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of 1
Current timestep = 80. State = [[-0.23733066 -0.06264877]]. Action = [[ 0.07141901  0.08853979  0.01534101 -0.61567783]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
State prediction error at timestep 80 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 80 of 1
Current timestep = 81. State = [[-0.23775285 -0.05939298]]. Action = [[-0.0286139   0.09282491 -0.03993621  0.25791812]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
State prediction error at timestep 81 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 82. State = [[-0.23859054 -0.05474672]]. Action = [[-0.08093625  0.0677459  -0.09896099 -0.53685987]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
State prediction error at timestep 82 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 83. State = [[-0.23947176 -0.0499281 ]]. Action = [[ 0.09909309  0.07333701  0.06930924 -0.28373033]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
State prediction error at timestep 83 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 83 of 1
Current timestep = 84. State = [[-0.24015948 -0.04478286]]. Action = [[ 0.08101139  0.05019147 -0.02173087 -0.8297365 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
State prediction error at timestep 84 is tensor(2.1158e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of 1
Current timestep = 85. State = [[-0.23922107 -0.04079382]]. Action = [[ 0.03263066  0.06205123  0.06991326 -0.7278631 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(6.9752e-05, grad_fn=<MseLossBackward0>)
Current timestep = 86. State = [[-0.23844287 -0.03699698]]. Action = [[-0.04700736 -0.03725195 -0.02906293  0.4807334 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
State prediction error at timestep 86 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 87. State = [[-0.23872998 -0.03544218]]. Action = [[ 0.04171959  0.06973486  0.01107413 -0.71254915]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
State prediction error at timestep 87 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 87 of 1
Current timestep = 88. State = [[-0.23872198 -0.03296502]]. Action = [[ 0.03622947 -0.04381115  0.09422236 -0.33573627]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.23778456 -0.03252261]]. Action = [[-0.02227118  0.01429655  0.09129895  0.78966784]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 90. State = [[-0.23769793 -0.03208858]]. Action = [[-0.02960798 -0.05607339  0.04214106  0.05469906]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 91. State = [[-0.23766933 -0.03208959]]. Action = [[-0.09775573  0.06320205  0.09191301  0.83704424]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
State prediction error at timestep 91 is tensor(9.8416e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 91 of 1
Current timestep = 92. State = [[-0.23806676 -0.03090707]]. Action = [[ 0.00246187  0.06227443 -0.03648274 -0.25844836]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
State prediction error at timestep 92 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 92 of 1
Current timestep = 93. State = [[-0.23861088 -0.02879143]]. Action = [[-0.04672338 -0.09249803  0.07550531  0.07269979]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
State prediction error at timestep 93 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 93 of 1
Current timestep = 94. State = [[-0.23872988 -0.02882192]]. Action = [[-0.05356414  0.09377874  0.09749522 -0.95137346]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 95. State = [[-0.23959814 -0.02680287]]. Action = [[ 0.06044609  0.07789906 -0.08909079  0.29057705]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
State prediction error at timestep 95 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 96. State = [[-0.24048802 -0.02362955]]. Action = [[-0.01979245  0.07304893  0.04472763 -0.59673923]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
State prediction error at timestep 96 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 96 of 1
Current timestep = 97. State = [[-0.24148285 -0.01954826]]. Action = [[ 0.05918833 -0.02289081  0.04338234  0.54764247]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
State prediction error at timestep 97 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 97 of 1
Current timestep = 98. State = [[-0.24175437 -0.01802253]]. Action = [[ 0.02175013 -0.01913406 -0.02645723 -0.73313713]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
State prediction error at timestep 98 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 99. State = [[-0.24166992 -0.01781357]]. Action = [[0.00785504 0.02443604 0.08005495 0.19545281]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
State prediction error at timestep 99 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 100. State = [[-0.24169533 -0.01742694]]. Action = [[ 0.06549153 -0.07499493  0.08248337  0.6196642 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 100 of 1
Current timestep = 101. State = [[-0.24079113 -0.01747401]]. Action = [[0.06187166 0.07745191 0.00239439 0.9815724 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
State prediction error at timestep 101 is tensor(4.9826e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 101 of 1
Current timestep = 102. State = [[-0.23921278 -0.01692711]]. Action = [[ 0.06876076  0.07877903 -0.06000049 -0.2929657 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
State prediction error at timestep 102 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 102 of 1
Current timestep = 103. State = [[-0.23658812 -0.01444681]]. Action = [[ 0.02842811 -0.02710838 -0.01578438  0.30858898]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
State prediction error at timestep 103 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 104. State = [[-0.23369436 -0.01388756]]. Action = [[-0.08929282 -0.07165656  0.01105897  0.6662493 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
State prediction error at timestep 104 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 104 of 1
Current timestep = 105. State = [[-0.23271413 -0.01393246]]. Action = [[ 0.05786023 -0.0271551   0.02819861 -0.24258679]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
State prediction error at timestep 105 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 105 of 1
Current timestep = 106. State = [[-0.23081936 -0.01400971]]. Action = [[ 0.08427057 -0.07482626  0.09219024  0.93865883]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(4.9803e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 106 of 1
Current timestep = 107. State = [[-0.22781907 -0.015574  ]]. Action = [[-0.03981909  0.0648488   0.05508424 -0.8667679 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 108. State = [[-0.22711131 -0.01553534]]. Action = [[ 0.06119192  0.01334037 -0.08038662 -0.26193428]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 109. State = [[-0.22556676 -0.01555037]]. Action = [[ 0.08946914 -0.03954528 -0.00256048 -0.5252412 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
State prediction error at timestep 109 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 109 of 1
Current timestep = 110. State = [[-0.22228405 -0.01534875]]. Action = [[ 0.03871641  0.00155995 -0.07138375 -0.4405794 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
State prediction error at timestep 110 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 110 of 1
Current timestep = 111. State = [[-0.21831782 -0.0149405 ]]. Action = [[-0.00385089  0.00786756  0.08116663  0.0641309 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
State prediction error at timestep 111 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 111 of 1
Current timestep = 112. State = [[-0.21537676 -0.01494696]]. Action = [[ 0.02298149  0.08448019 -0.03791552  0.8667376 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
State prediction error at timestep 112 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 113. State = [[-0.21306592 -0.01389929]]. Action = [[-0.00459307 -0.01738811 -0.09103686 -0.5518425 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.21229334 -0.01404104]]. Action = [[-0.08581232 -0.0142412  -0.03226553  0.21074891]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
State prediction error at timestep 114 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 114 of 1
Current timestep = 115. State = [[-0.21243498 -0.01393504]]. Action = [[-0.06879634  0.07317156 -0.06603561 -0.6789394 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
State prediction error at timestep 115 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 115 of 1
Current timestep = 116. State = [[-0.21296373 -0.01217789]]. Action = [[ 0.09259807 -0.08245243 -0.00626804  0.926872  ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 117. State = [[-0.21293241 -0.01229543]]. Action = [[-0.06905882  0.05473381 -0.04676172  0.12154031]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
State prediction error at timestep 117 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 118. State = [[-0.21294428 -0.01192628]]. Action = [[ 0.02730494  0.00146925  0.03822137 -0.40948284]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
State prediction error at timestep 118 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 118 of 1
Current timestep = 119. State = [[-0.21304421 -0.011747  ]]. Action = [[ 0.0107461   0.05360708 -0.02643893  0.49772322]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.21340095 -0.01047533]]. Action = [[0.01371695 0.01778893 0.09785723 0.57626295]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
State prediction error at timestep 120 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 121. State = [[-0.213935   -0.00859463]]. Action = [[-0.08442739  0.04416538 -0.03247979 -0.22382116]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 121 of 1
Current timestep = 122. State = [[-0.21456286 -0.00665075]]. Action = [[-0.0464269  -0.04468439  0.05766804 -0.31659985]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
State prediction error at timestep 122 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 123. State = [[-0.21469404 -0.00650805]]. Action = [[ 0.02058417 -0.04694877 -0.0199101   0.29475725]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of 1
Current timestep = 124. State = [[-0.21474329 -0.00645792]]. Action = [[-0.08302644  0.03548459 -0.07637791 -0.08611035]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.21567003 -0.0062431 ]]. Action = [[ 0.05720585 -0.02372193 -0.07695505  0.3006034 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
State prediction error at timestep 125 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.21578605 -0.00615173]]. Action = [[-0.07647918  0.05003493 -0.01138592  0.8979378 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 127. State = [[-0.21667325 -0.00554561]]. Action = [[-0.07069457 -0.07214032  0.06992627 -0.43390858]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 128. State = [[-0.21805388 -0.0065113 ]]. Action = [[-0.03963662  0.06775428 -0.04245303  0.882164  ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
State prediction error at timestep 128 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 128 of 1
Current timestep = 129. State = [[-0.21953365 -0.00523557]]. Action = [[0.01216239 0.01631868 0.02365067 0.2813095 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
State prediction error at timestep 129 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 129 of 1
Current timestep = 130. State = [[-0.22041446 -0.00375826]]. Action = [[ 0.00297574  0.0846576  -0.06210685  0.89206123]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 131. State = [[-0.22140314 -0.00068905]]. Action = [[ 0.08002352  0.06224669 -0.04973432  0.67069066]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 132. State = [[-0.22245432  0.00283582]]. Action = [[-0.09864735  0.09740598 -0.04442648  0.78366315]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 132 of 1
Current timestep = 133. State = [[-0.22408432  0.0079071 ]]. Action = [[ 0.09239725 -0.02963339  0.07724714 -0.20516992]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 133 of 1
Current timestep = 134. State = [[-0.22458342  0.00939831]]. Action = [[ 0.03036671 -0.07817635 -0.06459707 -0.07576722]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 135. State = [[-0.2244213   0.00921806]]. Action = [[ 0.03524924 -0.00984965  0.08206717  0.07759011]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 136. State = [[-0.22433245  0.0087634 ]]. Action = [[-0.08239926  0.04315104 -0.0246402  -0.92177665]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.22462334  0.00928476]]. Action = [[-0.07492906  0.0563741   0.04849664 -0.62430185]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 137 of 1
Current timestep = 138. State = [[-0.22528224  0.01127144]]. Action = [[ 0.04276585  0.05984116  0.03399736 -0.3142047 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 139. State = [[-0.22621374  0.01387838]]. Action = [[-0.06404612  0.05627405  0.04092575  0.43613803]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 140. State = [[-0.22729884  0.01718225]]. Action = [[ 0.05031026 -0.00330011 -0.04734503 -0.8878335 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 140 of -1
Current timestep = 141. State = [[-0.22793812  0.01900984]]. Action = [[-0.03390159 -0.01304109  0.07090297 -0.16967267]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 142. State = [[-0.22829305  0.01993411]]. Action = [[ 0.07400168  0.05516947 -0.09378072 -0.57413185]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 142 of -1
Current timestep = 143. State = [[-0.22870441  0.0212412 ]]. Action = [[-0.04665373 -0.03936772 -0.09238143  0.94606674]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
State prediction error at timestep 143 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 144. State = [[-0.22870107  0.02124305]]. Action = [[ 0.0247523  -0.08659073 -0.00645754  0.72959745]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 144 of -1
Current timestep = 145. State = [[-0.22868335  0.01976633]]. Action = [[-0.06250323 -0.09295718 -0.06450981 -0.03067493]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.22874498  0.01624382]]. Action = [[-0.04370147 -0.09217821 -0.02425228 -0.20864463]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 147. State = [[-0.22902523  0.01189697]]. Action = [[ 0.02284761  0.08224354  0.03422459 -0.3081391 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 147 of -1
Current timestep = 148. State = [[-0.22948997  0.01075883]]. Action = [[ 0.04993863  0.09353354 -0.00732546 -0.03587198]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.22985771  0.01210081]]. Action = [[-0.04421098  0.07540528  0.02887357 -0.55164117]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 150. State = [[-0.23085885  0.01516951]]. Action = [[-0.08206896  0.01127833 -0.02889543  0.69524944]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 151. State = [[-0.23183854  0.01771902]]. Action = [[-0.09094667 -0.08328912  0.09077565 -0.14078331]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, True, False]
State prediction error at timestep 151 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 151 of -1
Current timestep = 152. State = [[-0.23343083  0.01715274]]. Action = [[ 0.06243884  0.00459908 -0.0482461  -0.9667611 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 153. State = [[-0.23368737  0.01682823]]. Action = [[-0.0123314  -0.05370359 -0.01629023 -0.04543012]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 154. State = [[-0.23380132  0.0154014 ]]. Action = [[ 0.08933947 -0.08046607  0.04333255  0.460685  ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of -1
Current timestep = 155. State = [[-0.23360096  0.0130159 ]]. Action = [[0.09053386 0.0886598  0.07552443 0.3412838 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 156. State = [[-0.23324436  0.01326436]]. Action = [[ 0.09086401  0.03770272  0.09477577 -0.736085  ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 157. State = [[-0.23211673  0.01394456]]. Action = [[-0.06526735  0.00478071  0.01422209  0.8945849 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 157 of -1
Current timestep = 158. State = [[-0.23229593  0.01436947]]. Action = [[-0.08674227  0.09186662 -0.01636646 -0.3777064 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 159. State = [[-0.2332821   0.01679492]]. Action = [[ 0.0068652  -0.09501439 -0.08444507 -0.18073326]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 160. State = [[-0.23325813  0.01640206]]. Action = [[-0.07096152 -0.00756685  0.01135652  0.5828016 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 161. State = [[-0.2333652   0.01634019]]. Action = [[-0.00238156  0.0838254  -0.03927025  0.44015694]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.23396643  0.01810669]]. Action = [[ 0.08693191  0.06891296 -0.04585025  0.21640551]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.23446031  0.02042711]]. Action = [[-0.00283422 -0.02621891 -0.09758791  0.6219778 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 164. State = [[-0.23452045  0.02084907]]. Action = [[-0.05078048 -0.09234749 -0.02897078  0.6938057 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 165. State = [[-0.23452789  0.02013262]]. Action = [[-0.04066933 -0.0093494  -0.0311918  -0.64218086]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.23443222  0.01932251]]. Action = [[ 0.05130867 -0.08259068  0.08437885 -0.07739592]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 167. State = [[-0.2342792   0.01736579]]. Action = [[-0.07098339  0.08557443 -0.09237251 -0.6388642 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 167 of -1
Current timestep = 168. State = [[-0.23447484  0.01765879]]. Action = [[-0.07660961 -0.07860084 -0.08708064  0.26570344]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 169. State = [[-0.2354857   0.01608126]]. Action = [[-0.00297742  0.03930449 -0.00939861 -0.91065156]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 170. State = [[-0.23609596  0.01591294]]. Action = [[-0.01411822  0.00880154  0.06674904 -0.87303984]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 170 of -1
Current timestep = 171. State = [[-0.23640838  0.0156791 ]]. Action = [[-0.06212664 -0.09527536  0.02164261  0.78708315]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 172. State = [[-0.23766297  0.01327967]]. Action = [[-0.05749805  0.07816642 -0.07240729  0.4247465 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 172 of -1
Current timestep = 173. State = [[-0.2395902   0.01370542]]. Action = [[ 0.06772096 -0.02590439 -0.06942514  0.03717196]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 174. State = [[-0.24007551  0.01314186]]. Action = [[0.05061603 0.03391466 0.00531169 0.8973205 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 174 of -1
Current timestep = 175. State = [[-0.24023809  0.01345258]]. Action = [[ 0.05554528  0.03260686 -0.08792363  0.81379056]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 176. State = [[-0.2404061   0.01424891]]. Action = [[ 0.07868364  0.08007283  0.02533067 -0.76974714]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 177. State = [[-0.24082854  0.01610177]]. Action = [[-0.0956516   0.02767446  0.01906796  0.12673056]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 178. State = [[-0.24181812  0.0190098 ]]. Action = [[-0.03066284  0.0509553   0.03818431 -0.71720576]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 178 of -1
Current timestep = 179. State = [[-0.24314395  0.02247768]]. Action = [[-0.04773921  0.09529962 -0.02373086  0.1637361 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 180. State = [[-0.24478027  0.02724221]]. Action = [[-0.03774875 -0.0942146   0.01636692  0.4863329 ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of -1
Current timestep = 181. State = [[-0.24542226  0.02787896]]. Action = [[-0.03894413 -0.01914746 -0.09050169  0.16775131]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 182. State = [[-0.2460131   0.02782562]]. Action = [[-0.01430287  0.0429874   0.06085605 -0.6758179 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
State prediction error at timestep 182 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 182 of -1
Current timestep = 183. State = [[-0.24666227  0.02895225]]. Action = [[-0.00414434  0.00776981  0.04248375  0.09475553]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 184. State = [[-0.24731769  0.03001006]]. Action = [[ 0.02047779  0.06523485 -0.0015171  -0.34340513]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 185. State = [[-0.24828479  0.03229036]]. Action = [[ 0.0336354   0.0253557  -0.07041968  0.24388242]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 186. State = [[-0.24884667  0.03394939]]. Action = [[-0.03218844  0.07060374  0.02122946 -0.04225618]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 186 of -1
Current timestep = 187. State = [[-0.250147    0.03723146]]. Action = [[0.09685504 0.08763864 0.0458524  0.20216465]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 188. State = [[-0.2507722   0.04091241]]. Action = [[-0.09724273 -0.03561368 -0.01314425  0.4103657 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of -1
Current timestep = 189. State = [[-0.25165623  0.0427681 ]]. Action = [[ 0.01378588  0.07342186  0.02550528 -0.5557627 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 190. State = [[-0.25259328  0.04558884]]. Action = [[ 0.0646074   0.01482622  0.08185218 -0.75313187]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of -1
Current timestep = 191. State = [[-0.2527586   0.04776826]]. Action = [[-0.01730068  0.09253802  0.05596194  0.3818425 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 192. State = [[-0.25370485  0.05136482]]. Action = [[-0.04018528 -0.01361816  0.02708841 -0.69337034]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 193. State = [[-0.25409767  0.05357076]]. Action = [[ 0.08553595 -0.0638448   0.03395421  0.29204607]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
State prediction error at timestep 193 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 194. State = [[-0.25312614  0.05383017]]. Action = [[-0.06410588  0.03946479  0.02155977 -0.58858323]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
State prediction error at timestep 194 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 195. State = [[-0.2529196  0.0544401]]. Action = [[ 0.06602795 -0.07949264  0.08910959  0.82628345]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 195 of -1
Current timestep = 196. State = [[-0.25220475  0.05424506]]. Action = [[0.02875023 0.0872684  0.09174979 0.68202686]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 197. State = [[-0.25193748  0.05456467]]. Action = [[ 0.04376147 -0.01066268  0.02333891 -0.72452354]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of -1
Current timestep = 198. State = [[-0.25158587  0.05477262]]. Action = [[ 0.07335537 -0.00811983  0.06015823 -0.8584107 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 199. State = [[-0.24924825  0.05525142]]. Action = [[ 0.05385535 -0.00293004  0.07460611  0.09943247]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of -1
Current timestep = 200. State = [[-0.24604361  0.05597221]]. Action = [[-0.0620069   0.06444647 -0.04718177  0.43069935]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 201. State = [[-0.24530548  0.05670448]]. Action = [[-0.08120625 -0.07689349 -0.04353353  0.18470144]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
State prediction error at timestep 201 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 201 of -1
Current timestep = 202. State = [[-0.2454077   0.05627843]]. Action = [[-0.05534072 -0.07996581  0.0365848  -0.21008658]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 203. State = [[-0.2455596  0.0539735]]. Action = [[-0.03129566 -0.02629159 -0.02347044 -0.61831546]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 204. State = [[-0.24584869  0.05225859]]. Action = [[0.00173114 0.09359909 0.02496882 0.76160526]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 204 of -1
Current timestep = 205. State = [[-0.24624461  0.05316096]]. Action = [[-0.01553971  0.06279767 -0.05213142 -0.9580654 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 206. State = [[-0.24712794  0.05548948]]. Action = [[0.04872892 0.04523983 0.01583727 0.9774406 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of -1
Current timestep = 207. State = [[-0.24774334  0.0571858 ]]. Action = [[ 0.08284662 -0.0965315   0.08939964  0.6914606 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 208. State = [[-0.24728765  0.056643  ]]. Action = [[ 0.01281717 -0.03346219  0.04612619  0.38280225]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 209. State = [[-0.24673422  0.05605386]]. Action = [[-0.06490041  0.00148597  0.02523432  0.8052908 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 210. State = [[-0.2466401   0.05570751]]. Action = [[-0.03647925  0.04800604 -0.01142232 -0.42444003]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 211. State = [[-0.24679337  0.056037  ]]. Action = [[ 0.06877656 -0.07119455 -0.06879465 -0.44047534]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 212. State = [[-0.24662861  0.05517135]]. Action = [[-0.02420135  0.08607171 -0.01425253 -0.79336584]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[-0.24670273  0.05569927]]. Action = [[ 0.097106   -0.01968442 -0.02722634  0.6999413 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 213 of -1
Current timestep = 214. State = [[-0.24622269  0.05594908]]. Action = [[ 0.02287175  0.00746139  0.01393441 -0.92719525]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 215. State = [[-0.24550845  0.05618889]]. Action = [[ 0.01928318 -0.0534146   0.06113938 -0.8294449 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 215 of -1
Current timestep = 216. State = [[-0.24439174  0.05579327]]. Action = [[-0.00739996  0.02506813 -0.05850993 -0.35656017]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 217. State = [[-0.24401599  0.05576921]]. Action = [[-0.05228832 -0.09226831  0.05328607  0.3074261 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 218. State = [[-0.24378581  0.05335976]]. Action = [[-0.07541445 -0.07523482 -0.01392888  0.0618372 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 219. State = [[-0.24373753  0.0497946 ]]. Action = [[ 0.09033281 -0.06766362 -0.00702045 -0.8809717 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of -1
Current timestep = 220. State = [[-0.24339321  0.04538536]]. Action = [[-0.05898752 -0.04791585  0.07524662 -0.6623509 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 220 of -1
Current timestep = 221. State = [[-0.24331139  0.0424081 ]]. Action = [[ 0.09480091  0.00337081 -0.01766069 -0.35041153]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, True, False]
State prediction error at timestep 221 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 222. State = [[-0.24285002  0.0402398 ]]. Action = [[ 0.05751079 -0.09269603  0.09840152 -0.8230682 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 223. State = [[-0.24171421  0.03637551]]. Action = [[-0.08507942 -0.01272292  0.01344015 -0.81833345]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
State prediction error at timestep 223 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 224. State = [[-0.24166207  0.0337282 ]]. Action = [[ 0.02607358  0.06691947 -0.07340901  0.29384828]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of -1
Current timestep = 225. State = [[-0.24148835  0.03337481]]. Action = [[-0.05685103 -0.08291403 -0.0082402   0.3310125 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 226. State = [[-0.24151547  0.03150423]]. Action = [[0.01592306 0.09296475 0.01052604 0.07196772]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 227. State = [[-0.24167082  0.03182727]]. Action = [[ 0.09293712  0.05688379 -0.0932194   0.02496839]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
State prediction error at timestep 227 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 228. State = [[-0.24144368  0.03289215]]. Action = [[ 0.09689122  0.04433101 -0.02704819  0.12181616]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 229. State = [[-0.23963022  0.03414162]]. Action = [[ 0.09109978  0.04087255 -0.08210413 -0.97073823]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 229 of -1
Current timestep = 230. State = [[-0.235979    0.03587354]]. Action = [[-0.08337729 -0.09176141  0.06278015  0.24284434]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 231. State = [[-0.2349762   0.03600057]]. Action = [[-0.04315078  0.00167076  0.0091082   0.11260629]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of -1
Current timestep = 232. State = [[-0.23518746  0.03580455]]. Action = [[0.00848472 0.01920754 0.07751478 0.71985865]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
State prediction error at timestep 232 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 232 of -1
Current timestep = 233. State = [[-0.23509909  0.03576176]]. Action = [[-0.07437368 -0.08686506  0.04028762  0.60442495]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, True, False]
State prediction error at timestep 233 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 234. State = [[-0.23512033  0.03486188]]. Action = [[-0.04598218  0.09861296  0.06228485  0.1345694 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 235. State = [[-0.23564531  0.03568479]]. Action = [[-0.07966189 -0.07744621 -0.02202316  0.6944189 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 236. State = [[-0.23680715  0.03498338]]. Action = [[-0.07212447  0.0163906   0.08820335 -0.90602493]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, True, False]
State prediction error at timestep 236 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 236 of -1
Current timestep = 237. State = [[-0.23847233  0.03502313]]. Action = [[-0.08606067  0.09019091 -0.04646116  0.7728801 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 238. State = [[-0.24125835  0.03754794]]. Action = [[ 0.09004379 -0.04242343  0.02547693 -0.01466507]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, True, False]
State prediction error at timestep 238 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 238 of -1
Current timestep = 239. State = [[-0.24154052  0.03773566]]. Action = [[ 0.03599907  0.00266788  0.01913204 -0.10662752]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
State prediction error at timestep 239 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 240. State = [[-0.24148211  0.03760521]]. Action = [[ 0.06202061 -0.07246403  0.05541763  0.8781059 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 240 of -1
Current timestep = 241. State = [[-0.24131177  0.03639035]]. Action = [[ 0.06984638 -0.00472345 -0.00953795  0.9788873 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
State prediction error at timestep 241 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 242. State = [[-0.2407215   0.03568656]]. Action = [[ 0.00203884 -0.0290065   0.01909222  0.24160337]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
State prediction error at timestep 242 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 243. State = [[-0.24023211  0.03456405]]. Action = [[ 0.03110708  0.08455867 -0.03792785  0.38196826]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, False, True, False]
State prediction error at timestep 243 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 244. State = [[-0.2400904   0.03468589]]. Action = [[ 0.05145151 -0.07550892  0.03244019 -0.8228696 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of -1
Current timestep = 245. State = [[-0.23864716  0.0343305 ]]. Action = [[ 0.07209184 -0.01712586  0.0282258   0.8122461 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, False, True, False]
State prediction error at timestep 245 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 246. State = [[-0.23623244  0.03388317]]. Action = [[-0.03609098 -0.02510571 -0.06064525 -0.90904206]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, False, True, False]
State prediction error at timestep 246 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 246 of -1
Current timestep = 247. State = [[-0.23513082  0.03278283]]. Action = [[-0.08488658 -0.07274207  0.0156041   0.1274302 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
State prediction error at timestep 247 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 247 of -1
Current timestep = 248. State = [[-0.23490974  0.03048543]]. Action = [[ 0.09058731  0.03584809 -0.04646722  0.5613915 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 249. State = [[-0.2346311   0.02958175]]. Action = [[ 0.01915085 -0.07976965 -0.06144552 -0.172822  ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
State prediction error at timestep 249 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 249 of -1
Current timestep = 250. State = [[-0.23357637  0.02692644]]. Action = [[ 0.0366777  -0.02131359  0.00619747  0.15923822]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, True, False]
State prediction error at timestep 250 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 250 of -1
Current timestep = 251. State = [[-0.23253818  0.02443108]]. Action = [[-0.07251211  0.08904456 -0.0772839   0.8789873 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, True, False]
State prediction error at timestep 251 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 252. State = [[-0.23264089  0.02463048]]. Action = [[ 0.05572211  0.01919911 -0.05489438 -0.07158339]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.23239842  0.02479852]]. Action = [[-0.08443958 -0.08907783  0.03027909 -0.15228474]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, True, False]
State prediction error at timestep 253 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 253 of -1
Current timestep = 254. State = [[-0.23223507  0.02383849]]. Action = [[-0.06425026 -0.01911876 -0.02491466  0.08216035]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
State prediction error at timestep 254 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 255. State = [[-0.23221382  0.02266078]]. Action = [[-0.03467943  0.00126991 -0.05954093  0.20529187]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, True, False]
State prediction error at timestep 255 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 255 of -1
Current timestep = 256. State = [[-0.2324204   0.02188751]]. Action = [[-0.08140615  0.02793489  0.06571495  0.7626529 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(5.0418e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 256 of -1
Current timestep = 257. State = [[-0.23426843  0.02198393]]. Action = [[-0.02550892  0.09354997 -0.08339117  0.6420156 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 258. State = [[-0.23566245  0.02392053]]. Action = [[ 0.0814643  -0.09145741  0.01186512 -0.63122964]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
State prediction error at timestep 258 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 258 of -1
Current timestep = 259. State = [[-0.23570445  0.02260355]]. Action = [[ 0.01210473 -0.05536879  0.09653337 -0.7959576 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
State prediction error at timestep 259 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 259 of -1
Current timestep = 260. State = [[-0.2357296   0.02096515]]. Action = [[ 0.07651976  0.09665912 -0.08420037 -0.6173747 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(9.3175e-05, grad_fn=<MseLossBackward0>)
Current timestep = 261. State = [[-0.23582678  0.02120558]]. Action = [[ 0.02481151 -0.06327648  0.06284227  0.06365108]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, True, False]
State prediction error at timestep 261 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 261 of -1
Current timestep = 262. State = [[-0.23555031  0.02075984]]. Action = [[ 0.08490949 -0.0079609   0.05677932 -0.557125  ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, True, False]
State prediction error at timestep 262 is tensor(5.4249e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 262 of -1
Current timestep = 263. State = [[-0.23419018  0.02006784]]. Action = [[ 0.0822339  -0.07316343 -0.04438449 -0.86942786]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(7.1775e-05, grad_fn=<MseLossBackward0>)
Current timestep = 264. State = [[-0.23180597  0.01788159]]. Action = [[ 0.0679353   0.040726    0.02507306 -0.78033215]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, True, False]
State prediction error at timestep 264 is tensor(5.4814e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 264 of -1
Current timestep = 265. State = [[-0.22888221  0.01742505]]. Action = [[-0.0713677  -0.00784842  0.09445424 -0.3168465 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(9.6402e-05, grad_fn=<MseLossBackward0>)
Current timestep = 266. State = [[-0.22820473  0.01733659]]. Action = [[0.05883027 0.01668543 0.04903512 0.30558002]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(3.2226e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 266 of -1
Current timestep = 267. State = [[-0.22583628  0.01725595]]. Action = [[ 0.08695912 -0.06606163  0.07537966  0.84540296]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
State prediction error at timestep 267 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 268. State = [[-0.2215083   0.01597788]]. Action = [[ 0.04336808 -0.02303623 -0.0563204   0.54685974]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
State prediction error at timestep 268 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 269. State = [[-0.21773435  0.01488122]]. Action = [[ 3.2695763e-02 -2.5117427e-02  7.2039664e-05 -4.2059869e-01]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of 1
Current timestep = 270. State = [[-0.21485744  0.01330847]]. Action = [[-0.00136347  0.01707848 -0.09194684 -0.34510112]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
State prediction error at timestep 270 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 270 of 1
Current timestep = 271. State = [[-0.21354727  0.01271802]]. Action = [[ 0.01726558 -0.03065781 -0.0204988  -0.20250112]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
State prediction error at timestep 271 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 271 of 1
Current timestep = 272. State = [[-0.21270439  0.01170752]]. Action = [[ 0.06981509 -0.01887213  0.06244833 -0.25001472]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
State prediction error at timestep 272 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 272 of 1
Current timestep = 273. State = [[-0.21018955  0.01028765]]. Action = [[ 0.01753025  0.07747061 -0.02891761 -0.7118266 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
State prediction error at timestep 273 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 273 of 1
Current timestep = 274. State = [[-0.20821     0.01081303]]. Action = [[ 0.03655324  0.06906923  0.09258761 -0.87207276]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 275. State = [[-0.20524362  0.01229909]]. Action = [[ 0.06232012  0.04125697 -0.07242472  0.83552766]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 275 of 1
Current timestep = 276. State = [[-0.2019935   0.01400177]]. Action = [[ 0.05145235  0.05021692 -0.03341724  0.46201336]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of 1
Current timestep = 277. State = [[-0.19832091  0.01615509]]. Action = [[-0.09506376 -0.0536448  -0.00775819  0.4031644 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, True, False]
State prediction error at timestep 277 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 277 of 1
Current timestep = 278. State = [[-0.19685934  0.01662262]]. Action = [[ 0.0579788   0.07357874 -0.03350582 -0.6928576 ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 279. State = [[-0.19487894  0.01849736]]. Action = [[ 0.05885559  0.07984138  0.03838011 -0.32193506]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, True, False]
State prediction error at timestep 279 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 279 of 1
Current timestep = 280. State = [[-0.19191565  0.02169971]]. Action = [[ 0.07917786 -0.00344416 -0.09141617 -0.89469063]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, True, False]
State prediction error at timestep 280 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 280 of 1
Current timestep = 281. State = [[-0.1884252  0.0233588]]. Action = [[ 0.04993805 -0.00343698 -0.01642726 -0.5482967 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 282. State = [[-0.18596226  0.02422217]]. Action = [[-0.02687009  0.01183544 -0.07486661 -0.47102183]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, True, False]
State prediction error at timestep 282 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 283. State = [[-0.18538459  0.02492057]]. Action = [[-0.05035306  0.01727872  0.01590246  0.70256186]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 283 of 1
Current timestep = 284. State = [[-0.18585297  0.02592878]]. Action = [[-0.07651968  0.02440274  0.00347699 -0.4684688 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 284 of 1
Current timestep = 285. State = [[-0.18664734  0.0278008 ]]. Action = [[-0.06092614  0.05795038 -0.08900332 -0.8897637 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, True, False]
State prediction error at timestep 285 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 285 of 1
Current timestep = 286. State = [[-0.18782969  0.03095751]]. Action = [[ 0.04620916  0.03862359 -0.05355922  0.02577364]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, True, False]
State prediction error at timestep 286 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 287. State = [[-0.18875876  0.03330624]]. Action = [[-0.05409376  0.04983848  0.07094634  0.15481305]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 287 of 1
Current timestep = 288. State = [[-0.19001411  0.03663546]]. Action = [[ 0.0619035   0.07767215  0.05097788 -0.4709217 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
State prediction error at timestep 288 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 288 of 1
Current timestep = 289. State = [[-0.19125526  0.04027445]]. Action = [[-0.02504869 -0.01068795 -0.02621368  0.16177452]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
State prediction error at timestep 289 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 290. State = [[-0.19196336  0.0422523 ]]. Action = [[-0.00553031 -0.02751732 -0.06650823 -0.26836008]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 291. State = [[-0.1922508   0.04267944]]. Action = [[-0.07492344 -0.04348912 -0.04533568 -0.19468033]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 291 of 1
Current timestep = 292. State = [[-0.1923929   0.04285233]]. Action = [[-0.04049654  0.0673684  -0.07053193 -0.66090745]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 292 of 1
Current timestep = 293. State = [[-0.1930845   0.04440367]]. Action = [[ 0.02302958 -0.01954524 -0.04052487 -0.78608507]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 293 of 1
Current timestep = 294. State = [[-0.19336519  0.0449441 ]]. Action = [[ 0.04196937  0.09790689 -0.0658998  -0.45137542]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, True, False]
State prediction error at timestep 294 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 295. State = [[-0.19424021  0.04723456]]. Action = [[-0.01097642  0.00113928 -0.05428167  0.34359145]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, True, False]
State prediction error at timestep 295 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 295 of 1
Current timestep = 296. State = [[-0.19477043  0.04867351]]. Action = [[ 0.04577399  0.02360177  0.08817852 -0.33856535]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, True, False]
State prediction error at timestep 296 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 296 of 1
Current timestep = 297. State = [[-0.19504572  0.04944146]]. Action = [[0.05348556 0.01838414 0.07883193 0.81832516]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 298. State = [[-0.19477513  0.05025681]]. Action = [[ 0.09848464 -0.02418346  0.02971388 -0.6922994 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 299. State = [[-0.19280545  0.05079272]]. Action = [[-0.03953961  0.02640682 -0.08085139  0.0632931 ]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 299 of 1
Current timestep = 300. State = [[-0.19127364  0.0515258 ]]. Action = [[ 0.04961938  0.07123103  0.07097786 -0.24804825]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, True, False]
State prediction error at timestep 300 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 300 of 1
Current timestep = 301. State = [[-0.18865854  0.05389985]]. Action = [[-0.03601722  0.0877104  -0.07870309  0.87510574]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, True, False]
State prediction error at timestep 301 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 302. State = [[-0.1872812   0.05839872]]. Action = [[ 0.0553823  -0.01916752  0.00119188  0.12338412]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 302 of 1
Current timestep = 303. State = [[-0.18506376  0.06064893]]. Action = [[ 0.03774425  0.07339758  0.00692873 -0.9162089 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, True, False]
State prediction error at timestep 303 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 303 of 1
Current timestep = 304. State = [[-0.1827657   0.06354902]]. Action = [[-0.0492244  -0.08123425  0.03690854  0.6948731 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, True, False]
State prediction error at timestep 304 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 305. State = [[-0.18175343  0.06387901]]. Action = [[ 0.01644342 -0.01601879  0.05678656  0.98458433]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 306. State = [[-0.18128823  0.06394526]]. Action = [[ 0.02962273  0.0726913  -0.09769244 -0.7327607 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, True, False]
State prediction error at timestep 306 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 306 of 1
Current timestep = 307. State = [[-0.18033832  0.06517272]]. Action = [[-0.02352368 -0.03659196  0.07074635 -0.23824179]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, True, False]
State prediction error at timestep 307 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 307 of 1
Current timestep = 308. State = [[-0.1798237   0.06537712]]. Action = [[-0.06132656 -0.07654732 -0.02248085  0.5774083 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, True, False]
State prediction error at timestep 308 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 309. State = [[-0.17986396  0.06470297]]. Action = [[-0.05925458 -0.03563201 -0.05348634 -0.3601644 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, True, False]
State prediction error at timestep 309 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 309 of 1
Current timestep = 310. State = [[-0.17997795  0.06395082]]. Action = [[ 0.03141347  0.06955238 -0.08955985 -0.6500724 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, True, False]
State prediction error at timestep 310 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 311. State = [[-0.18009013  0.06410338]]. Action = [[ 0.09678305  0.01714213 -0.05496914  0.5299213 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 312. State = [[-0.17983094  0.06437056]]. Action = [[-0.06993942  0.0586734   0.03185431  0.63307166]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 312 of 1
Current timestep = 313. State = [[-0.18046968  0.06606032]]. Action = [[ 0.05473299  0.09005345 -0.09726837  0.21209383]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 314. State = [[-0.1808202   0.06945876]]. Action = [[0.09918744 0.07687051 0.04492294 0.9496927 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
State prediction error at timestep 314 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 315. State = [[-0.17931804  0.0734064 ]]. Action = [[-0.08214512 -0.01764675  0.0077043  -0.5582113 ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 315 of 1
Current timestep = 316. State = [[-0.17956433  0.07530963]]. Action = [[ 0.07837915 -0.01446851  0.03047507  0.46028197]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
State prediction error at timestep 316 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 317. State = [[-0.17858016  0.07619317]]. Action = [[-3.3166453e-02  4.0197372e-04  7.5892858e-02 -5.2225107e-01]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 318. State = [[-0.17820914  0.0765486 ]]. Action = [[-0.03230423 -0.05256987  0.02215469  0.8969649 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 318 of 1
Current timestep = 319. State = [[-0.17799816  0.07629331]]. Action = [[ 0.02399488 -0.07838364 -0.03461035  0.77299094]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, True, False]
State prediction error at timestep 319 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 320. State = [[-0.17718363  0.07490377]]. Action = [[ 0.07868726 -0.03883017  0.09666464 -0.60025674]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, True, False]
State prediction error at timestep 320 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 320 of 1
Current timestep = 321. State = [[-0.17579505  0.07298471]]. Action = [[ 0.03670334 -0.05455476 -0.02834389  0.65046656]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, True, False]
State prediction error at timestep 321 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 321 of 1
Current timestep = 322. State = [[-0.1743269   0.07073528]]. Action = [[ 0.05142326  0.09684675 -0.0962239   0.63003945]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, True, False]
State prediction error at timestep 322 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 323. State = [[-0.17215249  0.07153597]]. Action = [[-0.01014701  0.01421505 -0.08639314 -0.18802965]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, True, False]
State prediction error at timestep 323 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 323 of 1
Current timestep = 324. State = [[-0.17039338  0.07202881]]. Action = [[-0.04059894  0.03835122 -0.05830368 -0.5654747 ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 324 of 1
Current timestep = 325. State = [[-0.17032513  0.07256296]]. Action = [[-0.00984941 -0.04066842 -0.01009861 -0.30290312]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, True, False]
State prediction error at timestep 325 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 326. State = [[-0.17035659  0.07275423]]. Action = [[ 0.0735833  -0.01250878 -0.00954652 -0.41642344]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of 1
Current timestep = 327. State = [[-0.16951269  0.07275494]]. Action = [[-0.05941776  0.08161785  0.06476692 -0.1165027 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 327 of 1
Current timestep = 328. State = [[-0.16956739  0.07380488]]. Action = [[ 0.08672784 -0.02088694 -0.01122346 -0.05761051]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, True, False]
State prediction error at timestep 328 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 329. State = [[-0.16793521  0.0741195 ]]. Action = [[-0.09500168  0.04635654 -0.01826612 -0.00448561]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, True, False]
State prediction error at timestep 329 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 329 of 1
Current timestep = 330. State = [[-0.16793784  0.07540201]]. Action = [[-0.02505449  0.02374823 -0.0668292   0.79520667]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, True, False]
State prediction error at timestep 330 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 330 of 1
Current timestep = 331. State = [[-0.16862066  0.07644297]]. Action = [[-0.07802925 -0.06791744  0.04191872 -0.8278411 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 332. State = [[-0.16896988  0.07653725]]. Action = [[-0.03287004  0.03533187  0.02988941 -0.40099788]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, True, False]
State prediction error at timestep 332 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 333. State = [[-0.16936785  0.07718586]]. Action = [[-0.05177319  0.09202518 -0.05908806  0.6748463 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 333 of 1
Current timestep = 334. State = [[-0.17078044  0.08025428]]. Action = [[-0.07886496 -0.08303706 -0.03468086  0.70518756]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, True, False]
State prediction error at timestep 334 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 335. State = [[-0.17138988  0.08066554]]. Action = [[-0.0628539  -0.00603509  0.08393473 -0.83061546]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, True, False]
State prediction error at timestep 335 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 335 of 1
Current timestep = 336. State = [[-0.17265542  0.08070935]]. Action = [[ 0.01655362 -0.06049869 -0.07443355 -0.79063094]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of 1
Current timestep = 337. State = [[-0.17355213  0.07904004]]. Action = [[-0.07970463  0.0538804  -0.07571225  0.9105041 ]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, True, False]
State prediction error at timestep 337 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 338. State = [[-0.17597152  0.07949174]]. Action = [[-0.01455283 -0.03056797 -0.01192196 -0.22629106]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, True, False]
State prediction error at timestep 338 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 339. State = [[-0.17811017  0.07894763]]. Action = [[ 0.09803546 -0.01587312  0.08347268 -0.71928036]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, True, False]
State prediction error at timestep 339 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 339 of 1
Current timestep = 340. State = [[-0.17802624  0.07827348]]. Action = [[-0.0827878   0.0408339   0.03412666 -0.2028234 ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, True, False]
State prediction error at timestep 340 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 341. State = [[-0.1787763   0.07914248]]. Action = [[ 0.01601262  0.06590185 -0.05012676 -0.47244078]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 342. State = [[-0.17981982  0.08129613]]. Action = [[ 0.03974358  0.04430199 -0.00336261  0.8359995 ]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 343. State = [[-0.18063961  0.0830683 ]]. Action = [[-0.06180882 -0.01116781  0.08909512 -0.5204168 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 344. State = [[-0.18129356  0.08408061]]. Action = [[-0.08763008 -0.0480753   0.09387875  0.49295974]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 344 of -1
Current timestep = 345. State = [[-0.18251935  0.08421307]]. Action = [[ 0.03986568 -0.0474028   0.08049946  0.42846322]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, True, False]
State prediction error at timestep 345 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 345 of -1
Current timestep = 346. State = [[-0.1826906   0.08326812]]. Action = [[ 0.06364269 -0.00631452 -0.0130796  -0.94978034]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, True, False]
State prediction error at timestep 346 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 347. State = [[-0.18281512  0.0826893 ]]. Action = [[-0.02451704  0.01884427  0.04578557  0.6562904 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, True, False]
State prediction error at timestep 347 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 347 of -1
Current timestep = 348. State = [[-0.1827845   0.08280617]]. Action = [[ 0.08666203  0.0883062  -0.08730792 -0.59985733]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, True, False]
State prediction error at timestep 348 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 348 of -1
Current timestep = 349. State = [[-0.1830848   0.08365735]]. Action = [[ 0.00936711  0.05031858 -0.05317779 -0.04600632]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, True, False]
State prediction error at timestep 349 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 350. State = [[-0.18364061  0.08526878]]. Action = [[ 0.08132768  0.08185305  0.04039838 -0.00320202]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, True, False]
State prediction error at timestep 350 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 350 of -1
Current timestep = 351. State = [[-0.18356103  0.08800092]]. Action = [[-0.05839826 -0.09678478 -0.09349584 -0.8431043 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, True, False]
State prediction error at timestep 351 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 351 of -1
Current timestep = 352. State = [[-0.18367636  0.0881842 ]]. Action = [[-0.09356344  0.08032908  0.00398749  0.823894  ]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, True, False]
State prediction error at timestep 352 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 353. State = [[-0.18471904  0.09039613]]. Action = [[-0.00618858  0.03112111 -0.03253536 -0.12508452]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of -1
Current timestep = 354. State = [[-0.18576078  0.0924846 ]]. Action = [[ 0.05461154 -0.08783782  0.0785018   0.55113673]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, True, False]
State prediction error at timestep 354 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 354 of -1
Current timestep = 355. State = [[-0.18553135  0.09185766]]. Action = [[ 0.02419682 -0.06186683 -0.06072856  0.98248446]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 356. State = [[-0.18505234  0.09059975]]. Action = [[ 0.05590334  0.06548793 -0.03345318 -0.47963655]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, True, False]
State prediction error at timestep 356 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 356 of -1
Current timestep = 357. State = [[-0.18496339  0.09043027]]. Action = [[-0.08183291 -0.07907476  0.05461524  0.18230331]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, True, False]
State prediction error at timestep 357 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 358. State = [[-0.18489899  0.08937661]]. Action = [[-0.08211242 -0.09041168 -0.00508127  0.7123172 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 358 of -1
Current timestep = 359. State = [[-0.18496788  0.08676711]]. Action = [[-0.01302086  0.01444174 -0.03700346 -0.4591866 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, True, False]
State prediction error at timestep 359 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 359 of -1
Current timestep = 360. State = [[-0.18505324  0.08506779]]. Action = [[-0.06478442  0.00390136 -0.08396394 -0.8294363 ]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, True, False]
State prediction error at timestep 360 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 361. State = [[-0.18604942  0.08376497]]. Action = [[-0.00359022 -0.06249464  0.06304588  0.74683833]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 361 of -1
Current timestep = 362. State = [[-0.18659583  0.08129767]]. Action = [[-0.09092009 -0.01956213  0.03159883  0.25117075]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, True, False]
State prediction error at timestep 362 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 362 of -1
Current timestep = 363. State = [[-0.18843739  0.07932535]]. Action = [[-0.05338695 -0.00374131  0.08913011  0.23372924]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, True, False]
State prediction error at timestep 363 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 364. State = [[-0.19064912  0.07805412]]. Action = [[ 0.03793951 -0.02025066  0.08684386  0.10147381]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, True, False]
State prediction error at timestep 364 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 364 of -1
Current timestep = 365. State = [[-0.19173269  0.07699325]]. Action = [[-0.07676853  0.08324569 -0.01662753  0.72844183]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, True, False]
State prediction error at timestep 365 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 365 of -1
Current timestep = 366. State = [[-0.19438547  0.0784072 ]]. Action = [[ 0.08650579 -0.04379419 -0.0457561   0.8274994 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, True, False]
State prediction error at timestep 366 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 367. State = [[-0.194786    0.07798643]]. Action = [[-0.05504578 -0.06655096  0.05680009 -0.02684242]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, True, False]
State prediction error at timestep 367 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 367 of -1
Current timestep = 368. State = [[-0.1956844   0.07630663]]. Action = [[-0.08090718 -0.06914423 -0.04317715 -0.91125154]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of -1
Current timestep = 369. State = [[-0.19754873  0.07284118]]. Action = [[-0.03983749 -0.07897981  0.08279756  0.61252654]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, True, False]
State prediction error at timestep 369 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 370. State = [[-0.19911933  0.06853342]]. Action = [[0.04824577 0.06155855 0.06421431 0.6298554 ]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, True, False]
State prediction error at timestep 370 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 370 of -1
Current timestep = 371. State = [[-0.19960178  0.0671342 ]]. Action = [[ 0.03950379 -0.0735242  -0.06478223  0.9070128 ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, True, False]
State prediction error at timestep 371 is tensor(9.3110e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 371 of -1
Current timestep = 372. State = [[-0.19951765  0.06540711]]. Action = [[-0.04288616  0.0727509  -0.01550436  0.85977435]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, True, False]
State prediction error at timestep 372 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 373. State = [[-0.1998254   0.06550632]]. Action = [[-0.04627586 -0.07786055  0.05678616  0.46865273]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, False, True, False]
State prediction error at timestep 373 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 373 of -1
Current timestep = 374. State = [[-0.20078048  0.06384937]]. Action = [[ 0.0263469  -0.08215888 -0.07382891 -0.2808153 ]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, False, True, False]
State prediction error at timestep 374 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 374 of -1
Current timestep = 375. State = [[-0.20117764  0.06113109]]. Action = [[-0.05347458  0.03857129 -0.09498848 -0.74946594]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, False, True, False]
State prediction error at timestep 375 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 376. State = [[-0.2018207   0.05970687]]. Action = [[-0.01809554 -0.07899667  0.04933626 -0.50602406]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, False, True, False]
State prediction error at timestep 376 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 376 of -1
Current timestep = 377. State = [[-0.20261656  0.05642602]]. Action = [[0.02930925 0.02412983 0.07609088 0.55884254]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, False, True, False]
State prediction error at timestep 377 is tensor(7.9860e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of -1
Current timestep = 378. State = [[-0.20277296  0.05491029]]. Action = [[-0.04121053  0.00967916 -0.02440166  0.81089926]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, False, True, False]
State prediction error at timestep 378 is tensor(7.1161e-05, grad_fn=<MseLossBackward0>)
Current timestep = 379. State = [[-0.20359972  0.05447851]]. Action = [[-0.04229425  0.0466172   0.04983779  0.6425648 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, False, True, False]
State prediction error at timestep 379 is tensor(8.0289e-05, grad_fn=<MseLossBackward0>)
Current timestep = 380. State = [[-0.2054168   0.05517261]]. Action = [[ 0.00337274 -0.06960547 -0.0585474  -0.8179761 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, False, True, False]
State prediction error at timestep 380 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 380 of -1
Current timestep = 381. State = [[-0.20678636  0.05406429]]. Action = [[-0.06434018 -0.02545482 -0.07067581 -0.09746605]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, False, True, False]
State prediction error at timestep 381 is tensor(8.8753e-05, grad_fn=<MseLossBackward0>)
Current timestep = 382. State = [[-0.21003126  0.05263191]]. Action = [[-0.05944215  0.024016    0.08378024 -0.12188035]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, False, True, False]
State prediction error at timestep 382 is tensor(7.7621e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 382 of -1
Current timestep = 383. State = [[-0.21365972  0.05233671]]. Action = [[-0.03651433 -0.01992736  0.05786551 -0.04265255]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, False, True, False]
State prediction error at timestep 383 is tensor(4.7891e-05, grad_fn=<MseLossBackward0>)
Current timestep = 384. State = [[-0.217274    0.05170342]]. Action = [[ 0.07519875 -0.05116458  0.01046481 -0.6552388 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, False, True, False]
State prediction error at timestep 384 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 385. State = [[-0.21928674  0.0506663 ]]. Action = [[-0.06816103  0.092659   -0.08610995  0.8859954 ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, False, True, False]
State prediction error at timestep 385 is tensor(4.1558e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 385 of -1
Current timestep = 386. State = [[-0.22122012  0.0511392 ]]. Action = [[ 0.05773216 -0.06709767 -0.09120286  0.33303595]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, False, True, False]
State prediction error at timestep 386 is tensor(1.1585e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.22134495  0.05057886]]. Action = [[-0.040196   -0.05863603 -0.03561153 -0.7162465 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, False, True, False]
State prediction error at timestep 387 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 388. State = [[-0.22220325  0.0486029 ]]. Action = [[ 0.04309747 -0.09017272 -0.02558094 -0.20969671]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, False, True, False]
State prediction error at timestep 388 is tensor(1.6636e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 388 of -1
Current timestep = 389. State = [[-0.22180597  0.04487072]]. Action = [[-0.08236448 -0.05087758 -0.04344297  0.44215095]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, False, True, False]
State prediction error at timestep 389 is tensor(2.8316e-05, grad_fn=<MseLossBackward0>)
Current timestep = 390. State = [[-0.22306666  0.04088046]]. Action = [[ 0.01793651  0.05250128 -0.09431591  0.8109759 ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, False, True, False]
State prediction error at timestep 390 is tensor(1.0325e-05, grad_fn=<MseLossBackward0>)
Current timestep = 391. State = [[-0.22336291  0.03952594]]. Action = [[ 0.00956936  0.00493626  0.07688463 -0.8234721 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, False, True, False]
State prediction error at timestep 391 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 391 of -1
Current timestep = 392. State = [[-0.223284   0.0392632]]. Action = [[ 0.03367618  0.0522773   0.0444401  -0.6251523 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, False, True, False]
State prediction error at timestep 392 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 393. State = [[-0.22341195  0.03961566]]. Action = [[ 0.05570333  0.02964256 -0.05418637 -0.6665473 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, False, True, False]
State prediction error at timestep 393 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 393 of -1
Current timestep = 394. State = [[-0.22357647  0.0400328 ]]. Action = [[ 0.02816618  0.05158395 -0.0410373   0.65686655]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, False, True, False]
State prediction error at timestep 394 is tensor(1.2201e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 394 of -1
Current timestep = 395. State = [[-0.22387616  0.04146491]]. Action = [[-0.0938982   0.07939611  0.09613181  0.48652112]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, False, True, False]
State prediction error at timestep 395 is tensor(5.6839e-05, grad_fn=<MseLossBackward0>)
Current timestep = 396. State = [[-0.22527327  0.0451551 ]]. Action = [[-0.02086718  0.08523709 -0.05505839  0.879279  ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, False, True, False]
State prediction error at timestep 396 is tensor(4.3337e-07, grad_fn=<MseLossBackward0>)
Current timestep = 397. State = [[-0.22676821  0.04945205]]. Action = [[-0.06480259 -0.02819376  0.07482059  0.80599046]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, False, True, False]
State prediction error at timestep 397 is tensor(5.2513e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 397 of -1
Current timestep = 398. State = [[-0.22796248  0.05211228]]. Action = [[ 0.02963287 -0.02655543  0.03690738  0.50898314]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, False, True, False]
State prediction error at timestep 398 is tensor(2.2937e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 398 of -1
Current timestep = 399. State = [[-0.22816975  0.05257937]]. Action = [[-0.02989604  0.0334553  -0.08076394  0.00114048]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, False, True, False]
State prediction error at timestep 399 is tensor(9.3526e-05, grad_fn=<MseLossBackward0>)
Current timestep = 400. State = [[-0.22867937  0.05375909]]. Action = [[0.07760217 0.0993771  0.01491509 0.56164575]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, False, True, False]
State prediction error at timestep 400 is tensor(3.4521e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 400 of -1
Current timestep = 401. State = [[-0.22974719  0.05684111]]. Action = [[-0.01479968  0.0746941  -0.01773582  0.47660673]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, False, True, False]
State prediction error at timestep 401 is tensor(8.0160e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 401 of -1
Current timestep = 402. State = [[-0.23087592  0.06053972]]. Action = [[-0.06781141 -0.03755025  0.09754846  0.85723126]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, False, True, False]
State prediction error at timestep 402 is tensor(3.9073e-05, grad_fn=<MseLossBackward0>)
Current timestep = 403. State = [[-0.23168074  0.06245906]]. Action = [[-0.02607225  0.0042644  -0.06155888 -0.33410788]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, False, True, False]
State prediction error at timestep 403 is tensor(9.2305e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 403 of -1
Current timestep = 404. State = [[-0.23234385  0.06388186]]. Action = [[ 0.06420682 -0.05109962 -0.07682181  0.07310033]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, False, True, False]
State prediction error at timestep 404 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 404 of -1
Current timestep = 405. State = [[-0.23199448  0.06367525]]. Action = [[-0.0460295  -0.00821546  0.00187308  0.88960576]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, False, True, False]
State prediction error at timestep 405 is tensor(4.0826e-05, grad_fn=<MseLossBackward0>)
Current timestep = 406. State = [[-0.23185635  0.06355491]]. Action = [[ 0.03665126 -0.00303057 -0.00416388 -0.4895574 ]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, False, True, False]
State prediction error at timestep 406 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 406 of -1
Current timestep = 407. State = [[-0.23156409  0.06368528]]. Action = [[ 0.01956425  0.01489041 -0.03435214 -0.8461995 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, False, True, False]
State prediction error at timestep 407 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 408. State = [[-0.23156765  0.06367702]]. Action = [[-0.08390199 -0.0734866  -0.08290033  0.5411484 ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, False, True, False]
State prediction error at timestep 408 is tensor(8.7445e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 408 of -1
Current timestep = 409. State = [[-0.23142552  0.06264975]]. Action = [[ 0.039612   -0.00363998  0.00297855 -0.21058285]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, False, True, False]
State prediction error at timestep 409 is tensor(6.8213e-05, grad_fn=<MseLossBackward0>)
Current timestep = 410. State = [[-0.23112616  0.06215775]]. Action = [[ 0.06589036  0.00824219 -0.07818821 -0.57431567]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, False, True, False]
State prediction error at timestep 410 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 411. State = [[-0.23024742  0.06199608]]. Action = [[-0.04614455 -0.09109516  0.02408387 -0.9302663 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, False, True, False]
State prediction error at timestep 411 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 411 of -1
Current timestep = 412. State = [[-0.22975628  0.05986859]]. Action = [[-0.05991692  0.00651578  0.02234594 -0.519324  ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, False, True, False]
State prediction error at timestep 412 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 413. State = [[-0.23046985  0.05881463]]. Action = [[ 0.09807081  0.09157223 -0.0104521  -0.8995654 ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, False, True, False]
State prediction error at timestep 413 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 413 of -1
Current timestep = 414. State = [[-0.23037255  0.05926967]]. Action = [[ 0.03853209  0.08824756 -0.07920128  0.76870966]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, False, True, False]
State prediction error at timestep 414 is tensor(4.1287e-05, grad_fn=<MseLossBackward0>)
Current timestep = 415. State = [[-0.23028028  0.06129251]]. Action = [[-0.04019852 -0.02484119 -0.04864872 -0.12717986]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(6.9111e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of -1
Current timestep = 416. State = [[-0.23018084  0.06182855]]. Action = [[ 0.07219816 -0.03998878  0.05070146  0.6664754 ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, False, True, False]
State prediction error at timestep 416 is tensor(2.1701e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 416 of -1
Current timestep = 417. State = [[-0.22973175  0.06201191]]. Action = [[-0.07275785 -0.05121185 -0.06577562  0.85720396]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, False, True, False]
State prediction error at timestep 417 is tensor(6.7862e-05, grad_fn=<MseLossBackward0>)
Current timestep = 418. State = [[-0.22962138  0.06127729]]. Action = [[ 0.06932897  0.01398897  0.0222309  -0.32689047]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, False, True, False]
State prediction error at timestep 418 is tensor(9.4594e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 418 of -1
Current timestep = 419. State = [[-0.22927144  0.06098082]]. Action = [[-0.02243174  0.01180202  0.02813584 -0.2884959 ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(6.6584e-05, grad_fn=<MseLossBackward0>)
Current timestep = 420. State = [[-0.22922802  0.06088225]]. Action = [[-0.09259151  0.01593926  0.09236655 -0.53120255]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, False, True, False]
State prediction error at timestep 420 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 420 of -1
Current timestep = 421. State = [[-0.2292787   0.06101957]]. Action = [[-0.07053809 -0.00421961  0.05163015  0.89338064]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, False, True, False]
State prediction error at timestep 421 is tensor(4.7013e-05, grad_fn=<MseLossBackward0>)
Current timestep = 422. State = [[-0.23031174  0.06119827]]. Action = [[-0.09249803 -0.09004562 -0.00108479  0.28159595]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(5.6985e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 422 of -1
Current timestep = 423. State = [[-0.23186485  0.05939899]]. Action = [[-0.04950012 -0.03697135 -0.03628974  0.8697169 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 423 is [True, False, False, False, True, False]
State prediction error at timestep 423 is tensor(6.2988e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 423 of -1
Current timestep = 424. State = [[-0.23434648  0.05748665]]. Action = [[-0.05867096  0.0273856  -0.01101495  0.7129252 ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 424 is [True, False, False, False, True, False]
State prediction error at timestep 424 is tensor(5.1551e-05, grad_fn=<MseLossBackward0>)
Current timestep = 425. State = [[-0.23842368  0.05675318]]. Action = [[ 0.02687895  0.09597061 -0.07477625  0.83658624]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 425 is [True, False, False, False, True, False]
State prediction error at timestep 425 is tensor(9.7274e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 425 of -1
Current timestep = 426. State = [[-0.2408628   0.05855852]]. Action = [[-0.06317554  0.02778322 -0.06506073 -0.66357183]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 426 is [True, False, False, False, True, False]
State prediction error at timestep 426 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 426 of -1
Current timestep = 427. State = [[-0.24356237  0.06053427]]. Action = [[-0.0893537  -0.09667134 -0.06087912  0.04709399]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 427 is [True, False, False, False, True, False]
State prediction error at timestep 427 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 428. State = [[-0.24730122  0.05906383]]. Action = [[-0.01607399  0.01251123 -0.0233577  -0.99578786]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 428 is [True, False, False, False, True, False]
State prediction error at timestep 428 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 428 of -1
Current timestep = 429. State = [[-0.25238732  0.05816467]]. Action = [[-0.07388341  0.07457318 -0.08629544  0.8190696 ]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 429 is [True, False, False, False, True, False]
State prediction error at timestep 429 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 430. State = [[-0.25561985  0.06007025]]. Action = [[-0.05190213  0.08303388  0.02695637  0.7386099 ]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 431. State = [[-0.2589233   0.06330767]]. Action = [[-0.08174708 -0.06469209  0.07373542 -0.2676754 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 431 is [True, False, False, False, True, False]
State prediction error at timestep 431 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 432. State = [[-0.26217183  0.06424476]]. Action = [[-0.03271019  0.07945011  0.017754   -0.6940654 ]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 432 of -1
Current timestep = 433. State = [[-0.26547202  0.06728721]]. Action = [[-0.00510465  0.08698764 -0.07210897  0.8498138 ]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 433 is [True, False, False, False, True, False]
State prediction error at timestep 433 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 434. State = [[-0.2678769  0.0716482]]. Action = [[ 0.06548371  0.04702731  0.04336951 -0.6992129 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 434 is [True, False, False, False, True, False]
State prediction error at timestep 434 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 434 of -1
Current timestep = 435. State = [[-0.26921922  0.07459191]]. Action = [[-0.05356039 -0.07988668 -0.03680036 -0.00508702]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 435 is [True, False, False, False, True, False]
State prediction error at timestep 435 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 435 of -1
Current timestep = 436. State = [[-0.26964542  0.07490955]]. Action = [[-0.08370299  0.05908921 -0.09463053 -0.8132512 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 436 is [True, False, False, False, True, False]
State prediction error at timestep 436 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 437. State = [[-0.271611    0.07694919]]. Action = [[-0.02075323  0.06838948  0.07137486  0.8899989 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 437 is [True, False, False, False, True, False]
State prediction error at timestep 437 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 437 of -1
Current timestep = 438. State = [[-0.27347204  0.08029637]]. Action = [[ 0.0838011   0.07499791 -0.01972391  0.44131613]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 438 is [True, False, False, False, True, False]
State prediction error at timestep 438 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 438 of -1
Current timestep = 439. State = [[-0.27502745  0.08374669]]. Action = [[0.00779486 0.07430305 0.07936073 0.5408093 ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 439 is [True, False, False, False, True, False]
State prediction error at timestep 439 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 440. State = [[-0.27676216  0.08703993]]. Action = [[-0.07236509 -0.03545562 -0.08166566  0.9174396 ]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 440 is [True, False, False, False, True, False]
State prediction error at timestep 440 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 440 of -1
Current timestep = 441. State = [[-0.27790135  0.08904076]]. Action = [[-0.07506245  0.00557908  0.09584243 -0.8656665 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 441 is [True, False, False, False, True, False]
State prediction error at timestep 441 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 441 of -1
Current timestep = 442. State = [[-0.27918124  0.09110645]]. Action = [[ 0.02094833  0.06620491 -0.01310942  0.34717786]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 442 is [True, False, False, False, True, False]
State prediction error at timestep 442 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 443. State = [[-0.28078339  0.09391543]]. Action = [[-0.08204155  0.08458181  0.03784341  0.37864065]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 443 is [True, False, False, False, True, False]
State prediction error at timestep 443 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 443 of -1
Current timestep = 444. State = [[-0.28317866  0.09808118]]. Action = [[ 0.01570249  0.09098398 -0.05219567  0.5506835 ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 444 is [True, False, False, False, True, False]
State prediction error at timestep 444 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 445. State = [[-0.28568286  0.10278499]]. Action = [[ 0.05078412 -0.04633825 -0.07820551  0.6723132 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 445 is [True, False, False, False, True, False]
State prediction error at timestep 445 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 445 of -1
Current timestep = 446. State = [[-0.28628093  0.10436568]]. Action = [[ 0.07519735 -0.00703557 -0.06459504 -0.47822666]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 446 is [True, False, False, False, True, False]
State prediction error at timestep 446 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 446 of -1
Current timestep = 447. State = [[-0.2861723   0.10474171]]. Action = [[-0.07253601  0.07708671  0.08446201  0.90989053]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 447 is [True, False, False, False, True, False]
State prediction error at timestep 447 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 448. State = [[-0.28713897  0.10646123]]. Action = [[ 0.05895174 -0.08207446 -0.09722399 -0.57460797]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 448 is [True, False, False, False, True, False]
State prediction error at timestep 448 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 448 of -1
Current timestep = 449. State = [[-0.28656906  0.10654076]]. Action = [[-0.05202679 -0.01581611  0.03465085  0.6941514 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 449 is [True, False, False, False, True, False]
State prediction error at timestep 449 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 449 of -1
Current timestep = 450. State = [[-0.28640807  0.10635838]]. Action = [[ 0.0637191   0.06891096 -0.05069773  0.33012605]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 450 is [True, False, False, False, True, False]
State prediction error at timestep 450 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 451. State = [[-0.28648463  0.10642253]]. Action = [[-0.05292782 -0.06821577 -0.07213149 -0.7413869 ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 451 is [True, False, False, False, True, False]
State prediction error at timestep 451 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 451 of -1
Current timestep = 452. State = [[-0.28638753  0.10642836]]. Action = [[ 0.08682797 -0.01698335  0.08076818 -0.73956794]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 452 is [True, False, False, False, True, False]
State prediction error at timestep 452 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 452 of -1
Current timestep = 453. State = [[-0.2853005   0.10634881]]. Action = [[-0.01662926  0.0823387   0.05181146 -0.9065733 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 453 is [True, False, False, False, True, False]
State prediction error at timestep 453 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 454. State = [[-0.28504863  0.10677854]]. Action = [[-0.05464438 -0.09509461 -0.09838358  0.23068511]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 454 is [True, False, False, False, True, False]
State prediction error at timestep 454 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 454 of -1
Current timestep = 455. State = [[-0.2848441   0.10613889]]. Action = [[-0.0967416  -0.08106668  0.04653937  0.5755954 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 455 is [True, False, False, False, True, False]
State prediction error at timestep 455 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 455 of -1
Current timestep = 456. State = [[-0.28532916  0.1032779 ]]. Action = [[ 0.03896127 -0.09134807  0.02166145  0.94442964]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 456 is [True, False, False, False, True, False]
State prediction error at timestep 456 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 457. State = [[-0.28544378  0.09879661]]. Action = [[-0.04481447  0.04346461 -0.007288    0.774297  ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 457 is [True, False, False, False, True, False]
State prediction error at timestep 457 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 457 of -1
Current timestep = 458. State = [[-0.28628135  0.09708674]]. Action = [[ 0.00526849  0.08298565  0.0472072  -0.5803935 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 458 is [True, False, False, False, True, False]
State prediction error at timestep 458 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 459. State = [[-0.28684986  0.09803224]]. Action = [[ 0.03222609  0.08982571 -0.05302454  0.36355805]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 459 is [True, False, False, False, True, False]
State prediction error at timestep 459 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 459 of -1
Current timestep = 460. State = [[-0.28818128  0.10063006]]. Action = [[-0.04537997  0.06348341  0.05700708  0.12213945]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 460 is [True, False, False, False, True, False]
State prediction error at timestep 460 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 461. State = [[-0.28977612  0.10399771]]. Action = [[ 0.02961513  0.0089988   0.00291035 -0.34797645]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 461 is [True, False, False, False, True, False]
State prediction error at timestep 461 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 461 of -1
Current timestep = 462. State = [[-0.29067692  0.10606167]]. Action = [[-0.09456958 -0.01746827  0.09074011 -0.3296336 ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 462 is [True, False, False, False, True, False]
State prediction error at timestep 462 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 463. State = [[-0.29221532  0.10741994]]. Action = [[-0.00556117 -0.06843072  0.05941153 -0.8304511 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 463 is [True, False, False, False, True, False]
State prediction error at timestep 463 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 463 of -1
Current timestep = 464. State = [[-0.29344115  0.10678821]]. Action = [[ 0.05851167  0.08029168  0.06570929 -0.6267483 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 464 is [True, False, False, False, True, False]
State prediction error at timestep 464 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 465. State = [[-0.29360518  0.10754377]]. Action = [[ 0.00450973  0.01587888 -0.04236821  0.9258245 ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 465 is [True, False, False, False, True, False]
State prediction error at timestep 465 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of -1
Current timestep = 466. State = [[-0.2939852   0.10863937]]. Action = [[-0.0669897   0.06942197 -0.04611132 -0.9789513 ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 466 is [True, False, False, False, True, False]
State prediction error at timestep 466 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 466 of -1
Current timestep = 467. State = [[-0.29573405  0.11154718]]. Action = [[ 0.08634955  0.07418064  0.01678167 -0.6638618 ]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 467 is [True, False, False, False, True, False]
State prediction error at timestep 467 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 468. State = [[-0.2965518   0.11426625]]. Action = [[-2.8051138e-02  5.2167512e-02  6.0633570e-04 -7.9090810e-01]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 468 is [True, False, False, False, True, False]
State prediction error at timestep 468 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 469. State = [[-0.2977589   0.11755753]]. Action = [[ 0.0962921  -0.05407717 -0.07422479 -0.66425014]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 469 is [True, False, False, False, True, False]
State prediction error at timestep 469 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 469 of -1
Current timestep = 470. State = [[-0.29707548  0.1183229 ]]. Action = [[ 0.08392424  0.01266936 -0.0747842  -0.18572688]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 470 is [True, False, False, False, True, False]
State prediction error at timestep 470 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 471. State = [[-0.29517365  0.11910433]]. Action = [[-0.09207274 -0.00469216 -0.02559388  0.7932253 ]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 471 is [True, False, False, False, True, False]
State prediction error at timestep 471 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 472. State = [[-0.29470477  0.11958156]]. Action = [[ 0.04584683 -0.00760815 -0.06564806 -0.62137514]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 472 is [True, False, False, False, True, False]
State prediction error at timestep 472 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 472 of -1
Current timestep = 473. State = [[-0.2940359   0.11973734]]. Action = [[-0.09119327  0.04813028  0.04855754 -0.11438864]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 473 is [True, False, False, False, True, False]
State prediction error at timestep 473 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 474. State = [[-0.294722    0.12080296]]. Action = [[-0.03503716  0.06502265 -0.0421509  -0.4090833 ]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 474 is [True, False, False, False, True, False]
State prediction error at timestep 474 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 475. State = [[-0.29636246  0.12343702]]. Action = [[-0.08625812  0.06311721 -0.08803732  0.37038076]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 475 is [True, False, False, False, True, False]
State prediction error at timestep 475 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 475 of -1
Current timestep = 476. State = [[-0.29908478  0.12751207]]. Action = [[ 0.01588384  0.07509103 -0.0863575  -0.26735032]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 476 is [True, False, False, False, False, True]
State prediction error at timestep 476 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 477. State = [[-0.3013965   0.13136996]]. Action = [[-0.07925258  0.09083287 -0.04550938  0.39023995]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 477 is [True, False, False, False, False, True]
State prediction error at timestep 477 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 477 of -1
Current timestep = 478. State = [[-0.30507645  0.13731655]]. Action = [[-0.01535948 -0.03164329 -0.00308415  0.22777176]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 478 is [True, False, False, False, False, True]
State prediction error at timestep 478 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 479. State = [[-0.30653438  0.13966414]]. Action = [[ 0.0921144  -0.00659072  0.03590936  0.01999533]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 479 is [True, False, False, False, False, True]
State prediction error at timestep 479 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 480. State = [[-0.30646074  0.14005327]]. Action = [[0.01903006 0.06140498 0.08041479 0.5668268 ]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 480 is [True, False, False, False, False, True]
State prediction error at timestep 480 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 480 of -1
Current timestep = 481. State = [[-0.30686012  0.14119086]]. Action = [[-0.01175726  0.04518836 -0.05773598  0.60631335]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 481 is [True, False, False, False, False, True]
State prediction error at timestep 481 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 482. State = [[-0.30742547  0.1428165 ]]. Action = [[ 0.0845893  -0.05230708 -0.07891949  0.6850934 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 482 is [True, False, False, False, False, True]
State prediction error at timestep 482 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 482 of -1
Current timestep = 483. State = [[-0.30643743  0.1429979 ]]. Action = [[ 0.04206748 -0.09136472  0.06761517  0.6388993 ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 483 is [True, False, False, False, False, True]
State prediction error at timestep 483 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 483 of -1
Current timestep = 484. State = [[-0.30451784  0.14183909]]. Action = [[ 0.07858301 -0.073234    0.07338814 -0.7954693 ]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 484 is [True, False, False, False, False, True]
State prediction error at timestep 484 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 485. State = [[-0.3018179   0.13943647]]. Action = [[ 0.02190475 -0.04329587  0.08643193 -0.5771929 ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 485 is [True, False, False, False, False, True]
State prediction error at timestep 485 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 485 of -1
Current timestep = 486. State = [[-0.29924983  0.13756031]]. Action = [[ 0.03505402  0.06121833  0.05683983 -0.5359955 ]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 486 is [True, False, False, False, False, True]
State prediction error at timestep 486 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 486 of -1
Current timestep = 487. State = [[-0.29721048  0.13800631]]. Action = [[-0.08342192  0.05086026 -0.07172708  0.8220141 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 487 is [True, False, False, False, False, True]
State prediction error at timestep 487 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 488. State = [[-0.297183   0.1390147]]. Action = [[-0.0608827   0.08834983  0.08724243 -0.9291216 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 488 is [True, False, False, False, False, True]
State prediction error at timestep 488 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 488 of -1
Current timestep = 489. State = [[-0.2985695   0.14110023]]. Action = [[-0.0329614   0.09521664  0.08356015 -0.6755784 ]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 489 is [True, False, False, False, False, True]
State prediction error at timestep 489 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 490. State = [[-0.3005583   0.14426224]]. Action = [[-0.0839997  -0.08157506  0.02791006 -0.8249684 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 490 is [True, False, False, False, False, True]
State prediction error at timestep 490 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 490 of -1
Current timestep = 491. State = [[-0.30165836  0.14548701]]. Action = [[ 0.05923671 -0.02605446  0.07979877  0.9303458 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 491 is [True, False, False, False, False, True]
State prediction error at timestep 491 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 491 of -1
Current timestep = 492. State = [[-0.30179387  0.1455012 ]]. Action = [[-0.05810053  0.06577904 -0.09107546  0.08158076]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 492 is [True, False, False, False, False, True]
State prediction error at timestep 492 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 493. State = [[-0.30296132  0.14705737]]. Action = [[-0.05632515  0.07761074  0.03361892 -0.8238375 ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 493 is [True, False, False, False, False, True]
State prediction error at timestep 493 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 494. State = [[-0.30538002  0.15052134]]. Action = [[0.05789555 0.05648819 0.05144978 0.3822794 ]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 494 is [True, False, False, False, False, True]
State prediction error at timestep 494 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Current timestep = 495. State = [[-0.30676478  0.15288185]]. Action = [[-0.0475147   0.06712098  0.08383403  0.575909  ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 495 is [True, False, False, False, False, True]
State prediction error at timestep 495 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 495 of -1
Current timestep = 496. State = [[-0.30902064  0.15649188]]. Action = [[ 0.09626666  0.07329778 -0.04673212 -0.25664097]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 496 is [True, False, False, False, False, True]
State prediction error at timestep 496 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Current timestep = 497. State = [[-0.30852914  0.16008572]]. Action = [[-0.07770342 -0.04554266  0.08210099 -0.54858875]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 497 is [True, False, False, False, False, True]
State prediction error at timestep 497 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of -1
Current timestep = 498. State = [[-0.30944714  0.16159293]]. Action = [[ 0.08623888 -0.00700551  0.0573203  -0.88128966]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 498 is [True, False, False, False, False, True]
State prediction error at timestep 498 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 498 of -1
Current timestep = 499. State = [[-0.30887887  0.16185723]]. Action = [[ 0.0020919  -0.06324443  0.05366463  0.2525463 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 499 is [True, False, False, False, False, True]
State prediction error at timestep 499 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 500. State = [[-0.30853632  0.16181038]]. Action = [[ 0.00394694  0.02763758 -0.0268846  -0.9245264 ]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 500 is [True, False, False, False, False, True]
State prediction error at timestep 500 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 500 of -1
Current timestep = 501. State = [[-0.30840558  0.16192386]]. Action = [[-0.02994776  0.05366319  0.09241355  0.34800375]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 501 is [True, False, False, False, False, True]
State prediction error at timestep 501 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[-0.30836305  0.16233854]]. Action = [[ 0.08023799  0.04733904 -0.07349575 -0.3392443 ]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 502 is [True, False, False, False, False, True]
State prediction error at timestep 502 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 503. State = [[-0.30713323  0.16337074]]. Action = [[-0.00523986 -0.02693859 -0.01036767  0.8565968 ]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 503 is [True, False, False, False, False, True]
State prediction error at timestep 503 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 503 of -1
Current timestep = 504. State = [[-0.3066144   0.16392459]]. Action = [[-0.09185718  0.09093619  0.08591453 -0.7359324 ]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 504 is [True, False, False, False, False, True]
State prediction error at timestep 504 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 504 of -1
Current timestep = 505. State = [[-0.30822697  0.16605921]]. Action = [[-0.03725086 -0.0689296   0.02630497 -0.7219821 ]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 505 is [True, False, False, False, False, True]
State prediction error at timestep 505 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 506. State = [[-0.30868596  0.16651054]]. Action = [[-0.01190357 -0.04411688 -0.09562302  0.89979887]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 506 is [True, False, False, False, False, True]
State prediction error at timestep 506 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 506 of -1
Current timestep = 507. State = [[-0.30876926  0.16661322]]. Action = [[-0.04362974  0.04601086 -0.06685829 -0.31108963]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 507 is [True, False, False, False, False, True]
State prediction error at timestep 507 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 508. State = [[-0.30955496  0.16760665]]. Action = [[-0.0517186  -0.04705952  0.08509315 -0.9492439 ]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 508 is [True, False, False, False, False, True]
State prediction error at timestep 508 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 508 of -1
Current timestep = 509. State = [[-0.3101315   0.16803926]]. Action = [[-9.099572e-02 -2.682954e-04  8.000908e-02 -6.708308e-01]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 509 is [True, False, False, False, False, True]
State prediction error at timestep 509 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 509 of -1
Current timestep = 510. State = [[-0.3121939   0.16860998]]. Action = [[ 0.00137319  0.03491669 -0.09158824  0.35381162]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 510 is [True, False, False, False, False, True]
State prediction error at timestep 510 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 511. State = [[-0.31375015  0.17009264]]. Action = [[-0.00960571  0.02892538  0.0973125  -0.85903007]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 511 is [True, False, False, False, False, True]
State prediction error at timestep 511 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 511 of -1
Current timestep = 512. State = [[-0.31503725  0.17167081]]. Action = [[ 0.08565133  0.07649297  0.02961769 -0.16750264]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 512 is [True, False, False, False, False, True]
State prediction error at timestep 512 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 512 of -1
Current timestep = 513. State = [[-0.31612003  0.17305887]]. Action = [[ 0.00185995  0.05308374  0.0402732  -0.08804989]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 513 is [True, False, False, False, False, True]
State prediction error at timestep 513 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 514. State = [[-0.31752706  0.17506766]]. Action = [[0.04683579 0.04717857 0.01287832 0.7459111 ]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 514 is [True, False, False, False, False, True]
State prediction error at timestep 514 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 514 of -1
Current timestep = 515. State = [[-0.31819093  0.17683296]]. Action = [[-0.0486835   0.06383052  0.07062242  0.8835752 ]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 515 is [True, False, False, False, False, True]
State prediction error at timestep 515 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 516. State = [[-0.3196954   0.17920652]]. Action = [[ 0.0936751  -0.08473375  0.03423827  0.6109649 ]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 516 is [True, False, False, False, False, True]
State prediction error at timestep 516 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 516 of -1
Current timestep = 517. State = [[-0.31894955  0.17954282]]. Action = [[-0.06413788  0.04300425  0.07294684  0.26136458]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 517 is [True, False, False, False, False, True]
State prediction error at timestep 517 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 517 of -1
Current timestep = 518. State = [[-0.31946337  0.18048438]]. Action = [[-0.06968926  0.02889744  0.07290275  0.65091205]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 518 is [True, False, False, False, False, True]
State prediction error at timestep 518 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 518 of -1
Current timestep = 519. State = [[-0.32065776  0.18192005]]. Action = [[-0.00989122 -0.07454894  0.06623449 -0.07877004]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 519 is [True, False, False, False, False, True]
State prediction error at timestep 519 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 520. State = [[-0.32077172  0.18209589]]. Action = [[ 0.0971108  -0.00726195 -0.01181478  0.6867176 ]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 520 is [True, False, False, False, False, True]
State prediction error at timestep 520 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 520 of -1
Current timestep = 521. State = [[-0.3204586   0.18199292]]. Action = [[0.03120168 0.07155252 0.02119516 0.0111506 ]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 521 is [True, False, False, False, False, True]
State prediction error at timestep 521 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 521 of -1
Current timestep = 522. State = [[-0.3202229   0.18224975]]. Action = [[ 0.01559488  0.08206386 -0.05926181 -0.34338593]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 522 is [True, False, False, False, False, True]
State prediction error at timestep 522 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 523. State = [[-0.31955847  0.18340035]]. Action = [[ 0.06770127 -0.00896267  0.00279409 -0.37437695]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 523 is [True, False, False, False, False, True]
State prediction error at timestep 523 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 523 of -1
Current timestep = 524. State = [[-0.31736392  0.18500835]]. Action = [[ 0.04612153  0.0832103   0.06636889 -0.4811684 ]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 524 is [True, False, False, False, False, True]
State prediction error at timestep 524 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 525. State = [[-0.31426522  0.1877566 ]]. Action = [[ 0.0718508  -0.08764163 -0.07304783  0.91805005]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 525 is [True, False, False, False, False, True]
State prediction error at timestep 525 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 526. State = [[-0.3105482   0.18912444]]. Action = [[-0.06999688 -0.0981732  -0.01306631 -0.59511906]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 526 is [True, False, False, False, False, True]
State prediction error at timestep 526 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 526 of -1
Current timestep = 527. State = [[-0.30903426  0.188155  ]]. Action = [[-0.0826181  -0.02114634  0.08677434 -0.5824931 ]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 527 is [True, False, False, False, False, True]
State prediction error at timestep 527 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 528. State = [[-0.30871993  0.18733968]]. Action = [[ 0.09790812 -0.03423259  0.01211256  0.21761477]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 528 is [True, False, False, False, False, True]
State prediction error at timestep 528 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 528 of -1
Current timestep = 529. State = [[-0.3074004   0.18646185]]. Action = [[0.07282827 0.04973329 0.05201387 0.57850206]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 529 is [True, False, False, False, False, True]
State prediction error at timestep 529 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 529 of -1
Current timestep = 530. State = [[-0.306417    0.18653844]]. Action = [[ 0.05079918  0.06368398 -0.01160288  0.489133  ]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 530 is [True, False, False, False, False, True]
State prediction error at timestep 530 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 531. State = [[-0.30541056  0.18710296]]. Action = [[ 0.08729701 -0.04943829  0.03212283 -0.53901404]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 531 is [True, False, False, False, False, True]
State prediction error at timestep 531 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 531 of -1
Current timestep = 532. State = [[-0.30234605  0.18717416]]. Action = [[ 0.02479253  0.04143352  0.0818842  -0.32775295]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 532 is [True, False, False, False, False, True]
State prediction error at timestep 532 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 533. State = [[-0.2994659   0.18865466]]. Action = [[ 0.06155575  0.06194776 -0.07261477 -0.4464993 ]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 533 is [True, False, False, False, False, True]
State prediction error at timestep 533 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 533 of -1
Current timestep = 534. State = [[-0.29608864  0.19073763]]. Action = [[-0.0628062   0.05109761  0.06920721  0.68990815]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 534 is [True, False, False, False, False, True]
State prediction error at timestep 534 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 534 of -1
Current timestep = 535. State = [[-0.29386008  0.1933553 ]]. Action = [[ 0.07185889  0.08956256 -0.02611209  0.9127798 ]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 535 is [True, False, False, False, False, True]
State prediction error at timestep 535 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 536. State = [[-0.29070675  0.19633748]]. Action = [[ 0.09819121  0.02082781 -0.04541397  0.7484106 ]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 536 is [True, False, False, False, False, True]
State prediction error at timestep 536 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 536 of -1
Current timestep = 537. State = [[-0.2869031   0.19877936]]. Action = [[0.02462689 0.01723714 0.02585334 0.56488895]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 537 is [True, False, False, False, False, True]
State prediction error at timestep 537 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 538. State = [[-0.284031   0.2007079]]. Action = [[ 0.0647319  -0.03975018 -0.03155309 -0.58525896]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 538 is [True, False, False, False, False, True]
State prediction error at timestep 538 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 539. State = [[-0.2811061   0.20194729]]. Action = [[-0.02154913  0.04638737 -0.03533732 -0.8541152 ]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 539 is [True, False, False, False, False, True]
State prediction error at timestep 539 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 539 of -1
Current timestep = 540. State = [[-0.27921993  0.20320205]]. Action = [[-0.05291172 -0.06099961 -0.08512329  0.21803772]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 540 is [True, False, False, False, False, True]
State prediction error at timestep 540 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 541. State = [[-0.2791725   0.20324595]]. Action = [[ 0.08803565 -0.07428841 -0.07393934 -0.40473473]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 541 is [True, False, False, False, False, True]
State prediction error at timestep 541 is tensor(4.3532e-05, grad_fn=<MseLossBackward0>)
Current timestep = 542. State = [[-0.27757743  0.20140389]]. Action = [[-0.09233643  0.00031408 -0.09682681 -0.27527213]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 542 is [True, False, False, False, False, True]
State prediction error at timestep 542 is tensor(8.0128e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 542 of -1
Current timestep = 543. State = [[-0.27749124  0.20122404]]. Action = [[-0.02574358  0.02561735 -0.02341813  0.535982  ]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 543 is [True, False, False, False, False, True]
State prediction error at timestep 543 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 544. State = [[-0.27758595  0.20129687]]. Action = [[-0.03142265 -0.01103579 -0.05310422 -0.24191678]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 544 is [True, False, False, False, False, True]
State prediction error at timestep 544 is tensor(4.6373e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 544 of -1
Current timestep = 545. State = [[-0.27764684  0.20133382]]. Action = [[ 0.05475805 -0.06206476  0.00685928  0.23269331]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 545 is [True, False, False, False, False, True]
State prediction error at timestep 545 is tensor(6.5915e-05, grad_fn=<MseLossBackward0>)
Current timestep = 546. State = [[-0.27699184  0.19990882]]. Action = [[ 0.08895383 -0.01684555 -0.05470379 -0.74648184]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 546 is [True, False, False, False, False, True]
State prediction error at timestep 546 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 547. State = [[-0.27572477  0.19789329]]. Action = [[ 0.04355193 -0.07612578 -0.00365762  0.5102953 ]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 547 is [True, False, False, False, False, True]
State prediction error at timestep 547 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 547 of 1
Current timestep = 548. State = [[-0.27371958  0.19428676]]. Action = [[ 0.04040236 -0.01558902 -0.08794641 -0.68297887]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 548 is [True, False, False, False, False, True]
State prediction error at timestep 548 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 549. State = [[-0.2721484   0.19134957]]. Action = [[-0.04216677 -0.0213262   0.016168   -0.6164115 ]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 549 is [True, False, False, False, False, True]
State prediction error at timestep 549 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 550. State = [[-0.27136138  0.1892838 ]]. Action = [[-0.08513327 -0.08585229 -0.05891224  0.27040136]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 550 is [True, False, False, False, False, True]
State prediction error at timestep 550 is tensor(2.2831e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 550 of 1
Current timestep = 551. State = [[-0.27064732  0.18673277]]. Action = [[-0.02035525 -0.05966712 -0.06917791  0.0172596 ]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 551 is [True, False, False, False, False, True]
State prediction error at timestep 551 is tensor(4.5620e-05, grad_fn=<MseLossBackward0>)
Current timestep = 552. State = [[-0.27103755  0.1835304 ]]. Action = [[-0.06013454 -0.08858133 -0.07995351  0.32083   ]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 552 is [True, False, False, False, False, True]
State prediction error at timestep 552 is tensor(2.1534e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 552 of 1
Current timestep = 553. State = [[-0.2715059  0.1797178]]. Action = [[ 0.00572006  0.06302438 -0.02446406  0.9410932 ]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 553 is [True, False, False, False, False, True]
State prediction error at timestep 553 is tensor(7.2978e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 553 of 1
Current timestep = 554. State = [[-0.27174646  0.17784572]]. Action = [[-0.01771101 -0.06235547  0.07828517  0.8430784 ]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 554 is [True, False, False, False, False, True]
State prediction error at timestep 554 is tensor(3.0450e-05, grad_fn=<MseLossBackward0>)
Current timestep = 555. State = [[-0.27211037  0.17518376]]. Action = [[-0.08475267 -0.02602778  0.00607766  0.4959576 ]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 555 is [True, False, False, False, False, True]
State prediction error at timestep 555 is tensor(1.4176e-05, grad_fn=<MseLossBackward0>)
Current timestep = 556. State = [[-0.27361083  0.17300093]]. Action = [[-0.05282258 -0.08187653 -0.0459424  -0.25655854]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 556 is [True, False, False, False, False, True]
State prediction error at timestep 556 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 556 of 1
Current timestep = 557. State = [[-0.27623612  0.16967764]]. Action = [[ 0.09022645 -0.02424464  0.02672502  0.57295275]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 557 is [True, False, False, False, False, True]
State prediction error at timestep 557 is tensor(3.5149e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 557 of 1
Current timestep = 558. State = [[-0.27681807  0.16716276]]. Action = [[-0.09473433  0.04522576 -0.06416869  0.5484704 ]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 558 is [True, False, False, False, False, True]
State prediction error at timestep 558 is tensor(8.2475e-05, grad_fn=<MseLossBackward0>)
Current timestep = 559. State = [[-0.27828616  0.16655813]]. Action = [[-0.04615731 -0.0507797  -0.03414457  0.6574589 ]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 559 is [True, False, False, False, False, True]
State prediction error at timestep 559 is tensor(7.9023e-05, grad_fn=<MseLossBackward0>)
Current timestep = 560. State = [[-0.27970558  0.16546257]]. Action = [[-0.0161942  -0.0513588  -0.09251909 -0.23693526]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 560 is [True, False, False, False, False, True]
State prediction error at timestep 560 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 560 of 1
Current timestep = 561. State = [[-0.28063405  0.16319676]]. Action = [[ 0.08520468 -0.02825081  0.08851165  0.50424683]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 561 is [True, False, False, False, False, True]
State prediction error at timestep 561 is tensor(9.3549e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 561 of -1
Current timestep = 562. State = [[-0.28014576  0.16100504]]. Action = [[-0.08948339 -0.03585337 -0.04738503 -0.22126412]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 562 is [True, False, False, False, False, True]
State prediction error at timestep 562 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 563. State = [[-0.28087524  0.15799238]]. Action = [[-0.05412427 -0.01357633 -0.0706218   0.5433967 ]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 563 is [True, False, False, False, False, True]
State prediction error at timestep 563 is tensor(3.8230e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 563 of -1
Current timestep = 564. State = [[-0.28167173  0.15602149]]. Action = [[ 0.08294513 -0.06379369  0.02701332 -0.31746328]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 564 is [True, False, False, False, False, True]
State prediction error at timestep 564 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 564 of -1
Current timestep = 565. State = [[-0.28118435  0.15337257]]. Action = [[0.09011268 0.08637733 0.04370824 0.75029373]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 565 is [True, False, False, False, False, True]
State prediction error at timestep 565 is tensor(4.0349e-05, grad_fn=<MseLossBackward0>)
Current timestep = 566. State = [[-0.28114918  0.15331922]]. Action = [[-0.07036914 -0.04871295 -0.09664335  0.13887143]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 566 is [True, False, False, False, False, True]
State prediction error at timestep 566 is tensor(3.0322e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 566 of -1
Current timestep = 567. State = [[-0.28091443  0.15283237]]. Action = [[-0.00430423 -0.08221184 -0.01346205 -0.0158239 ]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 567 is [True, False, False, False, False, True]
State prediction error at timestep 567 is tensor(2.9645e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 567 of -1
Current timestep = 568. State = [[-0.2805651   0.15083075]]. Action = [[0.06289613 0.02802802 0.09660166 0.33002985]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 568 is [True, False, False, False, False, True]
State prediction error at timestep 568 is tensor(1.9990e-05, grad_fn=<MseLossBackward0>)
Current timestep = 569. State = [[-0.2800596   0.14986652]]. Action = [[ 0.04077572 -0.03350519  0.07177217  0.26644635]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 569 is [True, False, False, False, False, True]
State prediction error at timestep 569 is tensor(1.0948e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 569 of -1
Current timestep = 570. State = [[-0.27886686  0.14859296]]. Action = [[-0.06902133 -0.04209651 -0.05325554 -0.7070219 ]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 570 is [True, False, False, False, False, True]
State prediction error at timestep 570 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 571. State = [[-0.27893442  0.14677972]]. Action = [[-0.0378766  -0.03067766  0.07698911  0.1418966 ]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 571 is [True, False, False, False, False, True]
State prediction error at timestep 571 is tensor(2.0677e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 571 of -1
Current timestep = 572. State = [[-0.2794887   0.14450827]]. Action = [[ 0.08528986 -0.0370287   0.01058777  0.20242107]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 572 is [True, False, False, False, False, True]
State prediction error at timestep 572 is tensor(9.3256e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 572 of -1
Current timestep = 573. State = [[-0.27875447  0.1415698 ]]. Action = [[-0.05948086 -0.0266522  -0.00769743 -0.743476  ]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 573 is [True, False, False, False, False, True]
State prediction error at timestep 573 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 574. State = [[-0.27937758  0.13893726]]. Action = [[-0.0760967  -0.05045866  0.00527807 -0.6772477 ]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 574 is [True, False, False, False, False, True]
State prediction error at timestep 574 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 574 of -1
Current timestep = 575. State = [[-0.28184837  0.13595489]]. Action = [[0.05473114 0.09516913 0.04860518 0.06224477]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 575 is [True, False, False, False, False, True]
State prediction error at timestep 575 is tensor(5.1069e-05, grad_fn=<MseLossBackward0>)
Current timestep = 576. State = [[-0.28237915  0.1358551 ]]. Action = [[-0.02239656 -0.08894809 -0.0170995   0.943068  ]]. Reward = [0.]
Curr episode timestep = 576
Scene graph at timestep 576 is [True, False, False, False, False, True]
State prediction error at timestep 576 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 576 of -1
Current timestep = 577. State = [[-0.28305027  0.1343529 ]]. Action = [[-0.03450797 -0.06970073 -0.00569271 -0.24995506]]. Reward = [0.]
Curr episode timestep = 577
Scene graph at timestep 577 is [True, False, False, False, False, True]
State prediction error at timestep 577 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 577 of -1
Current timestep = 578. State = [[-0.28433445  0.13205592]]. Action = [[0.02635968 0.00402826 0.00683926 0.9471853 ]]. Reward = [0.]
Curr episode timestep = 578
Scene graph at timestep 578 is [True, False, False, False, False, True]
State prediction error at timestep 578 is tensor(8.4793e-05, grad_fn=<MseLossBackward0>)
Current timestep = 579. State = [[-0.2847411   0.13063745]]. Action = [[-0.03600876 -0.06426552  0.06422729 -0.02596432]]. Reward = [0.]
Curr episode timestep = 579
Scene graph at timestep 579 is [True, False, False, False, False, True]
State prediction error at timestep 579 is tensor(3.5337e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 579 of -1
Current timestep = 580. State = [[-0.28566286  0.1279648 ]]. Action = [[0.0564952  0.08009226 0.00063046 0.45076656]]. Reward = [0.]
Curr episode timestep = 580
Scene graph at timestep 580 is [True, False, False, False, False, True]
State prediction error at timestep 580 is tensor(2.7844e-06, grad_fn=<MseLossBackward0>)
Current timestep = 581. State = [[-0.28556398  0.1277941 ]]. Action = [[-0.08362171 -0.03420083 -0.05476926 -0.21135736]]. Reward = [0.]
Curr episode timestep = 581
Scene graph at timestep 581 is [True, False, False, False, False, True]
State prediction error at timestep 581 is tensor(7.3505e-05, grad_fn=<MseLossBackward0>)
Current timestep = 582. State = [[-0.28650376  0.12712985]]. Action = [[-0.02899645  0.04902213 -0.09154057 -0.7816653 ]]. Reward = [0.]
Curr episode timestep = 582
Scene graph at timestep 582 is [True, False, False, False, False, True]
State prediction error at timestep 582 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 582 of -1
Current timestep = 583. State = [[-0.2879347   0.12740332]]. Action = [[-0.01251332  0.09060795  0.0577096   0.8268125 ]]. Reward = [0.]
Curr episode timestep = 583
Scene graph at timestep 583 is [True, False, False, False, False, True]
State prediction error at timestep 583 is tensor(8.4907e-05, grad_fn=<MseLossBackward0>)
Current timestep = 584. State = [[-0.2892773   0.12953115]]. Action = [[ 0.09651142  0.0692995  -0.05525652 -0.3535245 ]]. Reward = [0.]
Curr episode timestep = 584
Scene graph at timestep 584 is [True, False, False, False, False, True]
State prediction error at timestep 584 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 584 of -1
Current timestep = 585. State = [[-0.2900287   0.13086405]]. Action = [[ 0.09721594 -0.01836061 -0.09172761 -0.09537196]]. Reward = [0.]
Curr episode timestep = 585
Scene graph at timestep 585 is [True, False, False, False, False, True]
State prediction error at timestep 585 is tensor(8.4115e-05, grad_fn=<MseLossBackward0>)
Current timestep = 586. State = [[-0.28951108  0.13101286]]. Action = [[ 0.06184528  0.04676033 -0.06201713 -0.04012907]]. Reward = [0.]
Curr episode timestep = 586
Scene graph at timestep 586 is [True, False, False, False, False, True]
State prediction error at timestep 586 is tensor(5.8861e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 586 of -1
Current timestep = 587. State = [[-0.2881993   0.13207328]]. Action = [[-0.08381931  0.0571524   0.08636964  0.23151779]]. Reward = [0.]
Curr episode timestep = 587
Scene graph at timestep 587 is [True, False, False, False, False, True]
State prediction error at timestep 587 is tensor(2.9348e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 587 of -1
Current timestep = 588. State = [[-0.2887565   0.13433261]]. Action = [[ 0.06406122 -0.02328043 -0.00402214  0.3353789 ]]. Reward = [0.]
Curr episode timestep = 588
Scene graph at timestep 588 is [True, False, False, False, False, True]
State prediction error at timestep 588 is tensor(3.3008e-06, grad_fn=<MseLossBackward0>)
Current timestep = 589. State = [[-0.28834108  0.13483405]]. Action = [[-0.02802084 -0.025194    0.01084433 -0.7111027 ]]. Reward = [0.]
Curr episode timestep = 589
Scene graph at timestep 589 is [True, False, False, False, False, True]
State prediction error at timestep 589 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 589 of -1
Current timestep = 590. State = [[-0.28819627  0.1350027 ]]. Action = [[-0.06365279  0.04880441  0.09158871 -0.4356569 ]]. Reward = [0.]
Curr episode timestep = 590
Scene graph at timestep 590 is [True, False, False, False, False, True]
State prediction error at timestep 590 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 590 of -1
Current timestep = 591. State = [[-0.28885627  0.13621177]]. Action = [[ 0.0815944  -0.06385769 -0.07758473  0.18565822]]. Reward = [0.]
Curr episode timestep = 591
Scene graph at timestep 591 is [True, False, False, False, False, True]
State prediction error at timestep 591 is tensor(1.8831e-05, grad_fn=<MseLossBackward0>)
Current timestep = 592. State = [[-0.28841442  0.13610564]]. Action = [[ 0.05620088 -0.08966798  0.00728531 -0.34222895]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 592 is [True, False, False, False, False, True]
State prediction error at timestep 592 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 592 of -1
Current timestep = 593. State = [[-0.2863029   0.13423288]]. Action = [[-0.04480215  0.00682181  0.00291984  0.28462577]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 593 is [True, False, False, False, False, True]
State prediction error at timestep 593 is tensor(4.3849e-05, grad_fn=<MseLossBackward0>)
Current timestep = 594. State = [[-0.28523436  0.13385981]]. Action = [[ 0.04777075 -0.09169581 -0.03955697  0.22330403]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 594 is [True, False, False, False, False, True]
State prediction error at timestep 594 is tensor(3.3096e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 594 of -1
Current timestep = 595. State = [[-0.28275874  0.13155101]]. Action = [[-0.03376038 -0.04372047  0.07489214 -0.74417186]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 595 is [True, False, False, False, False, True]
State prediction error at timestep 595 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 596. State = [[-0.28148377  0.12973702]]. Action = [[ 0.00921686  0.07034642  0.08949559 -0.8111004 ]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 596 is [True, False, False, False, False, True]
State prediction error at timestep 596 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 596 of -1
Current timestep = 597. State = [[-0.28059384  0.12985812]]. Action = [[ 0.09597366 -0.07000676  0.08840186  0.30682683]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 597 is [True, False, False, False, False, True]
State prediction error at timestep 597 is tensor(1.3213e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 597 of -1
Current timestep = 598. State = [[-0.27846098  0.12805848]]. Action = [[-0.04278696 -0.02706294  0.0337723   0.32040942]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 598 is [True, False, False, False, False, True]
State prediction error at timestep 598 is tensor(1.2600e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 598 of -1
Current timestep = 599. State = [[-0.27735808  0.12594517]]. Action = [[-0.09033621 -0.06735179 -0.08692337 -0.95653623]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 599 is [True, False, False, False, False, True]
State prediction error at timestep 599 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 600. State = [[-0.27818754  0.12320811]]. Action = [[ 0.06901569  0.02512682 -0.07964194 -0.8600392 ]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 600 is [True, False, False, False, True, False]
State prediction error at timestep 600 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 600 of -1
Current timestep = 601. State = [[-0.27777448  0.12180585]]. Action = [[ 0.02567584 -0.06224332 -0.03823539 -0.822817  ]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 601 is [True, False, False, False, True, False]
State prediction error at timestep 601 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 602. State = [[-0.27678227  0.11948393]]. Action = [[-0.0547205   0.0725571  -0.09443884 -0.1989137 ]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 602 is [True, False, False, False, True, False]
State prediction error at timestep 602 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 602 of -1
Current timestep = 603. State = [[-0.27701527  0.1191576 ]]. Action = [[-0.08142124 -0.02200209 -0.08779179 -0.46673453]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 603 is [True, False, False, False, True, False]
State prediction error at timestep 603 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 603 of -1
Current timestep = 604. State = [[-0.27805352  0.11868894]]. Action = [[ 0.05638716 -0.01317343  0.09055214 -0.04996109]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 604 is [True, False, False, False, True, False]
State prediction error at timestep 604 is tensor(3.6343e-05, grad_fn=<MseLossBackward0>)
Current timestep = 605. State = [[-0.27806625  0.1180188 ]]. Action = [[-0.0335003   0.03273176  0.05249905  0.97579503]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 605 is [True, False, False, False, True, False]
State prediction error at timestep 605 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 605 of -1
Current timestep = 606. State = [[-0.27832672  0.11822286]]. Action = [[-0.01124113  0.00121701 -0.08241726 -0.7619125 ]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 606 is [True, False, False, False, True, False]
State prediction error at timestep 606 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 607. State = [[-0.27853128  0.11844283]]. Action = [[-0.06808782  0.06977203  0.07909558 -0.7636687 ]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 607 is [True, False, False, False, True, False]
State prediction error at timestep 607 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 607 of -1
Current timestep = 608. State = [[-0.27998126  0.12014889]]. Action = [[-0.08284665 -0.07849419  0.01236181  0.29892755]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 608 is [True, False, False, False, True, False]
State prediction error at timestep 608 is tensor(2.7385e-05, grad_fn=<MseLossBackward0>)
Current timestep = 609. State = [[-0.2815841   0.11975206]]. Action = [[ 0.04392902 -0.07507347  0.0683789  -0.7285973 ]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 609 is [True, False, False, False, True, False]
State prediction error at timestep 609 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 610. State = [[-0.2819254  0.1176919]]. Action = [[ 0.03084224 -0.08359174 -0.0709492   0.0583638 ]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 610 is [True, False, False, False, True, False]
State prediction error at timestep 610 is tensor(2.8535e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 610 of -1
Current timestep = 611. State = [[-0.28186756  0.11434899]]. Action = [[ 0.0956376   0.06145509 -0.05852202 -0.88224506]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 611 is [True, False, False, False, True, False]
State prediction error at timestep 611 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 612. State = [[-0.28152505  0.1137535 ]]. Action = [[-0.07044151  0.00760803 -0.00725736  0.62708366]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 612 is [True, False, False, False, True, False]
State prediction error at timestep 612 is tensor(4.4399e-06, grad_fn=<MseLossBackward0>)
Current timestep = 613. State = [[-0.2814671   0.11368506]]. Action = [[-0.0626396  -0.032432   -0.0849775  -0.54240936]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 613 is [True, False, False, False, True, False]
State prediction error at timestep 613 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 613 of -1
Current timestep = 614. State = [[-0.28211305  0.11321498]]. Action = [[-7.8162387e-02 -4.0727854e-04  2.0406283e-02  8.7605476e-01]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 614 is [True, False, False, False, True, False]
State prediction error at timestep 614 is tensor(2.5981e-05, grad_fn=<MseLossBackward0>)
Current timestep = 615. State = [[-0.28345585  0.11287746]]. Action = [[ 0.05049939 -0.04597165  0.03794677  0.5680834 ]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 615 is [True, False, False, False, True, False]
State prediction error at timestep 615 is tensor(9.7523e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 615 of -1
Current timestep = 616. State = [[-0.28358614  0.11179131]]. Action = [[-0.03832706 -0.03981623  0.05930985 -0.13172013]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 616 is [True, False, False, False, True, False]
State prediction error at timestep 616 is tensor(5.2730e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 616 of -1
Current timestep = 617. State = [[-0.28425178  0.11017909]]. Action = [[-0.07511376  0.08048559 -0.04274864  0.60013914]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 617 is [True, False, False, False, True, False]
State prediction error at timestep 617 is tensor(2.7834e-05, grad_fn=<MseLossBackward0>)
Current timestep = 618. State = [[-0.2854946   0.11083464]]. Action = [[ 0.04645827 -0.05392923  0.08620019 -0.02722925]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 618 is [True, False, False, False, True, False]
State prediction error at timestep 618 is tensor(1.5484e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 618 of -1
Current timestep = 619. State = [[-0.2855361   0.11032937]]. Action = [[ 0.03787591  0.07502759  0.09483618 -0.623681  ]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 619 is [True, False, False, False, True, False]
State prediction error at timestep 619 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 619 of -1
Current timestep = 620. State = [[-0.28565046  0.11058675]]. Action = [[-0.02844626  0.06918574 -0.03787586  0.71379685]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 620 is [True, False, False, False, True, False]
State prediction error at timestep 620 is tensor(1.5796e-05, grad_fn=<MseLossBackward0>)
Current timestep = 621. State = [[-0.28667694  0.11217529]]. Action = [[ 0.01143278 -0.09915287 -0.08315828  0.02384102]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 621 is [True, False, False, False, True, False]
State prediction error at timestep 621 is tensor(2.7611e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 621 of -1
Current timestep = 622. State = [[-0.28657797  0.1118783 ]]. Action = [[-0.06236717  0.08236554  0.02577095 -0.5791518 ]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 622 is [True, False, False, False, True, False]
State prediction error at timestep 622 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of -1
Current timestep = 623. State = [[-0.28743684  0.11332341]]. Action = [[-0.06347808 -0.08283596  0.07874698 -0.2660876 ]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 623 is [True, False, False, False, True, False]
State prediction error at timestep 623 is tensor(7.0270e-05, grad_fn=<MseLossBackward0>)
Current timestep = 624. State = [[-0.28840977  0.11281729]]. Action = [[-0.01413194 -0.08034392 -0.04190332  0.6761745 ]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 624 is [True, False, False, False, True, False]
State prediction error at timestep 624 is tensor(7.2638e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 624 of -1
Current timestep = 625. State = [[-0.28924167  0.11041849]]. Action = [[-0.07664995 -0.02644786 -0.08680894  0.6388173 ]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 625 is [True, False, False, False, True, False]
State prediction error at timestep 625 is tensor(1.7153e-05, grad_fn=<MseLossBackward0>)
Current timestep = 626. State = [[-0.29177064  0.10769182]]. Action = [[-0.01717384  0.02107795  0.06643856  0.6220517 ]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 626 is [True, False, False, False, True, False]
State prediction error at timestep 626 is tensor(2.1492e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 626 of -1
Current timestep = 627. State = [[-0.29574865  0.10559551]]. Action = [[ 0.04484529 -0.05154488 -0.07596471 -0.9471237 ]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 627 is [True, False, False, False, True, False]
State prediction error at timestep 627 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 627 of -1
Current timestep = 628. State = [[-0.2980303   0.10257562]]. Action = [[ 0.0376356  -0.06165675 -0.05606532  0.8573545 ]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 628 is [True, False, False, False, True, False]
State prediction error at timestep 628 is tensor(5.3823e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 628 of -1
Current timestep = 629. State = [[-0.29852208  0.09981401]]. Action = [[-0.08504956  0.05940814  0.056808   -0.3094198 ]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 629 is [True, False, False, False, True, False]
State prediction error at timestep 629 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 630. State = [[-0.30006444  0.09933063]]. Action = [[-0.0566125  -0.04542971 -0.06830868  0.72335005]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 630 is [True, False, False, False, True, False]
State prediction error at timestep 630 is tensor(3.1136e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 630 of -1
Current timestep = 631. State = [[-0.30282867  0.09792869]]. Action = [[ 0.01368217  0.04002371 -0.05510416 -0.5668084 ]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 631 is [True, False, False, False, True, False]
State prediction error at timestep 631 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 632. State = [[-0.30396685  0.09754771]]. Action = [[0.02915961 0.01271397 0.05017764 0.7638415 ]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 632 is [True, False, False, False, True, False]
State prediction error at timestep 632 is tensor(1.2786e-05, grad_fn=<MseLossBackward0>)
Current timestep = 633. State = [[-0.304155    0.09769359]]. Action = [[ 0.03947634 -0.00753659  0.07300157  0.41583395]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 633 is [True, False, False, False, True, False]
State prediction error at timestep 633 is tensor(7.5284e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 633 of -1
Current timestep = 634. State = [[-0.30424207  0.09791069]]. Action = [[-0.03166828  0.04405958 -0.09692997  0.89737535]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 634 is [True, False, False, False, True, False]
State prediction error at timestep 634 is tensor(9.2512e-05, grad_fn=<MseLossBackward0>)
Current timestep = 635. State = [[-0.30460027  0.09845828]]. Action = [[ 0.01485173  0.01585232 -0.01304625 -0.9129016 ]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 635 is [True, False, False, False, True, False]
State prediction error at timestep 635 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 635 of -1
Current timestep = 636. State = [[-0.3048677   0.09895018]]. Action = [[ 0.0965647  -0.03972244  0.02820187  0.9408815 ]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 636 is [True, False, False, False, True, False]
State prediction error at timestep 636 is tensor(4.3773e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 636 of -1
Current timestep = 637. State = [[-0.30465034  0.09868178]]. Action = [[ 0.04347792 -0.07546553 -0.02527849  0.66883874]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 637 is [True, False, False, False, True, False]
State prediction error at timestep 637 is tensor(2.2658e-05, grad_fn=<MseLossBackward0>)
Current timestep = 638. State = [[-0.3033402   0.09717647]]. Action = [[-0.03546792  0.01111017  0.07714396 -0.477867  ]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 638 is [True, False, False, False, True, False]
State prediction error at timestep 638 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 638 of -1
Current timestep = 639. State = [[-0.30307502  0.09675287]]. Action = [[ 0.05492485  0.01596948 -0.02321389  0.12070966]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 639 is [True, False, False, False, True, False]
State prediction error at timestep 639 is tensor(4.3295e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 639 of -1
Current timestep = 640. State = [[-0.3026029   0.09672117]]. Action = [[ 0.04249451 -0.03895205 -0.08283229 -0.65765494]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 640 is [True, False, False, False, True, False]
State prediction error at timestep 640 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 641. State = [[-0.30089673  0.0961552 ]]. Action = [[-0.04741163  0.06482217  0.03239716  0.2564274 ]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 641 is [True, False, False, False, True, False]
State prediction error at timestep 641 is tensor(4.0729e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 641 of -1
Current timestep = 642. State = [[-0.30071035  0.09628915]]. Action = [[ 0.04739084 -0.03639571 -0.01216076  0.7084396 ]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 642 is [True, False, False, False, True, False]
State prediction error at timestep 642 is tensor(1.1051e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 642 of -1
Current timestep = 643. State = [[-0.2999742  0.0963828]]. Action = [[ 0.03141791  0.03586752 -0.07866891  0.7339026 ]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 643 is [True, False, False, False, True, False]
State prediction error at timestep 643 is tensor(3.4610e-05, grad_fn=<MseLossBackward0>)
Current timestep = 644. State = [[-0.29936248  0.09664012]]. Action = [[-0.07958242 -0.06660508  0.01440587  0.7343402 ]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 644 is [True, False, False, False, True, False]
State prediction error at timestep 644 is tensor(1.3394e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 644 of -1
Current timestep = 645. State = [[-0.29884592  0.09674956]]. Action = [[ 0.02356006  0.09144074 -0.07380496 -0.41212505]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 645 is [True, False, False, False, True, False]
State prediction error at timestep 645 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 645 of -1
Current timestep = 646. State = [[-0.29898342  0.09684567]]. Action = [[-0.02678349 -0.01684748 -0.03449208  0.02891576]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 646 is [True, False, False, False, True, False]
State prediction error at timestep 646 is tensor(3.7722e-05, grad_fn=<MseLossBackward0>)
Current timestep = 647. State = [[-0.2990102   0.09683681]]. Action = [[-0.02751211 -0.066653   -0.05175951  0.30197394]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 647 is [True, False, False, False, True, False]
State prediction error at timestep 647 is tensor(2.1049e-05, grad_fn=<MseLossBackward0>)
Current timestep = 648. State = [[-0.29903573  0.0968791 ]]. Action = [[-0.07237057  0.05345409 -0.05559195  0.73636734]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 648 is [True, False, False, False, True, False]
State prediction error at timestep 648 is tensor(1.1395e-05, grad_fn=<MseLossBackward0>)
Current timestep = 649. State = [[-0.3000286   0.09704924]]. Action = [[-0.0991238  -0.06284098  0.02459963 -0.94831413]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 649 is [True, False, False, False, True, False]
State prediction error at timestep 649 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 650. State = [[-0.30183315  0.09613595]]. Action = [[ 0.08069024 -0.06534109 -0.07523036  0.80106425]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 650 is [True, False, False, False, True, False]
State prediction error at timestep 650 is tensor(4.3135e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 650 of -1
Current timestep = 651. State = [[-0.30211136  0.09424575]]. Action = [[ 0.05153442 -0.02892779 -0.03206705  0.19402933]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 651 is [True, False, False, False, True, False]
State prediction error at timestep 651 is tensor(1.9719e-05, grad_fn=<MseLossBackward0>)
Current timestep = 652. State = [[-0.3015802   0.09249275]]. Action = [[ 0.07898482  0.03899456 -0.02613162  0.8536619 ]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 652 is [True, False, False, False, True, False]
State prediction error at timestep 652 is tensor(2.5130e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 652 of -1
Current timestep = 653. State = [[-0.30065927  0.09214726]]. Action = [[ 0.03978894 -0.07730488 -0.06598842 -0.37628055]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 653 is [True, False, False, False, True, False]
State prediction error at timestep 653 is tensor(9.4641e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 653 of -1
Current timestep = 654. State = [[-0.2986277   0.08904277]]. Action = [[-0.00267573 -0.00917026 -0.07040362 -0.20607322]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 654 is [True, False, False, False, True, False]
State prediction error at timestep 654 is tensor(6.4480e-05, grad_fn=<MseLossBackward0>)
Current timestep = 655. State = [[-0.2977382   0.08710495]]. Action = [[ 0.08988494  0.04228128 -0.0770296   0.57845175]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 655 is [True, False, False, False, True, False]
State prediction error at timestep 655 is tensor(1.9815e-05, grad_fn=<MseLossBackward0>)
Current timestep = 656. State = [[-0.29682565  0.08623544]]. Action = [[0.04360832 0.05685177 0.06721149 0.8427503 ]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 656 is [True, False, False, False, True, False]
State prediction error at timestep 656 is tensor(3.3248e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 656 of -1
Current timestep = 657. State = [[-0.29588655  0.08645705]]. Action = [[-0.02752712 -0.02194264  0.03151769  0.46167064]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 657 is [True, False, False, False, True, False]
State prediction error at timestep 657 is tensor(7.6949e-09, grad_fn=<MseLossBackward0>)
Current timestep = 658. State = [[-0.29588675  0.08634724]]. Action = [[0.03899052 0.07249134 0.02485889 0.5237851 ]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 658 is [True, False, False, False, True, False]
State prediction error at timestep 658 is tensor(4.0953e-06, grad_fn=<MseLossBackward0>)
Current timestep = 659. State = [[-0.29516524  0.08697076]]. Action = [[0.06572182 0.03402939 0.05372968 0.1917131 ]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 659 is [True, False, False, False, True, False]
State prediction error at timestep 659 is tensor(3.3894e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 659 of -1
Current timestep = 660. State = [[-0.29351562  0.08798511]]. Action = [[ 0.0164979   0.02850009 -0.04869546  0.35690832]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 660 is [True, False, False, False, True, False]
State prediction error at timestep 660 is tensor(3.3564e-06, grad_fn=<MseLossBackward0>)
Current timestep = 661. State = [[-0.29211402  0.08919506]]. Action = [[ 0.0416097  -0.05015951  0.04672425 -0.58802646]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 661 is [True, False, False, False, True, False]
State prediction error at timestep 661 is tensor(7.4093e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 661 of -1
Current timestep = 662. State = [[-0.29076567  0.08922569]]. Action = [[ 0.08261874 -0.0316002   0.08737076  0.00830901]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 662 is [True, False, False, False, True, False]
State prediction error at timestep 662 is tensor(2.3459e-05, grad_fn=<MseLossBackward0>)
Current timestep = 663. State = [[-0.2879131   0.08912665]]. Action = [[ 0.00611399 -0.00125562 -0.07531606  0.09720814]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 663 is [True, False, False, False, True, False]
State prediction error at timestep 663 is tensor(9.2598e-06, grad_fn=<MseLossBackward0>)
Current timestep = 664. State = [[-0.28570512  0.08946583]]. Action = [[-0.08321274  0.08647079  0.07798705 -0.54076225]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 664 is [True, False, False, False, True, False]
State prediction error at timestep 664 is tensor(8.0548e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 664 of -1
Current timestep = 665. State = [[-0.28579208  0.09015936]]. Action = [[-0.07863551 -0.06635509 -0.02025259 -0.31747103]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 665 is [True, False, False, False, True, False]
State prediction error at timestep 665 is tensor(7.5100e-05, grad_fn=<MseLossBackward0>)
Current timestep = 666. State = [[-0.2859046   0.09030873]]. Action = [[0.07785939 0.04935659 0.05235756 0.10277164]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 666 is [True, False, False, False, True, False]
State prediction error at timestep 666 is tensor(2.0439e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 666 of -1
Current timestep = 667. State = [[-0.2857102   0.09047435]]. Action = [[ 0.08741588  0.01668286 -0.02623571  0.18658912]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 667 is [True, False, False, False, True, False]
State prediction error at timestep 667 is tensor(4.4478e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 667 of -1
Current timestep = 668. State = [[-0.2843863   0.09096754]]. Action = [[ 0.05574126 -0.06209431  0.09160226 -0.2976004 ]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 668 is [True, False, False, False, True, False]
State prediction error at timestep 668 is tensor(6.8188e-05, grad_fn=<MseLossBackward0>)
Current timestep = 669. State = [[-0.2825024   0.09016151]]. Action = [[ 0.01554252 -0.08427621  0.02590153 -0.27295148]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 669 is [True, False, False, False, True, False]
State prediction error at timestep 669 is tensor(5.0697e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 669 of -1
Current timestep = 670. State = [[-0.2806034   0.08812416]]. Action = [[ 0.04601235  0.0500726  -0.08893993 -0.39420223]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 670 is [True, False, False, False, True, False]
State prediction error at timestep 670 is tensor(8.8759e-05, grad_fn=<MseLossBackward0>)
Current timestep = 671. State = [[-0.27818823  0.08787225]]. Action = [[-0.03233247 -0.07404602 -0.03161429 -0.24769384]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 671 is [True, False, False, False, True, False]
State prediction error at timestep 671 is tensor(4.5157e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 671 of -1
Current timestep = 672. State = [[-0.2765417   0.08660606]]. Action = [[ 0.0886345   0.05786373 -0.02796242  0.22139978]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 672 is [True, False, False, False, True, False]
State prediction error at timestep 672 is tensor(3.6798e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 672 of -1
Current timestep = 673. State = [[-0.27362898  0.08705301]]. Action = [[ 0.05923516  0.03354806 -0.04960438  0.43360353]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 673 is [True, False, False, False, True, False]
State prediction error at timestep 673 is tensor(5.0741e-05, grad_fn=<MseLossBackward0>)
Current timestep = 674. State = [[-0.27094236  0.08781711]]. Action = [[-0.03878399 -0.05545571  0.03089429  0.11933649]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 674 is [True, False, False, False, True, False]
State prediction error at timestep 674 is tensor(1.2642e-05, grad_fn=<MseLossBackward0>)
Current timestep = 675. State = [[-0.26977092  0.08735146]]. Action = [[-0.0731694  -0.08987075  0.06982356  0.3686682 ]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 675 is [True, False, False, False, True, False]
State prediction error at timestep 675 is tensor(1.0421e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 675 of -1
Current timestep = 676. State = [[-0.26885635  0.08531347]]. Action = [[ 0.0513052   0.0229836  -0.08428966 -0.8805393 ]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 676 is [True, False, False, False, True, False]
State prediction error at timestep 676 is tensor(5.3257e-05, grad_fn=<MseLossBackward0>)
Current timestep = 677. State = [[-0.26824647  0.08412752]]. Action = [[-0.04379211 -0.03302179 -0.09482849 -0.6486456 ]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 677 is [True, False, False, False, True, False]
State prediction error at timestep 677 is tensor(7.7424e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 677 of -1
Current timestep = 678. State = [[-0.2676468   0.08296461]]. Action = [[ 0.02731148 -0.01488526 -0.01399594 -0.5191016 ]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 678 is [True, False, False, False, True, False]
State prediction error at timestep 678 is tensor(6.4506e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 678 of -1
Current timestep = 679. State = [[-0.2669529   0.08177327]]. Action = [[ 0.07598131  0.09545118  0.07757936 -0.49425876]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 679 is [True, False, False, False, True, False]
State prediction error at timestep 679 is tensor(5.4506e-05, grad_fn=<MseLossBackward0>)
Current timestep = 680. State = [[-0.26685742  0.08189137]]. Action = [[ 0.06952598  0.01355748 -0.09161932 -0.7752592 ]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 680 is [True, False, False, False, True, False]
State prediction error at timestep 680 is tensor(3.6664e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 680 of -1
Current timestep = 681. State = [[-0.26527423  0.08234969]]. Action = [[-0.09000668 -0.0922557   0.03605513 -0.4124006 ]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 681 is [True, False, False, False, True, False]
State prediction error at timestep 681 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 682. State = [[-0.26483732  0.08151896]]. Action = [[-0.07832342  0.00875263 -0.00654694 -0.14797926]]. Reward = [0.]
Curr episode timestep = 682
Scene graph at timestep 682 is [True, False, False, False, True, False]
State prediction error at timestep 682 is tensor(1.0964e-05, grad_fn=<MseLossBackward0>)
Current timestep = 683. State = [[-0.2649409   0.08123986]]. Action = [[-0.00882812  0.00917079 -0.07639614  0.5992812 ]]. Reward = [0.]
Curr episode timestep = 683
Scene graph at timestep 683 is [True, False, False, False, True, False]
State prediction error at timestep 683 is tensor(1.2804e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 683 of -1
Current timestep = 684. State = [[-0.26507458  0.08111786]]. Action = [[-0.07328223 -0.08230379 -0.05805626 -0.66651726]]. Reward = [0.]
Curr episode timestep = 684
Scene graph at timestep 684 is [True, False, False, False, True, False]
State prediction error at timestep 684 is tensor(8.3862e-05, grad_fn=<MseLossBackward0>)
Current timestep = 685. State = [[-0.26635504  0.07901675]]. Action = [[-0.0029254  -0.06828743  0.0768242   0.31597376]]. Reward = [0.]
Curr episode timestep = 685
Scene graph at timestep 685 is [True, False, False, False, True, False]
State prediction error at timestep 685 is tensor(3.1866e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 685 of -1
Current timestep = 686. State = [[-0.26714176  0.0756995 ]]. Action = [[-0.01990124 -0.08625753  0.03218473 -0.33577472]]. Reward = [0.]
Curr episode timestep = 686
Scene graph at timestep 686 is [True, False, False, False, True, False]
State prediction error at timestep 686 is tensor(9.4086e-05, grad_fn=<MseLossBackward0>)
Current timestep = 687. State = [[-0.26755768  0.07152823]]. Action = [[ 0.03891822 -0.03601732  0.09379826 -0.68419623]]. Reward = [0.]
Curr episode timestep = 687
Scene graph at timestep 687 is [True, False, False, False, True, False]
State prediction error at timestep 687 is tensor(9.1451e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 687 of -1
Current timestep = 688. State = [[-0.2675245   0.06737573]]. Action = [[ 0.05867445 -0.06959347  0.02836902 -0.08809012]]. Reward = [0.]
Curr episode timestep = 688
Scene graph at timestep 688 is [True, False, False, False, True, False]
State prediction error at timestep 688 is tensor(1.0068e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 688 of -1
Current timestep = 689. State = [[-0.26611114  0.06364039]]. Action = [[-0.03111614  0.08446794 -0.00213262 -0.02358079]]. Reward = [0.]
Curr episode timestep = 689
Scene graph at timestep 689 is [True, False, False, False, True, False]
State prediction error at timestep 689 is tensor(1.0882e-05, grad_fn=<MseLossBackward0>)
Current timestep = 690. State = [[-0.26595113  0.06332812]]. Action = [[ 0.00659321  0.06233885 -0.02573921  0.8201361 ]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 690 is [True, False, False, False, True, False]
State prediction error at timestep 690 is tensor(1.0157e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 690 of -1
Current timestep = 691. State = [[-0.26607472  0.06339318]]. Action = [[ 0.03974421  0.03266568 -0.04758431  0.33809114]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 691 is [True, False, False, False, True, False]
State prediction error at timestep 691 is tensor(1.0197e-06, grad_fn=<MseLossBackward0>)
Current timestep = 692. State = [[-0.26618543  0.06403299]]. Action = [[-0.04655281  0.06163385 -0.09650364 -0.34006155]]. Reward = [0.]
Curr episode timestep = 692
Scene graph at timestep 692 is [True, False, False, False, True, False]
State prediction error at timestep 692 is tensor(3.7191e-05, grad_fn=<MseLossBackward0>)
Current timestep = 693. State = [[-0.2668887   0.06571114]]. Action = [[-0.09186318 -0.0194544   0.03408546 -0.3776878 ]]. Reward = [0.]
Curr episode timestep = 693
Scene graph at timestep 693 is [True, False, False, False, True, False]
State prediction error at timestep 693 is tensor(2.5574e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 693 of -1
Current timestep = 694. State = [[-0.2674736   0.06674809]]. Action = [[-0.02107228  0.02052531 -0.00507453  0.42096448]]. Reward = [0.]
Curr episode timestep = 694
Scene graph at timestep 694 is [True, False, False, False, True, False]
State prediction error at timestep 694 is tensor(3.9831e-08, grad_fn=<MseLossBackward0>)
Current timestep = 695. State = [[-0.26829728  0.06783164]]. Action = [[-0.09610488 -0.0580964  -0.08267085  0.95107675]]. Reward = [0.]
Curr episode timestep = 695
Scene graph at timestep 695 is [True, False, False, False, True, False]
State prediction error at timestep 695 is tensor(9.0581e-06, grad_fn=<MseLossBackward0>)
Current timestep = 696. State = [[-0.26999053  0.06770507]]. Action = [[ 0.00613193 -0.05293272 -0.02285473 -0.7284419 ]]. Reward = [0.]
Curr episode timestep = 696
Scene graph at timestep 696 is [True, False, False, False, True, False]
State prediction error at timestep 696 is tensor(6.0901e-05, grad_fn=<MseLossBackward0>)
Current timestep = 697. State = [[-0.27097356  0.06690958]]. Action = [[ 0.05872758 -0.01245111 -0.04440815 -0.600376  ]]. Reward = [0.]
Curr episode timestep = 697
Scene graph at timestep 697 is [True, False, False, False, True, False]
State prediction error at timestep 697 is tensor(7.4266e-05, grad_fn=<MseLossBackward0>)
Current timestep = 698. State = [[-0.27166522  0.06600279]]. Action = [[ 0.06460131 -0.065014    0.08179509  0.3828473 ]]. Reward = [0.]
Curr episode timestep = 698
Scene graph at timestep 698 is [True, False, False, False, True, False]
State prediction error at timestep 698 is tensor(1.0409e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 698 of -1
Current timestep = 699. State = [[-0.27096525  0.06400754]]. Action = [[-0.09133781 -0.0459      0.02080937 -0.4593886 ]]. Reward = [0.]
Curr episode timestep = 699
Scene graph at timestep 699 is [True, False, False, False, True, False]
State prediction error at timestep 699 is tensor(9.0431e-05, grad_fn=<MseLossBackward0>)
Current timestep = 700. State = [[-0.27195543  0.06164755]]. Action = [[ 0.06771227 -0.09136993 -0.045198    0.7461697 ]]. Reward = [0.]
Curr episode timestep = 700
Scene graph at timestep 700 is [True, False, False, False, True, False]
State prediction error at timestep 700 is tensor(1.1915e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 700 of -1
Current timestep = 701. State = [[-0.27180162  0.05747204]]. Action = [[ 0.03128416  0.03276198 -0.08852095  0.5409188 ]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 701 is [True, False, False, False, True, False]
State prediction error at timestep 701 is tensor(2.1801e-05, grad_fn=<MseLossBackward0>)
Current timestep = 702. State = [[-0.27126     0.05604187]]. Action = [[ 0.09141637  0.05311931  0.00656427 -0.09064788]]. Reward = [0.]
Curr episode timestep = 702
Scene graph at timestep 702 is [True, False, False, False, True, False]
State prediction error at timestep 702 is tensor(1.3756e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 702 of -1
Current timestep = 703. State = [[-0.27074894  0.05584582]]. Action = [[0.08104741 0.06473628 0.01474776 0.8276932 ]]. Reward = [0.]
Curr episode timestep = 703
Scene graph at timestep 703 is [True, False, False, False, True, False]
State prediction error at timestep 703 is tensor(1.6332e-05, grad_fn=<MseLossBackward0>)
Current timestep = 704. State = [[-0.26955134  0.05677233]]. Action = [[-0.01530696  0.08467353  0.0188389   0.69878554]]. Reward = [0.]
Curr episode timestep = 704
Scene graph at timestep 704 is [True, False, False, False, True, False]
State prediction error at timestep 704 is tensor(3.6255e-05, grad_fn=<MseLossBackward0>)
Current timestep = 705. State = [[-0.26904532  0.0581293 ]]. Action = [[ 0.07066237  0.03980044  0.02192441 -0.3125208 ]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 705 is [True, False, False, False, True, False]
State prediction error at timestep 705 is tensor(3.2766e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 705 of -1
Current timestep = 706. State = [[-0.2668242   0.06051655]]. Action = [[-0.00275972 -0.06983416  0.04024885  0.53813446]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 706 is [True, False, False, False, True, False]
State prediction error at timestep 706 is tensor(6.4132e-05, grad_fn=<MseLossBackward0>)
Current timestep = 707. State = [[-0.26552093  0.06082704]]. Action = [[ 0.06288327 -0.02197369  0.02503607  0.8538792 ]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 707 is [True, False, False, False, True, False]
State prediction error at timestep 707 is tensor(3.0632e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 707 of -1
Current timestep = 708. State = [[-0.2638298   0.06087659]]. Action = [[-0.0724896   0.07210734 -0.05226979  0.05101013]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 708 is [True, False, False, False, True, False]
State prediction error at timestep 708 is tensor(4.1872e-05, grad_fn=<MseLossBackward0>)
Current timestep = 709. State = [[-0.26347673  0.06145237]]. Action = [[ 0.02207865 -0.07168563  0.00736769  0.4832945 ]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 709 is [True, False, False, False, True, False]
State prediction error at timestep 709 is tensor(3.9126e-05, grad_fn=<MseLossBackward0>)
Current timestep = 710. State = [[-0.26318818  0.06109508]]. Action = [[ 0.02779434 -0.08314575  0.0233588   0.9257382 ]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 710 is [True, False, False, False, True, False]
State prediction error at timestep 710 is tensor(1.5494e-05, grad_fn=<MseLossBackward0>)
Current timestep = 711. State = [[-0.26186493  0.05934115]]. Action = [[-0.0192699   0.06189909  0.00776051 -0.6020301 ]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 711 is [True, False, False, False, True, False]
State prediction error at timestep 711 is tensor(5.5682e-05, grad_fn=<MseLossBackward0>)
Current timestep = 712. State = [[-0.2618101   0.05933503]]. Action = [[ 0.03301444 -0.09470834 -0.06902087  0.8476149 ]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 712 is [True, False, False, False, True, False]
State prediction error at timestep 712 is tensor(9.8062e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 712 of -1
Current timestep = 713. State = [[-0.26033023  0.05730436]]. Action = [[ 0.08025322  0.0021375   0.03498643 -0.40476674]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 713 is [True, False, False, False, True, False]
State prediction error at timestep 713 is tensor(7.8286e-05, grad_fn=<MseLossBackward0>)
Current timestep = 714. State = [[-0.2581574   0.05547753]]. Action = [[-0.06801212  0.06048613 -0.08080016  0.7878889 ]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 714 is [True, False, False, False, True, False]
State prediction error at timestep 714 is tensor(2.4180e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 714 of -1
Current timestep = 715. State = [[-0.25787294  0.05598962]]. Action = [[ 0.02384867  0.08339261 -0.03868929 -0.22540444]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 715 is [True, False, False, False, True, False]
State prediction error at timestep 715 is tensor(1.0907e-05, grad_fn=<MseLossBackward0>)
Current timestep = 716. State = [[-0.25726715  0.05725882]]. Action = [[ 0.03863164 -0.06119324  0.04809346 -0.41134048]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 716 is [True, False, False, False, True, False]
State prediction error at timestep 716 is tensor(3.2181e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 716 of 1
Current timestep = 717. State = [[-0.25664708  0.05708454]]. Action = [[-0.03080597  0.06242862 -0.0610463   0.4427563 ]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 717 is [True, False, False, False, True, False]
State prediction error at timestep 717 is tensor(7.2380e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 717 of 1
Current timestep = 718. State = [[-0.25651038  0.05821471]]. Action = [[-0.03992004  0.069731   -0.08365534  0.64672697]]. Reward = [0.]
Curr episode timestep = 718
Scene graph at timestep 718 is [True, False, False, False, True, False]
State prediction error at timestep 718 is tensor(2.4032e-05, grad_fn=<MseLossBackward0>)
Current timestep = 719. State = [[-0.25719362  0.06020635]]. Action = [[ 0.05486304 -0.08392327  0.03484835  0.09815931]]. Reward = [0.]
Curr episode timestep = 719
Scene graph at timestep 719 is [True, False, False, False, True, False]
State prediction error at timestep 719 is tensor(5.5937e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 719 of 1
Current timestep = 720. State = [[-0.25666076  0.06020343]]. Action = [[-0.03103389  0.02288798  0.05161167 -0.75601935]]. Reward = [0.]
Curr episode timestep = 720
Scene graph at timestep 720 is [True, False, False, False, True, False]
State prediction error at timestep 720 is tensor(2.8633e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 720 of 1
Current timestep = 721. State = [[-0.25640565  0.0604025 ]]. Action = [[-0.08321825 -0.03467964  0.06928331 -0.16428667]]. Reward = [0.]
Curr episode timestep = 721
Scene graph at timestep 721 is [True, False, False, False, True, False]
State prediction error at timestep 721 is tensor(7.1125e-06, grad_fn=<MseLossBackward0>)
Current timestep = 722. State = [[-0.25660202  0.06016411]]. Action = [[-0.09213669 -0.09167209  0.09296884 -0.12152255]]. Reward = [0.]
Curr episode timestep = 722
Scene graph at timestep 722 is [True, False, False, False, True, False]
State prediction error at timestep 722 is tensor(1.1906e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 722 of 1
Current timestep = 723. State = [[-0.25790948  0.05883394]]. Action = [[ 0.02860635  0.06636462  0.00346433 -0.16959524]]. Reward = [0.]
Curr episode timestep = 723
Scene graph at timestep 723 is [True, False, False, False, True, False]
State prediction error at timestep 723 is tensor(1.2047e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 723 of 1
Current timestep = 724. State = [[-0.25810352  0.05877414]]. Action = [[ 0.0852633  -0.05557754  0.06890508 -0.5943589 ]]. Reward = [0.]
Curr episode timestep = 724
Scene graph at timestep 724 is [True, False, False, False, True, False]
State prediction error at timestep 724 is tensor(3.7014e-05, grad_fn=<MseLossBackward0>)
Current timestep = 725. State = [[-0.25754505  0.05803246]]. Action = [[ 0.07188124 -0.06088269 -0.04473032  0.93737674]]. Reward = [0.]
Curr episode timestep = 725
Scene graph at timestep 725 is [True, False, False, False, True, False]
State prediction error at timestep 725 is tensor(2.0259e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 725 of 1
Current timestep = 726. State = [[-0.25584957  0.0559157 ]]. Action = [[-0.00184108  0.08613824 -0.01166154  0.34966123]]. Reward = [0.]
Curr episode timestep = 726
Scene graph at timestep 726 is [True, False, False, False, True, False]
State prediction error at timestep 726 is tensor(1.7142e-05, grad_fn=<MseLossBackward0>)
Current timestep = 727. State = [[-0.25580168  0.05601136]]. Action = [[-0.0789334  -0.05303351  0.04279714 -0.91676146]]. Reward = [0.]
Curr episode timestep = 727
Scene graph at timestep 727 is [True, False, False, False, True, False]
State prediction error at timestep 727 is tensor(3.2009e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 727 of 1
Current timestep = 728. State = [[-0.2558334  0.0558783]]. Action = [[0.02368903 0.07486764 0.03677697 0.7010269 ]]. Reward = [0.]
Curr episode timestep = 728
Scene graph at timestep 728 is [True, False, False, False, True, False]
State prediction error at timestep 728 is tensor(5.2222e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of 1
Current timestep = 729. State = [[-0.2559393   0.05611077]]. Action = [[-0.08069329 -0.03329674 -0.00904202 -0.03174615]]. Reward = [0.]
Curr episode timestep = 729
Scene graph at timestep 729 is [True, False, False, False, True, False]
State prediction error at timestep 729 is tensor(5.0760e-05, grad_fn=<MseLossBackward0>)
Current timestep = 730. State = [[-0.25609088  0.05623453]]. Action = [[-0.09625956  0.00650945  0.07483306 -0.27469492]]. Reward = [0.]
Curr episode timestep = 730
Scene graph at timestep 730 is [True, False, False, False, True, False]
State prediction error at timestep 730 is tensor(5.2303e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 730 of 1
Current timestep = 731. State = [[-0.2577569   0.05656899]]. Action = [[-0.09474986 -0.00765703 -0.09484527  0.5254427 ]]. Reward = [0.]
Curr episode timestep = 731
Scene graph at timestep 731 is [True, False, False, False, True, False]
State prediction error at timestep 731 is tensor(2.2128e-06, grad_fn=<MseLossBackward0>)
Current timestep = 732. State = [[-0.26009163  0.05680905]]. Action = [[ 0.00574721 -0.06383257 -0.01234087  0.57884455]]. Reward = [0.]
Curr episode timestep = 732
Scene graph at timestep 732 is [True, False, False, False, True, False]
State prediction error at timestep 732 is tensor(4.3207e-05, grad_fn=<MseLossBackward0>)
Current timestep = 733. State = [[-0.26153412  0.0557116 ]]. Action = [[ 0.04410087  0.0799432  -0.02487056  0.16653693]]. Reward = [0.]
Curr episode timestep = 733
Scene graph at timestep 733 is [True, False, False, False, True, False]
State prediction error at timestep 733 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 733 of 1
Current timestep = 734. State = [[-0.26186895  0.05611747]]. Action = [[ 0.01667769  0.06809232 -0.09384491  0.04759765]]. Reward = [0.]
Curr episode timestep = 734
Scene graph at timestep 734 is [True, False, False, False, True, False]
State prediction error at timestep 734 is tensor(5.8235e-05, grad_fn=<MseLossBackward0>)
Current timestep = 735. State = [[-0.26263174  0.05793975]]. Action = [[ 0.08076651  0.07679313 -0.02686107  0.1673168 ]]. Reward = [0.]
Curr episode timestep = 735
Scene graph at timestep 735 is [True, False, False, False, True, False]
State prediction error at timestep 735 is tensor(7.0464e-05, grad_fn=<MseLossBackward0>)
Current timestep = 736. State = [[-0.2636145   0.06065997]]. Action = [[ 0.00376896  0.08860186 -0.01979046 -0.09877479]]. Reward = [0.]
Curr episode timestep = 736
Scene graph at timestep 736 is [True, False, False, False, True, False]
State prediction error at timestep 736 is tensor(2.1159e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 736 of 1
Current timestep = 737. State = [[-0.2646849   0.06472256]]. Action = [[-0.060473    0.03120764 -0.0229925   0.8226296 ]]. Reward = [0.]
Curr episode timestep = 737
Scene graph at timestep 737 is [True, False, False, False, True, False]
State prediction error at timestep 737 is tensor(5.0262e-06, grad_fn=<MseLossBackward0>)
Current timestep = 738. State = [[-0.26655722  0.06838214]]. Action = [[ 0.07668013  0.03882546 -0.01876973 -0.12986904]]. Reward = [0.]
Curr episode timestep = 738
Scene graph at timestep 738 is [True, False, False, False, True, False]
State prediction error at timestep 738 is tensor(1.2530e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 738 of 1
Current timestep = 739. State = [[-0.26668876  0.07072734]]. Action = [[ 0.04707899  0.04887528  0.01112584 -0.6215117 ]]. Reward = [0.]
Curr episode timestep = 739
Scene graph at timestep 739 is [True, False, False, False, True, False]
State prediction error at timestep 739 is tensor(3.2025e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 739 of 1
Current timestep = 740. State = [[-0.26531214  0.073622  ]]. Action = [[-0.02196656  0.03249338 -0.05321465  0.47942078]]. Reward = [0.]
Curr episode timestep = 740
Scene graph at timestep 740 is [True, False, False, False, True, False]
State prediction error at timestep 740 is tensor(1.1109e-05, grad_fn=<MseLossBackward0>)
Current timestep = 741. State = [[-0.26475424  0.07614442]]. Action = [[ 0.02818262 -0.06551018 -0.08465531 -0.03425938]]. Reward = [0.]
Curr episode timestep = 741
Scene graph at timestep 741 is [True, False, False, False, True, False]
State prediction error at timestep 741 is tensor(2.1638e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 741 of 1
Current timestep = 742. State = [[-0.26438034  0.07639028]]. Action = [[ 0.0691785   0.08429398  0.03658319 -0.7975344 ]]. Reward = [0.]
Curr episode timestep = 742
Scene graph at timestep 742 is [True, False, False, False, True, False]
State prediction error at timestep 742 is tensor(2.8645e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 742 of 1
Current timestep = 743. State = [[-0.26238587  0.07842793]]. Action = [[0.0748424  0.05507835 0.08772177 0.27123034]]. Reward = [0.]
Curr episode timestep = 743
Scene graph at timestep 743 is [True, False, False, False, True, False]
State prediction error at timestep 743 is tensor(3.0094e-05, grad_fn=<MseLossBackward0>)
Current timestep = 744. State = [[-0.25860244  0.08134254]]. Action = [[ 0.03092439 -0.05644694  0.04820385 -0.4970023 ]]. Reward = [0.]
Curr episode timestep = 744
Scene graph at timestep 744 is [True, False, False, False, True, False]
State prediction error at timestep 744 is tensor(7.1576e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 744 of 1
Current timestep = 745. State = [[-0.25486583  0.08299344]]. Action = [[ 0.09320923  0.06805044  0.06868174 -0.11054581]]. Reward = [0.]
Curr episode timestep = 745
Scene graph at timestep 745 is [True, False, False, False, True, False]
State prediction error at timestep 745 is tensor(8.5017e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 745 of 1
Current timestep = 746. State = [[-0.2505286   0.08507125]]. Action = [[-0.08856722 -0.03688728 -0.01927574 -0.7854708 ]]. Reward = [0.]
Curr episode timestep = 746
Scene graph at timestep 746 is [True, False, False, False, True, False]
State prediction error at timestep 746 is tensor(7.4715e-05, grad_fn=<MseLossBackward0>)
Current timestep = 747. State = [[-0.25024387  0.08521193]]. Action = [[-0.00712536  0.04474696 -0.07044573  0.7965236 ]]. Reward = [0.]
Curr episode timestep = 747
Scene graph at timestep 747 is [True, False, False, False, True, False]
State prediction error at timestep 747 is tensor(1.0280e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 747 of 1
Current timestep = 748. State = [[-0.25049403  0.0860773 ]]. Action = [[-0.07001891  0.09058715 -0.08634868  0.15362072]]. Reward = [0.]
Curr episode timestep = 748
Scene graph at timestep 748 is [True, False, False, False, True, False]
State prediction error at timestep 748 is tensor(4.4884e-05, grad_fn=<MseLossBackward0>)
Current timestep = 749. State = [[-0.25206426  0.0892732 ]]. Action = [[ 0.06199528 -0.02532722  0.00170521 -0.936307  ]]. Reward = [0.]
Curr episode timestep = 749
Scene graph at timestep 749 is [True, False, False, False, True, False]
State prediction error at timestep 749 is tensor(2.5746e-06, grad_fn=<MseLossBackward0>)
Current timestep = 750. State = [[-0.25217304  0.09026498]]. Action = [[-0.0639332   0.02770817  0.07434953 -0.8431821 ]]. Reward = [0.]
Curr episode timestep = 750
Scene graph at timestep 750 is [True, False, False, False, True, False]
State prediction error at timestep 750 is tensor(3.5907e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 750 of 1
Current timestep = 751. State = [[-0.25301215  0.09173466]]. Action = [[ 0.00942156 -0.04645916 -0.00119426 -0.32095414]]. Reward = [0.]
Curr episode timestep = 751
Scene graph at timestep 751 is [True, False, False, False, True, False]
State prediction error at timestep 751 is tensor(4.1765e-05, grad_fn=<MseLossBackward0>)
Current timestep = 752. State = [[-0.253139    0.09181287]]. Action = [[-0.03577362  0.08842892 -0.04527171 -0.67138517]]. Reward = [0.]
Curr episode timestep = 752
Scene graph at timestep 752 is [True, False, False, False, True, False]
State prediction error at timestep 752 is tensor(7.1197e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 752 of 1
Current timestep = 753. State = [[-0.25448722  0.0943202 ]]. Action = [[-0.02442759  0.03678519  0.02947164  0.20715368]]. Reward = [0.]
Curr episode timestep = 753
Scene graph at timestep 753 is [True, False, False, False, True, False]
State prediction error at timestep 753 is tensor(7.3165e-05, grad_fn=<MseLossBackward0>)
Current timestep = 754. State = [[-0.2556103  0.0965107]]. Action = [[ 0.05417392 -0.0381667   0.00461292  0.8799465 ]]. Reward = [0.]
Curr episode timestep = 754
Scene graph at timestep 754 is [True, False, False, False, True, False]
State prediction error at timestep 754 is tensor(1.2254e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 754 of 1
Current timestep = 755. State = [[-0.25555304  0.09704152]]. Action = [[ 0.07124767  0.06763511 -0.08863943 -0.75837237]]. Reward = [0.]
Curr episode timestep = 755
Scene graph at timestep 755 is [True, False, False, False, True, False]
State prediction error at timestep 755 is tensor(2.0894e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 755 of 1
Current timestep = 756. State = [[-0.25426206  0.09858969]]. Action = [[-0.03496926  0.05227043 -0.04621139  0.2722485 ]]. Reward = [0.]
Curr episode timestep = 756
Scene graph at timestep 756 is [True, False, False, False, True, False]
State prediction error at timestep 756 is tensor(2.3414e-05, grad_fn=<MseLossBackward0>)
Current timestep = 757. State = [[-0.25356454  0.10116398]]. Action = [[ 0.09266465 -0.09432471  0.01189822  0.89459467]]. Reward = [0.]
Curr episode timestep = 757
Scene graph at timestep 757 is [True, False, False, False, True, False]
State prediction error at timestep 757 is tensor(1.9180e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 757 of 1
Current timestep = 758. State = [[-0.25210518  0.10075215]]. Action = [[ 0.07377882 -0.0121205   0.02859659  0.7350266 ]]. Reward = [0.]
Curr episode timestep = 758
Scene graph at timestep 758 is [True, False, False, False, True, False]
State prediction error at timestep 758 is tensor(5.0075e-05, grad_fn=<MseLossBackward0>)
Current timestep = 759. State = [[-0.2497584   0.10061176]]. Action = [[ 0.06004976  0.02841602 -0.02975281  0.55552423]]. Reward = [0.]
Curr episode timestep = 759
Scene graph at timestep 759 is [True, False, False, False, True, False]
State prediction error at timestep 759 is tensor(5.4526e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 759 of 1
Current timestep = 760. State = [[-0.2463538  0.1015272]]. Action = [[ 0.0887927   0.02565227 -0.0210212  -0.41702282]]. Reward = [0.]
Curr episode timestep = 760
Scene graph at timestep 760 is [True, False, False, False, True, False]
State prediction error at timestep 760 is tensor(9.3328e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 760 of 1
Current timestep = 761. State = [[-0.24141668  0.10383908]]. Action = [[ 0.08313901  0.04448128 -0.05939328  0.8115864 ]]. Reward = [0.]
Curr episode timestep = 761
Scene graph at timestep 761 is [True, False, False, False, True, False]
State prediction error at timestep 761 is tensor(1.8222e-05, grad_fn=<MseLossBackward0>)
Current timestep = 762. State = [[-0.23585887  0.10660042]]. Action = [[-0.0956068   0.06938582 -0.07088171 -0.8707057 ]]. Reward = [0.]
Curr episode timestep = 762
Scene graph at timestep 762 is [True, False, False, False, True, False]
State prediction error at timestep 762 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 762 of 1
Current timestep = 763. State = [[-0.23393263  0.10920472]]. Action = [[-0.02980342 -0.09814455 -0.08616439  0.725472  ]]. Reward = [0.]
Curr episode timestep = 763
Scene graph at timestep 763 is [True, False, False, False, True, False]
State prediction error at timestep 763 is tensor(1.4059e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 763 of 1
Current timestep = 764. State = [[-0.23387524  0.10884779]]. Action = [[-0.03324294 -0.05221172 -0.07998104  0.01460218]]. Reward = [0.]
Curr episode timestep = 764
Scene graph at timestep 764 is [True, False, False, False, True, False]
State prediction error at timestep 764 is tensor(1.3248e-05, grad_fn=<MseLossBackward0>)
Current timestep = 765. State = [[-0.23358466  0.10793734]]. Action = [[-0.02656659 -0.03677416  0.00668804  0.05855095]]. Reward = [0.]
Curr episode timestep = 765
Scene graph at timestep 765 is [True, False, False, False, True, False]
State prediction error at timestep 765 is tensor(1.9151e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 765 of 1
Current timestep = 766. State = [[-0.23320845  0.1064245 ]]. Action = [[-0.06291801  0.04257288  0.07645471 -0.0047251 ]]. Reward = [0.]
Curr episode timestep = 766
Scene graph at timestep 766 is [True, False, False, False, True, False]
State prediction error at timestep 766 is tensor(1.0684e-05, grad_fn=<MseLossBackward0>)
Current timestep = 767. State = [[-0.23360881  0.10612182]]. Action = [[-0.07663951 -0.08668771 -0.06556482  0.0114156 ]]. Reward = [0.]
Curr episode timestep = 767
Scene graph at timestep 767 is [True, False, False, False, True, False]
State prediction error at timestep 767 is tensor(2.0371e-05, grad_fn=<MseLossBackward0>)
Current timestep = 768. State = [[-0.23508142  0.10427061]]. Action = [[-0.09417581  0.06597096 -0.06093316  0.66318464]]. Reward = [0.]
Curr episode timestep = 768
Scene graph at timestep 768 is [True, False, False, False, True, False]
State prediction error at timestep 768 is tensor(3.1346e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 768 of 1
Current timestep = 769. State = [[-0.23722096  0.10452621]]. Action = [[ 0.08562613 -0.0798479   0.03684346  0.38253355]]. Reward = [0.]
Curr episode timestep = 769
Scene graph at timestep 769 is [True, False, False, False, True, False]
State prediction error at timestep 769 is tensor(1.8326e-05, grad_fn=<MseLossBackward0>)
Current timestep = 770. State = [[-0.23733877  0.10239376]]. Action = [[ 0.0746861  -0.08532303 -0.06901635  0.92152405]]. Reward = [0.]
Curr episode timestep = 770
Scene graph at timestep 770 is [True, False, False, False, True, False]
State prediction error at timestep 770 is tensor(2.8558e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 770 of 1
Current timestep = 771. State = [[-0.23621729  0.09938895]]. Action = [[-0.06014607 -0.0304693   0.00204583  0.66278183]]. Reward = [0.]
Curr episode timestep = 771
Scene graph at timestep 771 is [True, False, False, False, True, False]
State prediction error at timestep 771 is tensor(6.3692e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 771 of 1
Current timestep = 772. State = [[-0.23671588  0.09724414]]. Action = [[ 0.01877401  0.07898629 -0.07397707  0.46252906]]. Reward = [0.]
Curr episode timestep = 772
Scene graph at timestep 772 is [True, False, False, False, True, False]
State prediction error at timestep 772 is tensor(1.9527e-05, grad_fn=<MseLossBackward0>)
Current timestep = 773. State = [[-0.2367171   0.09756687]]. Action = [[ 0.03585463 -0.07709555 -0.0562053  -0.26515424]]. Reward = [0.]
Curr episode timestep = 773
Scene graph at timestep 773 is [True, False, False, False, True, False]
State prediction error at timestep 773 is tensor(3.5141e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 773 of 1
Current timestep = 774. State = [[-0.23603803  0.0960664 ]]. Action = [[-0.04991287 -0.06580865 -0.04810899  0.8734479 ]]. Reward = [0.]
Curr episode timestep = 774
Scene graph at timestep 774 is [True, False, False, False, True, False]
State prediction error at timestep 774 is tensor(5.5698e-05, grad_fn=<MseLossBackward0>)
Current timestep = 775. State = [[-0.23620468  0.09331615]]. Action = [[-0.0318296  -0.06819867  0.05269033  0.88936496]]. Reward = [0.]
Curr episode timestep = 775
Scene graph at timestep 775 is [True, False, False, False, True, False]
State prediction error at timestep 775 is tensor(9.4147e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 775 of 1
Current timestep = 776. State = [[-0.2363253   0.08970463]]. Action = [[ 0.09298376 -0.05986096  0.03952544 -0.2909655 ]]. Reward = [0.]
Curr episode timestep = 776
Scene graph at timestep 776 is [True, False, False, False, True, False]
State prediction error at timestep 776 is tensor(7.5416e-05, grad_fn=<MseLossBackward0>)
Current timestep = 777. State = [[-0.23494148  0.08575778]]. Action = [[0.02933726 0.08365124 0.0509732  0.5001019 ]]. Reward = [0.]
Curr episode timestep = 777
Scene graph at timestep 777 is [True, False, False, False, True, False]
State prediction error at timestep 777 is tensor(3.1414e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 777 of 1
Current timestep = 778. State = [[-0.23439513  0.08560744]]. Action = [[-0.03671817  0.09024622  0.00165548  0.6407037 ]]. Reward = [0.]
Curr episode timestep = 778
Scene graph at timestep 778 is [True, False, False, False, True, False]
State prediction error at timestep 778 is tensor(2.7996e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 778 of 1
Current timestep = 779. State = [[-0.23478739  0.08649873]]. Action = [[ 0.08701522 -0.09304812 -0.09172265 -0.37785536]]. Reward = [0.]
Curr episode timestep = 779
Scene graph at timestep 779 is [True, False, False, False, True, False]
State prediction error at timestep 779 is tensor(3.3203e-05, grad_fn=<MseLossBackward0>)
Current timestep = 780. State = [[-0.23417614  0.08544316]]. Action = [[-0.0506524   0.0847097  -0.00765087  0.95781946]]. Reward = [0.]
Curr episode timestep = 780
Scene graph at timestep 780 is [True, False, False, False, True, False]
State prediction error at timestep 780 is tensor(6.3489e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 780 of 1
Current timestep = 781. State = [[-0.23442225  0.08611945]]. Action = [[ 0.06119039  0.05821068 -0.07401724 -0.14343435]]. Reward = [0.]
Curr episode timestep = 781
Scene graph at timestep 781 is [True, False, False, False, True, False]
State prediction error at timestep 781 is tensor(1.5332e-05, grad_fn=<MseLossBackward0>)
Current timestep = 782. State = [[-0.23441756  0.08663206]]. Action = [[ 0.08662624  0.01122298 -0.0190158  -0.8064786 ]]. Reward = [0.]
Curr episode timestep = 782
Scene graph at timestep 782 is [True, False, False, False, True, False]
State prediction error at timestep 782 is tensor(2.2061e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 782 of 1
Current timestep = 783. State = [[-0.23299114  0.08786101]]. Action = [[0.02193178 0.09754517 0.04710037 0.5869248 ]]. Reward = [0.]
Curr episode timestep = 783
Scene graph at timestep 783 is [True, False, False, False, True, False]
State prediction error at timestep 783 is tensor(1.8015e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 783 of 1
Current timestep = 784. State = [[-0.23174979  0.09052569]]. Action = [[-0.01736247  0.06665356  0.03929856 -0.582639  ]]. Reward = [0.]
Curr episode timestep = 784
Scene graph at timestep 784 is [True, False, False, False, True, False]
State prediction error at timestep 784 is tensor(3.3675e-05, grad_fn=<MseLossBackward0>)
Current timestep = 785. State = [[-0.23142007  0.0939723 ]]. Action = [[-0.04184483 -0.04087653  0.07195842  0.5010724 ]]. Reward = [0.]
Curr episode timestep = 785
Scene graph at timestep 785 is [True, False, False, False, True, False]
State prediction error at timestep 785 is tensor(2.2416e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 785 of 1
Current timestep = 786. State = [[-0.23190905  0.09502666]]. Action = [[ 0.07121629 -0.01509156  0.01569153 -0.4814486 ]]. Reward = [0.]
Curr episode timestep = 786
Scene graph at timestep 786 is [True, False, False, False, True, False]
State prediction error at timestep 786 is tensor(5.7124e-05, grad_fn=<MseLossBackward0>)
Current timestep = 787. State = [[-0.23147479  0.09527442]]. Action = [[-0.08916185  0.05543447 -0.06709857 -0.9010084 ]]. Reward = [0.]
Curr episode timestep = 787
Scene graph at timestep 787 is [True, False, False, False, True, False]
State prediction error at timestep 787 is tensor(6.5432e-05, grad_fn=<MseLossBackward0>)
Current timestep = 788. State = [[-0.23217906  0.09662265]]. Action = [[-0.03407123 -0.02436712  0.05637028  0.58543324]]. Reward = [0.]
Curr episode timestep = 788
Scene graph at timestep 788 is [True, False, False, False, True, False]
State prediction error at timestep 788 is tensor(5.6632e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 788 of 1
Current timestep = 789. State = [[-0.23245578  0.09728604]]. Action = [[-0.01189278 -0.06086171  0.02552442 -0.6640952 ]]. Reward = [0.]
Curr episode timestep = 789
Scene graph at timestep 789 is [True, False, False, False, True, False]
State prediction error at timestep 789 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 790. State = [[-0.23258246  0.09743714]]. Action = [[-0.09346064  0.02414151  0.07417212 -0.00274682]]. Reward = [0.]
Curr episode timestep = 790
Scene graph at timestep 790 is [True, False, False, False, True, False]
State prediction error at timestep 790 is tensor(2.2983e-05, grad_fn=<MseLossBackward0>)
Current timestep = 791. State = [[-0.23287825  0.09782971]]. Action = [[-0.05752765  0.08422973  0.03381825 -0.55929136]]. Reward = [0.]
Curr episode timestep = 791
Scene graph at timestep 791 is [True, False, False, False, True, False]
State prediction error at timestep 791 is tensor(6.5070e-05, grad_fn=<MseLossBackward0>)
Current timestep = 792. State = [[-0.23489349  0.10082097]]. Action = [[-0.01517352  0.0150127  -0.03800225  0.09630322]]. Reward = [0.]
Curr episode timestep = 792
Scene graph at timestep 792 is [True, False, False, False, True, False]
State prediction error at timestep 792 is tensor(7.9550e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 792 of 1
Current timestep = 793. State = [[-0.23640363  0.10294397]]. Action = [[ 0.05469666 -0.05216473 -0.05754153 -0.87207544]]. Reward = [0.]
Curr episode timestep = 793
Scene graph at timestep 793 is [True, False, False, False, True, False]
State prediction error at timestep 793 is tensor(1.5348e-05, grad_fn=<MseLossBackward0>)
Current timestep = 794. State = [[-0.23641917  0.10329988]]. Action = [[-0.08724751  0.08742989 -0.0513575  -0.08184063]]. Reward = [0.]
Curr episode timestep = 794
Scene graph at timestep 794 is [True, False, False, False, True, False]
State prediction error at timestep 794 is tensor(5.3055e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 794 of 1
Current timestep = 795. State = [[-0.238131    0.10539342]]. Action = [[-0.01104376 -0.05972233 -0.01514614  0.81447434]]. Reward = [0.]
Curr episode timestep = 795
Scene graph at timestep 795 is [True, False, False, False, True, False]
State prediction error at timestep 795 is tensor(1.9000e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 795 of 1
Current timestep = 796. State = [[-0.2393669   0.10545236]]. Action = [[-0.07061794  0.07499028 -0.02088056  0.79106724]]. Reward = [0.]
Curr episode timestep = 796
Scene graph at timestep 796 is [True, False, False, False, True, False]
State prediction error at timestep 796 is tensor(3.5553e-05, grad_fn=<MseLossBackward0>)
Current timestep = 797. State = [[-0.24155657  0.10760655]]. Action = [[ 0.07911391  0.04018363 -0.07881753  0.51294744]]. Reward = [0.]
Curr episode timestep = 797
Scene graph at timestep 797 is [True, False, False, False, True, False]
State prediction error at timestep 797 is tensor(6.8485e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 797 of 1
Current timestep = 798. State = [[-0.24242051  0.10918386]]. Action = [[-0.08021329 -0.03724708 -0.03333114  0.89153886]]. Reward = [0.]
Curr episode timestep = 798
Scene graph at timestep 798 is [True, False, False, False, True, False]
State prediction error at timestep 798 is tensor(2.1090e-05, grad_fn=<MseLossBackward0>)
Current timestep = 799. State = [[-0.24319668  0.11001057]]. Action = [[-0.04479062  0.05596302 -0.01614453  0.06869507]]. Reward = [0.]
Curr episode timestep = 799
Scene graph at timestep 799 is [True, False, False, False, True, False]
State prediction error at timestep 799 is tensor(4.2983e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 799 of 1
Current timestep = 800. State = [[-0.24484594  0.11210104]]. Action = [[ 0.04964066 -0.00739083  0.01874657  0.05364823]]. Reward = [0.]
Curr episode timestep = 800
Scene graph at timestep 800 is [True, False, False, False, True, False]
State prediction error at timestep 800 is tensor(5.3327e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 800 of 1
Current timestep = 801. State = [[-0.24525818  0.11286495]]. Action = [[ 0.08228321  0.06606034  0.01498707 -0.66466737]]. Reward = [0.]
Curr episode timestep = 801
Scene graph at timestep 801 is [True, False, False, False, True, False]
State prediction error at timestep 801 is tensor(6.8801e-05, grad_fn=<MseLossBackward0>)
Current timestep = 802. State = [[-0.24576974  0.11389973]]. Action = [[ 0.04975022 -0.00786601  0.04204971 -0.82620174]]. Reward = [0.]
Curr episode timestep = 802
Scene graph at timestep 802 is [True, False, False, False, True, False]
State prediction error at timestep 802 is tensor(5.7272e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 802 of -1
Current timestep = 803. State = [[-0.24567132  0.11414922]]. Action = [[-0.00024845 -0.01235595 -0.01861219  0.09085798]]. Reward = [0.]
Curr episode timestep = 803
Scene graph at timestep 803 is [True, False, False, False, True, False]
State prediction error at timestep 803 is tensor(4.2065e-05, grad_fn=<MseLossBackward0>)
Current timestep = 804. State = [[-0.24560513  0.11425064]]. Action = [[-0.06633189 -0.05760235 -0.01053089  0.8199034 ]]. Reward = [0.]
Curr episode timestep = 804
Scene graph at timestep 804 is [True, False, False, False, True, False]
State prediction error at timestep 804 is tensor(2.2166e-05, grad_fn=<MseLossBackward0>)
Current timestep = 805. State = [[-0.2455471   0.11399315]]. Action = [[ 0.05571761 -0.07704476  0.04337933  0.75334346]]. Reward = [0.]
Curr episode timestep = 805
Scene graph at timestep 805 is [True, False, False, False, True, False]
State prediction error at timestep 805 is tensor(3.3981e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 805 of -1
Current timestep = 806. State = [[-0.24430853  0.11208047]]. Action = [[-0.09599855  0.08201341 -0.06361295 -0.6500216 ]]. Reward = [0.]
Curr episode timestep = 806
Scene graph at timestep 806 is [True, False, False, False, True, False]
State prediction error at timestep 806 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 807. State = [[-0.24491946  0.11296555]]. Action = [[-0.09644859 -0.02149042 -0.07237574  0.91456914]]. Reward = [0.]
Curr episode timestep = 807
Scene graph at timestep 807 is [True, False, False, False, True, False]
State prediction error at timestep 807 is tensor(1.9794e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 807 of -1
Current timestep = 808. State = [[-0.24642858  0.11333741]]. Action = [[ 0.05879178  0.07139487 -0.08146688  0.07681823]]. Reward = [0.]
Curr episode timestep = 808
Scene graph at timestep 808 is [True, False, False, False, True, False]
State prediction error at timestep 808 is tensor(2.2826e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 808 of -1
Current timestep = 809. State = [[-0.24697796  0.11429615]]. Action = [[-0.05757657 -0.08678883  0.06450897  0.52420974]]. Reward = [0.]
Curr episode timestep = 809
Scene graph at timestep 809 is [True, False, False, False, True, False]
State prediction error at timestep 809 is tensor(2.7562e-05, grad_fn=<MseLossBackward0>)
Current timestep = 810. State = [[-0.24757597  0.11391313]]. Action = [[ 0.05030098 -0.03226115  0.05666959  0.9331989 ]]. Reward = [0.]
Curr episode timestep = 810
Scene graph at timestep 810 is [True, False, False, False, True, False]
State prediction error at timestep 810 is tensor(5.2021e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 810 of -1
Current timestep = 811. State = [[-0.24739702  0.11323337]]. Action = [[ 0.08449107 -0.02328562 -0.01306731 -0.40850973]]. Reward = [0.]
Curr episode timestep = 811
Scene graph at timestep 811 is [True, False, False, False, True, False]
State prediction error at timestep 811 is tensor(8.6760e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 811 of -1
Current timestep = 812. State = [[-0.24638592  0.11188819]]. Action = [[ 0.02508975 -0.03203595 -0.0074365  -0.1572209 ]]. Reward = [0.]
Curr episode timestep = 812
Scene graph at timestep 812 is [True, False, False, False, True, False]
State prediction error at timestep 812 is tensor(3.4265e-05, grad_fn=<MseLossBackward0>)
Current timestep = 813. State = [[-0.24545333  0.1108363 ]]. Action = [[-0.08583585  0.09515851  0.09542043 -0.7197571 ]]. Reward = [0.]
Curr episode timestep = 813
Scene graph at timestep 813 is [True, False, False, False, True, False]
State prediction error at timestep 813 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 813 of -1
Current timestep = 814. State = [[-0.24599336  0.11194862]]. Action = [[ 0.06666892  0.08752943 -0.05768743  0.05743766]]. Reward = [0.]
Curr episode timestep = 814
Scene graph at timestep 814 is [True, False, False, False, True, False]
State prediction error at timestep 814 is tensor(1.5791e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 814 of -1
Current timestep = 815. State = [[-0.24657871  0.11307002]]. Action = [[ 0.08309305  0.03306284  0.03006745 -0.2868564 ]]. Reward = [0.]
Curr episode timestep = 815
Scene graph at timestep 815 is [True, False, False, False, True, False]
State prediction error at timestep 815 is tensor(6.8671e-05, grad_fn=<MseLossBackward0>)
Current timestep = 816. State = [[-0.24585092  0.11444203]]. Action = [[-0.00688453  0.0692055  -0.07056497 -0.29649675]]. Reward = [0.]
Curr episode timestep = 816
Scene graph at timestep 816 is [True, False, False, False, True, False]
State prediction error at timestep 816 is tensor(3.4423e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 816 of -1
Current timestep = 817. State = [[-0.24592552  0.11656011]]. Action = [[ 0.01593909  0.08252112  0.05253433 -0.8324073 ]]. Reward = [0.]
Curr episode timestep = 817
Scene graph at timestep 817 is [True, False, False, False, True, False]
State prediction error at timestep 817 is tensor(3.3009e-05, grad_fn=<MseLossBackward0>)
Current timestep = 818. State = [[-0.24598841  0.12040365]]. Action = [[-0.01904295  0.03305949 -0.05430968 -0.6201507 ]]. Reward = [0.]
Curr episode timestep = 818
Scene graph at timestep 818 is [True, False, False, False, True, False]
State prediction error at timestep 818 is tensor(9.2644e-05, grad_fn=<MseLossBackward0>)
Current timestep = 819. State = [[-0.24641137  0.1236825 ]]. Action = [[-0.00315819  0.00556673  0.05145932 -0.57455206]]. Reward = [0.]
Curr episode timestep = 819
Scene graph at timestep 819 is [True, False, False, False, True, False]
State prediction error at timestep 819 is tensor(7.4044e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 819 of -1
Current timestep = 820. State = [[-0.24654093  0.12561506]]. Action = [[-0.03276396 -0.01958374  0.02810185  0.30569184]]. Reward = [0.]
Curr episode timestep = 820
Scene graph at timestep 820 is [True, False, False, False, False, True]
State prediction error at timestep 820 is tensor(1.1599e-05, grad_fn=<MseLossBackward0>)
Current timestep = 821. State = [[-0.2469604   0.12633751]]. Action = [[ 0.0694709   0.03436101 -0.09006517 -0.48671877]]. Reward = [0.]
Curr episode timestep = 821
Scene graph at timestep 821 is [True, False, False, False, False, True]
State prediction error at timestep 821 is tensor(8.5528e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 821 of -1
Current timestep = 822. State = [[-0.2461689   0.12756577]]. Action = [[ 0.02494764  0.03560456 -0.08901452  0.49138856]]. Reward = [0.]
Curr episode timestep = 822
Scene graph at timestep 822 is [True, False, False, False, False, True]
State prediction error at timestep 822 is tensor(9.6265e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 822 of -1
Current timestep = 823. State = [[-0.24501635  0.12914963]]. Action = [[ 0.05091443 -0.06027546  0.06650441 -0.73636764]]. Reward = [0.]
Curr episode timestep = 823
Scene graph at timestep 823 is [True, False, False, False, False, True]
State prediction error at timestep 823 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 824. State = [[-0.24394809  0.12963982]]. Action = [[-0.09771341  0.01702034 -0.06904182 -0.15352416]]. Reward = [0.]
Curr episode timestep = 824
Scene graph at timestep 824 is [True, False, False, False, False, True]
State prediction error at timestep 824 is tensor(1.5298e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 824 of -1
Current timestep = 825. State = [[-0.24411567  0.13000964]]. Action = [[0.02496562 0.04103635 0.06665409 0.7954078 ]]. Reward = [0.]
Curr episode timestep = 825
Scene graph at timestep 825 is [True, False, False, False, False, True]
State prediction error at timestep 825 is tensor(4.2673e-05, grad_fn=<MseLossBackward0>)
Current timestep = 826. State = [[-0.24405792  0.13081089]]. Action = [[-0.05012788  0.02533019 -0.07125805 -0.5264017 ]]. Reward = [0.]
Curr episode timestep = 826
Scene graph at timestep 826 is [True, False, False, False, False, True]
State prediction error at timestep 826 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 826 of -1
Current timestep = 827. State = [[-0.24450785  0.1323396 ]]. Action = [[ 0.09517024  0.01523317 -0.06760827 -0.14853036]]. Reward = [0.]
Curr episode timestep = 827
Scene graph at timestep 827 is [True, False, False, False, False, True]
State prediction error at timestep 827 is tensor(5.0533e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 827 of -1
Current timestep = 828. State = [[-0.24333502  0.13347808]]. Action = [[ 0.03183348  0.03698117 -0.08146733  0.30736792]]. Reward = [0.]
Curr episode timestep = 828
Scene graph at timestep 828 is [True, False, False, False, False, True]
State prediction error at timestep 828 is tensor(1.1298e-06, grad_fn=<MseLossBackward0>)
Current timestep = 829. State = [[-0.24175295  0.13520207]]. Action = [[-0.0824597  -0.02012325  0.08194628 -0.42371905]]. Reward = [0.]
Curr episode timestep = 829
Scene graph at timestep 829 is [True, False, False, False, False, True]
State prediction error at timestep 829 is tensor(8.4484e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 829 of -1
Current timestep = 830. State = [[-0.24202843  0.13580503]]. Action = [[-0.05732182  0.06064484  0.01470961  0.22891855]]. Reward = [0.]
Curr episode timestep = 830
Scene graph at timestep 830 is [True, False, False, False, False, True]
State prediction error at timestep 830 is tensor(4.1232e-05, grad_fn=<MseLossBackward0>)
Current timestep = 831. State = [[-0.2434812   0.13794097]]. Action = [[ 0.01103777 -0.01340951  0.07102454  0.37111866]]. Reward = [0.]
Curr episode timestep = 831
Scene graph at timestep 831 is [True, False, False, False, False, True]
State prediction error at timestep 831 is tensor(7.7258e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 831 of -1
Current timestep = 832. State = [[-0.24405144  0.13918592]]. Action = [[ 0.09222787  0.09663702  0.02580633 -0.07396913]]. Reward = [0.]
Curr episode timestep = 832
Scene graph at timestep 832 is [True, False, False, False, False, True]
State prediction error at timestep 832 is tensor(2.4968e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 832 of -1
Current timestep = 833. State = [[-0.24324799  0.14244686]]. Action = [[-0.00722854  0.09587512  0.05401502  0.21206713]]. Reward = [0.]
Curr episode timestep = 833
Scene graph at timestep 833 is [True, False, False, False, False, True]
State prediction error at timestep 833 is tensor(5.0821e-05, grad_fn=<MseLossBackward0>)
Current timestep = 834. State = [[-0.24241145  0.14687693]]. Action = [[ 0.02132438  0.03066096  0.06833617 -0.36874962]]. Reward = [0.]
Curr episode timestep = 834
Scene graph at timestep 834 is [True, False, False, False, False, True]
State prediction error at timestep 834 is tensor(5.0637e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 834 of -1
Current timestep = 835. State = [[-0.24097824  0.15156901]]. Action = [[ 0.01987074  0.09394541 -0.09159306 -0.87694514]]. Reward = [0.]
Curr episode timestep = 835
Scene graph at timestep 835 is [True, False, False, False, False, True]
State prediction error at timestep 835 is tensor(9.7103e-05, grad_fn=<MseLossBackward0>)
Current timestep = 836. State = [[-0.2397509   0.15679204]]. Action = [[-0.0923272   0.05340902 -0.08224526 -0.13136965]]. Reward = [0.]
Curr episode timestep = 836
Scene graph at timestep 836 is [True, False, False, False, False, True]
State prediction error at timestep 836 is tensor(2.2283e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 836 of -1
Current timestep = 837. State = [[-0.24177617  0.16118124]]. Action = [[-0.04035714  0.08867631 -0.09790614  0.00570405]]. Reward = [0.]
Curr episode timestep = 837
Scene graph at timestep 837 is [True, False, False, False, False, True]
State prediction error at timestep 837 is tensor(1.0304e-05, grad_fn=<MseLossBackward0>)
Current timestep = 838. State = [[-0.24494827  0.16623668]]. Action = [[-0.07467146  0.03672337  0.03673845 -0.4174255 ]]. Reward = [0.]
Curr episode timestep = 838
Scene graph at timestep 838 is [True, False, False, False, False, True]
State prediction error at timestep 838 is tensor(1.8579e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 838 of -1
Current timestep = 839. State = [[-0.24793088  0.1708703 ]]. Action = [[-0.0601454  -0.08947439 -0.07645534  0.79726624]]. Reward = [0.]
Curr episode timestep = 839
Scene graph at timestep 839 is [True, False, False, False, False, True]
State prediction error at timestep 839 is tensor(2.1273e-05, grad_fn=<MseLossBackward0>)
Current timestep = 840. State = [[-0.24888393  0.1720789 ]]. Action = [[-0.05999637  0.03226522  0.04740364 -0.37626618]]. Reward = [0.]
Curr episode timestep = 840
Scene graph at timestep 840 is [True, False, False, False, False, True]
State prediction error at timestep 840 is tensor(6.0439e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 840 of -1
Current timestep = 841. State = [[-0.2504095   0.17412698]]. Action = [[-0.03340141 -0.07205946 -0.08747178 -0.5367072 ]]. Reward = [0.]
Curr episode timestep = 841
Scene graph at timestep 841 is [True, False, False, False, False, True]
State prediction error at timestep 841 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 842. State = [[-0.2517635   0.17349148]]. Action = [[ 0.01376802 -0.00174022 -0.06878994  0.47652054]]. Reward = [0.]
Curr episode timestep = 842
Scene graph at timestep 842 is [True, False, False, False, False, True]
State prediction error at timestep 842 is tensor(4.6289e-06, grad_fn=<MseLossBackward0>)
Current timestep = 843. State = [[-0.25250596  0.17322293]]. Action = [[-0.02334239 -0.0417222   0.0393228  -0.85272104]]. Reward = [0.]
Curr episode timestep = 843
Scene graph at timestep 843 is [True, False, False, False, False, True]
State prediction error at timestep 843 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 843 of -1
Current timestep = 844. State = [[-0.25336632  0.17224064]]. Action = [[-0.06177641  0.00225101  0.02738189  0.74807954]]. Reward = [0.]
Curr episode timestep = 844
Scene graph at timestep 844 is [True, False, False, False, False, True]
State prediction error at timestep 844 is tensor(1.7075e-06, grad_fn=<MseLossBackward0>)
Current timestep = 845. State = [[-0.25457594  0.17187059]]. Action = [[-0.09655347  0.08645058  0.05131986 -0.74543273]]. Reward = [0.]
Curr episode timestep = 845
Scene graph at timestep 845 is [True, False, False, False, False, True]
State prediction error at timestep 845 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 845 of -1
Current timestep = 846. State = [[-0.25773224  0.17434041]]. Action = [[ 0.01909132 -0.00518725  0.04224817  0.63427067]]. Reward = [0.]
Curr episode timestep = 846
Scene graph at timestep 846 is [True, False, False, False, False, True]
State prediction error at timestep 846 is tensor(1.5029e-05, grad_fn=<MseLossBackward0>)
Current timestep = 847. State = [[-0.25985622  0.17621158]]. Action = [[-0.03900434  0.06395843  0.09824128 -0.13352382]]. Reward = [0.]
Curr episode timestep = 847
Scene graph at timestep 847 is [True, False, False, False, False, True]
State prediction error at timestep 847 is tensor(2.9698e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 847 of -1
Current timestep = 848. State = [[-0.26203993  0.17876063]]. Action = [[-0.04442756  0.06761501  0.04080387 -0.08475399]]. Reward = [0.]
Curr episode timestep = 848
Scene graph at timestep 848 is [True, False, False, False, False, True]
State prediction error at timestep 848 is tensor(1.8507e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 848 of -1
Current timestep = 849. State = [[-0.26466307  0.18226832]]. Action = [[-0.04891603  0.02093146 -0.0327985  -0.30620372]]. Reward = [0.]
Curr episode timestep = 849
Scene graph at timestep 849 is [True, False, False, False, False, True]
State prediction error at timestep 849 is tensor(1.3888e-05, grad_fn=<MseLossBackward0>)
Current timestep = 850. State = [[-0.2674707   0.18538307]]. Action = [[-0.00052561  0.09580971 -0.00450241 -0.08494055]]. Reward = [0.]
Curr episode timestep = 850
Scene graph at timestep 850 is [True, False, False, False, False, True]
State prediction error at timestep 850 is tensor(3.8861e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 850 of -1
Current timestep = 851. State = [[-0.27004638  0.18907379]]. Action = [[ 0.05436509 -0.06452751 -0.03306575 -0.19629169]]. Reward = [0.]
Curr episode timestep = 851
Scene graph at timestep 851 is [True, False, False, False, False, True]
State prediction error at timestep 851 is tensor(1.0629e-06, grad_fn=<MseLossBackward0>)
Current timestep = 852. State = [[-0.27024627  0.18961814]]. Action = [[-0.09610108  0.03206981 -0.03496168 -0.598745  ]]. Reward = [0.]
Curr episode timestep = 852
Scene graph at timestep 852 is [True, False, False, False, False, True]
State prediction error at timestep 852 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 852 of -1
Current timestep = 853. State = [[-0.27192605  0.1912977 ]]. Action = [[ 0.05437484  0.08872908  0.00608058 -0.21967423]]. Reward = [0.]
Curr episode timestep = 853
Scene graph at timestep 853 is [True, False, False, False, False, True]
State prediction error at timestep 853 is tensor(2.0085e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 853 of -1
Current timestep = 854. State = [[-0.2737904   0.19357698]]. Action = [[-0.03370939  0.04090726 -0.03603499  0.01406062]]. Reward = [0.]
Curr episode timestep = 854
Scene graph at timestep 854 is [True, False, False, False, False, True]
State prediction error at timestep 854 is tensor(1.0415e-05, grad_fn=<MseLossBackward0>)
Current timestep = 855. State = [[-0.27565664  0.19609533]]. Action = [[ 0.06943039 -0.03994612 -0.02585615  0.7858808 ]]. Reward = [0.]
Curr episode timestep = 855
Scene graph at timestep 855 is [True, False, False, False, False, True]
State prediction error at timestep 855 is tensor(7.0176e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 855 of -1
Current timestep = 856. State = [[-0.27563134  0.19616279]]. Action = [[-0.08306148  0.02532775  0.01316299 -0.36163062]]. Reward = [0.]
Curr episode timestep = 856
Scene graph at timestep 856 is [True, False, False, False, False, True]
State prediction error at timestep 856 is tensor(2.8758e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 856 of -1
Current timestep = 857. State = [[-0.27674618  0.19757639]]. Action = [[-0.05946728  0.06365073  0.03752302  0.03988314]]. Reward = [0.]
Curr episode timestep = 857
Scene graph at timestep 857 is [True, False, False, False, False, True]
State prediction error at timestep 857 is tensor(5.7385e-06, grad_fn=<MseLossBackward0>)
Current timestep = 858. State = [[-0.279203    0.20054963]]. Action = [[ 0.05069513  0.03156178 -0.00366839 -0.5830226 ]]. Reward = [0.]
Curr episode timestep = 858
Scene graph at timestep 858 is [True, False, False, False, False, True]
State prediction error at timestep 858 is tensor(3.9102e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 858 of -1
Current timestep = 859. State = [[-0.28072673  0.20231819]]. Action = [[-0.0642444   0.06803525  0.08627396 -0.2780339 ]]. Reward = [0.]
Curr episode timestep = 859
Scene graph at timestep 859 is [True, False, False, False, False, True]
State prediction error at timestep 859 is tensor(1.9186e-05, grad_fn=<MseLossBackward0>)
Current timestep = 860. State = [[-0.2830918   0.20536168]]. Action = [[ 0.06494815  0.06543995 -0.07291128 -0.5443552 ]]. Reward = [0.]
Curr episode timestep = 860
Scene graph at timestep 860 is [True, False, False, False, False, True]
State prediction error at timestep 860 is tensor(5.2683e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 860 of -1
Current timestep = 861. State = [[-0.28476077  0.2074875 ]]. Action = [[-0.02746467 -0.0543455  -0.08132881  0.76006746]]. Reward = [0.]
Curr episode timestep = 861
Scene graph at timestep 861 is [True, False, False, False, False, True]
State prediction error at timestep 861 is tensor(3.9277e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 861 of -1
Current timestep = 862. State = [[-0.28525764  0.20828173]]. Action = [[-0.04280099 -0.04920533 -0.07373457  0.25330114]]. Reward = [0.]
Curr episode timestep = 862
Scene graph at timestep 862 is [True, False, False, False, False, True]
State prediction error at timestep 862 is tensor(1.5385e-05, grad_fn=<MseLossBackward0>)
Current timestep = 863. State = [[-0.28535306  0.20851746]]. Action = [[-0.09819496  0.00483318 -0.04679535  0.63895345]]. Reward = [0.]
Curr episode timestep = 863
Scene graph at timestep 863 is [True, False, False, False, False, True]
State prediction error at timestep 863 is tensor(3.9171e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 863 of -1
Current timestep = 864. State = [[-0.28638673  0.20949785]]. Action = [[ 0.02261724  0.04377902  0.01221975 -0.04297602]]. Reward = [0.]
Curr episode timestep = 864
Scene graph at timestep 864 is [True, False, False, False, False, True]
State prediction error at timestep 864 is tensor(6.3375e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 864 of -1
Current timestep = 865. State = [[-0.2872962   0.21049544]]. Action = [[ 0.09566874 -0.0491164  -0.00826181 -0.9794247 ]]. Reward = [0.]
Curr episode timestep = 865
Scene graph at timestep 865 is [True, False, False, False, False, True]
State prediction error at timestep 865 is tensor(9.9192e-05, grad_fn=<MseLossBackward0>)
Current timestep = 866. State = [[-0.28704742  0.21043773]]. Action = [[ 0.02693511 -0.00477231  0.01240989  0.9219761 ]]. Reward = [0.]
Curr episode timestep = 866
Scene graph at timestep 866 is [True, False, False, False, False, True]
State prediction error at timestep 866 is tensor(5.9456e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 866 of -1
Current timestep = 867. State = [[-0.28620312  0.21044216]]. Action = [[ 0.06500975  0.07985229 -0.02543322  0.9273455 ]]. Reward = [0.]
Curr episode timestep = 867
Scene graph at timestep 867 is [True, False, False, False, False, True]
State prediction error at timestep 867 is tensor(3.1938e-06, grad_fn=<MseLossBackward0>)
Current timestep = 868. State = [[-0.28566435  0.21059744]]. Action = [[-0.01648836 -0.06804786 -0.09498756  0.44030035]]. Reward = [0.]
Curr episode timestep = 868
Scene graph at timestep 868 is [True, False, False, False, False, True]
State prediction error at timestep 868 is tensor(4.2693e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 868 of -1
Current timestep = 869. State = [[-0.28476736  0.21104014]]. Action = [[-0.08600593  0.0825486   0.07394267  0.76212287]]. Reward = [0.]
Curr episode timestep = 869
Scene graph at timestep 869 is [True, False, False, False, False, True]
State prediction error at timestep 869 is tensor(2.5068e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 869 of -1
Current timestep = 870. State = [[-0.2856207   0.21198016]]. Action = [[-0.00433102 -0.06125424 -0.08295099  0.09538591]]. Reward = [0.]
Curr episode timestep = 870
Scene graph at timestep 870 is [True, False, False, False, False, True]
State prediction error at timestep 870 is tensor(1.4105e-07, grad_fn=<MseLossBackward0>)
Current timestep = 871. State = [[-0.2854577   0.21216826]]. Action = [[-0.03599358 -0.03892723  0.086514    0.45787644]]. Reward = [0.]
Curr episode timestep = 871
Scene graph at timestep 871 is [True, False, False, False, False, True]
State prediction error at timestep 871 is tensor(2.1197e-05, grad_fn=<MseLossBackward0>)
Current timestep = 872. State = [[-0.28563416  0.21182293]]. Action = [[ 0.04342005 -0.04239181 -0.07757084  0.7515614 ]]. Reward = [0.]
Curr episode timestep = 872
Scene graph at timestep 872 is [True, False, False, False, False, True]
State prediction error at timestep 872 is tensor(1.2723e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 872 of -1
Current timestep = 873. State = [[-0.2851085  0.2108347]]. Action = [[-0.05455265  0.01764472  0.06354346 -0.25748992]]. Reward = [0.]
Curr episode timestep = 873
Scene graph at timestep 873 is [True, False, False, False, False, True]
State prediction error at timestep 873 is tensor(3.2338e-05, grad_fn=<MseLossBackward0>)
Current timestep = 874. State = [[-0.28534475  0.21043105]]. Action = [[-0.05928582 -0.08775329 -0.02526413  0.12212396]]. Reward = [0.]
Curr episode timestep = 874
Scene graph at timestep 874 is [True, False, False, False, False, True]
State prediction error at timestep 874 is tensor(1.7722e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 874 of -1
Current timestep = 875. State = [[-0.2861667   0.20886573]]. Action = [[-0.06983487  0.07428529 -0.0772769   0.70363045]]. Reward = [0.]
Curr episode timestep = 875
Scene graph at timestep 875 is [True, False, False, False, False, True]
State prediction error at timestep 875 is tensor(2.1656e-05, grad_fn=<MseLossBackward0>)
Current timestep = 876. State = [[-0.2874983   0.20969625]]. Action = [[ 0.00841205  0.05076454 -0.0800204  -0.22312391]]. Reward = [0.]
Curr episode timestep = 876
Scene graph at timestep 876 is [True, False, False, False, False, True]
State prediction error at timestep 876 is tensor(1.1640e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 876 of -1
Current timestep = 877. State = [[-0.28861862  0.21089183]]. Action = [[-0.03709753  0.00941537 -0.08984557 -0.6760931 ]]. Reward = [0.]
Curr episode timestep = 877
Scene graph at timestep 877 is [True, False, False, False, False, True]
State prediction error at timestep 877 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 877 of -1
Current timestep = 878. State = [[-0.2901595   0.21242575]]. Action = [[ 0.00714816 -0.04444423 -0.03983233  0.94890416]]. Reward = [0.]
Curr episode timestep = 878
Scene graph at timestep 878 is [True, False, False, False, False, True]
State prediction error at timestep 878 is tensor(3.4950e-05, grad_fn=<MseLossBackward0>)
Current timestep = 879. State = [[-0.29050484  0.21245573]]. Action = [[ 0.092958    0.02789048 -0.08000426 -0.86824864]]. Reward = [0.]
Curr episode timestep = 879
Scene graph at timestep 879 is [True, False, False, False, False, True]
State prediction error at timestep 879 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 879 of -1
Current timestep = 880. State = [[-0.29030314  0.21257873]]. Action = [[-0.07352383 -0.00139023  0.06291915 -0.7290355 ]]. Reward = [0.]
Curr episode timestep = 880
Scene graph at timestep 880 is [True, False, False, False, False, True]
State prediction error at timestep 880 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 880 of -1
Current timestep = 881. State = [[-0.2905326   0.21274696]]. Action = [[ 0.05393035 -0.02536963 -0.007515   -0.26768094]]. Reward = [0.]
Curr episode timestep = 881
Scene graph at timestep 881 is [True, False, False, False, False, True]
State prediction error at timestep 881 is tensor(1.1509e-05, grad_fn=<MseLossBackward0>)
Current timestep = 882. State = [[-0.29040235  0.21269687]]. Action = [[ 0.00102563  0.00254995 -0.06660628  0.18777907]]. Reward = [0.]
Curr episode timestep = 882
Scene graph at timestep 882 is [True, False, False, False, False, True]
State prediction error at timestep 882 is tensor(1.0013e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 882 of -1
Current timestep = 883. State = [[-0.2903652   0.21265408]]. Action = [[ 0.03928361 -0.00073028  0.06848181 -0.41954166]]. Reward = [0.]
Curr episode timestep = 883
Scene graph at timestep 883 is [True, False, False, False, False, True]
State prediction error at timestep 883 is tensor(4.4282e-05, grad_fn=<MseLossBackward0>)
Current timestep = 884. State = [[-0.29016092  0.21256797]]. Action = [[0.0492874  0.04700846 0.05906086 0.62436795]]. Reward = [0.]
Curr episode timestep = 884
Scene graph at timestep 884 is [True, False, False, False, False, True]
State prediction error at timestep 884 is tensor(4.3292e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 884 of -1
Current timestep = 885. State = [[-0.28991428  0.2126789 ]]. Action = [[-0.05917831  0.03828361  0.01490115  0.62494373]]. Reward = [0.]
Curr episode timestep = 885
Scene graph at timestep 885 is [True, False, False, False, False, True]
State prediction error at timestep 885 is tensor(4.1524e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 885 of -1
Current timestep = 886. State = [[-0.29026067  0.21310188]]. Action = [[-0.04224506  0.05942739 -0.096715    0.27195144]]. Reward = [0.]
Curr episode timestep = 886
Scene graph at timestep 886 is [True, False, False, False, False, True]
State prediction error at timestep 886 is tensor(4.8814e-06, grad_fn=<MseLossBackward0>)
Current timestep = 887. State = [[-0.29170075  0.2146574 ]]. Action = [[ 0.06394172 -0.06250714  0.04971554  0.52454877]]. Reward = [0.]
Curr episode timestep = 887
Scene graph at timestep 887 is [True, False, False, False, False, True]
State prediction error at timestep 887 is tensor(2.7159e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 887 of -1
Current timestep = 888. State = [[-0.29155904  0.21463057]]. Action = [[ 0.02321164 -0.03648101  0.05581588 -0.3397025 ]]. Reward = [0.]
Curr episode timestep = 888
Scene graph at timestep 888 is [True, False, False, False, False, True]
State prediction error at timestep 888 is tensor(3.7481e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 888 of -1
Current timestep = 889. State = [[-0.2910304   0.21446738]]. Action = [[-0.05915054  0.02408131 -0.03766186 -0.8905971 ]]. Reward = [0.]
Curr episode timestep = 889
Scene graph at timestep 889 is [True, False, False, False, False, True]
State prediction error at timestep 889 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 890. State = [[-0.29115474  0.21458462]]. Action = [[ 0.02663269  0.0686347  -0.03841089 -0.3151387 ]]. Reward = [0.]
Curr episode timestep = 890
Scene graph at timestep 890 is [True, False, False, False, False, True]
State prediction error at timestep 890 is tensor(4.7618e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 890 of -1
Current timestep = 891. State = [[-0.2914772   0.21495204]]. Action = [[-0.06586699  0.01314533 -0.04083893 -0.59484184]]. Reward = [0.]
Curr episode timestep = 891
Scene graph at timestep 891 is [True, False, False, False, False, True]
State prediction error at timestep 891 is tensor(8.4886e-05, grad_fn=<MseLossBackward0>)
Current timestep = 892. State = [[-0.29278943  0.21629627]]. Action = [[-0.06034761  0.02276619  0.04438398 -0.61903703]]. Reward = [0.]
Curr episode timestep = 892
Scene graph at timestep 892 is [True, False, False, False, False, True]
State prediction error at timestep 892 is tensor(6.8733e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 892 of -1
Current timestep = 893. State = [[-0.29471898  0.2184583 ]]. Action = [[-0.08040641  0.04163218 -0.08187369  0.42747927]]. Reward = [0.]
Curr episode timestep = 893
Scene graph at timestep 893 is [True, False, False, False, False, True]
State prediction error at timestep 893 is tensor(4.3543e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 893 of -1
Current timestep = 894. State = [[-0.29754746  0.22152497]]. Action = [[0.06533588 0.06373245 0.06289699 0.98182833]]. Reward = [0.]
Curr episode timestep = 894
Scene graph at timestep 894 is [True, False, False, False, False, True]
State prediction error at timestep 894 is tensor(4.1280e-05, grad_fn=<MseLossBackward0>)
Current timestep = 895. State = [[-0.2995379   0.22371122]]. Action = [[-0.03084268  0.06821162  0.00296179  0.25429535]]. Reward = [0.]
Curr episode timestep = 895
Scene graph at timestep 895 is [True, False, False, False, False, True]
State prediction error at timestep 895 is tensor(2.4433e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 895 of -1
Current timestep = 896. State = [[-0.30224606  0.22690889]]. Action = [[-0.0495873  -0.04960201 -0.07634827  0.43803382]]. Reward = [0.]
Curr episode timestep = 896
Scene graph at timestep 896 is [True, False, False, False, False, True]
State prediction error at timestep 896 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 897. State = [[-0.30339682  0.22829714]]. Action = [[ 0.06264306  0.08894549 -0.0552323   0.26140714]]. Reward = [0.]
Curr episode timestep = 897
Scene graph at timestep 897 is [True, False, False, False, False, True]
State prediction error at timestep 897 is tensor(1.2870e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 897 of -1
Current timestep = 898. State = [[-0.30484018  0.22975945]]. Action = [[-0.04744122  0.03302575 -0.00542969 -0.50523907]]. Reward = [0.]
Curr episode timestep = 898
Scene graph at timestep 898 is [True, False, False, False, False, True]
State prediction error at timestep 898 is tensor(1.0468e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 898 of -1
Current timestep = 899. State = [[-0.30687228  0.23193525]]. Action = [[ 0.03304897  0.08827812 -0.01076578  0.57436216]]. Reward = [0.]
Curr episode timestep = 899
Scene graph at timestep 899 is [True, False, False, False, False, True]
State prediction error at timestep 899 is tensor(3.6035e-05, grad_fn=<MseLossBackward0>)
Current timestep = 900. State = [[-0.30782598  0.23522627]]. Action = [[-0.08150682  0.05597884  0.02831871 -0.83712286]]. Reward = [0.]
Curr episode timestep = 900
Scene graph at timestep 900 is [True, False, False, False, False, True]
State prediction error at timestep 900 is tensor(9.1327e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 900 of -1
Current timestep = 901. State = [[-0.2576814   0.00828732]]. Action = [[ 0.01594958  0.09674907  0.06922958 -0.11836886]]. Reward = [0.]
Curr episode timestep = 901
Scene graph at timestep 901 is [True, False, False, False, True, False]
State prediction error at timestep 901 is tensor(0.0250, grad_fn=<MseLossBackward0>)
Current timestep = 902. State = [[-0.26171038  0.00998249]]. Action = [[-0.07352336  0.09783968 -0.08560899 -0.42422444]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 902 is [True, False, False, False, True, False]
State prediction error at timestep 902 is tensor(2.1491e-05, grad_fn=<MseLossBackward0>)
Current timestep = 903. State = [[-0.28484017  0.02739853]]. Action = [[ 0.08193455  0.088407    0.00514437 -0.00220156]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 903 is [True, False, False, False, True, False]
State prediction error at timestep 903 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 904. State = [[-0.2995725   0.07031316]]. Action = [[-0.00289579 -0.03507438  0.04527307 -0.6274862 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 904 is [True, False, False, False, True, False]
State prediction error at timestep 904 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 905. State = [[-0.3122724   0.14180987]]. Action = [[ 0.03027148 -0.05808258 -0.0300275   0.8527709 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 905 is [True, False, False, False, False, True]
State prediction error at timestep 905 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 906. State = [[-0.31558615  0.19347587]]. Action = [[ 0.03176691 -0.05821533  0.06441892  0.34714043]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 906 is [True, False, False, False, False, True]
State prediction error at timestep 906 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 907. State = [[-0.3139002   0.22111407]]. Action = [[-0.04418242  0.08110984  0.01171168  0.66478014]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 907 is [True, False, False, False, False, True]
State prediction error at timestep 907 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 908. State = [[-0.31445035  0.23704822]]. Action = [[ 0.0790093  0.0674559  0.0363979 -0.4981063]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 908 is [True, False, False, False, False, True]
State prediction error at timestep 908 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 909. State = [[-0.31376886  0.24856336]]. Action = [[-0.06704371 -0.02112289  0.06536105  0.31250858]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 909 is [True, False, False, False, False, True]
State prediction error at timestep 909 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 910. State = [[-0.31343746  0.25615412]]. Action = [[ 0.05634218  0.08265538 -0.09068947  0.29075098]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 910 is [True, False, False, False, False, True]
State prediction error at timestep 910 is tensor(9.5219e-05, grad_fn=<MseLossBackward0>)
Current timestep = 911. State = [[-0.31301254  0.26168403]]. Action = [[ 0.06187861  0.00579152  0.06274865 -0.24590981]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 911 is [True, False, False, False, False, True]
State prediction error at timestep 911 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 912. State = [[-0.31125575  0.26488218]]. Action = [[ 0.01416261 -0.08641957 -0.03331908 -0.3247689 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 912 is [True, False, False, False, False, True]
State prediction error at timestep 912 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 913. State = [[-0.3091948   0.26564714]]. Action = [[ 0.0130893  -0.00873807 -0.07126586  0.28387392]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 913 is [True, False, False, False, False, True]
State prediction error at timestep 913 is tensor(8.7176e-05, grad_fn=<MseLossBackward0>)
Current timestep = 914. State = [[-0.30753508  0.26615334]]. Action = [[-0.01631109 -0.05038876  0.07006354  0.7616204 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 914 is [True, False, False, False, False, True]
State prediction error at timestep 914 is tensor(3.3132e-05, grad_fn=<MseLossBackward0>)
Current timestep = 915. State = [[-0.30655637  0.26565453]]. Action = [[-0.02572733 -0.07105851  0.00405618 -0.7281108 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 915 is [True, False, False, False, False, True]
State prediction error at timestep 915 is tensor(6.4019e-05, grad_fn=<MseLossBackward0>)
Current timestep = 916. State = [[-0.30579028  0.26381958]]. Action = [[-0.00294075 -0.06928177  0.00849533 -0.01054686]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 916 is [True, False, False, False, False, True]
State prediction error at timestep 916 is tensor(9.5379e-05, grad_fn=<MseLossBackward0>)
Current timestep = 917. State = [[-0.30462512  0.2614018 ]]. Action = [[ 0.03062723 -0.02679332 -0.07667576  0.98503137]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 917 is [True, False, False, False, False, True]
State prediction error at timestep 917 is tensor(2.7458e-05, grad_fn=<MseLossBackward0>)
Current timestep = 918. State = [[-0.30343783  0.25885838]]. Action = [[-0.0222926  -0.01529282  0.07369647 -0.9018896 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 918 is [True, False, False, False, False, True]
State prediction error at timestep 918 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 919. State = [[-0.3026242   0.25723118]]. Action = [[0.09461374 0.00132336 0.06228787 0.09153152]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 919 is [True, False, False, False, False, True]
State prediction error at timestep 919 is tensor(3.0988e-05, grad_fn=<MseLossBackward0>)
Current timestep = 920. State = [[-0.30141637  0.25551128]]. Action = [[ 0.05930389  0.03625853 -0.08597668  0.97546244]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 920 is [True, False, False, False, False, True]
State prediction error at timestep 920 is tensor(2.3290e-05, grad_fn=<MseLossBackward0>)
Current timestep = 921. State = [[-0.2996072  0.255079 ]]. Action = [[ 0.08413098  0.05743427 -0.05739511 -0.8484731 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 921 is [True, False, False, False, False, True]
State prediction error at timestep 921 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 922. State = [[-0.29655126  0.25639042]]. Action = [[-0.07690557  0.09362353 -0.05341689 -0.08599371]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 922 is [True, False, False, False, False, True]
State prediction error at timestep 922 is tensor(3.4783e-05, grad_fn=<MseLossBackward0>)
Current timestep = 923. State = [[-0.2953388  0.2591272]]. Action = [[-0.00330592  0.022108    0.01865665 -0.8994473 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 923 is [True, False, False, False, False, True]
State prediction error at timestep 923 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 924. State = [[-0.29531687  0.26078543]]. Action = [[-0.03904239  0.00408702 -0.06205888  0.3734281 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 924 is [True, False, False, False, False, True]
State prediction error at timestep 924 is tensor(8.9079e-05, grad_fn=<MseLossBackward0>)
Current timestep = 925. State = [[-0.29640657  0.2617911 ]]. Action = [[-0.08106686 -0.00104517  0.02182939  0.2605586 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 925 is [True, False, False, False, False, True]
State prediction error at timestep 925 is tensor(6.3552e-05, grad_fn=<MseLossBackward0>)
Current timestep = 926. State = [[-0.29798815  0.26327354]]. Action = [[ 0.0650269  -0.08555304 -0.08163066 -0.16215646]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 926 is [True, False, False, False, False, True]
State prediction error at timestep 926 is tensor(1.9069e-05, grad_fn=<MseLossBackward0>)
Current timestep = 927. State = [[-0.29762053  0.26280156]]. Action = [[ 0.09285604  0.07050941 -0.02636765  0.92042017]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 927 is [True, False, False, False, False, True]
State prediction error at timestep 927 is tensor(1.2742e-05, grad_fn=<MseLossBackward0>)
Current timestep = 928. State = [[-0.29676038  0.26317415]]. Action = [[-0.0177796  -0.02755119  0.01074807 -0.73650694]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 928 is [True, False, False, False, False, True]
State prediction error at timestep 928 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 929. State = [[-0.29636484  0.26320064]]. Action = [[-0.03412857  0.03831699 -0.02748823  0.5930337 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 929 is [True, False, False, False, False, True]
State prediction error at timestep 929 is tensor(4.1305e-05, grad_fn=<MseLossBackward0>)
Current timestep = 930. State = [[-0.29667023  0.26356593]]. Action = [[-0.05162046  0.07957231  0.0450659   0.20258737]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 930 is [True, False, False, False, False, True]
State prediction error at timestep 930 is tensor(3.6859e-05, grad_fn=<MseLossBackward0>)
Current timestep = 931. State = [[-0.29820848  0.26554313]]. Action = [[-0.07933853  0.03070555 -0.07492238 -0.16767359]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 931 is [True, False, False, False, False, True]
State prediction error at timestep 931 is tensor(4.3352e-05, grad_fn=<MseLossBackward0>)
Current timestep = 932. State = [[-0.3009304   0.26805824]]. Action = [[-0.04925393 -0.08774216 -0.07162727  0.6719835 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 932 is [True, False, False, False, False, True]
State prediction error at timestep 932 is tensor(6.8546e-05, grad_fn=<MseLossBackward0>)
Current timestep = 933. State = [[-0.30184275  0.2688164 ]]. Action = [[ 0.08616222 -0.0420929  -0.07864269 -0.38174015]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 933 is [True, False, False, False, False, True]
State prediction error at timestep 933 is tensor(4.6987e-05, grad_fn=<MseLossBackward0>)
Current timestep = 934. State = [[-0.30125725  0.26781335]]. Action = [[ 0.041949    0.0956125  -0.01338636  0.05576408]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 934 is [True, False, False, False, False, True]
State prediction error at timestep 934 is tensor(6.1628e-05, grad_fn=<MseLossBackward0>)
Current timestep = 935. State = [[-0.3011411   0.26836637]]. Action = [[-0.00687419  0.03200484 -0.07833934 -0.16441089]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 935 is [True, False, False, False, False, True]
State prediction error at timestep 935 is tensor(3.3974e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 935 of -1
Current timestep = 936. State = [[-0.3013433   0.26935494]]. Action = [[-0.0249833  -0.08114751  0.09445924  0.16448736]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 936 is [True, False, False, False, False, True]
State prediction error at timestep 936 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 936 of -1
Current timestep = 937. State = [[-0.30105752  0.26911297]]. Action = [[-0.08222727 -0.09645569 -0.08114419 -0.01349467]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 937 is [True, False, False, False, False, True]
State prediction error at timestep 937 is tensor(9.9675e-05, grad_fn=<MseLossBackward0>)
Current timestep = 938. State = [[-0.300528    0.26768687]]. Action = [[ 0.09018321 -0.05289594 -0.06247959  0.3160758 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 938 is [True, False, False, False, False, True]
State prediction error at timestep 938 is tensor(2.3814e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 938 of -1
Current timestep = 939. State = [[-0.299091    0.26500934]]. Action = [[ 0.07293958  0.01813857 -0.05195991  0.20764935]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 939 is [True, False, False, False, False, True]
State prediction error at timestep 939 is tensor(7.9451e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 939 of -1
Current timestep = 940. State = [[-0.29764855  0.2628761 ]]. Action = [[ 0.07690626 -0.06534809  0.06676597  0.12858963]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 940 is [True, False, False, False, False, True]
State prediction error at timestep 940 is tensor(1.5655e-05, grad_fn=<MseLossBackward0>)
Current timestep = 941. State = [[-0.2950873   0.25958204]]. Action = [[0.07592451 0.0215382  0.04402342 0.78001285]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 941 is [True, False, False, False, False, True]
State prediction error at timestep 941 is tensor(1.4037e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 941 of -1
Current timestep = 942. State = [[-0.29235384  0.25770947]]. Action = [[-0.09777835 -0.0534454   0.05761621 -0.73972833]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 942 is [True, False, False, False, False, True]
State prediction error at timestep 942 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 942 of -1
Current timestep = 943. State = [[-0.29174507  0.25600153]]. Action = [[-0.05485525  0.04076555  0.01620622  0.5200186 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 943 is [True, False, False, False, False, True]
State prediction error at timestep 943 is tensor(3.9282e-05, grad_fn=<MseLossBackward0>)
Current timestep = 944. State = [[-0.2922974   0.25637817]]. Action = [[0.00323065 0.01093085 0.08481529 0.8407538 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 944 is [True, False, False, False, False, True]
State prediction error at timestep 944 is tensor(7.9207e-06, grad_fn=<MseLossBackward0>)
Current timestep = 945. State = [[-0.29244408  0.2565328 ]]. Action = [[ 0.01661909 -0.04692514 -0.08704416 -0.74232316]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 945 is [True, False, False, False, False, True]
State prediction error at timestep 945 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 945 of -1
Current timestep = 946. State = [[-0.29212806  0.25590184]]. Action = [[-0.00350749  0.00925823 -0.07623079  0.48644662]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 946 is [True, False, False, False, False, True]
State prediction error at timestep 946 is tensor(3.1160e-05, grad_fn=<MseLossBackward0>)
Current timestep = 947. State = [[-0.29201567  0.25570285]]. Action = [[ 0.07104566 -0.02286137 -0.07015581 -0.13523501]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 947 is [True, False, False, False, False, True]
State prediction error at timestep 947 is tensor(2.4461e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 947 of -1
Current timestep = 948. State = [[-0.2911812   0.25446823]]. Action = [[-0.06153993 -0.07543704 -0.07423273  0.4280752 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 948 is [True, False, False, False, False, True]
State prediction error at timestep 948 is tensor(6.7579e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 948 of -1
Current timestep = 949. State = [[-0.29033986  0.25252494]]. Action = [[-0.00916597  0.05165955  0.00423982  0.11902213]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 949 is [True, False, False, False, False, True]
State prediction error at timestep 949 is tensor(6.4164e-06, grad_fn=<MseLossBackward0>)
Current timestep = 950. State = [[-0.29023588  0.252299  ]]. Action = [[0.02609713 0.00151976 0.05489869 0.90372896]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 950 is [True, False, False, False, False, True]
State prediction error at timestep 950 is tensor(2.7930e-06, grad_fn=<MseLossBackward0>)
Current timestep = 951. State = [[-0.2900152  0.2519893]]. Action = [[ 0.04088283  0.06305053 -0.09693485  0.7296288 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 951 is [True, False, False, False, False, True]
State prediction error at timestep 951 is tensor(1.4456e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 951 of -1
Current timestep = 952. State = [[-0.29023057  0.25254297]]. Action = [[ 0.04228183  0.09187224 -0.09012678  0.07034338]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 952 is [True, False, False, False, False, True]
State prediction error at timestep 952 is tensor(2.9857e-06, grad_fn=<MseLossBackward0>)
Current timestep = 953. State = [[-0.28980675  0.25412458]]. Action = [[-0.05127906  0.06529338 -0.0693017  -0.05802441]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 953 is [True, False, False, False, False, True]
State prediction error at timestep 953 is tensor(5.7172e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 953 of -1
Current timestep = 954. State = [[-0.29100257  0.25721014]]. Action = [[ 0.08474614  0.07847514 -0.0494123   0.11438584]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 954 is [True, False, False, False, False, True]
State prediction error at timestep 954 is tensor(3.1655e-06, grad_fn=<MseLossBackward0>)
Current timestep = 955. State = [[-0.28962952  0.26072487]]. Action = [[-0.02852117 -0.00144452  0.05071793  0.96183467]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 955 is [True, False, False, False, False, True]
State prediction error at timestep 955 is tensor(2.6082e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 955 of -1
Current timestep = 956. State = [[-0.2888019   0.26341537]]. Action = [[ 0.01539852 -0.07386138  0.07025605 -0.15085357]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 956 is [True, False, False, False, False, True]
State prediction error at timestep 956 is tensor(3.3780e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 956 of -1
Current timestep = 957. State = [[-0.28810146  0.26333135]]. Action = [[ 0.0805049  -0.01313686 -0.01328045  0.09986198]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 957 is [True, False, False, False, False, True]
State prediction error at timestep 957 is tensor(2.2140e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 957 of -1
Current timestep = 958. State = [[-0.2863481  0.2631921]]. Action = [[-0.06189419  0.01828144  0.01864973 -0.6153278 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 958 is [True, False, False, False, False, True]
State prediction error at timestep 958 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 959. State = [[-0.2862423   0.26351312]]. Action = [[ 0.05356238  0.04714339 -0.02391674 -0.41505814]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 959 is [True, False, False, False, False, True]
State prediction error at timestep 959 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 959 of -1
Current timestep = 960. State = [[-0.2850106   0.26417902]]. Action = [[0.02732538 0.02086407 0.04785075 0.5268469 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 960 is [True, False, False, False, False, True]
State prediction error at timestep 960 is tensor(4.5541e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 960 of -1
Current timestep = 961. State = [[-0.28299567  0.26537964]]. Action = [[-0.00939969 -0.02756159  0.09088577  0.68631697]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 961 is [True, False, False, False, False, True]
State prediction error at timestep 961 is tensor(4.3415e-05, grad_fn=<MseLossBackward0>)
Current timestep = 962. State = [[-0.2811041  0.2663609]]. Action = [[ 0.02392111  0.04762889 -0.07112017  0.23330522]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 962 is [True, False, False, False, False, True]
State prediction error at timestep 962 is tensor(1.1425e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 962 of -1
Current timestep = 963. State = [[-0.27962783  0.2675848 ]]. Action = [[ 0.05426375 -0.02616534 -0.02130681 -0.733247  ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 963 is [True, False, False, False, False, True]
State prediction error at timestep 963 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 963 of -1
Current timestep = 964. State = [[-0.27751982  0.26830795]]. Action = [[-0.02296381 -0.02159368  0.08300091 -0.7329931 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 964 is [True, False, False, False, False, True]
State prediction error at timestep 964 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 965. State = [[-0.27690113  0.2679997 ]]. Action = [[-0.06903043 -0.04710801 -0.04416177 -0.1491794 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 965 is [True, False, False, False, False, True]
State prediction error at timestep 965 is tensor(7.4791e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 965 of -1
Current timestep = 966. State = [[-0.27682242  0.26760483]]. Action = [[ 0.03097738 -0.04053611 -0.03003166  0.08907497]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 966 is [True, False, False, False, False, True]
State prediction error at timestep 966 is tensor(2.5581e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 966 of -1
Current timestep = 967. State = [[-0.27616337  0.2662529 ]]. Action = [[ 0.02818967 -0.00917877  0.09755649  0.75758886]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 967 is [True, False, False, False, False, True]
State prediction error at timestep 967 is tensor(1.1800e-05, grad_fn=<MseLossBackward0>)
Current timestep = 968. State = [[-0.27555642  0.26512438]]. Action = [[-0.01113503 -0.05617952  0.00219557  0.7340256 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 968 is [True, False, False, False, False, True]
State prediction error at timestep 968 is tensor(4.1973e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 968 of -1
Current timestep = 969. State = [[-0.27457777  0.2632521 ]]. Action = [[-0.0782038  -0.07886501  0.08724087  0.05782712]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 969 is [True, False, False, False, False, True]
State prediction error at timestep 969 is tensor(1.4349e-05, grad_fn=<MseLossBackward0>)
Current timestep = 970. State = [[-0.2738691  0.2613241]]. Action = [[-0.03724997  0.00453126 -0.07860641 -0.15224129]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 970 is [True, False, False, False, False, True]
State prediction error at timestep 970 is tensor(3.5142e-06, grad_fn=<MseLossBackward0>)
Current timestep = 971. State = [[-0.27401605  0.2605424 ]]. Action = [[ 0.07984071  0.05999894 -0.01193794  0.7414062 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 971 is [True, False, False, False, False, True]
State prediction error at timestep 971 is tensor(4.4648e-06, grad_fn=<MseLossBackward0>)
Current timestep = 972. State = [[-0.2742008   0.26048216]]. Action = [[ 0.05869567  0.06021822  0.01366436 -0.2099399 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 972 is [True, False, False, False, False, True]
State prediction error at timestep 972 is tensor(6.6526e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 972 of -1
Current timestep = 973. State = [[-0.27406687  0.260717  ]]. Action = [[-0.04415122 -0.06551632 -0.09193158 -0.80429476]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 973 is [True, False, False, False, False, True]
State prediction error at timestep 973 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 974. State = [[-0.2738735  0.2604259]]. Action = [[ 0.05858899  0.06321325  0.03842867 -0.65849686]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 974 is [True, False, False, False, False, True]
State prediction error at timestep 974 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 975. State = [[-0.2736086  0.2603466]]. Action = [[ 0.08605359 -0.09220131 -0.03053083 -0.33003986]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 975 is [True, False, False, False, False, True]
State prediction error at timestep 975 is tensor(6.8563e-05, grad_fn=<MseLossBackward0>)
Current timestep = 976. State = [[-0.27169538  0.25843278]]. Action = [[-0.05830207 -0.05183095  0.05456897 -0.35647297]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 976 is [True, False, False, False, False, True]
State prediction error at timestep 976 is tensor(8.1341e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 976 of 0
Current timestep = 977. State = [[-0.2707522  0.2564566]]. Action = [[-0.05031949 -0.09290511  0.08804999  0.500231  ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 977 is [True, False, False, False, False, True]
State prediction error at timestep 977 is tensor(2.5989e-05, grad_fn=<MseLossBackward0>)
Current timestep = 978. State = [[-0.2695682  0.2535037]]. Action = [[-0.02526359  0.01480652 -0.06153774 -0.2309115 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 978 is [True, False, False, False, False, True]
State prediction error at timestep 978 is tensor(2.3512e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 978 of 0
Current timestep = 979. State = [[-0.26924515  0.25221163]]. Action = [[-0.02272008 -0.01748276 -0.05550698 -0.00744236]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 979 is [True, False, False, False, False, True]
State prediction error at timestep 979 is tensor(7.8193e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 979 of 0
Current timestep = 980. State = [[-0.26886523  0.25130966]]. Action = [[ 0.09278934  0.08156509  0.08551631 -0.48797488]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 980 is [True, False, False, False, False, True]
State prediction error at timestep 980 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 980 of 0
Current timestep = 981. State = [[-0.26912054  0.25162768]]. Action = [[-0.06508467  0.05980604 -0.03242767 -0.1710509 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 981 is [True, False, False, False, False, True]
State prediction error at timestep 981 is tensor(6.3308e-06, grad_fn=<MseLossBackward0>)
Current timestep = 982. State = [[-0.27034265  0.2528589 ]]. Action = [[ 0.07822099  0.05739582 -0.08390697  0.19036007]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 982 is [True, False, False, False, False, True]
State prediction error at timestep 982 is tensor(5.5529e-06, grad_fn=<MseLossBackward0>)
Current timestep = 983. State = [[-0.2708736   0.25361636]]. Action = [[-0.08524095 -0.09643258 -0.00350748  0.12885082]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 983 is [True, False, False, False, False, True]
State prediction error at timestep 983 is tensor(2.2847e-05, grad_fn=<MseLossBackward0>)
Current timestep = 984. State = [[-0.2710698  0.253768 ]]. Action = [[-0.01411339  0.06001673 -0.03194239 -0.13426358]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 984 is [True, False, False, False, False, True]
State prediction error at timestep 984 is tensor(4.6015e-06, grad_fn=<MseLossBackward0>)
Current timestep = 985. State = [[-0.27179834  0.254617  ]]. Action = [[ 0.06665967 -0.04338906  0.04078411  0.54983544]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 985 is [True, False, False, False, False, True]
State prediction error at timestep 985 is tensor(5.3881e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 985 of 0
Current timestep = 986. State = [[-0.27157217  0.254286  ]]. Action = [[0.0624296  0.06479552 0.06123858 0.48315632]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 986 is [True, False, False, False, False, True]
State prediction error at timestep 986 is tensor(2.9194e-06, grad_fn=<MseLossBackward0>)
Current timestep = 987. State = [[-0.2711511   0.25458264]]. Action = [[-0.06893891 -0.03394534 -0.03018631 -0.4453035 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 987 is [True, False, False, False, False, True]
State prediction error at timestep 987 is tensor(8.9359e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 987 of 0
Current timestep = 988. State = [[-0.27114636  0.25460967]]. Action = [[ 0.02869565 -0.08627372  0.01689579 -0.48336506]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 988 is [True, False, False, False, False, True]
State prediction error at timestep 988 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 989. State = [[-0.27044758  0.2532471 ]]. Action = [[-0.04464192 -0.03501626 -0.00168668  0.942551  ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 989 is [True, False, False, False, False, True]
State prediction error at timestep 989 is tensor(2.3492e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 989 of 0
Current timestep = 990. State = [[-0.26973018  0.25173596]]. Action = [[ 0.03705791 -0.06855111 -0.05678292 -0.477517  ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 990 is [True, False, False, False, False, True]
State prediction error at timestep 990 is tensor(8.4868e-05, grad_fn=<MseLossBackward0>)
Current timestep = 991. State = [[-0.2686712   0.24953142]]. Action = [[ 0.03257782  0.0023121  -0.0828504   0.37750268]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 991 is [True, False, False, False, False, True]
State prediction error at timestep 991 is tensor(1.3060e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 991 of 0
Current timestep = 992. State = [[-0.26772866  0.24756712]]. Action = [[ 0.0812419  -0.02645832  0.01436989  0.48205805]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 992 is [True, False, False, False, False, True]
State prediction error at timestep 992 is tensor(9.3604e-06, grad_fn=<MseLossBackward0>)
Current timestep = 993. State = [[-0.26638684  0.24527916]]. Action = [[0.00246261 0.03898146 0.02515318 0.9436424 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 993 is [True, False, False, False, False, True]
State prediction error at timestep 993 is tensor(1.3399e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 993 of 1
Current timestep = 994. State = [[-0.26557043  0.24447522]]. Action = [[ 0.08071131 -0.09010518 -0.02878772 -0.01523691]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 994 is [True, False, False, False, False, True]
State prediction error at timestep 994 is tensor(7.5246e-06, grad_fn=<MseLossBackward0>)
Current timestep = 995. State = [[-0.2634725   0.24152176]]. Action = [[0.07772876 0.04247274 0.02035685 0.18590903]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 995 is [True, False, False, False, False, True]
State prediction error at timestep 995 is tensor(1.4880e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 995 of 1
Current timestep = 996. State = [[-0.2609054   0.24052234]]. Action = [[ 0.096325   -0.01916657 -0.0074596  -0.8820012 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 996 is [True, False, False, False, False, True]
State prediction error at timestep 996 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 996 of 1
Current timestep = 997. State = [[-0.2568997   0.23912698]]. Action = [[ 0.07932425 -0.01268587  0.0895825   0.37724614]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 997 is [True, False, False, False, False, True]
State prediction error at timestep 997 is tensor(2.8392e-06, grad_fn=<MseLossBackward0>)
Current timestep = 998. State = [[-0.25172368  0.23886634]]. Action = [[-0.0840805   0.08056071 -0.08855837 -0.08532619]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 998 is [True, False, False, False, False, True]
State prediction error at timestep 998 is tensor(1.0557e-05, grad_fn=<MseLossBackward0>)
Current timestep = 999. State = [[-0.24875839  0.24093118]]. Action = [[ 0.06048974 -0.00553293 -0.09351248  0.9005947 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 999 is [True, False, False, False, False, True]
State prediction error at timestep 999 is tensor(5.2371e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 999 of 1
Current timestep = 1000. State = [[-0.24630655  0.24203804]]. Action = [[-0.04053831 -0.00315832  0.01359066  0.13869023]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1000 is [True, False, False, False, False, True]
State prediction error at timestep 1000 is tensor(1.1164e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1000 of 1
Current timestep = 1001. State = [[-0.24614441  0.24240182]]. Action = [[-0.08940638 -0.07126857 -0.09212452 -0.29600608]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1001 is [True, False, False, False, False, True]
State prediction error at timestep 1001 is tensor(3.1565e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1002. State = [[-0.24612388  0.24170707]]. Action = [[-0.0815196  -0.07181726 -0.08934849 -0.30950707]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1002 is [True, False, False, False, False, True]
State prediction error at timestep 1002 is tensor(3.5073e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1003. State = [[-0.24598712  0.24026176]]. Action = [[-0.08949628  0.0613082  -0.01421808 -0.35711372]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1003 is [True, False, False, False, False, True]
State prediction error at timestep 1003 is tensor(7.9357e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1003 of 1
Current timestep = 1004. State = [[-0.24760996  0.24177374]]. Action = [[-0.03580047  0.07902392 -0.08112648  0.9301777 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1004 is [True, False, False, False, False, True]
State prediction error at timestep 1004 is tensor(3.5151e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1005. State = [[-0.25021166  0.24464183]]. Action = [[-0.02534369  0.09516785 -0.09321281  0.7644508 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1005 is [True, False, False, False, False, True]
State prediction error at timestep 1005 is tensor(1.0781e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1005 of 1
Current timestep = 1006. State = [[-0.25353155  0.24837336]]. Action = [[ 0.04182369 -0.022808    0.02630769 -0.7180326 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1006 is [True, False, False, False, False, True]
State prediction error at timestep 1006 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1007. State = [[-0.25446415  0.24936593]]. Action = [[-0.06482867 -0.08798722 -0.09111501  0.7262068 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1007 is [True, False, False, False, False, True]
State prediction error at timestep 1007 is tensor(4.0645e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1007 of 1
Current timestep = 1008. State = [[-0.25459504  0.24926391]]. Action = [[-0.04585925 -0.0265578  -0.04702934  0.4899553 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1008 is [True, False, False, False, False, True]
State prediction error at timestep 1008 is tensor(2.6738e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1009. State = [[-0.25531137  0.24880575]]. Action = [[ 0.04670949  0.07787802 -0.05304734  0.43868327]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1009 is [True, False, False, False, False, True]
State prediction error at timestep 1009 is tensor(3.9311e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1009 of -1
Current timestep = 1010. State = [[-0.25582957  0.24948308]]. Action = [[ 0.07608689  0.05418289  0.03365018 -0.19119275]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1010 is [True, False, False, False, False, True]
State prediction error at timestep 1010 is tensor(4.6872e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1010 of -1
Current timestep = 1011. State = [[-0.25613466  0.24989265]]. Action = [[ 0.0061592  -0.03505343  0.04660351 -0.79836804]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1011 is [True, False, False, False, False, True]
State prediction error at timestep 1011 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1012. State = [[-0.25609207  0.24978359]]. Action = [[-0.03463729 -0.03100004  0.01628365  0.37387967]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1012 is [True, False, False, False, False, True]
State prediction error at timestep 1012 is tensor(2.8993e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1012 of -1
Current timestep = 1013. State = [[-0.25604448  0.2496313 ]]. Action = [[ 0.04270998  0.05507805 -0.07093041  0.26516116]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1013 is [True, False, False, False, False, True]
State prediction error at timestep 1013 is tensor(6.2440e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1013 of -1
Current timestep = 1014. State = [[-0.25616187  0.24979746]]. Action = [[ 0.03130453  0.03044683 -0.02095415 -0.2476036 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1014 is [True, False, False, False, False, True]
State prediction error at timestep 1014 is tensor(5.5164e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1015. State = [[-0.25616187  0.24979746]]. Action = [[-0.01790597 -0.08461083  0.00929866 -0.20311719]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1015 is [True, False, False, False, False, True]
State prediction error at timestep 1015 is tensor(3.7567e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1015 of -1
Current timestep = 1016. State = [[-0.25582537  0.24900667]]. Action = [[-0.02742307 -0.02106965 -0.00206575  0.82315385]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1016 is [True, False, False, False, False, True]
State prediction error at timestep 1016 is tensor(1.0592e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1016 of -1
Current timestep = 1017. State = [[-0.25546783  0.2481763 ]]. Action = [[ 0.00099896 -0.07204406  0.01796941 -0.35750604]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1017 is [True, False, False, False, False, True]
State prediction error at timestep 1017 is tensor(9.0459e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1018. State = [[-0.25461164  0.24615063]]. Action = [[ 0.02968035  0.00686087 -0.05539186  0.7627387 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1018 is [True, False, False, False, False, True]
State prediction error at timestep 1018 is tensor(9.8434e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1018 of -1
Current timestep = 1019. State = [[-0.25402206  0.24483635]]. Action = [[ 0.09322906  0.07169618  0.06684483 -0.67514527]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1019 is [True, False, False, False, False, True]
State prediction error at timestep 1019 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1020. State = [[-0.25383756  0.24480967]]. Action = [[ 0.00125089  0.09156782 -0.09072953  0.7248527 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1020 is [True, False, False, False, False, True]
State prediction error at timestep 1020 is tensor(2.0349e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1020 of -1
Current timestep = 1021. State = [[-0.2538527   0.24559684]]. Action = [[0.08996747 0.08647475 0.07709195 0.9445143 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1021 is [True, False, False, False, False, True]
State prediction error at timestep 1021 is tensor(6.7263e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1021 of -1
Current timestep = 1022. State = [[-0.25233302  0.24776988]]. Action = [[-0.09824754 -0.05422858 -0.04549146 -0.86695313]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1022 is [True, False, False, False, False, True]
State prediction error at timestep 1022 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1023. State = [[-0.25287703  0.24853824]]. Action = [[-0.0823202  -0.02181204  0.06960208 -0.9489876 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1023 is [True, False, False, False, False, True]
State prediction error at timestep 1023 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1023 of -1
Current timestep = 1024. State = [[-0.25370425  0.24946243]]. Action = [[-0.03414805 -0.08650013  0.03061391 -0.5359906 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1024 is [True, False, False, False, False, True]
State prediction error at timestep 1024 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1024 of -1
Current timestep = 1025. State = [[-0.2539135   0.24935749]]. Action = [[ 0.04197072 -0.02259715  0.02199378  0.602666  ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1025 is [True, False, False, False, False, True]
State prediction error at timestep 1025 is tensor(1.0436e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1026. State = [[-0.25342357  0.24850927]]. Action = [[-0.02149333 -0.05031445 -0.07297352  0.9419154 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1026 is [True, False, False, False, False, True]
State prediction error at timestep 1026 is tensor(3.7088e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1026 of -1
Current timestep = 1027. State = [[-0.25294372  0.24720062]]. Action = [[-0.0442329   0.08378395  0.08817375 -0.7668129 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1027 is [True, False, False, False, False, True]
State prediction error at timestep 1027 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1028. State = [[-0.25361788  0.24799903]]. Action = [[ 0.07080633 -0.09284787 -0.03765634 -0.01968092]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1028 is [True, False, False, False, False, True]
State prediction error at timestep 1028 is tensor(7.5724e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1028 of -1
Current timestep = 1029. State = [[-0.2529261   0.24640541]]. Action = [[ 0.01864094 -0.00659923 -0.06761788 -0.16611296]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 1029 is [True, False, False, False, False, True]
State prediction error at timestep 1029 is tensor(1.2826e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1029 of -1
Current timestep = 1030. State = [[-0.25219363  0.24504964]]. Action = [[-0.01001451  0.04371733 -0.06158595  0.16620624]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 1030 is [True, False, False, False, False, True]
State prediction error at timestep 1030 is tensor(7.4681e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1031. State = [[-0.2523348   0.24518164]]. Action = [[-0.05605356  0.09420296 -0.09921224 -0.2917019 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 1031 is [True, False, False, False, False, True]
State prediction error at timestep 1031 is tensor(3.9729e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1031 of -1
Current timestep = 1032. State = [[-0.25405306  0.24723136]]. Action = [[ 0.00746779 -0.01302685  0.08836371 -0.82536864]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 1032 is [True, False, False, False, False, True]
State prediction error at timestep 1032 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1033. State = [[-0.25465712  0.24790339]]. Action = [[-0.0920924  -0.04219067  0.03528679 -0.76487505]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 1033 is [True, False, False, False, False, True]
State prediction error at timestep 1033 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1033 of -1
Current timestep = 1034. State = [[-0.2556805   0.24897203]]. Action = [[-0.07734837  0.08869851 -0.08721825 -0.5390285 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 1034 is [True, False, False, False, False, True]
State prediction error at timestep 1034 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1035. State = [[-0.25848615  0.25204134]]. Action = [[-0.01143489  0.05583365  0.00426985 -0.77428883]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 1035 is [True, False, False, False, False, True]
State prediction error at timestep 1035 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1035 of -1
Current timestep = 1036. State = [[-0.26130345  0.25510496]]. Action = [[-0.07031322 -0.05644041 -0.06085332 -0.98351043]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 1036 is [True, False, False, False, False, True]
State prediction error at timestep 1036 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1036 of -1
Current timestep = 1037. State = [[-0.26300916  0.25682122]]. Action = [[ 0.06366707  0.08817651 -0.05494416 -0.3711729 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 1037 is [True, False, False, False, False, True]
State prediction error at timestep 1037 is tensor(7.6694e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1038. State = [[-0.26446185  0.25843498]]. Action = [[ 0.0222455   0.06307472 -0.06740341  0.8875854 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 1038 is [True, False, False, False, False, True]
State prediction error at timestep 1038 is tensor(1.2158e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1038 of -1
Current timestep = 1039. State = [[-0.26601493  0.26026094]]. Action = [[ 0.06410737  0.01547331  0.03244821 -0.01851159]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 1039 is [True, False, False, False, False, True]
State prediction error at timestep 1039 is tensor(5.5550e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1040. State = [[-0.266361    0.26093027]]. Action = [[-0.08632924  0.01898573  0.04576354  0.20826137]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 1040 is [True, False, False, False, False, True]
State prediction error at timestep 1040 is tensor(1.1306e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1040 of -1
Current timestep = 1041. State = [[-0.26775044  0.26254636]]. Action = [[-0.01430482 -0.07987828  0.01510534  0.11306822]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 1041 is [True, False, False, False, False, True]
State prediction error at timestep 1041 is tensor(1.7790e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1042. State = [[-0.26793388  0.26257107]]. Action = [[-0.02505074 -0.0963452  -0.05268079  0.7684773 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 1042 is [True, False, False, False, False, True]
State prediction error at timestep 1042 is tensor(2.0479e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1042 of -1
Current timestep = 1043. State = [[-0.26740143  0.26132295]]. Action = [[ 0.06253331  0.04759113 -0.03259023 -0.83027613]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 1043 is [True, False, False, False, False, True]
State prediction error at timestep 1043 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1043 of -1
Current timestep = 1044. State = [[-0.26712605  0.26071882]]. Action = [[ 0.06221993  0.04405754 -0.08792294 -0.52273226]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 1044 is [True, False, False, False, False, True]
State prediction error at timestep 1044 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1045. State = [[-0.26699466  0.2606888 ]]. Action = [[0.03686772 0.0498128  0.02480958 0.88598657]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 1045 is [True, False, False, False, False, True]
State prediction error at timestep 1045 is tensor(4.0041e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1045 of -1
Current timestep = 1046. State = [[-0.2670308   0.26112932]]. Action = [[-0.07482958  0.05731735 -0.03053428 -0.79735917]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 1046 is [True, False, False, False, False, True]
State prediction error at timestep 1046 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1047. State = [[-0.26813322  0.26227453]]. Action = [[0.0769851  0.06505332 0.03770093 0.19330168]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 1047 is [True, False, False, False, False, True]
State prediction error at timestep 1047 is tensor(1.6417e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1047 of -1
Current timestep = 1048. State = [[-0.26809815  0.2633621 ]]. Action = [[ 0.04980699 -0.0408288  -0.02767271  0.14376259]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 1048 is [True, False, False, False, False, True]
State prediction error at timestep 1048 is tensor(1.4740e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1048 of -1
Current timestep = 1049. State = [[-0.26749057  0.2641421 ]]. Action = [[-0.01235558 -0.01013371 -0.08012609  0.69493794]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 1049 is [True, False, False, False, False, True]
State prediction error at timestep 1049 is tensor(3.0533e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1050. State = [[-0.26713195  0.26438966]]. Action = [[-0.04716019  0.01659616 -0.07477397  0.48130774]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 1050 is [True, False, False, False, False, True]
State prediction error at timestep 1050 is tensor(2.7510e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1050 of -1
Current timestep = 1051. State = [[-0.26725885  0.26455843]]. Action = [[-0.05702445  0.08088007  0.0032749  -0.64937013]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 1051 is [True, False, False, False, False, True]
State prediction error at timestep 1051 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1052. State = [[-0.26905134  0.26655304]]. Action = [[-0.01047482 -0.08505802 -0.01614126  0.09856725]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 1052 is [True, False, False, False, False, True]
State prediction error at timestep 1052 is tensor(4.1967e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1053. State = [[-0.2692526   0.26688474]]. Action = [[-0.05861693  0.04942244  0.0680336  -0.10936308]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 1053 is [True, False, False, False, False, True]
State prediction error at timestep 1053 is tensor(2.3425e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1053 of -1
Current timestep = 1054. State = [[-0.27102917  0.26874012]]. Action = [[-0.08574238  0.08459594  0.02327428  0.26830697]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 1054 is [True, False, False, False, False, True]
State prediction error at timestep 1054 is tensor(2.0647e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1055. State = [[-0.27438927  0.27238503]]. Action = [[-0.04402142 -0.07390869 -0.03477081  0.76401734]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 1055 is [True, False, False, False, False, True]
State prediction error at timestep 1055 is tensor(2.1239e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1056. State = [[-0.27578664  0.27378222]]. Action = [[0.01293075 0.00624673 0.07005019 0.21243274]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 1056 is [True, False, False, False, False, True]
State prediction error at timestep 1056 is tensor(7.3029e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1056 of -1
Current timestep = 1057. State = [[-0.27650282  0.27446756]]. Action = [[-0.01619181  0.09530211 -0.08133404  0.23791862]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 1057 is [True, False, False, False, False, True]
State prediction error at timestep 1057 is tensor(1.0654e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1058. State = [[-0.27882144  0.27686784]]. Action = [[-0.02823418  0.06828512 -0.07144432  0.61715364]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 1058 is [True, False, False, False, False, True]
State prediction error at timestep 1058 is tensor(6.9012e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1058 of -1
Current timestep = 1059. State = [[-0.28141996  0.2796512 ]]. Action = [[ 0.08330572 -0.00790523 -0.03736057  0.13979936]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 1059 is [True, False, False, False, False, True]
State prediction error at timestep 1059 is tensor(1.7430e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1059 of -1
Current timestep = 1060. State = [[-0.28169155  0.28009582]]. Action = [[-0.0298052   0.0887166   0.03209058 -0.9329862 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 1060 is [True, False, False, False, False, True]
State prediction error at timestep 1060 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1061. State = [[-0.28342336  0.28238633]]. Action = [[ 0.02454672  0.04480278 -0.0317835  -0.6710533 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 1061 is [True, False, False, False, False, True]
State prediction error at timestep 1061 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1061 of -1
Current timestep = 1062. State = [[-0.28387043  0.2851677 ]]. Action = [[-0.01042808  0.03511577 -0.07131781  0.48219275]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 1062 is [True, False, False, False, False, True]
State prediction error at timestep 1062 is tensor(3.6771e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1063. State = [[-0.28425092  0.287927  ]]. Action = [[-0.04818392 -0.06499413  0.0110237  -0.2209183 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 1063 is [True, False, False, False, False, True]
State prediction error at timestep 1063 is tensor(4.4131e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1063 of -1
Current timestep = 1064. State = [[-0.2846259   0.28850302]]. Action = [[-0.08213921 -0.05239166 -0.0503794   0.01465464]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 1064 is [True, False, False, False, False, True]
State prediction error at timestep 1064 is tensor(4.1144e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1065. State = [[-0.285136   0.2889656]]. Action = [[ 0.0813963  -0.02080159 -0.08661768 -0.6552467 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 1065 is [True, False, False, False, False, True]
State prediction error at timestep 1065 is tensor(6.6321e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1065 of -1
Current timestep = 1066. State = [[-0.28495255  0.2885868 ]]. Action = [[-0.03228962 -0.02138565  0.05910141 -0.8903795 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 1066 is [True, False, False, False, False, True]
State prediction error at timestep 1066 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1066 of -1
Current timestep = 1067. State = [[-0.28482795  0.28834996]]. Action = [[ 0.06736196  0.08583315  0.00630744 -0.54858106]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 1067 is [True, False, False, False, False, True]
State prediction error at timestep 1067 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1068. State = [[-0.2847015  0.2884187]]. Action = [[ 0.09110422 -0.05950771  0.04600022  0.72569346]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 1068 is [True, False, False, False, False, True]
State prediction error at timestep 1068 is tensor(2.1371e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1068 of -1
Current timestep = 1069. State = [[-0.28342596  0.28734046]]. Action = [[-0.06233872  0.05157258 -0.08700945  0.6224563 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 1069 is [True, False, False, False, False, True]
State prediction error at timestep 1069 is tensor(1.7558e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1070. State = [[-0.28343046  0.28755695]]. Action = [[ 0.07767824 -0.03501473 -0.06967337 -0.61373234]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 1070 is [True, False, False, False, False, True]
State prediction error at timestep 1070 is tensor(6.8965e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1070 of -1
Current timestep = 1071. State = [[-0.2821543  0.2876065]]. Action = [[-0.05399412  0.0612533  -0.03807721 -0.7746645 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 1071 is [True, False, False, False, False, True]
State prediction error at timestep 1071 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1072. State = [[-0.28225234  0.28806847]]. Action = [[-0.09730678 -0.09055886 -0.00878429  0.71678424]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 1072 is [True, False, False, False, False, True]
State prediction error at timestep 1072 is tensor(9.5563e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1072 of -1
Current timestep = 1073. State = [[-0.28266397  0.28816438]]. Action = [[-0.0419034   0.03435973 -0.0408929  -0.02151918]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 1073 is [True, False, False, False, False, True]
State prediction error at timestep 1073 is tensor(3.7742e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1073 of -1
Current timestep = 1074. State = [[-0.2835519   0.28890327]]. Action = [[-0.042199   -0.08632188 -0.00143814 -0.8340507 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 1074 is [True, False, False, False, False, True]
State prediction error at timestep 1074 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1075. State = [[-0.28354734  0.28853923]]. Action = [[ 0.08369825 -0.04565255  0.03876814 -0.8254935 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 1075 is [True, False, False, False, False, True]
State prediction error at timestep 1075 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1075 of -1
Current timestep = 1076. State = [[-0.28248638  0.28660828]]. Action = [[-0.08381929 -0.05446907  0.0439465   0.94071114]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 1076 is [True, False, False, False, False, True]
State prediction error at timestep 1076 is tensor(6.2548e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1077. State = [[-0.2821898   0.28488728]]. Action = [[0.04920229 0.07875068 0.0537646  0.11245739]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 1077 is [True, False, False, False, False, True]
State prediction error at timestep 1077 is tensor(1.7019e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1077 of -1
Current timestep = 1078. State = [[-0.2822447   0.28493935]]. Action = [[-0.07250584 -0.00774185 -0.08240038 -0.3359027 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 1078 is [True, False, False, False, False, True]
State prediction error at timestep 1078 is tensor(2.4605e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1078 of -1
Current timestep = 1079. State = [[-0.28298274  0.2852023 ]]. Action = [[ 0.03325019 -0.0984862   0.06660324  0.71811295]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 1079 is [True, False, False, False, False, True]
State prediction error at timestep 1079 is tensor(4.0525e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1080. State = [[-0.28208038  0.28309947]]. Action = [[ 0.05704298 -0.02556734 -0.07953174 -0.13011599]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 1080 is [True, False, False, False, False, True]
State prediction error at timestep 1080 is tensor(3.4662e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1080 of -1
Current timestep = 1081. State = [[-0.28087807  0.2809749 ]]. Action = [[ 0.09051139  0.00943311 -0.09492467  0.3479936 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 1081 is [True, False, False, False, False, True]
State prediction error at timestep 1081 is tensor(1.3934e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1082. State = [[-0.2797276   0.27901202]]. Action = [[-0.06303775 -0.05827111 -0.02137049  0.7239188 ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 1082 is [True, False, False, False, False, True]
State prediction error at timestep 1082 is tensor(1.8045e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1082 of -1
Current timestep = 1083. State = [[-0.2787735   0.27711272]]. Action = [[-0.09653904 -0.08505525  0.07834423  0.07439113]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 1083 is [True, False, False, False, False, True]
State prediction error at timestep 1083 is tensor(6.7130e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1083 of -1
Current timestep = 1084. State = [[-0.2794405   0.27416888]]. Action = [[ 0.02389114 -0.06982879  0.01078657 -0.81365114]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 1084 is [True, False, False, False, False, True]
State prediction error at timestep 1084 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1085. State = [[-0.27869713  0.27048776]]. Action = [[-0.02240231  0.02134699  0.07336681  0.03875256]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 1085 is [True, False, False, False, False, True]
State prediction error at timestep 1085 is tensor(1.1546e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1085 of -1
Current timestep = 1086. State = [[-0.27875814  0.26887324]]. Action = [[-0.02239086  0.03567035  0.03046543 -0.06241995]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 1086 is [True, False, False, False, False, True]
State prediction error at timestep 1086 is tensor(9.4356e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1087. State = [[-0.2793877  0.2690378]]. Action = [[-0.04881948 -0.01279805 -0.0868994  -0.9246478 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 1087 is [True, False, False, False, False, True]
State prediction error at timestep 1087 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1087 of -1
Current timestep = 1088. State = [[-0.2807173   0.26912308]]. Action = [[-0.02786286 -0.02244234  0.02310672  0.62834764]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 1088 is [True, False, False, False, False, True]
State prediction error at timestep 1088 is tensor(1.7075e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1088 of -1
Current timestep = 1089. State = [[-0.28178346  0.26887697]]. Action = [[ 0.07362031  0.08867001 -0.08285601  0.4610232 ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 1089 is [True, False, False, False, False, True]
State prediction error at timestep 1089 is tensor(3.0388e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1090. State = [[-0.28231558  0.2693556 ]]. Action = [[ 0.01236274 -0.01334171 -0.00278629 -0.70061564]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 1090 is [True, False, False, False, False, True]
State prediction error at timestep 1090 is tensor(9.5448e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1090 of -1
Current timestep = 1091. State = [[-0.282373   0.2695063]]. Action = [[ 0.0283022   0.03902062 -0.01054043  0.9467485 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 1091 is [True, False, False, False, False, True]
State prediction error at timestep 1091 is tensor(2.0489e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1092. State = [[-0.2824975  0.2696649]]. Action = [[-0.08957473 -0.07123836 -0.02677957  0.49923038]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 1092 is [True, False, False, False, False, True]
State prediction error at timestep 1092 is tensor(3.3679e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1092 of -1
Current timestep = 1093. State = [[-0.28280103  0.26979315]]. Action = [[-0.05139316  0.013466    0.0908535   0.08874726]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 1093 is [True, False, False, False, False, True]
State prediction error at timestep 1093 is tensor(1.7121e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1093 of -1
Current timestep = 1094. State = [[-0.28388554  0.27058697]]. Action = [[-0.09226542 -0.09262358 -0.0199608   0.704924  ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 1094 is [True, False, False, False, False, True]
State prediction error at timestep 1094 is tensor(1.1492e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1095. State = [[-0.2858631  0.2695252]]. Action = [[-0.0262294  -0.06662598 -0.08602305 -0.04170972]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 1095 is [True, False, False, False, False, True]
State prediction error at timestep 1095 is tensor(3.1645e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1095 of -1
Current timestep = 1096. State = [[-0.28804493  0.266808  ]]. Action = [[-0.00884113 -0.0670982   0.04188468 -0.87088996]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 1096 is [True, False, False, False, False, True]
State prediction error at timestep 1096 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1097. State = [[-0.29062578  0.26268303]]. Action = [[-0.03517601 -0.00277956 -0.08848881 -0.49381   ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 1097 is [True, False, False, False, False, True]
State prediction error at timestep 1097 is tensor(2.6105e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1097 of -1
Current timestep = 1098. State = [[-0.29317233  0.25988284]]. Action = [[ 0.03829115  0.06046031 -0.05929086 -0.6238107 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 1098 is [True, False, False, False, False, True]
State prediction error at timestep 1098 is tensor(5.9882e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1098 of -1
Current timestep = 1099. State = [[-0.29395208  0.2595726 ]]. Action = [[ 0.00086848 -0.05622032 -0.03772762  0.26905036]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 1099 is [True, False, False, False, False, True]
State prediction error at timestep 1099 is tensor(1.5033e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1100. State = [[-0.29391125  0.2584399 ]]. Action = [[ 0.04263724  0.09847616 -0.01575178  0.11861551]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 1100 is [True, False, False, False, False, True]
State prediction error at timestep 1100 is tensor(1.3023e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1100 of -1
Current timestep = 1101. State = [[-0.2944833   0.25889948]]. Action = [[-0.05457325 -0.05959318 -0.02966944 -0.43544722]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 1101 is [True, False, False, False, False, True]
State prediction error at timestep 1101 is tensor(6.1863e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1102. State = [[-0.2949236   0.25880083]]. Action = [[-0.0524663  -0.0339372   0.05512083  0.3971293 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 1102 is [True, False, False, False, False, True]
State prediction error at timestep 1102 is tensor(5.9813e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1102 of -1
Current timestep = 1103. State = [[-0.2961307   0.25826976]]. Action = [[-0.07118033 -0.00230572 -0.09649546  0.67627   ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 1103 is [True, False, False, False, False, True]
State prediction error at timestep 1103 is tensor(9.7001e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1103 of -1
Current timestep = 1104. State = [[-0.29846677  0.25793943]]. Action = [[ 5.0578266e-04 -9.1494299e-02  5.4710768e-02  5.4548049e-01]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 1104 is [True, False, False, False, False, True]
State prediction error at timestep 1104 is tensor(1.3819e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1105. State = [[-0.30073977  0.25512937]]. Action = [[-0.04816257  0.09204827  0.05005492 -0.88705015]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 1105 is [True, False, False, False, False, True]
State prediction error at timestep 1105 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1105 of -1
Current timestep = 1106. State = [[-0.30390355  0.25538558]]. Action = [[ 0.04851224  0.07546248  0.05169385 -0.38240266]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 1106 is [True, False, False, False, False, True]
State prediction error at timestep 1106 is tensor(5.5285e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1107. State = [[-0.30524808  0.25687602]]. Action = [[ 0.08010931  0.00519307 -0.07183865  0.49877357]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 1107 is [True, False, False, False, False, True]
State prediction error at timestep 1107 is tensor(2.5539e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1107 of -1
Current timestep = 1108. State = [[-0.30534515  0.25716373]]. Action = [[-0.07061186  0.06189021  0.01110341 -0.5132059 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 1108 is [True, False, False, False, False, True]
State prediction error at timestep 1108 is tensor(1.4223e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1108 of -1
Current timestep = 1109. State = [[-0.30754778  0.25960967]]. Action = [[ 0.08099813 -0.07902025 -0.03550158 -0.8011317 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 1109 is [True, False, False, False, False, True]
State prediction error at timestep 1109 is tensor(1.2407e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1109 of -1
Current timestep = 1110. State = [[-0.30704466  0.25899923]]. Action = [[-0.08113531  0.0030699  -0.03281295  0.04405928]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 1110 is [True, False, False, False, False, True]
State prediction error at timestep 1110 is tensor(6.8796e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1111. State = [[-0.30713633  0.25905585]]. Action = [[-0.02295439 -0.06995966  0.00649671 -0.70877326]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 1111 is [True, False, False, False, False, True]
State prediction error at timestep 1111 is tensor(3.1171e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1111 of -1
Current timestep = 1112. State = [[-0.30736664  0.25819594]]. Action = [[-0.03289813  0.04799285 -0.04872328 -0.6265524 ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 1112 is [True, False, False, False, False, True]
State prediction error at timestep 1112 is tensor(3.5974e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1113. State = [[-0.30815855  0.25827914]]. Action = [[ 0.06603526 -0.05031987 -0.0592516   0.3615669 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 1113 is [True, False, False, False, False, True]
State prediction error at timestep 1113 is tensor(4.7129e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1114. State = [[-0.30765012  0.25737822]]. Action = [[-0.05969142 -0.02274211 -0.08656739  0.60969996]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 1114 is [True, False, False, False, False, True]
State prediction error at timestep 1114 is tensor(1.0176e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1114 of -1
Current timestep = 1115. State = [[-0.3079882   0.25665072]]. Action = [[ 0.04759162  0.06603759 -0.02667511  0.9656962 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 1115 is [True, False, False, False, False, True]
State prediction error at timestep 1115 is tensor(1.2327e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1116. State = [[-0.30805835  0.2567655 ]]. Action = [[-0.03797624  0.02923689 -0.08430154 -0.9690701 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 1116 is [True, False, False, False, False, True]
State prediction error at timestep 1116 is tensor(7.5487e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1116 of -1
Current timestep = 1117. State = [[-0.30868652  0.25738192]]. Action = [[-0.03198715  0.04637975 -0.06014619  0.7680862 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 1117 is [True, False, False, False, False, True]
State prediction error at timestep 1117 is tensor(6.4635e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1117 of -1
Current timestep = 1118. State = [[-0.30998284  0.2587721 ]]. Action = [[ 0.08962543  0.00986209 -0.06854749  0.23061168]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 1118 is [True, False, False, False, False, True]
State prediction error at timestep 1118 is tensor(7.9693e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1119. State = [[-0.31006542  0.25908038]]. Action = [[-0.06603532 -0.06402049 -0.00531314  0.00632751]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 1119 is [True, False, False, False, False, True]
State prediction error at timestep 1119 is tensor(8.7075e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1120. State = [[-0.31012133  0.2591042 ]]. Action = [[0.04237805 0.07896639 0.04772312 0.6673193 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 1120 is [True, False, False, False, False, True]
State prediction error at timestep 1120 is tensor(4.3262e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1120 of -1
Current timestep = 1121. State = [[-0.31050134  0.25956473]]. Action = [[-0.07613336  0.00900541  0.09387211  0.7640109 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 1121 is [True, False, False, False, False, True]
State prediction error at timestep 1121 is tensor(1.9484e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1122. State = [[-0.31200388  0.2610471 ]]. Action = [[ 0.02678142  0.06969704 -0.08807916  0.7315664 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 1122 is [True, False, False, False, False, True]
State prediction error at timestep 1122 is tensor(6.0048e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1123. State = [[-0.31356052  0.26281738]]. Action = [[-0.0223885   0.03806009 -0.00829717 -0.02576417]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 1123 is [True, False, False, False, False, True]
State prediction error at timestep 1123 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1123 of -1
Current timestep = 1124. State = [[-0.3153417  0.264772 ]]. Action = [[ 0.07036319 -0.08191483 -0.07362063 -0.41823912]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 1124 is [True, False, False, False, False, True]
State prediction error at timestep 1124 is tensor(3.1771e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1124 of -1
Current timestep = 1125. State = [[-0.31477377  0.26452845]]. Action = [[-0.08137654 -0.00480912  0.03644402  0.9585885 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 1125 is [True, False, False, False, False, True]
State prediction error at timestep 1125 is tensor(1.9160e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1126. State = [[-0.31503856  0.26478165]]. Action = [[ 0.01059707 -0.0050156  -0.05841747 -0.08430701]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 1126 is [True, False, False, False, False, True]
State prediction error at timestep 1126 is tensor(9.9941e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1127. State = [[-0.31504714  0.26479226]]. Action = [[-0.02113761 -0.07357334 -0.06418827  0.54836214]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 1127 is [True, False, False, False, False, True]
State prediction error at timestep 1127 is tensor(1.5043e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1127 of -1
Current timestep = 1128. State = [[-0.31469655  0.26400825]]. Action = [[-0.02734104  0.04530519 -0.01994015 -0.6998559 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 1128 is [True, False, False, False, False, True]
State prediction error at timestep 1128 is tensor(3.0349e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1129. State = [[-0.3151643   0.26422113]]. Action = [[-0.03090604  0.04801858 -0.07182015  0.83622694]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 1129 is [True, False, False, False, False, True]
State prediction error at timestep 1129 is tensor(1.6206e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1130. State = [[-0.31649426  0.26537338]]. Action = [[-0.0804476  -0.08093935 -0.07221605  0.6072104 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 1130 is [True, False, False, False, False, True]
State prediction error at timestep 1130 is tensor(2.4269e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1130 of -1
Current timestep = 1131. State = [[-0.31824017  0.26535532]]. Action = [[-0.01062381 -0.0091297  -0.08241539  0.5490329 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 1131 is [True, False, False, False, False, True]
State prediction error at timestep 1131 is tensor(1.4535e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1132. State = [[-0.31926337  0.26508322]]. Action = [[ 0.09390724  0.07432824 -0.03174057 -0.4478594 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 1132 is [True, False, False, False, False, True]
State prediction error at timestep 1132 is tensor(6.7640e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1132 of -1
Current timestep = 1133. State = [[-0.31931463  0.26511884]]. Action = [[-0.01836109 -0.06515349 -0.05015878 -0.37440807]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 1133 is [True, False, False, False, False, True]
State prediction error at timestep 1133 is tensor(5.9243e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1134. State = [[-0.3190807   0.26465973]]. Action = [[-0.04846311 -0.00780132 -0.01902868  0.821403  ]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 1134 is [True, False, False, False, False, True]
State prediction error at timestep 1134 is tensor(1.5642e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1135. State = [[-0.31914946  0.26445892]]. Action = [[-0.01444311 -0.05502978 -0.00664455 -0.5192999 ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 1135 is [True, False, False, False, False, True]
State prediction error at timestep 1135 is tensor(1.2442e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1136. State = [[-0.31923297  0.26345327]]. Action = [[-0.03502431  0.00280039 -0.04917032 -0.9407262 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 1136 is [True, False, False, False, False, True]
State prediction error at timestep 1136 is tensor(6.8739e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1136 of -1
Current timestep = 1137. State = [[-0.31980702  0.26267302]]. Action = [[ 0.07054467 -0.06189097 -0.06109561  0.0771699 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 1137 is [True, False, False, False, False, True]
State prediction error at timestep 1137 is tensor(7.5500e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1137 of -1
Current timestep = 1138. State = [[-0.31873447  0.26037472]]. Action = [[-0.07471339 -0.02600634 -0.02387542  0.58339167]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 1138 is [True, False, False, False, False, True]
State prediction error at timestep 1138 is tensor(3.9251e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1139. State = [[-0.3192593  0.2582355]]. Action = [[ 0.05981355 -0.08428639 -0.00351387  0.76765966]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 1139 is [True, False, False, False, False, True]
State prediction error at timestep 1139 is tensor(1.0079e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1139 of -1
Current timestep = 1140. State = [[-0.31853038  0.25472298]]. Action = [[-0.04907069 -0.08439674  0.06083845  0.6942154 ]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 1140 is [True, False, False, False, False, True]
State prediction error at timestep 1140 is tensor(2.0073e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1141. State = [[-0.31884027  0.2507563 ]]. Action = [[-0.04048505  0.04118603 -0.013963    0.26889253]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 1141 is [True, False, False, False, False, True]
State prediction error at timestep 1141 is tensor(3.7553e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1141 of -1
Current timestep = 1142. State = [[-0.32100338  0.24851276]]. Action = [[ 0.01019202 -0.05672589 -0.02408056  0.97884834]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 1142 is [True, False, False, False, False, True]
State prediction error at timestep 1142 is tensor(4.6734e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1142 of -1
Current timestep = 1143. State = [[-0.3221968   0.24532318]]. Action = [[ 0.088637    0.06549574 -0.0617534   0.41556358]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 1143 is [True, False, False, False, False, True]
State prediction error at timestep 1143 is tensor(1.2943e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1144. State = [[-0.32183436  0.24471183]]. Action = [[ 0.00794484 -0.02865455 -0.02917711 -0.2911247 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 1144 is [True, False, False, False, False, True]
State prediction error at timestep 1144 is tensor(5.3761e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1144 of -1
Current timestep = 1145. State = [[-0.3213209   0.24370435]]. Action = [[-0.01792078  0.04959542 -0.02480098  0.8344387 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 1145 is [True, False, False, False, False, True]
State prediction error at timestep 1145 is tensor(1.3699e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1145 of -1
Current timestep = 1146. State = [[-0.3215177  0.2439314]]. Action = [[-0.03418858 -0.05055621 -0.04533595  0.7406373 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 1146 is [True, False, False, False, False, True]
State prediction error at timestep 1146 is tensor(1.2898e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1147. State = [[-0.32163393  0.24371842]]. Action = [[ 0.03577294  0.00126729 -0.07697994  0.44682074]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 1147 is [True, False, False, False, False, True]
State prediction error at timestep 1147 is tensor(5.9776e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1147 of -1
Current timestep = 1148. State = [[-0.32139602  0.24319515]]. Action = [[-0.06659765  0.02128067  0.00896907  0.07664204]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 1148 is [True, False, False, False, False, True]
State prediction error at timestep 1148 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1149. State = [[-0.3221561  0.2434427]]. Action = [[-0.08018277  0.06422117 -0.02253068 -0.5512957 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 1149 is [True, False, False, False, False, True]
State prediction error at timestep 1149 is tensor(2.0967e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1149 of -1
Current timestep = 1150. State = [[-0.3252255   0.24567537]]. Action = [[ 0.09187979  0.05835784 -0.03332143 -0.2724055 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 1150 is [True, False, False, False, False, True]
State prediction error at timestep 1150 is tensor(6.8010e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1151. State = [[-0.32601234  0.24743362]]. Action = [[ 0.0541125  -0.09328374 -0.09167653 -0.01118213]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 1151 is [True, False, False, False, False, True]
State prediction error at timestep 1151 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1151 of -1
Current timestep = 1152. State = [[-0.3251331   0.24668805]]. Action = [[-0.03091215 -0.08101055  0.04817582 -0.55299336]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 1152 is [True, False, False, False, False, True]
State prediction error at timestep 1152 is tensor(3.0545e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1153. State = [[-0.32447252  0.24498974]]. Action = [[0.03251243 0.00282323 0.09676515 0.85605645]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 1153 is [True, False, False, False, False, True]
State prediction error at timestep 1153 is tensor(1.7888e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1153 of -1
Current timestep = 1154. State = [[-0.3233092   0.24357359]]. Action = [[ 0.06516052 -0.03283823  0.01787464 -0.52953714]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 1154 is [True, False, False, False, False, True]
State prediction error at timestep 1154 is tensor(1.7770e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1154 of -1
Current timestep = 1155. State = [[-0.32169768  0.24121508]]. Action = [[ 0.00909679 -0.01258943 -0.02839373 -0.09624195]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 1155 is [True, False, False, False, False, True]
State prediction error at timestep 1155 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1156. State = [[-0.32051703  0.23895857]]. Action = [[-0.06388159 -0.0870064   0.03361023  0.5756991 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 1156 is [True, False, False, False, False, True]
State prediction error at timestep 1156 is tensor(3.3818e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1156 of -1
Current timestep = 1157. State = [[-0.31992865  0.23576559]]. Action = [[ 0.03871033 -0.05347407  0.02447711  0.4322543 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 1157 is [True, False, False, False, False, True]
State prediction error at timestep 1157 is tensor(6.0442e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1158. State = [[-0.31902042  0.23290952]]. Action = [[-0.03985412  0.06588159 -0.00810909 -0.36950952]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 1158 is [True, False, False, False, False, True]
State prediction error at timestep 1158 is tensor(3.2727e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1158 of -1
Current timestep = 1159. State = [[-0.3191763   0.23265353]]. Action = [[ 0.00653102  0.01072252 -0.00413926 -0.7720605 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 1159 is [True, False, False, False, False, True]
State prediction error at timestep 1159 is tensor(5.1642e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1159 of -1
Current timestep = 1160. State = [[-0.31920016  0.2329167 ]]. Action = [[0.04796436 0.08463297 0.00585999 0.87888646]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 1160 is [True, False, False, False, False, True]
State prediction error at timestep 1160 is tensor(7.9988e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1161. State = [[-0.3192823   0.23356007]]. Action = [[ 0.03025515 -0.02905051 -0.05159732 -0.5559689 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 1161 is [True, False, False, False, False, True]
State prediction error at timestep 1161 is tensor(1.1608e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1161 of -1
Current timestep = 1162. State = [[-0.31919307  0.2336478 ]]. Action = [[ 0.00911272  0.07810865 -0.07478762 -0.696939  ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 1162 is [True, False, False, False, False, True]
State prediction error at timestep 1162 is tensor(2.7880e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1163. State = [[-0.31931177  0.23449422]]. Action = [[ 0.09769662 -0.02757366 -0.09018306  0.9129207 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 1163 is [True, False, False, False, False, True]
State prediction error at timestep 1163 is tensor(3.5648e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1164. State = [[-0.31845304  0.23517504]]. Action = [[-0.00946841 -0.00624515  0.02260526  0.21956861]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 1164 is [True, False, False, False, False, True]
State prediction error at timestep 1164 is tensor(2.7665e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1164 of -1
Current timestep = 1165. State = [[-0.31808364  0.23570095]]. Action = [[-0.04646772 -0.04694958 -0.05516183 -0.7876157 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 1165 is [True, False, False, False, False, True]
State prediction error at timestep 1165 is tensor(2.7946e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1166. State = [[-0.31753665  0.23499954]]. Action = [[ 0.0027218  -0.09582096  0.0715818   0.93028617]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 1166 is [True, False, False, False, False, True]
State prediction error at timestep 1166 is tensor(4.6337e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1166 of -1
Current timestep = 1167. State = [[-0.3163364   0.23268563]]. Action = [[-0.04251435  0.07960839 -0.08738556 -0.3782711 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 1167 is [True, False, False, False, False, True]
State prediction error at timestep 1167 is tensor(2.5367e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1167 of -1
Current timestep = 1168. State = [[-0.31669655  0.23283252]]. Action = [[-0.03892734  0.08458865  0.03476781 -0.29607928]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 1168 is [True, False, False, False, False, True]
State prediction error at timestep 1168 is tensor(7.5751e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1169. State = [[-0.31804714  0.23450547]]. Action = [[-0.07998703  0.06471998  0.07960936 -0.4500841 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 1169 is [True, False, False, False, False, True]
State prediction error at timestep 1169 is tensor(3.5342e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1169 of -1
Current timestep = 1170. State = [[-0.32086998  0.23761605]]. Action = [[-0.03952726  0.03413153 -0.07165419 -0.5069543 ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 1170 is [True, False, False, False, False, True]
State prediction error at timestep 1170 is tensor(1.2983e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1171. State = [[-0.32385764  0.24063605]]. Action = [[ 0.03753961  0.06140035 -0.02430879 -0.89421296]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 1171 is [True, False, False, False, False, True]
State prediction error at timestep 1171 is tensor(1.3886e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1171 of -1
Current timestep = 1172. State = [[-0.3259757  0.242867 ]]. Action = [[-0.06585246 -0.05460998 -0.01017584  0.5041952 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 1172 is [True, False, False, False, False, True]
State prediction error at timestep 1172 is tensor(3.6942e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1172 of -1
Current timestep = 1173. State = [[-0.32726344  0.24414943]]. Action = [[-0.01292823  0.00612251 -0.09489643 -0.5765837 ]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 1173 is [True, False, False, False, False, True]
State prediction error at timestep 1173 is tensor(3.2436e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1174. State = [[-0.32836273  0.24532999]]. Action = [[-0.06908493  0.0025228   0.01510724  0.11904967]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 1174 is [True, False, False, False, False, True]
State prediction error at timestep 1174 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1174 of -1
Current timestep = 1175. State = [[-0.33019006  0.24682912]]. Action = [[0.08428217 0.02045367 0.07330661 0.8344734 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 1175 is [True, False, False, False, False, True]
State prediction error at timestep 1175 is tensor(2.3810e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1176. State = [[-0.33032635  0.24698766]]. Action = [[-0.06834482 -0.01278889 -0.08783187 -0.5697098 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 1176 is [True, False, False, False, False, True]
State prediction error at timestep 1176 is tensor(6.1180e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1177. State = [[-0.33095193  0.24753572]]. Action = [[ 0.08287681 -0.07553576 -0.07596354  0.01767135]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 1177 is [True, False, False, False, False, True]
State prediction error at timestep 1177 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1177 of -1
Current timestep = 1178. State = [[-0.33016625  0.24602342]]. Action = [[-0.00165014 -0.03313902 -0.05664205  0.57417965]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 1178 is [True, False, False, False, False, True]
State prediction error at timestep 1178 is tensor(2.6683e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1179. State = [[-0.32935843  0.24445167]]. Action = [[ 0.00397688 -0.04328883  0.00125308  0.01241708]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 1179 is [True, False, False, False, False, True]
State prediction error at timestep 1179 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1179 of -1
Current timestep = 1180. State = [[-0.32841524  0.24252382]]. Action = [[ 0.05496325  0.04752273 -0.05465164  0.67162704]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 1180 is [True, False, False, False, False, True]
State prediction error at timestep 1180 is tensor(3.1489e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1180 of -1
Current timestep = 1181. State = [[-0.3278216  0.242196 ]]. Action = [[ 0.03283108  0.07511698 -0.08134358 -0.12196845]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 1181 is [True, False, False, False, False, True]
State prediction error at timestep 1181 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1182. State = [[-0.32775626  0.24291651]]. Action = [[-0.01162174  0.07563005 -0.0421253  -0.55880183]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 1182 is [True, False, False, False, False, True]
State prediction error at timestep 1182 is tensor(9.2095e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1182 of -1
Current timestep = 1183. State = [[-0.32787156  0.24381562]]. Action = [[ 0.08609844 -0.05803893 -0.0407886   0.09717214]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 1183 is [True, False, False, False, False, True]
State prediction error at timestep 1183 is tensor(6.8573e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1183 of -1
Current timestep = 1184. State = [[-0.32710868  0.24390352]]. Action = [[-0.06936345 -0.01106687  0.00558828  0.34456384]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 1184 is [True, False, False, False, False, True]
State prediction error at timestep 1184 is tensor(2.2649e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1185. State = [[-0.3270269   0.24394388]]. Action = [[-0.0560164 -0.0540828 -0.0163308  0.9663553]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 1185 is [True, False, False, False, False, True]
State prediction error at timestep 1185 is tensor(9.1606e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1185 of -1
Current timestep = 1186. State = [[-0.32698196  0.24386492]]. Action = [[-0.04813289  0.00767682  0.06180621  0.9877659 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 1186 is [True, False, False, False, False, True]
State prediction error at timestep 1186 is tensor(2.5937e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1186 of -1
Current timestep = 1187. State = [[-0.327435   0.2441761]]. Action = [[-0.06762467 -0.0794418   0.03114358 -0.8614944 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 1187 is [True, False, False, False, False, True]
State prediction error at timestep 1187 is tensor(3.2733e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1188. State = [[-0.32836947  0.2427911 ]]. Action = [[ 0.07514412 -0.01359375  0.03891147  0.20649517]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 1188 is [True, False, False, False, False, True]
State prediction error at timestep 1188 is tensor(5.9870e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1188 of -1
Current timestep = 1189. State = [[-0.3283714  0.2413868]]. Action = [[ 0.07070381  0.0531152  -0.01323302  0.131971  ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 1189 is [True, False, False, False, False, True]
State prediction error at timestep 1189 is tensor(7.9633e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1189 of -1
Current timestep = 1190. State = [[-0.32795528  0.24144721]]. Action = [[0.01128433 0.09110277 0.04624797 0.34474778]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 1190 is [True, False, False, False, False, True]
State prediction error at timestep 1190 is tensor(1.0811e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1191. State = [[-0.32796827  0.24184364]]. Action = [[ 0.06597934 -0.03048118  0.02536099 -0.2529509 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 1191 is [True, False, False, False, False, True]
State prediction error at timestep 1191 is tensor(7.5678e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1191 of -1
Current timestep = 1192. State = [[-0.3273286   0.24196175]]. Action = [[-0.04346165 -0.09206837  0.03977636  0.6222248 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 1192 is [True, False, False, False, False, True]
State prediction error at timestep 1192 is tensor(3.8401e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1192 of -1
Current timestep = 1193. State = [[-0.326565    0.24085364]]. Action = [[0.01178648 0.05445605 0.0600391  0.5030823 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 1193 is [True, False, False, False, False, True]
State prediction error at timestep 1193 is tensor(2.1311e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1194. State = [[-0.3264269   0.24079943]]. Action = [[ 0.02624262  0.03463926 -0.03450775  0.82017875]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 1194 is [True, False, False, False, False, True]
State prediction error at timestep 1194 is tensor(1.5998e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1194 of -1
Current timestep = 1195. State = [[-0.32644528  0.24088463]]. Action = [[-0.02836762 -0.04035305 -0.09301046 -0.09336901]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 1195 is [True, False, False, False, False, True]
State prediction error at timestep 1195 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1195 of -1
Current timestep = 1196. State = [[-0.32641858  0.2408269 ]]. Action = [[-0.03489499 -0.01295692  0.02291214  0.44091773]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 1196 is [True, False, False, False, False, True]
State prediction error at timestep 1196 is tensor(1.1921e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1197. State = [[-0.3265403   0.24098326]]. Action = [[-0.07426189  0.0463867   0.0760311   0.92592156]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 1197 is [True, False, False, False, False, True]
State prediction error at timestep 1197 is tensor(2.3335e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1197 of -1
Current timestep = 1198. State = [[-0.32739383  0.2416868 ]]. Action = [[ 0.06828059 -0.028863    0.04620299  0.8731277 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 1198 is [True, False, False, False, False, True]
State prediction error at timestep 1198 is tensor(1.1547e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1199. State = [[-0.32730836  0.24152038]]. Action = [[ 0.04745146 -0.06603707 -0.09588628  0.48143828]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 1199 is [True, False, False, False, False, True]
State prediction error at timestep 1199 is tensor(3.1986e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1199 of -1
Current timestep = 1200. State = [[-0.32620293  0.24005464]]. Action = [[-0.06809995  0.05479396  0.06598365 -0.154297  ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 1200 is [True, False, False, False, False, True]
State prediction error at timestep 1200 is tensor(7.6424e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1200 of -1
Current timestep = 1201. State = [[-0.32647198  0.24020025]]. Action = [[-0.09383773  0.01060872 -0.07193644  0.51859665]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 1201 is [True, False, False, False, False, True]
State prediction error at timestep 1201 is tensor(4.2986e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1202. State = [[-0.32836258  0.24077702]]. Action = [[-0.0734056  -0.05973404 -0.08995852  0.6441381 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 1202 is [True, False, False, False, False, True]
State prediction error at timestep 1202 is tensor(1.5135e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1202 of -1
Current timestep = 1203. State = [[-0.33051425  0.24026337]]. Action = [[-0.01137749 -0.00438539 -0.0421514  -0.47292686]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 1203 is [True, False, False, False, False, True]
State prediction error at timestep 1203 is tensor(6.3315e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1203 of -1
Current timestep = 1204. State = [[-0.33212924  0.23979335]]. Action = [[ 0.01329509  0.01941786  0.00069897 -0.42117345]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 1204 is [True, False, False, False, False, True]
State prediction error at timestep 1204 is tensor(5.7914e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1205. State = [[-0.33282638  0.23999643]]. Action = [[ 0.02278163  0.00834124 -0.03553392 -0.57633305]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 1205 is [True, False, False, False, False, True]
State prediction error at timestep 1205 is tensor(3.2995e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1205 of -1
Current timestep = 1206. State = [[-0.33270833  0.2399672 ]]. Action = [[ 0.08036342 -0.05413709  0.06866839 -0.77704334]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 1206 is [True, False, False, False, False, True]
State prediction error at timestep 1206 is tensor(2.5874e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1206 of -1
Current timestep = 1207. State = [[-0.33186615  0.23869726]]. Action = [[-0.04832596 -0.01735429  0.06281982 -0.36283106]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 1207 is [True, False, False, False, False, True]
State prediction error at timestep 1207 is tensor(5.0451e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1208. State = [[-0.33140713  0.23775049]]. Action = [[ 0.05259567 -0.07826767  0.0834749   0.29248714]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 1208 is [True, False, False, False, False, True]
State prediction error at timestep 1208 is tensor(4.2908e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1208 of -1
Current timestep = 1209. State = [[-0.32992965  0.23463982]]. Action = [[ 0.01272521 -0.06656125 -0.01475723  0.25754094]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 1209 is [True, False, False, False, False, True]
State prediction error at timestep 1209 is tensor(2.4210e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1209 of -1
Current timestep = 1210. State = [[-0.32863066  0.23082915]]. Action = [[-0.06669707 -0.04904959 -0.04691352  0.8362857 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 1210 is [True, False, False, False, False, True]
State prediction error at timestep 1210 is tensor(1.7851e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1211. State = [[-0.3288122   0.22756499]]. Action = [[-0.01385631 -0.05603541  0.03269527 -0.5366908 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 1211 is [True, False, False, False, False, True]
State prediction error at timestep 1211 is tensor(4.9688e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1211 of -1
Current timestep = 1212. State = [[-0.32839686  0.22417659]]. Action = [[0.07587244 0.0226622  0.03963716 0.5494554 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 1212 is [True, False, False, False, False, True]
State prediction error at timestep 1212 is tensor(3.8726e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1212 of -1
Current timestep = 1213. State = [[-0.32731423  0.22200558]]. Action = [[ 0.06649672 -0.07037091  0.00545403 -0.8255337 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 1213 is [True, False, False, False, False, True]
State prediction error at timestep 1213 is tensor(6.2663e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1214. State = [[-0.32536978  0.21858793]]. Action = [[ 0.08654124 -0.02369589 -0.07065792 -0.8778027 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 1214 is [True, False, False, False, False, True]
State prediction error at timestep 1214 is tensor(7.6753e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1214 of -1
Current timestep = 1215. State = [[-0.32298848  0.21580088]]. Action = [[ 6.2560387e-02 -5.8285892e-04 -8.7836094e-02  8.4610260e-01]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 1215 is [True, False, False, False, False, True]
State prediction error at timestep 1215 is tensor(3.3877e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1216. State = [[-0.31946048  0.21325351]]. Action = [[ 0.07259919  0.08759687 -0.00401513  0.14912558]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 1216 is [True, False, False, False, False, True]
State prediction error at timestep 1216 is tensor(7.5041e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1217. State = [[-0.31839985  0.21411143]]. Action = [[-0.09630247  0.07121309 -0.06067101  0.19295835]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 1217 is [True, False, False, False, False, True]
State prediction error at timestep 1217 is tensor(1.0884e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1217 of -1
Current timestep = 1218. State = [[-0.31881404  0.21589468]]. Action = [[-0.09004644 -0.03739176 -0.02131951  0.27872062]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 1218 is [True, False, False, False, False, True]
State prediction error at timestep 1218 is tensor(9.4371e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1218 of -1
Current timestep = 1219. State = [[-0.31960854  0.21635379]]. Action = [[ 0.01090153 -0.06718214  0.03738198 -0.09354067]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 1219 is [True, False, False, False, False, True]
State prediction error at timestep 1219 is tensor(5.1104e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1220. State = [[-0.31954822  0.21592706]]. Action = [[-0.00731742  0.09086511 -0.04824727  0.1344155 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 1220 is [True, False, False, False, False, True]
State prediction error at timestep 1220 is tensor(1.9612e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1220 of -1
Current timestep = 1221. State = [[-0.32017496  0.21659108]]. Action = [[-0.05997008 -0.0650986   0.04466879 -0.36734676]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 1221 is [True, False, False, False, False, True]
State prediction error at timestep 1221 is tensor(7.6424e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1221 of -1
Current timestep = 1222. State = [[-0.3206588   0.21660256]]. Action = [[-0.01191525 -0.08104171  0.09340865  0.26387167]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 1222 is [True, False, False, False, False, True]
State prediction error at timestep 1222 is tensor(8.2202e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1223. State = [[-0.32098195  0.21495235]]. Action = [[ 0.06203263 -0.04315433  0.06558061  0.86647224]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 1223 is [True, False, False, False, False, True]
State prediction error at timestep 1223 is tensor(2.6629e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1223 of -1
Current timestep = 1224. State = [[-0.3199337   0.21229243]]. Action = [[-0.04235594 -0.03643766  0.0325072  -0.37337708]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 1224 is [True, False, False, False, False, True]
State prediction error at timestep 1224 is tensor(1.9823e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1224 of -1
Current timestep = 1225. State = [[-0.31970978  0.20965607]]. Action = [[ 0.09158177 -0.0652778   0.03429232 -0.9584529 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 1225 is [True, False, False, False, False, True]
State prediction error at timestep 1225 is tensor(7.4529e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1226. State = [[-0.31818548  0.20668912]]. Action = [[ 0.01705978  0.08238091 -0.08564226  0.87836254]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 1226 is [True, False, False, False, False, True]
State prediction error at timestep 1226 is tensor(4.7373e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1226 of -1
Current timestep = 1227. State = [[-0.31789958  0.20659743]]. Action = [[0.0985531  0.04455658 0.06782775 0.8386731 ]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 1227 is [True, False, False, False, False, True]
State prediction error at timestep 1227 is tensor(6.7308e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1228. State = [[-0.31717813  0.20673631]]. Action = [[-0.02803652 -0.03637917 -0.06655182  0.54807997]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 1228 is [True, False, False, False, False, True]
State prediction error at timestep 1228 is tensor(4.6536e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1228 of -1
Current timestep = 1229. State = [[-0.31692028  0.20643844]]. Action = [[-0.04554137 -0.0807647   0.05815559  0.76001453]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 1229 is [True, False, False, False, False, True]
State prediction error at timestep 1229 is tensor(2.3432e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1229 of -1
Current timestep = 1230. State = [[-0.3161128  0.2047463]]. Action = [[ 0.08724052 -0.06450054 -0.05669476 -0.9675536 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 1230 is [True, False, False, False, False, True]
State prediction error at timestep 1230 is tensor(6.2363e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1231. State = [[-0.31402928  0.20123167]]. Action = [[ 0.04750238  0.09768114  0.09391095 -0.7177116 ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 1231 is [True, False, False, False, False, True]
State prediction error at timestep 1231 is tensor(4.9043e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1231 of -1
Current timestep = 1232. State = [[-0.31358588  0.20083903]]. Action = [[ 0.02184967  0.03237917 -0.08156283 -0.2745465 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 1232 is [True, False, False, False, False, True]
State prediction error at timestep 1232 is tensor(8.0699e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1233. State = [[-0.31294435  0.2013973 ]]. Action = [[ 0.08708078  0.02332491 -0.01831011  0.25750482]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 1233 is [True, False, False, False, False, True]
State prediction error at timestep 1233 is tensor(1.0593e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1233 of -1
Current timestep = 1234. State = [[-0.31014803  0.20230572]]. Action = [[ 0.04219737 -0.08146684 -0.0532821  -0.0945121 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 1234 is [True, False, False, False, False, True]
State prediction error at timestep 1234 is tensor(8.2845e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1234 of -1
Current timestep = 1235. State = [[-0.30744344  0.20081191]]. Action = [[ 0.07867274 -0.02818199 -0.06706862 -0.6027311 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 1235 is [True, False, False, False, False, True]
State prediction error at timestep 1235 is tensor(1.9618e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1236. State = [[-0.3045157   0.19846937]]. Action = [[-0.03773384  0.03004289  0.08796645 -0.72089136]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 1236 is [True, False, False, False, False, True]
State prediction error at timestep 1236 is tensor(4.0686e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1236 of -1
Current timestep = 1237. State = [[-0.3036387   0.19857432]]. Action = [[-0.05301991  0.02523979  0.00853487  0.69686985]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 1237 is [True, False, False, False, False, True]
State prediction error at timestep 1237 is tensor(6.6635e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1237 of -1
Current timestep = 1238. State = [[-0.30367443  0.19861573]]. Action = [[-0.06752533 -0.04084232 -0.08811797 -0.42328715]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 1238 is [True, False, False, False, False, True]
State prediction error at timestep 1238 is tensor(1.0308e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1239. State = [[-0.30371755  0.19874217]]. Action = [[ 0.03729642  0.03317808 -0.05909542  0.32274532]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 1239 is [True, False, False, False, False, True]
State prediction error at timestep 1239 is tensor(7.4442e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1239 of -1
Current timestep = 1240. State = [[-0.30371755  0.19874217]]. Action = [[ 0.03004236  0.05124228  0.0457733  -0.40907705]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 1240 is [True, False, False, False, False, True]
State prediction error at timestep 1240 is tensor(2.5146e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1240 of -1
Current timestep = 1241. State = [[-0.30375355  0.19891293]]. Action = [[ 0.0557232   0.0445197  -0.07528119  0.6367943 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 1241 is [True, False, False, False, False, True]
State prediction error at timestep 1241 is tensor(4.6892e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1242. State = [[-0.30324063  0.19960053]]. Action = [[ 0.01951214 -0.07657085  0.08994002 -0.90468836]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 1242 is [True, False, False, False, False, True]
State prediction error at timestep 1242 is tensor(4.8997e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1242 of -1
Current timestep = 1243. State = [[-0.30291116  0.19953595]]. Action = [[0.06454832 0.06428026 0.07362399 0.08346558]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 1243 is [True, False, False, False, False, True]
State prediction error at timestep 1243 is tensor(8.4689e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1244. State = [[-0.30144542  0.20041329]]. Action = [[-0.02639553 -0.03415345  0.00736475 -0.27172494]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 1244 is [True, False, False, False, False, True]
State prediction error at timestep 1244 is tensor(9.4982e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1244 of -1
Current timestep = 1245. State = [[-0.3008306  0.2009013]]. Action = [[ 0.03523853  0.07367978  0.04935648 -0.48215175]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 1245 is [True, False, False, False, False, True]
State prediction error at timestep 1245 is tensor(1.8435e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1245 of -1
Current timestep = 1246. State = [[-0.29969126  0.20175427]]. Action = [[ 0.03353662  0.04171366 -0.05058504 -0.4961971 ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 1246 is [True, False, False, False, False, True]
State prediction error at timestep 1246 is tensor(1.1867e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1247. State = [[-0.29804754  0.20300409]]. Action = [[-0.03395148 -0.00805105  0.0386069   0.6963103 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 1247 is [True, False, False, False, False, True]
State prediction error at timestep 1247 is tensor(8.6655e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1248. State = [[-0.29724023  0.20386505]]. Action = [[-0.08541019  0.01337292 -0.06994089  0.0984633 ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 1248 is [True, False, False, False, False, True]
State prediction error at timestep 1248 is tensor(3.7432e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1248 of -1
Current timestep = 1249. State = [[-0.29817894  0.20506233]]. Action = [[-0.05556116  0.0190987   0.08523364  0.18133426]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 1249 is [True, False, False, False, False, True]
State prediction error at timestep 1249 is tensor(3.7776e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1250. State = [[-0.2991323   0.20625958]]. Action = [[ 0.02022028 -0.09151902 -0.09606775 -0.73415077]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 1250 is [True, False, False, False, False, True]
State prediction error at timestep 1250 is tensor(2.5515e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1250 of -1
Current timestep = 1251. State = [[-0.29903895  0.20591567]]. Action = [[ 0.03537183 -0.01122665 -0.0426976   0.4337058 ]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 1251 is [True, False, False, False, False, True]
State prediction error at timestep 1251 is tensor(2.8985e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1251 of -1
Current timestep = 1252. State = [[-0.29871687  0.20544647]]. Action = [[0.09432652 0.05765674 0.09917014 0.24912882]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 1252 is [True, False, False, False, False, True]
State prediction error at timestep 1252 is tensor(1.8972e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1253. State = [[-0.29827213  0.20541632]]. Action = [[ 0.07288749  0.04368287 -0.08930523 -0.8701873 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 1253 is [True, False, False, False, False, True]
State prediction error at timestep 1253 is tensor(4.2613e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1253 of -1
Current timestep = 1254. State = [[-0.29644993  0.20614748]]. Action = [[ 0.09527654 -0.07907724  0.01671179 -0.6542214 ]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 1254 is [True, False, False, False, False, True]
State prediction error at timestep 1254 is tensor(7.2037e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1255. State = [[-0.29326004  0.20473021]]. Action = [[-0.08930616 -0.06660557 -0.01556087  0.26322675]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 1255 is [True, False, False, False, False, True]
State prediction error at timestep 1255 is tensor(1.2002e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1255 of -1
Current timestep = 1256. State = [[-0.29200628  0.20288588]]. Action = [[-0.01583968 -0.06398158 -0.09015545 -0.14448905]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 1256 is [True, False, False, False, False, True]
State prediction error at timestep 1256 is tensor(3.2973e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1256 of -1
Current timestep = 1257. State = [[-0.29082447  0.20075311]]. Action = [[-0.08261491  0.0485745  -0.04755482 -0.34267294]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 1257 is [True, False, False, False, False, True]
State prediction error at timestep 1257 is tensor(1.4625e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1258. State = [[-0.29094684  0.20073263]]. Action = [[-0.00546949 -0.08735189 -0.08259057  0.6746813 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 1258 is [True, False, False, False, False, True]
State prediction error at timestep 1258 is tensor(1.4412e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1258 of -1
Current timestep = 1259. State = [[-0.2903597   0.19910257]]. Action = [[ 0.02002267 -0.03844127 -0.04150232  0.6978853 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 1259 is [True, False, False, False, False, True]
State prediction error at timestep 1259 is tensor(2.2323e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1259 of -1
Current timestep = 1260. State = [[-0.289392    0.19637813]]. Action = [[ 0.07022167 -0.07013458  0.00601304 -0.06317425]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 1260 is [True, False, False, False, False, True]
State prediction error at timestep 1260 is tensor(1.9236e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1261. State = [[-0.28787762  0.19263405]]. Action = [[2.2311509e-04 8.1027277e-02 1.6437843e-02 5.1648474e-01]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 1261 is [True, False, False, False, False, True]
State prediction error at timestep 1261 is tensor(2.3560e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1261 of -1
Current timestep = 1262. State = [[-0.28787374  0.19236696]]. Action = [[-0.06829956  0.09536753 -0.06477846  0.37252116]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 1262 is [True, False, False, False, False, True]
State prediction error at timestep 1262 is tensor(2.1511e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1262 of -1
Current timestep = 1263. State = [[-0.28887266  0.19358434]]. Action = [[ 0.02648688  0.06796791 -0.06123911 -0.47439075]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 1263 is [True, False, False, False, False, True]
State prediction error at timestep 1263 is tensor(8.6932e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1264. State = [[-0.28976676  0.19505379]]. Action = [[-0.06995057  0.02933421 -0.01077519 -0.747471  ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 1264 is [True, False, False, False, False, True]
State prediction error at timestep 1264 is tensor(2.4405e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1264 of -1
Current timestep = 1265. State = [[-0.29130283  0.19735774]]. Action = [[-0.0016194  -0.04319341  0.01445357 -0.04553276]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 1265 is [True, False, False, False, False, True]
State prediction error at timestep 1265 is tensor(2.9607e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1265 of -1
Current timestep = 1266. State = [[-0.29176992  0.19810365]]. Action = [[-0.07176975  0.0437819  -0.02257086  0.05366671]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 1266 is [True, False, False, False, False, True]
State prediction error at timestep 1266 is tensor(2.2516e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1267. State = [[-0.29333478  0.19987725]]. Action = [[ 0.00863874 -0.07565327  0.05652111 -0.32975328]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 1267 is [True, False, False, False, False, True]
State prediction error at timestep 1267 is tensor(2.7073e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1267 of -1
Current timestep = 1268. State = [[-0.2933643   0.19984086]]. Action = [[-0.0138488  -0.07426009 -0.08570948  0.908195  ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 1268 is [True, False, False, False, False, True]
State prediction error at timestep 1268 is tensor(1.7518e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1269. State = [[-0.29334167  0.1988414 ]]. Action = [[-0.01936613  0.08390746 -0.09356298 -0.10141432]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 1269 is [True, False, False, False, False, True]
State prediction error at timestep 1269 is tensor(2.0415e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1269 of -1
Current timestep = 1270. State = [[-0.29375988  0.19933447]]. Action = [[ 0.09596878 -0.08979969  0.07980732  0.8982961 ]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 1270 is [True, False, False, False, False, True]
State prediction error at timestep 1270 is tensor(1.8160e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1270 of -1
Current timestep = 1271. State = [[-0.29298863  0.19797602]]. Action = [[-0.05250266  0.07354171  0.01425824  0.4001987 ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 1271 is [True, False, False, False, False, True]
State prediction error at timestep 1271 is tensor(5.5968e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1272. State = [[-0.29323405  0.1982455 ]]. Action = [[-0.04542023 -0.06207019  0.08943618 -0.75258774]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 1272 is [True, False, False, False, False, True]
State prediction error at timestep 1272 is tensor(2.7487e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1272 of -1
Current timestep = 1273. State = [[-0.29354674  0.19793546]]. Action = [[ 0.07401068  0.07815232 -0.0785986  -0.78143144]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 1273 is [True, False, False, False, False, True]
State prediction error at timestep 1273 is tensor(4.6158e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1274. State = [[-0.2938231   0.19830762]]. Action = [[-0.08236262  0.09150475 -0.00557499 -0.5997425 ]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 1274 is [True, False, False, False, False, True]
State prediction error at timestep 1274 is tensor(2.2182e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1275. State = [[-0.29566774  0.20058568]]. Action = [[-0.04030094  0.03387845 -0.07504773  0.63920474]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 1275 is [True, False, False, False, False, True]
State prediction error at timestep 1275 is tensor(2.4190e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1275 of -1
Current timestep = 1276. State = [[-0.29792336  0.20339249]]. Action = [[-0.00337537 -0.05631061 -0.03293457  0.6114869 ]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 1276 is [True, False, False, False, False, True]
State prediction error at timestep 1276 is tensor(3.8007e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1276 of -1
Current timestep = 1277. State = [[-0.29849097  0.2040909 ]]. Action = [[-0.05652342  0.06671918  0.03523345 -0.7049167 ]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 1277 is [True, False, False, False, False, True]
State prediction error at timestep 1277 is tensor(1.9522e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1278. State = [[-0.3006414   0.20659906]]. Action = [[-0.04408045  0.05031457 -0.09532099 -0.5088502 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 1278 is [True, False, False, False, False, True]
State prediction error at timestep 1278 is tensor(3.6450e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1279. State = [[-0.30319142  0.20944215]]. Action = [[ 0.07822681  0.06688846 -0.05221375  0.53101635]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 1279 is [True, False, False, False, False, True]
State prediction error at timestep 1279 is tensor(1.9736e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1279 of -1
Current timestep = 1280. State = [[-0.30486616  0.2115584 ]]. Action = [[0.02402079 0.0607188  0.06181035 0.20235384]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 1280 is [True, False, False, False, False, True]
State prediction error at timestep 1280 is tensor(3.0166e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1281. State = [[-0.3061416   0.21336518]]. Action = [[ 0.05257776 -0.02754383  0.02404833  0.45034456]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 1281 is [True, False, False, False, False, True]
State prediction error at timestep 1281 is tensor(1.7149e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1281 of -1
Current timestep = 1282. State = [[-0.3060899   0.21365158]]. Action = [[-0.0634071  -0.06554598  0.09734175  0.5260935 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 1282 is [True, False, False, False, False, True]
State prediction error at timestep 1282 is tensor(1.3925e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1282 of -1
Current timestep = 1283. State = [[-0.3060526   0.21361615]]. Action = [[ 0.00554637 -0.0573675   0.01204469  0.6965083 ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 1283 is [True, False, False, False, False, True]
State prediction error at timestep 1283 is tensor(3.0103e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1284. State = [[-0.30561447  0.21293159]]. Action = [[-0.01913495 -0.09799174  0.08401912 -0.7224312 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 1284 is [True, False, False, False, False, True]
State prediction error at timestep 1284 is tensor(3.0542e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1284 of -1
Current timestep = 1285. State = [[-0.3046912   0.21064197]]. Action = [[ 0.01456042 -0.0197107   0.08852836 -0.76208264]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 1285 is [True, False, False, False, False, True]
State prediction error at timestep 1285 is tensor(5.6256e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1285 of -1
Current timestep = 1286. State = [[-0.30392805  0.20893478]]. Action = [[ 0.0254226   0.08117975 -0.04978861  0.3038416 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 1286 is [True, False, False, False, False, True]
State prediction error at timestep 1286 is tensor(2.0049e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1287. State = [[-0.30381867  0.20890613]]. Action = [[ 0.08381223  0.01421036 -0.04729459 -0.42403448]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 1287 is [True, False, False, False, False, True]
State prediction error at timestep 1287 is tensor(2.0664e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1287 of -1
Current timestep = 1288. State = [[-0.30324516  0.20900069]]. Action = [[-0.06623427 -0.07891227 -0.05583993  0.2628169 ]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 1288 is [True, False, False, False, False, True]
State prediction error at timestep 1288 is tensor(1.3011e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1288 of -1
Current timestep = 1289. State = [[-0.3029941  0.2080972]]. Action = [[-0.04966369 -0.09882894  0.06257389  0.77903247]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 1289 is [True, False, False, False, False, True]
State prediction error at timestep 1289 is tensor(1.4271e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1290. State = [[-0.30264506  0.2058566 ]]. Action = [[-9.9004135e-02  8.0430508e-04  9.1156192e-02 -8.2060879e-01]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 1290 is [True, False, False, False, False, True]
State prediction error at timestep 1290 is tensor(6.4401e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1290 of -1
Current timestep = 1291. State = [[-0.30404     0.20424134]]. Action = [[ 0.08502878 -0.0671652   0.08200679 -0.2171346 ]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 1291 is [True, False, False, False, False, True]
State prediction error at timestep 1291 is tensor(1.5535e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1292. State = [[-0.30314213  0.2012661 ]]. Action = [[ 0.09766489 -0.01395055  0.07104499 -0.67637867]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 1292 is [True, False, False, False, False, True]
State prediction error at timestep 1292 is tensor(4.9619e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1292 of -1
Current timestep = 1293. State = [[-0.30165488  0.19856283]]. Action = [[0.05097831 0.04484088 0.09111462 0.87513614]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 1293 is [True, False, False, False, False, True]
State prediction error at timestep 1293 is tensor(6.3879e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1293 of -1
Current timestep = 1294. State = [[-0.30076537  0.1975986 ]]. Action = [[ 0.0767603   0.06208093 -0.08922588  0.46658707]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 1294 is [True, False, False, False, False, True]
State prediction error at timestep 1294 is tensor(3.4402e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1295. State = [[-0.29952702  0.1980438 ]]. Action = [[-0.05317811 -0.0906271   0.03886857  0.8711095 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 1295 is [True, False, False, False, False, True]
State prediction error at timestep 1295 is tensor(9.3709e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1295 of -1
Current timestep = 1296. State = [[-0.2982287   0.19665831]]. Action = [[ 0.02953685 -0.09022499 -0.06468298  0.01431549]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 1296 is [True, False, False, False, False, True]
State prediction error at timestep 1296 is tensor(1.6253e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1296 of -1
Current timestep = 1297. State = [[-0.29607052  0.19321807]]. Action = [[-0.09627705 -0.04724899 -0.00603224  0.5369295 ]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 1297 is [True, False, False, False, False, True]
State prediction error at timestep 1297 is tensor(3.5164e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1298. State = [[-0.2957928  0.1904566]]. Action = [[ 0.03606621 -0.09561349 -0.04904712  0.411595  ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 1298 is [True, False, False, False, False, True]
State prediction error at timestep 1298 is tensor(3.1997e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1298 of -1
Current timestep = 1299. State = [[-0.29463297  0.18670744]]. Action = [[ 0.02420913  0.08144272  0.00958376 -0.9435261 ]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 1299 is [True, False, False, False, False, True]
State prediction error at timestep 1299 is tensor(9.4893e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1299 of -1
Current timestep = 1300. State = [[-0.29419014  0.1855527 ]]. Action = [[-0.03293532  0.00360256 -0.07957961 -0.7304539 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 1300 is [True, False, False, False, False, True]
State prediction error at timestep 1300 is tensor(3.3426e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1301. State = [[-0.29416665  0.18527012]]. Action = [[-0.03492311  0.05771141  0.01089619  0.3740654 ]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 1301 is [True, False, False, False, False, True]
State prediction error at timestep 1301 is tensor(1.6239e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1301 of -1
Current timestep = 1302. State = [[-0.29475096  0.18590325]]. Action = [[0.00980657 0.08763749 0.06522696 0.5795224 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 1302 is [True, False, False, False, False, True]
State prediction error at timestep 1302 is tensor(2.5085e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1303. State = [[-0.2958159   0.18712312]]. Action = [[-0.04195121 -0.06560833 -0.06028494  0.25130236]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 1303 is [True, False, False, False, False, True]
State prediction error at timestep 1303 is tensor(4.5662e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1303 of -1
Current timestep = 1304. State = [[-0.29611447  0.18739834]]. Action = [[-0.03930981  0.07267382 -0.06242669  0.5146687 ]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 1304 is [True, False, False, False, False, True]
State prediction error at timestep 1304 is tensor(1.2753e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1305. State = [[-0.29753947  0.18912354]]. Action = [[-0.04282786  0.03012467 -0.0495019   0.63003635]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 1305 is [True, False, False, False, False, True]
State prediction error at timestep 1305 is tensor(2.4092e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1305 of 1
Current timestep = 1306. State = [[-0.2993197  0.1912596]]. Action = [[ 7.7091135e-02  4.2432547e-04 -2.3338780e-02 -6.9824457e-01]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 1306 is [True, False, False, False, False, True]
State prediction error at timestep 1306 is tensor(9.0676e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1306 of 1
Current timestep = 1307. State = [[-0.29960704  0.19172084]]. Action = [[ 0.01687217  0.07499919 -0.02038342  0.8191987 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 1307 is [True, False, False, False, False, True]
State prediction error at timestep 1307 is tensor(4.0088e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1307 of 1
Current timestep = 1308. State = [[-0.30036035  0.19302168]]. Action = [[-0.09005393 -0.05094049 -0.04912302 -0.46503925]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 1308 is [True, False, False, False, False, True]
State prediction error at timestep 1308 is tensor(1.1902e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1309. State = [[-0.30134138  0.19416082]]. Action = [[ 0.0576734  -0.02056585 -0.0348681   0.08855295]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 1309 is [True, False, False, False, False, True]
State prediction error at timestep 1309 is tensor(5.0307e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1310. State = [[-0.3012231   0.19415477]]. Action = [[ 0.01486975  0.09006084 -0.01634106  0.68015933]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 1310 is [True, False, False, False, False, True]
State prediction error at timestep 1310 is tensor(4.0142e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1311. State = [[-0.3017377   0.19518647]]. Action = [[ 0.06501799  0.06458604 -0.04121237 -0.66735464]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 1311 is [True, False, False, False, False, True]
State prediction error at timestep 1311 is tensor(1.1787e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1312. State = [[-0.30206662  0.19695364]]. Action = [[ 0.09071172  0.01389952  0.02253846 -0.17126173]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 1312 is [True, False, False, False, False, True]
State prediction error at timestep 1312 is tensor(9.1013e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1312 of -1
Current timestep = 1313. State = [[-0.30033708  0.19843921]]. Action = [[ 0.07817972  0.04166644 -0.07693327 -0.42986614]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 1313 is [True, False, False, False, False, True]
State prediction error at timestep 1313 is tensor(5.2498e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1313 of -1
Current timestep = 1314. State = [[-0.2981939   0.20030536]]. Action = [[ 0.08012284  0.05656713 -0.04006807  0.2507707 ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 1314 is [True, False, False, False, False, True]
State prediction error at timestep 1314 is tensor(1.0112e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1315. State = [[-0.29432392  0.20297112]]. Action = [[ 0.08769912  0.05539092 -0.06972393 -0.860634  ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 1315 is [True, False, False, False, False, True]
State prediction error at timestep 1315 is tensor(5.6268e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1315 of -1
Current timestep = 1316. State = [[-0.28852087  0.20645572]]. Action = [[-0.00835814 -0.08797965 -0.03125051 -0.82809395]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 1316 is [True, False, False, False, False, True]
State prediction error at timestep 1316 is tensor(7.3748e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1317. State = [[-0.28448898  0.20794222]]. Action = [[-0.00165427  0.05825617 -0.02404581  0.58883595]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 1317 is [True, False, False, False, False, True]
State prediction error at timestep 1317 is tensor(9.4474e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1317 of -1
Current timestep = 1318. State = [[-0.2814796   0.20974594]]. Action = [[-0.01654565  0.07035709  0.0036304   0.00397897]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 1318 is [True, False, False, False, False, True]
State prediction error at timestep 1318 is tensor(7.2766e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1318 of -1
Current timestep = 1319. State = [[-0.28130233  0.21154529]]. Action = [[-0.06033829  0.06002345 -0.03123663 -0.06727958]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 1319 is [True, False, False, False, False, True]
State prediction error at timestep 1319 is tensor(2.4717e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1320. State = [[-0.28280273  0.21426426]]. Action = [[ 0.0580279   0.0152832  -0.01017304  0.25475812]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 1320 is [True, False, False, False, False, True]
State prediction error at timestep 1320 is tensor(2.1520e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1320 of 1
Current timestep = 1321. State = [[-0.28252497  0.21576537]]. Action = [[ 0.09587271 -0.00500531  0.077379   -0.37651992]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 1321 is [True, False, False, False, False, True]
State prediction error at timestep 1321 is tensor(8.0552e-09, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1321 of 1
Current timestep = 1322. State = [[-0.28075823  0.21708182]]. Action = [[ 0.03456161 -0.03791629 -0.0340118   0.44311452]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 1322 is [True, False, False, False, False, True]
State prediction error at timestep 1322 is tensor(4.3183e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1323. State = [[-0.27885407  0.21789055]]. Action = [[ 0.02578963 -0.07956926 -0.06359349  0.940622  ]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 1323 is [True, False, False, False, False, True]
State prediction error at timestep 1323 is tensor(6.3688e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1324. State = [[-0.27703667  0.21691617]]. Action = [[-0.00126053  0.06757028 -0.02413471 -0.7893172 ]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 1324 is [True, False, False, False, False, True]
State prediction error at timestep 1324 is tensor(5.4303e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1324 of 1
Current timestep = 1325. State = [[-0.27587104  0.21764168]]. Action = [[-0.04670852  0.02052735 -0.03152043 -0.14666092]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 1325 is [True, False, False, False, False, True]
State prediction error at timestep 1325 is tensor(1.5664e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1325 of 1
Current timestep = 1326. State = [[-0.2758183   0.21825643]]. Action = [[0.05846282 0.04458778 0.04734034 0.44311   ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 1326 is [True, False, False, False, False, True]
State prediction error at timestep 1326 is tensor(1.0679e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1327. State = [[-0.27448654  0.21917191]]. Action = [[ 0.065096    0.09467807  0.05672624 -0.5572801 ]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 1327 is [True, False, False, False, False, True]
State prediction error at timestep 1327 is tensor(1.5708e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1327 of 1
Current timestep = 1328. State = [[-0.27132082  0.22246218]]. Action = [[-0.01352677  0.06488795 -0.08501852 -0.7174509 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 1328 is [True, False, False, False, False, True]
State prediction error at timestep 1328 is tensor(6.9948e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1329. State = [[-0.2692475   0.22614789]]. Action = [[-0.09293803 -0.07705106 -0.02771027 -0.41370595]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 1329 is [True, False, False, False, False, True]
State prediction error at timestep 1329 is tensor(2.5570e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1330. State = [[-0.26872626  0.22752136]]. Action = [[ 0.06348663 -0.01890629 -0.08027266 -0.11125004]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 1330 is [True, False, False, False, False, True]
State prediction error at timestep 1330 is tensor(1.8189e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1330 of 1
Current timestep = 1331. State = [[-0.26752847  0.22794978]]. Action = [[ 0.02009417 -0.0566478  -0.02498631 -0.620577  ]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 1331 is [True, False, False, False, False, True]
State prediction error at timestep 1331 is tensor(4.9499e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1332. State = [[-0.2667083   0.22732867]]. Action = [[ 0.06408525  0.06460565 -0.02498804  0.8465731 ]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 1332 is [True, False, False, False, False, True]
State prediction error at timestep 1332 is tensor(3.0162e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1332 of 1
Current timestep = 1333. State = [[-0.26480377  0.22833902]]. Action = [[-0.0842347   0.0809632   0.07611204 -0.25953078]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 1333 is [True, False, False, False, False, True]
State prediction error at timestep 1333 is tensor(3.6664e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1333 of 1
Current timestep = 1334. State = [[-0.2649585   0.23026058]]. Action = [[ 0.05419979 -0.09325403  0.02086358 -0.20232064]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 1334 is [True, False, False, False, False, True]
State prediction error at timestep 1334 is tensor(1.0628e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1335. State = [[-0.26474574  0.2297574 ]]. Action = [[-0.0928968  -0.04793205 -0.05187208  0.47678697]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 1335 is [True, False, False, False, False, True]
State prediction error at timestep 1335 is tensor(2.4466e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1335 of 1
Current timestep = 1336. State = [[-0.2646737   0.22934245]]. Action = [[-0.09554249 -0.07345431 -0.01406396  0.6178541 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 1336 is [True, False, False, False, False, True]
State prediction error at timestep 1336 is tensor(3.1751e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1337. State = [[-0.26452345  0.22815295]]. Action = [[ 0.00356023 -0.09421235  0.09500916 -0.001513  ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 1337 is [True, False, False, False, False, True]
State prediction error at timestep 1337 is tensor(1.0009e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1337 of 1
Current timestep = 1338. State = [[-0.2637507   0.22559637]]. Action = [[ 0.05657082 -0.01209074 -0.05819765 -0.04149592]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 1338 is [True, False, False, False, False, True]
State prediction error at timestep 1338 is tensor(2.5126e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1338 of 1
Current timestep = 1339. State = [[-0.26298428  0.22360139]]. Action = [[0.09189457 0.05298739 0.02085693 0.95963955]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 1339 is [True, False, False, False, False, True]
State prediction error at timestep 1339 is tensor(3.8007e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1340. State = [[-0.26228538  0.2224192 ]]. Action = [[ 0.03453436 -0.0484832  -0.04952818  0.8477988 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 1340 is [True, False, False, False, False, True]
State prediction error at timestep 1340 is tensor(1.9988e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1340 of 1
Current timestep = 1341. State = [[-0.26093143  0.22020271]]. Action = [[ 0.07904271 -0.09264391  0.02228934 -0.39118755]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 1341 is [True, False, False, False, False, True]
State prediction error at timestep 1341 is tensor(9.8055e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1341 of 1
Current timestep = 1342. State = [[-0.25821063  0.2158479 ]]. Action = [[-0.0149494  -0.08149209  0.00747196 -0.48284125]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 1342 is [True, False, False, False, False, True]
State prediction error at timestep 1342 is tensor(4.3784e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1343. State = [[-0.25594774  0.21144722]]. Action = [[ 0.08309006 -0.05865574  0.00162476  0.0788703 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 1343 is [True, False, False, False, False, True]
State prediction error at timestep 1343 is tensor(2.5310e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1343 of 1
Current timestep = 1344. State = [[-0.25328726  0.20713228]]. Action = [[-0.01281916  0.05873039  0.09804938  0.7806308 ]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 1344 is [True, False, False, False, False, True]
State prediction error at timestep 1344 is tensor(8.0885e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1344 of 1
Current timestep = 1345. State = [[-0.25259542  0.20650049]]. Action = [[-0.08281726  0.09789408 -0.035693    0.65005565]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 1345 is [True, False, False, False, False, True]
State prediction error at timestep 1345 is tensor(3.0679e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1346. State = [[-0.25346792  0.20772651]]. Action = [[ 0.01701985  0.06021569 -0.04342633 -0.22438794]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 1346 is [True, False, False, False, False, True]
State prediction error at timestep 1346 is tensor(4.4035e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1346 of 1
Current timestep = 1347. State = [[-0.2542746   0.20903552]]. Action = [[-0.02747711 -0.00729576 -0.09763697  0.5223588 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 1347 is [True, False, False, False, False, True]
State prediction error at timestep 1347 is tensor(3.7483e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1347 of 1
Current timestep = 1348. State = [[-0.25493777  0.20998797]]. Action = [[-0.04257801 -0.05406048  0.03766244 -0.43950188]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 1348 is [True, False, False, False, False, True]
State prediction error at timestep 1348 is tensor(2.8357e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1349. State = [[-0.2550488   0.21012764]]. Action = [[ 0.00042669 -0.02180519 -0.00144842  0.22189796]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 1349 is [True, False, False, False, False, True]
State prediction error at timestep 1349 is tensor(9.3097e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1349 of 1
Current timestep = 1350. State = [[-0.2549254   0.20997716]]. Action = [[-0.07586035  0.03016714 -0.05654531  0.5305171 ]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 1350 is [True, False, False, False, False, True]
State prediction error at timestep 1350 is tensor(2.3734e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1350 of 1
Current timestep = 1351. State = [[-0.25556818  0.21054898]]. Action = [[-0.05526771 -0.07851891  0.01527385 -0.62697524]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 1351 is [True, False, False, False, False, True]
State prediction error at timestep 1351 is tensor(2.3191e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1352. State = [[-0.25631195  0.21017118]]. Action = [[-0.03017597  0.03015976 -0.00555266  0.5321436 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 1352 is [True, False, False, False, False, True]
State prediction error at timestep 1352 is tensor(2.1590e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1353. State = [[-0.25713554  0.21056356]]. Action = [[-0.08700921 -0.06533686 -0.08273785  0.03978527]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 1353 is [True, False, False, False, False, True]
State prediction error at timestep 1353 is tensor(1.6299e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1353 of 1
Current timestep = 1354. State = [[-0.25892323  0.2100017 ]]. Action = [[-0.08963215  0.02566528  0.0983879  -0.10059077]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 1354 is [True, False, False, False, False, True]
State prediction error at timestep 1354 is tensor(1.6491e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1355. State = [[-0.2618367   0.21082436]]. Action = [[-0.0092937   0.01975636  0.01943275  0.2374593 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 1355 is [True, False, False, False, False, True]
State prediction error at timestep 1355 is tensor(4.5698e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1356. State = [[-0.2644402   0.21165502]]. Action = [[ 0.05448977  0.0018342  -0.04835985 -0.49996006]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 1356 is [True, False, False, False, False, True]
State prediction error at timestep 1356 is tensor(5.4593e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1356 of 1
Current timestep = 1357. State = [[-0.2650706   0.21168265]]. Action = [[ 0.06006163  0.05186205 -0.08549894  0.87376785]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 1357 is [True, False, False, False, False, True]
State prediction error at timestep 1357 is tensor(2.3012e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1357 of 1
Current timestep = 1358. State = [[-0.26511446  0.2119291 ]]. Action = [[ 0.00592556  0.0032528   0.03103007 -0.5229263 ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 1358 is [True, False, False, False, False, True]
State prediction error at timestep 1358 is tensor(8.7646e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1359. State = [[-0.26527953  0.21208818]]. Action = [[0.0944474  0.0754688  0.08244524 0.6924188 ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 1359 is [True, False, False, False, False, True]
State prediction error at timestep 1359 is tensor(5.3352e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1359 of 1
Current timestep = 1360. State = [[-0.26526126  0.21226674]]. Action = [[ 0.02008077 -0.04587179 -0.0157852   0.94964314]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 1360 is [True, False, False, False, False, True]
State prediction error at timestep 1360 is tensor(1.8916e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1360 of 1
Current timestep = 1361. State = [[-0.2652998   0.21231905]]. Action = [[-0.01409601  0.06376428 -0.07625376  0.907794  ]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 1361 is [True, False, False, False, False, True]
State prediction error at timestep 1361 is tensor(1.4016e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1362. State = [[-0.26550743  0.21261519]]. Action = [[ 0.06966615 -0.02944426 -0.09096628  0.00029886]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 1362 is [True, False, False, False, False, True]
State prediction error at timestep 1362 is tensor(1.8598e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1363. State = [[-0.26509643  0.21276325]]. Action = [[ 0.06102205  0.00879721  0.03881187 -0.02197105]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 1363 is [True, False, False, False, False, True]
State prediction error at timestep 1363 is tensor(2.5680e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1363 of 1
Current timestep = 1364. State = [[-0.26377165  0.21305545]]. Action = [[ 0.04760788 -0.05142004  0.03268727  0.7634361 ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 1364 is [True, False, False, False, False, True]
State prediction error at timestep 1364 is tensor(3.3689e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1364 of 1
Current timestep = 1365. State = [[-0.26192385  0.21236064]]. Action = [[ 0.06163571  0.08758564 -0.06070108 -0.59170306]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 1365 is [True, False, False, False, False, True]
State prediction error at timestep 1365 is tensor(3.5788e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1366. State = [[-0.2594447   0.21407868]]. Action = [[-0.06995068  0.03387492 -0.02423459  0.13146102]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 1366 is [True, False, False, False, False, True]
State prediction error at timestep 1366 is tensor(1.1603e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1367. State = [[-0.25942016  0.21488254]]. Action = [[-0.03295813  0.02759311 -0.06949998  0.42416763]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 1367 is [True, False, False, False, False, True]
State prediction error at timestep 1367 is tensor(1.4218e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1367 of 1
Current timestep = 1368. State = [[-0.25993484  0.21599545]]. Action = [[-0.037016    0.05540169 -0.06518184 -0.20062673]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 1368 is [True, False, False, False, False, True]
State prediction error at timestep 1368 is tensor(9.7468e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1368 of 1
Current timestep = 1369. State = [[-0.26104367  0.21785603]]. Action = [[-0.07270692 -0.06689449 -0.01003855 -0.30860078]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 1369 is [True, False, False, False, False, True]
State prediction error at timestep 1369 is tensor(4.7559e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1370. State = [[-0.26141915  0.21838555]]. Action = [[-0.01398768 -0.01019309  0.02820895 -0.7463632 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 1370 is [True, False, False, False, False, True]
State prediction error at timestep 1370 is tensor(3.8390e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1370 of 1
Current timestep = 1371. State = [[-0.26179582  0.21884155]]. Action = [[-0.00081764  0.07097068 -0.08392433 -0.20897388]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 1371 is [True, False, False, False, False, True]
State prediction error at timestep 1371 is tensor(4.8293e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1372. State = [[-0.2629001   0.22025639]]. Action = [[0.04595347 0.07562774 0.05003227 0.2401092 ]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 1372 is [True, False, False, False, False, True]
State prediction error at timestep 1372 is tensor(2.1477e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1372 of 1
Current timestep = 1373. State = [[-0.264372    0.22227317]]. Action = [[-0.03348102 -0.06977213 -0.00987083 -0.6880674 ]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 1373 is [True, False, False, False, False, True]
State prediction error at timestep 1373 is tensor(2.7393e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1373 of 1
Current timestep = 1374. State = [[-0.26457304  0.2225859 ]]. Action = [[ 0.08688677  0.00568521 -0.01999538 -0.25380278]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 1374 is [True, False, False, False, False, True]
State prediction error at timestep 1374 is tensor(1.4358e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1375. State = [[-0.26436016  0.22246245]]. Action = [[-0.08247548 -0.04135196 -0.04710817  0.7433938 ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 1375 is [True, False, False, False, False, True]
State prediction error at timestep 1375 is tensor(1.3238e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1375 of 1
Current timestep = 1376. State = [[-0.26438835  0.22242764]]. Action = [[-0.00113339  0.0943707  -0.01985949  0.86208284]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 1376 is [True, False, False, False, False, True]
State prediction error at timestep 1376 is tensor(1.5543e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1376 of 1
Current timestep = 1377. State = [[-0.26545876  0.22380078]]. Action = [[-0.06748293 -0.00323587  0.0829783   0.806306  ]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 1377 is [True, False, False, False, False, True]
State prediction error at timestep 1377 is tensor(8.3792e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1378. State = [[-0.26679978  0.22529918]]. Action = [[-0.09731378  0.00807174 -0.09002376  0.12749708]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 1378 is [True, False, False, False, False, True]
State prediction error at timestep 1378 is tensor(8.1980e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1378 of 1
Current timestep = 1379. State = [[-0.2686377  0.2275134]]. Action = [[ 0.01410812 -0.07818694 -0.09177285  0.21976984]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 1379 is [True, False, False, False, False, True]
State prediction error at timestep 1379 is tensor(2.3006e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1379 of 1
Current timestep = 1380. State = [[-0.2687776   0.22746384]]. Action = [[ 0.09755117 -0.04068608 -0.0017949  -0.8832073 ]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 1380 is [True, False, False, False, False, True]
State prediction error at timestep 1380 is tensor(6.1448e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1381. State = [[-0.26807916  0.22623852]]. Action = [[-0.0384298  -0.06857152 -0.08206751  0.5501275 ]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 1381 is [True, False, False, False, False, True]
State prediction error at timestep 1381 is tensor(1.6338e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1381 of 1
Current timestep = 1382. State = [[-0.26726493  0.22458036]]. Action = [[ 0.05443827 -0.00343039  0.09125385 -0.8565806 ]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 1382 is [True, False, False, False, False, True]
State prediction error at timestep 1382 is tensor(7.2831e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1383. State = [[-0.26640186  0.22332385]]. Action = [[ 0.07662398  0.05938596 -0.02453393 -0.78707564]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 1383 is [True, False, False, False, False, True]
State prediction error at timestep 1383 is tensor(7.4115e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1384. State = [[-0.26605147  0.22310007]]. Action = [[-0.02367972 -0.05213607 -0.0814019  -0.4301747 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 1384 is [True, False, False, False, False, True]
State prediction error at timestep 1384 is tensor(1.1210e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1384 of 1
Current timestep = 1385. State = [[-0.26546118  0.2223912 ]]. Action = [[ 0.03013504  0.0109666  -0.09430375  0.33391428]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 1385 is [True, False, False, False, False, True]
State prediction error at timestep 1385 is tensor(1.6041e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1386. State = [[-0.26493952  0.22211416]]. Action = [[-0.02965845  0.06904576 -0.00459176  0.47631466]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 1386 is [True, False, False, False, False, True]
State prediction error at timestep 1386 is tensor(1.8585e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1386 of 1
Current timestep = 1387. State = [[-0.26513097  0.22234191]]. Action = [[-0.05029366  0.0767306  -0.01821689  0.04798424]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 1387 is [True, False, False, False, False, True]
State prediction error at timestep 1387 is tensor(3.6990e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1387 of 1
Current timestep = 1388. State = [[-0.26622626  0.2240207 ]]. Action = [[ 0.09270252  0.02367022 -0.07933108  0.67812514]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 1388 is [True, False, False, False, False, True]
State prediction error at timestep 1388 is tensor(2.7612e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1389. State = [[-0.26630127  0.22453365]]. Action = [[0.09398431 0.08966436 0.06357069 0.7346226 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 1389 is [True, False, False, False, False, True]
State prediction error at timestep 1389 is tensor(4.9309e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1389 of 1
Current timestep = 1390. State = [[-0.2649401   0.22696926]]. Action = [[-0.0923752   0.04373661 -0.00786501  0.5353198 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 1390 is [True, False, False, False, False, True]
State prediction error at timestep 1390 is tensor(4.0359e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1390 of 1
Current timestep = 1391. State = [[-0.26569483  0.22931212]]. Action = [[-0.03647708 -0.0375247   0.07069486 -0.2745732 ]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 1391 is [True, False, False, False, False, True]
State prediction error at timestep 1391 is tensor(8.8151e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1392. State = [[-0.26632887  0.23021133]]. Action = [[-0.04153109  0.03411064  0.01156243  0.6027942 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 1392 is [True, False, False, False, False, True]
State prediction error at timestep 1392 is tensor(3.6028e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1392 of 1
Current timestep = 1393. State = [[-0.26747742  0.23181167]]. Action = [[ 0.08980582  0.0602079  -0.05214241  0.5995054 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 1393 is [True, False, False, False, False, True]
State prediction error at timestep 1393 is tensor(1.9086e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1394. State = [[-0.26731607  0.23314813]]. Action = [[ 0.09575542 -0.06098439 -0.02322032  0.52551675]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 1394 is [True, False, False, False, False, True]
State prediction error at timestep 1394 is tensor(2.6433e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1394 of 1
Current timestep = 1395. State = [[-0.2663386   0.23343398]]. Action = [[-0.04719707  0.07370523 -0.02235489  0.65937364]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 1395 is [True, False, False, False, False, True]
State prediction error at timestep 1395 is tensor(4.7889e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1395 of 1
Current timestep = 1396. State = [[-0.26665747  0.2345967 ]]. Action = [[ 0.07741571 -0.02907626  0.0538085  -0.27921498]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 1396 is [True, False, False, False, False, True]
State prediction error at timestep 1396 is tensor(4.0175e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1397. State = [[-0.26507583  0.2355255 ]]. Action = [[-0.05010646  0.03537244 -0.00056399 -0.24975646]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 1397 is [True, False, False, False, False, True]
State prediction error at timestep 1397 is tensor(7.9759e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1398. State = [[-0.26460853  0.23637255]]. Action = [[-0.06979064  0.03269737  0.04996247  0.10098875]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 1398 is [True, False, False, False, False, True]
State prediction error at timestep 1398 is tensor(1.3267e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1398 of 1
Current timestep = 1399. State = [[-0.26566547  0.23787461]]. Action = [[-0.08189552  0.01881175  0.04461623  0.80175555]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 1399 is [True, False, False, False, False, True]
State prediction error at timestep 1399 is tensor(1.0403e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1400. State = [[-0.26741037  0.23987424]]. Action = [[-0.05583481 -0.02459509  0.01041909  0.86572003]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 1400 is [True, False, False, False, False, True]
State prediction error at timestep 1400 is tensor(7.0731e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1400 of 1
Current timestep = 1401. State = [[-0.26881117  0.24153484]]. Action = [[ 0.06778779  0.09400202 -0.02116982 -0.93717784]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 1401 is [True, False, False, False, False, True]
State prediction error at timestep 1401 is tensor(3.6405e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1401 of 1
Current timestep = 1402. State = [[-0.27007     0.24330206]]. Action = [[-0.02451061 -0.07775948  0.02046378  0.56642187]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 1402 is [True, False, False, False, False, True]
State prediction error at timestep 1402 is tensor(2.4337e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1403. State = [[-0.27042106  0.24369681]]. Action = [[ 0.01897351  0.04713329 -0.02155553 -0.10590392]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 1403 is [True, False, False, False, False, True]
State prediction error at timestep 1403 is tensor(2.5164e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1404. State = [[-0.27088755  0.244266  ]]. Action = [[-0.08098178 -0.05956649 -0.09166712  0.11980104]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 1404 is [True, False, False, False, False, True]
State prediction error at timestep 1404 is tensor(3.5727e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1404 of 1
Current timestep = 1405. State = [[-0.27122733  0.24460225]]. Action = [[-0.01675084  0.05820701  0.01662651 -0.77006245]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 1405 is [True, False, False, False, False, True]
State prediction error at timestep 1405 is tensor(5.3568e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1405 of 1
Current timestep = 1406. State = [[-0.2722246   0.24568984]]. Action = [[-0.01539693 -0.09055597 -0.03964747 -0.03365958]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 1406 is [True, False, False, False, False, True]
State prediction error at timestep 1406 is tensor(9.8105e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1407. State = [[-0.27221483  0.24567705]]. Action = [[-0.09729575 -0.00623315 -0.04528078 -0.7453606 ]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 1407 is [True, False, False, False, False, True]
State prediction error at timestep 1407 is tensor(5.5782e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1407 of 1
Current timestep = 1408. State = [[-0.27330413  0.24610643]]. Action = [[-0.08775255  0.08921621 -0.02571082 -0.01375264]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 1408 is [True, False, False, False, False, True]
State prediction error at timestep 1408 is tensor(5.0251e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1409. State = [[-0.2765397   0.24878165]]. Action = [[-0.0441258  -0.04472528 -0.08090205  0.2668153 ]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 1409 is [True, False, False, False, False, True]
State prediction error at timestep 1409 is tensor(8.3044e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1409 of 1
Current timestep = 1410. State = [[-0.27936965  0.2499651 ]]. Action = [[-0.06787745  0.06526668 -0.05219207 -0.3992113 ]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 1410 is [True, False, False, False, False, True]
State prediction error at timestep 1410 is tensor(5.8138e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1410 of 1
Current timestep = 1411. State = [[-0.2825787   0.25275165]]. Action = [[ 0.00656848 -0.06827754 -0.06265755  0.40034902]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 1411 is [True, False, False, False, False, True]
State prediction error at timestep 1411 is tensor(1.5135e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1412. State = [[-0.28398082  0.2524607 ]]. Action = [[-0.00559265 -0.02437592 -0.01438127  0.5548022 ]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 1412 is [True, False, False, False, False, True]
State prediction error at timestep 1412 is tensor(2.6655e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1412 of 1
Current timestep = 1413. State = [[-0.28463668  0.2520439 ]]. Action = [[-0.01355587  0.095649   -0.08148031 -0.7436929 ]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 1413 is [True, False, False, False, False, True]
State prediction error at timestep 1413 is tensor(4.1477e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1413 of 1
Current timestep = 1414. State = [[-0.28640026  0.25362137]]. Action = [[-0.03779706 -0.04762365  0.06353856 -0.6482284 ]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 1414 is [True, False, False, False, False, True]
State prediction error at timestep 1414 is tensor(3.6329e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1415. State = [[-0.28732628  0.25395438]]. Action = [[ 1.1976808e-04 -8.1703216e-02  3.0499548e-03  6.5483546e-01]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 1415 is [True, False, False, False, False, True]
State prediction error at timestep 1415 is tensor(1.6511e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1415 of -1
Current timestep = 1416. State = [[-0.28773528  0.25247288]]. Action = [[ 0.04775377  0.05104063  0.05932716 -0.7298737 ]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 1416 is [True, False, False, False, False, True]
State prediction error at timestep 1416 is tensor(3.3954e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1416 of -1
Current timestep = 1417. State = [[-0.2878054   0.25240955]]. Action = [[-0.0175595   0.06906054 -0.02130499  0.52547586]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 1417 is [True, False, False, False, False, True]
State prediction error at timestep 1417 is tensor(4.8882e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1418. State = [[-0.2885748   0.25317532]]. Action = [[ 0.07750211 -0.06796312  0.01138703 -0.7908739 ]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 1418 is [True, False, False, False, False, True]
State prediction error at timestep 1418 is tensor(2.1279e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1418 of -1
Current timestep = 1419. State = [[-0.28830498  0.25261346]]. Action = [[-0.06184033  0.05024629 -0.05734775 -0.86205995]]. Reward = [0.]
Curr episode timestep = 517
Scene graph at timestep 1419 is [True, False, False, False, False, True]
State prediction error at timestep 1419 is tensor(8.6832e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1419 of -1
Current timestep = 1420. State = [[-0.28847086  0.25278622]]. Action = [[-0.07952987 -0.06470362 -0.00959551 -0.55870515]]. Reward = [0.]
Curr episode timestep = 518
Scene graph at timestep 1420 is [True, False, False, False, False, True]
State prediction error at timestep 1420 is tensor(1.0407e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1421. State = [[-0.28900456  0.25276506]]. Action = [[ 6.7822225e-02  8.3299138e-02  1.3251603e-04 -4.8556006e-01]]. Reward = [0.]
Curr episode timestep = 519
Scene graph at timestep 1421 is [True, False, False, False, False, True]
State prediction error at timestep 1421 is tensor(7.6208e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1422. State = [[-0.28940833  0.2533059 ]]. Action = [[-0.01694055 -0.07010139 -0.04207921 -0.31717485]]. Reward = [0.]
Curr episode timestep = 520
Scene graph at timestep 1422 is [True, False, False, False, False, True]
State prediction error at timestep 1422 is tensor(9.3476e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1422 of -1
Current timestep = 1423. State = [[-0.28940064  0.25315547]]. Action = [[ 0.01988047 -0.08561891 -0.01322637  0.00458658]]. Reward = [0.]
Curr episode timestep = 521
Scene graph at timestep 1423 is [True, False, False, False, False, True]
State prediction error at timestep 1423 is tensor(2.2177e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1423 of -1
Current timestep = 1424. State = [[-0.28853163  0.25125402]]. Action = [[ 8.8467337e-02 -7.5708330e-04 -8.9571230e-02 -7.6153094e-01]]. Reward = [0.]
Curr episode timestep = 522
Scene graph at timestep 1424 is [True, False, False, False, False, True]
State prediction error at timestep 1424 is tensor(4.1187e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1425. State = [[-0.28736955  0.24931185]]. Action = [[-7.7867612e-02 -1.2754649e-04 -3.4230500e-02  5.3353655e-01]]. Reward = [0.]
Curr episode timestep = 523
Scene graph at timestep 1425 is [True, False, False, False, False, True]
State prediction error at timestep 1425 is tensor(5.1389e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1425 of -1
Current timestep = 1426. State = [[-0.28728262  0.24826868]]. Action = [[ 0.06407417 -0.03018528 -0.04169446  0.77769744]]. Reward = [0.]
Curr episode timestep = 524
Scene graph at timestep 1426 is [True, False, False, False, False, True]
State prediction error at timestep 1426 is tensor(1.2522e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1426 of -1
Current timestep = 1427. State = [[-0.28630742  0.24628547]]. Action = [[-0.04515181  0.08228631  0.07927915 -0.8373015 ]]. Reward = [0.]
Curr episode timestep = 525
Scene graph at timestep 1427 is [True, False, False, False, False, True]
State prediction error at timestep 1427 is tensor(4.8478e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1427 of -1
Current timestep = 1428. State = [[-0.28690317  0.24675812]]. Action = [[ 0.04439137  0.01925328  0.07557216 -0.516931  ]]. Reward = [0.]
Curr episode timestep = 526
Scene graph at timestep 1428 is [True, False, False, False, False, True]
State prediction error at timestep 1428 is tensor(1.6680e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1429. State = [[-0.2870108   0.24691154]]. Action = [[ 0.05883973 -0.03131796  0.02845744 -0.09207052]]. Reward = [0.]
Curr episode timestep = 527
Scene graph at timestep 1429 is [True, False, False, False, False, True]
State prediction error at timestep 1429 is tensor(9.5623e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1429 of -1
Current timestep = 1430. State = [[-0.28654867  0.24620059]]. Action = [[ 0.0970156  -0.03574749 -0.00988288  0.5503299 ]]. Reward = [0.]
Curr episode timestep = 528
Scene graph at timestep 1430 is [True, False, False, False, False, True]
State prediction error at timestep 1430 is tensor(3.7077e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1430 of -1
Current timestep = 1431. State = [[-0.28480282  0.24436334]]. Action = [[0.01979724 0.0509707  0.01343234 0.00994945]]. Reward = [0.]
Curr episode timestep = 529
Scene graph at timestep 1431 is [True, False, False, False, False, True]
State prediction error at timestep 1431 is tensor(1.8098e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1432. State = [[-0.2844356  0.244545 ]]. Action = [[-0.07054873  0.09799009  0.08829925  0.6949241 ]]. Reward = [0.]
Curr episode timestep = 530
Scene graph at timestep 1432 is [True, False, False, False, False, True]
State prediction error at timestep 1432 is tensor(3.2862e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1433. State = [[-0.28526413  0.24585786]]. Action = [[ 0.09344841  0.08219454 -0.01268585 -0.5724941 ]]. Reward = [0.]
Curr episode timestep = 531
Scene graph at timestep 1433 is [True, False, False, False, False, True]
State prediction error at timestep 1433 is tensor(6.8972e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1433 of -1
Current timestep = 1434. State = [[-0.28382915  0.24779183]]. Action = [[ 0.0881123  -0.06550087 -0.06373204 -0.22564691]]. Reward = [0.]
Curr episode timestep = 532
Scene graph at timestep 1434 is [True, False, False, False, False, True]
State prediction error at timestep 1434 is tensor(1.0674e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1434 of -1
Current timestep = 1435. State = [[-0.28201687  0.24865144]]. Action = [[-0.05121273  0.09474256 -0.02286486  0.9737499 ]]. Reward = [0.]
Curr episode timestep = 533
Scene graph at timestep 1435 is [True, False, False, False, False, True]
State prediction error at timestep 1435 is tensor(1.7623e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1436. State = [[-0.28197777  0.25008214]]. Action = [[-0.08300166  0.03104686  0.03566021 -0.475599  ]]. Reward = [0.]
Curr episode timestep = 534
Scene graph at timestep 1436 is [True, False, False, False, False, True]
State prediction error at timestep 1436 is tensor(1.1180e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1436 of -1
Current timestep = 1437. State = [[-0.28349304  0.2519185 ]]. Action = [[-0.06544393 -0.04851201 -0.01746698  0.8068303 ]]. Reward = [0.]
Curr episode timestep = 535
Scene graph at timestep 1437 is [True, False, False, False, False, True]
State prediction error at timestep 1437 is tensor(2.2351e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1438. State = [[-0.28446707  0.25289178]]. Action = [[-0.08533244 -0.00537125 -0.03543991  0.23701942]]. Reward = [0.]
Curr episode timestep = 536
Scene graph at timestep 1438 is [True, False, False, False, False, True]
State prediction error at timestep 1438 is tensor(1.4386e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1438 of -1
Current timestep = 1439. State = [[-0.28608793  0.25452986]]. Action = [[ 0.0317013  -0.09295403 -0.09401842  0.14917755]]. Reward = [0.]
Curr episode timestep = 537
Scene graph at timestep 1439 is [True, False, False, False, False, True]
State prediction error at timestep 1439 is tensor(1.5875e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1439 of -1
Current timestep = 1440. State = [[-0.28595212  0.25404766]]. Action = [[-0.0557323   0.06854557  0.0716167   0.02307141]]. Reward = [0.]
Curr episode timestep = 538
Scene graph at timestep 1440 is [True, False, False, False, False, True]
State prediction error at timestep 1440 is tensor(2.9188e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1440 of -1
Current timestep = 1441. State = [[-0.2870192   0.25502792]]. Action = [[0.04220889 0.07364947 0.01950841 0.365335  ]]. Reward = [0.]
Curr episode timestep = 539
Scene graph at timestep 1441 is [True, False, False, False, False, True]
State prediction error at timestep 1441 is tensor(8.7416e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1442. State = [[-0.28822675  0.2564002 ]]. Action = [[-0.05900493 -0.03334734  0.02359872  0.60348105]]. Reward = [0.]
Curr episode timestep = 540
Scene graph at timestep 1442 is [True, False, False, False, False, True]
State prediction error at timestep 1442 is tensor(4.4122e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1442 of -1
Current timestep = 1443. State = [[-0.2891698  0.2574161]]. Action = [[0.05519951 0.03616204 0.07957231 0.01436567]]. Reward = [0.]
Curr episode timestep = 541
Scene graph at timestep 1443 is [True, False, False, False, False, True]
State prediction error at timestep 1443 is tensor(3.9696e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1443 of -1
Current timestep = 1444. State = [[-0.2895982   0.25785914]]. Action = [[ 0.04980529 -0.06431356  0.03192831 -0.16905355]]. Reward = [0.]
Curr episode timestep = 542
Scene graph at timestep 1444 is [True, False, False, False, False, True]
State prediction error at timestep 1444 is tensor(1.6818e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1444 of -1
Current timestep = 1445. State = [[-0.28933838  0.2575241 ]]. Action = [[0.05966935 0.00361957 0.00466061 0.8382187 ]]. Reward = [0.]
Curr episode timestep = 543
Scene graph at timestep 1445 is [True, False, False, False, False, True]
State prediction error at timestep 1445 is tensor(1.3603e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1446. State = [[-0.28883913  0.25708812]]. Action = [[-0.03095573  0.03880598 -0.03680922 -0.7498914 ]]. Reward = [0.]
Curr episode timestep = 544
Scene graph at timestep 1446 is [True, False, False, False, False, True]
State prediction error at timestep 1446 is tensor(3.8363e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1447. State = [[-0.28883722  0.25709355]]. Action = [[-0.06415552  0.09189761 -0.0514399   0.7016431 ]]. Reward = [0.]
Curr episode timestep = 545
Scene graph at timestep 1447 is [True, False, False, False, False, True]
State prediction error at timestep 1447 is tensor(3.2156e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1447 of -1
Current timestep = 1448. State = [[-0.29028252  0.25879198]]. Action = [[ 0.08915787  0.06757344 -0.06055824 -0.8541342 ]]. Reward = [0.]
Curr episode timestep = 546
Scene graph at timestep 1448 is [True, False, False, False, False, True]
State prediction error at timestep 1448 is tensor(1.9809e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1448 of -1
Current timestep = 1449. State = [[-0.2905468  0.2606016]]. Action = [[ 0.0929729  -0.04876813  0.04684552 -0.8111535 ]]. Reward = [0.]
Curr episode timestep = 547
Scene graph at timestep 1449 is [True, False, False, False, False, True]
State prediction error at timestep 1449 is tensor(2.0700e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1450. State = [[-0.2888871  0.2616184]]. Action = [[-0.03354377  0.05066479 -0.08402918 -0.42750055]]. Reward = [0.]
Curr episode timestep = 548
Scene graph at timestep 1450 is [True, False, False, False, False, True]
State prediction error at timestep 1450 is tensor(1.3421e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1451. State = [[-0.2877547  0.2628707]]. Action = [[ 0.05750809 -0.06688754 -0.0876673  -0.7004201 ]]. Reward = [0.]
Curr episode timestep = 549
Scene graph at timestep 1451 is [True, False, False, False, False, True]
State prediction error at timestep 1451 is tensor(2.2554e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1451 of -1
Current timestep = 1452. State = [[-0.2864587   0.26304287]]. Action = [[-0.03841509 -0.06309763  0.04683013 -0.68619245]]. Reward = [0.]
Curr episode timestep = 550
Scene graph at timestep 1452 is [True, False, False, False, False, True]
State prediction error at timestep 1452 is tensor(3.4191e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1453. State = [[-0.28550002  0.2622205 ]]. Action = [[-0.05510792 -0.00428699 -0.05691464  0.23937154]]. Reward = [0.]
Curr episode timestep = 551
Scene graph at timestep 1453 is [True, False, False, False, False, True]
State prediction error at timestep 1453 is tensor(2.2725e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1453 of -1
Current timestep = 1454. State = [[-0.2853458   0.26202685]]. Action = [[ 0.08352397  0.0264622  -0.00117647  0.00694799]]. Reward = [0.]
Curr episode timestep = 552
Scene graph at timestep 1454 is [True, False, False, False, False, True]
State prediction error at timestep 1454 is tensor(8.0082e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1454 of -1
Current timestep = 1455. State = [[-0.2848054  0.262176 ]]. Action = [[-0.04028226 -0.04970768 -0.07591845  0.35326636]]. Reward = [0.]
Curr episode timestep = 553
Scene graph at timestep 1455 is [True, False, False, False, False, True]
State prediction error at timestep 1455 is tensor(5.2083e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1456. State = [[-0.28437242  0.26155198]]. Action = [[-0.05374316 -0.03030087 -0.05995706  0.86329365]]. Reward = [0.]
Curr episode timestep = 554
Scene graph at timestep 1456 is [True, False, False, False, False, True]
State prediction error at timestep 1456 is tensor(1.7676e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1457. State = [[-0.28421053  0.26126835]]. Action = [[-0.0767435  -0.093688   -0.01912179  0.56570005]]. Reward = [0.]
Curr episode timestep = 555
Scene graph at timestep 1457 is [True, False, False, False, False, True]
State prediction error at timestep 1457 is tensor(4.4295e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1457 of -1
Current timestep = 1458. State = [[-0.28353646  0.25951278]]. Action = [[ 0.06276137 -0.06167613 -0.0208064   0.78786874]]. Reward = [0.]
Curr episode timestep = 556
Scene graph at timestep 1458 is [True, False, False, False, False, True]
State prediction error at timestep 1458 is tensor(1.3574e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1459. State = [[-0.28207973  0.25705448]]. Action = [[ 0.00631858 -0.03350313 -0.09145292  0.4807142 ]]. Reward = [0.]
Curr episode timestep = 557
Scene graph at timestep 1459 is [True, False, False, False, False, True]
State prediction error at timestep 1459 is tensor(6.8451e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1459 of -1
Current timestep = 1460. State = [[-0.28110301  0.2546174 ]]. Action = [[ 0.00951741 -0.06909709 -0.05043399 -0.6239512 ]]. Reward = [0.]
Curr episode timestep = 558
Scene graph at timestep 1460 is [True, False, False, False, False, True]
State prediction error at timestep 1460 is tensor(3.8762e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1460 of 1
Current timestep = 1461. State = [[-0.2796807  0.2511725]]. Action = [[ 0.07035973 -0.05549705  0.02369981  0.87600803]]. Reward = [0.]
Curr episode timestep = 559
Scene graph at timestep 1461 is [True, False, False, False, False, True]
State prediction error at timestep 1461 is tensor(3.3271e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1462. State = [[-0.27744815  0.24670687]]. Action = [[ 0.09107802 -0.04669855 -0.01931909  0.5997982 ]]. Reward = [0.]
Curr episode timestep = 560
Scene graph at timestep 1462 is [True, False, False, False, False, True]
State prediction error at timestep 1462 is tensor(3.7256e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1463. State = [[-0.27482277  0.24194899]]. Action = [[ 0.0224184  -0.05177311  0.09279753  0.04077196]]. Reward = [0.]
Curr episode timestep = 561
Scene graph at timestep 1463 is [True, False, False, False, False, True]
State prediction error at timestep 1463 is tensor(8.4850e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1464. State = [[-0.2726445   0.23808548]]. Action = [[-0.08560237  0.03009459  0.09588964  0.31404257]]. Reward = [0.]
Curr episode timestep = 562
Scene graph at timestep 1464 is [True, False, False, False, False, True]
State prediction error at timestep 1464 is tensor(5.3108e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1464 of 1
Current timestep = 1465. State = [[-0.2725872   0.23744065]]. Action = [[0.04533278 0.03442534 0.09720498 0.35248375]]. Reward = [0.]
Curr episode timestep = 563
Scene graph at timestep 1465 is [True, False, False, False, False, True]
State prediction error at timestep 1465 is tensor(1.1518e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1465 of 1
Current timestep = 1466. State = [[-0.2722895   0.23738353]]. Action = [[ 0.09241117 -0.07517198  0.06515942  0.6186738 ]]. Reward = [0.]
Curr episode timestep = 564
Scene graph at timestep 1466 is [True, False, False, False, False, True]
State prediction error at timestep 1466 is tensor(2.5381e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1466 of 1
Current timestep = 1467. State = [[-0.27043065  0.2348452 ]]. Action = [[-0.07341759  0.06581111 -0.05661006 -0.8269689 ]]. Reward = [0.]
Curr episode timestep = 565
Scene graph at timestep 1467 is [True, False, False, False, False, True]
State prediction error at timestep 1467 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1468. State = [[-0.2704958   0.23489685]]. Action = [[ 0.04000656 -0.0584135  -0.08928011  0.23321974]]. Reward = [0.]
Curr episode timestep = 566
Scene graph at timestep 1468 is [True, False, False, False, False, True]
State prediction error at timestep 1468 is tensor(1.4147e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1468 of 1
Current timestep = 1469. State = [[-0.26974723  0.23370425]]. Action = [[0.08399571 0.071542   0.06505276 0.39160502]]. Reward = [0.]
Curr episode timestep = 567
Scene graph at timestep 1469 is [True, False, False, False, False, True]
State prediction error at timestep 1469 is tensor(5.3159e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1469 of 1
Current timestep = 1470. State = [[-0.26903063  0.23391499]]. Action = [[ 0.02663312  0.01966208 -0.09330569 -0.55087477]]. Reward = [0.]
Curr episode timestep = 568
Scene graph at timestep 1470 is [True, False, False, False, False, True]
State prediction error at timestep 1470 is tensor(1.9859e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1471. State = [[-0.26816916  0.23441143]]. Action = [[ 0.0411169  -0.00347626  0.00318561  0.44161177]]. Reward = [0.]
Curr episode timestep = 569
Scene graph at timestep 1471 is [True, False, False, False, False, True]
State prediction error at timestep 1471 is tensor(2.1484e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1472. State = [[-0.26710832  0.23437007]]. Action = [[-0.08401069 -0.05391098 -0.01047746 -0.58540034]]. Reward = [0.]
Curr episode timestep = 570
Scene graph at timestep 1472 is [True, False, False, False, False, True]
State prediction error at timestep 1472 is tensor(2.0191e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1472 of 1
Current timestep = 1473. State = [[-0.26627868  0.2332476 ]]. Action = [[ 0.03280208  0.05695597 -0.02119376 -0.22629845]]. Reward = [0.]
Curr episode timestep = 571
Scene graph at timestep 1473 is [True, False, False, False, False, True]
State prediction error at timestep 1473 is tensor(1.3012e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1473 of 1
Current timestep = 1474. State = [[-0.26600522  0.2337175 ]]. Action = [[ 0.00091822  0.0699786   0.07090556 -0.39947045]]. Reward = [0.]
Curr episode timestep = 572
Scene graph at timestep 1474 is [True, False, False, False, False, True]
State prediction error at timestep 1474 is tensor(8.3451e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1475. State = [[-0.26629797  0.2343376 ]]. Action = [[ 0.03530399 -0.05056616  0.05482931  0.6992924 ]]. Reward = [0.]
Curr episode timestep = 573
Scene graph at timestep 1475 is [True, False, False, False, False, True]
State prediction error at timestep 1475 is tensor(1.8956e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1476. State = [[-0.26584178  0.23437761]]. Action = [[-0.04668913 -0.06820483 -0.03143023  0.4590795 ]]. Reward = [0.]
Curr episode timestep = 574
Scene graph at timestep 1476 is [True, False, False, False, False, True]
State prediction error at timestep 1476 is tensor(1.8582e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1476 of 1
Current timestep = 1477. State = [[-0.26544917  0.23346874]]. Action = [[-0.0181758   0.06390927 -0.0152884   0.17995608]]. Reward = [0.]
Curr episode timestep = 575
Scene graph at timestep 1477 is [True, False, False, False, False, True]
State prediction error at timestep 1477 is tensor(9.2822e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1477 of 1
Current timestep = 1478. State = [[-0.26538593  0.23348357]]. Action = [[ 0.09178353 -0.08356875 -0.01811965  0.2526139 ]]. Reward = [0.]
Curr episode timestep = 576
Scene graph at timestep 1478 is [True, False, False, False, False, True]
State prediction error at timestep 1478 is tensor(1.2817e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1479. State = [[-0.2642      0.23194526]]. Action = [[-0.07017887  0.05088604  0.00071875  0.47407508]]. Reward = [0.]
Curr episode timestep = 577
Scene graph at timestep 1479 is [True, False, False, False, False, True]
State prediction error at timestep 1479 is tensor(2.7253e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1479 of 1
Current timestep = 1480. State = [[-0.26419148  0.23189344]]. Action = [[ 0.00859828 -0.08898884  0.01557942  0.2624824 ]]. Reward = [0.]
Curr episode timestep = 578
Scene graph at timestep 1480 is [True, False, False, False, False, True]
State prediction error at timestep 1480 is tensor(1.9941e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1480 of 1
Current timestep = 1481. State = [[-0.26327506  0.23019834]]. Action = [[ 0.00876281 -0.09098075  0.01966114  0.26804113]]. Reward = [0.]
Curr episode timestep = 579
Scene graph at timestep 1481 is [True, False, False, False, False, True]
State prediction error at timestep 1481 is tensor(2.2861e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1482. State = [[-0.2614143   0.22723272]]. Action = [[ 0.03456176 -0.0648092  -0.07524662  0.13894117]]. Reward = [0.]
Curr episode timestep = 580
Scene graph at timestep 1482 is [True, False, False, False, False, True]
State prediction error at timestep 1482 is tensor(2.1169e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1482 of 1
Current timestep = 1483. State = [[-0.25932598  0.2239842 ]]. Action = [[-0.02954726 -0.00475325 -0.00605534  0.3516363 ]]. Reward = [0.]
Curr episode timestep = 581
Scene graph at timestep 1483 is [True, False, False, False, False, True]
State prediction error at timestep 1483 is tensor(1.4331e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1483 of 1
Current timestep = 1484. State = [[-0.25860193  0.2223649 ]]. Action = [[-0.00909566 -0.05074989 -0.05735587 -0.51143223]]. Reward = [0.]
Curr episode timestep = 582
Scene graph at timestep 1484 is [True, False, False, False, False, True]
State prediction error at timestep 1484 is tensor(2.4178e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1485. State = [[-0.2578971   0.22058138]]. Action = [[-0.03961816  0.06808259 -0.04241637 -0.5918816 ]]. Reward = [0.]
Curr episode timestep = 583
Scene graph at timestep 1485 is [True, False, False, False, False, True]
State prediction error at timestep 1485 is tensor(2.2891e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1485 of 1
Current timestep = 1486. State = [[-0.25796318  0.22059976]]. Action = [[ 0.06647306 -0.06117321 -0.00198605 -0.12203181]]. Reward = [0.]
Curr episode timestep = 584
Scene graph at timestep 1486 is [True, False, False, False, False, True]
State prediction error at timestep 1486 is tensor(3.3703e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1486 of 1
Current timestep = 1487. State = [[-0.2572643   0.21904989]]. Action = [[-0.06441456 -0.02509095 -0.01429049  0.41978884]]. Reward = [0.]
Curr episode timestep = 585
Scene graph at timestep 1487 is [True, False, False, False, False, True]
State prediction error at timestep 1487 is tensor(6.1410e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1488. State = [[-0.25731823  0.21754043]]. Action = [[-0.08989596 -0.07218875 -0.03968629  0.79264927]]. Reward = [0.]
Curr episode timestep = 586
Scene graph at timestep 1488 is [True, False, False, False, False, True]
State prediction error at timestep 1488 is tensor(6.1421e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1488 of 1
Current timestep = 1489. State = [[-0.25870672  0.21438833]]. Action = [[ 0.02332737 -0.06881986  0.04049663  0.8257835 ]]. Reward = [0.]
Curr episode timestep = 587
Scene graph at timestep 1489 is [True, False, False, False, False, True]
State prediction error at timestep 1489 is tensor(2.3400e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1490. State = [[-0.25936142  0.2103577 ]]. Action = [[-0.05287088  0.08276189  0.02879637 -0.3540944 ]]. Reward = [0.]
Curr episode timestep = 588
Scene graph at timestep 1490 is [True, False, False, False, False, True]
State prediction error at timestep 1490 is tensor(1.5980e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1490 of 1
Current timestep = 1491. State = [[-0.2605609  0.2099845]]. Action = [[ 0.07512405  0.06252725  0.02652974 -0.5764484 ]]. Reward = [0.]
Curr episode timestep = 589
Scene graph at timestep 1491 is [True, False, False, False, False, True]
State prediction error at timestep 1491 is tensor(4.8448e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1492. State = [[-0.26059568  0.21018516]]. Action = [[-0.0701151  -0.06133064 -0.07474923 -0.82562476]]. Reward = [0.]
Curr episode timestep = 590
Scene graph at timestep 1492 is [True, False, False, False, False, True]
State prediction error at timestep 1492 is tensor(6.7233e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1493. State = [[-0.26092255  0.2102793 ]]. Action = [[ 0.05973036 -0.05402104  0.0511416  -0.6102634 ]]. Reward = [0.]
Curr episode timestep = 591
Scene graph at timestep 1493 is [True, False, False, False, False, True]
State prediction error at timestep 1493 is tensor(1.2687e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1493 of -1
Current timestep = 1494. State = [[-0.26039383  0.20901474]]. Action = [[ 0.06739753 -0.0210549   0.0844234   0.66387916]]. Reward = [0.]
Curr episode timestep = 592
Scene graph at timestep 1494 is [True, False, False, False, False, True]
State prediction error at timestep 1494 is tensor(4.5673e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1495. State = [[-0.25943777  0.20751455]]. Action = [[-0.09800732  0.02205069 -0.01993346  0.60673404]]. Reward = [0.]
Curr episode timestep = 593
Scene graph at timestep 1495 is [True, False, False, False, False, True]
State prediction error at timestep 1495 is tensor(3.5018e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1496. State = [[-0.2597436   0.20733248]]. Action = [[0.09819467 0.08112567 0.05399764 0.676013  ]]. Reward = [0.]
Curr episode timestep = 594
Scene graph at timestep 1496 is [True, False, False, False, False, True]
State prediction error at timestep 1496 is tensor(3.3645e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1496 of -1
Current timestep = 1497. State = [[-0.25981358  0.20766933]]. Action = [[ 0.06447674  0.06029394 -0.02235483 -0.21100545]]. Reward = [0.]
Curr episode timestep = 595
Scene graph at timestep 1497 is [True, False, False, False, False, True]
State prediction error at timestep 1497 is tensor(5.4940e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1498. State = [[-0.25976726  0.2082686 ]]. Action = [[-0.04115322 -0.01762554 -0.06685023 -0.61029655]]. Reward = [0.]
Curr episode timestep = 596
Scene graph at timestep 1498 is [True, False, False, False, False, True]
State prediction error at timestep 1498 is tensor(1.1641e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1498 of -1
Current timestep = 1499. State = [[-0.25994933  0.20858893]]. Action = [[-0.02599952 -0.03784258  0.07148347  0.47973764]]. Reward = [0.]
Curr episode timestep = 597
Scene graph at timestep 1499 is [True, False, False, False, False, True]
State prediction error at timestep 1499 is tensor(5.2480e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1500. State = [[-0.26000136  0.20855744]]. Action = [[ 0.05147875 -0.02454999 -0.03239032  0.8743322 ]]. Reward = [0.]
Curr episode timestep = 598
Scene graph at timestep 1500 is [True, False, False, False, False, True]
State prediction error at timestep 1500 is tensor(1.7176e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1501. State = [[-0.25957027  0.20807907]]. Action = [[-0.01844827  0.06633794 -0.04131706  0.01058197]]. Reward = [0.]
Curr episode timestep = 599
Scene graph at timestep 1501 is [True, False, False, False, False, True]
State prediction error at timestep 1501 is tensor(3.9916e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1502. State = [[-0.25975     0.20837642]]. Action = [[-0.01371689  0.02876873  0.09536792  0.03743875]]. Reward = [0.]
Curr episode timestep = 600
Scene graph at timestep 1502 is [True, False, False, False, False, True]
State prediction error at timestep 1502 is tensor(3.1726e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1502 of -1
Current timestep = 1503. State = [[-0.2602153   0.20917276]]. Action = [[-0.02871507  0.01908634  0.04786702  0.79116845]]. Reward = [0.]
Curr episode timestep = 601
Scene graph at timestep 1503 is [True, False, False, False, False, True]
State prediction error at timestep 1503 is tensor(2.4969e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1504. State = [[-0.26087743  0.21011949]]. Action = [[ 0.02627615  0.01943848 -0.08300421  0.36429906]]. Reward = [0.]
Curr episode timestep = 602
Scene graph at timestep 1504 is [True, False, False, False, False, True]
State prediction error at timestep 1504 is tensor(7.8849e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1504 of -1
Current timestep = 1505. State = [[-0.26122898  0.21093403]]. Action = [[-0.01734673 -0.02084965  0.04904195  0.72397494]]. Reward = [0.]
Curr episode timestep = 603
Scene graph at timestep 1505 is [True, False, False, False, False, True]
State prediction error at timestep 1505 is tensor(3.0049e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1506. State = [[-0.26121405  0.21101148]]. Action = [[ 0.09757597  0.03241474 -0.08578619  0.52148557]]. Reward = [0.]
Curr episode timestep = 604
Scene graph at timestep 1506 is [True, False, False, False, False, True]
State prediction error at timestep 1506 is tensor(2.5983e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1506 of -1
Current timestep = 1507. State = [[-0.2607908   0.21142413]]. Action = [[-0.05340371 -0.08640669 -0.01301852 -0.8117702 ]]. Reward = [0.]
Curr episode timestep = 605
Scene graph at timestep 1507 is [True, False, False, False, False, True]
State prediction error at timestep 1507 is tensor(4.5144e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1508. State = [[-0.26038912  0.2109679 ]]. Action = [[ 0.08976524 -0.06306372  0.06822074  0.82493377]]. Reward = [0.]
Curr episode timestep = 606
Scene graph at timestep 1508 is [True, False, False, False, False, True]
State prediction error at timestep 1508 is tensor(3.3013e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1508 of -1
Current timestep = 1509. State = [[-0.2586354   0.20903197]]. Action = [[-0.08769193 -0.01115626  0.09824158 -0.79998446]]. Reward = [0.]
Curr episode timestep = 607
Scene graph at timestep 1509 is [True, False, False, False, False, True]
State prediction error at timestep 1509 is tensor(5.4255e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1509 of -1
Current timestep = 1510. State = [[-0.25837645  0.20852602]]. Action = [[-0.00623583 -0.08671547  0.04270739 -0.36282212]]. Reward = [0.]
Curr episode timestep = 608
Scene graph at timestep 1510 is [True, False, False, False, False, True]
State prediction error at timestep 1510 is tensor(5.6281e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1511. State = [[-0.25743505  0.20642887]]. Action = [[ 0.09433902 -0.03709669 -0.06125282 -0.3431077 ]]. Reward = [0.]
Curr episode timestep = 609
Scene graph at timestep 1511 is [True, False, False, False, False, True]
State prediction error at timestep 1511 is tensor(5.7491e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1511 of -1
Current timestep = 1512. State = [[-0.25565884  0.20407473]]. Action = [[-0.08794549  0.07558923  0.02596516 -0.66329354]]. Reward = [0.]
Curr episode timestep = 610
Scene graph at timestep 1512 is [True, False, False, False, False, True]
State prediction error at timestep 1512 is tensor(3.4579e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1513. State = [[-0.2558427   0.20436792]]. Action = [[ 0.03019414  0.09584399  0.00862793 -0.76884604]]. Reward = [0.]
Curr episode timestep = 611
Scene graph at timestep 1513 is [True, False, False, False, False, True]
State prediction error at timestep 1513 is tensor(2.3015e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1513 of -1
Current timestep = 1514. State = [[-0.25650898  0.20536938]]. Action = [[-0.01970071  0.06011737 -0.04459941  0.5216081 ]]. Reward = [0.]
Curr episode timestep = 612
Scene graph at timestep 1514 is [True, False, False, False, False, True]
State prediction error at timestep 1514 is tensor(1.3868e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1515. State = [[-0.2576501  0.2072219]]. Action = [[-0.07477991  0.06049695 -0.07725696 -0.3651694 ]]. Reward = [0.]
Curr episode timestep = 613
Scene graph at timestep 1515 is [True, False, False, False, False, True]
State prediction error at timestep 1515 is tensor(1.1171e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1516. State = [[-0.25934285  0.20977096]]. Action = [[ 0.03414931 -0.07071184 -0.07498308  0.57154787]]. Reward = [0.]
Curr episode timestep = 614
Scene graph at timestep 1516 is [True, False, False, False, False, True]
State prediction error at timestep 1516 is tensor(3.2756e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1517. State = [[-0.2593797   0.20981692]]. Action = [[ 0.00762244 -0.02733128 -0.00693506  0.75507545]]. Reward = [0.]
Curr episode timestep = 615
Scene graph at timestep 1517 is [True, False, False, False, False, True]
State prediction error at timestep 1517 is tensor(2.3511e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1517 of 1
Current timestep = 1518. State = [[-0.25909722  0.20970309]]. Action = [[ 0.09235471 -0.07510921  0.01759695  0.33673334]]. Reward = [0.]
Curr episode timestep = 616
Scene graph at timestep 1518 is [True, False, False, False, False, True]
State prediction error at timestep 1518 is tensor(3.3160e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1519. State = [[-0.2577637   0.20806983]]. Action = [[-0.09384159  0.09062428 -0.05915798 -0.4215402 ]]. Reward = [0.]
Curr episode timestep = 617
Scene graph at timestep 1519 is [True, False, False, False, False, True]
State prediction error at timestep 1519 is tensor(7.3172e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1520. State = [[-0.2583444   0.20879254]]. Action = [[-0.08819663  0.06039915 -0.02219002 -0.0487842 ]]. Reward = [0.]
Curr episode timestep = 618
Scene graph at timestep 1520 is [True, False, False, False, False, True]
State prediction error at timestep 1520 is tensor(2.5007e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1520 of 1
Current timestep = 1521. State = [[-0.260131   0.2109915]]. Action = [[ 0.03775061 -0.01335509 -0.08633704 -0.7879474 ]]. Reward = [0.]
Curr episode timestep = 619
Scene graph at timestep 1521 is [True, False, False, False, False, True]
State prediction error at timestep 1521 is tensor(3.0878e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1522. State = [[-0.2605657   0.21162169]]. Action = [[ 0.00266347  0.02788048  0.04172555 -0.6204172 ]]. Reward = [0.]
Curr episode timestep = 620
Scene graph at timestep 1522 is [True, False, False, False, False, True]
State prediction error at timestep 1522 is tensor(5.2233e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1523. State = [[-0.26103994  0.21241085]]. Action = [[ 0.02765784  0.01238652 -0.01580755  0.836746  ]]. Reward = [0.]
Curr episode timestep = 621
Scene graph at timestep 1523 is [True, False, False, False, False, True]
State prediction error at timestep 1523 is tensor(1.3604e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1523 of 1
Current timestep = 1524. State = [[-0.26116133  0.21262854]]. Action = [[ 0.05400325 -0.0527377   0.03458837  0.6891577 ]]. Reward = [0.]
Curr episode timestep = 622
Scene graph at timestep 1524 is [True, False, False, False, False, True]
State prediction error at timestep 1524 is tensor(2.4688e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1525. State = [[-0.26096874  0.21249267]]. Action = [[ 0.01909318 -0.06887439 -0.06282216  0.5342536 ]]. Reward = [0.]
Curr episode timestep = 623
Scene graph at timestep 1525 is [True, False, False, False, False, True]
State prediction error at timestep 1525 is tensor(3.5638e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1525 of 1
Current timestep = 1526. State = [[-0.2598179   0.21123911]]. Action = [[-0.03238068  0.0065374  -0.03661245 -0.3580398 ]]. Reward = [0.]
Curr episode timestep = 624
Scene graph at timestep 1526 is [True, False, False, False, False, True]
State prediction error at timestep 1526 is tensor(3.5523e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1527. State = [[-0.25974622  0.21114568]]. Action = [[-0.05183802  0.06751437  0.00852251 -0.5273283 ]]. Reward = [0.]
Curr episode timestep = 625
Scene graph at timestep 1527 is [True, False, False, False, False, True]
State prediction error at timestep 1527 is tensor(4.5461e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1527 of 1
Current timestep = 1528. State = [[-0.26019245  0.21169865]]. Action = [[-0.06655148  0.01375757 -0.01845063  0.13975608]]. Reward = [0.]
Curr episode timestep = 626
Scene graph at timestep 1528 is [True, False, False, False, False, True]
State prediction error at timestep 1528 is tensor(3.2860e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1528 of 1
Current timestep = 1529. State = [[-0.26108018  0.21268578]]. Action = [[-0.05115727 -0.08802753 -0.05968488  0.28239453]]. Reward = [0.]
Curr episode timestep = 627
Scene graph at timestep 1529 is [True, False, False, False, False, True]
State prediction error at timestep 1529 is tensor(4.8922e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1530. State = [[-0.261576    0.21230327]]. Action = [[-0.00372929 -0.03080622  0.08966959 -0.08234048]]. Reward = [0.]
Curr episode timestep = 628
Scene graph at timestep 1530 is [True, False, False, False, False, True]
State prediction error at timestep 1530 is tensor(1.0918e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1531. State = [[-0.26183662  0.21190046]]. Action = [[-0.09539679  0.00669533 -0.01303507 -0.9490957 ]]. Reward = [0.]
Curr episode timestep = 629
Scene graph at timestep 1531 is [True, False, False, False, False, True]
State prediction error at timestep 1531 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1531 of 1
Current timestep = 1532. State = [[-0.2634625   0.21190733]]. Action = [[-0.06191812  0.07389007  0.0535777   0.7998204 ]]. Reward = [0.]
Curr episode timestep = 630
Scene graph at timestep 1532 is [True, False, False, False, False, True]
State prediction error at timestep 1532 is tensor(1.4088e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1533. State = [[-0.26602104  0.21396957]]. Action = [[0.07297688 0.0206447  0.00227604 0.60295093]]. Reward = [0.]
Curr episode timestep = 631
Scene graph at timestep 1533 is [True, False, False, False, False, True]
State prediction error at timestep 1533 is tensor(2.3387e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1533 of 1
Current timestep = 1534. State = [[-0.26664665  0.21490502]]. Action = [[ 0.03635242 -0.01520037 -0.05663602  0.38760018]]. Reward = [0.]
Curr episode timestep = 632
Scene graph at timestep 1534 is [True, False, False, False, False, True]
State prediction error at timestep 1534 is tensor(1.4904e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1535. State = [[-0.2666007   0.21502121]]. Action = [[ 0.07219895  0.04638138 -0.03977815 -0.1613605 ]]. Reward = [0.]
Curr episode timestep = 633
Scene graph at timestep 1535 is [True, False, False, False, False, True]
State prediction error at timestep 1535 is tensor(1.4211e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1535 of -1
Current timestep = 1536. State = [[-0.26668578  0.21522711]]. Action = [[-0.06676941  0.05495986 -0.01239641  0.01491606]]. Reward = [0.]
Curr episode timestep = 634
Scene graph at timestep 1536 is [True, False, False, False, False, True]
State prediction error at timestep 1536 is tensor(2.5092e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1536 of -1
Current timestep = 1537. State = [[-0.2681934   0.21688886]]. Action = [[-0.09888177  0.03093832 -0.01780394  0.14042103]]. Reward = [0.]
Curr episode timestep = 635
Scene graph at timestep 1537 is [True, False, False, False, False, True]
State prediction error at timestep 1537 is tensor(3.1376e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1538. State = [[-0.27055266  0.21963376]]. Action = [[-0.07645305 -0.02448545  0.05590416  0.64736724]]. Reward = [0.]
Curr episode timestep = 636
Scene graph at timestep 1538 is [True, False, False, False, False, True]
State prediction error at timestep 1538 is tensor(3.1850e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1539. State = [[-0.2728214   0.22216834]]. Action = [[-0.07693072  0.03726412 -0.01920303  0.3125304 ]]. Reward = [0.]
Curr episode timestep = 637
Scene graph at timestep 1539 is [True, False, False, False, False, True]
State prediction error at timestep 1539 is tensor(8.4085e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1539 of -1
Current timestep = 1540. State = [[-0.27541223  0.2248776 ]]. Action = [[ 0.00717373 -0.02747443  0.09089004 -0.29716134]]. Reward = [0.]
Curr episode timestep = 638
Scene graph at timestep 1540 is [True, False, False, False, False, True]
State prediction error at timestep 1540 is tensor(2.3016e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1540 of -1
Current timestep = 1541. State = [[-0.27664077  0.22579111]]. Action = [[ 0.00108345 -0.08908069 -0.08846275  0.13784921]]. Reward = [0.]
Curr episode timestep = 639
Scene graph at timestep 1541 is [True, False, False, False, False, True]
State prediction error at timestep 1541 is tensor(1.0877e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1542. State = [[-0.2773981  0.2243424]]. Action = [[ 0.02344459  0.07056265 -0.02711129  0.06020093]]. Reward = [0.]
Curr episode timestep = 640
Scene graph at timestep 1542 is [True, False, False, False, False, True]
State prediction error at timestep 1542 is tensor(4.4031e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1542 of -1
Current timestep = 1543. State = [[-0.2776289   0.22439069]]. Action = [[-0.06288759 -0.05995003 -0.04638429 -0.7273884 ]]. Reward = [0.]
Curr episode timestep = 641
Scene graph at timestep 1543 is [True, False, False, False, False, True]
State prediction error at timestep 1543 is tensor(2.4336e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1543 of -1
Current timestep = 1544. State = [[-0.27815628  0.22395523]]. Action = [[ 2.8643981e-03 -4.9129128e-05  9.4612367e-02  7.5595236e-01]]. Reward = [0.]
Curr episode timestep = 642
Scene graph at timestep 1544 is [True, False, False, False, False, True]
State prediction error at timestep 1544 is tensor(3.3922e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1545. State = [[-0.27847356  0.22366576]]. Action = [[ 0.02275185 -0.04265635  0.02832258 -0.52395445]]. Reward = [0.]
Curr episode timestep = 643
Scene graph at timestep 1545 is [True, False, False, False, False, True]
State prediction error at timestep 1545 is tensor(2.3736e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1546. State = [[-0.2783016   0.22243854]]. Action = [[-0.049584   -0.09751065 -0.08196142 -0.49552268]]. Reward = [0.]
Curr episode timestep = 644
Scene graph at timestep 1546 is [True, False, False, False, False, True]
State prediction error at timestep 1546 is tensor(1.0586e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1546 of -1
Current timestep = 1547. State = [[-0.2783733   0.21949488]]. Action = [[-0.06373195 -0.06678567 -0.02372357  0.42548954]]. Reward = [0.]
Curr episode timestep = 645
Scene graph at timestep 1547 is [True, False, False, False, False, True]
State prediction error at timestep 1547 is tensor(4.5466e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1547 of -1
Current timestep = 1548. State = [[-0.28090554  0.21435378]]. Action = [[-0.04024458 -0.06547642 -0.08038543 -0.8605832 ]]. Reward = [0.]
Curr episode timestep = 646
Scene graph at timestep 1548 is [True, False, False, False, False, True]
State prediction error at timestep 1548 is tensor(9.4941e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1549. State = [[-0.28434026  0.20941517]]. Action = [[ 0.04353621 -0.02463625  0.00829524  0.8780663 ]]. Reward = [0.]
Curr episode timestep = 647
Scene graph at timestep 1549 is [True, False, False, False, False, True]
State prediction error at timestep 1549 is tensor(2.1757e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1549 of -1
Current timestep = 1550. State = [[-0.28662387  0.20536849]]. Action = [[ 0.02395926 -0.06132814 -0.04330018  0.7052188 ]]. Reward = [0.]
Curr episode timestep = 648
Scene graph at timestep 1550 is [True, False, False, False, False, True]
State prediction error at timestep 1550 is tensor(8.3178e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1551. State = [[-0.28814778  0.20176199]]. Action = [[-0.09005109 -0.04913193  0.00091445  0.6944716 ]]. Reward = [0.]
Curr episode timestep = 649
Scene graph at timestep 1551 is [True, False, False, False, False, True]
State prediction error at timestep 1551 is tensor(7.8090e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1551 of -1
Current timestep = 1552. State = [[-0.29077134  0.19834287]]. Action = [[-0.09586459 -0.02565466 -0.02353296  0.3740462 ]]. Reward = [0.]
Curr episode timestep = 650
Scene graph at timestep 1552 is [True, False, False, False, False, True]
State prediction error at timestep 1552 is tensor(3.1259e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1552 of -1
Current timestep = 1553. State = [[-0.29310575  0.1960292 ]]. Action = [[-0.01027964  0.05271635 -0.08501619 -0.9663609 ]]. Reward = [0.]
Curr episode timestep = 651
Scene graph at timestep 1553 is [True, False, False, False, False, True]
State prediction error at timestep 1553 is tensor(3.6317e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1554. State = [[-0.29425994  0.19566345]]. Action = [[0.05463208 0.01647259 0.04678767 0.9134908 ]]. Reward = [0.]
Curr episode timestep = 652
Scene graph at timestep 1554 is [True, False, False, False, False, True]
State prediction error at timestep 1554 is tensor(1.4075e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1554 of -1
Current timestep = 1555. State = [[-0.2942653   0.19576266]]. Action = [[-0.06308398  0.03331143 -0.0604274  -0.56091064]]. Reward = [0.]
Curr episode timestep = 653
Scene graph at timestep 1555 is [True, False, False, False, False, True]
State prediction error at timestep 1555 is tensor(1.2375e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1555 of -1
Current timestep = 1556. State = [[-0.2953487   0.19695854]]. Action = [[-0.06863452  0.04519802  0.07096431  0.19811308]]. Reward = [0.]
Curr episode timestep = 654
Scene graph at timestep 1556 is [True, False, False, False, False, True]
State prediction error at timestep 1556 is tensor(2.3973e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1556 of -1
Current timestep = 1557. State = [[-0.2980201   0.19915502]]. Action = [[-0.08652308 -0.04606657  0.04858956  0.3601941 ]]. Reward = [0.]
Curr episode timestep = 655
Scene graph at timestep 1557 is [True, False, False, False, False, True]
State prediction error at timestep 1557 is tensor(1.3593e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1558. State = [[-0.30129156  0.19961149]]. Action = [[ 0.05617661  0.03912737 -0.06035146 -0.69307834]]. Reward = [0.]
Curr episode timestep = 656
Scene graph at timestep 1558 is [True, False, False, False, False, True]
State prediction error at timestep 1558 is tensor(6.4642e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1558 of -1
Current timestep = 1559. State = [[-0.30351147  0.20000166]]. Action = [[ 0.01789998  0.09306688 -0.0479791   0.9495071 ]]. Reward = [0.]
Curr episode timestep = 657
Scene graph at timestep 1559 is [True, False, False, False, False, True]
State prediction error at timestep 1559 is tensor(7.9983e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1560. State = [[-0.30568615  0.20221722]]. Action = [[-0.09335908  0.01734443 -0.02053904  0.5732074 ]]. Reward = [0.]
Curr episode timestep = 658
Scene graph at timestep 1560 is [True, False, False, False, False, True]
State prediction error at timestep 1560 is tensor(1.3424e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1560 of -1
Current timestep = 1561. State = [[-0.30856258  0.20471063]]. Action = [[ 0.06425346 -0.03472261  0.07156842  0.747581  ]]. Reward = [0.]
Curr episode timestep = 659
Scene graph at timestep 1561 is [True, False, False, False, False, True]
State prediction error at timestep 1561 is tensor(1.9207e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1561 of -1
Current timestep = 1562. State = [[-0.3089351   0.20451525]]. Action = [[ 0.01188284 -0.05829218 -0.01590808  0.9847517 ]]. Reward = [0.]
Curr episode timestep = 660
Scene graph at timestep 1562 is [True, False, False, False, False, True]
State prediction error at timestep 1562 is tensor(1.4959e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1563. State = [[-0.30894998  0.20327103]]. Action = [[ 0.01259343  0.08965202 -0.06369344  0.01322734]]. Reward = [0.]
Curr episode timestep = 661
Scene graph at timestep 1563 is [True, False, False, False, False, True]
State prediction error at timestep 1563 is tensor(6.4798e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1564. State = [[-0.30952957  0.20390281]]. Action = [[-0.01534869  0.07332481  0.05416881  0.89866805]]. Reward = [0.]
Curr episode timestep = 662
Scene graph at timestep 1564 is [True, False, False, False, False, True]
State prediction error at timestep 1564 is tensor(1.9491e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1564 of -1
Current timestep = 1565. State = [[-0.31093436  0.20561297]]. Action = [[0.08612261 0.08670115 0.0214922  0.8896074 ]]. Reward = [0.]
Curr episode timestep = 663
Scene graph at timestep 1565 is [True, False, False, False, False, True]
State prediction error at timestep 1565 is tensor(1.7877e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1566. State = [[-0.3118522   0.20765334]]. Action = [[ 0.02472117  0.04391489 -0.08296405  0.6722952 ]]. Reward = [0.]
Curr episode timestep = 664
Scene graph at timestep 1566 is [True, False, False, False, False, True]
State prediction error at timestep 1566 is tensor(4.2012e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1566 of -1
Current timestep = 1567. State = [[-0.31216517  0.2095061 ]]. Action = [[-0.05188113 -0.05014275  0.01879217 -0.97888345]]. Reward = [0.]
Curr episode timestep = 665
Scene graph at timestep 1567 is [True, False, False, False, False, True]
State prediction error at timestep 1567 is tensor(3.7406e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1567 of -1
Current timestep = 1568. State = [[-0.31259584  0.20995854]]. Action = [[-0.03019003  0.04491241  0.00751348  0.7028494 ]]. Reward = [0.]
Curr episode timestep = 666
Scene graph at timestep 1568 is [True, False, False, False, False, True]
State prediction error at timestep 1568 is tensor(3.1619e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1569. State = [[-0.31415886  0.21146283]]. Action = [[-0.04549593 -0.02028159  0.05165782  0.4353025 ]]. Reward = [0.]
Curr episode timestep = 667
Scene graph at timestep 1569 is [True, False, False, False, False, True]
State prediction error at timestep 1569 is tensor(4.8192e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1569 of -1
Current timestep = 1570. State = [[-0.3151805   0.21252652]]. Action = [[-0.07360907  0.00817157 -0.07117553 -0.5081528 ]]. Reward = [0.]
Curr episode timestep = 668
Scene graph at timestep 1570 is [True, False, False, False, False, True]
State prediction error at timestep 1570 is tensor(5.1377e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1571. State = [[-0.31678542  0.21432348]]. Action = [[-0.03993427  0.06577393 -0.03462768  0.52620506]]. Reward = [0.]
Curr episode timestep = 669
Scene graph at timestep 1571 is [True, False, False, False, False, True]
State prediction error at timestep 1571 is tensor(2.9599e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1571 of -1
Current timestep = 1572. State = [[-0.3194404   0.21722224]]. Action = [[ 0.01841106  0.03202172  0.08943852 -0.40069354]]. Reward = [0.]
Curr episode timestep = 670
Scene graph at timestep 1572 is [True, False, False, False, False, True]
State prediction error at timestep 1572 is tensor(2.1056e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1572 of -1
Current timestep = 1573. State = [[-0.3210071   0.21956949]]. Action = [[ 0.07925642 -0.06910054 -0.05809966 -0.15808183]]. Reward = [0.]
Curr episode timestep = 671
Scene graph at timestep 1573 is [True, False, False, False, False, True]
State prediction error at timestep 1573 is tensor(2.4018e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1574. State = [[-0.32022092  0.2198158 ]]. Action = [[-0.06663465 -0.0830263  -0.09007673  0.96264815]]. Reward = [0.]
Curr episode timestep = 672
Scene graph at timestep 1574 is [True, False, False, False, False, True]
State prediction error at timestep 1574 is tensor(5.7485e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1574 of -1
Current timestep = 1575. State = [[-0.31978852  0.21882418]]. Action = [[ 0.05013373  0.05935419 -0.08371199  0.4806069 ]]. Reward = [0.]
Curr episode timestep = 673
Scene graph at timestep 1575 is [True, False, False, False, False, True]
State prediction error at timestep 1575 is tensor(7.3197e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1575 of -1
Current timestep = 1576. State = [[-0.31960124  0.219048  ]]. Action = [[0.06563347 0.00916195 0.00350316 0.12237477]]. Reward = [0.]
Curr episode timestep = 674
Scene graph at timestep 1576 is [True, False, False, False, False, True]
State prediction error at timestep 1576 is tensor(1.4856e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1577. State = [[-0.31888    0.2195666]]. Action = [[0.0266519  0.06452484 0.05280424 0.72153926]]. Reward = [0.]
Curr episode timestep = 675
Scene graph at timestep 1577 is [True, False, False, False, False, True]
State prediction error at timestep 1577 is tensor(4.0769e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1577 of -1
Current timestep = 1578. State = [[-0.31846523  0.22004107]]. Action = [[ 0.08639526  0.06400969 -0.0578911  -0.26300615]]. Reward = [0.]
Curr episode timestep = 676
Scene graph at timestep 1578 is [True, False, False, False, False, True]
State prediction error at timestep 1578 is tensor(1.8026e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1578 of -1
Current timestep = 1579. State = [[-0.31722644  0.22114603]]. Action = [[-0.04187099 -0.06704894 -0.08773023 -0.32647395]]. Reward = [0.]
Curr episode timestep = 677
Scene graph at timestep 1579 is [True, False, False, False, False, True]
State prediction error at timestep 1579 is tensor(2.0657e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1580. State = [[-0.31705105  0.2212945 ]]. Action = [[-0.07952707  0.08641673 -0.07520382 -0.27769494]]. Reward = [0.]
Curr episode timestep = 678
Scene graph at timestep 1580 is [True, False, False, False, False, True]
State prediction error at timestep 1580 is tensor(6.8145e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1580 of -1
Current timestep = 1581. State = [[-0.31812632  0.22234987]]. Action = [[ 0.05785752  0.02145754 -0.08338858 -0.93134636]]. Reward = [0.]
Curr episode timestep = 679
Scene graph at timestep 1581 is [True, False, False, False, False, True]
State prediction error at timestep 1581 is tensor(7.5730e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1582. State = [[-0.31770143  0.22313315]]. Action = [[-4.8525631e-05  3.3962898e-02  9.2369169e-03  8.4542489e-01]]. Reward = [0.]
Curr episode timestep = 680
Scene graph at timestep 1582 is [True, False, False, False, False, True]
State prediction error at timestep 1582 is tensor(2.0852e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1583. State = [[-0.31749868  0.22469287]]. Action = [[-0.08379583  0.01014853 -0.03436814 -0.95590323]]. Reward = [0.]
Curr episode timestep = 681
Scene graph at timestep 1583 is [True, False, False, False, False, True]
State prediction error at timestep 1583 is tensor(3.1331e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1583 of -1
Current timestep = 1584. State = [[-0.318939    0.22636615]]. Action = [[ 0.05402086  0.06937861 -0.08788592 -0.59665483]]. Reward = [0.]
Curr episode timestep = 682
Scene graph at timestep 1584 is [True, False, False, False, False, True]
State prediction error at timestep 1584 is tensor(1.2348e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1585. State = [[-0.3191782   0.22899461]]. Action = [[-0.04250025  0.08827095  0.03197993 -0.23625445]]. Reward = [0.]
Curr episode timestep = 683
Scene graph at timestep 1585 is [True, False, False, False, False, True]
State prediction error at timestep 1585 is tensor(6.8418e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1586. State = [[-0.3202219   0.23312192]]. Action = [[-0.02470494  0.04649957 -0.04772321  0.7346622 ]]. Reward = [0.]
Curr episode timestep = 684
Scene graph at timestep 1586 is [True, False, False, False, False, True]
State prediction error at timestep 1586 is tensor(4.6566e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1586 of -1
Current timestep = 1587. State = [[-0.3217112  0.2371011]]. Action = [[ 0.04608152  0.04239222 -0.00270054 -0.24777234]]. Reward = [0.]
Curr episode timestep = 685
Scene graph at timestep 1587 is [True, False, False, False, False, True]
State prediction error at timestep 1587 is tensor(2.1576e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1588. State = [[-0.32227135  0.23969851]]. Action = [[ 0.03701741  0.00235174  0.01360067 -0.613114  ]]. Reward = [0.]
Curr episode timestep = 686
Scene graph at timestep 1588 is [True, False, False, False, False, True]
State prediction error at timestep 1588 is tensor(1.3598e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1589. State = [[-0.32196298  0.24119744]]. Action = [[0.09455206 0.03866843 0.00037958 0.26535392]]. Reward = [0.]
Curr episode timestep = 687
Scene graph at timestep 1589 is [True, False, False, False, False, True]
State prediction error at timestep 1589 is tensor(9.0512e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1589 of -1
Current timestep = 1590. State = [[-0.3197242   0.24325554]]. Action = [[ 0.0609464   0.0177466  -0.09066509  0.04853201]]. Reward = [0.]
Curr episode timestep = 688
Scene graph at timestep 1590 is [True, False, False, False, False, True]
State prediction error at timestep 1590 is tensor(1.0550e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1591. State = [[-0.31715283  0.24532633]]. Action = [[-0.07442035 -0.0981309   0.0099108  -0.30994558]]. Reward = [0.]
Curr episode timestep = 689
Scene graph at timestep 1591 is [True, False, False, False, False, True]
State prediction error at timestep 1591 is tensor(1.8983e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1591 of -1
Current timestep = 1592. State = [[-0.31634247  0.24590127]]. Action = [[ 0.03522546  0.09641733 -0.04426477  0.24995196]]. Reward = [0.]
Curr episode timestep = 690
Scene graph at timestep 1592 is [True, False, False, False, False, True]
State prediction error at timestep 1592 is tensor(4.9629e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1593. State = [[-0.3151537   0.24716383]]. Action = [[-0.08240703 -0.00671528 -0.00302903 -0.30826622]]. Reward = [0.]
Curr episode timestep = 691
Scene graph at timestep 1593 is [True, False, False, False, False, True]
State prediction error at timestep 1593 is tensor(2.1851e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1593 of -1
Current timestep = 1594. State = [[-0.31582424  0.24793231]]. Action = [[-0.09873309  0.03287425 -0.0636903   0.74978876]]. Reward = [0.]
Curr episode timestep = 692
Scene graph at timestep 1594 is [True, False, False, False, False, True]
State prediction error at timestep 1594 is tensor(2.0445e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1595. State = [[-0.31820863  0.2502578 ]]. Action = [[-0.07902603  0.01334395 -0.01994413 -0.60374844]]. Reward = [0.]
Curr episode timestep = 693
Scene graph at timestep 1595 is [True, False, False, False, False, True]
State prediction error at timestep 1595 is tensor(2.9754e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1595 of -1
Current timestep = 1596. State = [[-0.32211322  0.25421572]]. Action = [[ 0.02022579 -0.02046077  0.04780222 -0.34288704]]. Reward = [0.]
Curr episode timestep = 694
Scene graph at timestep 1596 is [True, False, False, False, False, True]
State prediction error at timestep 1596 is tensor(2.5272e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1596 of -1
Current timestep = 1597. State = [[-0.3224716   0.25455567]]. Action = [[-0.04670903  0.04806096 -0.04299157  0.6510043 ]]. Reward = [0.]
Curr episode timestep = 695
Scene graph at timestep 1597 is [True, False, False, False, False, True]
State prediction error at timestep 1597 is tensor(3.5284e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1597 of -1
Current timestep = 1598. State = [[-0.32422063  0.25612026]]. Action = [[-0.08527713  0.00593446  0.02193759  0.06607711]]. Reward = [0.]
Curr episode timestep = 696
Scene graph at timestep 1598 is [True, False, False, False, False, True]
State prediction error at timestep 1598 is tensor(5.2869e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1599. State = [[-0.3265749   0.25830173]]. Action = [[ 0.0228375   0.09193077 -0.09497826  0.78042614]]. Reward = [0.]
Curr episode timestep = 697
Scene graph at timestep 1599 is [True, False, False, False, False, True]
State prediction error at timestep 1599 is tensor(9.2836e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1599 of -1
Current timestep = 1600. State = [[-0.32935455  0.26104093]]. Action = [[-0.06692916  0.0958866  -0.00988363  0.9841745 ]]. Reward = [0.]
Curr episode timestep = 698
Scene graph at timestep 1600 is [True, False, False, False, False, True]
State prediction error at timestep 1600 is tensor(7.3497e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1600 of -1
Current timestep = 1601. State = [[-0.33330256  0.26523504]]. Action = [[-0.04828832  0.03840334 -0.07783847 -0.56298107]]. Reward = [0.]
Curr episode timestep = 699
Scene graph at timestep 1601 is [True, False, False, False, False, True]
State prediction error at timestep 1601 is tensor(8.4241e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1602. State = [[-0.33707446  0.26887372]]. Action = [[-0.04683863 -0.05067261 -0.08766212  0.64405656]]. Reward = [0.]
Curr episode timestep = 700
Scene graph at timestep 1602 is [True, False, False, False, False, True]
State prediction error at timestep 1602 is tensor(3.5499e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1602 of -1
Current timestep = 1603. State = [[-0.33931163  0.27079132]]. Action = [[ 0.08613903  0.04987601 -0.08497279  0.13521194]]. Reward = [0.]
Curr episode timestep = 701
Scene graph at timestep 1603 is [True, False, False, False, False, True]
State prediction error at timestep 1603 is tensor(9.7667e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1604. State = [[-0.33973068  0.27129084]]. Action = [[ 0.06036652 -0.08287723  0.05755468  0.3209585 ]]. Reward = [0.]
Curr episode timestep = 702
Scene graph at timestep 1604 is [True, False, False, False, False, True]
State prediction error at timestep 1604 is tensor(5.2248e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1604 of -1
Current timestep = 1605. State = [[-0.3389187   0.27118403]]. Action = [[ 0.02488592  0.07836575  0.08808547 -0.52142155]]. Reward = [0.]
Curr episode timestep = 703
Scene graph at timestep 1605 is [True, False, False, False, False, True]
State prediction error at timestep 1605 is tensor(2.8175e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1605 of -1
Current timestep = 1606. State = [[-0.3385796  0.2714255]]. Action = [[-0.05062703 -0.09173495 -0.06902586  0.963454  ]]. Reward = [0.]
Curr episode timestep = 704
Scene graph at timestep 1606 is [True, False, False, False, False, True]
State prediction error at timestep 1606 is tensor(1.3275e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1607. State = [[-0.33837274  0.2711014 ]]. Action = [[ 0.04457218 -0.05187881 -0.0056828   0.04441917]]. Reward = [0.]
Curr episode timestep = 705
Scene graph at timestep 1607 is [True, False, False, False, False, True]
State prediction error at timestep 1607 is tensor(8.3623e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1607 of -1
Current timestep = 1608. State = [[-0.33744767  0.26966467]]. Action = [[-0.06848031 -0.06263077 -0.07859749 -0.27340007]]. Reward = [0.]
Curr episode timestep = 706
Scene graph at timestep 1608 is [True, False, False, False, False, True]
State prediction error at timestep 1608 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1609. State = [[-0.33656213  0.2680528 ]]. Action = [[ 0.05320538  0.09045427  0.02728603 -0.8178353 ]]. Reward = [0.]
Curr episode timestep = 707
Scene graph at timestep 1609 is [True, False, False, False, False, True]
State prediction error at timestep 1609 is tensor(1.2721e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1609 of -1
Current timestep = 1610. State = [[-0.33660537  0.2680912 ]]. Action = [[-0.08983829 -0.05382462 -0.04103974 -0.1319921 ]]. Reward = [0.]
Curr episode timestep = 708
Scene graph at timestep 1610 is [True, False, False, False, False, True]
State prediction error at timestep 1610 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1610 of -1
Current timestep = 1611. State = [[-0.33670947  0.2680528 ]]. Action = [[-0.00253005  0.08909675  0.02733005  0.8936794 ]]. Reward = [0.]
Curr episode timestep = 709
Scene graph at timestep 1611 is [True, False, False, False, False, True]
State prediction error at timestep 1611 is tensor(3.7300e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1612. State = [[-0.33776245  0.26893917]]. Action = [[-0.0323927   0.07667262 -0.02227545  0.37599933]]. Reward = [0.]
Curr episode timestep = 710
Scene graph at timestep 1612 is [True, False, False, False, False, True]
State prediction error at timestep 1612 is tensor(7.2566e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1612 of -1
Current timestep = 1613. State = [[-0.3398764  0.2707366]]. Action = [[-0.06243748  0.04428048 -0.03229692 -0.6569739 ]]. Reward = [0.]
Curr episode timestep = 711
Scene graph at timestep 1613 is [True, False, False, False, False, True]
State prediction error at timestep 1613 is tensor(3.0527e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1613 of -1
Current timestep = 1614. State = [[-0.34273338  0.27330035]]. Action = [[ 0.06616677 -0.09424237 -0.06152269  0.75387275]]. Reward = [0.]
Curr episode timestep = 712
Scene graph at timestep 1614 is [True, False, False, False, False, True]
State prediction error at timestep 1614 is tensor(2.1941e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1615. State = [[-0.34249714  0.27311885]]. Action = [[ 0.06966244 -0.07636814  0.02106731 -0.03801858]]. Reward = [0.]
Curr episode timestep = 713
Scene graph at timestep 1615 is [True, False, False, False, False, True]
State prediction error at timestep 1615 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1616. State = [[-0.3409848   0.27130958]]. Action = [[ 0.02094376 -0.00775472  0.01729418 -0.14553285]]. Reward = [0.]
Curr episode timestep = 714
Scene graph at timestep 1616 is [True, False, False, False, False, True]
State prediction error at timestep 1616 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1616 of -1
Current timestep = 1617. State = [[-0.34006247  0.2700543 ]]. Action = [[ 0.07435768  0.01852521 -0.08131094 -0.26536375]]. Reward = [0.]
Curr episode timestep = 715
Scene graph at timestep 1617 is [True, False, False, False, False, True]
State prediction error at timestep 1617 is tensor(5.6383e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1618. State = [[-0.33889443  0.2692677 ]]. Action = [[ 0.01660587  0.0965462  -0.08055615  0.20333254]]. Reward = [0.]
Curr episode timestep = 716
Scene graph at timestep 1618 is [True, False, False, False, False, True]
State prediction error at timestep 1618 is tensor(4.9569e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1619. State = [[-0.33855674  0.27002603]]. Action = [[-0.04933659  0.06139482 -0.06544569  0.17115593]]. Reward = [0.]
Curr episode timestep = 717
Scene graph at timestep 1619 is [True, False, False, False, False, True]
State prediction error at timestep 1619 is tensor(9.4579e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1619 of -1
Current timestep = 1620. State = [[-0.33909154  0.27149627]]. Action = [[0.04109178 0.01859163 0.01558057 0.5726843 ]]. Reward = [0.]
Curr episode timestep = 718
Scene graph at timestep 1620 is [True, False, False, False, False, True]
State prediction error at timestep 1620 is tensor(3.6054e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1621. State = [[-0.33832562  0.27238935]]. Action = [[0.02867187 0.01588279 0.01979169 0.25306547]]. Reward = [0.]
Curr episode timestep = 719
Scene graph at timestep 1621 is [True, False, False, False, False, True]
State prediction error at timestep 1621 is tensor(6.5965e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1622. State = [[-0.33724892  0.2736095 ]]. Action = [[-0.07140304 -0.01772709 -0.02415351  0.5972941 ]]. Reward = [0.]
Curr episode timestep = 720
Scene graph at timestep 1622 is [True, False, False, False, False, True]
State prediction error at timestep 1622 is tensor(3.3059e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1622 of -1
Current timestep = 1623. State = [[-0.33778095  0.27433836]]. Action = [[-0.06728585  0.08585043 -0.04070277 -0.79199487]]. Reward = [0.]
Curr episode timestep = 721
Scene graph at timestep 1623 is [True, False, False, False, False, True]
State prediction error at timestep 1623 is tensor(2.1752e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1623 of -1
Current timestep = 1624. State = [[-0.33970693  0.27654874]]. Action = [[ 0.04759162  0.00064752 -0.05992246 -0.05750167]]. Reward = [0.]
Curr episode timestep = 722
Scene graph at timestep 1624 is [True, False, False, False, False, True]
State prediction error at timestep 1624 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1625. State = [[-0.339888    0.27773938]]. Action = [[ 0.02921123  0.01794706 -0.02698425  0.81286526]]. Reward = [0.]
Curr episode timestep = 723
Scene graph at timestep 1625 is [True, False, False, False, False, True]
State prediction error at timestep 1625 is tensor(1.6664e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1625 of -1
Current timestep = 1626. State = [[-0.33951503  0.2787753 ]]. Action = [[-0.08234601  0.08550911  0.05670495  0.5373758 ]]. Reward = [0.]
Curr episode timestep = 724
Scene graph at timestep 1626 is [True, False, False, False, False, True]
State prediction error at timestep 1626 is tensor(3.5588e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1627. State = [[-0.3410165   0.28222618]]. Action = [[ 0.01614623 -0.01214802 -0.07138966 -0.65777695]]. Reward = [0.]
Curr episode timestep = 725
Scene graph at timestep 1627 is [True, False, False, False, False, True]
State prediction error at timestep 1627 is tensor(2.2924e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1628. State = [[-0.34166273  0.28393608]]. Action = [[-0.0641267   0.08736999  0.00364961 -0.7730497 ]]. Reward = [0.]
Curr episode timestep = 726
Scene graph at timestep 1628 is [True, False, False, False, False, True]
State prediction error at timestep 1628 is tensor(7.0521e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1629. State = [[-0.3437344   0.28714678]]. Action = [[-0.00207389 -0.07530929 -0.01187482 -0.1419316 ]]. Reward = [0.]
Curr episode timestep = 727
Scene graph at timestep 1629 is [True, False, False, False, False, True]
State prediction error at timestep 1629 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1630. State = [[-0.34452358  0.28782144]]. Action = [[-0.0283545   0.07813633 -0.06374829 -0.7173757 ]]. Reward = [0.]
Curr episode timestep = 728
Scene graph at timestep 1630 is [True, False, False, False, False, True]
State prediction error at timestep 1630 is tensor(1.2686e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1631. State = [[-0.34623048  0.28990245]]. Action = [[-0.04116547  0.03369103 -0.08857761 -0.35180384]]. Reward = [0.]
Curr episode timestep = 729
Scene graph at timestep 1631 is [True, False, False, False, False, True]
State prediction error at timestep 1631 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1632. State = [[-0.34861225  0.29246867]]. Action = [[-0.08989018  0.09244735 -0.03756658 -0.16721421]]. Reward = [0.]
Curr episode timestep = 730
Scene graph at timestep 1632 is [True, False, False, False, False, True]
State prediction error at timestep 1632 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1633. State = [[-0.3524909   0.29691818]]. Action = [[ 0.03633568  0.08769875 -0.06759104 -0.47368455]]. Reward = [0.]
Curr episode timestep = 731
Scene graph at timestep 1633 is [True, False, False, False, False, True]
State prediction error at timestep 1633 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1634. State = [[-0.35408604  0.3016089 ]]. Action = [[-0.0637776  -0.05454562  0.08668194 -0.0468064 ]]. Reward = [0.]
Curr episode timestep = 732
Scene graph at timestep 1634 is [True, False, False, False, False, True]
State prediction error at timestep 1634 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 1635. State = [[-0.35612166  0.30397117]]. Action = [[-0.04594655 -0.0566308  -0.08443843 -0.6453312 ]]. Reward = [0.]
Curr episode timestep = 733
Scene graph at timestep 1635 is [True, False, False, False, False, True]
State prediction error at timestep 1635 is tensor(0.0001, grad_fn=<MseLossBackward0>)
