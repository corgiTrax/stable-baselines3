Current timestep = 0. State = [[-0.32600108  0.07271419]]. Action = [[0.00547141 0.09844287 0.         0.72573614]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0707, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.33165324  0.07564779]]. Action = [[-0.08991172 -0.00939942  0.          0.92522764]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0629, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.3327267   0.07375138]]. Action = [[ 0.06055454 -0.05036603  0.          0.0222559 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0551, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.32805854  0.07549114]]. Action = [[ 0.08049697  0.0493673   0.         -0.50405097]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0355, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of -1
Current timestep = 4. State = [[-0.32509932  0.07515267]]. Action = [[ 0.02328773 -0.05199834  0.          0.06493044]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0423, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.32763597  0.07605463]]. Action = [[-0.06590287  0.04403711  0.         -0.71666926]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0182, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.33447975  0.07240126]]. Action = [[-0.0944499  -0.09542103  0.         -0.55179054]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0163, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.34204453  0.06976098]]. Action = [[-0.08649857  0.01675485  0.          0.75424707]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0301, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.34271887  0.06625143]]. Action = [[ 0.06662089 -0.0699874   0.          0.36496592]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0284, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.33857858  0.06122486]]. Action = [[ 0.05232341 -0.04853386  0.          0.96739995]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0167, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of -1
Current timestep = 10. State = [[-0.3408226   0.06342527]]. Action = [[-0.09423078  0.09772468  0.          0.9580879 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0146, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of -1
Current timestep = 11. State = [[-0.3475621   0.07009299]]. Action = [[-0.06761932  0.08877895  0.         -0.7775144 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of -1
Current timestep = 12. State = [[-0.35214615  0.06887522]]. Action = [[-0.02129613 -0.08025254  0.         -0.9554103 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.35108864  0.06247583]]. Action = [[ 0.06954683 -0.08383784  0.          0.8053738 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.348669    0.05622814]]. Action = [[ 0.03646707 -0.073333    0.          0.9921409 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of -1
Current timestep = 15. State = [[-0.34878284  0.05190501]]. Action = [[-0.00998233 -0.03214703  0.         -0.21259898]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of 1
Current timestep = 16. State = [[-0.35129938  0.0451481 ]]. Action = [[-0.03489441 -0.09524915  0.         -0.46976435]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.35015357  0.04454949]]. Action = [[ 0.0635163   0.08017255  0.         -0.04261672]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of 1
Current timestep = 18. State = [[-0.35226268  0.04923645]]. Action = [[-0.07545794  0.06545139  0.         -0.8054052 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 18 of 1
Current timestep = 19. State = [[-0.35565487  0.05160147]]. Action = [[-0.01580929  0.01787363  0.         -0.62018526]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 19 of 1
Current timestep = 20. State = [[-0.35271978  0.05373265]]. Action = [[0.08636617 0.02659764 0.         0.31571126]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0073, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 20 of 1
Current timestep = 21. State = [[-0.3550571   0.05531453]]. Action = [[-0.09388986  0.0111334   0.          0.03344512]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 21 of 1
Current timestep = 22. State = [[-0.35583568  0.05154135]]. Action = [[ 0.06814859 -0.09286395  0.         -0.547562  ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 22 of 1
Current timestep = 23. State = [[-0.35019168  0.04453779]]. Action = [[ 0.08828158 -0.0902546   0.          0.2043463 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 23 of 1
Current timestep = 24. State = [[-0.34371886  0.03760764]]. Action = [[ 0.07311607 -0.08339415  0.         -0.6374924 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 24 of 1
Current timestep = 25. State = [[-0.33776164  0.03127256]]. Action = [[ 0.05851554 -0.0662307   0.         -0.7174548 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
State prediction error at timestep 25 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.3339726   0.02554929]]. Action = [[ 0.01692252 -0.05257763  0.         -0.5046261 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 26 of 1
Current timestep = 27. State = [[-0.32841858  0.01995743]]. Action = [[ 0.07455435 -0.04914572  0.         -0.845465  ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 27 of 1
Current timestep = 28. State = [[-0.32830626  0.01977881]]. Action = [[-0.08057415  0.07314379  0.          0.7178693 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 28 of 1
Current timestep = 29. State = [[-0.32636178  0.02335269]]. Action = [[ 0.07713055  0.0584488   0.         -0.20907867]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 29 of 1
Current timestep = 30. State = [[-0.32201028  0.02687438]]. Action = [[0.03719694 0.04306758 0.         0.91587687]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(9.8412e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 30 of 1
Current timestep = 31. State = [[-0.32087484  0.03085239]]. Action = [[-0.00825605  0.0558848   0.         -0.6975853 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.31653145  0.02925319]]. Action = [[ 0.09159034 -0.07262917  0.         -0.0199551 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 32 of 1
Current timestep = 33. State = [[-0.31438228  0.03022771]]. Action = [[-0.03129892  0.0649926   0.         -0.4439305 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
State prediction error at timestep 33 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 33 of 1
Current timestep = 34. State = [[-0.31665105  0.03487306]]. Action = [[-0.03591584  0.05532338  0.         -0.9843995 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 34 of 1
Current timestep = 35. State = [[-0.31542736  0.03573505]]. Action = [[ 0.04721586 -0.027675    0.          0.73098695]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.3175134   0.03298064]]. Action = [[-0.07411295 -0.04880783  0.         -0.94026226]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 36 of 1
Current timestep = 37. State = [[-0.3205424   0.02694477]]. Action = [[-0.02388232 -0.08723827  0.         -0.676834  ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 37 of 1
Current timestep = 38. State = [[-0.31980383  0.02523841]]. Action = [[ 0.02564175  0.02558053  0.         -0.6907437 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.3228769   0.02922546]]. Action = [[-0.08591609  0.07412096  0.         -0.5142666 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 39 of 1
Current timestep = 40. State = [[-0.32282248  0.02778042]]. Action = [[ 0.06255252 -0.0802653   0.         -0.49211287]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 40 of 1
Current timestep = 41. State = [[-0.31572634  0.02361608]]. Action = [[ 0.09689099 -0.03796876  0.         -0.4406222 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.30987182  0.02720279]]. Action = [[ 0.03722649  0.09627963  0.         -0.69204974]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.30841565  0.02825253]]. Action = [[-0.01148129 -0.0370589   0.          0.88339376]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
State prediction error at timestep 43 is tensor(7.0319e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.3058535   0.02771088]]. Action = [[ 0.04460324  0.01001731  0.         -0.2980826 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 44 of 1
Current timestep = 45. State = [[-0.30110025  0.0258578 ]]. Action = [[ 0.05711833 -0.04338052  0.          0.5241926 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 45 of 1
Current timestep = 46. State = [[-0.29781422  0.02756149]]. Action = [[0.0157657  0.0591966  0.         0.58347535]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 46 of 1
Current timestep = 47. State = [[-0.29793143  0.03376189]]. Action = [[-0.02665114  0.09001059  0.         -0.21718156]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.29465696  0.03442217]]. Action = [[ 0.0771321  -0.05284992  0.         -0.7236353 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(9.9407e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 48 of 1
Current timestep = 49. State = [[-0.2880033  0.0371149]]. Action = [[ 0.07140274  0.07045711  0.         -0.4494937 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 49 of 1
Current timestep = 50. State = [[-0.282453    0.04382412]]. Action = [[ 0.04889569  0.07595015  0.         -0.8398055 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 50 of 1
Current timestep = 51. State = [[-0.28464374  0.04730071]]. Action = [[-0.09393702  0.00557691  0.          0.38927436]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 51 of 1
Current timestep = 52. State = [[-0.28781322  0.04383238]]. Action = [[-0.02773237 -0.08361594  0.         -0.4780475 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.29137814  0.03997634]]. Action = [[-0.07005791 -0.02983223  0.         -0.30274105]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
State prediction error at timestep 53 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 53 of 1
Current timestep = 54. State = [[-0.29038563  0.03475709]]. Action = [[ 0.04834663 -0.0831732   0.         -0.01870781]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 54 of 1
Current timestep = 55. State = [[-0.29041317  0.02826226]]. Action = [[-0.04689783 -0.07150906  0.          0.30291247]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
State prediction error at timestep 55 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.2911646   0.02200324]]. Action = [[-0.00886305 -0.06040503  0.         -0.23183239]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 56 of 1
Current timestep = 57. State = [[-0.29373866  0.02165182]]. Action = [[-0.06299165  0.05831029  0.          0.85934925]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
State prediction error at timestep 57 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 57 of 1
Current timestep = 58. State = [[-0.29159167  0.02554316]]. Action = [[0.074916   0.06436653 0.         0.24350953]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 58 of 1
Current timestep = 59. State = [[-0.28937358  0.03050154]]. Action = [[-0.00088666  0.06323225  0.         -0.569052  ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
State prediction error at timestep 59 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 59 of 1
Current timestep = 60. State = [[-0.28633192  0.03135742]]. Action = [[ 0.06107097 -0.02590631  0.          0.420982  ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
State prediction error at timestep 60 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 60 of 1
Current timestep = 61. State = [[-0.28861582  0.02988737]]. Action = [[-0.08449722 -0.01530807  0.         -0.60182166]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
State prediction error at timestep 61 is tensor(4.7211e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 61 of 1
Current timestep = 62. State = [[-0.28961378  0.02721983]]. Action = [[ 0.03470323 -0.04034635  0.          0.7821224 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
State prediction error at timestep 62 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 62 of 1
Current timestep = 63. State = [[-0.28662854  0.02641152]]. Action = [[ 0.04905132  0.00681567  0.         -0.4932468 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
State prediction error at timestep 63 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 63 of 1
Current timestep = 64. State = [[-0.28746292  0.02583016]]. Action = [[-0.04332168 -0.01574776  0.          0.19935012]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
State prediction error at timestep 64 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 64 of 1
Current timestep = 65. State = [[-0.29020762  0.02483435]]. Action = [[-0.02927617 -0.00461961  0.          0.61198914]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
State prediction error at timestep 65 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 65 of 1
Current timestep = 66. State = [[-0.29576236  0.0217141 ]]. Action = [[-0.09051077 -0.04824681  0.          0.8271005 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 66 of 1
Current timestep = 67. State = [[-0.30089954  0.01474339]]. Action = [[-0.04394078 -0.0931306   0.         -0.8854726 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 67 of 1
Current timestep = 68. State = [[-0.3002312   0.00893461]]. Action = [[ 0.04991549 -0.04139296  0.          0.02539253]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
State prediction error at timestep 68 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 68 of 1
Current timestep = 69. State = [[-0.29621658  0.00712098]]. Action = [[ 0.05846813  0.00400659  0.         -0.02703726]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
State prediction error at timestep 69 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 69 of 1
Current timestep = 70. State = [[-0.2903315   0.00397436]]. Action = [[ 0.08578683 -0.05529439  0.         -0.6917356 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 70 of 1
Current timestep = 71. State = [[-0.29122436 -0.00046012]]. Action = [[-0.07535294 -0.0370234   0.          0.56018066]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
State prediction error at timestep 71 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 71 of 1
Current timestep = 72. State = [[-0.29617912 -0.00424797]]. Action = [[-0.0505808  -0.02560218  0.         -0.60745347]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
State prediction error at timestep 72 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 72 of 1
Current timestep = 73. State = [[-0.2955565  -0.00764084]]. Action = [[ 0.05637174 -0.02621967  0.          0.8332969 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
State prediction error at timestep 73 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 73 of 1
Current timestep = 74. State = [[-0.29487348 -0.01387585]]. Action = [[-0.01012416 -0.08748574  0.          0.66260684]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
State prediction error at timestep 74 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 74 of 1
Current timestep = 75. State = [[-0.29162523 -0.01922552]]. Action = [[ 0.07380726 -0.03063679  0.          0.11232316]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 75 of 1
Current timestep = 76. State = [[-0.28533885 -0.02333735]]. Action = [[ 0.08221719 -0.04125887  0.          0.18227923]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
State prediction error at timestep 76 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 76 of 1
Current timestep = 77. State = [[-0.28502077 -0.02759807]]. Action = [[-0.04968848 -0.03433776  0.          0.71992564]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
State prediction error at timestep 77 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 77 of 1
Current timestep = 78. State = [[-0.28216758 -0.02597633]]. Action = [[ 0.08394537  0.08559329  0.         -0.8931817 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
State prediction error at timestep 78 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 78 of 1
Current timestep = 79. State = [[-0.28011933 -0.02425923]]. Action = [[-0.00744228  0.00302781  0.          0.45748115]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 79 of 1
Current timestep = 80. State = [[-0.27607605 -0.02791228]]. Action = [[ 0.08375733 -0.06583898  0.         -0.7221494 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
State prediction error at timestep 80 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 80 of 1
Current timestep = 81. State = [[-0.27694866 -0.02678757]]. Action = [[-0.08491497  0.08900034  0.         -0.38071674]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
State prediction error at timestep 81 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 81 of 1
Current timestep = 82. State = [[-0.2778896 -0.0226162]]. Action = [[ 0.04067066  0.03742429  0.         -0.36843437]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
State prediction error at timestep 82 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 82 of 1
Current timestep = 83. State = [[-0.27450177 -0.02322164]]. Action = [[ 0.06032748 -0.04979196  0.          0.6867237 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
State prediction error at timestep 83 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 83 of 1
Current timestep = 84. State = [[-0.27471977 -0.01988471]]. Action = [[-0.03790891  0.08785313  0.          0.38094616]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
State prediction error at timestep 84 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 84 of 1
Current timestep = 85. State = [[-0.28120413 -0.01836461]]. Action = [[-0.09337548 -0.03020565  0.         -0.24495602]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.2872947  -0.01655222]]. Action = [[-0.04717581  0.04522661  0.         -0.08469415]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
State prediction error at timestep 86 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 86 of 1
Current timestep = 87. State = [[-0.29334348 -0.01382672]]. Action = [[-0.0714285   0.02072868  0.         -0.909334  ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
State prediction error at timestep 87 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 87 of 1
Current timestep = 88. State = [[-0.29454795 -0.0095598 ]]. Action = [[0.03938954 0.05816532 0.         0.10826743]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 88 of -1
Current timestep = 89. State = [[-0.2925154  -0.00224868]]. Action = [[0.03899842 0.0836736  0.         0.83642864]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 89 of -1
Current timestep = 90. State = [[-0.2902942  0.0006876]]. Action = [[ 0.03872152 -0.02835832  0.         -0.37269115]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 90 of -1
Current timestep = 91. State = [[-0.2909811  -0.00331718]]. Action = [[-0.02086799 -0.09868501  0.          0.7926737 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
State prediction error at timestep 91 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 91 of -1
Current timestep = 92. State = [[-0.29447046 -0.0088672 ]]. Action = [[-0.04709395 -0.0670763   0.          0.0638088 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
State prediction error at timestep 92 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 92 of -1
Current timestep = 93. State = [[-0.2939697  -0.00691598]]. Action = [[ 0.04742394  0.08021916  0.         -0.8395194 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
State prediction error at timestep 93 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 93 of -1
Current timestep = 94. State = [[-0.29400334 -0.00680478]]. Action = [[-0.0227971  -0.04980759  0.         -0.6658125 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 94 of -1
Current timestep = 95. State = [[-0.29406112 -0.00418067]]. Action = [[ 0.01355658  0.07736204  0.         -0.9213084 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
State prediction error at timestep 95 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 95 of -1
Current timestep = 96. State = [[-0.29490328 -0.00492745]]. Action = [[-0.02097146 -0.06248956  0.         -0.6608384 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
State prediction error at timestep 96 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 96 of -1
Current timestep = 97. State = [[-0.29192528 -0.00735801]]. Action = [[ 0.07392073 -0.01626669  0.         -0.1168921 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
State prediction error at timestep 97 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 97 of -1
Current timestep = 98. State = [[-0.28927675 -0.00708474]]. Action = [[ 0.00827041  0.01296861  0.         -0.6590174 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
State prediction error at timestep 98 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 98 of -1
Current timestep = 99. State = [[-0.28785232 -0.00263511]]. Action = [[0.01941326 0.08075618 0.         0.09661949]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
State prediction error at timestep 99 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 99 of -1
Current timestep = 100. State = [[-0.28401348  0.00199374]]. Action = [[ 0.06598324  0.03580377  0.         -0.6383221 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(4.1626e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 100 of -1
Current timestep = 101. State = [[-0.277556    0.00834149]]. Action = [[ 0.09177034  0.08277447  0.         -0.90809804]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
State prediction error at timestep 101 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 101 of -1
Current timestep = 102. State = [[-0.2751668   0.01396769]]. Action = [[-0.0058133   0.03698909  0.         -0.56544864]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
State prediction error at timestep 102 is tensor(9.3620e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 102 of -1
Current timestep = 103. State = [[-0.27215672  0.0126028 ]]. Action = [[ 0.06322581 -0.07689485  0.          0.42410243]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
State prediction error at timestep 103 is tensor(9.8649e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 103 of -1
Current timestep = 104. State = [[-0.26664257  0.01218028]]. Action = [[ 0.06700557  0.01387324  0.         -0.354203  ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
State prediction error at timestep 104 is tensor(7.4311e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 104 of -1
Current timestep = 105. State = [[-0.260338    0.00955183]]. Action = [[ 0.07168899 -0.07719664  0.         -0.2948206 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
State prediction error at timestep 105 is tensor(2.7357e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 105 of -1
Current timestep = 106. State = [[-0.25586295  0.01265635]]. Action = [[ 0.0227589   0.0978509   0.         -0.18049157]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(4.8055e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 106 of -1
Current timestep = 107. State = [[-0.2513896   0.01434825]]. Action = [[ 0.05896731 -0.03323717  0.          0.52258205]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(8.7207e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 107 of -1
Current timestep = 108. State = [[-0.24493207  0.01416151]]. Action = [[0.07379217 0.00140904 0.         0.55314946]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 108 of 1
Current timestep = 109. State = [[-0.24236469  0.01530034]]. Action = [[-0.01865209  0.01590218  0.          0.4875307 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
State prediction error at timestep 109 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 109 of 1
Current timestep = 110. State = [[-0.24252932  0.02009979]]. Action = [[-0.02230338  0.08558383  0.          0.45989788]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
State prediction error at timestep 110 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 110 of 1
Current timestep = 111. State = [[-0.2434783   0.02650438]]. Action = [[-0.0295091   0.07186138  0.          0.08639717]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
State prediction error at timestep 111 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 111 of 1
Current timestep = 112. State = [[-0.24301375  0.02483064]]. Action = [[ 0.00622706 -0.09192502  0.         -0.8388813 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
State prediction error at timestep 112 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 112 of 1
Current timestep = 113. State = [[-0.23772167  0.01860153]]. Action = [[ 0.07840117 -0.08420084  0.          0.7322631 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.23718888  0.015521  ]]. Action = [[-0.07166152 -0.00811399  0.         -0.19822633]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
State prediction error at timestep 114 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 114 of 1
Current timestep = 115. State = [[-0.23535565  0.01955923]]. Action = [[0.05545235 0.08986665 0.         0.46975958]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
State prediction error at timestep 115 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 115 of 1
Current timestep = 116. State = [[-0.22857744  0.01813956]]. Action = [[ 0.09755535 -0.09089936  0.          0.3851869 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 116 of 1
Current timestep = 117. State = [[-0.22475527  0.01332151]]. Action = [[-0.0015038  -0.04682183  0.          0.8457434 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
State prediction error at timestep 117 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 117 of 1
Current timestep = 118. State = [[-0.2231524   0.01589391]]. Action = [[ 0.00855763  0.09072495  0.         -0.6854348 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
State prediction error at timestep 118 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 118 of 1
Current timestep = 119. State = [[-0.2247002   0.02129505]]. Action = [[-0.05403366  0.06359673  0.         -0.25245404]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.22341986  0.02095638]]. Action = [[ 0.04598685 -0.04801299  0.          0.13331544]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
State prediction error at timestep 120 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 120 of 1
Current timestep = 121. State = [[-0.2168091   0.01541897]]. Action = [[ 0.0922545  -0.08646846  0.          0.18051279]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 121 of 1
Current timestep = 122. State = [[-0.2117544  0.0129855]]. Action = [[0.02650321 0.00715393 0.         0.63147295]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
State prediction error at timestep 122 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 122 of 1
Current timestep = 123. State = [[-0.21353783  0.00903709]]. Action = [[-0.07879685 -0.06913431  0.         -0.44833827]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 123 of 1
Current timestep = 124. State = [[-0.21197656  0.00992759]]. Action = [[0.05396093 0.07643766 0.         0.01568115]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.20629394  0.01450446]]. Action = [[ 0.0709165  0.0538652  0.        -0.7933801]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
State prediction error at timestep 125 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.20029841  0.01396489]]. Action = [[ 0.0663792  -0.04839889  0.         -0.14579374]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.19607382  0.01620616]]. Action = [[0.03049613 0.06948958 0.         0.63306105]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 127 of 1
Current timestep = 128. State = [[-0.19613978  0.0172652 ]]. Action = [[-0.03569733 -0.02074075  0.         -0.77728176]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
State prediction error at timestep 128 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 128 of 1
Current timestep = 129. State = [[-0.19471331  0.01400642]]. Action = [[ 0.0306939  -0.0555954   0.         -0.93369156]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
State prediction error at timestep 129 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 129 of 1
Current timestep = 130. State = [[-0.19312134  0.01019961]]. Action = [[-0.0046023  -0.04057001  0.          0.89010155]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 130 of 1
Current timestep = 131. State = [[-0.18998812  0.0084605 ]]. Action = [[ 0.04444029 -0.00281255  0.         -0.89720106]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.1919704   0.00595262]]. Action = [[-0.08862869 -0.03772309  0.          0.8192699 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 132 of 1
Current timestep = 133. State = [[-0.19079475  0.00398157]]. Action = [[ 0.05406601 -0.00404291  0.         -0.49432707]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 133 of 1
Current timestep = 134. State = [[-0.18881835  0.00774253]]. Action = [[-0.00441301  0.08848799  0.          0.09355152]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.18543904  0.01330019]]. Action = [[ 0.05989621  0.05999134  0.         -0.12226504]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 135 of 1
Current timestep = 136. State = [[-0.18456268  0.01155487]]. Action = [[-0.02428448 -0.08078486  0.         -0.8948339 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.1857174   0.00921132]]. Action = [[-0.02206949 -0.00157355  0.         -0.05361462]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 137 of 1
Current timestep = 138. State = [[-0.1833679   0.00533281]]. Action = [[ 0.05168623 -0.07266496  0.          0.79661083]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 138 of 1
Current timestep = 139. State = [[-0.17712553 -0.00158997]]. Action = [[ 0.08786295 -0.09138995  0.         -0.07985562]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 139 of 1
Current timestep = 140. State = [[-0.17135435 -0.00483656]]. Action = [[ 0.05186374  0.00282942  0.         -0.4725458 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 140 of 1
Current timestep = 141. State = [[-0.17249691 -0.00206048]]. Action = [[-0.07686187  0.07871132  0.         -0.6799419 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.1741195   0.00089934]]. Action = [[0.00082122 0.02773929 0.         0.01154196]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 142 of 1
Current timestep = 143. State = [[-0.17579155 -0.00331771]]. Action = [[-0.03682037 -0.09572562  0.          0.6127136 ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
State prediction error at timestep 143 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 143 of 1
Current timestep = 144. State = [[-0.17233613 -0.00343546]]. Action = [[0.08870447 0.06188498 0.         0.20784247]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 144 of 1
Current timestep = 145. State = [[-0.1738923  -0.00523303]]. Action = [[-0.08837165 -0.06966022  0.          0.36989307]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 145 of 1
Current timestep = 146. State = [[-0.17935681 -0.00405672]]. Action = [[-0.07162268  0.07406243  0.          0.6418474 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 146 of 1
Current timestep = 147. State = [[-0.17844215 -0.0063391 ]]. Action = [[ 0.07083962 -0.08741037  0.          0.06059754]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 147 of 1
Current timestep = 148. State = [[-0.1735951  -0.00917267]]. Action = [[ 0.04831164  0.00080948  0.         -0.7801238 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 148 of 1
Current timestep = 149. State = [[-0.16892523 -0.01404233]]. Action = [[ 0.05004772 -0.08473343  0.         -0.94435716]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 149 of 1
Current timestep = 150. State = [[-0.16209297 -0.013543  ]]. Action = [[0.09425817 0.07264882 0.         0.37518167]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 150 of 1
Current timestep = 151. State = [[-0.1560097  -0.01324911]]. Action = [[ 0.05561682 -0.02570024  0.         -0.60914254]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, True, False]
State prediction error at timestep 151 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 151 of 1
Current timestep = 152. State = [[-0.15558928 -0.01863149]]. Action = [[-0.04348239 -0.08231889  0.          0.19328988]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 152 of 1
Current timestep = 153. State = [[-0.15490507 -0.01716597]]. Action = [[0.0173348  0.09762158 0.         0.72468257]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 153 of 1
Current timestep = 154. State = [[-0.155609   -0.01300761]]. Action = [[-0.04045594  0.0400072   0.         -0.87131596]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0080, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 154 of 1
Current timestep = 155. State = [[-0.15767609 -0.0162311 ]]. Action = [[-0.03692561 -0.08456733  0.         -0.35527867]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 155 of 1
Current timestep = 156. State = [[-0.1583473  -0.01721038]]. Action = [[-0.00832641  0.03645521  0.         -0.27483952]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 156 of 1
Current timestep = 157. State = [[-0.15662904 -0.0149345 ]]. Action = [[0.02806374 0.0291734  0.         0.5612154 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 157 of 1
Current timestep = 158. State = [[-0.1560728  -0.01383882]]. Action = [[-1.41865015e-02  5.43154776e-04  0.00000000e+00 -6.17266357e-01]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 158 of 1
Current timestep = 159. State = [[-0.15605506 -0.0088032 ]]. Action = [[ 6.137043e-05  9.474910e-02  0.000000e+00 -5.090759e-01]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 159 of 1
Current timestep = 160. State = [[-0.15610544 -0.00661053]]. Action = [[-0.004219   -0.02412834  0.          0.2648461 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 160 of 1
Current timestep = 161. State = [[-0.15537831 -0.00494802]]. Action = [[ 0.01523104  0.03021736  0.         -0.77243596]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 161 of 1
Current timestep = 162. State = [[-0.15280056 -0.00342422]]. Action = [[ 0.04451568 -0.00355405  0.          0.09783435]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 162 of 1
Current timestep = 163. State = [[-0.1516065  -0.00419697]]. Action = [[-0.00129797 -0.03034095  0.          0.6195754 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 163 of 1
Current timestep = 164. State = [[-0.15552312 -0.00289922]]. Action = [[-0.08543888  0.03593303  0.         -0.17273843]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 164 of 1
Current timestep = 165. State = [[-0.1600008  -0.00440368]]. Action = [[-0.04518316 -0.05547833  0.          0.8897219 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 165 of 1
Current timestep = 166. State = [[-0.16048196 -0.00988668]]. Action = [[ 0.01817407 -0.08012812  0.          0.49083948]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 166 of 1
Current timestep = 167. State = [[-0.16216587 -0.00963066]]. Action = [[-0.04448966  0.05897982  0.          0.38666296]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 167 of 1
Current timestep = 168. State = [[-0.159384   -0.01259835]]. Action = [[ 0.09722178 -0.09367599  0.          0.53564453]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 168 of 1
Current timestep = 169. State = [[-0.15196215 -0.0172999 ]]. Action = [[ 0.09556485 -0.03180317  0.         -0.9369366 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0065, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 169 of 1
Current timestep = 170. State = [[-0.14506641 -0.02352007]]. Action = [[ 0.07600868 -0.08727472  0.         -0.7885006 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 170 of 1
Current timestep = 171. State = [[-0.14589345 -0.02561883]]. Action = [[-0.08953383  0.04430511  0.         -0.9025282 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(0.0076, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 171 of 1
Current timestep = 172. State = [[-0.14476249 -0.02877989]]. Action = [[ 0.08075079 -0.06847352  0.          0.9038348 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 172 of 1
Current timestep = 173. State = [[-0.14413276 -0.03561598]]. Action = [[-0.03647603 -0.07130264  0.          0.88339937]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 173 of 1
Current timestep = 174. State = [[-0.14507608 -0.04314687]]. Action = [[-0.01162089 -0.07155155  0.         -0.91347176]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0084, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 174 of 1
Current timestep = 175. State = [[-0.14567724 -0.04259552]]. Action = [[-0.01612107  0.09478527  0.         -0.7971004 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.14435564 -0.04198775]]. Action = [[ 0.02928741 -0.01113463  0.         -0.3512308 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 176 of 1
Current timestep = 177. State = [[-0.1419793  -0.04363377]]. Action = [[ 0.0261767  -0.00829627  0.          0.24790382]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 177 of 1
Current timestep = 178. State = [[-0.14529185 -0.04806205]]. Action = [[-0.09163745 -0.06325752  0.          0.3295728 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 178 of 1
Current timestep = 179. State = [[-0.14669779 -0.0535444 ]]. Action = [[ 0.01433487 -0.04800857  0.          0.94930506]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 179 of 1
Current timestep = 180. State = [[-0.14429295 -0.05219929]]. Action = [[ 0.03804996  0.07742857  0.         -0.8041016 ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 180 of 1
Current timestep = 181. State = [[-0.13929868 -0.05248648]]. Action = [[ 0.07930378 -0.04106741  0.         -0.8174323 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.13640994 -0.05112295]]. Action = [[0.01008658 0.05557138 0.         0.33455098]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
State prediction error at timestep 182 is tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 182 of 1
Current timestep = 183. State = [[-0.13920365 -0.04503485]]. Action = [[-0.06810404  0.09531678  0.          0.33271515]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 183 of 1
Current timestep = 184. State = [[-0.14541501 -0.04352689]]. Action = [[-0.09127899 -0.02779184  0.          0.745427  ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 184 of 1
Current timestep = 185. State = [[-0.15268502 -0.04867313]]. Action = [[-0.09636355 -0.09056202  0.          0.22322106]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 185 of 1
Current timestep = 186. State = [[-0.15167242 -0.05298034]]. Action = [[ 0.08464671 -0.0328869   0.         -0.14415836]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 186 of 1
Current timestep = 187. State = [[-0.15300946 -0.05210962]]. Action = [[-0.07815127  0.04510123  0.          0.18217564]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 187 of 1
Current timestep = 188. State = [[-0.15559436 -0.04657611]]. Action = [[2.0243973e-04 8.2921885e-02 0.0000000e+00 9.1610885e-01]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 188 of 1
Current timestep = 189. State = [[-0.15894288 -0.04161086]]. Action = [[-0.04913901  0.03815608  0.          0.76359653]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 189 of 1
Current timestep = 190. State = [[-0.16478296 -0.04443936]]. Action = [[-0.07264464 -0.09381796  0.          0.7066684 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 190 of 1
Current timestep = 191. State = [[-0.16564976 -0.0430751 ]]. Action = [[ 0.04681315  0.07335754  0.         -0.6439773 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 191 of 1
Current timestep = 192. State = [[-0.16444676 -0.03920971]]. Action = [[0.02217938 0.02043974 0.         0.7406099 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 192 of 1
Current timestep = 193. State = [[-0.16507    -0.03289104]]. Action = [[-0.00375225  0.09294512  0.          0.13146615]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
State prediction error at timestep 193 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 193 of -1
Current timestep = 194. State = [[-0.16391642 -0.03157033]]. Action = [[ 0.05008287 -0.05364889  0.          0.7960677 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
State prediction error at timestep 194 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 194 of -1
Current timestep = 195. State = [[-0.16473967 -0.0302947 ]]. Action = [[-0.02814182  0.03442407  0.         -0.6838403 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 195 of -1
Current timestep = 196. State = [[-0.16382362 -0.03296298]]. Action = [[ 0.05241113 -0.09060663  0.          0.7808876 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 196 of -1
Current timestep = 197. State = [[-0.16321632 -0.03661003]]. Action = [[-0.00671043 -0.03036372  0.          0.00210011]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 197 of -1
Current timestep = 198. State = [[-0.16324538 -0.04048108]]. Action = [[ 0.00682299 -0.05610947  0.          0.4137317 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 198 of -1
Current timestep = 199. State = [[-0.16639152 -0.04130343]]. Action = [[-0.0683341   0.02792936  0.         -0.81902784]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 199 of -1
Current timestep = 200. State = [[-0.1650019  -0.03986437]]. Action = [[0.07200954 0.02298602 0.         0.80892324]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 200 of -1
Current timestep = 201. State = [[-0.16168264 -0.03606086]]. Action = [[ 0.0316819   0.06422562  0.         -0.7724134 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
State prediction error at timestep 201 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 201 of -1
Current timestep = 202. State = [[-0.15714976 -0.03598995]]. Action = [[ 0.07575869 -0.0405851   0.          0.48403144]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 202 of -1
Current timestep = 203. State = [[-0.15138653 -0.03372988]]. Action = [[ 0.07423029  0.06170588  0.         -0.34920162]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 203 of -1
Current timestep = 204. State = [[-0.15183249 -0.03053281]]. Action = [[-0.0561965   0.0235825   0.          0.16992688]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 204 of -1
Current timestep = 205. State = [[-0.15362395 -0.03113442]]. Action = [[-0.01214766 -0.03163867  0.          0.5495937 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 205 of -1
Current timestep = 206. State = [[-0.15803932 -0.02970097]]. Action = [[-0.0919937   0.04509547  0.          0.35894275]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 206 of 1
Current timestep = 207. State = [[-0.16346425 -0.03111827]]. Action = [[-0.06767203 -0.05502376  0.          0.43584025]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 207 of 1
Current timestep = 208. State = [[-0.16950291 -0.02835728]]. Action = [[-0.08993266  0.08657319  0.         -0.39181244]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 208 of 1
Current timestep = 209. State = [[-0.17054498 -0.03009162]]. Action = [[ 0.04225356 -0.09665462  0.          0.28994942]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 209 of 1
Current timestep = 210. State = [[-0.16854203 -0.03218048]]. Action = [[ 0.00933317  0.01247513  0.         -0.92215747]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 210 of 1
Current timestep = 211. State = [[-0.16366471 -0.03674692]]. Action = [[ 0.08490918 -0.09501429  0.         -0.4512542 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 211 of 1
Current timestep = 212. State = [[-0.1607587  -0.03875935]]. Action = [[-0.00282031  0.02454438  0.          0.364264  ]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 212 of 1
Current timestep = 213. State = [[-0.15869686 -0.03800159]]. Action = [[0.03461888 0.01323126 0.         0.4790479 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 213 of 1
Current timestep = 214. State = [[-0.16031104 -0.04071711]]. Action = [[-0.05818386 -0.05288879  0.         -0.7358932 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 214 of 1
Current timestep = 215. State = [[-0.15863046 -0.04468319]]. Action = [[ 0.06167316 -0.03446104  0.         -0.70593315]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 215 of 1
Current timestep = 216. State = [[-0.15367684 -0.04899843]]. Action = [[ 0.06324404 -0.05115293  0.         -0.00199223]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 216 of 1
Current timestep = 217. State = [[-0.15547027 -0.04893022]]. Action = [[-0.09052156  0.05748815  0.          0.8718256 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 217 of -1
Current timestep = 218. State = [[-0.16033751 -0.0453408 ]]. Action = [[-0.04543073  0.05475254  0.         -0.1124838 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 218 of -1
Current timestep = 219. State = [[-0.1582409  -0.04236028]]. Action = [[ 0.08587307  0.02512046  0.         -0.49533266]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 219 of -1
Current timestep = 220. State = [[-0.15221804 -0.04558835]]. Action = [[ 0.08564652 -0.09068131  0.         -0.7196089 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 220 of -1
Current timestep = 221. State = [[-0.15052447 -0.05266749]]. Action = [[-0.01313391 -0.0918852   0.         -0.7739809 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, True, False]
State prediction error at timestep 221 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 221 of 1
Current timestep = 222. State = [[-0.1470866  -0.05700739]]. Action = [[ 0.0735329  -0.01958759  0.          0.01930785]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 222 of 1
Current timestep = 223. State = [[-0.14475478 -0.05412981]]. Action = [[ 0.00121419  0.08723023  0.         -0.9686981 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
State prediction error at timestep 223 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 223 of 1
Current timestep = 224. State = [[-0.14175373 -0.04718277]]. Action = [[ 0.05836297  0.09650975  0.         -0.95591414]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 224 of 1
Current timestep = 225. State = [[-0.13658768 -0.04694675]]. Action = [[ 0.07314984 -0.06097743  0.         -0.7813808 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 225 of 1
Current timestep = 226. State = [[-0.13349836 -0.0447719 ]]. Action = [[ 0.01236288  0.07095189  0.         -0.34221292]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 226 of 1
Current timestep = 227. State = [[-0.1295729  -0.03746007]]. Action = [[ 0.0665584   0.09492602  0.         -0.70368266]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
State prediction error at timestep 227 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 227 of 1
Current timestep = 228. State = [[-0.12955676 -0.03731472]]. Action = [[-0.04694101 -0.07862801  0.         -0.80566823]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 228 of 1
Current timestep = 229. State = [[-0.12584667 -0.04000397]]. Action = [[ 0.08821946 -0.02555209  0.         -0.2679593 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 229 of 1
Current timestep = 230. State = [[-0.12487044 -0.04387577]]. Action = [[-0.04702106 -0.06972198  0.          0.88346934]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 230 of 1
Current timestep = 231. State = [[-0.12677029 -0.0435657 ]]. Action = [[-0.03791754  0.04945821  0.          0.22534728]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 231 of 1
Current timestep = 232. State = [[-0.1263851  -0.04630374]]. Action = [[ 0.00905635 -0.07915263  0.         -0.33117795]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
State prediction error at timestep 232 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 232 of 1
Current timestep = 233. State = [[-0.12816666 -0.0532334 ]]. Action = [[-0.06447507 -0.08397794  0.         -0.17242062]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, True, False]
State prediction error at timestep 233 is tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 233 of 1
Current timestep = 234. State = [[-0.13290232 -0.05648542]]. Action = [[-0.0857624   0.00862708  0.          0.30043507]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 234 of 1
Current timestep = 235. State = [[-0.13245174 -0.05535532]]. Action = [[ 0.04071156  0.04291724  0.         -0.895938  ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 235 of 1
Current timestep = 236. State = [[-0.12962633 -0.05929122]]. Action = [[ 0.02593566 -0.08998498  0.          0.9307947 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, True, False]
State prediction error at timestep 236 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 236 of 1
Current timestep = 237. State = [[-0.12707122 -0.05790829]]. Action = [[0.02807421 0.09485836 0.         0.07566929]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 237 of 1
Current timestep = 238. State = [[-0.1268265  -0.05568226]]. Action = [[-0.01636323  0.00161825  0.          0.56288826]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, True, False]
State prediction error at timestep 238 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 238 of 1
Current timestep = 239. State = [[-0.12461933 -0.05375838]]. Action = [[0.05271266 0.03867512 0.         0.13586605]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
State prediction error at timestep 239 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 239 of 1
Current timestep = 240. State = [[-0.12122793 -0.05240794]]. Action = [[4.2003967e-02 2.1658093e-04 0.0000000e+00 3.6082995e-01]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 240 of 1
Current timestep = 241. State = [[-0.11659247 -0.05418864]]. Action = [[ 0.07201641 -0.04362461  0.          0.6045202 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
State prediction error at timestep 241 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 241 of 1
Current timestep = 242. State = [[-0.11814647 -0.05280973]]. Action = [[-0.0841995   0.05710722  0.         -0.4340346 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
State prediction error at timestep 242 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 242 of 1
Current timestep = 243. State = [[-0.12248909 -0.0553361 ]]. Action = [[-0.03621595 -0.08708459  0.         -0.09096122]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, False, True, False]
State prediction error at timestep 243 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 243 of 1
Current timestep = 244. State = [[-0.12705591 -0.05757942]]. Action = [[-0.06419095  0.00464717  0.         -0.7859915 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 244 of 1
Current timestep = 245. State = [[-0.12770288 -0.05909672]]. Action = [[ 0.0309616  -0.02819093  0.         -0.7263334 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, False, True, False]
State prediction error at timestep 245 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 245 of 1
Current timestep = 246. State = [[-0.12945767 -0.05687828]]. Action = [[-0.04650318  0.06461995  0.         -0.04957461]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, False, True, False]
State prediction error at timestep 246 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 246 of 1
Current timestep = 247. State = [[-0.1335999  -0.05064928]]. Action = [[-0.04999832  0.08835281  0.         -0.15043771]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
State prediction error at timestep 247 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 247 of 1
Current timestep = 248. State = [[-0.13457392 -0.0479491 ]]. Action = [[ 0.02381468 -0.00967871  0.          0.38231945]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 248 of 1
Current timestep = 249. State = [[-0.13474175 -0.05048342]]. Action = [[-0.00405899 -0.06028199  0.         -0.7887403 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
State prediction error at timestep 249 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 249 of 1
Current timestep = 250. State = [[-0.13804922 -0.05473115]]. Action = [[-0.05645727 -0.05453246  0.          0.08225417]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, True, False]
State prediction error at timestep 250 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 250 of 1
Current timestep = 251. State = [[-0.13595697 -0.0571834 ]]. Action = [[ 0.0856965  -0.01428275  0.          0.93942165]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, True, False]
State prediction error at timestep 251 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 251 of 1
Current timestep = 252. State = [[-0.1301111 -0.0531075]]. Action = [[ 0.08485211  0.08916015  0.         -0.6085198 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.13071743 -0.04563755]]. Action = [[-0.05376393  0.09384296  0.         -0.8559177 ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, True, False]
State prediction error at timestep 253 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 253 of -1
Current timestep = 254. State = [[-0.13435327 -0.04234792]]. Action = [[-0.03425052 -0.00174691  0.         -0.70462346]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
State prediction error at timestep 254 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 254 of -1
Current timestep = 255. State = [[-0.13130698 -0.04394685]]. Action = [[ 0.09941801 -0.05216471  0.         -0.89710987]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, True, False]
State prediction error at timestep 255 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 255 of -1
Current timestep = 256. State = [[-0.13221796 -0.04186823]]. Action = [[-0.08127139  0.06802996  0.          0.4307065 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 256 of -1
Current timestep = 257. State = [[-0.13814037 -0.03878236]]. Action = [[-0.06374665  0.00863624  0.         -0.3124733 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 257 of -1
Current timestep = 258. State = [[-0.14471339 -0.03733621]]. Action = [[-0.07956789  0.00622722  0.          0.43822432]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
State prediction error at timestep 258 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 258 of -1
Current timestep = 259. State = [[-0.1474603  -0.03899082]]. Action = [[ 0.00856938 -0.05215138  0.          0.08171368]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
State prediction error at timestep 259 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 259 of -1
Current timestep = 260. State = [[-0.15054823 -0.04348341]]. Action = [[-0.05086492 -0.06936179  0.          0.7664912 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 260 of -1
Current timestep = 261. State = [[-0.15416838 -0.04905841]]. Action = [[-0.03246299 -0.06825558  0.          0.7501137 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, True, False]
State prediction error at timestep 261 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 261 of -1
Current timestep = 262. State = [[-0.15481766 -0.05436605]]. Action = [[ 0.01865485 -0.05430936  0.          0.70156765]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, True, False]
State prediction error at timestep 262 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 262 of -1
Current timestep = 263. State = [[-0.15028463 -0.05323379]]. Action = [[ 0.09766851  0.06917987  0.         -0.58320093]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 263 of -1
Current timestep = 264. State = [[-0.14964832 -0.04828498]]. Action = [[-0.02672246  0.06983078  0.          0.45232618]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, True, False]
State prediction error at timestep 264 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 264 of -1
Current timestep = 265. State = [[-0.15242381 -0.04266472]]. Action = [[-0.02568213  0.07518274  0.          0.5275481 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 265 of -1
Current timestep = 266. State = [[-0.15357783 -0.03954525]]. Action = [[0.01099956 0.01253884 0.         0.67403376]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 266 of -1
Current timestep = 267. State = [[-0.1562125  -0.03757467]]. Action = [[-0.04249089  0.02040742  0.         -0.1706391 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
State prediction error at timestep 267 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 267 of -1
Current timestep = 268. State = [[-0.15769032 -0.0413082 ]]. Action = [[ 0.00892997 -0.09785683  0.          0.37172937]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
State prediction error at timestep 268 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 268 of -1
Current timestep = 269. State = [[-0.16206983 -0.04758791]]. Action = [[-0.08443718 -0.0718297   0.          0.52706933]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[-0.16135722 -0.0476323 ]]. Action = [[ 0.07123344  0.0495353   0.         -0.5344321 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
State prediction error at timestep 270 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 270 of -1
Current timestep = 271. State = [[-0.16252795 -0.04258255]]. Action = [[-0.05410247  0.07550652  0.          0.22700238]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
State prediction error at timestep 271 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 271 of -1
Current timestep = 272. State = [[-0.16590731 -0.0428137 ]]. Action = [[-0.02883752 -0.05267926  0.          0.6475346 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
State prediction error at timestep 272 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 272 of -1
Current timestep = 273. State = [[-0.16577215 -0.04782765]]. Action = [[ 0.03131182 -0.07355264  0.          0.95325756]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
State prediction error at timestep 273 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 273 of -1
Current timestep = 274. State = [[-0.16283916 -0.05117672]]. Action = [[ 0.05237477 -0.02099164  0.         -0.20352983]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 274 of -1
Current timestep = 275. State = [[-0.16197936 -0.05707136]]. Action = [[-0.00644375 -0.09693091  0.         -0.23094714]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 275 of -1
Current timestep = 276. State = [[-0.15969835 -0.06066042]]. Action = [[0.0526023  0.00125668 0.         0.7232394 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 276 of -1
Current timestep = 277. State = [[-0.15958117 -0.05736112]]. Action = [[-0.02550939  0.08879787  0.          0.47234225]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, True, False]
State prediction error at timestep 277 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 277 of -1
Current timestep = 278. State = [[-0.1648003  -0.05089149]]. Action = [[-0.08930526  0.09542536  0.         -0.5945606 ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 278 of -1
Current timestep = 279. State = [[-0.16402344 -0.05179121]]. Action = [[ 0.09135266 -0.0841729   0.         -0.80821407]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, True, False]
State prediction error at timestep 279 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 279 of -1
Current timestep = 280. State = [[-0.16175619 -0.05770971]]. Action = [[-0.01585566 -0.06178629  0.          0.16263819]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, True, False]
State prediction error at timestep 280 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 280 of -1
Current timestep = 281. State = [[-0.16456799 -0.05796367]]. Action = [[-0.07400366  0.0562017   0.         -0.56033456]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 281 of -1
Current timestep = 282. State = [[-0.16589117 -0.05650198]]. Action = [[-0.00152902  0.01526272  0.          0.6123655 ]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, True, False]
State prediction error at timestep 282 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 282 of -1
Current timestep = 283. State = [[-0.16727917 -0.05275479]]. Action = [[-0.03750423  0.07217567  0.         -0.91846806]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 283 of -1
Current timestep = 284. State = [[-0.17186442 -0.05257796]]. Action = [[-0.08083881 -0.03688765  0.         -0.7134645 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 284 of -1
Current timestep = 285. State = [[-0.16971885 -0.05733477]]. Action = [[ 0.09422327 -0.08176776  0.          0.89723825]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, True, False]
State prediction error at timestep 285 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 285 of -1
Current timestep = 286. State = [[-0.17042093 -0.05632157]]. Action = [[-0.08761737  0.08258314  0.          0.89712644]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, True, False]
State prediction error at timestep 286 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 286 of -1
Current timestep = 287. State = [[-0.177073   -0.05181196]]. Action = [[-0.08107738  0.04177452  0.          0.16323757]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 287 of -1
Current timestep = 288. State = [[-0.1774031  -0.04700656]]. Action = [[ 0.06146082  0.05537964  0.         -0.42582858]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
State prediction error at timestep 288 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 288 of -1
Current timestep = 289. State = [[-0.17406073 -0.04847872]]. Action = [[ 0.05752004 -0.08870163  0.          0.54638183]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
State prediction error at timestep 289 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 289 of -1
Current timestep = 290. State = [[-0.17697927 -0.05179332]]. Action = [[-0.08103664 -0.02948192  0.         -0.249322  ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 290 of -1
Current timestep = 291. State = [[-0.18335089 -0.05580105]]. Action = [[-0.0650797  -0.0656289   0.         -0.17654765]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 291 of -1
Current timestep = 292. State = [[-0.1873245  -0.05818609]]. Action = [[-0.02092691 -0.00584009  0.          0.3295275 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 292 of -1
Current timestep = 293. State = [[-0.19029295 -0.06245041]]. Action = [[-0.02691568 -0.07341556  0.         -0.7933658 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 293 of -1
Current timestep = 294. State = [[-0.1875455  -0.06888183]]. Action = [[ 0.09146548 -0.07613201  0.          0.57002866]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, True, False]
State prediction error at timestep 294 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 294 of -1
Current timestep = 295. State = [[-0.18676026 -0.06994646]]. Action = [[-0.02480275  0.0449575   0.         -0.900307  ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, True, False]
State prediction error at timestep 295 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 295 of -1
Current timestep = 296. State = [[-0.18462834 -0.0738434 ]]. Action = [[ 0.08160246 -0.09198307  0.          0.81499755]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, True, False]
State prediction error at timestep 296 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 296 of -1
Current timestep = 297. State = [[-0.1838246  -0.07360031]]. Action = [[-0.02037428  0.08227252  0.          0.6097542 ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 297 of -1
Current timestep = 298. State = [[-0.18323019 -0.0684452 ]]. Action = [[0.04626323 0.07029802 0.         0.38949764]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 298 of -1
Current timestep = 299. State = [[-0.183109  -0.0701412]]. Action = [[ 0.00154492 -0.07680337  0.         -0.89236665]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 299 of -1
Current timestep = 300. State = [[-0.18827535 -0.06974158]]. Action = [[-0.09299789  0.0611738   0.         -0.67567706]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, True, False]
State prediction error at timestep 300 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 300 of -1
Current timestep = 301. State = [[-0.1886775 -0.0706135]]. Action = [[ 0.07671905 -0.05831149  0.          0.48132133]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, True, False]
State prediction error at timestep 301 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 301 of -1
Current timestep = 302. State = [[-0.19122115 -0.07107159]]. Action = [[-0.08823712  0.03145764  0.         -0.2816689 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 302 of -1
Current timestep = 303. State = [[-0.19306254 -0.06879587]]. Action = [[0.02453186 0.03115328 0.         0.5466989 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, True, False]
State prediction error at timestep 303 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 303 of -1
Current timestep = 304. State = [[-0.19473734 -0.06957314]]. Action = [[-0.03238072 -0.03942743  0.         -0.67067313]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, True, False]
State prediction error at timestep 304 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 304 of -1
Current timestep = 305. State = [[-0.19767663 -0.06588826]]. Action = [[-0.03080583  0.09482778  0.          0.3595171 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 305 of -1
Current timestep = 306. State = [[-0.19724776 -0.06428166]]. Action = [[ 0.04218138 -0.03215517  0.          0.66730344]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, True, False]
State prediction error at timestep 306 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 306 of -1
Current timestep = 307. State = [[-0.19166496 -0.06225605]]. Action = [[ 0.09879334  0.04171393  0.         -0.2356571 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, True, False]
State prediction error at timestep 307 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 307 of -1
Current timestep = 308. State = [[-0.19093817 -0.06418814]]. Action = [[-0.03846562 -0.08132945  0.         -0.6684125 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, True, False]
State prediction error at timestep 308 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 308 of -1
Current timestep = 309. State = [[-0.19579695 -0.06780984]]. Action = [[-0.07867797 -0.0277949   0.          0.984766  ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, True, False]
State prediction error at timestep 309 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 309 of -1
Current timestep = 310. State = [[-0.19512083 -0.0653867 ]]. Action = [[0.06102801 0.07172287 0.         0.21987319]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, True, False]
State prediction error at timestep 310 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 310 of -1
Current timestep = 311. State = [[-0.1944511  -0.06340946]]. Action = [[-0.01779085 -0.00562849  0.          0.8237605 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 311 of -1
Current timestep = 312. State = [[-0.19216996 -0.06506878]]. Action = [[ 0.05528184 -0.03683294  0.         -0.98781323]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 312 of -1
Current timestep = 313. State = [[-0.19103666 -0.06408012]]. Action = [[-0.00912727  0.03811251  0.         -0.960531  ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 313 of -1
Current timestep = 314. State = [[-0.18682931 -0.05978218]]. Action = [[0.09037936 0.05878516 0.         0.78365624]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
State prediction error at timestep 314 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 314 of -1
Current timestep = 315. State = [[-0.18008414 -0.05563659]]. Action = [[ 0.0883062   0.03008427  0.         -0.83678746]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 315 of -1
Current timestep = 316. State = [[-0.17737192 -0.04932968]]. Action = [[0.00275483 0.08755451 0.         0.7755058 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
State prediction error at timestep 316 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 316 of -1
Current timestep = 317. State = [[-0.18091725 -0.04460484]]. Action = [[-0.07630865  0.02425607  0.          0.842723  ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 317 of -1
Current timestep = 318. State = [[-0.17987813 -0.03816261]]. Action = [[ 0.06427569  0.09032791  0.         -0.5165949 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 318 of -1
Current timestep = 319. State = [[-0.17670104 -0.03854686]]. Action = [[ 0.02962404 -0.09638508  0.         -0.5730113 ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, True, False]
State prediction error at timestep 319 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 319 of 1
Current timestep = 320. State = [[-0.17588587 -0.04357813]]. Action = [[-0.01035931 -0.07425913  0.         -0.5967319 ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, True, False]
State prediction error at timestep 320 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 320 of 1
Current timestep = 321. State = [[-0.17276621 -0.04431172]]. Action = [[ 0.05521739  0.01790784  0.         -0.37068617]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, True, False]
State prediction error at timestep 321 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 321 of 1
Current timestep = 322. State = [[-0.17356507 -0.03933749]]. Action = [[-0.06417686  0.08504356  0.          0.42480016]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, True, False]
State prediction error at timestep 322 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 322 of 1
Current timestep = 323. State = [[-0.17674495 -0.03180399]]. Action = [[-0.0390484   0.09253167  0.         -0.28934157]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, True, False]
State prediction error at timestep 323 is tensor(4.3488e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 323 of 1
Current timestep = 324. State = [[-0.1736213  -0.03027999]]. Action = [[ 0.09236719 -0.05128176  0.          0.10337675]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 324 of 1
Current timestep = 325. State = [[-0.17099038 -0.02627381]]. Action = [[-0.01604781  0.09267994  0.          0.03965294]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, True, False]
State prediction error at timestep 325 is tensor(4.8598e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 325 of 1
Current timestep = 326. State = [[-0.173062   -0.02649689]]. Action = [[-0.0474037  -0.0747159   0.         -0.73558265]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 326 of 1
Current timestep = 327. State = [[-0.17626022 -0.02500767]]. Action = [[-0.05025773  0.05991151  0.          0.7156756 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 327 of 1
Current timestep = 328. State = [[-0.17656618 -0.02408749]]. Action = [[ 0.01483188 -0.02654739  0.         -0.8449246 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, True, False]
State prediction error at timestep 328 is tensor(5.7015e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 328 of 1
Current timestep = 329. State = [[-0.17239025 -0.02453644]]. Action = [[ 0.07153077 -0.0089512   0.          0.22317636]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, True, False]
State prediction error at timestep 329 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 329 of 1
Current timestep = 330. State = [[-0.17426623 -0.02372957]]. Action = [[-0.09071644  0.01495178  0.         -0.94116235]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, True, False]
State prediction error at timestep 330 is tensor(2.9207e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 330 of 1
Current timestep = 331. State = [[-0.17891736 -0.02287768]]. Action = [[-0.04657612  0.00324708  0.          0.6447842 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(9.1819e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 331 of 1
Current timestep = 332. State = [[-0.17930762 -0.01763788]]. Action = [[ 0.02611729  0.0948735   0.         -0.06542206]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, True, False]
State prediction error at timestep 332 is tensor(1.1187e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 332 of 1
Current timestep = 333. State = [[-0.1787795  -0.01200826]]. Action = [[ 0.00742525  0.04018535  0.         -0.9634255 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 333 of 1
Current timestep = 334. State = [[-0.18101269 -0.00800976]]. Action = [[-0.03890841  0.03338685  0.         -0.6265143 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, True, False]
State prediction error at timestep 334 is tensor(9.2165e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 334 of 1
Current timestep = 335. State = [[-0.18566088 -0.00494815]]. Action = [[-0.06099304  0.02105401  0.         -0.70943296]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, True, False]
State prediction error at timestep 335 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 335 of 1
Current timestep = 336. State = [[-0.18643495 -0.0041866 ]]. Action = [[ 0.03389391 -0.0192634   0.         -0.03328335]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(9.7172e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 336 of 1
Current timestep = 337. State = [[-0.19054481  0.000435  ]]. Action = [[-0.09290059  0.08633534  0.         -0.41469097]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, True, False]
State prediction error at timestep 337 is tensor(9.5872e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 337 of 1
Current timestep = 338. State = [[-0.1944808   0.00804069]]. Action = [[-0.00394454  0.07869504  0.         -0.62058634]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, True, False]
State prediction error at timestep 338 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 338 of -1
Current timestep = 339. State = [[-0.20015536  0.01064514]]. Action = [[-0.08290622 -0.02655151  0.         -0.87959236]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, True, False]
State prediction error at timestep 339 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 339 of -1
Current timestep = 340. State = [[-0.20786315  0.01253124]]. Action = [[-0.08320947  0.02549183  0.         -0.6103941 ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, True, False]
State prediction error at timestep 340 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 340 of -1
Current timestep = 341. State = [[-0.20781061  0.00997603]]. Action = [[ 0.09004989 -0.09586656  0.         -0.7210555 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 341 of -1
Current timestep = 342. State = [[-0.21027143  0.01056629]]. Action = [[-0.09730816  0.06055515  0.         -0.36009508]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 342 of -1
Current timestep = 343. State = [[-0.21000452  0.01274576]]. Action = [[ 0.093655   -0.00903754  0.         -0.54143715]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 343 of -1
Current timestep = 344. State = [[-0.20940657  0.01587036]]. Action = [[-0.01525525  0.05012817  0.         -0.7929349 ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 344 of -1
Current timestep = 345. State = [[-0.2069868   0.02200952]]. Action = [[ 0.08044872  0.07752421  0.         -0.6827438 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, True, False]
State prediction error at timestep 345 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 345 of -1
Current timestep = 346. State = [[-0.20433937  0.02436241]]. Action = [[ 0.03419704 -0.02164912  0.         -0.97857916]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, True, False]
State prediction error at timestep 346 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 346 of -1
Current timestep = 347. State = [[-0.20010975  0.02314454]]. Action = [[ 0.08384521 -0.03282953  0.         -0.85368824]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, True, False]
State prediction error at timestep 347 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 347 of -1
Current timestep = 348. State = [[-0.19819438  0.02686347]]. Action = [[ 0.00206444  0.08154074  0.         -0.01167965]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, True, False]
State prediction error at timestep 348 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 348 of -1
Current timestep = 349. State = [[-0.19626923  0.02496198]]. Action = [[ 0.04890954 -0.09870636  0.         -0.21742785]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, True, False]
State prediction error at timestep 349 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 349 of -1
Current timestep = 350. State = [[-0.19626145  0.01987235]]. Action = [[-0.02900519 -0.05216566  0.          0.5353248 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, True, False]
State prediction error at timestep 350 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 350 of -1
Current timestep = 351. State = [[-0.19246791  0.02258583]]. Action = [[ 0.08748151  0.09044392  0.         -0.54822904]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, True, False]
State prediction error at timestep 351 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 351 of -1
Current timestep = 352. State = [[-0.18726452  0.02689574]]. Action = [[ 0.05381917  0.03149999  0.         -0.0811196 ]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, True, False]
State prediction error at timestep 352 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 352 of -1
Current timestep = 353. State = [[-0.18947446  0.0322392 ]]. Action = [[-0.08989915  0.08386911  0.          0.12683249]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 353 of -1
Current timestep = 354. State = [[-0.19319004  0.03295757]]. Action = [[-0.03115468 -0.04374943  0.         -0.35682094]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, True, False]
State prediction error at timestep 354 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 354 of -1
Current timestep = 355. State = [[-0.18958482  0.03639821]]. Action = [[0.0924022  0.07816876 0.         0.52647495]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 355 of -1
Current timestep = 356. State = [[-0.18969949  0.04006496]]. Action = [[-0.05507202  0.00903513  0.          0.31024623]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, True, False]
State prediction error at timestep 356 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 356 of -1
Current timestep = 357. State = [[-0.18948324  0.04280003]]. Action = [[ 0.03034005  0.02762849  0.         -0.9124505 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, True, False]
State prediction error at timestep 357 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 357 of -1
Current timestep = 358. State = [[-0.18505788  0.04711384]]. Action = [[ 0.07486866  0.04607303  0.         -0.8695866 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 358 of -1
Current timestep = 359. State = [[-0.18014567  0.04843143]]. Action = [[ 0.05710319 -0.02710541  0.          0.7870287 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, True, False]
State prediction error at timestep 359 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 359 of -1
Current timestep = 360. State = [[-0.17714036  0.05180471]]. Action = [[0.02166246 0.0605763  0.         0.42248368]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, True, False]
State prediction error at timestep 360 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 360 of -1
Current timestep = 361. State = [[-0.17212856  0.05558329]]. Action = [[0.08328799 0.01962338 0.         0.8764188 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 361 of -1
Current timestep = 362. State = [[-0.16799702  0.05847446]]. Action = [[ 0.02829889  0.02338146  0.         -0.41158187]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, True, False]
State prediction error at timestep 362 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 362 of -1
Current timestep = 363. State = [[-0.17077921  0.06490427]]. Action = [[-0.09289663  0.09668612  0.          0.87441874]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, True, False]
State prediction error at timestep 363 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 363 of -1
Current timestep = 364. State = [[-0.16872332  0.06560855]]. Action = [[ 0.09492279 -0.07134801  0.         -0.6731323 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, True, False]
State prediction error at timestep 364 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 364 of -1
Current timestep = 365. State = [[-0.16241293  0.06303827]]. Action = [[ 0.04886257 -0.03139534  0.          0.4706986 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, True, False]
State prediction error at timestep 365 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 365 of -1
Current timestep = 366. State = [[-0.16102225  0.0621973 ]]. Action = [[-0.04063534 -0.00695677  0.         -0.70274705]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, True, False]
State prediction error at timestep 366 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 366 of -1
Current timestep = 367. State = [[-0.15688588  0.06514228]]. Action = [[ 0.0692027   0.05662096  0.         -0.10286492]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, True, False]
State prediction error at timestep 367 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 367 of -1
Current timestep = 368. State = [[-0.15670583  0.06955405]]. Action = [[-0.06813906  0.04607112  0.         -0.5188022 ]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 368 of 1
Current timestep = 369. State = [[-0.15773478  0.07438918]]. Action = [[-0.01191688  0.05443258  0.         -0.520149  ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, True, False]
State prediction error at timestep 369 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 369 of 1
Current timestep = 370. State = [[-0.15272187  0.08100417]]. Action = [[ 0.09614099  0.07730473  0.         -0.43670487]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, True, False]
State prediction error at timestep 370 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 370 of 1
Current timestep = 371. State = [[-0.1496687   0.08759647]]. Action = [[-0.00150749  0.05768413  0.          0.24221921]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, True, False]
State prediction error at timestep 371 is tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 371 of 1
Current timestep = 372. State = [[-0.14540343  0.08696278]]. Action = [[ 0.0745415  -0.07790264  0.          0.5765537 ]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, True, False]
State prediction error at timestep 372 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 372 of 1
Current timestep = 373. State = [[-0.14483833  0.08308414]]. Action = [[-0.05290949 -0.05492084  0.         -0.84248877]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, False, True, False]
State prediction error at timestep 373 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 373 of 1
Current timestep = 374. State = [[-0.14127469  0.08133907]]. Action = [[ 0.08093139 -0.01396354  0.          0.5242524 ]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, False, True, False]
State prediction error at timestep 374 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 374 of 1
Current timestep = 375. State = [[-0.14005925  0.07783619]]. Action = [[-0.04424628 -0.06614585  0.         -0.81611097]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, False, True, False]
State prediction error at timestep 375 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 375 of 1
Current timestep = 376. State = [[-0.1429086   0.07810614]]. Action = [[-0.06439121  0.04570741  0.          0.45013583]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, False, True, False]
State prediction error at timestep 376 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 376 of 1
Current timestep = 377. State = [[-0.14177817  0.08328953]]. Action = [[ 0.04137696  0.078371    0.         -0.09760213]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, False, True, False]
State prediction error at timestep 377 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 377 of 1
Current timestep = 378. State = [[-0.13932642  0.08875449]]. Action = [[0.01973333 0.05446947 0.         0.13596249]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, False, True, False]
State prediction error at timestep 378 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 378 of 1
Current timestep = 379. State = [[-0.14150868  0.09384625]]. Action = [[-0.06342594  0.0546524   0.          0.5774479 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, False, True, False]
State prediction error at timestep 379 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 379 of 1
Current timestep = 380. State = [[-0.1473528   0.09525304]]. Action = [[-0.09190839 -0.02307994  0.         -0.0374257 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, False, True, False]
State prediction error at timestep 380 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 380 of 1
Current timestep = 381. State = [[-0.1518676   0.09333769]]. Action = [[-0.04559977 -0.04435622  0.         -0.73659575]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, False, True, False]
State prediction error at timestep 381 is tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 381 of 1
Current timestep = 382. State = [[-0.15316582  0.08777166]]. Action = [[-0.00197016 -0.09741582  0.          0.46908796]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, False, True, False]
State prediction error at timestep 382 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 382 of 1
Current timestep = 383. State = [[-0.1544477   0.08970743]]. Action = [[-0.022797    0.09346395  0.         -0.86268085]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, False, True, False]
State prediction error at timestep 383 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 383 of -1
Current timestep = 384. State = [[-0.15892443  0.092229  ]]. Action = [[-0.07083982 -0.00867703  0.          0.63090205]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, False, True, False]
State prediction error at timestep 384 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 384 of -1
Current timestep = 385. State = [[-0.16192035  0.08885591]]. Action = [[-0.01199368 -0.07216595  0.         -0.32019502]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, False, True, False]
State prediction error at timestep 385 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 385 of -1
Current timestep = 386. State = [[-0.16552393  0.08266564]]. Action = [[-0.05673499 -0.08088524  0.         -0.48076904]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, False, True, False]
State prediction error at timestep 386 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.17112966  0.07960925]]. Action = [[-0.0737408  -0.00339995  0.         -0.75540775]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, False, True, False]
State prediction error at timestep 387 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 387 of -1
Current timestep = 388. State = [[-0.17526671  0.08054267]]. Action = [[-0.02622116  0.033847    0.          0.07907486]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, False, True, False]
State prediction error at timestep 388 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 388 of -1
Current timestep = 389. State = [[-0.17429827  0.07779571]]. Action = [[ 0.06236441 -0.06590575  0.          0.6068368 ]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, False, True, False]
State prediction error at timestep 389 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 389 of -1
Current timestep = 390. State = [[-0.17518777  0.07600213]]. Action = [[-0.02984262  0.01462118  0.         -0.00652564]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, False, True, False]
State prediction error at timestep 390 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 390 of -1
Current timestep = 391. State = [[-0.17433771  0.07408182]]. Action = [[ 0.05715913 -0.03149428  0.          0.6485143 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, False, True, False]
State prediction error at timestep 391 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 391 of -1
Current timestep = 392. State = [[-0.1772242   0.07582352]]. Action = [[-0.07017796  0.06844933  0.         -0.64529395]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, False, True, False]
State prediction error at timestep 392 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 392 of -1
Current timestep = 393. State = [[-0.17730236  0.07916447]]. Action = [[ 0.07329843  0.03471876  0.         -0.3552264 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, False, True, False]
State prediction error at timestep 393 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 393 of -1
Current timestep = 394. State = [[-0.17661364  0.08226817]]. Action = [[0.00700842 0.04162858 0.         0.34264314]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, False, True, False]
State prediction error at timestep 394 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 394 of -1
Current timestep = 395. State = [[-0.17696992  0.08301536]]. Action = [[ 0.014413   -0.01221991  0.          0.16560507]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, False, True, False]
State prediction error at timestep 395 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 395 of -1
Current timestep = 396. State = [[-0.17323893  0.0798646 ]]. Action = [[ 0.09075708 -0.05961014  0.         -0.5816625 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, False, True, False]
State prediction error at timestep 396 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 396 of -1
Current timestep = 397. State = [[-0.17234632  0.07993199]]. Action = [[-0.02104814  0.03926653  0.         -0.19760907]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, False, True, False]
State prediction error at timestep 397 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 397 of -1
Current timestep = 398. State = [[-0.17802538  0.07904162]]. Action = [[-0.09797485 -0.03678443  0.          0.00917268]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, False, True, False]
State prediction error at timestep 398 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 398 of -1
Current timestep = 399. State = [[-0.1819464   0.07688168]]. Action = [[-0.0210686  -0.0202799   0.         -0.11815679]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, False, True, False]
State prediction error at timestep 399 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 399 of -1
Current timestep = 400. State = [[-0.18414167  0.0714515 ]]. Action = [[-0.02762383 -0.0919497   0.         -0.14399713]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, False, True, False]
State prediction error at timestep 400 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 400 of -1
Current timestep = 401. State = [[-0.18304467  0.06553921]]. Action = [[ 0.03816517 -0.0538828   0.         -0.97744775]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, False, True, False]
State prediction error at timestep 401 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 401 of -1
Current timestep = 402. State = [[-0.18576302  0.06314874]]. Action = [[-0.08146419  0.00342146  0.          0.7859819 ]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, False, True, False]
State prediction error at timestep 402 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 402 of -1
Current timestep = 403. State = [[-0.18642479  0.05793367]]. Action = [[ 0.03479397 -0.0870811   0.         -0.6147201 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, False, True, False]
State prediction error at timestep 403 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 403 of -1
Current timestep = 404. State = [[-0.18256582  0.05399427]]. Action = [[ 0.0620975  -0.00475386  0.         -0.41089475]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, False, True, False]
State prediction error at timestep 404 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 404 of -1
Current timestep = 405. State = [[-0.18140106  0.05487278]]. Action = [[-0.01063366  0.04804317  0.         -0.4952637 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, False, True, False]
State prediction error at timestep 405 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 405 of -1
Current timestep = 406. State = [[-0.18078382  0.05271269]]. Action = [[ 0.02062781 -0.04921007  0.         -0.5886691 ]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, False, True, False]
State prediction error at timestep 406 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 406 of -1
Current timestep = 407. State = [[-0.18322961  0.05278384]]. Action = [[-0.06041088  0.05067497  0.         -0.39645183]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, False, True, False]
State prediction error at timestep 407 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 407 of -1
Current timestep = 408. State = [[-0.18136992  0.058515  ]]. Action = [[0.08225279 0.09944362 0.         0.63070965]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, False, True, False]
State prediction error at timestep 408 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 408 of -1
Current timestep = 409. State = [[-0.18214245  0.06466646]]. Action = [[-0.04772738  0.06396916  0.         -0.4815097 ]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, False, True, False]
State prediction error at timestep 409 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 409 of -1
Current timestep = 410. State = [[-0.1836168   0.06481463]]. Action = [[ 0.00707344 -0.04516359  0.         -0.4783057 ]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, False, True, False]
State prediction error at timestep 410 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 410 of -1
Current timestep = 411. State = [[-0.18459131  0.06302526]]. Action = [[-0.01448896 -0.02030544  0.          0.7275748 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, False, True, False]
State prediction error at timestep 411 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 411 of -1
Current timestep = 412. State = [[-0.1833313   0.05811081]]. Action = [[ 0.03770194 -0.0918574   0.          0.40693116]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, False, True, False]
State prediction error at timestep 412 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 412 of -1
Current timestep = 413. State = [[-0.17955983  0.05220309]]. Action = [[ 0.05493195 -0.06402253  0.         -0.58467734]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, False, True, False]
State prediction error at timestep 413 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 413 of -1
Current timestep = 414. State = [[-0.1763845   0.04590563]]. Action = [[ 0.02523262 -0.07706375  0.          0.8451227 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, False, True, False]
State prediction error at timestep 414 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 414 of -1
Current timestep = 415. State = [[-0.17385375  0.04635667]]. Action = [[ 0.02522457  0.07333498  0.         -0.892309  ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 415 of -1
Current timestep = 416. State = [[-0.17481221  0.04861672]]. Action = [[-0.04290318  0.02152058  0.         -0.39246225]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, False, True, False]
State prediction error at timestep 416 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 416 of 1
Current timestep = 417. State = [[-0.17265195  0.04707482]]. Action = [[ 0.06113913 -0.03319874  0.          0.5732198 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, False, True, False]
State prediction error at timestep 417 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 417 of 1
Current timestep = 418. State = [[-0.17120403  0.04099487]]. Action = [[-0.01636685 -0.08955954  0.          0.16361046]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, False, True, False]
State prediction error at timestep 418 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 418 of 1
Current timestep = 419. State = [[-0.17207815  0.04022228]]. Action = [[-0.02640935  0.05683101  0.          0.27163982]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 419 of 1
Current timestep = 420. State = [[-0.16832983  0.03774124]]. Action = [[ 0.08347972 -0.0660058   0.         -0.04525518]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, False, True, False]
State prediction error at timestep 420 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 420 of 1
Current timestep = 421. State = [[-0.16101886  0.03860174]]. Action = [[0.08825962 0.06913204 0.         0.42700732]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, False, True, False]
State prediction error at timestep 421 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 421 of 1
Current timestep = 422. State = [[-0.1573452   0.03528644]]. Action = [[ 0.00592866 -0.094462    0.         -0.2452153 ]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 422 of 1
Current timestep = 423. State = [[-0.15135992  0.02925845]]. Action = [[ 0.09540249 -0.05040438  0.          0.5635153 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 423 is [True, False, False, False, True, False]
State prediction error at timestep 423 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 423 of 1
Current timestep = 424. State = [[-0.15157324  0.02479414]]. Action = [[-0.0922026 -0.0316727  0.        -0.5983974]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 424 is [True, False, False, False, True, False]
State prediction error at timestep 424 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 424 of 1
Current timestep = 425. State = [[-0.14961627  0.02487776]]. Action = [[ 0.07285305  0.042893    0.         -0.67023665]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 425 is [True, False, False, False, True, False]
State prediction error at timestep 425 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 425 of 1
Current timestep = 426. State = [[-0.14310138  0.02993136]]. Action = [[0.08575619 0.0896428  0.         0.76356554]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 426 is [True, False, False, False, True, False]
State prediction error at timestep 426 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 426 of 1
Current timestep = 427. State = [[-0.14309248  0.03571857]]. Action = [[-0.05883621  0.06668273  0.          0.44961333]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 427 is [True, False, False, False, True, False]
State prediction error at timestep 427 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 427 of 1
Current timestep = 428. State = [[-0.14528254  0.04071372]]. Action = [[-0.01950016  0.05354036  0.          0.3127619 ]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 428 is [True, False, False, False, True, False]
State prediction error at timestep 428 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 428 of 1
Current timestep = 429. State = [[-0.14140858  0.04458532]]. Action = [[0.09326942 0.02712064 0.         0.84045434]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 429 is [True, False, False, False, True, False]
State prediction error at timestep 429 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 429 of 1
Current timestep = 430. State = [[-0.13903593  0.04521034]]. Action = [[-0.00695437 -0.0263786   0.          0.7258961 ]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 430 of 1
Current timestep = 431. State = [[-0.14091311  0.04405554]]. Action = [[-0.04842305 -0.02636325  0.         -0.84828085]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 431 is [True, False, False, False, True, False]
State prediction error at timestep 431 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 431 of 1
Current timestep = 432. State = [[-0.1370221   0.04850161]]. Action = [[0.09815723 0.08850277 0.         0.2757138 ]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 432 of 1
Current timestep = 433. State = [[-0.13793296  0.04776699]]. Action = [[-0.08803951 -0.09543113  0.         -0.27761996]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 433 is [True, False, False, False, True, False]
State prediction error at timestep 433 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 433 of 1
Current timestep = 434. State = [[-0.13686082  0.04840716]]. Action = [[ 0.05608737  0.05342942  0.         -0.28249395]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 434 is [True, False, False, False, True, False]
State prediction error at timestep 434 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 434 of 1
Current timestep = 435. State = [[-0.13095605  0.04794534]]. Action = [[ 0.08350105 -0.05123835  0.          0.5312569 ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 435 is [True, False, False, False, True, False]
State prediction error at timestep 435 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 435 of 1
Current timestep = 436. State = [[-0.13170367  0.0492194 ]]. Action = [[-0.08645001  0.04940193  0.          0.36231303]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 436 is [True, False, False, False, True, False]
State prediction error at timestep 436 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 436 of 1
Current timestep = 437. State = [[-0.13453193  0.05195604]]. Action = [[-0.01476938  0.01804797  0.          0.8789661 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 437 is [True, False, False, False, True, False]
State prediction error at timestep 437 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 437 of 1
Current timestep = 438. State = [[-0.13445762  0.05609555]]. Action = [[ 0.01568357  0.05870087  0.         -0.88726175]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 438 is [True, False, False, False, True, False]
State prediction error at timestep 438 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 438 of 1
Current timestep = 439. State = [[-0.13722606  0.0613154 ]]. Action = [[-0.05947768  0.05368573  0.         -0.71795774]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 439 is [True, False, False, False, True, False]
State prediction error at timestep 439 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 439 of 1
Current timestep = 440. State = [[-0.13675722  0.05898746]]. Action = [[ 0.05172414 -0.09905875  0.          0.92106736]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 440 is [True, False, False, False, True, False]
State prediction error at timestep 440 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 440 of 1
Current timestep = 441. State = [[-0.13298732  0.06063918]]. Action = [[ 0.05111554  0.07807828  0.         -0.7857082 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 441 is [True, False, False, False, True, False]
State prediction error at timestep 441 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 441 of 1
Current timestep = 442. State = [[-0.13455139  0.06004715]]. Action = [[-0.06383844 -0.06568544  0.         -0.5525626 ]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 442 is [True, False, False, False, True, False]
State prediction error at timestep 442 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 442 of 1
Current timestep = 443. State = [[-0.13654782  0.05935643]]. Action = [[-0.01385225  0.01692414  0.          0.5357895 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 443 is [True, False, False, False, True, False]
State prediction error at timestep 443 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 443 of 1
Current timestep = 444. State = [[-0.13414866  0.05759787]]. Action = [[ 0.05446572 -0.04731554  0.         -0.6588068 ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 444 is [True, False, False, False, True, False]
State prediction error at timestep 444 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 444 of 1
Current timestep = 445. State = [[-0.1350402   0.05407982]]. Action = [[-0.05653703 -0.04194575  0.         -0.6130543 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 445 is [True, False, False, False, True, False]
State prediction error at timestep 445 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 445 of 1
Current timestep = 446. State = [[-0.14035803  0.05490563]]. Action = [[-0.08890095  0.04864433  0.         -0.07782233]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 446 is [True, False, False, False, True, False]
State prediction error at timestep 446 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 446 of 1
Current timestep = 447. State = [[-0.14168428  0.05268551]]. Action = [[ 0.02258806 -0.06818648  0.         -0.43635106]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 447 is [True, False, False, False, True, False]
State prediction error at timestep 447 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 447 of 1
Current timestep = 448. State = [[-0.13789436  0.04693017]]. Action = [[ 0.0632282  -0.06798778  0.          0.177073  ]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 448 is [True, False, False, False, True, False]
State prediction error at timestep 448 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 448 of 1
Current timestep = 449. State = [[-0.14008108  0.04410995]]. Action = [[-0.09454104  0.00309367  0.         -0.4951213 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 449 is [True, False, False, False, True, False]
State prediction error at timestep 449 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 449 of 1
Current timestep = 450. State = [[-0.14160217  0.04372792]]. Action = [[0.0240332  0.00733148 0.         0.8350183 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 450 is [True, False, False, False, True, False]
State prediction error at timestep 450 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 450 of 1
Current timestep = 451. State = [[-0.13957283  0.04063516]]. Action = [[ 0.03983297 -0.05276515  0.         -0.2652316 ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 451 is [True, False, False, False, True, False]
State prediction error at timestep 451 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 451 of 1
Current timestep = 452. State = [[-0.14112875  0.03448075]]. Action = [[-0.04820365 -0.07513787  0.         -0.30374694]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 452 is [True, False, False, False, True, False]
State prediction error at timestep 452 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 452 of -1
Current timestep = 453. State = [[-0.1427531   0.02664506]]. Action = [[-0.00475971 -0.08717313  0.          0.5590708 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 453 is [True, False, False, False, True, False]
State prediction error at timestep 453 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 453 of -1
Current timestep = 454. State = [[-0.14094593  0.02047521]]. Action = [[ 0.04313443 -0.03984813  0.          0.81487536]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 454 is [True, False, False, False, True, False]
State prediction error at timestep 454 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 454 of -1
Current timestep = 455. State = [[-0.14237975  0.02068667]]. Action = [[-0.05236033  0.06505307  0.          0.66218495]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 455 is [True, False, False, False, True, False]
State prediction error at timestep 455 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 455 of -1
Current timestep = 456. State = [[-0.14217196  0.01952251]]. Action = [[ 0.04406077 -0.03255673  0.          0.13810897]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 456 is [True, False, False, False, True, False]
State prediction error at timestep 456 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 456 of -1
Current timestep = 457. State = [[-0.14041878  0.01853404]]. Action = [[ 0.02274585  0.02147541  0.         -0.98474777]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 457 is [True, False, False, False, True, False]
State prediction error at timestep 457 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 457 of -1
Current timestep = 458. State = [[-0.14027396  0.02144014]]. Action = [[-3.0241907e-04  6.5659277e-02  0.0000000e+00  8.1047750e-01]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 458 is [True, False, False, False, True, False]
State prediction error at timestep 458 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 458 of -1
Current timestep = 459. State = [[-0.14457798  0.0247534 ]]. Action = [[-0.07805635  0.03826677  0.         -0.1870166 ]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 459 is [True, False, False, False, True, False]
State prediction error at timestep 459 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 459 of -1
Current timestep = 460. State = [[-0.1468301   0.02198005]]. Action = [[ 0.00880434 -0.07634766  0.          0.86910045]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 460 is [True, False, False, False, True, False]
State prediction error at timestep 460 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 460 of -1
Current timestep = 461. State = [[-0.14581196  0.01862713]]. Action = [[ 0.02487619 -0.01973451  0.         -0.9389087 ]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 461 is [True, False, False, False, True, False]
State prediction error at timestep 461 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 461 of -1
Current timestep = 462. State = [[-0.14657903  0.016418  ]]. Action = [[-0.02559755 -0.02521592  0.          0.6056    ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 462 is [True, False, False, False, True, False]
State prediction error at timestep 462 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 462 of -1
Current timestep = 463. State = [[-0.15055537  0.01218551]]. Action = [[-0.06706325 -0.06072606  0.          0.73507285]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 463 is [True, False, False, False, True, False]
State prediction error at timestep 463 is tensor(8.0929e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 463 of -1
Current timestep = 464. State = [[-0.14813395  0.00981376]]. Action = [[ 0.09222215 -0.00062227  0.          0.6099    ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 464 is [True, False, False, False, True, False]
State prediction error at timestep 464 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 464 of -1
Current timestep = 465. State = [[-0.14761552  0.01141632]]. Action = [[-0.03965111  0.04594036  0.          0.87572074]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 465 is [True, False, False, False, True, False]
State prediction error at timestep 465 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 465 of -1
Current timestep = 466. State = [[-0.144398    0.01596085]]. Action = [[0.094957   0.06845929 0.         0.55359435]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 466 is [True, False, False, False, True, False]
State prediction error at timestep 466 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 466 of -1
Current timestep = 467. State = [[-0.14518912  0.02170324]]. Action = [[-0.06134705  0.07141695  0.          0.44194484]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 467 is [True, False, False, False, True, False]
State prediction error at timestep 467 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 467 of -1
Current timestep = 468. State = [[-0.15050374  0.02149535]]. Action = [[-0.06660388 -0.05616198  0.         -0.42918026]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 468 is [True, False, False, False, True, False]
State prediction error at timestep 468 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 468 of -1
Current timestep = 469. State = [[-0.1532799   0.01935394]]. Action = [[-0.00967009 -0.01965644  0.          0.8977604 ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 469 is [True, False, False, False, True, False]
State prediction error at timestep 469 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 469 of -1
Current timestep = 470. State = [[-0.15625036  0.0149216 ]]. Action = [[-0.0475918  -0.08045634  0.         -0.8412318 ]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 470 is [True, False, False, False, True, False]
State prediction error at timestep 470 is tensor(5.1073e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 470 of -1
Current timestep = 471. State = [[-0.16099858  0.01332886]]. Action = [[-0.06734899  0.01778005  0.         -0.4672984 ]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 471 is [True, False, False, False, True, False]
State prediction error at timestep 471 is tensor(9.3806e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 471 of -1
Current timestep = 472. State = [[-0.1673809   0.01052428]]. Action = [[-0.08799537 -0.05922972  0.         -0.29314065]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 472 is [True, False, False, False, True, False]
State prediction error at timestep 472 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 472 of -1
Current timestep = 473. State = [[-0.17040522  0.00422146]]. Action = [[-0.00452209 -0.08269847  0.          0.8340132 ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 473 is [True, False, False, False, True, False]
State prediction error at timestep 473 is tensor(4.4780e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 473 of -1
Current timestep = 474. State = [[-0.169719   -0.00118287]]. Action = [[ 0.02739341 -0.043472    0.         -0.501233  ]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 474 is [True, False, False, False, True, False]
State prediction error at timestep 474 is tensor(3.7818e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 474 of -1
Current timestep = 475. State = [[-0.16986328 -0.0030639 ]]. Action = [[-0.00786038  0.0094707   0.         -0.81451535]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 475 is [True, False, False, False, True, False]
State prediction error at timestep 475 is tensor(2.1626e-06, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 475 of -1
Current timestep = 476. State = [[-0.16728343 -0.00828964]]. Action = [[ 0.07125125 -0.08911372  0.         -0.45690858]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 476 is [True, False, False, False, True, False]
State prediction error at timestep 476 is tensor(1.1494e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 476 of -1
Current timestep = 477. State = [[-0.16909248 -0.01612669]]. Action = [[-0.0699112  -0.07771468  0.         -0.48689306]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 477 is [True, False, False, False, True, False]
State prediction error at timestep 477 is tensor(5.1643e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 477 of -1
Current timestep = 478. State = [[-0.17147142 -0.02070598]]. Action = [[-0.00161381 -0.0100125   0.          0.36711597]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 478 is [True, False, False, False, True, False]
State prediction error at timestep 478 is tensor(8.1427e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 478 of -1
Current timestep = 479. State = [[-0.17123124 -0.0238836 ]]. Action = [[ 0.01996269 -0.02139977  0.         -0.40872115]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 479 is [True, False, False, False, True, False]
State prediction error at timestep 479 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 479 of -1
Current timestep = 480. State = [[-0.17600879 -0.03004288]]. Action = [[-0.09810743 -0.07547728  0.          0.32659245]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 480 is [True, False, False, False, True, False]
State prediction error at timestep 480 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 480 of -1
Current timestep = 481. State = [[-0.17547892 -0.03661163]]. Action = [[ 0.07962195 -0.05095414  0.         -0.9241635 ]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 481 is [True, False, False, False, True, False]
State prediction error at timestep 481 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 481 of -1
Current timestep = 482. State = [[-0.17528932 -0.04351632]]. Action = [[-0.0306752  -0.06815961  0.         -0.8905649 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 482 is [True, False, False, False, True, False]
State prediction error at timestep 482 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 482 of -1
Current timestep = 483. State = [[-0.18062536 -0.04600887]]. Action = [[-0.09048726  0.03653372  0.          0.7016425 ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 483 is [True, False, False, False, True, False]
State prediction error at timestep 483 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 483 of -1
Current timestep = 484. State = [[-0.18212691 -0.04974816]]. Action = [[ 0.03576415 -0.05752114  0.          0.8414242 ]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 484 is [True, False, False, False, True, False]
State prediction error at timestep 484 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 484 of -1
Current timestep = 485. State = [[-0.18507501 -0.04999904]]. Action = [[-0.07430978  0.06713907  0.         -0.00385588]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 485 is [True, False, False, False, True, False]
State prediction error at timestep 485 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 485 of -1
Current timestep = 486. State = [[-0.18556282 -0.0518837 ]]. Action = [[ 0.04982956 -0.05260249  0.          0.39599156]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 486 is [True, False, False, False, True, False]
State prediction error at timestep 486 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 486 of -1
Current timestep = 487. State = [[-0.18310302 -0.05645225]]. Action = [[ 0.03278843 -0.04170983  0.          0.5911708 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 487 is [True, False, False, False, True, False]
State prediction error at timestep 487 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 487 of -1
Current timestep = 488. State = [[-0.18356265 -0.0604948 ]]. Action = [[-0.0235167  -0.03344165  0.         -0.27325523]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 488 is [True, False, False, False, True, False]
State prediction error at timestep 488 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 488 of -1
Current timestep = 489. State = [[-0.18174964 -0.05929323]]. Action = [[ 0.05592374  0.06569082  0.         -0.66467774]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 489 is [True, False, False, False, True, False]
State prediction error at timestep 489 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 489 of -1
Current timestep = 490. State = [[-0.17723507 -0.05660862]]. Action = [[0.07008121 0.02431556 0.         0.91795206]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 490 is [True, False, False, False, True, False]
State prediction error at timestep 490 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 490 of -1
Current timestep = 491. State = [[-0.17984302 -0.05191914]]. Action = [[-0.09144846  0.0850075   0.         -0.6744486 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 491 is [True, False, False, False, True, False]
State prediction error at timestep 491 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 491 of -1
Current timestep = 492. State = [[-0.18779801 -0.0527785 ]]. Action = [[-0.09925469 -0.06775379  0.         -0.5310874 ]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 492 is [True, False, False, False, True, False]
State prediction error at timestep 492 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 492 of -1
Current timestep = 493. State = [[-0.19542855 -0.0533387 ]]. Action = [[-0.08346933  0.02615581  0.          0.66678166]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 493 is [True, False, False, False, True, False]
State prediction error at timestep 493 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 493 of -1
Current timestep = 494. State = [[-0.19971584 -0.05043176]]. Action = [[-0.02179042  0.04131987  0.          0.46622157]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 494 is [True, False, False, False, True, False]
State prediction error at timestep 494 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 494 of -1
Current timestep = 495. State = [[-0.20442308 -0.05304356]]. Action = [[-0.0646551  -0.08648782  0.         -0.6413839 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 495 is [True, False, False, False, True, False]
State prediction error at timestep 495 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 495 of -1
Current timestep = 496. State = [[-0.20588486 -0.05714152]]. Action = [[ 0.02371898 -0.03865931  0.          0.6673182 ]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 496 is [True, False, False, False, True, False]
State prediction error at timestep 496 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 496 of -1
Current timestep = 497. State = [[-0.20353505 -0.05492625]]. Action = [[0.05278299 0.06344684 0.         0.26891327]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 497 of -1
Current timestep = 498. State = [[-0.20361392 -0.05709079]]. Action = [[-0.01561294 -0.08791915  0.         -0.7369584 ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 498 is [True, False, False, False, True, False]
State prediction error at timestep 498 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 498 of -1
Current timestep = 499. State = [[-0.20164667 -0.06212225]]. Action = [[ 0.06257919 -0.05318955  0.         -0.02911454]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 499 of -1
Current timestep = 500. State = [[-0.19665845 -0.06447695]]. Action = [[ 0.07923927 -0.01187938  0.          0.395378  ]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 500 is [True, False, False, False, True, False]
State prediction error at timestep 500 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 500 of -1
Current timestep = 501. State = [[-0.19772482 -0.06181245]]. Action = [[-0.06242768  0.07433895  0.          0.22176933]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 501 is [True, False, False, False, True, False]
State prediction error at timestep 501 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[-0.19929627 -0.06314617]]. Action = [[ 0.02266383 -0.06622998  0.         -0.7887536 ]]. Reward = [0.]
Curr episode timestep = 502
Scene graph at timestep 502 is [True, False, False, False, True, False]
State prediction error at timestep 502 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 502 of -1
Current timestep = 503. State = [[-0.20018306 -0.0692932 ]]. Action = [[-0.01691885 -0.07919081  0.          0.38255095]]. Reward = [0.]
Curr episode timestep = 503
Scene graph at timestep 503 is [True, False, False, False, True, False]
State prediction error at timestep 503 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 503 of -1
Current timestep = 504. State = [[-0.20040265 -0.07735973]]. Action = [[ 0.01047125 -0.09816615  0.         -0.6524794 ]]. Reward = [0.]
Curr episode timestep = 504
Scene graph at timestep 504 is [True, False, False, False, True, False]
State prediction error at timestep 504 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 504 of -1
Current timestep = 505. State = [[-0.19762865 -0.08030438]]. Action = [[0.05223253 0.0230692  0.         0.74827635]]. Reward = [0.]
Curr episode timestep = 505
Scene graph at timestep 505 is [True, False, False, False, True, False]
State prediction error at timestep 505 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 505 of -1
Current timestep = 506. State = [[-0.19747028 -0.08021663]]. Action = [[-0.02534228  0.01574401  0.         -0.16291404]]. Reward = [0.]
Curr episode timestep = 506
Scene graph at timestep 506 is [True, False, False, False, True, False]
State prediction error at timestep 506 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 506 of -1
Current timestep = 507. State = [[-0.1952716  -0.07688228]]. Action = [[ 0.06102101  0.07912841  0.         -0.20071876]]. Reward = [0.]
Curr episode timestep = 507
Scene graph at timestep 507 is [True, False, False, False, True, False]
State prediction error at timestep 507 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 507 of -1
Current timestep = 508. State = [[-0.19775814 -0.0750315 ]]. Action = [[-0.0813548   0.00397159  0.          0.83922267]]. Reward = [0.]
Curr episode timestep = 508
Scene graph at timestep 508 is [True, False, False, False, True, False]
State prediction error at timestep 508 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 508 of -1
Current timestep = 509. State = [[-0.19610833 -0.07221422]]. Action = [[ 0.08262066  0.06083582  0.         -0.96530396]]. Reward = [0.]
Curr episode timestep = 509
Scene graph at timestep 509 is [True, False, False, False, True, False]
State prediction error at timestep 509 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 509 of -1
Current timestep = 510. State = [[-0.19073126 -0.07096021]]. Action = [[ 0.06876249 -0.01868279  0.         -0.24248779]]. Reward = [0.]
Curr episode timestep = 510
Scene graph at timestep 510 is [True, False, False, False, True, False]
State prediction error at timestep 510 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 510 of -1
Current timestep = 511. State = [[-0.19161686 -0.06759891]]. Action = [[-0.06223296  0.0713981   0.         -0.46082127]]. Reward = [0.]
Curr episode timestep = 511
Scene graph at timestep 511 is [True, False, False, False, True, False]
State prediction error at timestep 511 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 511 of -1
Current timestep = 512. State = [[-0.19216092 -0.0614206 ]]. Action = [[ 0.03048087  0.06908572  0.         -0.9531793 ]]. Reward = [0.]
Curr episode timestep = 512
Scene graph at timestep 512 is [True, False, False, False, True, False]
State prediction error at timestep 512 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 512 of -1
Current timestep = 513. State = [[-0.18930522 -0.06086907]]. Action = [[ 0.05147106 -0.05837036  0.          0.49733412]]. Reward = [0.]
Curr episode timestep = 513
Scene graph at timestep 513 is [True, False, False, False, True, False]
State prediction error at timestep 513 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 513 of -1
Current timestep = 514. State = [[-0.18906924 -0.06438321]]. Action = [[-0.02324053 -0.06275243  0.          0.8765526 ]]. Reward = [0.]
Curr episode timestep = 514
Scene graph at timestep 514 is [True, False, False, False, True, False]
State prediction error at timestep 514 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 514 of -1
Current timestep = 515. State = [[-0.18824399 -0.06398129]]. Action = [[ 0.0261476   0.03352862  0.         -0.9111366 ]]. Reward = [0.]
Curr episode timestep = 515
Scene graph at timestep 515 is [True, False, False, False, True, False]
State prediction error at timestep 515 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 515 of -1
Current timestep = 516. State = [[-0.18861233 -0.05801337]]. Action = [[-0.02442135  0.09146517  0.         -0.6486336 ]]. Reward = [0.]
Curr episode timestep = 516
Scene graph at timestep 516 is [True, False, False, False, True, False]
State prediction error at timestep 516 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 516 of -1
Current timestep = 517. State = [[-0.1923999  -0.05934199]]. Action = [[-0.06879039 -0.09341213  0.          0.231202  ]]. Reward = [0.]
