Current timestep = 0. State = [[-0.32604954 -0.08628346]]. Action = [[0.00435984 0.09830839 0.         0.7271644 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0668, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.33171427 -0.08338373]]. Action = [[-0.08950701 -0.00990611  0.          0.9261981 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0582, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 1 of 0
Current timestep = 2. State = [[-0.33283383 -0.08530018]]. Action = [[ 0.0591953  -0.05022874  0.          0.01753402]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0498, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.32827222 -0.08361258]]. Action = [[ 0.07941534  0.04839215  0.         -0.50824416]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0282, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 3 of -1
Current timestep = 4. State = [[-0.32541162 -0.08398326]]. Action = [[ 0.02239845 -0.05188038  0.          0.06062174]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0375, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 4 of 1
Current timestep = 5. State = [[-0.32796675 -0.08312983]]. Action = [[-0.0650274   0.04302271  0.         -0.7176385 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 5 of 1
Current timestep = 6. State = [[-0.33477855 -0.08681779]]. Action = [[-0.09400506 -0.09525163  0.         -0.55290127]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 6 of 1
Current timestep = 7. State = [[-0.3422992  -0.08951994]]. Action = [[-0.08572423  0.01562224  0.          0.750232  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0277, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.34299538 -0.09307873]]. Action = [[ 0.06536894 -0.06984878  0.          0.35946965]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0267, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.33896858 -0.09812841]]. Action = [[ 0.05112741 -0.04880217  0.          0.96609235]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0148, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 9 of -1
Current timestep = 10. State = [[-0.34126276 -0.09595422]]. Action = [[-0.09369723  0.09746768  0.          0.95627165]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 10 of 1
Current timestep = 11. State = [[-0.34794775 -0.08934801]]. Action = [[-0.06643744  0.08786661  0.         -0.7742701 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 11 of -1
Current timestep = 12. State = [[-0.3524779 -0.0905951]]. Action = [[-0.02073304 -0.08006974  0.         -0.953666  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.35145253 -0.09699097]]. Action = [[ 0.06851716 -0.08366282  0.          0.79928064]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.3490931  -0.10324334]]. Action = [[ 0.03594377 -0.07340407  0.          0.9914515 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
State prediction error at timestep 14 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 14 of -1
Current timestep = 15. State = [[-0.3492051  -0.10763322]]. Action = [[-0.00938684 -0.0333802   0.         -0.21281189]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 15 of -1
Current timestep = 16. State = [[-0.3516624  -0.11442283]]. Action = [[-0.03384993 -0.09516405  0.         -0.46593124]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 16 of -1
Current timestep = 17. State = [[-0.35049215 -0.11506777]]. Action = [[ 0.06302872  0.07866377  0.         -0.04544705]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 17 of -1
Current timestep = 18. State = [[-0.35256913 -0.1105453 ]]. Action = [[-0.07450773  0.06322706  0.         -0.7987211 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 18 of 1
Current timestep = 19. State = [[-0.35588723 -0.10842355]]. Action = [[-0.0151241   0.01501335  0.         -0.61243546]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(8.3032e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 19 of -1
Current timestep = 20. State = [[-0.3529294  -0.10658315]]. Action = [[0.08587851 0.02360664 0.         0.30805802]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
State prediction error at timestep 20 is tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 20 of -1
Current timestep = 21. State = [[-0.3552443  -0.10530661]]. Action = [[-0.09349927  0.00813707  0.          0.03198695]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
State prediction error at timestep 21 is tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 21 of -1
Current timestep = 22. State = [[-0.3560001  -0.10925151]]. Action = [[ 0.0674574  -0.09286922  0.         -0.5376913 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.3503707  -0.11632421]]. Action = [[ 0.08775287 -0.09032382  0.          0.20089722]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 23 of -1
Current timestep = 24. State = [[-0.34393305 -0.12332004]]. Action = [[ 0.07226434 -0.0836765   0.         -0.62550634]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
State prediction error at timestep 24 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 24 of 1
Current timestep = 25. State = [[-0.338045   -0.12975693]]. Action = [[ 0.05743415 -0.06716742  0.         -0.7057046 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, True, False, False]
State prediction error at timestep 25 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.33434442 -0.135629  ]]. Action = [[ 0.01587658 -0.05406368  0.         -0.4929161 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, True, False, False]
State prediction error at timestep 26 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 26 of 1
Current timestep = 27. State = [[-0.32889485 -0.14139305]]. Action = [[ 0.0733547  -0.0507746   0.         -0.83603394]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, True, False, False]
State prediction error at timestep 27 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 27 of 1
Current timestep = 28. State = [[-0.32884207 -0.14178202]]. Action = [[-0.08043855  0.07095058  0.          0.70862484]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, True, False, False]
State prediction error at timestep 28 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 28 of 1
Current timestep = 29. State = [[-0.3269625  -0.13847685]]. Action = [[ 0.07569515  0.05576753  0.         -0.20216435]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, True, False, False]
State prediction error at timestep 29 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 29 of 1
Current timestep = 30. State = [[-0.32276702 -0.13526149]]. Action = [[0.03476859 0.04020453 0.         0.9106114 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, True, False, False]
State prediction error at timestep 30 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 30 of -1
Current timestep = 31. State = [[-0.32183194 -0.13158888]]. Action = [[-0.01072341  0.05343957  0.         -0.6870496 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, True, False, False]
State prediction error at timestep 31 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.31762272 -0.13337763]]. Action = [[ 0.09072233 -0.07327565  0.         -0.01742512]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, True, False, False]
State prediction error at timestep 32 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 32 of 1
Current timestep = 33. State = [[-0.31563935 -0.13262151]]. Action = [[-0.03406612  0.06319768  0.         -0.43546098]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, True, False, False]
State prediction error at timestep 33 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 33 of 1
Current timestep = 34. State = [[-0.31816524 -0.12822469]]. Action = [[-0.03899949  0.05345771  0.         -0.98308897]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, True, False, False]
State prediction error at timestep 34 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 34 of 1
Current timestep = 35. State = [[-0.31727034 -0.12759434]]. Action = [[ 0.04323674 -0.02907844  0.          0.72494924]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, True, False, False]
State prediction error at timestep 35 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.3196317  -0.13053074]]. Action = [[-0.07595457 -0.04963168  0.         -0.9378149 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, True, False, False]
State prediction error at timestep 36 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 36 of -1
Current timestep = 37. State = [[-0.3230629 -0.1366883]]. Action = [[-0.02892287 -0.08735245  0.         -0.67358625]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, True, False, False]
State prediction error at timestep 37 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 37 of -1
Current timestep = 38. State = [[-0.32287955 -0.13852811]]. Action = [[ 0.01961317  0.02470384  0.         -0.6895131 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, True, False, False]
State prediction error at timestep 38 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 38 of -1
Current timestep = 39. State = [[-0.3263287  -0.13467449]]. Action = [[-0.0874325   0.07378779  0.         -0.5165596 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, True, False, False]
State prediction error at timestep 39 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 39 of -1
Current timestep = 40. State = [[-0.3266571  -0.13619336]]. Action = [[ 0.05771794 -0.08032751  0.         -0.49662477]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, True, False, False]
State prediction error at timestep 40 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[-0.319832   -0.14039661]]. Action = [[ 0.09634056 -0.03795537  0.         -0.44817507]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, True, False, False]
State prediction error at timestep 41 is tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 41 of -1
Current timestep = 42. State = [[-0.31442124 -0.1368533 ]]. Action = [[ 0.02985174  0.09632755  0.         -0.6989912 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.31374136 -0.1358025 ]]. Action = [[-0.01997603 -0.03665127  0.          0.8815143 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, True, False, False]
State prediction error at timestep 43 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.31201255 -0.13631925]]. Action = [[ 0.03705629  0.01083443  0.         -0.31406224]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, True, False, False]
State prediction error at timestep 44 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 44 of 1
Current timestep = 45. State = [[-0.30801934 -0.13812166]]. Action = [[ 0.05047431 -0.04266078  0.          0.51495194]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, True, False, False]
State prediction error at timestep 45 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 45 of 1
Current timestep = 46. State = [[-0.3056245  -0.13635884]]. Action = [[0.00594115 0.06006943 0.         0.5753076 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, True, False, False]
State prediction error at timestep 46 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 46 of 1
Current timestep = 47. State = [[-0.3067133  -0.13014588]]. Action = [[-0.03622417  0.09032283  0.         -0.24044716]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, True, False, False]
State prediction error at timestep 47 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.3042125 -0.1293895]]. Action = [[ 0.07278945 -0.05177694  0.         -0.7409788 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, True, False, False]
State prediction error at timestep 48 is tensor(5.7693e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 48 of -1
Current timestep = 49. State = [[-0.298151   -0.12659858]]. Action = [[ 0.06599826  0.07134343  0.         -0.47560143]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, True, False, False]
State prediction error at timestep 49 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 49 of 1
Current timestep = 50. State = [[-0.29333007 -0.11980322]]. Action = [[ 0.04020061  0.07672177  0.         -0.85387546]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 50 of 1
Current timestep = 51. State = [[-0.29603612 -0.11619728]]. Action = [[-0.09539088  0.00752448  0.          0.37358713]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 51 of 1
Current timestep = 52. State = [[-0.30005223 -0.11962385]]. Action = [[-0.03881961 -0.08291006  0.         -0.50827354]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.30455264 -0.12342639]]. Action = [[-0.07638757 -0.02774891  0.         -0.3360783 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
State prediction error at timestep 53 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 53 of -1
Current timestep = 54. State = [[-0.30456007 -0.12857683]]. Action = [[ 0.03887311 -0.08235345  0.         -0.05051565]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, True, False, False]
State prediction error at timestep 54 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 54 of -1
Current timestep = 55. State = [[-0.30570266 -0.13497993]]. Action = [[-0.05684292 -0.07018974  0.          0.279783  ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, True, False, False]
State prediction error at timestep 55 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 55 of -1
Current timestep = 56. State = [[-0.3077298  -0.14112243]]. Action = [[-0.02168216 -0.05867187  0.         -0.26951778]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, True, False, False]
State prediction error at timestep 56 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 56 of 1
Current timestep = 57. State = [[-0.31143555 -0.14133304]]. Action = [[-0.07091916  0.05984498  0.          0.85797143]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, True, False, False]
State prediction error at timestep 57 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 57 of -1
Current timestep = 58. State = [[-0.31014165 -0.13725631]]. Action = [[0.06935482 0.0657466  0.         0.21454227]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, True, False, False]
State prediction error at timestep 58 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 58 of -1
Current timestep = 59. State = [[-0.30898985 -0.132099  ]]. Action = [[-0.0140919   0.06470919  0.         -0.6026439 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, True, False, False]
State prediction error at timestep 59 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 59 of 1
Current timestep = 60. State = [[-0.30698743 -0.1309903 ]]. Action = [[ 0.0529517  -0.02316169  0.          0.39836383]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, True, False, False]
State prediction error at timestep 60 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 60 of 1
Current timestep = 61. State = [[-0.3100177  -0.13215405]]. Action = [[-0.08829913 -0.01238718  0.         -0.6333768 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, True, False, False]
State prediction error at timestep 61 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 61 of 1
Current timestep = 62. State = [[-0.3120167  -0.13456096]]. Action = [[ 0.02327086 -0.03768999  0.          0.7750784 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, True, False, False]
State prediction error at timestep 62 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.31011727 -0.13506034]]. Action = [[ 0.03926594  0.00999854  0.         -0.52779865]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, True, False, False]
State prediction error at timestep 63 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 63 of -1
Current timestep = 64. State = [[-0.31204742 -0.1352866 ]]. Action = [[-0.05349667 -0.01247378  0.          0.16586399]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, True, False, False]
State prediction error at timestep 64 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 64 of -1
Current timestep = 65. State = [[-0.31604347 -0.13594289]]. Action = [[-0.04061817 -0.00114848  0.          0.59419155]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, True, False, False]
State prediction error at timestep 65 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 65 of -1
Current timestep = 66. State = [[-0.32250652 -0.13874635]]. Action = [[-0.09276054 -0.04546292  0.          0.8198359 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, True, False, False]
State prediction error at timestep 66 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 66 of -1
Current timestep = 67. State = [[-0.328758   -0.14550187]]. Action = [[-0.05367894 -0.09265179  0.         -0.8961849 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, True, False, False]
State prediction error at timestep 67 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.32936507 -0.15098855]]. Action = [[ 0.04071102 -0.0381937   0.         -0.01015091]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, True, False, False]
State prediction error at timestep 68 is tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 68 of -1
Current timestep = 69. State = [[-0.3263646  -0.15230402]]. Action = [[ 0.05047629  0.00800221  0.         -0.06186944]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, True, False, False]
State prediction error at timestep 69 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 69 of -1
Current timestep = 70. State = [[-0.321122   -0.15488255]]. Action = [[ 0.08270835 -0.05248303  0.         -0.71158016]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, True, False, False]
State prediction error at timestep 70 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 70 of -1
Current timestep = 71. State = [[-0.3226006  -0.15876909]]. Action = [[-0.08025423 -0.03341431  0.          0.54008615]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, True, False, False]
State prediction error at timestep 71 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 71 of 1
Current timestep = 72. State = [[-0.32854337 -0.16197768]]. Action = [[-0.05889529 -0.02154542  0.         -0.6279106 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, True, False, False]
State prediction error at timestep 72 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 72 of 1
Current timestep = 73. State = [[-0.3289555  -0.16479152]]. Action = [[ 0.04853032 -0.02201426  0.          0.8250339 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, True, False, False]
State prediction error at timestep 73 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 73 of 1
Current timestep = 74. State = [[-0.3293525  -0.17059505]]. Action = [[-0.02102365 -0.08647005  0.          0.64737904]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, True, False, False]
State prediction error at timestep 74 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.3269902  -0.17543252]]. Action = [[ 0.0686504  -0.02631444  0.          0.08718789]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, True, False, False]
State prediction error at timestep 75 is tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.32118583 -0.17887978]]. Action = [[ 0.07860089 -0.03718986  0.          0.15892231]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, True, False, False]
State prediction error at timestep 76 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 76 of -1
Current timestep = 77. State = [[-0.32147262 -0.18249117]]. Action = [[-0.0573786  -0.02985714  0.          0.7076216 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, True, False, False]
State prediction error at timestep 77 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 77 of -1
Current timestep = 78. State = [[-0.31924635 -0.18049775]]. Action = [[ 0.08077373  0.08704656  0.         -0.8962076 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, True, False, False]
State prediction error at timestep 78 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 78 of 1
Current timestep = 79. State = [[-0.317866  -0.1782324]]. Action = [[-0.01751216  0.00867192  0.          0.43992782]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, True, False, False]
State prediction error at timestep 79 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 79 of 1
Current timestep = 80. State = [[-0.314432   -0.18132384]]. Action = [[ 0.0806906  -0.06234742  0.         -0.7281188 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, True, False, False]
State prediction error at timestep 80 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 80 of 1
Current timestep = 81. State = [[-0.31568122 -0.17994685]]. Action = [[-0.08761399  0.09021873  0.         -0.39317775]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, True, False, False]
State prediction error at timestep 81 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 81 of 1
Current timestep = 82. State = [[-0.3173515  -0.17535348]]. Action = [[ 0.03213952  0.04286902  0.         -0.37971246]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, True, False, False]
State prediction error at timestep 82 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 82 of 1
Current timestep = 83. State = [[-0.31474197 -0.17534852]]. Action = [[ 0.05374653 -0.04437998  0.          0.6757057 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, True, False, False]
State prediction error at timestep 83 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 83 of -1
Current timestep = 84. State = [[-0.31579235 -0.17165314]]. Action = [[-0.04631562  0.08937413  0.          0.3673532 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, True, False, False]
State prediction error at timestep 84 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 84 of -1
Current timestep = 85. State = [[-0.3230196  -0.16957536]]. Action = [[-0.09459894 -0.02311455  0.         -0.25211835]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.33013168 -0.16710374]]. Action = [[-0.05490036  0.05126522  0.         -0.09251708]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, True, False, False]
State prediction error at timestep 86 is tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 86 of 1
Current timestep = 87. State = [[-0.33722675 -0.1636724 ]]. Action = [[-0.07625969  0.02854335  0.         -0.9074717 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, True, False, False]
State prediction error at timestep 87 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 87 of -1
Current timestep = 88. State = [[-0.3395907 -0.1587411]]. Action = [[0.02977944 0.06360794 0.         0.10083985]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, True, False, False]
State prediction error at timestep 88 is tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 88 of -1
Current timestep = 89. State = [[-0.33866283 -0.15095401]]. Action = [[0.02915566 0.0861693  0.         0.83008397]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, True, False, False]
State prediction error at timestep 89 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 89 of -1
Current timestep = 90. State = [[-0.3374992  -0.14729662]]. Action = [[ 0.02870757 -0.01935488  0.         -0.37170923]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, True, False, False]
State prediction error at timestep 90 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 90 of 1
Current timestep = 91. State = [[-0.3393931  -0.15082459]]. Action = [[-0.03147396 -0.09837717  0.          0.78607666]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, True, False, False]
State prediction error at timestep 91 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 91 of 1
Current timestep = 92. State = [[-0.34422606 -0.1558713 ]]. Action = [[-0.05559605 -0.06085392  0.          0.05952895]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, True, False, False]
State prediction error at timestep 92 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 92 of -1
Current timestep = 93. State = [[-0.34511438 -0.15349896]]. Action = [[ 0.03749264  0.08357289  0.         -0.8356049 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, True, False, False]
State prediction error at timestep 93 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 93 of -1
Current timestep = 94. State = [[-0.3463482  -0.15259382]]. Action = [[-0.03392569 -0.0407352   0.         -0.6616234 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, True, False, False]
State prediction error at timestep 94 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.3477447  -0.14928232]]. Action = [[ 0.00110892  0.08136535  0.         -0.9186236 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, True, False, False]
State prediction error at timestep 95 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 95 of -1
Current timestep = 96. State = [[-0.34988153 -0.14928181]]. Action = [[-0.03267292 -0.05427058  0.         -0.65735817]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, True, False, False]
State prediction error at timestep 96 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 96 of -1
Current timestep = 97. State = [[-0.34798208 -0.15060993]]. Action = [[ 0.06716257 -0.00392943  0.         -0.11983448]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, True, False, False]
State prediction error at timestep 97 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 97 of -1
Current timestep = 98. State = [[-0.34645548 -0.14901371]]. Action = [[-0.0050102   0.0250584   0.         -0.65675414]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, True, False, False]
State prediction error at timestep 98 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 98 of -1
Current timestep = 99. State = [[-0.34631282 -0.14371656]]. Action = [[0.0060809  0.08447302 0.         0.08897901]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, True, False, False]
State prediction error at timestep 99 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 99 of 1
Current timestep = 100. State = [[-0.343451   -0.13811754]]. Action = [[ 0.05726785  0.04652049  0.         -0.6375803 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, True, False, False]
State prediction error at timestep 100 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 100 of 1
Current timestep = 101. State = [[-0.33744863 -0.13086578]]. Action = [[ 0.08909085  0.0861996   0.         -0.9064896 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, True, False, False]
State prediction error at timestep 101 is tensor(9.0231e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 101 of 1
Current timestep = 102. State = [[-0.33589265 -0.12417465]]. Action = [[-0.01951872  0.04787368  0.         -0.56707656]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
State prediction error at timestep 102 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 102 of 1
Current timestep = 103. State = [[-0.33411145 -0.12455157]]. Action = [[ 0.05363388 -0.06961186  0.          0.41084087]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
State prediction error at timestep 103 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 103 of 1
Current timestep = 104. State = [[-0.3297296  -0.12378321]]. Action = [[ 0.05800038  0.02740938  0.         -0.36194313]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
State prediction error at timestep 104 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 104 of 1
Current timestep = 105. State = [[-0.32444727 -0.12526816]]. Action = [[ 0.06358682 -0.0696546   0.         -0.3052088 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, True, False, False]
State prediction error at timestep 105 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 105 of 1
Current timestep = 106. State = [[-0.3212252  -0.12157547]]. Action = [[ 0.00860628  0.09825394  0.         -0.19493043]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 106 of 1
Current timestep = 107. State = [[-0.31808758 -0.11874389]]. Action = [[ 0.04848332 -0.01875998  0.          0.5041555 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 107 of 1
Current timestep = 108. State = [[-0.31276917 -0.11730315]]. Action = [[0.06624293 0.01624225 0.         0.5342767 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 108 of 1
Current timestep = 109. State = [[-0.3115627  -0.11452644]]. Action = [[-0.03155182  0.0298646   0.          0.46682203]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
State prediction error at timestep 109 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 109 of 1
Current timestep = 110. State = [[-0.31338415 -0.10872304]]. Action = [[-0.03467077  0.08860811  0.          0.4380834 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
State prediction error at timestep 110 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 110 of 1
Current timestep = 111. State = [[-0.31595    -0.10156535]]. Action = [[-0.04097113  0.07774673  0.          0.06280005]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
State prediction error at timestep 111 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 111 of 1
Current timestep = 112. State = [[-0.31720015 -0.10261679]]. Action = [[-0.00697774 -0.08859526  0.         -0.84233576]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
State prediction error at timestep 112 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 112 of 1
Current timestep = 113. State = [[-0.31334332 -0.1082387 ]]. Action = [[ 0.07223748 -0.07832521  0.          0.7167448 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 113 of -1
Current timestep = 114. State = [[-0.31383297 -0.11014053]]. Action = [[-0.07673667  0.00722762  0.         -0.21984994]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
State prediction error at timestep 114 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 114 of -1
Current timestep = 115. State = [[-0.31332508 -0.10520041]]. Action = [[0.04548503 0.09206379 0.         0.4465518 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
State prediction error at timestep 115 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 115 of 1
Current timestep = 116. State = [[-0.30748898 -0.10589504]]. Action = [[ 0.09664924 -0.08729752  0.          0.3612479 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
State prediction error at timestep 116 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 116 of 1
Current timestep = 117. State = [[-0.30467674 -0.10958846]]. Action = [[-0.01329365 -0.03346136  0.          0.83547664]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
State prediction error at timestep 117 is tensor(2.6072e-05, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 117 of 1
Current timestep = 118. State = [[-0.3045263  -0.10614999]]. Action = [[-0.00327955  0.09281593  0.         -0.69386023]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
State prediction error at timestep 118 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 118 of 1
Current timestep = 119. State = [[-0.3073105  -0.09993028]]. Action = [[-0.06090713  0.07143965  0.         -0.27172697]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.3073461  -0.09909502]]. Action = [[ 0.03571107 -0.03493551  0.          0.11007428]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
State prediction error at timestep 120 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 120 of 1
Current timestep = 121. State = [[-0.3017395  -0.10364829]]. Action = [[ 0.08978654 -0.08170487  0.          0.15770996]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
State prediction error at timestep 121 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 121 of -1
Current timestep = 122. State = [[-0.29773372 -0.10479494]]. Action = [[0.01555564 0.02209461 0.         0.6144712 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
State prediction error at timestep 122 is tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 122 of -1
Current timestep = 123. State = [[-0.3005008 -0.1074553]]. Action = [[-0.0818271  -0.06000307  0.         -0.4624673 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
State prediction error at timestep 123 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 123 of 1
Current timestep = 124. State = [[-0.30007216 -0.10576093]]. Action = [[ 0.04504854  0.0819545   0.         -0.00554162]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.29543462 -0.10005062]]. Action = [[ 0.06433987  0.06365798  0.         -0.79887974]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
State prediction error at timestep 125 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.29040757 -0.09917006]]. Action = [[ 0.05931968 -0.03597105  0.         -0.16542041]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.28731093 -0.09573753]]. Action = [[0.0205666  0.07638823 0.         0.61935663]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
State prediction error at timestep 127 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 127 of 1
Current timestep = 128. State = [[-0.28857106 -0.09331495]]. Action = [[-0.04367074 -0.00618297  0.         -0.7849333 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
State prediction error at timestep 128 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 128 of 1
Current timestep = 129. State = [[-0.28840497 -0.0952127 ]]. Action = [[ 0.02118492 -0.04455311  0.         -0.93633556]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
State prediction error at timestep 129 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 129 of 1
Current timestep = 130. State = [[-0.28804642 -0.09764938]]. Action = [[-0.01417997 -0.02761719  0.          0.88602734]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 130 of 1
Current timestep = 131. State = [[-0.28603697 -0.09784456]]. Action = [[ 0.03609882  0.01176671  0.         -0.9018532 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.28877807 -0.09879344]]. Action = [[-0.09034212 -0.02464034  0.          0.8131392 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 132 of 1
Current timestep = 133. State = [[-0.28847638 -0.0991754 ]]. Action = [[ 0.04689167  0.01037884  0.         -0.5121368 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 133 of 1
Current timestep = 134. State = [[-0.28756902 -0.09427217]]. Action = [[-0.01351334  0.09126056  0.          0.07210112]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.2852361  -0.08764685]]. Action = [[ 0.05367703  0.06836786  0.         -0.14457226]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 135 of 1
Current timestep = 136. State = [[-0.28537804 -0.08827997]]. Action = [[-0.03233483 -0.07535858  0.         -0.90100884]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.2875771 -0.0894018]]. Action = [[-0.03002272  0.01234039  0.         -0.07636738]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 137 of 1
Current timestep = 138. State = [[-0.28616884 -0.09214327]]. Action = [[ 0.04530761 -0.06539903  0.          0.79165864]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 138 of 1
Current timestep = 139. State = [[-0.28045377 -0.09832881]]. Action = [[ 0.08589006 -0.08876788  0.         -0.10374713]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 139 of 1
Current timestep = 140. State = [[-0.27512547 -0.10038754]]. Action = [[ 0.04588928  0.01647405  0.         -0.4951377 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 140 of 1
Current timestep = 141. State = [[-0.27674875 -0.09648669]]. Action = [[-0.07990769  0.08333052  0.         -0.69787633]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
State prediction error at timestep 141 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.27911654 -0.09240467]]. Action = [[-0.00674167  0.03986605  0.         -0.01288456]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 142 of 1
Current timestep = 143. State = [[-0.28157964 -0.09587619]]. Action = [[-0.04310669 -0.0943604   0.          0.6028737 ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
State prediction error at timestep 143 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 143 of 1
Current timestep = 144. State = [[-0.27875745 -0.09552352]]. Action = [[0.08708876 0.06967133 0.         0.1869545 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 144 of 1
Current timestep = 145. State = [[-0.28063098 -0.09628782]]. Action = [[-0.08985406 -0.06168256  0.          0.35279846]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 145 of 1
Current timestep = 146. State = [[-0.28666097 -0.09426236]]. Action = [[-0.07485248  0.0796706   0.          0.6327802 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 146 of 1
Current timestep = 147. State = [[-0.28637055 -0.09596916]]. Action = [[ 0.06744892 -0.0835811   0.          0.03642488]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 147 of 1
Current timestep = 148. State = [[-0.28213713 -0.09762365]]. Action = [[ 0.04326922  0.01484253  0.         -0.7944441 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.27805063 -0.10143398]]. Action = [[ 0.04527994 -0.08013837  0.         -0.9491701 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 149 of -1
Current timestep = 150. State = [[-0.2715434  -0.10010687]]. Action = [[0.0935827  0.07872332 0.         0.35620642]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 150 of -1
Current timestep = 151. State = [[-0.2658105 -0.0984595]]. Action = [[ 0.05148501 -0.01177686  0.         -0.6302147 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, True, False]
State prediction error at timestep 151 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 151 of 1
Current timestep = 152. State = [[-0.26583576 -0.1027723 ]]. Action = [[-0.0481324  -0.07703395  0.          0.16942298]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 152 of 1
Current timestep = 153. State = [[-0.26572996 -0.10084052]]. Action = [[0.01179937 0.09822667 0.         0.7157059 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 153 of 1
Current timestep = 154. State = [[-0.26708487 -0.09574106]]. Action = [[-0.04504065  0.05171651  0.         -0.88031644]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 154 of 1
Current timestep = 155. State = [[-0.26983914 -0.09809837]]. Action = [[-0.0415696  -0.07978872  0.         -0.38108653]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 155 of 1
Current timestep = 156. State = [[-0.27120918 -0.0980624 ]]. Action = [[-0.0136967   0.04886725  0.         -0.30149108]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 156 of 1
Current timestep = 157. State = [[-0.27025965 -0.09442709]]. Action = [[0.02294953 0.0425364  0.         0.5448768 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 157 of 1
Current timestep = 158. State = [[-0.27049878 -0.09178814]]. Action = [[-0.01940226  0.01585382  0.         -0.63582826]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 158 of 1
Current timestep = 159. State = [[-0.27134874 -0.08581608]]. Action = [[-0.00538097  0.0961697   0.         -0.5301799 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 159 of -1
Current timestep = 160. State = [[-0.2723373  -0.08243322]]. Action = [[-0.00965281 -0.00897338  0.          0.24102688]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 160 of -1
Current timestep = 161. State = [[-0.27248695 -0.07928074]]. Action = [[ 0.0096949   0.04394626  0.         -0.7832941 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 161 of 1
Current timestep = 162. State = [[-0.2707673  -0.07614134]]. Action = [[0.03962062 0.0123135  0.         0.07268751]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 162 of 1
Current timestep = 163. State = [[-0.27040073 -0.07524246]]. Action = [[-0.0068947  -0.01531193  0.          0.6039841 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
State prediction error at timestep 163 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 163 of -1
Current timestep = 164. State = [[-0.27495027 -0.07231277]]. Action = [[-0.08656905  0.04932783  0.         -0.19594729]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 164 of 1
Current timestep = 165. State = [[-0.2800664  -0.07228933]]. Action = [[-0.04920878 -0.04352697  0.          0.88411915]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 165 of 1
Current timestep = 166. State = [[-0.28120795 -0.07659049]]. Action = [[ 0.0122908  -0.07377981  0.          0.47215307]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 166 of 1
Current timestep = 167. State = [[-0.28348702 -0.07509875]]. Action = [[-0.04854851  0.0690038   0.          0.3663435 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 167 of -1
Current timestep = 168. State = [[-0.2809485  -0.07727859]]. Action = [[ 0.09668217 -0.09149356  0.          0.5183468 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 168 of -1
Current timestep = 169. State = [[-0.2735071  -0.08061863]]. Action = [[ 0.09474871 -0.0163495   0.         -0.93838924]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 169 of -1
Current timestep = 170. State = [[-0.26654974 -0.08564063]]. Action = [[ 0.0727464  -0.08298872  0.         -0.7936831 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 170 of -1
Current timestep = 171. State = [[-0.26725498 -0.08630603]]. Action = [[-0.09026622  0.0569227   0.         -0.904475  ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 171 of -1
Current timestep = 172. State = [[-0.26611963 -0.08834781]]. Action = [[ 0.07800574 -0.05891358  0.          0.89869237]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 172 of 1
Current timestep = 173. State = [[-0.26551884 -0.09407326]]. Action = [[-0.04104059 -0.06238178  0.          0.8773377 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
State prediction error at timestep 173 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 173 of 1
Current timestep = 174. State = [[-0.26665625 -0.10047213]]. Action = [[-0.01724227 -0.06261572  0.         -0.91492003]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 174 of 1
Current timestep = 175. State = [[-0.2675749  -0.09911003]]. Action = [[-0.0216373  0.0963318  0.        -0.8014697]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.26674777 -0.0971234 ]]. Action = [[ 0.02338678  0.00578117  0.         -0.36643678]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 176 of 1
Current timestep = 177. State = [[-0.26501966 -0.0969457 ]]. Action = [[0.02016026 0.00867768 0.         0.22771168]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 177 of 1
Current timestep = 178. State = [[-0.26878625 -0.09982734]]. Action = [[-0.09231    -0.05208275  0.          0.30941892]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 178 of 1
Current timestep = 179. State = [[-0.27079123 -0.10378733]]. Action = [[ 0.00811893 -0.0338551   0.          0.9460325 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 179 of 1
Current timestep = 180. State = [[-0.269124   -0.10118501]]. Action = [[ 0.03227241  0.08361708  0.         -0.80970323]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 180 of 1
Current timestep = 181. State = [[-0.2647573  -0.09998076]]. Action = [[ 0.07643244 -0.02582701  0.         -0.8227862 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 181 of -1
Current timestep = 182. State = [[-0.2626183  -0.09706515]]. Action = [[0.00384148 0.0665564  0.         0.312052  ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
State prediction error at timestep 182 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 182 of -1
Current timestep = 183. State = [[-0.26623228 -0.09004802]]. Action = [[-0.07090744  0.09670796  0.          0.3101549 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.2732119  -0.08721933]]. Action = [[-0.0920436  -0.01133041  0.          0.73298633]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 184 of 1
Current timestep = 185. State = [[-0.2811153  -0.09130188]]. Action = [[-0.09666248 -0.08705721  0.          0.20015013]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 185 of -1
Current timestep = 186. State = [[-0.28078723 -0.09432027]]. Action = [[ 0.08241773 -0.01686885  0.         -0.16677427]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
State prediction error at timestep 186 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 186 of -1
Current timestep = 187. State = [[-0.2826143  -0.09178728]]. Action = [[-0.08009245  0.05777159  0.          0.15929532]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 187 of -1
Current timestep = 188. State = [[-0.2859947  -0.08518957]]. Action = [[-0.00598507  0.08763304  0.          0.91223633]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 188 of 1
Current timestep = 189. State = [[-0.29023656 -0.07899681]]. Action = [[-0.05333415  0.05171046  0.          0.754694  ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 189 of -1
Current timestep = 190. State = [[-0.29676923 -0.08091354]]. Action = [[-0.07506832 -0.09153908  0.          0.6969607 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 190 of -1
Current timestep = 191. State = [[-0.29850128 -0.07913873]]. Action = [[ 0.04134507  0.08028791  0.         -0.65658486]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
State prediction error at timestep 191 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.298204   -0.07399318]]. Action = [[0.01590343 0.03574302 0.         0.73359156]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 192 of -1
Current timestep = 193. State = [[-0.2997433 -0.0667892]]. Action = [[-0.00992306  0.09490896  0.          0.11542451]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
State prediction error at timestep 193 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 193 of -1
Current timestep = 194. State = [[-0.2993987  -0.06456222]]. Action = [[ 0.04480817 -0.04124708  0.          0.79265547]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
State prediction error at timestep 194 is tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 194 of -1
Current timestep = 195. State = [[-0.30101854 -0.06197916]]. Action = [[-0.03339112  0.0477984   0.         -0.6936269 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 195 of -1
Current timestep = 196. State = [[-0.3007379  -0.06388965]]. Action = [[ 0.04732809 -0.08738262  0.          0.7787447 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
State prediction error at timestep 196 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 196 of -1
Current timestep = 197. State = [[-0.30063456 -0.0664542 ]]. Action = [[-0.01242942 -0.01554184  0.         -0.00865269]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 197 of -1
Current timestep = 198. State = [[-0.30109477 -0.06899175]]. Action = [[ 0.00104177 -0.04459912  0.          0.40861964]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 198 of -1
Current timestep = 199. State = [[-0.3045001  -0.06836246]]. Action = [[-0.07045416  0.04149649  0.         -0.8245863 ]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 199 of -1
Current timestep = 200. State = [[-0.30348343 -0.06535921]]. Action = [[0.06846727 0.03689053 0.         0.80993474]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 200 of -1
Current timestep = 201. State = [[-0.30060634 -0.06025196]]. Action = [[ 0.02641565  0.07230169  0.         -0.77828753]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
State prediction error at timestep 201 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 201 of -1
Current timestep = 202. State = [[-0.2964698  -0.05887195]]. Action = [[ 0.07263721 -0.02740142  0.          0.48441732]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 202 of -1
Current timestep = 203. State = [[-0.29103923 -0.05546181]]. Action = [[ 0.07104088  0.07006698  0.         -0.35550177]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 203 of 1
Current timestep = 204. State = [[-0.29177767 -0.05098841]]. Action = [[-0.05821181  0.03684271  0.          0.1687038 ]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 204 of 1
Current timestep = 205. State = [[-0.2941241 -0.0500746]]. Action = [[-0.01605882 -0.01810001  0.          0.5523343 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 205 of 1
Current timestep = 206. State = [[-0.29901704 -0.04715711]]. Action = [[-0.09201529  0.05577851  0.          0.36056864]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
State prediction error at timestep 206 is tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 206 of 1
Current timestep = 207. State = [[-0.30486032 -0.04710118]]. Action = [[-0.06863649 -0.04431885  0.          0.43843532]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 207 of 1
Current timestep = 208. State = [[-0.31135327 -0.04321133]]. Action = [[-0.08990742  0.08971793  0.         -0.39756095]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 208 of 1
Current timestep = 209. State = [[-0.31281778 -0.04449655]]. Action = [[ 0.03823715 -0.09557988  0.          0.29095387]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
State prediction error at timestep 209 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 209 of -1
Current timestep = 210. State = [[-0.31168562 -0.04531372]]. Action = [[ 0.00575243  0.02589703  0.         -0.92548585]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
State prediction error at timestep 210 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[-0.307519   -0.04891257]]. Action = [[ 0.08294635 -0.09345629  0.         -0.45870054]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
State prediction error at timestep 211 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 211 of -1
Current timestep = 212. State = [[-0.30524707 -0.04973115]]. Action = [[-0.0055597   0.03696852  0.          0.36511242]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[-0.30371138 -0.0475046 ]]. Action = [[0.03139501 0.02635398 0.         0.48049212]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 213 of -1
Current timestep = 214. State = [[-0.30563045 -0.04886706]]. Action = [[-0.05850847 -0.04233081  0.         -0.74430704]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 214 of -1
Current timestep = 215. State = [[-0.30420917 -0.05146825]]. Action = [[ 0.05895936 -0.02194159  0.         -0.7155952 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
State prediction error at timestep 215 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 215 of -1
Current timestep = 216. State = [[-0.2994272  -0.05455938]]. Action = [[ 0.06067229 -0.0404714   0.         -0.01080447]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 216 of -1
Current timestep = 217. State = [[-0.3012661  -0.05315325]]. Action = [[-0.09008802  0.06575634  0.          0.8740629 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 217 of -1
Current timestep = 218. State = [[-0.30625665 -0.04849227]]. Action = [[-0.04577295  0.06339765  0.         -0.12533164]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 218 of 1
Current timestep = 219. State = [[-0.30464822 -0.04440696]]. Action = [[ 0.08439992  0.03712366  0.         -0.51112515]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 219 of 1
Current timestep = 220. State = [[-0.29900876 -0.04706111]]. Action = [[ 0.08419961 -0.08805933  0.         -0.7330439 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 220 of 1
Current timestep = 221. State = [[-0.29722258 -0.05395575]]. Action = [[-0.01438148 -0.08962431  0.         -0.78629476]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, True, False]
State prediction error at timestep 221 is tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 221 of 1
Current timestep = 222. State = [[-0.29360533 -0.05744605]]. Action = [[ 0.0716332  -0.00666876  0.          0.00337398]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 222 of 1
Current timestep = 223. State = [[-0.2912289  -0.05374838]]. Action = [[-3.6554039e-04  9.0147443e-02  0.0000000e+00 -9.7163570e-01]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
State prediction error at timestep 223 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 223 of 1
Current timestep = 224. State = [[-0.28835672 -0.04640269]]. Action = [[ 0.0561763   0.09736194  0.         -0.9598863 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 224 of 1
Current timestep = 225. State = [[-0.28338885 -0.04570418]]. Action = [[ 0.07124772 -0.05240481  0.         -0.7948077 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 225 of 1
Current timestep = 226. State = [[-0.28054485 -0.04293944]]. Action = [[ 0.01038444  0.07712815  0.         -0.36360097]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 226 of 1
Current timestep = 227. State = [[-0.27693933 -0.03541364]]. Action = [[ 0.06450417  0.09618261  0.         -0.7199428 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
State prediction error at timestep 227 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 227 of 1
Current timestep = 228. State = [[-0.27721617 -0.03498429]]. Action = [[-0.04740698 -0.07365988  0.         -0.81821275]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 228 of 1
Current timestep = 229. State = [[-0.27356982 -0.03694902]]. Action = [[ 0.08711792 -0.01346952  0.         -0.28922695]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 229 of 1
Current timestep = 230. State = [[-0.2723415  -0.04010423]]. Action = [[-0.04754117 -0.06312392  0.          0.8840585 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 230 of 1
Current timestep = 231. State = [[-0.27413654 -0.03878419]]. Action = [[-0.03870545  0.05895563  0.          0.21042645]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 231 of 1
Current timestep = 232. State = [[-0.27382374 -0.04058389]]. Action = [[ 0.00713617 -0.07446949  0.         -0.3527683 ]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
State prediction error at timestep 232 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 232 of 1
Current timestep = 233. State = [[-0.2755392  -0.04669798]]. Action = [[-0.06465838 -0.0803386   0.         -0.19355291]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, True, False]
State prediction error at timestep 233 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 233 of 1
Current timestep = 234. State = [[-0.28012675 -0.04852264]]. Action = [[-0.08559676  0.0212881   0.          0.28695905]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 234 of 1
Current timestep = 235. State = [[-0.28003713 -0.04572139]]. Action = [[ 0.03844362  0.05329353  0.         -0.90361834]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 235 of 1
Current timestep = 236. State = [[-0.27752954 -0.04866709]]. Action = [[ 0.02361114 -0.0877182   0.          0.93156624]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, True, False]
State prediction error at timestep 236 is tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 236 of -1
Current timestep = 237. State = [[-0.27530357 -0.04677885]]. Action = [[0.02567721 0.09621308 0.         0.05898678]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 237 of -1
Current timestep = 238. State = [[-0.2754951  -0.04339769]]. Action = [[-0.01814718  0.01443139  0.          0.557227  ]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, True, False]
State prediction error at timestep 238 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 238 of -1
Current timestep = 239. State = [[-0.27375767 -0.04017572]]. Action = [[0.05022646 0.04955613 0.         0.1213491 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
State prediction error at timestep 239 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 239 of -1
Current timestep = 240. State = [[-0.27093446 -0.03760808]]. Action = [[0.03926823 0.01303977 0.         0.3513676 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 240 of -1
Current timestep = 241. State = [[-0.26670492 -0.03834229]]. Action = [[ 0.06989811 -0.03331406  0.          0.60069346]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
State prediction error at timestep 241 is tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 241 of -1
Current timestep = 242. State = [[-0.26854324 -0.03575249]]. Action = [[-0.08426146  0.06576607  0.         -0.45097375]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
State prediction error at timestep 242 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 242 of -1
Current timestep = 243. State = [[-0.27302855 -0.0376724 ]]. Action = [[-0.0379242  -0.08425019  0.         -0.1062218 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, False, True, False]
State prediction error at timestep 243 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 243 of -1
Current timestep = 244. State = [[-0.27761447 -0.03890777]]. Action = [[-0.06492243  0.01748514  0.         -0.7962169 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 244 of 1
Current timestep = 245. State = [[-0.27849784 -0.03910457]]. Action = [[ 0.02761159 -0.01631784  0.         -0.7379159 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, False, True, False]
State prediction error at timestep 245 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 245 of 1
Current timestep = 246. State = [[-0.28060648 -0.03573534]]. Action = [[-0.0479882   0.07213575  0.         -0.06336123]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, False, True, False]
State prediction error at timestep 246 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 246 of 1
Current timestep = 247. State = [[-0.28522205 -0.0286746 ]]. Action = [[-0.05135803  0.09125049  0.         -0.16498506]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
State prediction error at timestep 247 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 247 of 1
Current timestep = 248. State = [[-0.28687638 -0.02492752]]. Action = [[0.02018298 0.00309075 0.         0.37534404]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
State prediction error at timestep 248 is tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 248 of -1
Current timestep = 249. State = [[-0.28776163 -0.02641265]]. Action = [[-0.00722892 -0.05212395  0.         -0.79888237]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
State prediction error at timestep 249 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 249 of -1
Current timestep = 250. State = [[-0.291386   -0.02962479]]. Action = [[-0.05755461 -0.04547388  0.          0.06910968]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, True, False]
State prediction error at timestep 250 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 250 of -1
Current timestep = 251. State = [[-0.28960907 -0.03080308]]. Action = [[ 0.08378149 -0.00164912  0.          0.94027543]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, True, False]
State prediction error at timestep 251 is tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 251 of -1
Current timestep = 252. State = [[-0.28395468 -0.02607287]]. Action = [[ 0.08283425  0.09178179  0.         -0.6240688 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.2848202  -0.01825561]]. Action = [[-0.05487088  0.09538116  0.         -0.86458516]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, True, False]
State prediction error at timestep 253 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 253 of -1
Current timestep = 254. State = [[-0.2889909  -0.01412631]]. Action = [[-0.03626293  0.01094873  0.         -0.7191536 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
State prediction error at timestep 254 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 254 of -1
Current timestep = 255. State = [[-0.28627008 -0.01494425]]. Action = [[ 0.09925874 -0.04273835  0.         -0.9045061 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, True, False]
State prediction error at timestep 255 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 255 of -1
Current timestep = 256. State = [[-0.28750774 -0.01178752]]. Action = [[-0.08122866  0.07465791  0.          0.41971207]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 256 of -1
Current timestep = 257. State = [[-0.29359928 -0.00778143]]. Action = [[-0.06431685  0.02118973  0.         -0.33580863]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 257 of -1
Current timestep = 258. State = [[-0.3002962  -0.00512495]]. Action = [[-0.07955478  0.0189312   0.          0.42581153]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
State prediction error at timestep 258 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 258 of -1
Current timestep = 259. State = [[-0.30328456 -0.00562482]]. Action = [[ 0.00501841 -0.04243054  0.          0.05955946]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
State prediction error at timestep 259 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 259 of -1
Current timestep = 260. State = [[-0.30648726 -0.00916183]]. Action = [[-0.05200034 -0.06229166  0.          0.76338196]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
State prediction error at timestep 260 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 260 of -1
Current timestep = 261. State = [[-0.31002873 -0.01370059]]. Action = [[-0.03439907 -0.06088699  0.          0.7459737 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, True, False]
State prediction error at timestep 261 is tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 261 of -1
Current timestep = 262. State = [[-0.3106619  -0.01786608]]. Action = [[ 0.01497997 -0.04451668  0.          0.6951462 ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, True, False]
State prediction error at timestep 262 is tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 262 of -1
Current timestep = 263. State = [[-0.3061784  -0.01586911]]. Action = [[ 0.09715351  0.07579959  0.         -0.610134  ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 263 of -1
Current timestep = 264. State = [[-0.30575    -0.01014523]]. Action = [[-0.02885418  0.07638586  0.          0.4349419 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, True, False]
State prediction error at timestep 264 is tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 264 of -1
Current timestep = 265. State = [[-0.3090579  -0.00374718]]. Action = [[-0.02790185  0.08076861  0.          0.5127778 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 265 of -1
Current timestep = 266. State = [[-0.3109712   0.00040928]]. Action = [[0.0074109  0.02589349 0.         0.66515803]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 266 of -1
Current timestep = 267. State = [[-0.31429642  0.00357636]]. Action = [[-0.04406511  0.03343882  0.         -0.20577532]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
State prediction error at timestep 267 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 267 of -1
Current timestep = 268. State = [[-0.31631526  0.00047285]]. Action = [[ 0.00532703 -0.09725054  0.          0.34925735]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
State prediction error at timestep 268 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 268 of -1
Current timestep = 269. State = [[-0.32074443 -0.00507332]]. Action = [[-0.08431233 -0.06459836  0.          0.51055455]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[-0.32040673 -0.00399816]]. Action = [[ 0.06814093  0.05976868  0.         -0.56824076]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
State prediction error at timestep 270 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 270 of -1
Current timestep = 271. State = [[-0.32178771  0.00197776]]. Action = [[-0.05509903  0.0812475   0.          0.19678843]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
State prediction error at timestep 271 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 271 of -1
Current timestep = 272. State = [[-0.32536173  0.0028517 ]]. Action = [[-0.03098725 -0.04181529  0.          0.63591635]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
State prediction error at timestep 272 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 272 of -1
Current timestep = 273. State = [[-0.32531786 -0.00121794]]. Action = [[ 0.02741862 -0.06649891  0.          0.95377505]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
State prediction error at timestep 273 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 273 of -1
Current timestep = 274. State = [[-0.3224368  -0.00357109]]. Action = [[ 0.04861357 -0.00687455  0.         -0.24489439]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 274 of -1
Current timestep = 275. State = [[-0.32146016 -0.00890442]]. Action = [[-0.00939339 -0.09601175  0.         -0.2734335 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 275 of -1
Current timestep = 276. State = [[-0.31893107 -0.01155408]]. Action = [[0.04883445 0.01578705 0.         0.7146859 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 276 of -1
Current timestep = 277. State = [[-0.31880593 -0.00734561]]. Action = [[-0.02768268  0.09166744  0.          0.4504683 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, True, False]
State prediction error at timestep 277 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 277 of -1
Current timestep = 278. State = [[-3.2413906e-01 -1.6043489e-04]]. Action = [[-0.08899416  0.09664995  0.         -0.63185215]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 278 of -1
Current timestep = 279. State = [[-0.3232532  -0.00093721]]. Action = [[ 0.08975215 -0.07954803  0.         -0.83220524]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, True, False]
State prediction error at timestep 279 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 279 of -1
Current timestep = 280. State = [[-0.32153386 -0.00574942]]. Action = [[-0.0188795  -0.05207217  0.          0.12575257]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, True, False]
State prediction error at timestep 280 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 280 of -1
Current timestep = 281. State = [[-0.32500893 -0.00427873]]. Action = [[-0.07420234  0.06584441  0.         -0.6009204 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 281 of -1
Current timestep = 282. State = [[-0.3272444  -0.00082633]]. Action = [[-0.00545245  0.02962139  0.          0.59899986]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, True, False]
State prediction error at timestep 282 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 282 of -1
Current timestep = 283. State = [[-0.3297819   0.00452519]]. Action = [[-0.03979728  0.07893377  0.         -0.9316601 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 283 of -1
Current timestep = 284. State = [[-0.33543763  0.00645939]]. Action = [[-0.08090468 -0.02353352  0.         -0.7464962 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 284 of -1
Current timestep = 285. State = [[-0.3340937   0.00274369]]. Action = [[ 0.09291246 -0.07656635  0.          0.8984494 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, True, False]
State prediction error at timestep 285 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 285 of -1
Current timestep = 286. State = [[-0.33576024  0.00509494]]. Action = [[-0.0875188   0.08711063  0.          0.8986018 ]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, True, False]
State prediction error at timestep 286 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 286 of -1
Current timestep = 287. State = [[-0.3427566   0.01082881]]. Action = [[-0.0812508   0.05372632  0.          0.12924552]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 287 of -1
Current timestep = 288. State = [[-0.3438916   0.01674399]]. Action = [[ 0.05700655  0.06533811  0.         -0.46978456]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
State prediction error at timestep 288 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 288 of -1
Current timestep = 289. State = [[-0.34115124  0.01555524]]. Action = [[ 0.05289643 -0.0855428   0.          0.5345472 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
State prediction error at timestep 289 is tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 289 of -1
Current timestep = 290. State = [[-0.34391344  0.01296637]]. Action = [[-0.08134256 -0.01587193  0.         -0.2924527 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 290 of -1
Current timestep = 291. State = [[-0.3497378   0.00991155]]. Action = [[-0.06622943 -0.05722652  0.         -0.21828288]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 291 of -1
Current timestep = 292. State = [[-0.35336098  0.00875733]]. Action = [[-0.02450339  0.00853693  0.          0.30711842]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 292 of -1
Current timestep = 293. State = [[-0.35623786  0.0055096 ]]. Action = [[-0.03022771 -0.06693576  0.         -0.8192585 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 293 of -1
Current timestep = 294. State = [[-0.3533625  -0.00042318]]. Action = [[ 0.08984869 -0.07038824  0.          0.5600405 ]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, True, False]
State prediction error at timestep 294 is tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 294 of -1
Current timestep = 295. State = [[-0.35261527 -0.00064485]]. Action = [[-0.02823182  0.05599163  0.         -0.91538274]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, True, False]
State prediction error at timestep 295 is tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 295 of -1
Current timestep = 296. State = [[-0.34987903 -0.00437554]]. Action = [[ 0.07884523 -0.09017762  0.          0.8153887 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, True, False]
State prediction error at timestep 296 is tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 296 of -1
Current timestep = 297. State = [[-0.34914124 -0.00368372]]. Action = [[-0.02400875  0.08677285  0.          0.601545  ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 297 of -1
Current timestep = 298. State = [[-0.34837765  0.00197488]]. Action = [[0.04158086 0.07714079 0.         0.37070656]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 298 of -1
Current timestep = 299. State = [[-0.3481502   0.00068425]]. Action = [[-0.00292517 -0.07212862  0.         -0.90727186]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 299 of -1
Current timestep = 300. State = [[-0.35302025  0.00182496]]. Action = [[-0.09298342  0.06932104  0.         -0.70641625]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, True, False]
State prediction error at timestep 300 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 300 of -1
Current timestep = 301. State = [[-0.35291728  0.00115246]]. Action = [[ 0.07354397 -0.05083564  0.          0.4670273 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, True, False]
State prediction error at timestep 301 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 301 of -1
Current timestep = 302. State = [[-0.35512146  0.00181046]]. Action = [[-0.08841876  0.04252965  0.         -0.32027638]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 302 of -1
Current timestep = 303. State = [[-0.357094    0.00526362]]. Action = [[0.01933002 0.04207849 0.         0.5347359 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, True, False]
State prediction error at timestep 303 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 303 of -1
Current timestep = 304. State = [[-0.35908842  0.00546688]]. Action = [[-0.03591521 -0.03018428  0.         -0.7008655 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, True, False]
State prediction error at timestep 304 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 304 of -1
Current timestep = 305. State = [[-0.36243176  0.00985752]]. Action = [[-0.03453894  0.09623002  0.          0.33854938]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 305 of -1
Current timestep = 306. State = [[-0.36270303  0.01224878]]. Action = [[ 0.03671671 -0.0225505   0.          0.6604991 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, True, False]
State prediction error at timestep 306 is tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 306 of -1
Current timestep = 307. State = [[-0.3580357   0.01503967]]. Action = [[ 0.09845293  0.05133862  0.         -0.2734574 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, True, False]
State prediction error at timestep 307 is tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 307 of -1
Current timestep = 308. State = [[-0.35781938  0.01333328]]. Action = [[-0.04202768 -0.07847668  0.         -0.6976952 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, True, False]
State prediction error at timestep 308 is tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 308 of -1
Current timestep = 309. State = [[-0.36305198  0.01063943]]. Action = [[-0.07959428 -0.01825003  0.          0.9855552 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, True, False]
State prediction error at timestep 309 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 309 of -1
Current timestep = 310. State = [[-0.36331138  0.0141324 ]]. Action = [[0.05604873 0.0776066  0.         0.19224977]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, True, False]
State prediction error at timestep 310 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 310 of -1
Current timestep = 311. State = [[-0.36356276  0.0170656 ]]. Action = [[-0.02266888  0.00502641  0.          0.8219323 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 311 of -1
Current timestep = 312. State = [[-0.36196727  0.01635008]]. Action = [[ 0.04994132 -0.02812354  0.         -0.989914  ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 312 of -1
Current timestep = 313. State = [[-0.36145195  0.01819392]]. Action = [[-0.01447995  0.04788982  0.         -0.9662788 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 313 of -1
Current timestep = 314. State = [[-0.35793364  0.02325628]]. Action = [[0.08839899 0.06655917 0.         0.78035223]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
State prediction error at timestep 314 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 314 of -1
Current timestep = 315. State = [[-0.3518325   0.02779481]]. Action = [[ 0.08599617  0.04039908  0.         -0.8543    ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 315 of -1
Current timestep = 316. State = [[-0.34981832  0.03419094]]. Action = [[-0.003038    0.09058631  0.          0.77210927]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
State prediction error at timestep 316 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 316 of -1
Current timestep = 317. State = [[-0.35382     0.03955684]]. Action = [[-0.07747789  0.03485752  0.          0.8418107 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 317 of -1
Current timestep = 318. State = [[-0.35363677  0.04661651]]. Action = [[ 0.05935491  0.09274103  0.         -0.54865074]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 318 of 1
Current timestep = 319. State = [[-0.35098013  0.04641113]]. Action = [[ 0.02343391 -0.09594692  0.         -0.60308564]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, True, False]
State prediction error at timestep 319 is tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 319 of -1
Current timestep = 320. State = [[-0.3500531   0.04157609]]. Action = [[-0.01569535 -0.07015387  0.         -0.6256783 ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, True, False]
State prediction error at timestep 320 is tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 320 of -1
Current timestep = 321. State = [[-0.34675977  0.04172295]]. Action = [[ 0.04974813  0.02875846  0.         -0.40315008]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, True, False]
State prediction error at timestep 321 is tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 321 of -1
Current timestep = 322. State = [[-0.34763616  0.04766376]]. Action = [[-0.06609322  0.0885082   0.          0.4122181 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, True, False]
State prediction error at timestep 322 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 322 of -1
Current timestep = 323. State = [[-0.35118532  0.05604696]]. Action = [[-0.04272341  0.09439588  0.         -0.31979775]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, True, False]
State prediction error at timestep 323 is tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 323 of -1
Current timestep = 324. State = [[-0.34819576  0.05800686]]. Action = [[ 0.09062213 -0.04366215  0.          0.08268189]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 324 of -1
Current timestep = 325. State = [[-0.34673434  0.06249334]]. Action = [[-0.02108833  0.09448273  0.          0.01850498]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, True, False]
State prediction error at timestep 325 is tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 325 of -1
Current timestep = 326. State = [[-0.34957427  0.0629871 ]]. Action = [[-0.05052559 -0.07041279  0.         -0.75685537]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 326 of -1
Current timestep = 327. State = [[-0.35329917  0.06566318]]. Action = [[-0.0531754   0.0673861   0.          0.71995497]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 327 of -1
Current timestep = 328. State = [[-0.35455266  0.06799971]]. Action = [[ 0.00871353 -0.01631574  0.         -0.8599325 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, True, False]
State prediction error at timestep 328 is tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 328 of -1
Current timestep = 329. State = [[-0.35125935  0.06870743]]. Action = [[0.06709572 0.00215627 0.         0.21640599]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, True, False]
State prediction error at timestep 329 is tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 329 of -1
Current timestep = 330. State = [[-0.35372004  0.0708595 ]]. Action = [[-0.09090638  0.0260736   0.         -0.9486895 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, True, False]
State prediction error at timestep 330 is tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 330 of -1
Current timestep = 331. State = [[-0.35901105  0.07327271]]. Action = [[-0.04983745  0.01457267  0.          0.6536796 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 331 of -1
Current timestep = 332. State = [[-0.36051998  0.07935894]]. Action = [[ 0.01982011  0.09616106  0.         -0.07609683]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, True, False]
State prediction error at timestep 332 is tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 332 of -1
Current timestep = 333. State = [[-0.36145005  0.08543041]]. Action = [[ 0.00136937  0.04993898  0.         -0.96856827]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 333 of -1
Current timestep = 334. State = [[-0.36491537  0.09012841]]. Action = [[-0.04278969  0.04372393  0.         -0.64508766]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, True, False]
State prediction error at timestep 334 is tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 334 of -1
Current timestep = 335. State = [[-0.3706317  0.0942606]]. Action = [[-0.06334887  0.03213745  0.         -0.7266501 ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, True, False]
State prediction error at timestep 335 is tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 335 of -1
Current timestep = 336. State = [[-0.37253568  0.09611597]]. Action = [[ 0.02756424 -0.00817129  0.         -0.03635341]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0080, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 336 of -1
Current timestep = 337. State = [[-0.3778319   0.10147195]]. Action = [[-0.09305387  0.08942359  0.         -0.42890906]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, True, False]
State prediction error at timestep 337 is tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 337 of -1
Current timestep = 338. State = [[-0.3827778   0.10952552]]. Action = [[-0.00975132  0.08323961  0.         -0.6368975 ]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, True, False]
State prediction error at timestep 338 is tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 338 of -1
Current timestep = 339. State = [[-0.3852164   0.11337002]]. Action = [[ 0.          0.          0.         -0.89038014]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, True, False]
State prediction error at timestep 339 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 339 of -1
Current timestep = 340. State = [[-0.3868044   0.11477323]]. Action = [[ 0.         0.         0.        -0.6263695]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, True, False]
State prediction error at timestep 340 is tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 340 of -1
Current timestep = 341. State = [[-0.38803658  0.11595452]]. Action = [[ 0.          0.          0.         -0.73649526]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 341 of -1
Current timestep = 342. State = [[-0.38899803  0.11700187]]. Action = [[ 0.         0.         0.        -0.3727262]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0076, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 342 of -1
Current timestep = 343. State = [[-0.38971114  0.1178988 ]]. Action = [[ 0.         0.         0.        -0.5570739]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 343 of -1
Current timestep = 344. State = [[-0.39020273  0.11863448]]. Action = [[ 0.         0.         0.        -0.8065036]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)
Human Feedback received at timestep 344 of -1
Current timestep = 345. State = [[-0.39050794  0.11920642]]. Action = [[ 0.          0.          0.         -0.69840014]]. Reward = [0.]
