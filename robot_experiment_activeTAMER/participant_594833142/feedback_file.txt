Current timestep = 0. State = [[-0.22493333  0.0965227 ]]. Action = [[ 0.06134588 -0.07926404  0.10084191  0.6906066 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0393, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 0 of 1
Current timestep = 1. State = [[-0.21931298  0.0929539 ]]. Action = [[-0.23216607  0.1630454  -0.17375576 -0.90656227]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0271, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.18775477  0.04351944]]. Action = [[ 0.11017054 -0.17409065  0.23838907 -0.82412636]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2 of 1
Current timestep = 3. State = [[-0.208733   -0.03313444]]. Action = [[-0.15420695 -0.21680537 -0.08960131 -0.9430443 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3 of 0
Current timestep = 4. State = [[-0.21639831 -0.04883398]]. Action = [[-0.22281727 -0.08053747 -0.01463331 -0.96832937]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
State prediction error at timestep 4 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.21639831 -0.04883398]]. Action = [[-0.15505303  0.2238228  -0.17885415 -0.8224647 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.21639831 -0.04883398]]. Action = [[-0.22757803 -0.23565508 -0.01582104 -0.9437242 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
State prediction error at timestep 6 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.15772685 -0.0968092 ]]. Action = [[ 0.24831915 -0.16037396  0.10457081 -0.4514178 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0075, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of 1
Current timestep = 8. State = [[-0.16117255 -0.1152983 ]]. Action = [[-0.11798003 -0.01211624  0.22982892  0.6865344 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0106, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.14892147 -0.05354374]]. Action = [[0.11237454 0.2198531  0.17929465 0.3901825 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9 of 1
Current timestep = 10. State = [[-0.16297325 -0.07161575]]. Action = [[-0.11049998 -0.11847945 -0.12446442 -0.8657481 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0091, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 10 of 0
Current timestep = 11. State = [[-0.15501054 -0.07511062]]. Action = [[ 0.09405395  0.00577033 -0.18669635 -0.27027392]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 11 of 1
Current timestep = 12. State = [[-0.11458226 -0.09050938]]. Action = [[ 0.12825158 -0.05719917  0.2435717  -0.15462005]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0093, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 12 of 1
Current timestep = 13. State = [[-0.13402522 -0.12216851]]. Action = [[-0.15447266 -0.08326304 -0.11009926 -0.10979438]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.19461632 -0.15864924]]. Action = [[-0.18977492 -0.09560747  0.19800317 -0.15428549]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, True, False, False]
State prediction error at timestep 14 is tensor(0.0146, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 14 of -1
Current timestep = 15. State = [[-0.207619   -0.16593482]]. Action = [[-0.22764096 -0.1526222  -0.22775222 -0.09182918]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, True, False, False]
State prediction error at timestep 15 is tensor(0.0144, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 15 of -1
Current timestep = 16. State = [[-0.20582762 -0.15834247]]. Action = [[ 0.03006789  0.04054186 -0.15724765 -0.6884109 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, True, False, False]
State prediction error at timestep 16 is tensor(0.0115, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of 0
Current timestep = 17. State = [[-0.16187643 -0.11023536]]. Action = [[ 0.17267895  0.14687526  0.2441678  -0.94915557]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0127, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 17 of 1
Current timestep = 18. State = [[-0.12738241 -0.11891145]]. Action = [[ 0.09575361 -0.08380118  0.1863302   0.5653815 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 18 of 1
Current timestep = 19. State = [[-0.06779119 -0.13791564]]. Action = [[ 0.19435668 -0.05329061 -0.21196587  0.7903708 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, True, False, False]
State prediction error at timestep 19 is tensor(0.0100, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 19 of 1
Current timestep = 20. State = [[ 0.00247953 -0.1500625 ]]. Action = [[ 0.19633132 -0.03245662 -0.19965848  0.0006572 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [False, True, False, True, False, False]
State prediction error at timestep 20 is tensor(0.0139, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 20 of 1
Current timestep = 21. State = [[ 0.02150851 -0.15346645]]. Action = [[0.17403358 0.05584824 0.24018055 0.70043993]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [False, True, False, True, False, False]
State prediction error at timestep 21 is tensor(0.0223, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 21 of -1
Current timestep = 22. State = [[ 0.00381354 -0.21862122]]. Action = [[-0.09718049 -0.21044712  0.09993795  0.84601796]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [False, True, False, True, False, False]
State prediction error at timestep 22 is tensor(0.0286, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[ 0.00449758 -0.22694843]]. Action = [[ 0.03097561  0.03383011 -0.02185625 -0.89851683]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [False, True, False, True, False, False]
State prediction error at timestep 23 is tensor(0.0350, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 23 of 0
Current timestep = 24. State = [[-0.02346988 -0.26966658]]. Action = [[-0.1410285  -0.1494088  -0.00856003  0.40250742]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [False, True, False, True, False, False]
State prediction error at timestep 24 is tensor(0.0265, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 24 of -1
Current timestep = 25. State = [[-0.02923359 -0.27885422]]. Action = [[-0.10570128 -0.18556133  0.16502428  0.67771935]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [False, True, False, True, False, False]
State prediction error at timestep 25 is tensor(0.0275, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 25 of -1
Current timestep = 26. State = [[-0.03004492 -0.27908438]]. Action = [[-0.1775655  -0.18098505 -0.02509025 -0.25667977]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [False, True, False, True, False, False]
State prediction error at timestep 26 is tensor(0.0229, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 26 of -1
Current timestep = 27. State = [[-0.03004492 -0.27908438]]. Action = [[-0.10276946 -0.14445814 -0.16724144  0.699682  ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [False, True, False, True, False, False]
State prediction error at timestep 27 is tensor(0.0237, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 27 of -1
Current timestep = 28. State = [[-0.0796783 -0.2668202]]. Action = [[-0.18655114  0.06718811  0.10616875  0.96593857]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, True, False, False]
State prediction error at timestep 28 is tensor(0.0156, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 28 of -1
Current timestep = 29. State = [[-0.10049473 -0.28269556]]. Action = [[-0.00564145 -0.08053803 -0.19518867 -0.14720005]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, True, False, False]
State prediction error at timestep 29 is tensor(0.0185, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 29 of -1
Current timestep = 30. State = [[-0.13500512 -0.28883737]]. Action = [[-0.12305398  0.01111448  0.20530027  0.47672582]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, True, False, False]
State prediction error at timestep 30 is tensor(0.0180, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 30 of -1
Current timestep = 31. State = [[-0.1455327  -0.28887329]]. Action = [[ 0.01816285 -0.17077935 -0.09656353 -0.96199274]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, True, False, False]
State prediction error at timestep 31 is tensor(0.0160, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 31 of -1
Current timestep = 32. State = [[-0.11211472 -0.23071799]]. Action = [[0.15490192 0.19541934 0.20638517 0.68358135]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, True, False, False]
State prediction error at timestep 32 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 32 of 1
Current timestep = 33. State = [[-0.11761156 -0.26246727]]. Action = [[-0.04788721 -0.17699794 -0.06536412 -0.4737404 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, True, False, False]
State prediction error at timestep 33 is tensor(0.0119, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 33 of -1
Current timestep = 34. State = [[-0.11943314 -0.27149957]]. Action = [[-0.03553218 -0.19219327  0.02343458 -0.12390912]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, True, False, False]
State prediction error at timestep 34 is tensor(0.0122, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.09702109 -0.2173353 ]]. Action = [[0.08676693 0.20096612 0.1881429  0.6589867 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, True, False, False]
State prediction error at timestep 35 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.04586338 -0.24760932]]. Action = [[ 0.19345516 -0.17314729  0.03575677 -0.7656398 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [False, True, False, True, False, False]
State prediction error at timestep 36 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 36 of 1
Current timestep = 37. State = [[-0.03025245 -0.25692865]]. Action = [[ 0.23402679  0.16150153 -0.09355158 -0.38536036]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [False, True, False, True, False, False]
State prediction error at timestep 37 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 37 of -1
Current timestep = 38. State = [[-0.03028241 -0.2569779 ]]. Action = [[-0.01908645 -0.2196579  -0.10898338 -0.6569835 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [False, True, False, True, False, False]
State prediction error at timestep 38 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 38 of -1
Current timestep = 39. State = [[ 0.01466355 -0.26260567]]. Action = [[ 0.15533996 -0.02328518 -0.0898028   0.34660673]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [False, True, False, True, False, False]
State prediction error at timestep 39 is tensor(0.0098, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 39 of 1
Current timestep = 40. State = [[ 0.03154538 -0.26585078]]. Action = [[ 0.11433071 -0.17853522 -0.11570282  0.5185516 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [False, True, False, True, False, False]
State prediction error at timestep 40 is tensor(0.0127, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[ 0.03219087 -0.26568955]]. Action = [[-0.01661068 -0.15194437  0.2461114  -0.1771493 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [False, True, False, True, False, False]
State prediction error at timestep 41 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of -1
Current timestep = 42. State = [[ 0.01713119 -0.28045374]]. Action = [[-0.10574478 -0.02042186 -0.09066461 -0.61983776]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [False, True, False, True, False, False]
State prediction error at timestep 42 is tensor(0.0089, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of 0
Current timestep = 43. State = [[ 0.01451722 -0.28354388]]. Action = [[ 0.12660909  0.14954108 -0.14898059 -0.75661725]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [False, True, False, True, False, False]
State prediction error at timestep 43 is tensor(0.0109, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of 0
Current timestep = 44. State = [[ 0.02281054 -0.2313355 ]]. Action = [[ 0.03235063  0.18093115  0.05978253 -0.8430263 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 45. State = [[-0.00540529 -0.20633714]]. Action = [[-0.12299     0.04334655 -0.24229336 -0.4466105 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 46. State = [[-0.04732058 -0.16359523]]. Action = [[-0.13049379  0.13540548  0.15772474  0.28529203]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 47. State = [[-0.09056252 -0.14420968]]. Action = [[-0.10347474  0.02873683 -0.06211221  0.7110392 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, True, False, False]
State prediction error at timestep 47 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 47 of 0
Current timestep = 48. State = [[-0.07022837 -0.1783751 ]]. Action = [[ 0.17632532 -0.14440018 -0.19464953  0.6248263 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, True, False, False]
State prediction error at timestep 48 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 48 of 0
Current timestep = 49. State = [[-0.0992614  -0.20460792]]. Action = [[-0.18290527 -0.05078995 -0.23950353 -0.69762486]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 50. State = [[-0.11111579 -0.21353279]]. Action = [[-3.7647784e-04 -2.1107033e-02  1.7838937e-01 -5.8348227e-01]]. Reward = [0.]
Curr episode timestep = 50
