Current timestep = 0. State = [[-0.20544364  0.22767018]]. Action = [[ 0.06298006 -0.07997689  0.10207769  0.69315684]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Current timestep = 1. State = [[-0.21070525  0.23488313]]. Action = [[-0.23293044  0.16461542 -0.17287695 -0.9050577 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, False, True]
Scene graph at timestep 1 is [True, False, False, False, False, True]
State prediction error at timestep 1 is tensor(0.0593, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.21931489  0.23817222]]. Action = [[ 0.11270717 -0.1756288   0.2386589  -0.82141685]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, False, True]
Current timestep = 3. State = [[-0.214881    0.21670817]]. Action = [[-0.15693314 -0.21787259 -0.08750707 -0.9405355 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, False, True]
Scene graph at timestep 3 is [True, False, False, False, False, True]
State prediction error at timestep 3 is tensor(0.0292, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3 of -1
Current timestep = 4. State = [[-0.22778185  0.19567116]]. Action = [[-0.22495103 -0.08034179 -0.01222239 -0.966055  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, False, True]
Current timestep = 5. State = [[-0.25375122  0.20350295]]. Action = [[-0.15813974  0.2256642  -0.17807521 -0.8144805 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, False, True]
Current timestep = 6. State = [[-0.26992053  0.21845555]]. Action = [[-0.2298123  -0.23626721 -0.01328072 -0.94046175]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, False, True]
Current timestep = 7. State = [[-0.26399237  0.20836155]]. Action = [[ 0.24866396 -0.1608114   0.10771695 -0.43774474]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, False, True]
Scene graph at timestep 7 is [True, False, False, False, False, True]
State prediction error at timestep 7 is tensor(0.0183, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.25581548  0.19562167]]. Action = [[-0.12279217 -0.00689156  0.23059553  0.6920903 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, False, True]
Current timestep = 9. State = [[-0.257553    0.20640472]]. Action = [[0.11636674 0.22321227 0.18116283 0.40323162]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, False, True]
Scene graph at timestep 9 is [True, False, False, False, False, True]
State prediction error at timestep 9 is tensor(0.0231, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9 of -1
Current timestep = 10. State = [[-0.25479752  0.21575741]]. Action = [[-0.1154694  -0.11623606 -0.12387317 -0.8621801 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, False, True]
Scene graph at timestep 10 is [True, False, False, False, False, True]
State prediction error at timestep 10 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 10 of 0
Current timestep = 11. State = [[-0.25435868  0.21286362]]. Action = [[ 0.09638286  0.01264125 -0.18690915 -0.25733042]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, False, True]
Current timestep = 12. State = [[-0.24842073  0.20792651]]. Action = [[ 0.1310767  -0.05118793  0.2440641  -0.14146996]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, False, True]
Scene graph at timestep 12 is [True, False, False, False, False, True]
State prediction error at timestep 12 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 12 of 0
Current timestep = 13. State = [[-0.24455088  0.1989428 ]]. Action = [[-0.1603557  -0.07648796 -0.11004987 -0.09481072]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, False, True]
Scene graph at timestep 13 is [True, False, False, False, False, True]
State prediction error at timestep 13 is tensor(0.0080, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of 0
Current timestep = 14. State = [[-0.24563326  0.19705634]]. Action = [[-0.19479199 -0.08877537  0.20044345 -0.14128089]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, False, True]
Current timestep = 15. State = [[-0.24563326  0.19705634]]. Action = [[-0.23047186 -0.15032762 -0.22885284 -0.08089387]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, False, True]
Current timestep = 16. State = [[-0.24613789  0.19780166]]. Action = [[ 0.02999252  0.05349681 -0.1588179  -0.68972623]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, False, True]
Scene graph at timestep 16 is [True, False, False, False, False, True]
State prediction error at timestep 16 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of 0
Current timestep = 17. State = [[-0.24274014  0.20569152]]. Action = [[ 0.17603567  0.15919209  0.24490565 -0.95140314]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, False, True]
Current timestep = 18. State = [[-0.23104405  0.20968679]]. Action = [[ 0.09610668 -0.07587934  0.18972355  0.5797541 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, False, True]
Current timestep = 19. State = [[-0.21365038  0.20670906]]. Action = [[ 0.19606787 -0.04243873 -0.21385425  0.8023629 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, False, True]
Scene graph at timestep 19 is [True, False, False, False, False, True]
State prediction error at timestep 19 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 19 of 0
Current timestep = 20. State = [[-0.18479297  0.20297217]]. Action = [[ 0.19724026 -0.01849172 -0.20196898  0.00794077]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, False, True]
Current timestep = 21. State = [[-0.16314769  0.20610784]]. Action = [[0.17309937 0.07454959 0.24124959 0.71458054]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, False, True]
Current timestep = 22. State = [[-0.14813526  0.19969839]]. Action = [[-0.10591936 -0.20865935  0.10234773  0.8570008 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, False, True]
Current timestep = 23. State = [[-0.14656839  0.19050707]]. Action = [[ 0.0236184   0.054088   -0.02381791 -0.90870166]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, False, True]
Scene graph at timestep 23 is [True, False, False, False, False, True]
State prediction error at timestep 23 is tensor(0.0113, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 23 of 1
Current timestep = 24. State = [[-0.14853998  0.18411928]]. Action = [[-0.14981416 -0.1428969  -0.01059799  0.4098177 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, False, True]
Current timestep = 25. State = [[-0.15593517  0.16596167]]. Action = [[-0.11603042 -0.18355757  0.16779196  0.68951535]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, False, True]
Current timestep = 26. State = [[-0.17113775  0.14072977]]. Action = [[-0.18525013 -0.17871812 -0.02926572 -0.28003806]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, False, True]
Scene graph at timestep 26 is [True, False, False, False, False, True]
State prediction error at timestep 26 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 26 of 0
Current timestep = 27. State = [[-0.19774927  0.11354242]]. Action = [[-0.11474834 -0.13859014 -0.17362525  0.7084304 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, False, True]
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 27 of 0
Current timestep = 28. State = [[-0.2187962   0.10625109]]. Action = [[-0.19396244  0.08569664  0.10624418  0.9697689 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
Current timestep = 29. State = [[-0.23013365  0.10820063]]. Action = [[-0.02034467 -0.06946686 -0.20082846 -0.17575264]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
Current timestep = 30. State = [[-0.24305311  0.10608391]]. Action = [[-0.13559453  0.02755362  0.20705435  0.47390807]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
Current timestep = 31. State = [[-0.25451708  0.09608129]]. Action = [[ 0.00203732 -0.16809161 -0.1055235  -0.96929365]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
Current timestep = 32. State = [[-0.25625616  0.09413663]]. Action = [[0.14456224 0.2052837  0.20741117 0.686558  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
Current timestep = 33. State = [[-0.25444707  0.0927343 ]]. Action = [[-0.06632012 -0.1732526  -0.07464842 -0.50694746]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
Current timestep = 34. State = [[-0.2551096   0.07614699]]. Action = [[-0.05599105 -0.1897054   0.01553297 -0.15672374]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
Current timestep = 35. State = [[-0.25751695  0.07195001]]. Action = [[0.06641367 0.20850176 0.18706599 0.65630364]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
Current timestep = 36. State = [[-0.24939023  0.06973076]]. Action = [[ 0.18255591 -0.16770585  0.02689272 -0.78758323]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
Current timestep = 37. State = [[-0.22951648  0.07068012]]. Action = [[ 0.23006767  0.17086542 -0.10310668 -0.4201734 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
Current timestep = 38. State = [[-0.21362166  0.06647149]]. Action = [[-0.04689761 -0.21705958 -0.11822814 -0.68552846]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
Current timestep = 39. State = [[-0.20585647  0.05490024]]. Action = [[ 0.13499251 -0.0102984  -0.09957689  0.3258083 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
Current timestep = 40. State = [[-0.19358347  0.04160522]]. Action = [[ 0.08688635 -0.1715784  -0.12429202  0.504326  ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
Current timestep = 41. State = [[-0.1854119   0.01798201]]. Action = [[-0.04971726 -0.14303064  0.24594751 -0.20927233]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.18555981  0.00482189]]. Action = [[-0.13215968 -0.00907619 -0.09974131 -0.64377517]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0079, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of 0
Current timestep = 43. State = [[-0.19365425  0.0103214 ]]. Action = [[ 0.09678155  0.15440956 -0.15589656 -0.77390295]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
Current timestep = 44. State = [[-0.19510712  0.02845222]]. Action = [[-0.00564405  0.18359071  0.04985759 -0.85465634]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0068, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 44 of -1
Current timestep = 45. State = [[-0.19792767  0.05002273]]. Action = [[-0.1493278   0.05011868 -0.24279992 -0.46642983]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 45 of -1
Current timestep = 46. State = [[-0.20865715  0.06574541]]. Action = [[-0.15619454  0.1373148   0.14930129  0.26986885]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of -1
Current timestep = 47. State = [[-0.23093866  0.08040713]]. Action = [[-0.13338077  0.0325326  -0.07354382  0.70490646]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
Current timestep = 48. State = [[-0.23352396  0.07463335]]. Action = [[ 0.151957   -0.13889077 -0.19836733  0.61885333]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
Current timestep = 49. State = [[-0.2373267   0.06452084]]. Action = [[-0.1994327  -0.04691368 -0.23999457 -0.7032667 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
Current timestep = 50. State = [[-0.24308537  0.05748724]]. Action = [[-0.043751   -0.01991907  0.1697284  -0.5909135 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
Current timestep = 51. State = [[-0.2487775   0.05517098]]. Action = [[-0.2328875  -0.1937939   0.08922026  0.53062034]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 51 of -1
Current timestep = 52. State = [[-0.24557076  0.04018136]]. Action = [[ 0.16095823 -0.21278088 -0.2226738  -0.26404607]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
Current timestep = 53. State = [[-0.23639518  0.03452649]]. Action = [[0.15589172 0.1384995  0.19040245 0.8058343 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
Current timestep = 54. State = [[-0.22565602  0.02592431]]. Action = [[ 0.06832328 -0.21492694  0.14398223  0.00410354]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
Current timestep = 55. State = [[-0.22216186  0.02585814]]. Action = [[-0.06393696  0.23610908  0.00933671  0.05452907]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
Current timestep = 56. State = [[-0.21372783  0.04542965]]. Action = [[0.23093712 0.14305848 0.2263636  0.6044924 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of 1
Current timestep = 57. State = [[-0.19439882  0.0540486 ]]. Action = [[-0.0505368  -0.12807545  0.08511442 -0.8989197 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
Current timestep = 58. State = [[-0.18926956  0.03789401]]. Action = [[ 0.12024072 -0.18895426 -0.16815005  0.9237089 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 58 of 1
Current timestep = 59. State = [[-0.18299296  0.01946989]]. Action = [[ 0.02299044 -0.03615719 -0.02487218  0.91577315]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
Current timestep = 60. State = [[-0.17218809  0.02030876]]. Action = [[0.21321332 0.06374651 0.01040587 0.3251443 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
Current timestep = 61. State = [[-0.16101907  0.01682754]]. Action = [[-0.16388814 -0.12742293  0.08758947 -0.04686522]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
Current timestep = 62. State = [[-1.545419e-01 -1.262570e-04]]. Action = [[ 0.23037934 -0.14849828 -0.01284106  0.43473208]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
Current timestep = 63. State = [[-0.14757402 -0.01666412]]. Action = [[-0.11783059 -0.06054603 -0.20062831  0.8809614 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
Current timestep = 64. State = [[-0.1485944  -0.02580828]]. Action = [[-0.03794831 -0.06365445  0.20553419 -0.50688255]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
Current timestep = 65. State = [[-0.14664242 -0.03172902]]. Action = [[ 0.14907372  0.00422245 -0.23742783 -0.59112674]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
Current timestep = 66. State = [[-0.1482371  -0.03840799]]. Action = [[-0.16212797 -0.08391632 -0.23343417 -0.79208344]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(0.0108, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 66 of 1
Current timestep = 67. State = [[-0.16016676 -0.04708553]]. Action = [[-0.2031151  -0.00783771 -0.21995988 -0.32189202]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0073, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.1790201 -0.0367088]]. Action = [[-0.12207784  0.23408055  0.11484516  0.86032736]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
Current timestep = 69. State = [[-0.18424682 -0.02272223]]. Action = [[ 0.19797471 -0.04007336  0.17558268  0.2172513 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
Current timestep = 70. State = [[-0.17860207 -0.01704584]]. Action = [[ 0.04300174  0.09931844 -0.13287303 -0.15064895]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of 0
Current timestep = 71. State = [[-1.8277188e-01 -1.5737832e-04]]. Action = [[-0.2239509   0.19090295 -0.23829192  0.5202048 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
Current timestep = 72. State = [[-0.18594013  0.01410397]]. Action = [[ 0.19736075 -0.03277864 -0.1295823   0.6505194 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
Current timestep = 73. State = [[-0.1868436   0.01131885]]. Action = [[-0.16036804 -0.06555359 -0.06575501  0.12348735]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
Current timestep = 74. State = [[-0.18491831  0.006965  ]]. Action = [[ 0.16021484 -0.03773174 -0.13337012  0.2434181 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
Current timestep = 75. State = [[-0.17782931  0.00402112]]. Action = [[0.11934981 0.01040256 0.03598401 0.98834443]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
Current timestep = 76. State = [[-0.17330493 -0.01005322]]. Action = [[-0.12037122 -0.23839466 -0.05199303  0.59702206]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
Current timestep = 77. State = [[-0.17060944 -0.02506505]]. Action = [[ 0.13368869  0.02545437 -0.1012266   0.01619077]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
Current timestep = 78. State = [[-0.16349947 -0.0319667 ]]. Action = [[ 0.11869588 -0.07135904 -0.09405947  0.3759868 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
Current timestep = 79. State = [[-0.15730564 -0.03051513]]. Action = [[-0.19511844  0.10787296  0.22415754 -0.3041122 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of 0
Current timestep = 80. State = [[-0.15686272 -0.03642187]]. Action = [[ 0.11784592 -0.21858948  0.23144639 -0.787244  ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
Current timestep = 81. State = [[-0.1521612  -0.05125427]]. Action = [[ 0.0740326  -0.00992908  0.15791464 -0.4725201 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
Current timestep = 82. State = [[-0.14381893 -0.04977802]]. Action = [[0.08726478 0.08747929 0.23441046 0.07539833]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
Current timestep = 83. State = [[-0.13921514 -0.04580876]]. Action = [[-0.09745677  0.01854232  0.24281216 -0.62445843]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
Current timestep = 84. State = [[-0.1477025  -0.03493187]]. Action = [[-0.21526796  0.15659857 -0.05235977 -0.636654  ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
Current timestep = 85. State = [[-0.154436 -0.025482]]. Action = [[ 0.06746423 -0.06787059  0.16322142 -0.49985766]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 85 of 0
Current timestep = 86. State = [[-0.15660962 -0.01771832]]. Action = [[-0.03667149  0.1694988  -0.17338404  0.21270132]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
Current timestep = 87. State = [[-0.16455378  0.00345429]]. Action = [[-0.16664769  0.1792407  -0.22334564  0.24106193]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
Current timestep = 88. State = [[-0.17289709  0.01727558]]. Action = [[ 0.071087   -0.01591225  0.20842427 -0.32556307]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of -1
Current timestep = 89. State = [[-0.17495632  0.02614375]]. Action = [[ 0.00518858  0.13288361  0.08653826 -0.6628195 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 89 of -1
Current timestep = 90. State = [[-0.17098701  0.03218444]]. Action = [[ 0.18442386 -0.09070885 -0.04844046 -0.93296903]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of 1
Current timestep = 91. State = [[-0.16738743  0.02039011]]. Action = [[-0.19040152 -0.13036603  0.08462942 -0.18266362]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
Current timestep = 92. State = [[-0.17335065  0.01172712]]. Action = [[ 0.00216481 -0.01381254  0.12019882  0.8972198 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
Current timestep = 93. State = [[-0.17097607  0.00072302]]. Action = [[ 0.10141569 -0.1508425   0.22145265  0.23083961]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
Current timestep = 94. State = [[-0.16403238 -0.00472538]]. Action = [[0.10996395 0.12510663 0.21389383 0.2717862 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 0
Current timestep = 95. State = [[-0.16290799 -0.00435173]]. Action = [[-0.1652803  -0.08102386 -0.12112594  0.83704185]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
Current timestep = 96. State = [[-0.17786145 -0.02117119]]. Action = [[-0.20863988 -0.20120694 -0.07812388 -0.7065933 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
Current timestep = 97. State = [[-0.18749325 -0.03103872]]. Action = [[ 0.10040641  0.08127365 -0.17359637 -0.6318707 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
Current timestep = 98. State = [[-0.1938461  -0.01744252]]. Action = [[-0.20339385  0.21449208 -0.218995   -0.13690752]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
Current timestep = 99. State = [[-0.20872746  0.00729183]]. Action = [[-0.07249741  0.14049488  0.1665988  -0.49379778]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
Current timestep = 100. State = [[-0.22374356  0.01353409]]. Action = [[-0.16409142 -0.11516297 -0.02978908 -0.44158542]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 100 of -1
Current timestep = 101. State = [[-0.23308668 -0.00377556]]. Action = [[ 0.23869008 -0.17391366 -0.17192516  0.6387235 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
Current timestep = 102. State = [[-0.22807693 -0.0140781 ]]. Action = [[-0.09644157  0.01145521  0.14201975 -0.8332189 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
Current timestep = 103. State = [[-0.23094288 -0.02361242]]. Action = [[-0.05672634 -0.13361436 -0.19501133  0.06637585]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
Current timestep = 104. State = [[-0.22406973 -0.03070457]]. Action = [[ 0.24009335  0.02828249 -0.20691499 -0.7800534 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
Current timestep = 105. State = [[-0.21110012 -0.02753366]]. Action = [[ 0.12433708  0.06920272 -0.22135326 -0.7984083 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
Current timestep = 106. State = [[-0.20747808 -0.01247161]]. Action = [[-0.15009369  0.21396798 -0.05494466  0.01173806]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 106 of 0
Current timestep = 107. State = [[-0.20857261 -0.0071268 ]]. Action = [[ 0.05096605 -0.20873569 -0.23004517  0.27106738]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
Current timestep = 108. State = [[-0.2062173  -0.01517997]]. Action = [[0.01172477 0.04602024 0.19590127 0.96460736]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
Current timestep = 109. State = [[-0.203526   -0.02051298]]. Action = [[ 0.07603231 -0.0852149   0.11339039 -0.81647843]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
Current timestep = 110. State = [[-0.20412873 -0.03613454]]. Action = [[-0.1367543  -0.19248328 -0.19662756 -0.03057492]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
Current timestep = 111. State = [[-0.20201173 -0.05228057]]. Action = [[ 0.19723284 -0.02437975  0.16986787 -0.9197812 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
Current timestep = 112. State = [[-0.19602937 -0.06060434]]. Action = [[-0.13083349 -0.05991918  0.05923587  0.17754686]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
Current timestep = 113. State = [[-0.18975098 -0.07058038]]. Action = [[ 0.24424735 -0.09408981 -0.21615782  0.5641669 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0086, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.17081943 -0.0848569 ]]. Action = [[ 0.14205462 -0.08351499 -0.11959589  0.5681714 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
Current timestep = 115. State = [[-0.15963775 -0.09830591]]. Action = [[-0.07238138 -0.11556241 -0.21736243  0.93382263]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
Current timestep = 116. State = [[-0.15199898 -0.10267346]]. Action = [[ 0.1809457   0.07013145 -0.24195488 -0.04538757]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
Current timestep = 117. State = [[-0.14497933 -0.09244633]]. Action = [[-0.12204532  0.13495308  0.04187247  0.21098125]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
Current timestep = 118. State = [[-0.1466364  -0.08284023]]. Action = [[-0.01110259  0.05057621  0.15843713 -0.08769947]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
Current timestep = 119. State = [[-0.1391476 -0.0859678]]. Action = [[ 0.23538741 -0.15879692 -0.1210283   0.04714   ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.12162609 -0.08528559]]. Action = [[0.03933623 0.17693314 0.11521181 0.8593385 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
Current timestep = 121. State = [[-0.10970611 -0.08722529]]. Action = [[ 0.19616234 -0.22132586  0.16669971  0.96158767]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
Current timestep = 122. State = [[-0.08823173 -0.10344726]]. Action = [[ 0.09103182 -0.08287394 -0.21854767  0.87986684]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
Current timestep = 123. State = [[-0.08108106 -0.11545669]]. Action = [[-0.12052038 -0.0657101   0.11851507 -0.47331965]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
Current timestep = 124. State = [[-0.07588065 -0.10887416]]. Action = [[ 0.13754603  0.2068508  -0.15330076  0.57236385]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0133, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.05983016 -0.08436871]]. Action = [[ 0.20382202  0.21081    -0.13210003 -0.9212558 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
Current timestep = 126. State = [[-0.24294546  0.01782268]]. Action = [[-0.24098332 -0.03493804  0.08964917  0.27407634]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of 0
Current timestep = 127. State = [[-0.23097067  0.02838646]]. Action = [[ 0.1780433   0.12128335  0.22241664 -0.7399946 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 127 is [True, False, False, False, True, False]
Current timestep = 128. State = [[-0.21336575  0.04241031]]. Action = [[0.07560894 0.09523103 0.11988536 0.30965066]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 128 is [True, False, False, False, True, False]
Current timestep = 129. State = [[-0.20693164  0.06107586]]. Action = [[-0.0371857   0.16654348  0.01098189  0.6688874 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 129 is [True, False, False, False, True, False]
Current timestep = 130. State = [[-0.1974086   0.06475914]]. Action = [[ 0.20100123 -0.16840301 -0.14456284 -0.03030372]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 130 is [True, False, False, False, True, False]
Current timestep = 131. State = [[-0.1866517   0.05209725]]. Action = [[-0.16215637 -0.1387786  -0.03342682 -0.7919912 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 131 is [True, False, False, False, True, False]
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.1884341   0.03857395]]. Action = [[-0.02556276  0.01121247  0.23362887 -0.83582467]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 132 is [True, False, False, False, True, False]
Current timestep = 133. State = [[-0.18705547  0.02975046]]. Action = [[ 0.07332101 -0.15791221  0.1976535  -0.9342867 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 133 is [True, False, False, False, True, False]
Current timestep = 134. State = [[-0.18140745  0.00734127]]. Action = [[ 0.13582578 -0.1968951   0.04811391 -0.6636111 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 134 is [True, False, False, False, True, False]
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.17478427 -0.00917227]]. Action = [[-0.02132638  0.02214405  0.00087383 -0.56927425]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 135 is [True, False, False, False, True, False]
Current timestep = 136. State = [[-0.17003547 -0.00072061]]. Action = [[0.1296249  0.15184808 0.16194266 0.22215712]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 136 is [True, False, False, False, True, False]
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.1597712   0.00611369]]. Action = [[ 0.03733563 -0.0628923  -0.07021368 -0.53071344]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 137 is [True, False, False, False, True, False]
Current timestep = 138. State = [[-0.1566756 -0.0021854]]. Action = [[ 0.00788069 -0.11042248 -0.12002885  0.790715  ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 138 is [True, False, False, False, True, False]
Current timestep = 139. State = [[-0.15806322  0.00135186]]. Action = [[-0.14663702  0.18514758 -0.19339842 -0.7386918 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 139 is [True, False, False, False, True, False]
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 139 of 0
Current timestep = 140. State = [[-0.1665721   0.02442187]]. Action = [[-0.11683309  0.20389026 -0.12682074 -0.5735718 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 140 is [True, False, False, False, True, False]
Current timestep = 141. State = [[-0.17152846  0.03617901]]. Action = [[ 0.11736661 -0.0598077   0.12242833 -0.49462914]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 141 is [True, False, False, False, True, False]
Current timestep = 142. State = [[-0.16985688  0.02963482]]. Action = [[-0.04548779 -0.10983679 -0.14302789 -0.44625854]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 142 is [True, False, False, False, True, False]
Current timestep = 143. State = [[-0.17895178  0.0149307 ]]. Action = [[-0.23233177 -0.12747525  0.02348533  0.28926766]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 143 is [True, False, False, False, True, False]
Current timestep = 144. State = [[-0.19082859 -0.00447011]]. Action = [[-0.0181669  -0.137528   -0.02451032  0.40066767]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 144 is [True, False, False, False, True, False]
Current timestep = 145. State = [[-0.18960547 -0.00420327]]. Action = [[0.17882937 0.17934337 0.20178533 0.5779904 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 145 is [True, False, False, False, True, False]
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.19093396 -0.00848884]]. Action = [[-0.17762108 -0.22026128  0.17805868  0.602268  ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 146 is [True, False, False, False, True, False]
Current timestep = 147. State = [[-0.19622777 -0.02707739]]. Action = [[-0.05295652 -0.09959546 -0.1743563   0.19206715]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 147 is [True, False, False, False, True, False]
Current timestep = 148. State = [[-0.20668487 -0.04582919]]. Action = [[-0.13752694 -0.1460338   0.10683715  0.79709077]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 148 is [True, False, False, False, True, False]
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.22171447 -0.07008383]]. Action = [[-0.0033354  -0.14670467 -0.08957252  0.65064836]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 149 is [True, False, False, False, True, False]
Current timestep = 150. State = [[-0.22065479 -0.06737844]]. Action = [[0.10169551 0.22473162 0.1856558  0.5068314 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 150 is [True, False, False, False, True, False]
Current timestep = 151. State = [[-0.2259011 -0.0440895]]. Action = [[-0.22058615  0.2319259   0.15098393  0.7783601 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 151 is [True, False, False, False, True, False]
Current timestep = 152. State = [[-0.24218744 -0.01118842]]. Action = [[-0.12623046  0.18144956 -0.01022395 -0.784405  ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 152 is [True, False, False, False, True, False]
Current timestep = 153. State = [[-0.2511193  0.0139664]]. Action = [[0.13981956 0.12991852 0.00210962 0.86727333]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 153 is [True, False, False, False, True, False]
Current timestep = 154. State = [[-0.25039276  0.023001  ]]. Action = [[-0.16306926 -0.15004303 -0.20963657  0.7886524 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 154 is [True, False, False, False, True, False]
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of -1
Current timestep = 155. State = [[-0.24689998  0.01767214]]. Action = [[ 0.06420293 -0.15106757  0.04238907  0.8028791 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 155 is [True, False, False, False, True, False]
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of 0
Current timestep = 156. State = [[-0.23251171  0.01928577]]. Action = [[ 0.2453807   0.20010185  0.23133641 -0.99658555]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 156 is [True, False, False, False, True, False]
Current timestep = 157. State = [[-0.22186258  0.02669237]]. Action = [[-0.16219263 -0.07205078  0.09535226  0.64412785]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 157 is [True, False, False, False, True, False]
Current timestep = 158. State = [[-0.22133857  0.03350685]]. Action = [[ 0.12884527  0.14160222  0.04817212 -0.8441788 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 158 is [True, False, False, False, True, False]
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of 1
Current timestep = 159. State = [[-0.20747992  0.04391692]]. Action = [[ 0.22760421  0.05102021 -0.00617476 -0.42480528]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 159 is [True, False, False, False, True, False]
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 159 of 1
Current timestep = 160. State = [[-0.18646455  0.0519457 ]]. Action = [[-0.0064019   0.05115625 -0.17933102 -0.76793915]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 160 is [True, False, False, False, True, False]
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of 0
Current timestep = 161. State = [[-0.18738289  0.0707597 ]]. Action = [[-0.01292251  0.20130497  0.12519062 -0.750653  ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 161 is [True, False, False, False, True, False]
Current timestep = 162. State = [[-0.19308628  0.08020013]]. Action = [[-0.19882225 -0.07776438 -0.24410875  0.6538632 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 162 is [True, False, False, False, True, False]
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.19723937  0.088173  ]]. Action = [[ 0.12622377  0.1458078  -0.17569847  0.12388599]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 163 is [True, False, False, False, True, False]
Current timestep = 164. State = [[-0.20159559  0.10016447]]. Action = [[-0.07163268  0.08467937  0.05126256 -0.648152  ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 164 is [True, False, False, False, True, False]
Current timestep = 165. State = [[-0.20274872  0.12057934]]. Action = [[ 0.09603569  0.21524718 -0.21380559 -0.4743619 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 165 is [True, False, False, False, True, False]
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.20036158  0.13541034]]. Action = [[-0.17560895 -0.09526064 -0.1985769  -0.39429152]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 166 is [True, False, False, False, True, False]
Scene graph at timestep 166 is [True, False, False, False, False, True]
State prediction error at timestep 166 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of 0
Current timestep = 167. State = [[-0.20343326  0.13360228]]. Action = [[ 0.15797746  0.03433034  0.04697895 -0.0231539 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 167 is [True, False, False, False, False, True]
Current timestep = 168. State = [[-0.20189612  0.1348325 ]]. Action = [[-0.09900463 -0.00957622 -0.01310179  0.6566397 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 168 is [True, False, False, False, False, True]
Scene graph at timestep 168 is [True, False, False, False, False, True]
State prediction error at timestep 168 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of 0
Current timestep = 169. State = [[-0.20394659  0.14606927]]. Action = [[ 0.05713689  0.21705467 -0.15879278 -0.82672185]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 169 is [True, False, False, False, False, True]
Scene graph at timestep 169 is [True, False, False, False, False, True]
State prediction error at timestep 169 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of -1
Current timestep = 170. State = [[-0.19488446  0.1541514 ]]. Action = [[ 0.22787791 -0.11299582 -0.05033012  0.91571295]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 170 is [True, False, False, False, False, True]
Current timestep = 171. State = [[-0.18064454  0.15933852]]. Action = [[-0.03447121  0.14807403  0.06090593  0.9027381 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 171 is [True, False, False, False, False, True]
Current timestep = 172. State = [[-0.17595014  0.17299126]]. Action = [[ 0.05122587  0.11998227  0.12817633 -0.94703054]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 172 is [True, False, False, False, False, True]
Current timestep = 173. State = [[-0.16353846  0.17058748]]. Action = [[ 0.13413244 -0.20545205 -0.11849293  0.688269  ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 173 is [True, False, False, False, False, True]
Current timestep = 174. State = [[-0.151436    0.15154268]]. Action = [[ 0.0655424 -0.1506541  0.1614511 -0.9553273]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 174 is [True, False, False, False, False, True]
Current timestep = 175. State = [[-0.15093198  0.13539386]]. Action = [[-0.22478774 -0.111545   -0.12448025 -0.90967155]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 175 is [True, False, False, False, False, True]
Scene graph at timestep 175 is [True, False, False, False, False, True]
State prediction error at timestep 175 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of 0
Current timestep = 176. State = [[-0.15267095  0.13150716]]. Action = [[ 0.14236191  0.14164317  0.24495244 -0.66673416]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 176 is [True, False, False, False, False, True]
Scene graph at timestep 176 is [True, False, False, False, False, True]
State prediction error at timestep 176 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of -1
Current timestep = 177. State = [[-0.14490917  0.13383366]]. Action = [[ 0.1895887  -0.0546407   0.18969539 -0.13207662]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 177 is [True, False, False, False, False, True]
Current timestep = 178. State = [[-0.13178097  0.1244387 ]]. Action = [[-0.07458706 -0.17171805  0.00441268 -0.63477296]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 178 is [True, False, False, False, False, True]
Current timestep = 179. State = [[-0.1324259   0.11299121]]. Action = [[-0.12118581 -0.03750747 -0.21917765  0.62486935]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 179 is [True, False, False, False, True, False]
Current timestep = 180. State = [[-0.14528066  0.1175292 ]]. Action = [[-0.20468453  0.12278098  0.00473079  0.06767356]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 180 is [True, False, False, False, True, False]
Current timestep = 181. State = [[-0.15857966  0.11267612]]. Action = [[-0.01824009 -0.17229778  0.03285727  0.12959933]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 181 is [True, False, False, False, True, False]
Current timestep = 182. State = [[-0.16314296  0.11229456]]. Action = [[-0.0409335   0.14637935 -0.09158871  0.12583888]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 182 is [True, False, False, False, True, False]
Current timestep = 183. State = [[-0.17406556  0.11426444]]. Action = [[-0.16632164 -0.09224477 -0.01242548 -0.65698916]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 183 is [True, False, False, False, True, False]
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of 0
Current timestep = 184. State = [[-0.19986907  0.10837782]]. Action = [[-0.22652729 -0.0204673   0.20269811  0.7744193 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 184 is [True, False, False, False, True, False]
Current timestep = 185. State = [[-0.21892856  0.11837398]]. Action = [[ 0.10031664  0.22838825 -0.10253343  0.8420304 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 185 is [True, False, False, False, True, False]
Current timestep = 186. State = [[-0.21803495  0.12351335]]. Action = [[ 0.03900036 -0.1471021   0.04064578  0.42012298]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 186 is [True, False, False, False, True, False]
Current timestep = 187. State = [[-0.20940603  0.10902807]]. Action = [[ 0.15110594 -0.14932822  0.1552074   0.9911907 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 187 is [True, False, False, False, True, False]
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 187 of 0
Current timestep = 188. State = [[-0.19387442  0.08537973]]. Action = [[ 0.15808493 -0.15236223 -0.11499521 -0.33034128]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 188 is [True, False, False, False, True, False]
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of 1
Current timestep = 189. State = [[-0.17508043  0.08103313]]. Action = [[ 0.16541958  0.19251549 -0.23206517  0.1403091 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 189 is [True, False, False, False, True, False]
Current timestep = 190. State = [[-0.15968099  0.08367773]]. Action = [[ 0.0269309  -0.13980238  0.02823082  0.8999026 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 190 is [True, False, False, False, True, False]
Current timestep = 191. State = [[-0.14526494  0.07291266]]. Action = [[ 0.24508762 -0.08475682  0.05209562  0.6027684 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 191 is [True, False, False, False, True, False]
Current timestep = 192. State = [[-0.1204467   0.06901063]]. Action = [[ 0.12483364  0.04201609  0.00563729 -0.95515835]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 192 is [True, False, False, False, True, False]
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 192 of 1
Current timestep = 193. State = [[-0.10282364  0.06593657]]. Action = [[ 0.08954993 -0.07426989  0.04636204  0.6717148 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 193 is [True, False, False, False, True, False]
Current timestep = 194. State = [[-0.08625024  0.07106128]]. Action = [[ 0.18845361  0.16420943  0.19689932 -0.7463509 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 194 is [True, False, False, False, True, False]
Current timestep = 195. State = [[-0.07050149  0.07433748]]. Action = [[-0.08404356 -0.12122792 -0.19865283 -0.92722124]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 195 is [True, False, False, False, True, False]
Current timestep = 196. State = [[-0.0705894   0.06402589]]. Action = [[-0.12918225 -0.11738957  0.1882104   0.68451357]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 196 is [True, False, False, False, True, False]
Current timestep = 197. State = [[-0.06621061  0.04526289]]. Action = [[ 0.23016337 -0.1632545  -0.15148376 -0.08659005]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 197 is [True, False, False, False, True, False]
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.05922358  0.03794065]]. Action = [[-0.01017103  0.21262664 -0.09805164 -0.69063944]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 198 is [True, False, False, False, True, False]
Current timestep = 199. State = [[-0.06678402  0.05434078]]. Action = [[-0.24138962  0.05534694 -0.22178271 -0.3737083 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 199 is [True, False, False, False, True, False]
Current timestep = 200. State = [[-0.06886593  0.05309467]]. Action = [[ 0.13557708 -0.18508948  0.07312256  0.40390217]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 200 is [True, False, False, False, True, False]
Current timestep = 201. State = [[-0.0636852   0.02999693]]. Action = [[ 0.03198141 -0.20338257  0.19557637 -0.514446  ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 201 is [True, False, False, False, True, False]
Current timestep = 202. State = [[-0.06229046  0.0013957 ]]. Action = [[-0.07996467 -0.19592907 -0.18958434  0.32683873]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 202 is [True, False, False, False, True, False]
Current timestep = 203. State = [[-0.06530513 -0.00434618]]. Action = [[-0.01897779  0.19090468  0.04183424  0.2942798 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 203 is [True, False, False, False, True, False]
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of 1
Current timestep = 204. State = [[-0.06171997  0.01620197]]. Action = [[ 0.20339125  0.20753682 -0.20510879  0.8667779 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 204 is [True, False, False, False, True, False]
Current timestep = 205. State = [[-0.05719819  0.02600685]]. Action = [[-0.02755253 -0.04232651  0.00434607  0.07841599]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 205 is [True, False, False, False, True, False]
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of 1
Current timestep = 206. State = [[-0.05734572  0.02632795]]. Action = [[-0.01729788 -0.01350868 -0.00767234 -0.7930619 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 206 is [True, False, False, False, True, False]
Current timestep = 207. State = [[-0.05222148  0.02945017]]. Action = [[ 0.14276838  0.06351915 -0.21990438  0.7413621 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 207 is [True, False, False, False, True, False]
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 207 of 1
Current timestep = 208. State = [[-0.03941429  0.04443235]]. Action = [[ 0.06750792  0.14774984 -0.12517391 -0.9494501 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 208 is [True, False, False, False, True, False]
Current timestep = 209. State = [[-0.14922324 -0.12652951]]. Action = [[-0.20089288 -0.05095178 -0.1975958  -0.97178996]]. Reward = [100.]
Curr episode timestep = 82
Scene graph at timestep 209 is [False, True, False, False, True, False]
Current timestep = 210. State = [[-0.13310008 -0.14763975]]. Action = [[-0.09835228 -0.0897105  -0.00838098 -0.3748147 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 210 is [True, False, False, True, False, False]
Current timestep = 211. State = [[-0.13365693 -0.16264112]]. Action = [[ 0.10959446 -0.12768981 -0.07492691  0.5044879 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 211 is [True, False, False, True, False, False]
Current timestep = 212. State = [[-0.12558456 -0.18503647]]. Action = [[ 0.14911938 -0.23168461 -0.17903806 -0.40661752]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 212 is [True, False, False, True, False, False]
Current timestep = 213. State = [[-0.10303134 -0.2063175 ]]. Action = [[ 0.19974345 -0.06097636  0.17905289 -0.02191836]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 213 is [True, False, False, True, False, False]
Current timestep = 214. State = [[-0.07422039 -0.21376395]]. Action = [[ 0.21665817  0.01173741 -0.03235559 -0.38219953]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 214 is [True, False, False, True, False, False]
Current timestep = 215. State = [[-0.05834286 -0.2051242 ]]. Action = [[-0.14673974  0.21977493  0.19666442 -0.81813055]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 215 is [True, False, False, True, False, False]
Current timestep = 216. State = [[-0.05975895 -0.18617843]]. Action = [[-0.1004222   0.12778503  0.11072081  0.07334208]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 216 is [True, False, False, True, False, False]
Current timestep = 217. State = [[-0.06967133 -0.18965262]]. Action = [[-0.17588016 -0.20256877 -0.12026861  0.09696913]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 217 is [True, False, False, True, False, False]
Current timestep = 218. State = [[-0.08146744 -0.19096394]]. Action = [[-0.05413407  0.17042273  0.12395957 -0.21299505]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 218 is [True, False, False, True, False, False]
Scene graph at timestep 218 is [True, False, False, True, False, False]
State prediction error at timestep 218 is tensor(0.0134, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of 0
Current timestep = 219. State = [[-0.09200215 -0.17846332]]. Action = [[-0.03057408  0.01916742 -0.11552241 -0.83374065]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 219 is [True, False, False, True, False, False]
Current timestep = 220. State = [[-0.09441316 -0.18975359]]. Action = [[ 0.07889247 -0.23061034  0.08580887  0.18516803]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 220 is [True, False, False, True, False, False]
Current timestep = 221. State = [[-0.10180056 -0.2096459 ]]. Action = [[-0.17650631 -0.09746636  0.08178025 -0.7503754 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 221 is [True, False, False, True, False, False]
Current timestep = 222. State = [[-0.11403587 -0.23468272]]. Action = [[-0.08294177 -0.23392203 -0.10512614  0.80948234]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 222 is [True, False, False, True, False, False]
Current timestep = 223. State = [[-0.12779441 -0.24137813]]. Action = [[-0.12643698  0.20628747 -0.01924175 -0.5527729 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 223 is [True, False, False, True, False, False]
Current timestep = 224. State = [[-0.13645639 -0.2281899 ]]. Action = [[-0.01589419  0.09756461 -0.16062339 -0.8878669 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 224 is [True, False, False, True, False, False]
Scene graph at timestep 224 is [True, False, False, True, False, False]
State prediction error at timestep 224 is tensor(0.0141, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of -1
Current timestep = 225. State = [[-0.14113973 -0.21312498]]. Action = [[-0.06681457  0.09011507  0.21537715  0.09164333]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 225 is [True, False, False, True, False, False]
Current timestep = 226. State = [[-0.1553539  -0.21445948]]. Action = [[-0.10960376 -0.13844416 -0.04572703  0.00567985]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 226 is [True, False, False, True, False, False]
Current timestep = 227. State = [[-0.16029334 -0.23270045]]. Action = [[ 0.12071532 -0.21809818  0.17654148 -0.2866556 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 227 is [True, False, False, True, False, False]
Current timestep = 228. State = [[-0.15986869 -0.25205374]]. Action = [[ 0.01329571 -0.08925354 -0.09376359 -0.8861786 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 228 is [True, False, False, True, False, False]
Scene graph at timestep 228 is [True, False, False, True, False, False]
State prediction error at timestep 228 is tensor(0.0173, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 228 of -1
Current timestep = 229. State = [[-0.16405757 -0.26244178]]. Action = [[-0.18467791  0.05461627 -0.23829617  0.18328762]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 229 is [True, False, False, True, False, False]
Current timestep = 230. State = [[-0.16807407 -0.27183375]]. Action = [[ 0.1911251  -0.2038728  -0.09889922 -0.6689099 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 230 is [True, False, False, True, False, False]
Scene graph at timestep 230 is [True, False, False, True, False, False]
State prediction error at timestep 230 is tensor(0.0185, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 230 of -1
Current timestep = 231. State = [[-0.1646227  -0.28249192]]. Action = [[ 0.03010964 -0.00725652 -0.02250305 -0.11776495]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 231 is [True, False, False, True, False, False]
Scene graph at timestep 231 is [True, False, False, True, False, False]
State prediction error at timestep 231 is tensor(0.0216, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of -1
Current timestep = 232. State = [[-0.1701561  -0.27997297]]. Action = [[-0.22054584  0.12310365  0.11483577 -0.20251828]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 232 is [True, False, False, True, False, False]
Current timestep = 233. State = [[-0.17797747 -0.26646003]]. Action = [[-0.0499015   0.19718578  0.21664804  0.58506465]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 233 is [True, False, False, True, False, False]
Current timestep = 234. State = [[-0.18060665 -0.25905877]]. Action = [[ 0.0644924  -0.14456262  0.08532453 -0.25366652]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 234 is [True, False, False, True, False, False]
Scene graph at timestep 234 is [True, False, False, True, False, False]
State prediction error at timestep 234 is tensor(0.0169, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 234 of -1
Current timestep = 235. State = [[-0.18168361 -0.27196786]]. Action = [[ 0.0555968  -0.16556254 -0.11659104 -0.8029099 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 235 is [True, False, False, True, False, False]
Scene graph at timestep 235 is [True, False, False, True, False, False]
State prediction error at timestep 235 is tensor(0.0162, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 235 of -1
Current timestep = 236. State = [[-0.18215862 -0.28234893]]. Action = [[-0.22575955 -0.21832378 -0.02643888 -0.2626931 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 236 is [True, False, False, True, False, False]
Current timestep = 237. State = [[-0.18546519 -0.2782571 ]]. Action = [[-0.16737053  0.14815634  0.03244114 -0.37527114]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 237 is [True, False, False, True, False, False]
Scene graph at timestep 237 is [True, False, False, True, False, False]
State prediction error at timestep 237 is tensor(0.0152, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 237 of -1
Current timestep = 238. State = [[-0.18234049 -0.2598503 ]]. Action = [[ 0.17631161  0.2173543  -0.2051143  -0.37049258]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 238 is [True, False, False, True, False, False]
Current timestep = 239. State = [[-0.17565736 -0.2389032 ]]. Action = [[ 0.0712775   0.03755865 -0.04256916 -0.46756345]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 239 is [True, False, False, True, False, False]
Current timestep = 240. State = [[-0.16735774 -0.22388028]]. Action = [[ 0.08950484  0.11685944 -0.16177717  0.97331023]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 240 is [True, False, False, True, False, False]
Current timestep = 241. State = [[-0.16168545 -0.2153869 ]]. Action = [[ 0.04039598 -0.01763402 -0.04629029  0.19535804]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 241 is [True, False, False, True, False, False]
Current timestep = 242. State = [[-0.14731072 -0.20675123]]. Action = [[ 0.23845881  0.10925829 -0.15796188 -0.12435102]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 242 is [True, False, False, True, False, False]
Current timestep = 243. State = [[-0.12905882 -0.1927346 ]]. Action = [[-0.0102181   0.11069849 -0.10665366 -0.5490024 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 243 is [True, False, False, True, False, False]
Current timestep = 244. State = [[-0.12066689 -0.17824697]]. Action = [[ 0.15396482  0.06530356  0.12525994 -0.66255087]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 244 is [True, False, False, True, False, False]
Scene graph at timestep 244 is [True, False, False, True, False, False]
State prediction error at timestep 244 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of 1
Current timestep = 245. State = [[-0.11154044 -0.15782823]]. Action = [[-0.16034874  0.22419614 -0.20314226 -0.93168813]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 245 is [True, False, False, True, False, False]
Current timestep = 246. State = [[-0.12130933 -0.14480889]]. Action = [[-0.17883635 -0.02012402 -0.05527571 -0.31450254]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 246 is [True, False, False, True, False, False]
Scene graph at timestep 246 is [True, False, False, True, False, False]
State prediction error at timestep 246 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 246 of 0
Current timestep = 247. State = [[-0.13816124 -0.13611537]]. Action = [[-0.12094146  0.12523866 -0.06641416  0.2660998 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 247 is [True, False, False, True, False, False]
Current timestep = 248. State = [[-0.14651906 -0.14050147]]. Action = [[ 0.03399122 -0.21798149  0.20438203 -0.97158706]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 248 is [True, False, False, True, False, False]
Current timestep = 249. State = [[-0.15615974 -0.13878629]]. Action = [[-0.14984901  0.20752466 -0.14878263 -0.44914687]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 249 is [True, False, False, True, False, False]
Current timestep = 250. State = [[-0.16085643 -0.13877706]]. Action = [[ 0.12076664 -0.18684669 -0.08699635 -0.4524474 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 250 is [True, False, False, True, False, False]
Current timestep = 251. State = [[-0.16965275 -0.15600398]]. Action = [[-0.23892033 -0.11779901  0.21443027  0.8983463 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 251 is [True, False, False, True, False, False]
Current timestep = 252. State = [[-0.19007343 -0.17757656]]. Action = [[-0.18206055 -0.15013878  0.1450572  -0.5152393 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 252 is [True, False, False, True, False, False]
Scene graph at timestep 252 is [True, False, False, True, False, False]
State prediction error at timestep 252 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.22015232 -0.2081988 ]]. Action = [[-0.19054438 -0.24073431  0.10425711 -0.24900377]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 253 is [True, False, False, True, False, False]
Current timestep = 254. State = [[-0.24844956 -0.2305804 ]]. Action = [[-0.20840633 -0.07598791  0.21299148  0.6895468 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 254 is [True, False, False, True, False, False]
Current timestep = 255. State = [[-0.2575869  -0.23253624]]. Action = [[ 0.23737365  0.10261038 -0.15127526  0.6300473 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 255 is [True, False, False, True, False, False]
Current timestep = 256. State = [[-0.25238797 -0.227627  ]]. Action = [[-0.22560267 -0.19990692  0.06343699 -0.5421339 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 256 is [True, False, False, True, False, False]
Scene graph at timestep 256 is [True, False, False, True, False, False]
State prediction error at timestep 256 is tensor(0.0086, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 256 of -1
Current timestep = 257. State = [[-0.25124422 -0.22648975]]. Action = [[-0.14517651  0.1928922   0.18632078 -0.6736422 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 257 is [True, False, False, True, False, False]
Scene graph at timestep 257 is [True, False, False, True, False, False]
State prediction error at timestep 257 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of -1
Current timestep = 258. State = [[-0.24290216 -0.23275872]]. Action = [[ 0.18514782 -0.15022957 -0.08848834 -0.7627003 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 258 is [True, False, False, True, False, False]
Current timestep = 259. State = [[-0.23006341 -0.23816334]]. Action = [[ 0.04961592  0.02310187 -0.17459382  0.43331265]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 259 is [True, False, False, True, False, False]
Current timestep = 260. State = [[-0.22291934 -0.22646067]]. Action = [[0.01013115 0.21494365 0.06226915 0.31914568]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 260 is [True, False, False, True, False, False]
Current timestep = 261. State = [[-0.2190763  -0.21189968]]. Action = [[0.01441216 0.05333692 0.08405641 0.09130442]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 261 is [True, False, False, True, False, False]
Current timestep = 262. State = [[-0.20844483 -0.21468109]]. Action = [[ 0.19828886 -0.1747466   0.0485554   0.70564413]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 262 is [True, False, False, True, False, False]
Current timestep = 263. State = [[-0.20086782 -0.21941265]]. Action = [[-0.19586383  0.07529223 -0.16653556  0.01704597]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 263 is [True, False, False, True, False, False]
Scene graph at timestep 263 is [True, False, False, True, False, False]
State prediction error at timestep 263 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 263 of 1
Current timestep = 264. State = [[-0.21894807 -0.22558816]]. Action = [[-0.2440346  -0.08880946  0.172656   -0.83220303]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 264 is [True, False, False, True, False, False]
Current timestep = 265. State = [[-0.23541802 -0.23824878]]. Action = [[-0.01584512 -0.11648655  0.1618984   0.953398  ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 265 is [True, False, False, True, False, False]
Current timestep = 266. State = [[-0.24822712 -0.23854549]]. Action = [[-0.1629758   0.15427935  0.11804083  0.7512734 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 266 is [True, False, False, True, False, False]
Current timestep = 267. State = [[-0.24974407 -0.22058988]]. Action = [[ 0.16383874  0.20184502 -0.20735475 -0.8372716 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 267 is [True, False, False, True, False, False]
Current timestep = 268. State = [[-0.239287  -0.1969529]]. Action = [[0.15175998 0.09966463 0.08727914 0.20939481]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 268 is [True, False, False, True, False, False]
Current timestep = 269. State = [[-0.23954052 -0.18832558]]. Action = [[-0.16946824 -0.02332114 -0.17722894 -0.520488  ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 269 is [True, False, False, True, False, False]
Scene graph at timestep 269 is [True, False, False, True, False, False]
State prediction error at timestep 269 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[-0.23629963 -0.19371335]]. Action = [[ 0.20481348 -0.13664728  0.15880197 -0.23473859]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 270 is [True, False, False, True, False, False]
Current timestep = 271. State = [[-0.23159426 -0.1982038 ]]. Action = [[-0.05865881  0.00231415  0.09911001 -0.00740421]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 271 is [True, False, False, True, False, False]
Current timestep = 272. State = [[-0.23444702 -0.20777342]]. Action = [[-0.05591542 -0.13088514 -0.03370291 -0.9664171 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 272 is [True, False, False, True, False, False]
Current timestep = 273. State = [[-0.2368304 -0.2248425]]. Action = [[ 0.00113678 -0.10200414  0.17787004  0.8521323 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 273 is [True, False, False, True, False, False]
Current timestep = 274. State = [[-0.24080887 -0.24639808]]. Action = [[-0.04007052 -0.2099066   0.23321217 -0.5238609 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 274 is [True, False, False, True, False, False]
Scene graph at timestep 274 is [True, False, False, True, False, False]
State prediction error at timestep 274 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 274 of -1
Current timestep = 275. State = [[-0.24415271 -0.2609661 ]]. Action = [[ 0.0138168   0.12930804  0.14098758 -0.6552441 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 275 is [True, False, False, True, False, False]
Scene graph at timestep 275 is [True, False, False, True, False, False]
State prediction error at timestep 275 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 275 of 0
Current timestep = 276. State = [[-0.24228984 -0.2551261 ]]. Action = [[-0.19662647  0.1903094   0.05003285 -0.7319889 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 276 is [True, False, False, True, False, False]
Scene graph at timestep 276 is [True, False, False, True, False, False]
State prediction error at timestep 276 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of -1
Current timestep = 277. State = [[-0.23607858 -0.24765497]]. Action = [[ 0.14840937  0.08995581  0.11776111 -0.80626625]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 277 is [True, False, False, True, False, False]
Current timestep = 278. State = [[-0.21864581 -0.23231371]]. Action = [[ 0.2209098   0.13153887 -0.22350852  0.3861202 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 278 is [True, False, False, True, False, False]
Current timestep = 279. State = [[-0.20498191 -0.22653377]]. Action = [[-0.07851008 -0.10797508 -0.1498145  -0.97071874]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 279 is [True, False, False, True, False, False]
Current timestep = 280. State = [[-0.20023027 -0.23121148]]. Action = [[ 0.13800794 -0.05841908 -0.24763943 -0.31822985]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 280 is [True, False, False, True, False, False]
Current timestep = 281. State = [[-0.19359607 -0.23006995]]. Action = [[-0.08925849  0.1230244  -0.2104372   0.38400352]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 281 is [True, False, False, True, False, False]
Scene graph at timestep 281 is [True, False, False, True, False, False]
State prediction error at timestep 281 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 281 of 0
Current timestep = 282. State = [[-0.18915929 -0.23388869]]. Action = [[ 0.15710294 -0.1667183  -0.00951414  0.08488119]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 282 is [True, False, False, True, False, False]
Current timestep = 283. State = [[-0.17997809 -0.23943594]]. Action = [[ 0.04655141  0.01402977 -0.11283983 -0.3058133 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 283 is [True, False, False, True, False, False]
Current timestep = 284. State = [[-0.18483694 -0.25391772]]. Action = [[-0.17561856 -0.2313716   0.11808309  0.6961968 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 284 is [True, False, False, True, False, False]
Current timestep = 285. State = [[-0.19296862 -0.264038  ]]. Action = [[-0.1476809   0.18939605 -0.14992526  0.76067364]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 285 is [True, False, False, True, False, False]
Current timestep = 286. State = [[-0.19626853 -0.2513122 ]]. Action = [[ 0.15541291  0.08273685 -0.10152644 -0.02620941]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 286 is [True, False, False, True, False, False]
Current timestep = 287. State = [[-0.19269933 -0.23008709]]. Action = [[-0.00786789  0.22081858 -0.23484682  0.87812686]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 287 is [True, False, False, True, False, False]
Scene graph at timestep 287 is [True, False, False, True, False, False]
State prediction error at timestep 287 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 287 of 0
Current timestep = 288. State = [[-0.1893013  -0.20476459]]. Action = [[-0.00600931  0.08954051  0.23841453 -0.81411034]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 288 is [True, False, False, True, False, False]
Current timestep = 289. State = [[-0.18773392 -0.20582387]]. Action = [[ 8.3678812e-02 -1.7144266e-01 -3.7179887e-04  9.2833531e-01]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 289 is [True, False, False, True, False, False]
Current timestep = 290. State = [[-0.18779762 -0.22296304]]. Action = [[-0.04863252 -0.18347871 -0.07484493  0.33800006]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 290 is [True, False, False, True, False, False]
Scene graph at timestep 290 is [True, False, False, True, False, False]
State prediction error at timestep 290 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 290 of -1
Current timestep = 291. State = [[-0.1866962  -0.22870976]]. Action = [[ 0.06151831  0.24624166  0.08278123 -0.2843765 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 291 is [True, False, False, True, False, False]
Current timestep = 292. State = [[-0.17977244 -0.20219731]]. Action = [[ 0.03991079  0.21726003 -0.19210638  0.6946442 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 292 is [True, False, False, True, False, False]
Current timestep = 293. State = [[-0.16667703 -0.17770015]]. Action = [[ 0.22021037  0.08494207 -0.23638818  0.0350858 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 293 is [True, False, False, True, False, False]
Current timestep = 294. State = [[-0.14661495 -0.1654605 ]]. Action = [[ 0.08231822  0.04884291  0.00675336 -0.11417234]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 294 is [True, False, False, True, False, False]
Current timestep = 295. State = [[-0.14423688 -0.14963843]]. Action = [[-0.22655939  0.20031255 -0.21125886 -0.49819827]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 295 is [True, False, False, True, False, False]
Current timestep = 296. State = [[-0.16093604 -0.13911277]]. Action = [[-0.19596075 -0.05079609 -0.16268003  0.00394762]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 296 is [True, False, False, True, False, False]
Current timestep = 297. State = [[-0.16935085 -0.13805106]]. Action = [[ 0.17142266  0.01244012 -0.20705266  0.36616826]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 297 is [True, False, False, True, False, False]
Scene graph at timestep 297 is [True, False, False, True, False, False]
State prediction error at timestep 297 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 297 of 1
Current timestep = 298. State = [[-0.16789098 -0.12868488]]. Action = [[-0.05409652  0.12695116 -0.15840867 -0.7736476 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 298 is [True, False, False, True, False, False]
Scene graph at timestep 298 is [True, False, False, True, False, False]
State prediction error at timestep 298 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 298 of 1
Current timestep = 299. State = [[-0.16780612 -0.13086933]]. Action = [[ 0.02579308 -0.22881642 -0.19641954 -0.19133806]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 299 is [True, False, False, True, False, False]
Current timestep = 300. State = [[-0.17068163 -0.13622667]]. Action = [[-0.11929309  0.13909543  0.00347462 -0.7923978 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 300 is [True, False, False, True, False, False]
Current timestep = 301. State = [[-0.16991347 -0.14239381]]. Action = [[ 0.22534809 -0.2169902   0.22219688 -0.8796604 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 301 is [True, False, False, True, False, False]
Current timestep = 302. State = [[-0.15502398 -0.15932871]]. Action = [[ 0.16154301 -0.10551141 -0.23112737  0.35512865]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 302 is [True, False, False, True, False, False]
Current timestep = 303. State = [[-0.14552025 -0.16778484]]. Action = [[-0.12173007  0.01877889  0.21050316 -0.8449894 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 303 is [True, False, False, True, False, False]
Current timestep = 304. State = [[-0.15485685 -0.16451198]]. Action = [[-0.21734646  0.11243516 -0.04459625  0.59738755]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 304 is [True, False, False, True, False, False]
Scene graph at timestep 304 is [True, False, False, True, False, False]
State prediction error at timestep 304 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 304 of 0
Current timestep = 305. State = [[-0.1679821  -0.16720617]]. Action = [[ 0.06787473 -0.1658191  -0.18684423 -0.5483405 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 305 is [True, False, False, True, False, False]
Scene graph at timestep 305 is [True, False, False, True, False, False]
State prediction error at timestep 305 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of -1
Current timestep = 306. State = [[-0.15526739 -0.16406743]]. Action = [[ 0.24482614  0.23244691 -0.21670459  0.5769603 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 306 is [True, False, False, True, False, False]
Current timestep = 307. State = [[-0.13583271 -0.15143488]]. Action = [[ 0.16955638 -0.03400737  0.01999262  0.38024914]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 307 is [True, False, False, True, False, False]
Current timestep = 308. State = [[-0.12375861 -0.14490426]]. Action = [[-0.05269787  0.08620521 -0.10735883  0.3031813 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 308 is [True, False, False, True, False, False]
Current timestep = 309. State = [[-0.11674306 -0.1421991 ]]. Action = [[ 0.1527248  -0.03939509  0.19444188 -0.7046461 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 309 is [True, False, False, True, False, False]
Current timestep = 310. State = [[-0.1095795  -0.13184598]]. Action = [[-0.15872765  0.18505919  0.23168522  0.87784505]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 310 is [True, False, False, True, False, False]
Current timestep = 311. State = [[-0.11364906 -0.12737931]]. Action = [[-0.06938198 -0.10170488  0.22313267 -0.33189118]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 311 is [True, False, False, True, False, False]
Scene graph at timestep 311 is [True, False, False, True, False, False]
State prediction error at timestep 311 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 311 of 1
Current timestep = 312. State = [[-0.11045934 -0.1262921 ]]. Action = [[ 0.21842843  0.08407289 -0.06881934 -0.86590576]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 312 is [True, False, False, True, False, False]
Scene graph at timestep 312 is [True, False, False, True, False, False]
State prediction error at timestep 312 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 312 of 1
Current timestep = 313. State = [[-0.10813171 -0.12240639]]. Action = [[-0.20315072 -0.00990683 -0.24787675 -0.5876981 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 313 is [True, False, False, True, False, False]
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 313 of -1
Current timestep = 314. State = [[-0.12098855 -0.11529161]]. Action = [[-0.18598637  0.14767689 -0.10917222  0.70428073]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 314 is [True, False, False, False, True, False]
Current timestep = 315. State = [[-0.12838417 -0.09547111]]. Action = [[ 0.21379828  0.12901336 -0.13009545  0.01764011]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 315 is [True, False, False, False, True, False]
Current timestep = 316. State = [[-0.11776946 -0.08289759]]. Action = [[ 0.13955534  0.02889487 -0.24374487  0.33400297]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 316 is [True, False, False, False, True, False]
Current timestep = 317. State = [[-0.1114632  -0.07747453]]. Action = [[-0.02090435  0.02887765 -0.07891959  0.5705993 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 317 is [True, False, False, False, True, False]
Current timestep = 318. State = [[-0.11341579 -0.07193347]]. Action = [[-0.14383648  0.04921466 -0.1931975   0.40034175]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 318 is [True, False, False, False, True, False]
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 318 of 1
Current timestep = 319. State = [[-0.11852341 -0.06316315]]. Action = [[-0.03530538  0.06182337 -0.06658813 -0.529511  ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 319 is [True, False, False, False, True, False]
Current timestep = 320. State = [[-0.12603372 -0.05862328]]. Action = [[-0.15487787  0.00119999 -0.06715938  0.29773092]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 320 is [True, False, False, False, True, False]
Current timestep = 321. State = [[-0.13070583 -0.04357838]]. Action = [[ 0.19004697  0.2150777  -0.1411879  -0.9689618 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 321 is [True, False, False, False, True, False]
Current timestep = 322. State = [[-0.11998518 -0.03467902]]. Action = [[ 0.21134788 -0.11847803  0.1031141   0.8162465 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 322 is [True, False, False, False, True, False]
Current timestep = 323. State = [[-0.11145602 -0.04328622]]. Action = [[-0.16646008 -0.10709293  0.03805602 -0.7979778 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 323 is [True, False, False, False, True, False]
Current timestep = 324. State = [[-0.10723497 -0.05415061]]. Action = [[ 0.1985203  -0.08414872 -0.09849477  0.5246334 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 324 is [True, False, False, False, True, False]
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 324 of 0
Current timestep = 325. State = [[-0.09658428 -0.05421611]]. Action = [[ 0.02169269  0.17932338 -0.1732387   0.42108727]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 325 is [True, False, False, False, True, False]
Current timestep = 326. State = [[-0.10117084 -0.05847696]]. Action = [[-0.23569843 -0.22002594 -0.18149713 -0.53667325]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 326 is [True, False, False, False, True, False]
Current timestep = 327. State = [[-0.115065   -0.06772861]]. Action = [[-0.11913204  0.03588855  0.1680823   0.95893264]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 327 is [True, False, False, False, True, False]
Current timestep = 328. State = [[-0.12024234 -0.07838465]]. Action = [[ 0.12028611 -0.17050667 -0.22594953  0.61182356]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 328 is [True, False, False, False, True, False]
Current timestep = 329. State = [[-0.11691169 -0.08477736]]. Action = [[ 0.07680255  0.04988194 -0.0295507  -0.9370175 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 329 is [True, False, False, False, True, False]
Current timestep = 330. State = [[-0.12439813 -0.09905602]]. Action = [[-0.21048626 -0.24047683  0.07105109 -0.85525465]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 330 is [True, False, False, False, True, False]
Current timestep = 331. State = [[-0.1271011  -0.12254525]]. Action = [[ 0.17750704 -0.14770296 -0.23055084 -0.7103595 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 331 is [True, False, False, False, True, False]
Current timestep = 332. State = [[-0.12478758 -0.1473397 ]]. Action = [[-0.06740429 -0.19587009 -0.14002831 -0.3765483 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 332 is [True, False, False, False, True, False]
Current timestep = 333. State = [[-0.12186243 -0.1678499 ]]. Action = [[ 0.09772819 -0.04786149  0.02714086 -0.21346837]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 333 is [True, False, False, True, False, False]
Scene graph at timestep 333 is [True, False, False, True, False, False]
State prediction error at timestep 333 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 333 of -1
Current timestep = 334. State = [[-0.11933828 -0.18220642]]. Action = [[-0.05848464 -0.12463471 -0.18901247  0.84379137]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 334 is [True, False, False, True, False, False]
Current timestep = 335. State = [[-0.11831532 -0.20204753]]. Action = [[ 0.09579799 -0.18810405  0.18819737  0.764869  ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 335 is [True, False, False, True, False, False]
Current timestep = 336. State = [[-0.2675142  0.2186942]]. Action = [[ 0.21372497 -0.07267386 -0.12955324  0.5130198 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 336 is [True, False, False, True, False, False]
Scene graph at timestep 336 is [True, False, False, False, False, True]
State prediction error at timestep 336 is tensor(0.0864, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of -1
Current timestep = 337. State = [[-0.26158702  0.24576947]]. Action = [[-0.10000724  0.18571746  0.20300192 -0.21980882]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 337 is [True, False, False, False, False, True]
Current timestep = 338. State = [[-0.26158702  0.24576947]]. Action = [[-0.23372228 -0.19612081  0.09953114  0.39675975]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 338 is [True, False, False, False, False, True]
Current timestep = 339. State = [[-0.26478603  0.24923913]]. Action = [[-0.07247666  0.03624246 -0.11457652 -0.9598737 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 339 is [True, False, False, False, False, True]
Current timestep = 340. State = [[-0.2684171  0.2528776]]. Action = [[-0.15201627  0.17266327 -0.03906527 -0.4618429 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 340 is [True, False, False, False, False, True]
Current timestep = 341. State = [[-0.26413763  0.24992625]]. Action = [[ 0.11702821 -0.0488013  -0.10572246  0.8606639 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 341 is [True, False, False, False, False, True]
Scene graph at timestep 341 is [True, False, False, False, False, True]
State prediction error at timestep 341 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 341 of -1
Current timestep = 342. State = [[-0.2546184   0.24086854]]. Action = [[ 0.11936656 -0.04813504  0.02195796 -0.50018847]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 342 is [True, False, False, False, False, True]
Scene graph at timestep 342 is [True, False, False, False, False, True]
State prediction error at timestep 342 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 342 of 0
Current timestep = 343. State = [[-0.24021964  0.24091135]]. Action = [[-0.09495506 -0.00711207  0.19120961 -0.8409279 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 343 is [True, False, False, False, False, True]
Scene graph at timestep 343 is [True, False, False, False, False, True]
State prediction error at timestep 343 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 343 of -1
Current timestep = 344. State = [[-0.25094065  0.2535743 ]]. Action = [[-0.09962043  0.21382517 -0.14553474 -0.22819662]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 344 is [True, False, False, False, False, True]
Current timestep = 345. State = [[-0.2530219   0.26758263]]. Action = [[ 0.21656698  0.0577395  -0.16201903  0.22886837]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 345 is [True, False, False, False, False, True]
Current timestep = 346. State = [[-0.24336034  0.28391212]]. Action = [[-0.05417261  0.16333437 -0.03810404 -0.5793791 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 346 is [True, False, False, False, False, True]
Current timestep = 347. State = [[-0.24630783  0.29705983]]. Action = [[-0.06157038 -0.00857952 -0.10178618  0.19519758]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 347 is [True, False, False, False, False, True]
Current timestep = 348. State = [[-0.24869092  0.29881468]]. Action = [[ 0.11514893  0.06345373 -0.03955881 -0.90580547]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 348 is [True, False, False, False, False, True]
Current timestep = 349. State = [[-0.24900834  0.29922113]]. Action = [[ 0.23389062  0.06154418 -0.2293586   0.41055083]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 349 is [True, False, False, False, False, True]
Current timestep = 350. State = [[-0.24917133  0.29934934]]. Action = [[1.8059802e-01 1.6488653e-01 4.8029423e-04 8.8515830e-01]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 350 is [True, False, False, False, False, True]
Current timestep = 351. State = [[-0.24924342  0.29930797]]. Action = [[ 0.13560015  0.24466002  0.02454165 -0.42667758]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 351 is [True, False, False, False, False, True]
Current timestep = 352. State = [[-0.24924342  0.29930797]]. Action = [[ 0.01652429  0.0912829  -0.23865332  0.12544191]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 352 is [True, False, False, False, False, True]
Current timestep = 353. State = [[-0.25158718  0.300712  ]]. Action = [[-0.08172688 -0.0138576   0.08094338  0.9295747 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 353 is [True, False, False, False, False, True]
Scene graph at timestep 353 is [True, False, False, False, False, True]
State prediction error at timestep 353 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of -1
Current timestep = 354. State = [[-0.254952    0.30159384]]. Action = [[-0.19375342  0.05689994  0.13866913 -0.19699728]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 354 is [True, False, False, False, False, True]
Current timestep = 355. State = [[-0.25502676  0.3015379 ]]. Action = [[-0.22429794 -0.13299577  0.02687433  0.951236  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 355 is [True, False, False, False, False, True]
Scene graph at timestep 355 is [True, False, False, False, False, True]
State prediction error at timestep 355 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 355 of -1
Current timestep = 356. State = [[-0.25503394  0.30151197]]. Action = [[0.2212694  0.21288452 0.02977073 0.02645159]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 356 is [True, False, False, False, False, True]
Current timestep = 357. State = [[-0.25503394  0.30151197]]. Action = [[0.19375446 0.04263189 0.17602098 0.8449873 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 357 is [True, False, False, False, False, True]
Current timestep = 358. State = [[-0.2528359   0.29603592]]. Action = [[-0.01019579 -0.12967266 -0.02033058 -0.3208279 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 358 is [True, False, False, False, False, True]
Scene graph at timestep 358 is [True, False, False, False, False, True]
State prediction error at timestep 358 is tensor(3.2810e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 358 of 0
Current timestep = 359. State = [[-0.25043318  0.28843725]]. Action = [[-0.00325629  0.11723992  0.14781451 -0.17004538]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 359 is [True, False, False, False, False, True]
Current timestep = 360. State = [[-0.2504028   0.28837514]]. Action = [[-0.1605682   0.05820152 -0.13071907  0.29335022]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 360 is [True, False, False, False, False, True]
Current timestep = 361. State = [[-0.24727194  0.27979612]]. Action = [[ 0.01803571 -0.15232582  0.04320064 -0.9139192 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 361 is [True, False, False, False, False, True]
Current timestep = 362. State = [[-0.24270277  0.25694507]]. Action = [[ 0.01242331 -0.23775673 -0.07649764 -0.45320123]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 362 is [True, False, False, False, False, True]
Current timestep = 363. State = [[-0.2416199   0.23371918]]. Action = [[-0.03699403 -0.08960265 -0.06142658  0.513839  ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 363 is [True, False, False, False, False, True]
Current timestep = 364. State = [[-0.24172442  0.22188154]]. Action = [[ 0.02345106 -0.04774088 -0.10071886  0.78121614]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 364 is [True, False, False, False, False, True]
Current timestep = 365. State = [[-0.24004531  0.21862292]]. Action = [[ 0.02621299  0.04258054  0.11424699 -0.4965365 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 365 is [True, False, False, False, False, True]
Current timestep = 366. State = [[-0.24107648  0.22183429]]. Action = [[-0.02516359  0.04886639 -0.03345624  0.74505734]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 366 is [True, False, False, False, False, True]
Current timestep = 367. State = [[-0.24222907  0.22437595]]. Action = [[-0.22649626  0.22658092 -0.18124208  0.30867887]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 367 is [True, False, False, False, False, True]
Current timestep = 368. State = [[-0.2350822   0.21471289]]. Action = [[ 0.21530661 -0.16178608  0.14906675 -0.27045608]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 368 is [True, False, False, False, False, True]
Scene graph at timestep 368 is [True, False, False, False, False, True]
State prediction error at timestep 368 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of 1
Current timestep = 369. State = [[-0.23036101  0.20609328]]. Action = [[-0.15490276  0.07711169  0.05069482  0.85306203]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 369 is [True, False, False, False, False, True]
Current timestep = 370. State = [[-0.23478796  0.21373399]]. Action = [[ 0.07485798  0.09315783 -0.18648827  0.04008377]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 370 is [True, False, False, False, False, True]
Current timestep = 371. State = [[-0.22628954  0.22411135]]. Action = [[ 0.24331689  0.13603312  0.08461189 -0.7844757 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 371 is [True, False, False, False, False, True]
Current timestep = 372. State = [[-0.20297664  0.24547504]]. Action = [[ 0.1643337   0.18962732 -0.11807773  0.17634213]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 372 is [True, False, False, False, False, True]
Current timestep = 373. State = [[-0.18848874  0.25112692]]. Action = [[-0.08769765 -0.19823918  0.24653208 -0.26333016]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 373 is [True, False, False, False, False, True]
Current timestep = 374. State = [[-0.19399688  0.24418911]]. Action = [[-0.20175159 -0.06243557 -0.2053558  -0.81047726]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 374 is [True, False, False, False, False, True]
Current timestep = 375. State = [[-0.20448011  0.24896078]]. Action = [[ 0.05963314  0.204505   -0.06562132  0.03659046]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 375 is [True, False, False, False, False, True]
Scene graph at timestep 375 is [True, False, False, False, False, True]
State prediction error at timestep 375 is tensor(5.2238e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of 0
Current timestep = 376. State = [[-0.20736256  0.2515597 ]]. Action = [[ 0.00225234 -0.15552577  0.05089858  0.4857911 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 376 is [True, False, False, False, False, True]
Current timestep = 377. State = [[-0.20963007  0.2525927 ]]. Action = [[-0.07599571  0.1228188  -0.09100956 -0.6811491 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 377 is [True, False, False, False, False, True]
Current timestep = 378. State = [[-0.21580686  0.24663158]]. Action = [[-0.12991597 -0.23151219 -0.18176788 -0.74297976]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 378 is [True, False, False, False, False, True]
Current timestep = 379. State = [[-0.2220332   0.22280437]]. Action = [[-0.03877294 -0.2056604   0.0730451   0.71244454]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 379 is [True, False, False, False, False, True]
Current timestep = 380. State = [[-0.2227972   0.21202162]]. Action = [[0.18492815 0.13648617 0.20274341 0.49840963]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 380 is [True, False, False, False, False, True]
Current timestep = 381. State = [[-0.21102725  0.22416735]]. Action = [[0.21619439 0.19334906 0.2415024  0.612195  ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 381 is [True, False, False, False, False, True]
Current timestep = 382. State = [[-0.20086108  0.23268321]]. Action = [[-0.17922558 -0.08420154  0.09638044  0.7413434 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 382 is [True, False, False, False, False, True]
Current timestep = 383. State = [[-0.2014332   0.23324873]]. Action = [[ 0.08569843  0.04404473  0.20079994 -0.3952862 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 383 is [True, False, False, False, False, True]
Current timestep = 384. State = [[-0.20420212  0.23671088]]. Action = [[-0.12488228  0.01056257 -0.15559463 -0.06718761]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 384 is [True, False, False, False, False, True]
Current timestep = 385. State = [[-0.21542764  0.23765005]]. Action = [[-0.21114172 -0.08834141 -0.03316207  0.87935483]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 385 is [True, False, False, False, False, True]
Current timestep = 386. State = [[-0.22363187  0.23879053]]. Action = [[ 0.20921984  0.15502906 -0.17337151  0.08163834]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 386 is [True, False, False, False, False, True]
Scene graph at timestep 386 is [True, False, False, False, False, True]
State prediction error at timestep 386 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.22201431  0.24267614]]. Action = [[-0.09758613 -0.04041621  0.22965527 -0.86082804]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 387 is [True, False, False, False, False, True]
Current timestep = 388. State = [[-0.22872224  0.24569134]]. Action = [[-0.1457577   0.00289392  0.21172893  0.94207215]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 388 is [True, False, False, False, False, True]
Current timestep = 389. State = [[-0.23170364  0.25075257]]. Action = [[ 0.21222132  0.07988983 -0.01976524  0.03333855]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 389 is [True, False, False, False, False, True]
Current timestep = 390. State = [[-0.22943571  0.25092113]]. Action = [[-0.02272199 -0.04465692 -0.17766587 -0.7272734 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 390 is [True, False, False, False, False, True]
Current timestep = 391. State = [[-0.22721905  0.252603  ]]. Action = [[ 0.08582076  0.06969544 -0.24352673 -0.83974564]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 391 is [True, False, False, False, False, True]
Current timestep = 392. State = [[-0.2252953  0.2494876]]. Action = [[-0.12361494 -0.13749652  0.10703015  0.36770427]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 392 is [True, False, False, False, False, True]
Current timestep = 393. State = [[-0.22097753  0.23807172]]. Action = [[ 0.06609368 -0.10336819  0.02591011 -0.9202159 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 393 is [True, False, False, False, False, True]
Current timestep = 394. State = [[-0.22446734  0.22621578]]. Action = [[-0.1806949  -0.08396998  0.18705738 -0.86382467]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 394 is [True, False, False, False, False, True]
Scene graph at timestep 394 is [True, False, False, False, False, True]
State prediction error at timestep 394 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 394 of 0
Current timestep = 395. State = [[-0.2348453   0.21984541]]. Action = [[-0.09860787 -0.013005   -0.13504755 -0.91879666]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 395 is [True, False, False, False, False, True]
Current timestep = 396. State = [[-0.24452946  0.21417941]]. Action = [[-0.06684744 -0.07247791 -0.0673281  -0.85690606]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 396 is [True, False, False, False, False, True]
Current timestep = 397. State = [[-0.25111616  0.2074631 ]]. Action = [[-0.1941406  -0.21830356 -0.21889277 -0.00764865]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 397 is [True, False, False, False, False, True]
Current timestep = 398. State = [[-0.25163022  0.21327662]]. Action = [[ 0.11525965  0.17967683 -0.16980894  0.5154505 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 398 is [True, False, False, False, False, True]
Current timestep = 399. State = [[-0.24950354  0.211836  ]]. Action = [[ 0.02995205 -0.16469356 -0.13958807  0.09600091]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 399 is [True, False, False, False, False, True]
Current timestep = 400. State = [[-0.24729201  0.20723724]]. Action = [[-0.21364562  0.03993276  0.08026934 -0.41618025]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 400 is [True, False, False, False, False, True]
Current timestep = 401. State = [[-0.23807149  0.19407456]]. Action = [[ 0.17810524 -0.17699704  0.17831618 -0.73550224]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 401 is [True, False, False, False, False, True]
Scene graph at timestep 401 is [True, False, False, False, False, True]
State prediction error at timestep 401 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 401 of 0
Current timestep = 402. State = [[-0.22268978  0.18835618]]. Action = [[ 0.08731991  0.24227771  0.17934197 -0.32871616]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 402 is [True, False, False, False, False, True]
Current timestep = 403. State = [[-0.21530312  0.21271375]]. Action = [[ 0.07646966  0.22275245  0.1363354  -0.51222014]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 403 is [True, False, False, False, False, True]
Current timestep = 404. State = [[-0.21430069  0.23180035]]. Action = [[-0.16219264 -0.02041726 -0.2055556   0.9744644 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 404 is [True, False, False, False, False, True]
Current timestep = 405. State = [[-0.21599764  0.23958698]]. Action = [[ 0.13141268  0.09645337  0.23948267 -0.72017163]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 405 is [True, False, False, False, False, True]
Scene graph at timestep 405 is [True, False, False, False, False, True]
State prediction error at timestep 405 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of 0
Current timestep = 406. State = [[-0.21327333  0.2462216 ]]. Action = [[-0.08590147 -0.02969506  0.20489138  0.37216997]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 406 is [True, False, False, False, False, True]
Scene graph at timestep 406 is [True, False, False, False, False, True]
State prediction error at timestep 406 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 406 of -1
Current timestep = 407. State = [[-0.2178487  0.2472855]]. Action = [[-0.15598734 -0.04516314 -0.17782038 -0.33025187]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 407 is [True, False, False, False, False, True]
Current timestep = 408. State = [[-0.23801488  0.2568163 ]]. Action = [[-0.22258961  0.13970858 -0.07438381  0.8612633 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 408 is [True, False, False, False, False, True]
Current timestep = 409. State = [[-0.25593063  0.26669696]]. Action = [[-0.22087115  0.06863415 -0.13692151  0.27645886]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 409 is [True, False, False, False, False, True]
Current timestep = 410. State = [[-0.26143998  0.27250057]]. Action = [[ 0.07480752  0.14600441 -0.09957084  0.05068541]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 410 is [True, False, False, False, False, True]
Current timestep = 411. State = [[-0.25559694  0.27960765]]. Action = [[ 0.1905173   0.04237136 -0.11733402  0.29301882]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 411 is [True, False, False, False, False, True]
Current timestep = 412. State = [[-0.23422149  0.28034326]]. Action = [[ 0.2455239  -0.06797768  0.13579845 -0.5506434 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 412 is [True, False, False, False, False, True]
Current timestep = 413. State = [[-0.20472501  0.26374602]]. Action = [[ 0.21922818 -0.19779526  0.06541702 -0.8238796 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 413 is [True, False, False, False, False, True]
Scene graph at timestep 413 is [True, False, False, False, False, True]
State prediction error at timestep 413 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 413 of 1
Current timestep = 414. State = [[-0.18436833  0.24964382]]. Action = [[-0.22518739 -0.04553241 -0.18349124 -0.09765261]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 414 is [True, False, False, False, False, True]
Scene graph at timestep 414 is [True, False, False, False, False, True]
State prediction error at timestep 414 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 414 of -1
Current timestep = 415. State = [[-0.19869395  0.25927278]]. Action = [[-0.164657    0.18429762 -0.24195498 -0.6697272 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 415 is [True, False, False, False, False, True]
Scene graph at timestep 415 is [True, False, False, False, False, True]
State prediction error at timestep 415 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of -1
Current timestep = 416. State = [[-0.20531863  0.26103362]]. Action = [[ 0.1551837  -0.2271952  -0.08171424 -0.84004   ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 416 is [True, False, False, False, False, True]
Scene graph at timestep 416 is [True, False, False, False, False, True]
State prediction error at timestep 416 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 416 of 1
Current timestep = 417. State = [[-0.19293118  0.24960986]]. Action = [[ 0.17530036  0.13710773  0.00851834 -0.9303911 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 417 is [True, False, False, False, False, True]
Current timestep = 418. State = [[-0.18932669  0.24204059]]. Action = [[-0.10398363 -0.22996882 -0.15160322 -0.8054192 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 418 is [True, False, False, False, False, True]
Current timestep = 419. State = [[-0.1823408  0.2377111]]. Action = [[ 0.20099056  0.13178036 -0.03231703 -0.4829173 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 419 is [True, False, False, False, False, True]
Scene graph at timestep 419 is [True, False, False, False, False, True]
State prediction error at timestep 419 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 419 of 0
Current timestep = 420. State = [[-0.1752922   0.23896304]]. Action = [[-0.10245655 -0.08947577 -0.05886787 -0.5395171 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 420 is [True, False, False, False, False, True]
Current timestep = 421. State = [[-0.17535146  0.22190563]]. Action = [[-0.03517321 -0.23223877  0.1313256  -0.75056106]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 421 is [True, False, False, False, False, True]
Current timestep = 422. State = [[-0.16660105  0.20061426]]. Action = [[ 0.2090868  -0.06262459  0.04975498  0.6494901 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 422 is [True, False, False, False, False, True]
Scene graph at timestep 422 is [True, False, False, False, False, True]
State prediction error at timestep 422 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 422 of 1
Current timestep = 423. State = [[-0.15273511  0.17740597]]. Action = [[ 0.02923813 -0.20842716 -0.19777727 -0.29689133]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 423 is [True, False, False, False, False, True]
Current timestep = 424. State = [[-0.13859573  0.16233702]]. Action = [[ 0.22971523  0.05137286  0.01428673 -0.5331285 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 424 is [True, False, False, False, False, True]
Current timestep = 425. State = [[-0.13189065  0.1604528 ]]. Action = [[-0.17180717 -0.02514085 -0.0625236  -0.857375  ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 425 is [True, False, False, False, False, True]
Scene graph at timestep 425 is [True, False, False, False, False, True]
State prediction error at timestep 425 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 425 of 1
Current timestep = 426. State = [[-0.14062536  0.17133479]]. Action = [[-0.17609155  0.17235899 -0.19968411  0.8224579 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 426 is [True, False, False, False, False, True]
Current timestep = 427. State = [[-0.160701   0.1841565]]. Action = [[-0.23640598 -0.03805412  0.15807083  0.02201867]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 427 is [True, False, False, False, False, True]
Scene graph at timestep 427 is [True, False, False, False, False, True]
State prediction error at timestep 427 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 427 of -1
Current timestep = 428. State = [[-0.18113577  0.18320103]]. Action = [[ 0.09184098 -0.00070365  0.0605022   0.3660822 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 428 is [True, False, False, False, False, True]
Current timestep = 429. State = [[-0.17191027  0.1855786 ]]. Action = [[ 0.24284124  0.08536443 -0.13615492  0.04634333]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 429 is [True, False, False, False, False, True]
Current timestep = 430. State = [[-0.15608546  0.18353346]]. Action = [[ 0.11156827 -0.07458481  0.1328572   0.18692136]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 430 is [True, False, False, False, False, True]
Scene graph at timestep 430 is [True, False, False, False, False, True]
State prediction error at timestep 430 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 430 of 0
Current timestep = 431. State = [[-0.1453097   0.17279412]]. Action = [[-0.13245769 -0.13656257  0.00635546 -0.72951096]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 431 is [True, False, False, False, False, True]
Current timestep = 432. State = [[-0.14606644  0.17698884]]. Action = [[ 0.06023091  0.20229548  0.2163699  -0.02910745]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 432 is [True, False, False, False, False, True]
Scene graph at timestep 432 is [True, False, False, False, False, True]
State prediction error at timestep 432 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 432 of 0
Current timestep = 433. State = [[-0.14083377  0.19736564]]. Action = [[ 0.21508294  0.22778124 -0.15450047 -0.26233852]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 433 is [True, False, False, False, False, True]
Current timestep = 434. State = [[-0.12080146  0.22310634]]. Action = [[ 0.12429968  0.14845717 -0.0374777  -0.19471717]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 434 is [True, False, False, False, False, True]
Current timestep = 435. State = [[-0.11311228  0.23695838]]. Action = [[-0.20821522 -0.0634768  -0.19113325  0.74979115]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 435 is [True, False, False, False, False, True]
Scene graph at timestep 435 is [True, False, False, False, False, True]
State prediction error at timestep 435 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 435 of 0
Current timestep = 436. State = [[-0.11255214  0.23945566]]. Action = [[0.18281707 0.05892244 0.1799528  0.28549492]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 436 is [True, False, False, False, False, True]
Scene graph at timestep 436 is [True, False, False, False, False, True]
State prediction error at timestep 436 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 436 of 0
Current timestep = 437. State = [[-0.10196865  0.24713685]]. Action = [[ 0.22396839  0.12606895 -0.23049152  0.15244603]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 437 is [True, False, False, False, False, True]
Current timestep = 438. State = [[-0.08757258  0.2728519 ]]. Action = [[-0.1425644   0.23080704 -0.03197682 -0.9254747 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 438 is [True, False, False, False, False, True]
Current timestep = 439. State = [[-0.08914221  0.28853437]]. Action = [[-0.2105847   0.182477   -0.04446115 -0.02628422]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 439 is [True, False, False, False, False, True]
Current timestep = 440. State = [[-0.09554709  0.29753724]]. Action = [[-0.17212701  0.05301178 -0.07868111  0.301391  ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 440 is [True, False, False, False, False, True]
Current timestep = 441. State = [[-0.10155092  0.3054714 ]]. Action = [[ 0.09025368  0.21629521  0.18935427 -0.36201197]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 441 is [True, False, False, False, False, True]
Current timestep = 442. State = [[-0.10235889  0.30650273]]. Action = [[-0.16211571  0.08548191 -0.11196944 -0.0663628 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 442 is [True, False, False, False, False, True]
Current timestep = 443. State = [[-0.10215667  0.3052798 ]]. Action = [[-0.03821111 -0.05327839 -0.02284163  0.38282478]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 443 is [True, False, False, False, False, True]
Current timestep = 444. State = [[-0.10225005  0.3044526 ]]. Action = [[ 0.08268791  0.0181497  -0.10492232 -0.36836135]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 444 is [True, False, False, False, False, True]
Current timestep = 445. State = [[-0.10231298  0.30443186]]. Action = [[-0.18410687  0.19804153  0.10414931  0.9026102 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 445 is [True, False, False, False, False, True]
Current timestep = 446. State = [[-0.09905498  0.29546228]]. Action = [[ 0.06646505 -0.1349949  -0.04454678 -0.51514995]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 446 is [True, False, False, False, False, True]
Current timestep = 447. State = [[-0.09715965  0.2877561 ]]. Action = [[0.07184955 0.09575641 0.14207023 0.67405736]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 447 is [True, False, False, False, False, True]
Current timestep = 448. State = [[-0.09766646  0.28650773]]. Action = [[ 0.15634722  0.18965852  0.07235089 -0.20604116]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 448 is [True, False, False, False, False, True]
Current timestep = 449. State = [[-0.09397143  0.28484645]]. Action = [[0.19882584 0.05685267 0.05956528 0.6667032 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 449 is [True, False, False, False, False, True]
Current timestep = 450. State = [[-0.08722024  0.28172895]]. Action = [[-0.06731769 -0.09140337 -0.2086931  -0.93742967]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 450 is [True, False, False, False, False, True]
Current timestep = 451. State = [[-0.08166801  0.27357653]]. Action = [[ 0.09538144 -0.08380648 -0.08146283 -0.33609176]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 451 is [True, False, False, False, False, True]
Current timestep = 452. State = [[-0.07142282  0.26517287]]. Action = [[ 0.16639772  0.0512315  -0.1970111  -0.3226598 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 452 is [True, False, False, False, False, True]
Current timestep = 453. State = [[-0.07076844  0.27799007]]. Action = [[-0.22579801  0.18493253 -0.04873586  0.33833098]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 453 is [True, False, False, False, False, True]
Current timestep = 454. State = [[-0.07674189  0.27887523]]. Action = [[-0.03076692 -0.22791398 -0.16144392  0.5107523 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 454 is [True, False, False, False, False, True]
Scene graph at timestep 454 is [True, False, False, False, False, True]
State prediction error at timestep 454 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 454 of 0
Current timestep = 455. State = [[-0.0749846   0.26605344]]. Action = [[ 0.16042113  0.0305419  -0.0956623   0.5153867 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 455 is [True, False, False, False, False, True]
Current timestep = 456. State = [[-0.07382452  0.26632702]]. Action = [[-0.01822239  0.04296428  0.05111036 -0.30685163]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 456 is [True, False, False, False, False, True]
Current timestep = 457. State = [[-0.069676    0.27369946]]. Action = [[ 0.11294419  0.1113956  -0.1870948   0.82609046]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 457 is [True, False, False, False, False, True]
Current timestep = 458. State = [[-0.06090055  0.2854651 ]]. Action = [[ 0.01612547  0.01446223 -0.139436    0.71374416]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 458 is [True, False, False, False, False, True]
Current timestep = 459. State = [[-0.05870825  0.28821012]]. Action = [[ 0.2081716   0.2015335  -0.16082275 -0.19125557]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 459 is [True, False, False, False, False, True]
Current timestep = 460. State = [[-0.05783875  0.28096122]]. Action = [[-0.13165765 -0.17269365  0.01998445  0.40324736]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 460 is [True, False, False, False, False, True]
Scene graph at timestep 460 is [True, False, False, False, False, True]
State prediction error at timestep 460 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of 0
Current timestep = 461. State = [[-0.06403522  0.26799396]]. Action = [[-0.22123878 -0.09807917  0.00370821 -0.6754342 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 461 is [True, False, False, False, False, True]
Current timestep = 462. State = [[-0.07898466  0.24690466]]. Action = [[-0.13703537 -0.23348333 -0.10725644  0.55256844]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 462 is [True, False, False, False, False, True]
Current timestep = 463. State = [[-0.24645174  0.01568264]]. Action = [[-0.08767012  0.08148935  0.1847507   0.25858164]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 463 is [True, False, False, False, False, True]
Current timestep = 464. State = [[-0.23302734  0.02426749]]. Action = [[ 0.20133156  0.09868023 -0.2128108  -0.40218937]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 464 is [True, False, False, False, True, False]
Current timestep = 465. State = [[-0.20707755  0.02981264]]. Action = [[ 0.23991773 -0.03194436  0.18724674 -0.30942655]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 465 is [True, False, False, False, True, False]
Scene graph at timestep 465 is [True, False, False, False, True, False]
State prediction error at timestep 465 is tensor(3.2246e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of 1
Current timestep = 466. State = [[-0.16890523  0.02723948]]. Action = [[ 0.24644631 -0.06943747 -0.08792113  0.8445728 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 466 is [True, False, False, False, True, False]
Scene graph at timestep 466 is [True, False, False, False, True, False]
State prediction error at timestep 466 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 466 of 1
Current timestep = 467. State = [[-0.14760469  0.02906625]]. Action = [[-0.06163141  0.09262466 -0.04672211 -0.10003865]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 467 is [True, False, False, False, True, False]
Current timestep = 468. State = [[-0.14739528  0.02441721]]. Action = [[ 0.00634253 -0.15292628  0.0317578  -0.87886626]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 468 is [True, False, False, False, True, False]
Current timestep = 469. State = [[-0.14297047  0.02191435]]. Action = [[ 0.156068    0.10382286  0.21014833 -0.01315254]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 469 is [True, False, False, False, True, False]
Scene graph at timestep 469 is [True, False, False, False, True, False]
State prediction error at timestep 469 is tensor(8.1602e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 469 of 0
Current timestep = 470. State = [[-0.11990501  0.03189641]]. Action = [[ 0.19646728  0.10630277 -0.02423733 -0.40086782]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 470 is [True, False, False, False, True, False]
Current timestep = 471. State = [[-0.101102    0.03104643]]. Action = [[ 0.03561351 -0.14642026 -0.12673937 -0.35375428]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 471 is [True, False, False, False, True, False]
Current timestep = 472. State = [[-0.08807577  0.02697345]]. Action = [[ 0.18353164  0.01493132 -0.17949986  0.9405887 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 472 is [True, False, False, False, True, False]
Current timestep = 473. State = [[-0.07768663  0.0250195 ]]. Action = [[-0.18696386 -0.02651015 -0.06983988 -0.7428446 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 473 is [True, False, False, False, True, False]
Scene graph at timestep 473 is [True, False, False, False, True, False]
State prediction error at timestep 473 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 473 of 1
Current timestep = 474. State = [[-0.07323249  0.02076742]]. Action = [[ 0.2058376  -0.03127055 -0.06688221  0.51512456]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 474 is [True, False, False, False, True, False]
Current timestep = 475. State = [[-0.06052341  0.03041002]]. Action = [[ 0.22957253  0.21713907 -0.01744834  0.9260708 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 475 is [True, False, False, False, True, False]
Current timestep = 476. State = [[-0.03768001  0.03084241]]. Action = [[-0.02513327 -0.19700617 -0.22800976 -0.3860857 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 476 is [True, False, False, False, True, False]
Current timestep = 477. State = [[-0.15218405 -0.199323  ]]. Action = [[ 0.00741851 -0.05053806 -0.0859219  -0.6169785 ]]. Reward = [100.]
Curr episode timestep = 13
Scene graph at timestep 477 is [False, True, False, False, True, False]
Current timestep = 478. State = [[-0.13314146 -0.22390611]]. Action = [[ 0.07482338 -0.05695042 -0.14420596 -0.23903322]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 478 is [True, False, False, True, False, False]
Current timestep = 479. State = [[-0.12453105 -0.22796138]]. Action = [[0.02677888 0.02101064 0.10357285 0.23515975]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 479 is [True, False, False, True, False, False]
Current timestep = 480. State = [[-0.11834373 -0.22285351]]. Action = [[ 0.02919471  0.12212211 -0.11482373  0.54284334]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 480 is [True, False, False, True, False, False]
Current timestep = 481. State = [[-0.1193341  -0.21338212]]. Action = [[-0.1532895   0.09294373 -0.0810045   0.9502988 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 481 is [True, False, False, True, False, False]
Scene graph at timestep 481 is [True, False, False, True, False, False]
State prediction error at timestep 481 is tensor(5.2104e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 481 of 1
Current timestep = 482. State = [[-0.1153427  -0.19828701]]. Action = [[ 0.21515667  0.11135262 -0.06901959 -0.5954829 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 482 is [True, False, False, True, False, False]
Current timestep = 483. State = [[-0.09875806 -0.1742485 ]]. Action = [[ 0.2385922   0.22720814 -0.22904673 -0.8975404 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 483 is [True, False, False, True, False, False]
Current timestep = 484. State = [[-0.0846386  -0.16255406]]. Action = [[-0.16680133 -0.07156152 -0.20061803  0.13739097]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 484 is [True, False, False, True, False, False]
Current timestep = 485. State = [[-0.08547131 -0.16381006]]. Action = [[-0.02322917  0.00690252 -0.15290178  0.96273065]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 485 is [True, False, False, True, False, False]
Current timestep = 486. State = [[-0.08411454 -0.16525224]]. Action = [[ 0.1208632  -0.04073593  0.1059432   0.47382498]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 486 is [True, False, False, True, False, False]
Current timestep = 487. State = [[-0.08862001 -0.17738476]]. Action = [[-2.1506803e-01 -1.4795645e-01  6.6554546e-04  9.6563303e-01]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 487 is [True, False, False, True, False, False]
Current timestep = 488. State = [[-0.08834723 -0.1920094 ]]. Action = [[ 0.2258071  -0.10134542  0.01784134  0.7070646 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 488 is [True, False, False, True, False, False]
Current timestep = 489. State = [[-0.0788662  -0.20501372]]. Action = [[ 0.05109209 -0.10303953  0.11175135  0.61212003]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 489 is [True, False, False, True, False, False]
Current timestep = 490. State = [[-0.07183569 -0.20249796]]. Action = [[ 0.09267431  0.17336768 -0.14509086 -0.68175304]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 490 is [True, False, False, True, False, False]
Scene graph at timestep 490 is [True, False, False, True, False, False]
State prediction error at timestep 490 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 490 of 0
Current timestep = 491. State = [[-0.0525919  -0.18343465]]. Action = [[ 0.20727289  0.1743395  -0.0806284   0.88983274]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 491 is [True, False, False, True, False, False]
Scene graph at timestep 491 is [True, False, False, True, False, False]
State prediction error at timestep 491 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 491 of 1
Current timestep = 492. State = [[-0.03703545 -0.18317148]]. Action = [[-0.16289976 -0.19559963 -0.22941111  0.5395453 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 492 is [True, False, False, True, False, False]
Current timestep = 493. State = [[-0.0483383  -0.20240232]]. Action = [[-0.19980168 -0.10800654  0.06354365  0.58417344]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 493 is [False, True, False, True, False, False]
Scene graph at timestep 493 is [False, True, False, True, False, False]
State prediction error at timestep 493 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 493 of -1
Current timestep = 494. State = [[-0.05740332 -0.21840389]]. Action = [[ 0.07910737 -0.07526368  0.04935092  0.26036143]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 494 is [False, True, False, True, False, False]
Current timestep = 495. State = [[-0.05288576 -0.2105084 ]]. Action = [[ 0.15959054  0.17926225  0.11494526 -0.96658325]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 495 is [True, False, False, True, False, False]
Current timestep = 496. State = [[-0.05044259 -0.20355807]]. Action = [[-0.1345261  -0.01494759 -0.16065255  0.08237696]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 496 is [True, False, False, True, False, False]
Current timestep = 497. State = [[-0.05745992 -0.21349174]]. Action = [[-0.17556208 -0.14274508  0.20331049 -0.6316204 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 497 is [True, False, False, True, False, False]
Scene graph at timestep 497 is [True, False, False, True, False, False]
State prediction error at timestep 497 is tensor(8.0351e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of 0
Current timestep = 498. State = [[-0.06362993 -0.2119374 ]]. Action = [[0.14572757 0.18704236 0.00486603 0.32842672]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 498 is [True, False, False, True, False, False]
Current timestep = 499. State = [[-0.06073535 -0.18705095]]. Action = [[-0.01210056  0.21778697 -0.17304905  0.6941626 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 499 is [True, False, False, True, False, False]
Scene graph at timestep 499 is [True, False, False, True, False, False]
State prediction error at timestep 499 is tensor(8.3317e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of 1
Current timestep = 500. State = [[-0.06672323 -0.17550647]]. Action = [[-0.23003763 -0.13919759 -0.03451002 -0.00679868]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 500 is [True, False, False, True, False, False]
Current timestep = 501. State = [[-0.08560997 -0.17538135]]. Action = [[-0.22616942  0.1614674   0.24383926  0.31040025]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 501 is [True, False, False, True, False, False]
Scene graph at timestep 501 is [True, False, False, True, False, False]
State prediction error at timestep 501 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[-0.11035065 -0.1718848 ]]. Action = [[-0.06986085 -0.11315048 -0.02884696  0.9713789 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 502 is [True, False, False, True, False, False]
Current timestep = 503. State = [[-0.11571482 -0.1719704 ]]. Action = [[ 0.04438064  0.11032298 -0.09661742 -0.05434746]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 503 is [True, False, False, True, False, False]
Current timestep = 504. State = [[-0.11064158 -0.1637512 ]]. Action = [[ 0.20657939  0.01524219 -0.0727112   0.9142103 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 504 is [True, False, False, True, False, False]
Scene graph at timestep 504 is [True, False, False, True, False, False]
State prediction error at timestep 504 is tensor(2.3703e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 504 of 0
Current timestep = 505. State = [[-0.10449967 -0.15047747]]. Action = [[-0.02481532  0.16165096  0.18663311  0.210886  ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 505 is [True, False, False, True, False, False]
Current timestep = 506. State = [[-0.10904063 -0.12749496]]. Action = [[-0.19021268  0.18927076  0.21616644  0.84580064]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 506 is [True, False, False, True, False, False]
Scene graph at timestep 506 is [True, False, False, True, False, False]
State prediction error at timestep 506 is tensor(5.2938e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 506 of 0
Current timestep = 507. State = [[-0.11050887 -0.09558413]]. Action = [[ 0.22073776  0.2018528  -0.1870512  -0.00327045]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 507 is [True, False, False, True, False, False]
Current timestep = 508. State = [[-0.09460052 -0.07588591]]. Action = [[ 0.24024698  0.08252689 -0.1078569   0.75220275]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 508 is [True, False, False, False, True, False]
Scene graph at timestep 508 is [True, False, False, False, True, False]
State prediction error at timestep 508 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 508 of 1
Current timestep = 509. State = [[-0.06827758 -0.06457433]]. Action = [[0.18167517 0.02278373 0.09889299 0.00634229]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 509 is [True, False, False, False, True, False]
Current timestep = 510. State = [[-0.04481865 -0.04985928]]. Action = [[0.19300154 0.19110793 0.18350655 0.16771138]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 510 is [True, False, False, False, True, False]
Scene graph at timestep 510 is [False, True, False, False, True, False]
State prediction error at timestep 510 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 510 of 1
Current timestep = 511. State = [[-0.23107408 -0.13590145]]. Action = [[ 0.24762276 -0.21683373 -0.15014273  0.14720464]]. Reward = [100.]
Curr episode timestep = 33
Scene graph at timestep 511 is [False, True, False, False, True, False]
Current timestep = 512. State = [[-0.23392051 -0.16449742]]. Action = [[-0.21910332 -0.18457231 -0.20282196 -0.37996483]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 512 is [True, False, False, True, False, False]
Current timestep = 513. State = [[-0.23628551 -0.18238007]]. Action = [[ 0.18759349 -0.08101085  0.07942164 -0.12693083]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 513 is [True, False, False, True, False, False]
Current timestep = 514. State = [[-0.22968975 -0.18267657]]. Action = [[ 0.04206276  0.12403974  0.09404179 -0.06078947]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 514 is [True, False, False, True, False, False]
Current timestep = 515. State = [[-0.23608764 -0.1890507 ]]. Action = [[-0.23567498 -0.14955632 -0.17178942 -0.340891  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 515 is [True, False, False, True, False, False]
Current timestep = 516. State = [[-0.250181   -0.19627133]]. Action = [[-0.15821832  0.05647376  0.2385318   0.08134949]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 516 is [True, False, False, True, False, False]
Current timestep = 517. State = [[-0.26069862 -0.19714604]]. Action = [[-0.2072723  -0.23983254 -0.12583137  0.6904471 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 517 is [True, False, False, True, False, False]
Scene graph at timestep 517 is [True, False, False, True, False, False]
State prediction error at timestep 517 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 517 of -1
Current timestep = 518. State = [[-0.2638871  -0.19675054]]. Action = [[-0.16328822 -0.20028287  0.01212671  0.03042233]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 518 is [True, False, False, True, False, False]
Scene graph at timestep 518 is [True, False, False, True, False, False]
State prediction error at timestep 518 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 518 of -1
Current timestep = 519. State = [[-0.25656128 -0.18655561]]. Action = [[ 0.22839656  0.14737457 -0.20254308 -0.17937768]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 519 is [True, False, False, True, False, False]
Current timestep = 520. State = [[-0.23708713 -0.16784492]]. Action = [[ 0.21889043  0.10438874  0.01332632 -0.3390035 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 520 is [True, False, False, True, False, False]
Current timestep = 521. State = [[-0.21479021 -0.15505429]]. Action = [[ 0.16557205  0.05142283 -0.0101057   0.2383256 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 521 is [True, False, False, True, False, False]
Current timestep = 522. State = [[-0.20360832 -0.15859012]]. Action = [[-0.14956939 -0.13091823  0.09793758  0.5820564 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 522 is [True, False, False, True, False, False]
Current timestep = 523. State = [[-0.2049291  -0.17264654]]. Action = [[ 0.07566488 -0.1631545  -0.05218804 -0.504615  ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 523 is [True, False, False, True, False, False]
Current timestep = 524. State = [[-0.19541605 -0.17547478]]. Action = [[0.19044775 0.13385105 0.03708524 0.90999603]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 524 is [True, False, False, True, False, False]
Current timestep = 525. State = [[-0.18906443 -0.18547519]]. Action = [[-0.14116858 -0.2278455   0.20088416  0.31518316]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 525 is [True, False, False, True, False, False]
Current timestep = 526. State = [[-0.18237226 -0.2042619 ]]. Action = [[ 0.22258663 -0.1258149   0.20696211 -0.5359434 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 526 is [True, False, False, True, False, False]
Current timestep = 527. State = [[-0.157043   -0.20440985]]. Action = [[ 0.23777181  0.16746134  0.14516029 -0.12507212]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 527 is [True, False, False, True, False, False]
Scene graph at timestep 527 is [True, False, False, True, False, False]
State prediction error at timestep 527 is tensor(4.9441e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 527 of 0
Current timestep = 528. State = [[-0.12276226 -0.20026779]]. Action = [[ 0.21310222 -0.065897   -0.24605778 -0.9459377 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 528 is [True, False, False, True, False, False]
Scene graph at timestep 528 is [True, False, False, True, False, False]
State prediction error at timestep 528 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 528 of 0
Current timestep = 529. State = [[-0.10003449 -0.21404538]]. Action = [[ 0.0181798  -0.16543134  0.17094052  0.9932753 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 529 is [True, False, False, True, False, False]
Scene graph at timestep 529 is [True, False, False, True, False, False]
State prediction error at timestep 529 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 529 of -1
Current timestep = 530. State = [[-0.10111826 -0.22669733]]. Action = [[-0.18037996  0.02951407  0.0874669   0.5928843 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 530 is [True, False, False, True, False, False]
Scene graph at timestep 530 is [True, False, False, True, False, False]
State prediction error at timestep 530 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 530 of -1
Current timestep = 531. State = [[-0.11104032 -0.2338105 ]]. Action = [[-0.1316164  -0.02674563  0.02686197  0.87100315]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 531 is [True, False, False, True, False, False]
Current timestep = 532. State = [[-0.1222499  -0.24854271]]. Action = [[-0.06664142 -0.19634056  0.04566669  0.7531878 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 532 is [True, False, False, True, False, False]
Current timestep = 533. State = [[-0.13158354 -0.25421086]]. Action = [[-0.03867894  0.11787125 -0.06944731 -0.7692624 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 533 is [True, False, False, True, False, False]
Scene graph at timestep 533 is [True, False, False, True, False, False]
State prediction error at timestep 533 is tensor(8.6902e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 533 of -1
Current timestep = 534. State = [[-0.13073522 -0.2603719 ]]. Action = [[ 0.21346545 -0.2225618  -0.23475072  0.773834  ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 534 is [True, False, False, True, False, False]
Current timestep = 535. State = [[-0.12081205 -0.27224416]]. Action = [[ 0.13655803 -0.06735265  0.24651977 -0.8012995 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 535 is [True, False, False, True, False, False]
Current timestep = 536. State = [[-0.10683582 -0.2837046 ]]. Action = [[ 0.05535668 -0.09499228 -0.1883813  -0.27907538]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 536 is [True, False, False, True, False, False]
Scene graph at timestep 536 is [True, False, False, True, False, False]
State prediction error at timestep 536 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 536 of -1
Current timestep = 537. State = [[-0.0987618  -0.29312128]]. Action = [[ 0.22111672 -0.21953946 -0.12435789 -0.02275336]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 537 is [True, False, False, True, False, False]
Current timestep = 538. State = [[-0.0987618  -0.29312128]]. Action = [[ 0.24581927 -0.16022305  0.13365597 -0.7598641 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 538 is [True, False, False, True, False, False]
Current timestep = 539. State = [[-0.08815915 -0.28154922]]. Action = [[ 0.21773794  0.21120238  0.10299313 -0.62985355]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 539 is [True, False, False, True, False, False]
Scene graph at timestep 539 is [True, False, False, True, False, False]
State prediction error at timestep 539 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 539 of 0
Current timestep = 540. State = [[-0.05711048 -0.25308216]]. Action = [[ 0.23071745  0.2426629   0.00791937 -0.6061519 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 540 is [True, False, False, True, False, False]
Current timestep = 541. State = [[-0.04482238 -0.23685601]]. Action = [[-0.22952753  0.05438867  0.00247589 -0.46562368]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 541 is [True, False, False, True, False, False]
Current timestep = 542. State = [[-0.0470883  -0.22952607]]. Action = [[-0.02391817  0.04055193  0.08066809  0.9044634 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 542 is [False, True, False, True, False, False]
Scene graph at timestep 542 is [False, True, False, True, False, False]
State prediction error at timestep 542 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 542 of 1
Current timestep = 543. State = [[-0.05689473 -0.22238082]]. Action = [[-0.24457632  0.09383157  0.1752133   0.01742637]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 543 is [False, True, False, True, False, False]
Scene graph at timestep 543 is [True, False, False, True, False, False]
State prediction error at timestep 543 is tensor(1.3793e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 543 of -1
Current timestep = 544. State = [[-0.0780016  -0.21279271]]. Action = [[ 0.1158151  -0.01269689 -0.03167197  0.7228993 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 544 is [True, False, False, True, False, False]
Current timestep = 545. State = [[-0.07881204 -0.22373015]]. Action = [[-0.01787242 -0.21218099 -0.09295017  0.8749896 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 545 is [True, False, False, True, False, False]
Current timestep = 546. State = [[-0.08770698 -0.24262846]]. Action = [[-0.23802678 -0.07963017  0.08754766  0.45787072]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 546 is [True, False, False, True, False, False]
Current timestep = 547. State = [[-0.09292109 -0.26162657]]. Action = [[ 0.18769175 -0.19258283 -0.22983342 -0.51095545]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 547 is [True, False, False, True, False, False]
Current timestep = 548. State = [[-0.08879296 -0.2830363 ]]. Action = [[ 0.10014442 -0.16710226  0.14981967 -0.8239418 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 548 is [True, False, False, True, False, False]
Current timestep = 549. State = [[-0.0852951  -0.29637972]]. Action = [[-0.05017796 -0.00085044  0.03505749  0.56516886]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 549 is [True, False, False, True, False, False]
Scene graph at timestep 549 is [True, False, False, True, False, False]
State prediction error at timestep 549 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 549 of -1
Current timestep = 550. State = [[-0.08950349 -0.30256742]]. Action = [[-0.1646699  -0.00177115 -0.14835304 -0.6170883 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 550 is [True, False, False, True, False, False]
Current timestep = 551. State = [[-0.09454743 -0.30613655]]. Action = [[-0.02373578 -0.2374219   0.13887417  0.5061592 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 551 is [True, False, False, True, False, False]
Current timestep = 552. State = [[-0.09570923 -0.30716744]]. Action = [[-0.12130985 -0.0468629   0.1812976   0.76672304]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 552 is [True, False, False, True, False, False]
Scene graph at timestep 552 is [True, False, False, True, False, False]
State prediction error at timestep 552 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 552 of -1
Current timestep = 553. State = [[-0.10450243 -0.30409196]]. Action = [[-0.22431384  0.10361686 -0.23487176  0.49384785]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 553 is [True, False, False, True, False, False]
Current timestep = 554. State = [[-0.11849866 -0.29929507]]. Action = [[ 0.13677633 -0.21893269  0.04959828  0.35982704]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 554 is [True, False, False, True, False, False]
Current timestep = 555. State = [[-0.11705362 -0.28557894]]. Action = [[ 0.06455523  0.22702444 -0.04755054 -0.18241346]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 555 is [True, False, False, True, False, False]
Current timestep = 556. State = [[-0.12195073 -0.26344866]]. Action = [[-0.19819026  0.09985164  0.21874833 -0.8455745 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 556 is [True, False, False, True, False, False]
Scene graph at timestep 556 is [True, False, False, True, False, False]
State prediction error at timestep 556 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 556 of -1
Current timestep = 557. State = [[-0.13761167 -0.25650805]]. Action = [[ 0.05990347 -0.13026752 -0.04532191  0.6005187 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 557 is [True, False, False, True, False, False]
Current timestep = 558. State = [[-0.13969764 -0.26350003]]. Action = [[-0.04885276 -0.02531266 -0.15576944  0.5742037 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 558 is [True, False, False, True, False, False]
Current timestep = 559. State = [[-0.13870123 -0.26852447]]. Action = [[ 0.14481041 -0.06671432  0.16581601 -0.8575176 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 559 is [True, False, False, True, False, False]
Current timestep = 560. State = [[-0.12720478 -0.27509668]]. Action = [[ 0.20828903 -0.09900112 -0.14299056  0.8476701 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 560 is [True, False, False, True, False, False]
Current timestep = 561. State = [[-0.11540312 -0.28040615]]. Action = [[-0.11605874  0.07619721 -0.16086365  0.21136451]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 561 is [True, False, False, True, False, False]
Current timestep = 562. State = [[-0.11057284 -0.28221843]]. Action = [[ 0.18609232 -0.08695099 -0.00626859  0.2573185 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 562 is [True, False, False, True, False, False]
Current timestep = 563. State = [[-0.10023772 -0.28240818]]. Action = [[0.09354737 0.03215325 0.12424472 0.08729839]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 563 is [True, False, False, True, False, False]
Current timestep = 564. State = [[-0.08385592 -0.27818412]]. Action = [[ 0.177791    0.03867659 -0.15930018 -0.8284674 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 564 is [True, False, False, True, False, False]
Current timestep = 565. State = [[-0.06869925 -0.2775872 ]]. Action = [[ 0.13602868 -0.22868273 -0.05781727 -0.29532206]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 565 is [True, False, False, True, False, False]
Current timestep = 566. State = [[-0.05423971 -0.26463366]]. Action = [[ 0.24464482  0.19834226  0.01962101 -0.00256449]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 566 is [True, False, False, True, False, False]
Scene graph at timestep 566 is [True, False, False, True, False, False]
State prediction error at timestep 566 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 566 of 1
Current timestep = 567. State = [[-0.01920989 -0.23578694]]. Action = [[0.2192865  0.24820006 0.06574693 0.91897714]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 567 is [True, False, False, True, False, False]
Current timestep = 568. State = [[ 0.00778719 -0.21275625]]. Action = [[ 0.22673184  0.06018841 -0.22998872 -0.55572844]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 568 is [False, True, False, True, False, False]
Current timestep = 569. State = [[ 0.02568817 -0.19501433]]. Action = [[-0.17573626  0.22492135  0.15347865  0.38602602]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 569 is [False, True, False, True, False, False]
Current timestep = 570. State = [[ 0.01592865 -0.19131818]]. Action = [[-0.24526541 -0.16936783  0.00291368  0.9663241 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 570 is [False, True, False, True, False, False]
Current timestep = 571. State = [[ 0.00820457 -0.20812072]]. Action = [[ 0.07755154 -0.13855903 -0.0918414  -0.75390923]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 571 is [False, True, False, True, False, False]
Current timestep = 572. State = [[ 0.01232881 -0.20644909]]. Action = [[ 0.20646143  0.13409811 -0.21749306 -0.93407744]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 572 is [False, True, False, True, False, False]
Current timestep = 573. State = [[ 0.01140586 -0.20872115]]. Action = [[-0.24828026 -0.11408813 -0.15784061 -0.45257163]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 573 is [False, True, False, True, False, False]
Current timestep = 574. State = [[ 0.00601074 -0.20899962]]. Action = [[-0.08097243  0.12306657  0.00695425 -0.7891988 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 574 is [False, True, False, True, False, False]
Current timestep = 575. State = [[ 0.00420037 -0.19174865]]. Action = [[ 0.06128502  0.20861447  0.02802616 -0.03711641]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 575 is [False, True, False, True, False, False]
Current timestep = 576. State = [[ 0.0055968  -0.16500771]]. Action = [[0.04532757 0.12501276 0.18771693 0.87921464]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 576 is [False, True, False, True, False, False]
Current timestep = 577. State = [[ 0.00553534 -0.14753355]]. Action = [[-0.05222315  0.09018856 -0.08467411  0.36524653]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 577 is [False, True, False, True, False, False]
Current timestep = 578. State = [[-0.00489753 -0.14736943]]. Action = [[-0.22673173 -0.08716348 -0.2008183   0.6350596 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 578 is [False, True, False, True, False, False]
Current timestep = 579. State = [[-0.02176979 -0.14115505]]. Action = [[-0.07688591  0.14297643  0.11545649 -0.61063325]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 579 is [False, True, False, True, False, False]
Current timestep = 580. State = [[-0.02800472 -0.14020145]]. Action = [[ 0.1767942  -0.17453739  0.19897902  0.12008035]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 580 is [False, True, False, True, False, False]
Current timestep = 581. State = [[-0.02558286 -0.13560392]]. Action = [[-0.00361447  0.19856477 -0.13681898 -0.978249  ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 581 is [False, True, False, True, False, False]
Current timestep = 582. State = [[-0.0308438  -0.11772203]]. Action = [[-0.20610452  0.15715784  0.02601141 -0.5665321 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 582 is [False, True, False, True, False, False]
Current timestep = 583. State = [[-0.03667977 -0.09519313]]. Action = [[0.11456102 0.13422    0.21090424 0.09071136]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 583 is [False, True, False, False, True, False]
Scene graph at timestep 583 is [False, True, False, False, True, False]
State prediction error at timestep 583 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 583 of 1
Current timestep = 584. State = [[-0.18032575  0.07515259]]. Action = [[ 0.08966362 -0.01062895  0.10324588  0.7799834 ]]. Reward = [100.]
Curr episode timestep = 72
Scene graph at timestep 584 is [False, True, False, False, True, False]
Current timestep = 585. State = [[-0.17376998  0.09200417]]. Action = [[-0.2267154   0.08805349 -0.1892004   0.80881023]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 585 is [True, False, False, False, True, False]
Scene graph at timestep 585 is [True, False, False, False, True, False]
State prediction error at timestep 585 is tensor(5.0614e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 585 of -1
Current timestep = 586. State = [[-0.17791584  0.08914453]]. Action = [[ 0.19509739 -0.20885561  0.17042226 -0.8675505 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 586 is [True, False, False, False, True, False]
Current timestep = 587. State = [[-0.1646114   0.06550119]]. Action = [[ 0.18191212 -0.16975676 -0.22836076 -0.35535228]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 587 is [True, False, False, False, True, False]
Current timestep = 588. State = [[-0.14607112  0.03758691]]. Action = [[ 0.02536982 -0.22390024  0.12196821  0.8401878 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 588 is [True, False, False, False, True, False]
Current timestep = 589. State = [[-0.14233623  0.03287403]]. Action = [[-0.01887251  0.21468824  0.03022379 -0.3923093 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 589 is [True, False, False, False, True, False]
Scene graph at timestep 589 is [True, False, False, False, True, False]
State prediction error at timestep 589 is tensor(3.1539e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 589 of 1
Current timestep = 590. State = [[-0.14866681  0.05238567]]. Action = [[-0.20214695  0.16304612 -0.21758144 -0.7770758 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 590 is [True, False, False, False, True, False]
Current timestep = 591. State = [[-0.15320694  0.06372681]]. Action = [[ 0.12776423 -0.0376558   0.14209187 -0.82416   ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 591 is [True, False, False, False, True, False]
Scene graph at timestep 591 is [True, False, False, False, True, False]
State prediction error at timestep 591 is tensor(2.5317e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 591 of 0
Current timestep = 592. State = [[-0.14654773  0.0500533 ]]. Action = [[ 0.13727707 -0.22294298  0.11769271 -0.8819103 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 592 is [True, False, False, False, True, False]
Scene graph at timestep 592 is [True, False, False, False, True, False]
State prediction error at timestep 592 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 592 of 1
Current timestep = 593. State = [[-0.13804708  0.0299945 ]]. Action = [[-0.04001801 -0.06804746 -0.13901041  0.04468608]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 593 is [True, False, False, False, True, False]
Current timestep = 594. State = [[-0.13144808  0.03248386]]. Action = [[ 0.15506244  0.14938301 -0.00570928  0.913429  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 594 is [True, False, False, False, True, False]
Current timestep = 595. State = [[-0.12032634  0.02655567]]. Action = [[-0.07154068 -0.22353315 -0.1538259  -0.7835236 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 595 is [True, False, False, False, True, False]
Current timestep = 596. State = [[-0.12454874  0.01404115]]. Action = [[-0.16686164 -0.03661901 -0.18761688  0.82727087]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 596 is [True, False, False, False, True, False]
Current timestep = 597. State = [[-0.13552298 -0.00629562]]. Action = [[-0.16358043 -0.22290005 -0.24246292 -0.21353388]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 597 is [True, False, False, False, True, False]
Current timestep = 598. State = [[-0.15737495 -0.0096697 ]]. Action = [[-0.20938952  0.21086264 -0.14278439 -0.31595027]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 598 is [True, False, False, False, True, False]
Scene graph at timestep 598 is [True, False, False, False, True, False]
State prediction error at timestep 598 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 598 of -1
Current timestep = 599. State = [[-0.1806628   0.00362166]]. Action = [[ 3.9349645e-02  4.1741133e-04  5.1996261e-02 -5.5448270e-01]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 599 is [True, False, False, False, True, False]
Current timestep = 600. State = [[-0.1763132  -0.00441698]]. Action = [[ 0.11841738 -0.13999374 -0.14727691 -0.91061884]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 600 is [True, False, False, False, True, False]
Scene graph at timestep 600 is [True, False, False, False, True, False]
State prediction error at timestep 600 is tensor(8.0480e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 600 of 1
Current timestep = 601. State = [[-0.178008   -0.01184704]]. Action = [[-0.21391393  0.04757178 -0.04592034 -0.76652074]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 601 is [True, False, False, False, True, False]
Current timestep = 602. State = [[-0.19491325 -0.02489557]]. Action = [[-0.24643826 -0.24121968 -0.0084002  -0.7411962 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 602 is [True, False, False, False, True, False]
Current timestep = 603. State = [[-0.21194851 -0.03052457]]. Action = [[ 0.16013753  0.18102074 -0.05905199  0.21863711]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 603 is [True, False, False, False, True, False]
Current timestep = 604. State = [[-0.20309712 -0.01401105]]. Action = [[ 0.19419387  0.14565101  0.17874038 -0.2608595 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 604 is [True, False, False, False, True, False]
Current timestep = 605. State = [[-0.19071789 -0.00729539]]. Action = [[ 0.11995736 -0.0886656  -0.09029336  0.8167156 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 605 is [True, False, False, False, True, False]
Current timestep = 606. State = [[-1.8135917e-01  1.3907479e-04]]. Action = [[ 0.00892842  0.16707852 -0.03384449  0.4740467 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 606 is [True, False, False, False, True, False]
Current timestep = 607. State = [[-0.17083712 -0.00439717]]. Action = [[ 0.16067415 -0.23348707  0.17219526 -0.01221353]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 607 is [True, False, False, False, True, False]
Current timestep = 608. State = [[-0.15145232 -0.0263079 ]]. Action = [[ 0.1304282  -0.17408156  0.07605073 -0.25772816]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 608 is [True, False, False, False, True, False]
Scene graph at timestep 608 is [True, False, False, False, True, False]
State prediction error at timestep 608 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 608 of 1
Current timestep = 609. State = [[-0.13836004 -0.03331521]]. Action = [[-0.02737629  0.18224496  0.13095132  0.9500654 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 609 is [True, False, False, False, True, False]
Scene graph at timestep 609 is [True, False, False, False, True, False]
State prediction error at timestep 609 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 609 of 0
Current timestep = 610. State = [[-0.13930427 -0.02297935]]. Action = [[-0.04539913  0.00944847 -0.06759271 -0.9033336 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 610 is [True, False, False, False, True, False]
Current timestep = 611. State = [[-0.14135066 -0.01543388]]. Action = [[-0.05548629  0.10119128  0.12225527 -0.58001655]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 611 is [True, False, False, False, True, False]
Current timestep = 612. State = [[-0.14024718 -0.01369797]]. Action = [[ 0.13432193 -0.11693408  0.11111781  0.89797413]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 612 is [True, False, False, False, True, False]
Current timestep = 613. State = [[-0.12985674 -0.00745267]]. Action = [[0.19394696 0.19881016 0.1274966  0.10926676]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 613 is [True, False, False, False, True, False]
Current timestep = 614. State = [[-0.10109856 -0.00487531]]. Action = [[ 0.24851501 -0.1330531   0.06600657 -0.91287243]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 614 is [True, False, False, False, True, False]
Current timestep = 615. State = [[-0.08060729 -0.02172253]]. Action = [[-0.03485484 -0.21775655  0.17917305 -0.84908336]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 615 is [True, False, False, False, True, False]
Current timestep = 616. State = [[-0.06882658 -0.05010461]]. Action = [[ 0.20692194 -0.22711864  0.10588008  0.8313427 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 616 is [True, False, False, False, True, False]
Current timestep = 617. State = [[-0.04406794 -0.05752069]]. Action = [[0.1737389  0.20308119 0.15625852 0.43211055]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 617 is [True, False, False, False, True, False]
Current timestep = 618. State = [[-0.18578492 -0.02134301]]. Action = [[ 0.0581795  -0.21419851  0.01477039  0.257051  ]]. Reward = [100.]
Curr episode timestep = 33
Scene graph at timestep 618 is [False, True, False, False, True, False]
Current timestep = 619. State = [[-0.17712681 -0.03255581]]. Action = [[-0.14861457 -0.11455971 -0.11714871  0.93677604]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 619 is [True, False, False, False, True, False]
Current timestep = 620. State = [[-0.17959538 -0.04225064]]. Action = [[ 0.01096475 -0.01302558 -0.23743148  0.39167213]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 620 is [True, False, False, False, True, False]
Current timestep = 621. State = [[-0.18042861 -0.05464767]]. Action = [[-0.00909089 -0.14874943 -0.12535541 -0.5957911 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 621 is [True, False, False, False, True, False]
Scene graph at timestep 621 is [True, False, False, False, True, False]
State prediction error at timestep 621 is tensor(2.0474e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 621 of 0
Current timestep = 622. State = [[-0.1831426  -0.07912482]]. Action = [[-0.00403622 -0.19100918  0.22293395  0.16047585]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 622 is [True, False, False, False, True, False]
Scene graph at timestep 622 is [True, False, False, False, True, False]
State prediction error at timestep 622 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of -1
Current timestep = 623. State = [[-0.1869597  -0.09539099]]. Action = [[-0.07551783 -0.01942632  0.19529584  0.10311639]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 623 is [True, False, False, False, True, False]
Scene graph at timestep 623 is [True, False, False, False, True, False]
State prediction error at timestep 623 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 623 of -1
Current timestep = 624. State = [[-0.18458483 -0.11141199]]. Action = [[ 0.24213994 -0.22188224 -0.09062214  0.3408531 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 624 is [True, False, False, False, True, False]
Current timestep = 625. State = [[-0.16514759 -0.11532965]]. Action = [[0.13166332 0.20908844 0.05594319 0.7937963 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 625 is [True, False, False, False, True, False]
Current timestep = 626. State = [[-0.15227762 -0.1028946 ]]. Action = [[ 0.09442362  0.05281526  0.15371549 -0.16461009]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 626 is [True, False, False, False, True, False]
Scene graph at timestep 626 is [True, False, False, False, True, False]
State prediction error at timestep 626 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 626 of 1
Current timestep = 627. State = [[-0.1461426  -0.09597852]]. Action = [[-0.20668471 -0.01019876  0.01542047  0.85987425]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 627 is [True, False, False, False, True, False]
Scene graph at timestep 627 is [True, False, False, False, True, False]
State prediction error at timestep 627 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 627 of 0
Current timestep = 628. State = [[-0.1603618 -0.1070983]]. Action = [[-0.1939978  -0.13251929 -0.1033093   0.32080197]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 628 is [True, False, False, False, True, False]
Current timestep = 629. State = [[-0.16889857 -0.10778304]]. Action = [[ 0.08616513  0.14202943 -0.01132868 -0.25277   ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 629 is [True, False, False, False, True, False]
Current timestep = 630. State = [[-0.170821   -0.11363006]]. Action = [[-0.04478279 -0.21735035 -0.20527036  0.9785328 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 630 is [True, False, False, False, True, False]
Current timestep = 631. State = [[-0.17545393 -0.12926538]]. Action = [[-0.05419746 -0.06793317  0.14878118  0.8923013 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 631 is [True, False, False, False, True, False]
Current timestep = 632. State = [[-0.18608843 -0.1374823 ]]. Action = [[-0.18022291  0.01253462 -0.11465132  0.47842383]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 632 is [True, False, False, True, False, False]
Current timestep = 633. State = [[-0.19849183 -0.1414358 ]]. Action = [[ 0.02196354 -0.04276524 -0.10153514  0.47695732]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 633 is [True, False, False, True, False, False]
Scene graph at timestep 633 is [True, False, False, True, False, False]
State prediction error at timestep 633 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 633 of -1
Current timestep = 634. State = [[-0.2058858  -0.14347959]]. Action = [[-0.11610952  0.03549948  0.09697032 -0.10761535]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 634 is [True, False, False, True, False, False]
Scene graph at timestep 634 is [True, False, False, True, False, False]
State prediction error at timestep 634 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of -1
Current timestep = 635. State = [[-0.20438413 -0.12964895]]. Action = [[ 0.23121601  0.2025367   0.10077408 -0.81875885]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 635 is [True, False, False, True, False, False]
Scene graph at timestep 635 is [True, False, False, True, False, False]
State prediction error at timestep 635 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 635 of 0
Current timestep = 636. State = [[-0.19185127 -0.11911271]]. Action = [[ 0.14688087 -0.1699618  -0.22743712 -0.83454907]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 636 is [True, False, False, True, False, False]
Scene graph at timestep 636 is [True, False, False, False, True, False]
State prediction error at timestep 636 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 636 of 0
Current timestep = 637. State = [[-0.18322855 -0.12766415]]. Action = [[-0.12429807  0.04840687 -0.00925645  0.8394507 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 637 is [True, False, False, False, True, False]
Scene graph at timestep 637 is [True, False, False, True, False, False]
State prediction error at timestep 637 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 637 of 0
Current timestep = 638. State = [[-0.17938803 -0.11749128]]. Action = [[0.18465385 0.14903718 0.02219215 0.7701297 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 638 is [True, False, False, True, False, False]
Current timestep = 639. State = [[-0.16796356 -0.09511854]]. Action = [[ 0.13824636  0.20558739  0.07638747 -0.93307817]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 639 is [True, False, False, False, True, False]
Current timestep = 640. State = [[-0.16256267 -0.09292831]]. Action = [[-0.17449757 -0.20892611  0.173082    0.5582273 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 640 is [True, False, False, False, True, False]
Current timestep = 641. State = [[-0.17263648 -0.09399179]]. Action = [[-0.18291064  0.13054904 -0.18601917 -0.11108643]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 641 is [True, False, False, False, True, False]
Current timestep = 642. State = [[-0.17386727 -0.07516861]]. Action = [[ 0.23355654  0.22745949  0.23290479 -0.90958774]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 642 is [True, False, False, False, True, False]
Current timestep = 643. State = [[-0.16067322 -0.06696416]]. Action = [[ 0.20633799 -0.15110841 -0.11393872  0.9002929 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 643 is [True, False, False, False, True, False]
Scene graph at timestep 643 is [True, False, False, False, True, False]
State prediction error at timestep 643 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 643 of 1
Current timestep = 644. State = [[-0.13624816 -0.07201809]]. Action = [[ 0.16085544 -0.03046128 -0.10511565  0.9648676 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 644 is [True, False, False, False, True, False]
Current timestep = 645. State = [[-0.12706573 -0.08235889]]. Action = [[-0.12192914 -0.12564956 -0.08400697  0.91298974]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 645 is [True, False, False, False, True, False]
Current timestep = 646. State = [[-0.13706464 -0.09241669]]. Action = [[-0.22799286  0.01642707  0.173619    0.9082936 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 646 is [True, False, False, False, True, False]
Current timestep = 647. State = [[-0.14920983 -0.09514791]]. Action = [[-0.07845406  0.01791665 -0.12294537 -0.26820862]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 647 is [True, False, False, False, True, False]
Current timestep = 648. State = [[-0.16640761 -0.0883968 ]]. Action = [[-0.24382566  0.09598556  0.17301643  0.17725956]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 648 is [True, False, False, False, True, False]
Current timestep = 649. State = [[-0.18739326 -0.06883471]]. Action = [[ 0.0037654   0.22599837  0.16515446 -0.92055273]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 649 is [True, False, False, False, True, False]
Current timestep = 650. State = [[-0.18879426 -0.06118498]]. Action = [[ 0.13274449 -0.19269174  0.06289455 -0.21985263]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 650 is [True, False, False, False, True, False]
Scene graph at timestep 650 is [True, False, False, False, True, False]
State prediction error at timestep 650 is tensor(1.7121e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 650 of -1
Current timestep = 651. State = [[-0.18176804 -0.06754228]]. Action = [[ 0.14725137  0.08642668  0.22301695 -0.86096203]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 651 is [True, False, False, False, True, False]
Scene graph at timestep 651 is [True, False, False, False, True, False]
State prediction error at timestep 651 is tensor(3.8609e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 651 of 1
Current timestep = 652. State = [[-0.18153854 -0.06057579]]. Action = [[-0.22927597  0.0257847   0.04340434  0.87402976]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 652 is [True, False, False, False, True, False]
Current timestep = 653. State = [[-0.18178555 -0.07185075]]. Action = [[ 0.20981526 -0.24406672  0.05672657  0.98353493]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 653 is [True, False, False, False, True, False]
Current timestep = 654. State = [[-0.17278129 -0.07946851]]. Action = [[ 0.08783793  0.1286233  -0.17941761  0.6998882 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 654 is [True, False, False, False, True, False]
Current timestep = 655. State = [[-0.1568359  -0.06834923]]. Action = [[0.17880315 0.13615346 0.09240001 0.17833912]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 655 is [True, False, False, False, True, False]
Current timestep = 656. State = [[-0.14703123 -0.06933615]]. Action = [[-0.17218158 -0.18901975 -0.15402626  0.12691116]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 656 is [True, False, False, False, True, False]
Scene graph at timestep 656 is [True, False, False, False, True, False]
State prediction error at timestep 656 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 656 of 0
Current timestep = 657. State = [[-0.14885181 -0.08932833]]. Action = [[ 0.09726095 -0.1716251  -0.15240777  0.02126467]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 657 is [True, False, False, False, True, False]
Current timestep = 658. State = [[-0.14195831 -0.09561626]]. Action = [[ 0.13939935  0.08625716  0.15382034 -0.35828447]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 658 is [True, False, False, False, True, False]
Scene graph at timestep 658 is [True, False, False, False, True, False]
State prediction error at timestep 658 is tensor(1.0013e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 658 of 1
Current timestep = 659. State = [[-0.13596687 -0.10561825]]. Action = [[-0.20342131 -0.18765707 -0.04916164 -0.2161004 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 659 is [True, False, False, False, True, False]
Scene graph at timestep 659 is [True, False, False, False, True, False]
State prediction error at timestep 659 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 659 of -1
Current timestep = 660. State = [[-0.14038606 -0.12931691]]. Action = [[ 0.16831174 -0.19457129  0.20889682  0.4911033 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 660 is [True, False, False, False, True, False]
Current timestep = 661. State = [[-0.13951209 -0.1365608 ]]. Action = [[-0.16689146  0.14639509 -0.15039499  0.58945847]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 661 is [True, False, False, True, False, False]
Current timestep = 662. State = [[-0.13450857 -0.14144999]]. Action = [[ 0.24314448 -0.17117448  0.03617805  0.69834185]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 662 is [True, False, False, True, False, False]
Current timestep = 663. State = [[-0.1252648  -0.14377417]]. Action = [[ 0.03284904  0.08618176 -0.05861662  0.8303771 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 663 is [True, False, False, True, False, False]
Scene graph at timestep 663 is [True, False, False, True, False, False]
State prediction error at timestep 663 is tensor(1.7225e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 663 of 0
Current timestep = 664. State = [[-0.12105751 -0.14246458]]. Action = [[ 0.05323693 -0.06021877  0.15047532 -0.8536778 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 664 is [True, False, False, True, False, False]
Current timestep = 665. State = [[-0.10515288 -0.14242767]]. Action = [[ 0.23775434  0.03633952 -0.16010308  0.9458847 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 665 is [True, False, False, True, False, False]
Current timestep = 666. State = [[-0.08937969 -0.15505107]]. Action = [[-0.1539421  -0.20496443 -0.1073271   0.28850532]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 666 is [True, False, False, True, False, False]
Current timestep = 667. State = [[-0.08448163 -0.16285805]]. Action = [[0.17345065 0.0897108  0.07092297 0.4204842 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 667 is [True, False, False, True, False, False]
Scene graph at timestep 667 is [True, False, False, True, False, False]
State prediction error at timestep 667 is tensor(4.3948e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 667 of 0
Current timestep = 668. State = [[-0.06619682 -0.17084272]]. Action = [[ 0.19785377 -0.18769166  0.07701537 -0.26163208]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 668 is [True, False, False, True, False, False]
Current timestep = 669. State = [[-0.03977008 -0.18156314]]. Action = [[ 0.22853595 -0.01323445 -0.02261437 -0.9113552 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 669 is [True, False, False, True, False, False]
Current timestep = 670. State = [[-0.01087827 -0.1826904 ]]. Action = [[0.15254045 0.03509215 0.01737311 0.22359991]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 670 is [False, True, False, True, False, False]
Current timestep = 671. State = [[ 0.01151249 -0.16977216]]. Action = [[ 0.09624958  0.22760868 -0.2253321   0.4314754 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 671 is [False, True, False, True, False, False]
Current timestep = 672. State = [[ 0.03145107 -0.16615859]]. Action = [[ 0.17373207 -0.16969275  0.15511358 -0.02029735]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 672 is [False, True, False, True, False, False]
Current timestep = 673. State = [[ 0.04396857 -0.18420605]]. Action = [[-0.07549173 -0.18487725  0.12262601 -0.93040323]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 673 is [False, True, False, True, False, False]
Current timestep = 674. State = [[ 0.04313713 -0.19780758]]. Action = [[ 0.14165837 -0.11242634  0.15552625 -0.54375196]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 674 is [False, True, False, True, False, False]
Current timestep = 675. State = [[ 0.04904749 -0.18982416]]. Action = [[ 0.12937671  0.1686709  -0.0542897   0.45973647]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 675 is [False, True, False, True, False, False]
Current timestep = 676. State = [[ 0.0603517  -0.18241927]]. Action = [[ 0.18011731 -0.16228491 -0.22298332  0.5015948 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 676 is [False, True, False, True, False, False]
Current timestep = 677. State = [[ 0.06438477 -0.18113261]]. Action = [[ 0.2388894  -0.03381106 -0.13732564 -0.17181158]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 677 is [False, False, True, True, False, False]
Current timestep = 678. State = [[ 0.06438152 -0.18121253]]. Action = [[ 0.1859009  -0.13143392 -0.16602638  0.28648746]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 678 is [False, False, True, True, False, False]
Scene graph at timestep 678 is [False, False, True, True, False, False]
State prediction error at timestep 678 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 678 of 0
Current timestep = 679. State = [[ 0.06438477 -0.18113261]]. Action = [[ 0.03938159 -0.05116971 -0.09822857 -0.9103547 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 679 is [False, False, True, True, False, False]
Scene graph at timestep 679 is [False, False, True, True, False, False]
State prediction error at timestep 679 is tensor(4.7907e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 679 of -1
Current timestep = 680. State = [[ 0.06438477 -0.18113261]]. Action = [[ 0.04431471  0.1984289  -0.2207122   0.17049026]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 680 is [False, False, True, True, False, False]
Current timestep = 681. State = [[ 0.06346748 -0.19367547]]. Action = [[ 0.00459969 -0.21920125 -0.16670306 -0.9093525 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 681 is [False, False, True, True, False, False]
Scene graph at timestep 681 is [False, False, True, True, False, False]
State prediction error at timestep 681 is tensor(2.9252e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 681 of -1
Current timestep = 682. State = [[ 0.06180394 -0.21661066]]. Action = [[-0.03283931 -0.13615984  0.13031214 -0.97020185]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 682 is [False, False, True, True, False, False]
Scene graph at timestep 682 is [False, False, True, True, False, False]
State prediction error at timestep 682 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 682 of -1
Current timestep = 683. State = [[ 0.06076488 -0.22743274]]. Action = [[ 0.10303742 -0.17783143 -0.0522425   0.5818505 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 683 is [False, False, True, True, False, False]
Scene graph at timestep 683 is [False, False, True, True, False, False]
State prediction error at timestep 683 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 683 of -1
Current timestep = 684. State = [[ 0.06075684 -0.22751227]]. Action = [[ 0.22038805  0.19162178 -0.09297842  0.55508375]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 684 is [False, False, True, True, False, False]
Scene graph at timestep 684 is [False, False, True, True, False, False]
State prediction error at timestep 684 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 684 of -1
Current timestep = 685. State = [[ 0.06097762 -0.2252977 ]]. Action = [[-0.01989317  0.07472509 -0.22395703 -0.39419174]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 685 is [False, False, True, True, False, False]
Current timestep = 686. State = [[ 0.06025084 -0.2186369 ]]. Action = [[-0.17058466  0.10871577 -0.16062245 -0.48395574]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 686 is [False, False, True, True, False, False]
Current timestep = 687. State = [[ 0.05713263 -0.20356117]]. Action = [[-0.09664695  0.15507802 -0.21898909 -0.7471451 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 687 is [False, False, True, True, False, False]
Current timestep = 688. State = [[ 0.05350012 -0.1782131 ]]. Action = [[0.0019592  0.22668862 0.1957305  0.1630503 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 688 is [False, False, True, True, False, False]
Scene graph at timestep 688 is [False, False, True, True, False, False]
State prediction error at timestep 688 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 688 of 1
Current timestep = 689. State = [[ 0.04316996 -0.16102858]]. Action = [[-0.11719157 -0.09370548  0.19337392 -0.95911014]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 689 is [False, False, True, True, False, False]
Current timestep = 690. State = [[ 0.03732033 -0.17864309]]. Action = [[ 0.12145066 -0.2264719   0.03787598  0.5212598 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 690 is [False, True, False, True, False, False]
Current timestep = 691. State = [[ 0.03956524 -0.1878188 ]]. Action = [[ 0.13516355  0.06390792 -0.07435702  0.23875666]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 691 is [False, True, False, True, False, False]
Current timestep = 692. State = [[ 0.04384606 -0.18564971]]. Action = [[ 0.09356371  0.01074716  0.08843973 -0.6559171 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 692 is [False, True, False, True, False, False]
Current timestep = 693. State = [[ 0.0446278 -0.1979365]]. Action = [[-0.20743556 -0.2096972   0.18377471 -0.8416985 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 693 is [False, True, False, True, False, False]
Current timestep = 694. State = [[ 0.03688956 -0.21351784]]. Action = [[-0.16512118  0.01540679 -0.16155754  0.50699663]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 694 is [False, True, False, True, False, False]
Current timestep = 695. State = [[ 0.03037749 -0.21658172]]. Action = [[ 0.20412469  0.19201502 -0.21685253  0.27306163]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 695 is [False, True, False, True, False, False]
Current timestep = 696. State = [[ 0.02153239 -0.2075582 ]]. Action = [[-0.2310604   0.19492     0.0150134  -0.33763075]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 696 is [False, True, False, True, False, False]
Current timestep = 697. State = [[ 0.00692484 -0.183786  ]]. Action = [[0.14626047 0.14701003 0.15605348 0.36841762]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 697 is [False, True, False, True, False, False]
Current timestep = 698. State = [[-0.00121536 -0.18623932]]. Action = [[-0.22523537 -0.2353303   0.21017212  0.00102139]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 698 is [False, True, False, True, False, False]
Current timestep = 699. State = [[-0.00648438 -0.1961087 ]]. Action = [[ 0.20814899  0.03689235 -0.14787072  0.51842654]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 699 is [False, True, False, True, False, False]
Current timestep = 700. State = [[-0.00074194 -0.19861004]]. Action = [[ 0.09511909 -0.08066216  0.08033776  0.19313991]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 700 is [False, True, False, True, False, False]
Current timestep = 701. State = [[ 0.00229101 -0.2148152 ]]. Action = [[ 0.0498026  -0.23702237 -0.16888706  0.38985598]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 701 is [False, True, False, True, False, False]
Current timestep = 702. State = [[ 0.01603218 -0.21993932]]. Action = [[ 0.24114412  0.22670597 -0.03579246 -0.13017482]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 702 is [False, True, False, True, False, False]
Current timestep = 703. State = [[ 0.0451981  -0.20892121]]. Action = [[ 0.23203218 -0.00573109  0.15274954 -0.76190275]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 703 is [False, True, False, True, False, False]
Scene graph at timestep 703 is [False, True, False, True, False, False]
State prediction error at timestep 703 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 703 of -1
Current timestep = 704. State = [[ 0.06784832 -0.19564293]]. Action = [[-0.22650684  0.18251723  0.15365753 -0.08952129]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 704 is [False, True, False, True, False, False]
Scene graph at timestep 704 is [False, False, True, True, False, False]
State prediction error at timestep 704 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 704 of 1
Current timestep = 705. State = [[ 0.0658059  -0.18632796]]. Action = [[ 0.0272564   0.00393593  0.22391999 -0.38349783]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 705 is [False, False, True, True, False, False]
Current timestep = 706. State = [[ 0.06578308 -0.18631238]]. Action = [[0.15911776 0.09765291 0.07576865 0.57896376]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 706 is [False, False, True, True, False, False]
Current timestep = 707. State = [[ 0.06578308 -0.18631238]]. Action = [[ 0.05185306  0.02477831  0.14646256 -0.34394717]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 707 is [False, False, True, True, False, False]
Current timestep = 708. State = [[ 0.06578308 -0.18631238]]. Action = [[ 0.20971727 -0.11921838 -0.07239306  0.9311695 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 708 is [False, False, True, True, False, False]
Current timestep = 709. State = [[ 0.05925654 -0.17849793]]. Action = [[-0.20592956  0.14394325  0.04761872 -0.25110555]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 709 is [False, False, True, True, False, False]
Scene graph at timestep 709 is [False, False, True, True, False, False]
State prediction error at timestep 709 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 709 of 1
Current timestep = 710. State = [[ 0.03759804 -0.16394532]]. Action = [[ 0.20222169  0.02360666 -0.10995346 -0.6095933 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 710 is [False, False, True, True, False, False]
Current timestep = 711. State = [[ 0.04140744 -0.15157679]]. Action = [[ 0.15925357  0.20301387 -0.18789501  0.97625685]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 711 is [False, True, False, True, False, False]
Current timestep = 712. State = [[ 0.04411254 -0.13557209]]. Action = [[ 0.23398185  0.07890958  0.07497549 -0.6534774 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 712 is [False, True, False, True, False, False]
Current timestep = 713. State = [[ 0.04407986 -0.13338695]]. Action = [[ 0.16194189  0.01427022 -0.12950456 -0.25558388]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 713 is [False, True, False, True, False, False]
Current timestep = 714. State = [[ 0.04619114 -0.14051874]]. Action = [[ 0.10611477 -0.1577262  -0.19253589  0.38956797]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 714 is [False, True, False, True, False, False]
Current timestep = 715. State = [[ 0.04795901 -0.14745979]]. Action = [[ 0.15390998 -0.2036209  -0.13030718  0.09800851]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 715 is [False, True, False, True, False, False]
Scene graph at timestep 715 is [False, True, False, True, False, False]
State prediction error at timestep 715 is tensor(3.3021e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 715 of -1
Current timestep = 716. State = [[ 0.04850827 -0.14819813]]. Action = [[ 0.22317213  0.00152683  0.01795372 -0.06171   ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 716 is [False, True, False, True, False, False]
Scene graph at timestep 716 is [False, True, False, True, False, False]
State prediction error at timestep 716 is tensor(4.2360e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 716 of -1
Current timestep = 717. State = [[ 0.04850755 -0.14788654]]. Action = [[-0.05295986  0.03796923  0.00165927 -0.03332531]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 717 is [False, True, False, True, False, False]
Current timestep = 718. State = [[ 0.04850622 -0.14741988]]. Action = [[ 0.15769589 -0.22618331 -0.03414021 -0.29179633]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 718 is [False, True, False, True, False, False]
Current timestep = 719. State = [[ 0.04719166 -0.15853636]]. Action = [[-0.05140056 -0.20194298 -0.18666717 -0.49292386]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 719 is [False, True, False, True, False, False]
Current timestep = 720. State = [[ 0.04430808 -0.18528298]]. Action = [[-0.00236002 -0.23409879 -0.03016396 -0.8831515 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 720 is [False, True, False, True, False, False]
Scene graph at timestep 720 is [False, True, False, True, False, False]
State prediction error at timestep 720 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 720 of -1
Current timestep = 721. State = [[ 0.04296414 -0.20736776]]. Action = [[ 0.19104367  0.15699556 -0.12868887  0.7266278 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 721 is [False, True, False, True, False, False]
Current timestep = 722. State = [[ 0.04078327 -0.21872477]]. Action = [[-0.02163626 -0.17684627 -0.04004638 -0.08931381]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 722 is [False, True, False, True, False, False]
Current timestep = 723. State = [[ 0.03951037 -0.2317385 ]]. Action = [[ 0.16374332  0.09332982  0.06113666 -0.03854853]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 723 is [False, True, False, True, False, False]
Current timestep = 724. State = [[ 0.03337661 -0.24441652]]. Action = [[-0.15848318 -0.14986607 -0.0906719   0.5883446 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 724 is [False, True, False, True, False, False]
Current timestep = 725. State = [[ 0.02301553 -0.26840764]]. Action = [[-0.06050742 -0.18023182  0.07302588 -0.87097365]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 725 is [False, True, False, True, False, False]
Current timestep = 726. State = [[ 0.02258964 -0.28396723]]. Action = [[ 0.2003972  -0.04266322 -0.21328399  0.7465899 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 726 is [False, True, False, True, False, False]
Current timestep = 727. State = [[ 0.03542746 -0.29009268]]. Action = [[ 0.19273514 -0.06013414 -0.09943081 -0.55957896]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 727 is [False, True, False, True, False, False]
Current timestep = 728. State = [[ 0.0529246  -0.29224107]]. Action = [[0.01795509 0.07260162 0.22997868 0.7826557 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 728 is [False, True, False, True, False, False]
Scene graph at timestep 728 is [False, False, True, True, False, False]
State prediction error at timestep 728 is tensor(9.7294e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of -1
Current timestep = 729. State = [[ 0.0586331  -0.27859682]]. Action = [[-0.10432291  0.2291575   0.11709082  0.88763404]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 729 is [False, False, True, True, False, False]
Current timestep = 730. State = [[ 0.05943856 -0.27045602]]. Action = [[ 0.02155876 -0.09845386  0.14541954  0.7557254 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 730 is [False, False, True, True, False, False]
Current timestep = 731. State = [[ 0.06106287 -0.26129806]]. Action = [[ 0.02241886  0.19129777  0.15226918 -0.36651272]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 731 is [False, False, True, True, False, False]
Current timestep = 732. State = [[ 0.06251115 -0.2506189 ]]. Action = [[ 0.21414107  0.17797858 -0.02574357 -0.8654796 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 732 is [False, False, True, True, False, False]
Current timestep = 733. State = [[ 0.05925641 -0.26189378]]. Action = [[-0.05561678 -0.24276103 -0.21377918  0.84564114]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 733 is [False, False, True, True, False, False]
Scene graph at timestep 733 is [False, False, True, True, False, False]
State prediction error at timestep 733 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 733 of -1
Current timestep = 734. State = [[ 0.05756564 -0.27299157]]. Action = [[ 0.03240848  0.10279083 -0.15107037 -0.92116636]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 734 is [False, False, True, True, False, False]
Current timestep = 735. State = [[ 0.05811453 -0.26990855]]. Action = [[ 0.18502265 -0.05856976 -0.19540378 -0.08067977]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 735 is [False, False, True, True, False, False]
Current timestep = 736. State = [[ 0.05817625 -0.26951447]]. Action = [[ 0.13388556 -0.15543737 -0.10316008 -0.6381741 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 736 is [False, False, True, True, False, False]
Current timestep = 737. State = [[ 0.05817625 -0.26951447]]. Action = [[ 0.2206164  -0.09516454 -0.16549557  0.8521528 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 737 is [False, False, True, True, False, False]
Scene graph at timestep 737 is [False, False, True, True, False, False]
State prediction error at timestep 737 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 737 of -1
Current timestep = 738. State = [[ 0.05840028 -0.26599625]]. Action = [[-0.0786844   0.06928271 -0.07054915  0.01443779]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 738 is [False, False, True, True, False, False]
Current timestep = 739. State = [[ 0.05887397 -0.2625934 ]]. Action = [[ 0.13684142  0.20124477  0.15322042 -0.868775  ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 739 is [False, False, True, True, False, False]
Current timestep = 740. State = [[ 0.06028706 -0.24851133]]. Action = [[-0.00335643  0.23819476  0.17427507 -0.8965238 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 740 is [False, False, True, True, False, False]
Current timestep = 741. State = [[ 0.05986802 -0.22871472]]. Action = [[0.08276981 0.12629479 0.00971115 0.41607583]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 741 is [False, False, True, True, False, False]
Current timestep = 742. State = [[ 0.05235263 -0.23647296]]. Action = [[-0.10941213 -0.17076662  0.02350867  0.16976762]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 742 is [False, False, True, True, False, False]
Current timestep = 743. State = [[ 0.04727751 -0.23186244]]. Action = [[-0.03339288  0.24586803  0.18156862  0.19585085]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 743 is [False, False, True, True, False, False]
Current timestep = 744. State = [[ 0.03590205 -0.2297016 ]]. Action = [[-0.04033807 -0.21414566  0.10537383  0.9840622 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 744 is [False, True, False, True, False, False]
Current timestep = 745. State = [[-0.15339068  0.006134  ]]. Action = [[ 0.12815368  0.19179341  0.1076172  -0.67296475]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 745 is [False, True, False, True, False, False]
Current timestep = 746. State = [[-0.1322912   0.00916085]]. Action = [[ 0.1853359   0.04769886  0.19393694 -0.8719495 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 746 is [True, False, False, False, True, False]
Current timestep = 747. State = [[-0.10233716  0.00650786]]. Action = [[ 0.22676536 -0.10423648  0.10884076 -0.22992206]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 747 is [True, False, False, False, True, False]
Current timestep = 748. State = [[-0.08508966 -0.00479167]]. Action = [[-0.12130141 -0.13332777  0.04807851 -0.10951692]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 748 is [True, False, False, False, True, False]
Current timestep = 749. State = [[-0.09330185 -0.00389243]]. Action = [[-0.2440962   0.17757761 -0.10256469  0.03728557]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 749 is [True, False, False, False, True, False]
Current timestep = 750. State = [[-0.09719449  0.01734098]]. Action = [[ 0.21313956  0.21212602  0.02957931 -0.9588218 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 750 is [True, False, False, False, True, False]
Current timestep = 751. State = [[-0.08871017  0.01843828]]. Action = [[ 0.1642254  -0.22585714 -0.12683202  0.44246006]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 751 is [True, False, False, False, True, False]
Current timestep = 752. State = [[-0.08102338  0.01247821]]. Action = [[-0.20474716  0.04933935 -0.02173381 -0.8172388 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 752 is [True, False, False, False, True, False]
Current timestep = 753. State = [[-0.07923694  0.00353531]]. Action = [[ 0.17008808 -0.15767682 -0.07407866 -0.5440104 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 753 is [True, False, False, False, True, False]
Current timestep = 754. State = [[-0.06941358  0.00509954]]. Action = [[ 0.19383332  0.20677394  0.10305712 -0.279889  ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 754 is [True, False, False, False, True, False]
Current timestep = 755. State = [[-0.06041465  0.02326407]]. Action = [[-0.17334174  0.1388753  -0.06166781 -0.7231936 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 755 is [True, False, False, False, True, False]
Current timestep = 756. State = [[-0.06732435  0.04607743]]. Action = [[-0.06256172  0.16905409  0.01303256  0.1702795 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 756 is [True, False, False, False, True, False]
Current timestep = 757. State = [[-0.07147447  0.06675526]]. Action = [[ 0.08618     0.11078009 -0.02981615  0.42324984]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 757 is [True, False, False, False, True, False]
Current timestep = 758. State = [[-0.06705576  0.07653896]]. Action = [[ 0.13109347  0.04800394 -0.09449261  0.47619772]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 758 is [True, False, False, False, True, False]
Scene graph at timestep 758 is [True, False, False, False, True, False]
State prediction error at timestep 758 is tensor(4.6857e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 758 of 1
Current timestep = 759. State = [[-0.05463522  0.09052417]]. Action = [[ 0.05491054  0.12360373 -0.17844053  0.42280412]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 759 is [True, False, False, False, True, False]
Current timestep = 760. State = [[-0.04148632  0.09983405]]. Action = [[ 0.10932156  0.00905183 -0.20947981 -0.4935825 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 760 is [True, False, False, False, True, False]
Current timestep = 761. State = [[-0.2436669   0.02843222]]. Action = [[0.11350101 0.1773904  0.12559527 0.7024381 ]]. Reward = [100.]
Curr episode timestep = 15
Scene graph at timestep 761 is [False, True, False, False, True, False]
Current timestep = 762. State = [[-0.23140405  0.03204121]]. Action = [[ 0.22331452 -0.02107528  0.08541018 -0.9633361 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 762 is [True, False, False, False, True, False]
Current timestep = 763. State = [[-0.20469645  0.02476189]]. Action = [[ 0.13987878 -0.14671867  0.19570991 -0.9473315 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 763 is [True, False, False, False, True, False]
Scene graph at timestep 763 is [True, False, False, False, True, False]
State prediction error at timestep 763 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 763 of 1
Current timestep = 764. State = [[-0.17693892  0.00168466]]. Action = [[ 0.24708444 -0.2253796   0.23589662 -0.7862555 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 764 is [True, False, False, False, True, False]
Current timestep = 765. State = [[-0.14758933 -0.00063886]]. Action = [[ 0.20565438  0.24029279 -0.18180235 -0.39038903]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 765 is [True, False, False, False, True, False]
Current timestep = 766. State = [[-0.12239692  0.0117159 ]]. Action = [[ 0.08444434  0.0418354  -0.0521442  -0.14805669]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 766 is [True, False, False, False, True, False]
Current timestep = 767. State = [[-0.11289027  0.00387846]]. Action = [[-0.0400541  -0.22610526 -0.16403511  0.24308479]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 767 is [True, False, False, False, True, False]
Current timestep = 768. State = [[-0.10832351 -0.0161809 ]]. Action = [[ 0.09519786 -0.14913021 -0.03412774 -0.8073306 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 768 is [True, False, False, False, True, False]
Scene graph at timestep 768 is [True, False, False, False, True, False]
State prediction error at timestep 768 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 768 of 1
Current timestep = 769. State = [[-0.10635015 -0.02787993]]. Action = [[-0.1919978   0.05685812 -0.00135528  0.80230546]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 769 is [True, False, False, False, True, False]
Current timestep = 770. State = [[-0.10613819 -0.0156008 ]]. Action = [[ 0.17768228  0.18430734 -0.07871944  0.5969913 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 770 is [True, False, False, False, True, False]
Current timestep = 771. State = [[-0.09466892 -0.00560527]]. Action = [[ 0.23376289 -0.0324741   0.21657711 -0.38634497]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 771 is [True, False, False, False, True, False]
Scene graph at timestep 771 is [True, False, False, False, True, False]
State prediction error at timestep 771 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 771 of 1
Current timestep = 772. State = [[-0.068811   -0.00755489]]. Action = [[ 0.01307416 -0.07719302  0.03777698 -0.24371499]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 772 is [True, False, False, False, True, False]
Scene graph at timestep 772 is [True, False, False, False, True, False]
State prediction error at timestep 772 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 772 of 0
Current timestep = 773. State = [[-0.06576443 -0.00076763]]. Action = [[ 0.08095399  0.19452289 -0.21439555 -0.74960464]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 773 is [True, False, False, False, True, False]
Current timestep = 774. State = [[-0.06513689  0.023064  ]]. Action = [[-0.21349226  0.24376518  0.0086717  -0.67179877]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 774 is [True, False, False, False, True, False]
Current timestep = 775. State = [[-0.07698626  0.05850988]]. Action = [[-0.07472153  0.23555481 -0.14271905 -0.32637298]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 775 is [True, False, False, False, True, False]
Scene graph at timestep 775 is [True, False, False, False, True, False]
State prediction error at timestep 775 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 775 of -1
Current timestep = 776. State = [[-0.08577608  0.07411249]]. Action = [[-0.07338077 -0.16381371  0.11248732 -0.275419  ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 776 is [True, False, False, False, True, False]
Current timestep = 777. State = [[-0.08684084  0.06540626]]. Action = [[-0.02568139  0.01252449  0.05482802 -0.0062995 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 777 is [True, False, False, False, True, False]
Current timestep = 778. State = [[-0.08365685  0.05616545]]. Action = [[ 0.19369537 -0.12490359  0.21880245 -0.01463175]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 778 is [True, False, False, False, True, False]
Current timestep = 779. State = [[-0.0728174   0.05293567]]. Action = [[ 0.18645084  0.09493396 -0.08113831  0.9103744 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 779 is [True, False, False, False, True, False]
Scene graph at timestep 779 is [True, False, False, False, True, False]
State prediction error at timestep 779 is tensor(4.1654e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 779 of 1
Current timestep = 780. State = [[-0.05025665  0.04675132]]. Action = [[ 0.15610033 -0.16176578 -0.07018612 -0.44523013]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 780 is [True, False, False, False, True, False]
Current timestep = 781. State = [[-0.04001422  0.03547243]]. Action = [[-0.10430619 -0.0588939  -0.13923077 -0.6302978 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 781 is [True, False, False, False, True, False]
Current timestep = 782. State = [[-0.04300847  0.0255329 ]]. Action = [[-0.17336756 -0.05943841  0.21088594  0.00515258]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 782 is [False, True, False, False, True, False]
Current timestep = 783. State = [[-0.04309854  0.00558832]]. Action = [[ 0.11168325 -0.2322535  -0.23978455  0.9497435 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 783 is [False, True, False, False, True, False]
Current timestep = 784. State = [[-0.04464572 -0.02436306]]. Action = [[-0.11019242 -0.24711235 -0.08551916  0.15961456]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 784 is [False, True, False, False, True, False]
Current timestep = 785. State = [[-0.05575738 -0.05892747]]. Action = [[-0.14507772 -0.22942242 -0.12378588 -0.5781695 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 785 is [False, True, False, False, True, False]
Current timestep = 786. State = [[-0.0615426  -0.08644239]]. Action = [[ 0.10980004 -0.10723874  0.06233808 -0.17225802]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 786 is [True, False, False, False, True, False]
Current timestep = 787. State = [[-0.05828142 -0.09493259]]. Action = [[0.09300044 0.06092042 0.07510579 0.8313999 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 787 is [True, False, False, False, True, False]
Scene graph at timestep 787 is [True, False, False, False, True, False]
State prediction error at timestep 787 is tensor(5.7922e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 787 of 0
Current timestep = 788. State = [[-0.05465272 -0.09178836]]. Action = [[ 0.083134    0.04025313 -0.12450641  0.67800105]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 788 is [True, False, False, False, True, False]
Current timestep = 789. State = [[-0.05257765 -0.0903839 ]]. Action = [[-0.11036226 -0.02874613  0.14124751  0.92709374]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 789 is [True, False, False, False, True, False]
Current timestep = 790. State = [[-0.05241995 -0.08997207]]. Action = [[0.09923476 0.02577838 0.24128526 0.89068925]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 790 is [True, False, False, False, True, False]
Current timestep = 791. State = [[-0.05560097 -0.07688995]]. Action = [[-0.21977657  0.22820368  0.18135959  0.81283665]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 791 is [True, False, False, False, True, False]
Current timestep = 792. State = [[-0.05894712 -0.05846499]]. Action = [[0.05019408 0.04289073 0.17093891 0.14726996]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 792 is [True, False, False, False, True, False]
Current timestep = 793. State = [[-0.06007085 -0.04632348]]. Action = [[ 0.01585004  0.11186355 -0.069961    0.5719644 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 793 is [True, False, False, False, True, False]
Current timestep = 794. State = [[-0.05896145 -0.02325966]]. Action = [[ 0.09810483  0.22632277 -0.15247154 -0.67282945]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 794 is [True, False, False, False, True, False]
Current timestep = 795. State = [[-0.04962774 -0.0177913 ]]. Action = [[ 0.20422122 -0.19838493 -0.18068402 -0.94568807]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 795 is [True, False, False, False, True, False]
Current timestep = 796. State = [[-0.25133938  0.18361676]]. Action = [[ 0.14772952  0.1543926  -0.15292324 -0.6193945 ]]. Reward = [100.]
Curr episode timestep = 34
Scene graph at timestep 796 is [False, True, False, False, True, False]
Current timestep = 797. State = [[-0.24641049  0.20066755]]. Action = [[-0.04619451 -0.09921983 -0.08319905 -0.65823084]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 797 is [True, False, False, False, False, True]
Current timestep = 798. State = [[-0.24519804  0.1890919 ]]. Action = [[-0.04918075 -0.15455832  0.21556422  0.42741752]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 798 is [True, False, False, False, False, True]
Current timestep = 799. State = [[-0.23658304  0.16259839]]. Action = [[ 0.22205871 -0.22691877 -0.22611243 -0.90877163]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 799 is [True, False, False, False, False, True]
Current timestep = 800. State = [[-0.23227046  0.15055987]]. Action = [[-0.07023102  0.09532559 -0.05517736  0.1592443 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 800 is [True, False, False, False, False, True]
Current timestep = 801. State = [[-0.22826478  0.15469141]]. Action = [[ 0.13062337  0.08000743 -0.04970582  0.3303932 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 801 is [True, False, False, False, False, True]
Current timestep = 802. State = [[-0.21652971  0.15535377]]. Action = [[ 0.07001472 -0.08939078 -0.20863032 -0.0607655 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 802 is [True, False, False, False, False, True]
Current timestep = 803. State = [[-0.21641447  0.15649901]]. Action = [[-0.18019022  0.03137183 -0.10137844  0.10014141]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 803 is [True, False, False, False, False, True]
Current timestep = 804. State = [[-0.22340113  0.16189092]]. Action = [[-0.10118093  0.03531408 -0.21932752  0.8068696 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 804 is [True, False, False, False, False, True]
Current timestep = 805. State = [[-0.23758784  0.16100128]]. Action = [[-0.20078464 -0.10323486 -0.08819309  0.75050664]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 805 is [True, False, False, False, False, True]
Scene graph at timestep 805 is [True, False, False, False, False, True]
State prediction error at timestep 805 is tensor(9.6763e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 805 of 0
Current timestep = 806. State = [[-0.25256133  0.13843934]]. Action = [[ 0.06037289 -0.24236996  0.12978601 -0.00496912]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 806 is [True, False, False, False, False, True]
Current timestep = 807. State = [[-0.24303596  0.12837276]]. Action = [[ 0.23220053  0.12194055 -0.2177378  -0.5975266 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 807 is [True, False, False, False, False, True]
Current timestep = 808. State = [[-0.22933108  0.13585305]]. Action = [[ 0.14266533  0.08609211  0.07204252 -0.10128665]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 808 is [True, False, False, False, False, True]
Current timestep = 809. State = [[-0.22409418  0.14409421]]. Action = [[-0.243995    0.01159662 -0.13454707  0.02236998]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 809 is [True, False, False, False, False, True]
Current timestep = 810. State = [[-0.22316074  0.14515924]]. Action = [[ 0.22823647 -0.02704567  0.17851272  0.94750214]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 810 is [True, False, False, False, False, True]
Current timestep = 811. State = [[-0.22043364  0.13007501]]. Action = [[-0.1542702  -0.23699856 -0.16331795 -0.57867557]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 811 is [True, False, False, False, False, True]
Current timestep = 812. State = [[-0.22250038  0.11688457]]. Action = [[-0.05538666  0.02570176 -0.07237867  0.42035186]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 812 is [True, False, False, False, False, True]
Scene graph at timestep 812 is [True, False, False, False, True, False]
State prediction error at timestep 812 is tensor(8.7544e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 812 of 0
Current timestep = 813. State = [[-0.2224655   0.11644015]]. Action = [[ 0.13632756  0.04513419  0.0724012  -0.6535446 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 813 is [True, False, False, False, True, False]
Current timestep = 814. State = [[-0.22714435  0.13111366]]. Action = [[-0.10164046  0.24070466 -0.2342127  -0.51206595]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 814 is [True, False, False, False, True, False]
Current timestep = 815. State = [[-0.22284436  0.13284479]]. Action = [[ 0.23496628 -0.22079238 -0.00732583  0.1488496 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 815 is [True, False, False, False, False, True]
Scene graph at timestep 815 is [True, False, False, False, False, True]
State prediction error at timestep 815 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 815 of 0
Current timestep = 816. State = [[-0.20267183  0.13367552]]. Action = [[ 0.16241556  0.24596533  0.12260157 -0.20546007]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 816 is [True, False, False, False, False, True]
Current timestep = 817. State = [[-0.18851005  0.15466855]]. Action = [[-0.01123874  0.12287503 -0.07411191 -0.97112787]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 817 is [True, False, False, False, False, True]
Scene graph at timestep 817 is [True, False, False, False, False, True]
State prediction error at timestep 817 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 817 of 0
Current timestep = 818. State = [[-0.17963527  0.18137641]]. Action = [[ 0.16326618  0.21124989 -0.10823572 -0.4182626 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 818 is [True, False, False, False, False, True]
Current timestep = 819. State = [[-0.16479355  0.18293317]]. Action = [[-0.04250279 -0.24757099  0.02752265  0.5544448 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 819 is [True, False, False, False, False, True]
Current timestep = 820. State = [[-0.1545948   0.17053212]]. Action = [[ 0.20189139 -0.02812704  0.07440567  0.8477813 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 820 is [True, False, False, False, False, True]
Current timestep = 821. State = [[-0.14772119  0.17702864]]. Action = [[-0.10568792  0.19016707 -0.18217382 -0.8729403 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 821 is [True, False, False, False, False, True]
Current timestep = 822. State = [[-0.14932057  0.17801905]]. Action = [[-0.05293183 -0.1837234   0.00892714 -0.888169  ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 822 is [True, False, False, False, False, True]
Current timestep = 823. State = [[-0.15124509  0.16533467]]. Action = [[-0.11645773 -0.11178342 -0.1309676  -0.32966036]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 823 is [True, False, False, False, False, True]
Current timestep = 824. State = [[-0.15861356  0.14851226]]. Action = [[-0.1271602  -0.1700566   0.21701258 -0.1121248 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 824 is [True, False, False, False, False, True]
Current timestep = 825. State = [[-0.17761612  0.14717537]]. Action = [[-0.1496721   0.22199214  0.06479117 -0.16363871]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 825 is [True, False, False, False, False, True]
Current timestep = 826. State = [[-0.1818372   0.15603267]]. Action = [[ 0.229072   -0.06161098 -0.17261323  0.5726142 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 826 is [True, False, False, False, False, True]
Scene graph at timestep 826 is [True, False, False, False, False, True]
State prediction error at timestep 826 is tensor(4.9318e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 826 of 0
Current timestep = 827. State = [[-0.16709599  0.1373674 ]]. Action = [[ 0.22714666 -0.22647817 -0.15263695  0.05392885]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 827 is [True, False, False, False, False, True]
Current timestep = 828. State = [[-0.14711006  0.10930707]]. Action = [[ 0.1285576  -0.14686623 -0.02381587  0.34522808]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 828 is [True, False, False, False, False, True]
Current timestep = 829. State = [[-0.13066767  0.09940081]]. Action = [[ 0.15148771  0.07269329 -0.1265579  -0.9952953 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 829 is [True, False, False, False, True, False]
Current timestep = 830. State = [[-0.11647443  0.10163221]]. Action = [[-0.01019119 -0.03452112  0.193881   -0.18475956]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 830 is [True, False, False, False, True, False]
Current timestep = 831. State = [[-0.11435111  0.09819707]]. Action = [[ 0.03453946 -0.01922092  0.11142883  0.071697  ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 831 is [True, False, False, False, True, False]
Current timestep = 832. State = [[-0.11543752  0.08380527]]. Action = [[-0.19339381 -0.22804993  0.20893502 -0.68136084]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 832 is [True, False, False, False, True, False]
Current timestep = 833. State = [[-0.11321169  0.06086565]]. Action = [[ 0.11118394 -0.1360196  -0.08851248  0.8567493 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 833 is [True, False, False, False, True, False]
Current timestep = 834. State = [[-0.11501148  0.04455231]]. Action = [[-0.20980726 -0.0659644  -0.1270128   0.3424977 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 834 is [True, False, False, False, True, False]
Current timestep = 835. State = [[-0.12183898  0.03421025]]. Action = [[ 0.03035229 -0.0524057   0.18753105 -0.0141868 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 835 is [True, False, False, False, True, False]
Current timestep = 836. State = [[-0.1200082   0.02961011]]. Action = [[ 0.13326731 -0.00764351  0.19306788  0.8192878 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 836 is [True, False, False, False, True, False]
Current timestep = 837. State = [[-0.12006111  0.02275734]]. Action = [[-0.14502084 -0.07305101 -0.22174582 -0.95912665]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 837 is [True, False, False, False, True, False]
Scene graph at timestep 837 is [True, False, False, False, True, False]
State prediction error at timestep 837 is tensor(7.5326e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 837 of 1
Current timestep = 838. State = [[-0.12414176  0.0199253 ]]. Action = [[ 0.12465769  0.10724995  0.2366865  -0.48479307]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 838 is [True, False, False, False, True, False]
Current timestep = 839. State = [[-0.11321373  0.03445552]]. Action = [[ 0.2364139   0.17914122  0.1905691  -0.70424247]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 839 is [True, False, False, False, True, False]
Current timestep = 840. State = [[-0.08957651  0.05612802]]. Action = [[ 0.1791727   0.13672161  0.05218503 -0.21515375]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 840 is [True, False, False, False, True, False]
Current timestep = 841. State = [[-0.07985973  0.07632849]]. Action = [[-0.17106488  0.14069173 -0.00303558 -0.517182  ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 841 is [True, False, False, False, True, False]
Current timestep = 842. State = [[-0.08828125  0.09772319]]. Action = [[-0.10967948  0.1139183  -0.05504796 -0.01070118]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 842 is [True, False, False, False, True, False]
Current timestep = 843. State = [[-0.09266198  0.10825097]]. Action = [[0.03265461 0.00539628 0.09916666 0.3880968 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 843 is [True, False, False, False, True, False]
Current timestep = 844. State = [[-0.08814974  0.09938812]]. Action = [[ 0.16232258 -0.17280743 -0.21968205  0.08046532]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 844 is [True, False, False, False, True, False]
Current timestep = 845. State = [[-0.08440794  0.08903755]]. Action = [[-0.09107926 -0.03051302 -0.12512583 -0.68229735]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 845 is [True, False, False, False, True, False]
Current timestep = 846. State = [[-0.08696347  0.07941703]]. Action = [[-0.11332843 -0.11844003  0.02259254  0.7492242 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 846 is [True, False, False, False, True, False]
Current timestep = 847. State = [[-0.09305637  0.0839296 ]]. Action = [[-0.0507929   0.21218291  0.11127818  0.53120995]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 847 is [True, False, False, False, True, False]
Current timestep = 848. State = [[-0.09463074  0.08093533]]. Action = [[ 0.0563769  -0.2263952  -0.08450404 -0.58212227]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 848 is [True, False, False, False, True, False]
Current timestep = 849. State = [[-0.08820856  0.05967466]]. Action = [[ 0.13850936 -0.16375016 -0.03127293 -0.91765493]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 849 is [True, False, False, False, True, False]
Current timestep = 850. State = [[-0.08601259  0.03251924]]. Action = [[-0.11912212 -0.21031795 -0.15920132 -0.5654293 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 850 is [True, False, False, False, True, False]
Current timestep = 851. State = [[-0.08224356  0.02624483]]. Action = [[0.18890756 0.192213   0.23727405 0.83652115]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 851 is [True, False, False, False, True, False]
Scene graph at timestep 851 is [True, False, False, False, True, False]
State prediction error at timestep 851 is tensor(2.1784e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 851 of 1
Current timestep = 852. State = [[-0.08246008  0.04332976]]. Action = [[-0.1299989   0.17507046  0.16458625 -0.68177485]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 852 is [True, False, False, False, True, False]
Scene graph at timestep 852 is [True, False, False, False, True, False]
State prediction error at timestep 852 is tensor(4.6823e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 852 of -1
Current timestep = 853. State = [[-0.09024093  0.05104631]]. Action = [[-0.12260678 -0.15400371 -0.15990557 -0.03347439]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 853 is [True, False, False, False, True, False]
Current timestep = 854. State = [[-0.08994207  0.04068783]]. Action = [[ 0.11042213 -0.05994776 -0.18935065  0.5959214 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 854 is [True, False, False, False, True, False]
Current timestep = 855. State = [[-0.08774333  0.04141104]]. Action = [[ 0.07673687  0.13450623 -0.10965666  0.7940781 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 855 is [True, False, False, False, True, False]
Current timestep = 856. State = [[-0.08718934  0.04424528]]. Action = [[ 1.0958314e-04 -3.4643099e-02 -1.2315196e-01 -9.0133744e-01]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 856 is [True, False, False, False, True, False]
Current timestep = 857. State = [[-0.07931812  0.04702633]]. Action = [[ 0.17604268  0.06171742 -0.21718392 -0.41511458]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 857 is [True, False, False, False, True, False]
Current timestep = 858. State = [[-0.06767776  0.04465228]]. Action = [[-0.0453424  -0.13475397 -0.23341659 -0.4331001 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 858 is [True, False, False, False, True, False]
Scene graph at timestep 858 is [True, False, False, False, True, False]
State prediction error at timestep 858 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 858 of 1
Current timestep = 859. State = [[-0.05958641  0.02806068]]. Action = [[ 0.16268724 -0.13848066  0.03078946  0.789685  ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 859 is [True, False, False, False, True, False]
Current timestep = 860. State = [[-0.04352243  0.00464183]]. Action = [[ 0.19785628 -0.22532451 -0.04367782 -0.2533049 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 860 is [True, False, False, False, True, False]
Current timestep = 861. State = [[-0.221607   -0.01203161]]. Action = [[ 0.12814993  0.05340979 -0.05333686  0.02607441]]. Reward = [100.]
Curr episode timestep = 64
Scene graph at timestep 861 is [False, True, False, False, True, False]
Current timestep = 862. State = [[-0.21728563 -0.00252829]]. Action = [[-0.18149707  0.2242909  -0.11288926 -0.7019592 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 862 is [True, False, False, False, True, False]
Current timestep = 863. State = [[-0.21680534 -0.00222891]]. Action = [[ 0.19817108 -0.23849513 -0.15472652 -0.16345763]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 863 is [True, False, False, False, True, False]
Current timestep = 864. State = [[-0.21394396 -0.00994088]]. Action = [[-0.03316775  0.07039642  0.22284347 -0.26002157]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 864 is [True, False, False, False, True, False]
Scene graph at timestep 864 is [True, False, False, False, True, False]
State prediction error at timestep 864 is tensor(6.4126e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 864 of 0
Current timestep = 865. State = [[-0.2149984  -0.01019286]]. Action = [[-0.08375284 -0.02335373  0.05069369 -0.0406124 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 865 is [True, False, False, False, True, False]
Current timestep = 866. State = [[-0.2175532  -0.01246705]]. Action = [[-0.05484506 -0.02875873  0.09023255  0.18320024]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 866 is [True, False, False, False, True, False]
Current timestep = 867. State = [[-0.22663455 -0.02674486]]. Action = [[-0.16530527 -0.19356766  0.0393039   0.5676886 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 867 is [True, False, False, False, True, False]
Current timestep = 868. State = [[-0.23829237 -0.04277743]]. Action = [[ 0.00177273 -0.03410915 -0.20918217 -0.13029516]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 868 is [True, False, False, False, True, False]
Scene graph at timestep 868 is [True, False, False, False, True, False]
State prediction error at timestep 868 is tensor(1.4775e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 868 of -1
Current timestep = 869. State = [[-0.23559211 -0.03624471]]. Action = [[ 0.206307    0.22339875 -0.13332029 -0.6838121 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 869 is [True, False, False, False, True, False]
Current timestep = 870. State = [[-0.21783297 -0.01281418]]. Action = [[0.2250852  0.15487522 0.21659061 0.98781574]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 870 is [True, False, False, False, True, False]
Current timestep = 871. State = [[-0.20205869  0.01045167]]. Action = [[-0.00910518  0.18073875 -0.06539366  0.3053403 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 871 is [True, False, False, False, True, False]
Current timestep = 872. State = [[-0.19451961  0.02930215]]. Action = [[ 0.1113157   0.07524306 -0.16879924  0.6634815 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 872 is [True, False, False, False, True, False]
Scene graph at timestep 872 is [True, False, False, False, True, False]
State prediction error at timestep 872 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 872 of 1
Current timestep = 873. State = [[-0.17978927  0.02983843]]. Action = [[ 0.0357821  -0.17639841  0.17318434 -0.83402246]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 873 is [True, False, False, False, True, False]
Scene graph at timestep 873 is [True, False, False, False, True, False]
State prediction error at timestep 873 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 873 of 0
Current timestep = 874. State = [[-0.17836335  0.03296335]]. Action = [[ 0.01578057  0.24196434 -0.17058493  0.6285682 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 874 is [True, False, False, False, True, False]
Current timestep = 875. State = [[-0.16791542  0.03822398]]. Action = [[ 0.16304094 -0.14270577  0.10251781 -0.8855828 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 875 is [True, False, False, False, True, False]
Current timestep = 876. State = [[-0.15686803  0.02341614]]. Action = [[-0.10029413 -0.18021984 -0.22537516 -0.8788333 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 876 is [True, False, False, False, True, False]
Scene graph at timestep 876 is [True, False, False, False, True, False]
State prediction error at timestep 876 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 876 of 0
Current timestep = 877. State = [[-0.14914626 -0.00237986]]. Action = [[ 0.22844881 -0.17615953  0.21068871  0.572659  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 877 is [True, False, False, False, True, False]
Current timestep = 878. State = [[-0.1367411  -0.02997295]]. Action = [[-0.04465756 -0.23552383  0.10630643  0.58656526]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 878 is [True, False, False, False, True, False]
Scene graph at timestep 878 is [True, False, False, False, True, False]
State prediction error at timestep 878 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 878 of 1
Current timestep = 879. State = [[-0.14112084 -0.05243792]]. Action = [[-0.22419313 -0.01148573  0.22050512 -0.76462126]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 879 is [True, False, False, False, True, False]
Current timestep = 880. State = [[-0.14880297 -0.05864952]]. Action = [[-0.00731343 -0.0414649  -0.14608574  0.97477865]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 880 is [True, False, False, False, True, False]
Current timestep = 881. State = [[-0.15002005 -0.06103881]]. Action = [[ 0.13366008  0.00360209  0.04837707 -0.6313867 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 881 is [True, False, False, False, True, False]
Current timestep = 882. State = [[-0.14949481 -0.0535411 ]]. Action = [[-0.00814089  0.13710329 -0.05658895  0.26988506]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 882 is [True, False, False, False, True, False]
Scene graph at timestep 882 is [True, False, False, False, True, False]
State prediction error at timestep 882 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 882 of 0
Current timestep = 883. State = [[-0.14160648 -0.03085624]]. Action = [[ 0.18310818  0.22927266 -0.20941353  0.6172315 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 883 is [True, False, False, False, True, False]
Current timestep = 884. State = [[-0.13057521 -0.007364  ]]. Action = [[-0.00752094  0.13114142  0.23909631 -0.78433084]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 884 is [True, False, False, False, True, False]
Current timestep = 885. State = [[-0.12695627  0.01292434]]. Action = [[ 0.09499055  0.12868395 -0.03794108  0.5309386 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 885 is [True, False, False, False, True, False]
Current timestep = 886. State = [[-0.10982145  0.01317485]]. Action = [[ 0.14255726 -0.18735471 -0.17047048  0.95645595]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 886 is [True, False, False, False, True, False]
Scene graph at timestep 886 is [True, False, False, False, True, False]
State prediction error at timestep 886 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 886 of 1
Current timestep = 887. State = [[-0.08732004  0.00655955]]. Action = [[ 0.15165263  0.04695401 -0.1565409   0.49050033]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 887 is [True, False, False, False, True, False]
Current timestep = 888. State = [[-0.07190328 -0.00169422]]. Action = [[ 0.05385655 -0.16740215 -0.19845885  0.83912003]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 888 is [True, False, False, False, True, False]
Current timestep = 889. State = [[-0.05434493 -0.02467542]]. Action = [[ 0.24415559 -0.2223148   0.19120997  0.43689108]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 889 is [True, False, False, False, True, False]
Current timestep = 890. State = [[-0.19220018  0.04975579]]. Action = [[ 0.01308373  0.08027574  0.08402109 -0.36829168]]. Reward = [100.]
Curr episode timestep = 28
Scene graph at timestep 890 is [True, False, False, False, True, False]
Current timestep = 891. State = [[-0.18567404  0.06836412]]. Action = [[-0.16570003  0.16691804  0.21109843 -0.8276962 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 891 is [True, False, False, False, True, False]
Current timestep = 892. State = [[-0.19668396  0.07634164]]. Action = [[-0.14762837 -0.10398227 -0.1715647   0.3987261 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 892 is [True, False, False, False, True, False]
Current timestep = 893. State = [[-0.19994123  0.06285273]]. Action = [[ 0.05969349 -0.1654299   0.15623385 -0.18603057]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 893 is [True, False, False, False, True, False]
Current timestep = 894. State = [[-0.19532304  0.05668848]]. Action = [[ 0.17031744  0.11947793 -0.06437626 -0.8378178 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 894 is [True, False, False, False, True, False]
Scene graph at timestep 894 is [True, False, False, False, True, False]
State prediction error at timestep 894 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 894 of 0
Current timestep = 895. State = [[-0.18068588  0.0519235 ]]. Action = [[ 0.23667169 -0.16785744 -0.21953912  0.5827167 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 895 is [True, False, False, False, True, False]
Current timestep = 896. State = [[-0.16730319  0.03938578]]. Action = [[-0.17250222 -0.08240624  0.05508861  0.33641028]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 896 is [True, False, False, False, True, False]
Current timestep = 897. State = [[-0.17062761  0.03293262]]. Action = [[-0.08520025  0.02569613  0.12073138  0.38000345]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 897 is [True, False, False, False, True, False]
Scene graph at timestep 897 is [True, False, False, False, True, False]
State prediction error at timestep 897 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 897 of 0
Current timestep = 898. State = [[-0.18253744  0.04031463]]. Action = [[-0.16202028  0.13570178  0.17107415  0.48233306]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 898 is [True, False, False, False, True, False]
Current timestep = 899. State = [[-0.20806645  0.06392457]]. Action = [[-0.2284184   0.21790084  0.02001029 -0.32107902]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 899 is [True, False, False, False, True, False]
Current timestep = 900. State = [[-0.22565551  0.09234878]]. Action = [[ 0.0718945   0.1837101  -0.18603529 -0.9144541 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 900 is [True, False, False, False, True, False]
Current timestep = 901. State = [[-0.23211165  0.10107249]]. Action = [[-0.13739836 -0.11210838  0.16473371  0.22628379]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 901 is [True, False, False, False, True, False]
Current timestep = 902. State = [[-0.24312344  0.08822253]]. Action = [[-0.10286453 -0.15642563 -0.15664352  0.87996054]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 902 is [True, False, False, False, True, False]
Current timestep = 903. State = [[-0.24566127  0.08706643]]. Action = [[ 0.23226279  0.20285863 -0.06594194  0.15585184]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 903 is [True, False, False, False, True, False]
Current timestep = 904. State = [[-0.2307018  0.0947955]]. Action = [[ 0.20308292 -0.02213307  0.12327555 -0.80226535]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 904 is [True, False, False, False, True, False]
Current timestep = 905. State = [[-0.21450366  0.08608772]]. Action = [[-0.02784659 -0.19898598  0.12101272 -0.5060201 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 905 is [True, False, False, False, True, False]
Current timestep = 906. State = [[-0.21021572  0.06064016]]. Action = [[ 0.03041327 -0.22331269 -0.10490948 -0.77627665]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 906 is [True, False, False, False, True, False]
Current timestep = 907. State = [[-0.21213213  0.05270969]]. Action = [[-0.15607846  0.15963244 -0.20745361 -0.3265283 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 907 is [True, False, False, False, True, False]
Current timestep = 908. State = [[-0.21996133  0.07177779]]. Action = [[0.01468748 0.22615871 0.1245721  0.01000953]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 908 is [True, False, False, False, True, False]
Scene graph at timestep 908 is [True, False, False, False, True, False]
State prediction error at timestep 908 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 908 of -1
Current timestep = 909. State = [[-0.2259084   0.07800772]]. Action = [[-0.06204258 -0.21975845  0.16342813 -0.8490125 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 909 is [True, False, False, False, True, False]
Current timestep = 910. State = [[-0.21935758  0.05543464]]. Action = [[ 0.19543898 -0.17529133  0.11126816  0.75924945]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 910 is [True, False, False, False, True, False]
Current timestep = 911. State = [[-0.21237855  0.03651149]]. Action = [[-0.08884282 -0.08463365 -0.22556953 -0.14128035]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 911 is [True, False, False, False, True, False]
Current timestep = 912. State = [[-0.2209822   0.04131811]]. Action = [[-0.21944241  0.22034997  0.08721471 -0.80673176]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 912 is [True, False, False, False, True, False]
Current timestep = 913. State = [[-0.22858365  0.04425839]]. Action = [[ 0.17834008 -0.14946266 -0.20955884 -0.3410027 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 913 is [True, False, False, False, True, False]
Current timestep = 914. State = [[-0.21896492  0.04655493]]. Action = [[0.17100072 0.17326203 0.20662642 0.7150465 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 914 is [True, False, False, False, True, False]
Current timestep = 915. State = [[-0.21471933  0.05899376]]. Action = [[-0.19011681  0.08280119 -0.22938037 -0.9628787 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 915 is [True, False, False, False, True, False]
Current timestep = 916. State = [[-0.22459854  0.07907199]]. Action = [[-0.10594514  0.18124396 -0.1133686   0.06370938]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 916 is [True, False, False, False, True, False]
Current timestep = 917. State = [[-0.22573793  0.10382392]]. Action = [[ 0.23604208  0.19459826 -0.16455038  0.73825   ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 917 is [True, False, False, False, True, False]
Current timestep = 918. State = [[-0.20551415  0.12127871]]. Action = [[ 0.24230394  0.09557483 -0.06174091  0.9017079 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 918 is [True, False, False, False, True, False]
Current timestep = 919. State = [[-0.188146    0.12699583]]. Action = [[-0.04559243 -0.10034502 -0.05874097  0.84554684]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 919 is [True, False, False, False, True, False]
Current timestep = 920. State = [[-0.18389402  0.11347266]]. Action = [[ 0.01013064 -0.21480319  0.04945642  0.5907228 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 920 is [True, False, False, False, False, True]
Current timestep = 921. State = [[-0.18528363  0.10444313]]. Action = [[-0.14142513  0.06937861 -0.15041716 -0.9010784 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 921 is [True, False, False, False, True, False]
Current timestep = 922. State = [[-0.1846743   0.09475467]]. Action = [[ 0.10185963 -0.17763829 -0.23642984 -0.97088706]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 922 is [True, False, False, False, True, False]
Current timestep = 923. State = [[-0.19017555  0.08808644]]. Action = [[-0.21975473  0.04905784  0.14008012 -0.30050886]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 923 is [True, False, False, False, True, False]
Current timestep = 924. State = [[-0.209848    0.09169314]]. Action = [[-0.17697178  0.04449105  0.02419904 -0.6565795 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 924 is [True, False, False, False, True, False]
Current timestep = 925. State = [[-0.2312513   0.10397696]]. Action = [[-0.15342447  0.14645597 -0.23229377  0.840009  ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 925 is [True, False, False, False, True, False]
Current timestep = 926. State = [[-0.23890674  0.10089871]]. Action = [[ 0.20610118 -0.20491384 -0.06201391 -0.82616156]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 926 is [True, False, False, False, True, False]
Current timestep = 927. State = [[-0.24206522  0.0945781 ]]. Action = [[-0.19853142  0.06319574  0.18779558  0.39633584]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 927 is [True, False, False, False, True, False]
Current timestep = 928. State = [[-0.25018495  0.08415231]]. Action = [[-0.04754356 -0.1748563  -0.13143739 -0.09858835]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 928 is [True, False, False, False, True, False]
Current timestep = 929. State = [[-0.25337964  0.07086436]]. Action = [[ 0.01587692 -0.04540062 -0.07510729  0.8429215 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 929 is [True, False, False, False, True, False]
Scene graph at timestep 929 is [True, False, False, False, True, False]
State prediction error at timestep 929 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 929 of -1
Current timestep = 930. State = [[-0.25368097  0.05442283]]. Action = [[ 0.0472008  -0.14506659  0.16606718 -0.81403583]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 930 is [True, False, False, False, True, False]
Current timestep = 931. State = [[-0.24832703  0.04186855]]. Action = [[ 0.13142121 -0.05593267 -0.0996761   0.12201047]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 931 is [True, False, False, False, True, False]
Current timestep = 932. State = [[-0.2372024   0.03365697]]. Action = [[ 0.13617143 -0.00300449 -0.07290545 -0.6314479 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 932 is [True, False, False, False, True, False]
Scene graph at timestep 932 is [True, False, False, False, True, False]
State prediction error at timestep 932 is tensor(7.7483e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 932 of 0
Current timestep = 933. State = [[-0.23168768  0.04371896]]. Action = [[-0.05689299  0.22452801  0.13409853  0.09467423]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 933 is [True, False, False, False, True, False]
Scene graph at timestep 933 is [True, False, False, False, True, False]
State prediction error at timestep 933 is tensor(3.6222e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 933 of -1
Current timestep = 934. State = [[-0.23793061  0.06773292]]. Action = [[-0.06642285  0.14752823 -0.20905513 -0.27097023]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 934 is [True, False, False, False, True, False]
Current timestep = 935. State = [[-0.23953238  0.06749785]]. Action = [[ 0.00738001 -0.19286363 -0.12099236  0.41584682]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 935 is [True, False, False, False, True, False]
Current timestep = 936. State = [[-0.2297511   0.05344936]]. Action = [[ 0.22436163 -0.10513934  0.18102276 -0.37211692]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 936 is [True, False, False, False, True, False]
Current timestep = 937. State = [[-0.20722193  0.03728998]]. Action = [[ 0.20073307 -0.07769644  0.16609162  0.00504339]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 937 is [True, False, False, False, True, False]
Current timestep = 938. State = [[-0.17739856  0.02551409]]. Action = [[ 0.23716474 -0.08704229  0.06131417 -0.13793904]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 938 is [True, False, False, False, True, False]
Current timestep = 939. State = [[-0.151199   0.0236945]]. Action = [[ 0.14884645  0.1011143  -0.17453995  0.11559701]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 939 is [True, False, False, False, True, False]
Current timestep = 940. State = [[-0.12639399  0.01922036]]. Action = [[ 0.16401678 -0.1801613   0.07217368 -0.37718165]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 940 is [True, False, False, False, True, False]
Current timestep = 941. State = [[-0.10343846 -0.00284055]]. Action = [[ 0.13916731 -0.19112965 -0.04595035  0.7045517 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 941 is [True, False, False, False, True, False]
Current timestep = 942. State = [[-0.09035761 -0.02581091]]. Action = [[-0.04166    -0.10548298 -0.21583134 -0.75874484]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 942 is [True, False, False, False, True, False]
Current timestep = 943. State = [[-0.09450822 -0.03795433]]. Action = [[-0.2301969  -0.03745948 -0.16499373 -0.4774446 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 943 is [True, False, False, False, True, False]
Current timestep = 944. State = [[-0.10316782 -0.03552441]]. Action = [[-0.02395341  0.13908207  0.08705747  0.38279438]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 944 is [True, False, False, False, True, False]
Current timestep = 945. State = [[-0.10589677 -0.02428556]]. Action = [[ 0.01321125  0.05376831 -0.09565827 -0.03527886]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 945 is [True, False, False, False, True, False]
Scene graph at timestep 945 is [True, False, False, False, True, False]
State prediction error at timestep 945 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 945 of 1
Current timestep = 946. State = [[-0.11161713 -0.03029533]]. Action = [[-0.10582733 -0.18155153  0.08259505  0.8568692 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 946 is [True, False, False, False, True, False]
Current timestep = 947. State = [[-0.1290285  -0.04674288]]. Action = [[-0.23970892 -0.09251149  0.20716801 -0.44482112]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 947 is [True, False, False, False, True, False]
Scene graph at timestep 947 is [True, False, False, False, True, False]
State prediction error at timestep 947 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 947 of -1
Current timestep = 948. State = [[-0.16250324 -0.06705337]]. Action = [[-0.17452121 -0.15203893  0.08638328 -0.06227404]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 948 is [True, False, False, False, True, False]
Current timestep = 949. State = [[-0.17014626 -0.08646072]]. Action = [[ 0.17450982 -0.14888331  0.23247609 -0.8222009 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 949 is [True, False, False, False, True, False]
Current timestep = 950. State = [[-0.16730832 -0.11060022]]. Action = [[-0.04065187 -0.17839018 -0.16849613 -0.04734784]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 950 is [True, False, False, False, True, False]
Current timestep = 951. State = [[-0.1748591  -0.12153341]]. Action = [[-0.19460069  0.079725   -0.05165565  0.00476861]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 951 is [True, False, False, False, True, False]
Current timestep = 952. State = [[-0.1889158  -0.12418865]]. Action = [[-0.1559314  -0.01500766  0.22924691  0.6924404 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 952 is [True, False, False, False, True, False]
Current timestep = 953. State = [[-0.21385653 -0.1313978 ]]. Action = [[-0.17039073 -0.07280758 -0.12776756  0.47192264]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 953 is [True, False, False, False, True, False]
Current timestep = 954. State = [[-0.2280029 -0.1426134]]. Action = [[ 0.06550086 -0.13861586 -0.11131328  0.65473616]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 954 is [True, False, False, True, False, False]
Current timestep = 955. State = [[-0.22506486 -0.15167558]]. Action = [[ 0.13761914 -0.0257618  -0.06013349  0.47156358]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 955 is [True, False, False, True, False, False]
Current timestep = 956. State = [[-0.22749072 -0.15097427]]. Action = [[-0.21498245  0.10681522  0.12589157  0.8906996 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 956 is [True, False, False, True, False, False]
Scene graph at timestep 956 is [True, False, False, True, False, False]
State prediction error at timestep 956 is tensor(5.4431e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 956 of -1
Current timestep = 957. State = [[-0.24659142 -0.1351431 ]]. Action = [[-0.1679968   0.21961224  0.04798132  0.9631536 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 957 is [True, False, False, True, False, False]
Current timestep = 958. State = [[-0.2530165  -0.12211654]]. Action = [[ 0.17990535 -0.06052484  0.01023817 -0.17260796]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 958 is [True, False, False, True, False, False]
Current timestep = 959. State = [[-0.24230608 -0.12627213]]. Action = [[ 0.18793893 -0.08899778  0.00110322  0.689157  ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 959 is [True, False, False, False, True, False]
Current timestep = 960. State = [[-0.22930175 -0.13990714]]. Action = [[ 0.08547008 -0.19718926  0.07446438 -0.26271534]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 960 is [True, False, False, True, False, False]
Current timestep = 961. State = [[-0.22321838 -0.14354447]]. Action = [[-0.08096281  0.21357632 -0.10514431  0.1882143 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 961 is [True, False, False, True, False, False]
Current timestep = 962. State = [[-0.21741802 -0.12467977]]. Action = [[ 0.13642687  0.17808047 -0.12408668 -0.53984475]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 962 is [True, False, False, True, False, False]
Current timestep = 963. State = [[-0.20003578 -0.10983218]]. Action = [[ 0.22217321 -0.01556058  0.20999914 -0.9048789 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 963 is [True, False, False, False, True, False]
Current timestep = 964. State = [[-0.19161285 -0.11827084]]. Action = [[-0.19318642 -0.19916569 -0.05847767  0.56395197]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 964 is [True, False, False, False, True, False]
Scene graph at timestep 964 is [True, False, False, False, True, False]
State prediction error at timestep 964 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 964 of 0
Current timestep = 965. State = [[-0.19218032 -0.12633745]]. Action = [[ 0.13902178  0.09971464 -0.18304297  0.6893841 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 965 is [True, False, False, False, True, False]
Scene graph at timestep 965 is [True, False, False, True, False, False]
State prediction error at timestep 965 is tensor(8.1425e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 965 of 1
Current timestep = 966. State = [[-0.18113692 -0.10721756]]. Action = [[ 0.07075277  0.24078232  0.02689913 -0.30959916]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 966 is [True, False, False, True, False, False]
Scene graph at timestep 966 is [True, False, False, False, True, False]
State prediction error at timestep 966 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 966 of 1
Current timestep = 967. State = [[-0.17232838 -0.09499882]]. Action = [[ 0.10862485 -0.15739074 -0.18316211  0.15761721]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 967 is [True, False, False, False, True, False]
Current timestep = 968. State = [[-0.15665951 -0.10014454]]. Action = [[ 0.16700011  0.01759064 -0.20183496  0.5576272 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 968 is [True, False, False, False, True, False]
Current timestep = 969. State = [[-0.1428772  -0.11049265]]. Action = [[ 0.00637257 -0.18491289 -0.01579098 -0.7486933 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 969 is [True, False, False, False, True, False]
Current timestep = 970. State = [[-0.13550973 -0.12810041]]. Action = [[ 0.05238354 -0.0698162   0.09481627 -0.11599797]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 970 is [True, False, False, False, True, False]
Scene graph at timestep 970 is [True, False, False, True, False, False]
State prediction error at timestep 970 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 970 of 1
Current timestep = 971. State = [[-0.12124006 -0.12895599]]. Action = [[ 0.15365359  0.1369245   0.16297692 -0.30511653]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 971 is [True, False, False, True, False, False]
Scene graph at timestep 971 is [True, False, False, True, False, False]
State prediction error at timestep 971 is tensor(2.3651e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 971 of 1
Current timestep = 972. State = [[-0.10141758 -0.12024631]]. Action = [[ 0.08316714  0.03161964 -0.22313452  0.19543731]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 972 is [True, False, False, True, False, False]
Current timestep = 973. State = [[-0.09286399 -0.12667692]]. Action = [[ 0.0856967  -0.192939    0.05782738 -0.4111361 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 973 is [True, False, False, False, True, False]
Current timestep = 974. State = [[-0.08498909 -0.1283509 ]]. Action = [[-0.05043323  0.1946843  -0.16678378  0.94449353]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 974 is [True, False, False, True, False, False]
Current timestep = 975. State = [[-0.09155613 -0.11024437]]. Action = [[-0.23776434  0.18712223  0.08325586  0.739321  ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 975 is [True, False, False, True, False, False]
Current timestep = 976. State = [[-0.10970502 -0.10138446]]. Action = [[-0.20505127 -0.08260897 -0.17585492  0.2715478 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 976 is [True, False, False, False, True, False]
Scene graph at timestep 976 is [True, False, False, False, True, False]
State prediction error at timestep 976 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 976 of 0
Current timestep = 977. State = [[-0.12986112 -0.09143396]]. Action = [[ 0.07371446  0.23031434 -0.09182432 -0.8296133 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 977 is [True, False, False, False, True, False]
Current timestep = 978. State = [[-0.12108808 -0.06940144]]. Action = [[ 0.20799905  0.07579705 -0.09943599 -0.534789  ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 978 is [True, False, False, False, True, False]
Current timestep = 979. State = [[-0.11298585 -0.07293463]]. Action = [[-0.05655292 -0.21083875 -0.10003102 -0.20800269]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 979 is [True, False, False, False, True, False]
Current timestep = 980. State = [[-0.10798125 -0.09363747]]. Action = [[ 0.12302467 -0.2146658   0.23436809  0.9196526 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 980 is [True, False, False, False, True, False]
Scene graph at timestep 980 is [True, False, False, False, True, False]
State prediction error at timestep 980 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 980 of 1
Current timestep = 981. State = [[-0.10587704 -0.10893628]]. Action = [[-0.16153371  0.1305592  -0.23731361 -0.7403948 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 981 is [True, False, False, False, True, False]
Current timestep = 982. State = [[-0.1149983  -0.09466335]]. Action = [[-0.16815333  0.14887682 -0.2181756  -0.7006602 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 982 is [True, False, False, False, True, False]
Scene graph at timestep 982 is [True, False, False, False, True, False]
State prediction error at timestep 982 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 982 of -1
Current timestep = 983. State = [[-0.12605996 -0.09069819]]. Action = [[ 0.02768224 -0.18141533  0.0493921  -0.80225015]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 983 is [True, False, False, False, True, False]
Current timestep = 984. State = [[-0.13112569 -0.11230991]]. Action = [[ 0.01929229 -0.22797145 -0.17107168  0.65291536]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 984 is [True, False, False, False, True, False]
Current timestep = 985. State = [[-0.1238331 -0.1263921]]. Action = [[ 0.21055704  0.06755358 -0.01018879 -0.7028705 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 985 is [True, False, False, False, True, False]
Current timestep = 986. State = [[-0.11035447 -0.11368991]]. Action = [[ 0.05451751  0.21916431  0.24372762 -0.37018657]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 986 is [True, False, False, True, False, False]
Current timestep = 987. State = [[-0.09895259 -0.08845618]]. Action = [[ 0.09427696  0.21883523  0.18421271 -0.02117866]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 987 is [True, False, False, False, True, False]
Current timestep = 988. State = [[-0.08274618 -0.08117858]]. Action = [[ 0.21936062 -0.20928621 -0.0032616   0.18623602]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 988 is [True, False, False, False, True, False]
Current timestep = 989. State = [[-0.06860011 -0.09159141]]. Action = [[-0.08262412 -0.04371355 -0.14830542  0.2977531 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 989 is [True, False, False, False, True, False]
Scene graph at timestep 989 is [True, False, False, False, True, False]
State prediction error at timestep 989 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 989 of 1
Current timestep = 990. State = [[-0.06995843 -0.09399914]]. Action = [[-0.1125866   0.05474535  0.12172368 -0.4958005 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 990 is [True, False, False, False, True, False]
Current timestep = 991. State = [[-0.07772332 -0.09734815]]. Action = [[-0.14318109 -0.05535416 -0.07374202  0.64990485]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 991 is [True, False, False, False, True, False]
Scene graph at timestep 991 is [True, False, False, False, True, False]
State prediction error at timestep 991 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 991 of 0
Current timestep = 992. State = [[-0.09617909 -0.09101568]]. Action = [[-0.22414209  0.21262011  0.03957725  0.6100662 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 992 is [True, False, False, False, True, False]
Current timestep = 993. State = [[-0.10958957 -0.07742997]]. Action = [[ 0.2081421  -0.07024491  0.12474215  0.7670462 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 993 is [True, False, False, False, True, False]
Current timestep = 994. State = [[-0.10112782 -0.0694039 ]]. Action = [[0.1014086  0.1594122  0.11440507 0.5009551 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 994 is [True, False, False, False, True, False]
Current timestep = 995. State = [[-0.09346085 -0.04838857]]. Action = [[-0.02810434  0.24455553  0.23035485 -0.8308918 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 995 is [True, False, False, False, True, False]
Scene graph at timestep 995 is [True, False, False, False, True, False]
State prediction error at timestep 995 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 995 of -1
Current timestep = 996. State = [[-0.10067867 -0.013039  ]]. Action = [[-0.18074144  0.17259371  0.03034163 -0.17956781]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 996 is [True, False, False, False, True, False]
Current timestep = 997. State = [[-0.1057128   0.00644851]]. Action = [[ 0.10855597  0.09888041  0.06457692 -0.6530549 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 997 is [True, False, False, False, True, False]
Current timestep = 998. State = [[-0.09780698  0.00326745]]. Action = [[ 0.24003565 -0.23107278  0.01368922  0.52236533]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 998 is [True, False, False, False, True, False]
Current timestep = 999. State = [[-0.07354954 -0.01374915]]. Action = [[ 0.16561246 -0.08058359 -0.14128913 -0.43895137]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 999 is [True, False, False, False, True, False]
Current timestep = 1000. State = [[-0.04820051 -0.01312553]]. Action = [[ 0.17479819  0.12792784 -0.02908362 -0.5041774 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1000 is [True, False, False, False, True, False]
Scene graph at timestep 1000 is [False, True, False, False, True, False]
State prediction error at timestep 1000 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1000 of 1
Current timestep = 1001. State = [[-0.20464505  0.19321291]]. Action = [[ 0.22229767 -0.18634792  0.02911159  0.24778879]]. Reward = [100.]
Curr episode timestep = 110
Scene graph at timestep 1001 is [False, True, False, False, True, False]
Current timestep = 1002. State = [[-0.19086078  0.2265373 ]]. Action = [[0.05337691 0.17143703 0.19219041 0.27982366]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1002 is [True, False, False, False, False, True]
Current timestep = 1003. State = [[-0.18186522  0.24650723]]. Action = [[ 0.09029916  0.12531185 -0.12604816  0.09057987]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1003 is [True, False, False, False, False, True]
Current timestep = 1004. State = [[-0.16364682  0.26085454]]. Action = [[ 0.18725926  0.06793943 -0.02911761  0.7710366 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1004 is [True, False, False, False, False, True]
Current timestep = 1005. State = [[-0.1375331   0.25508922]]. Action = [[ 0.14905596 -0.22601558  0.14955914 -0.4567982 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1005 is [True, False, False, False, False, True]
Current timestep = 1006. State = [[-0.11203261  0.2287441 ]]. Action = [[ 0.2419734  -0.21241364  0.02827081  0.6310849 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1006 is [True, False, False, False, False, True]
Scene graph at timestep 1006 is [True, False, False, False, False, True]
State prediction error at timestep 1006 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1006 of -1
Current timestep = 1007. State = [[-0.07497038  0.19944467]]. Action = [[ 0.21512386 -0.20608012 -0.15687793 -0.12538457]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1007 is [True, False, False, False, False, True]
Scene graph at timestep 1007 is [True, False, False, False, False, True]
State prediction error at timestep 1007 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1007 of 0
Current timestep = 1008. State = [[-0.05101576  0.1860766 ]]. Action = [[0.09650123 0.04292649 0.19235986 0.51842165]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1008 is [True, False, False, False, False, True]
Current timestep = 1009. State = [[-0.03714231  0.1742143 ]]. Action = [[ 0.08342025 -0.23608953 -0.08940612  0.46378076]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1009 is [True, False, False, False, False, True]
Current timestep = 1010. State = [[-0.01952392  0.15627123]]. Action = [[ 0.16382992 -0.02653041  0.19837415 -0.14324743]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1010 is [False, True, False, False, False, True]
Current timestep = 1011. State = [[-0.0080547   0.15318504]]. Action = [[-0.05762547  0.00496191 -0.2349039   0.37850857]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1011 is [False, True, False, False, False, True]
Current timestep = 1012. State = [[-0.00174065  0.15248139]]. Action = [[ 0.18611175  0.01501966  0.08519927 -0.48777115]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1012 is [False, True, False, False, False, True]
Current timestep = 1013. State = [[0.02204968 0.15193789]]. Action = [[ 0.21677652 -0.01843786  0.12752932  0.26084208]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1013 is [False, True, False, False, False, True]
Scene graph at timestep 1013 is [False, True, False, False, False, True]
State prediction error at timestep 1013 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1013 of -1
Current timestep = 1014. State = [[0.05228014 0.15113762]]. Action = [[-0.00141519 -0.03561229  0.03516555  0.7676643 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1014 is [False, True, False, False, False, True]
Current timestep = 1015. State = [[0.05443561 0.14062396]]. Action = [[-0.11413363 -0.15896961  0.18685818 -0.71915895]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1015 is [False, False, True, False, False, True]
Scene graph at timestep 1015 is [False, False, True, False, False, True]
State prediction error at timestep 1015 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1015 of -1
Current timestep = 1016. State = [[0.0576171  0.12472278]]. Action = [[ 0.0652203  -0.08679074  0.04573214 -0.2851405 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1016 is [False, False, True, False, False, True]
Scene graph at timestep 1016 is [False, False, True, False, True, False]
State prediction error at timestep 1016 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1016 of 0
Current timestep = 1017. State = [[0.06028009 0.1171973 ]]. Action = [[ 0.13761407  0.23849335 -0.04967284 -0.6257235 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1017 is [False, False, True, False, True, False]
Current timestep = 1018. State = [[0.06028009 0.1171973 ]]. Action = [[0.10824245 0.17692298 0.21036083 0.9385139 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1018 is [False, False, True, False, True, False]
Scene graph at timestep 1018 is [False, False, True, False, True, False]
State prediction error at timestep 1018 is tensor(6.2959e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1018 of -1
Current timestep = 1019. State = [[0.06028009 0.1171973 ]]. Action = [[ 0.08298367 -0.05463693 -0.23241471 -0.38055474]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1019 is [False, False, True, False, True, False]
Current timestep = 1020. State = [[0.05665875 0.12774046]]. Action = [[-0.06970727  0.21402943 -0.21065511 -0.8302325 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1020 is [False, False, True, False, True, False]
Scene graph at timestep 1020 is [False, False, True, False, False, True]
State prediction error at timestep 1020 is tensor(7.6303e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1020 of -1
Current timestep = 1021. State = [[0.05131035 0.14202508]]. Action = [[-0.0132111  -0.02316111 -0.03029351  0.10637522]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1021 is [False, False, True, False, False, True]
Scene graph at timestep 1021 is [False, False, True, False, False, True]
State prediction error at timestep 1021 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1021 of -1
Current timestep = 1022. State = [[0.04900986 0.14813027]]. Action = [[-0.00422639  0.11192203 -0.1757434  -0.30570912]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1022 is [False, False, True, False, False, True]
Current timestep = 1023. State = [[0.04206492 0.16440181]]. Action = [[-0.1905661   0.13033926  0.00086403  0.8080394 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1023 is [False, True, False, False, False, True]
Current timestep = 1024. State = [[0.02716933 0.1885845 ]]. Action = [[-0.21351826  0.13856298 -0.20821631  0.4100647 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1024 is [False, True, False, False, False, True]
Scene graph at timestep 1024 is [False, True, False, False, False, True]
State prediction error at timestep 1024 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1024 of -1
Current timestep = 1025. State = [[0.00901239 0.20565066]]. Action = [[ 0.11077821  0.03273764  0.10526857 -0.6970459 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1025 is [False, True, False, False, False, True]
Current timestep = 1026. State = [[0.00451385 0.21543583]]. Action = [[-0.1727474   0.11628836  0.06011945 -0.29422855]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1026 is [False, True, False, False, False, True]
Scene graph at timestep 1026 is [False, True, False, False, False, True]
State prediction error at timestep 1026 is tensor(8.7087e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1026 of -1
Current timestep = 1027. State = [[-0.00246451  0.227283  ]]. Action = [[-0.03816795 -0.00819319  0.17924714 -0.18562156]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1027 is [False, True, False, False, False, True]
Current timestep = 1028. State = [[-0.00137027  0.21914703]]. Action = [[ 0.01304516 -0.14366318 -0.1751843   0.9143293 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1028 is [False, True, False, False, False, True]
Scene graph at timestep 1028 is [False, True, False, False, False, True]
State prediction error at timestep 1028 is tensor(2.6368e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1028 of 0
Current timestep = 1029. State = [[-0.00570804  0.21062738]]. Action = [[-0.00643474  0.08307979 -0.13831037  0.7633449 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1029 is [False, True, False, False, False, True]
Scene graph at timestep 1029 is [False, True, False, False, False, True]
State prediction error at timestep 1029 is tensor(1.9685e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1029 of -1
Current timestep = 1030. State = [[-0.00522468  0.21349911]]. Action = [[ 0.17659867 -0.0250475  -0.03501445 -0.43107647]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1030 is [False, True, False, False, False, True]
Current timestep = 1031. State = [[0.00270027 0.19881806]]. Action = [[ 0.08978701 -0.19593245 -0.1447991  -0.9194884 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1031 is [False, True, False, False, False, True]
Scene graph at timestep 1031 is [False, True, False, False, False, True]
State prediction error at timestep 1031 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1031 of 1
Current timestep = 1032. State = [[0.01746339 0.16984618]]. Action = [[ 0.22142145 -0.17931768 -0.07780018 -0.7167752 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1032 is [False, True, False, False, False, True]
Current timestep = 1033. State = [[0.037874   0.15682001]]. Action = [[0.20690516 0.08117136 0.01631618 0.5013943 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1033 is [False, True, False, False, False, True]
Scene graph at timestep 1033 is [False, True, False, False, False, True]
State prediction error at timestep 1033 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1033 of -1
Current timestep = 1034. State = [[0.06142968 0.15704696]]. Action = [[-0.05047147 -0.07723382  0.06185177 -0.1341328 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1034 is [False, True, False, False, False, True]
Scene graph at timestep 1034 is [False, False, True, False, False, True]
State prediction error at timestep 1034 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1034 of -1
Current timestep = 1035. State = [[0.06112485 0.15655245]]. Action = [[-0.06894195  0.05859241  0.09084916  0.82957006]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1035 is [False, False, True, False, False, True]
Current timestep = 1036. State = [[0.0602708  0.15802029]]. Action = [[ 0.05887365 -0.18942945 -0.13837294  0.479069  ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1036 is [False, False, True, False, False, True]
Scene graph at timestep 1036 is [False, False, True, False, False, True]
State prediction error at timestep 1036 is tensor(4.1137e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1036 of -1
Current timestep = 1037. State = [[0.0602708  0.15802029]]. Action = [[ 0.22169566  0.03640804 -0.09249601  0.6998131 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1037 is [False, False, True, False, False, True]
Current timestep = 1038. State = [[0.06227526 0.14637968]]. Action = [[-0.08968493 -0.23284796 -0.19323735 -0.36769688]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1038 is [False, False, True, False, False, True]
Scene graph at timestep 1038 is [False, False, True, False, False, True]
State prediction error at timestep 1038 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1038 of 0
Current timestep = 1039. State = [[0.06243281 0.13300222]]. Action = [[ 0.16326562 -0.05204467  0.17553777 -0.7444315 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1039 is [False, False, True, False, False, True]
Scene graph at timestep 1039 is [False, False, True, False, False, True]
State prediction error at timestep 1039 is tensor(6.7881e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1039 of -1
Current timestep = 1040. State = [[0.06279591 0.12515907]]. Action = [[-0.0010234  -0.10280424  0.12431169  0.98732066]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1040 is [False, False, True, False, False, True]
Current timestep = 1041. State = [[0.06242124 0.11892778]]. Action = [[ 0.14353493 -0.21540949 -0.18327513  0.16287231]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1041 is [False, False, True, False, False, True]
Current timestep = 1042. State = [[0.05952513 0.11597616]]. Action = [[-0.07827216 -0.0010782  -0.14021009 -0.56829774]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1042 is [False, False, True, False, True, False]
Current timestep = 1043. State = [[0.0485568  0.11822043]]. Action = [[-0.24457501  0.02973056  0.00951308  0.8151933 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1043 is [False, False, True, False, True, False]
Current timestep = 1044. State = [[-0.264004   -0.15263413]]. Action = [[-0.10737509 -0.17777024 -0.19785857 -0.08828807]]. Reward = [100.]
Curr episode timestep = 42
Scene graph at timestep 1044 is [False, True, False, False, True, False]
Current timestep = 1045. State = [[-0.26709336 -0.18266262]]. Action = [[-0.07815099 -0.20948835  0.16324162  0.63263416]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1045 is [True, False, False, True, False, False]
Current timestep = 1046. State = [[-0.26164678 -0.18848425]]. Action = [[ 0.21764565  0.16521859 -0.11864531  0.6306615 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1046 is [True, False, False, True, False, False]
Scene graph at timestep 1046 is [True, False, False, True, False, False]
State prediction error at timestep 1046 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1046 of 0
Current timestep = 1047. State = [[-0.24683605 -0.17038122]]. Action = [[-0.03046897  0.17145646  0.10622689 -0.17849058]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1047 is [True, False, False, True, False, False]
Current timestep = 1048. State = [[-0.24558541 -0.1612936 ]]. Action = [[-0.19595884 -0.16758586 -0.1671253  -0.9122409 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1048 is [True, False, False, True, False, False]
Current timestep = 1049. State = [[-0.24841765 -0.17085257]]. Action = [[-0.09324422 -0.20272484  0.10388583 -0.00592285]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1049 is [True, False, False, True, False, False]
Scene graph at timestep 1049 is [True, False, False, True, False, False]
State prediction error at timestep 1049 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1049 of -1
Current timestep = 1050. State = [[-0.2545696  -0.18399389]]. Action = [[-0.14831543 -0.0148229  -0.16085084  0.08734453]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1050 is [True, False, False, True, False, False]
Current timestep = 1051. State = [[-0.25254673 -0.17345724]]. Action = [[0.05197626 0.21471497 0.06879282 0.7597687 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1051 is [True, False, False, True, False, False]
Current timestep = 1052. State = [[-0.24187602 -0.14756607]]. Action = [[0.22608638 0.18994412 0.07522959 0.82013226]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1052 is [True, False, False, True, False, False]
Current timestep = 1053. State = [[-0.22105905 -0.14156061]]. Action = [[ 0.1812959  -0.17290713  0.1961421   0.7786225 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1053 is [True, False, False, True, False, False]
Current timestep = 1054. State = [[-0.19707966 -0.14541174]]. Action = [[0.12435445 0.05165502 0.11380488 0.65616155]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1054 is [True, False, False, True, False, False]
Current timestep = 1055. State = [[-0.17650796 -0.1546693 ]]. Action = [[ 0.18184698 -0.185487    0.22011626  0.69319177]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1055 is [True, False, False, True, False, False]
Scene graph at timestep 1055 is [True, False, False, True, False, False]
State prediction error at timestep 1055 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1055 of 1
Current timestep = 1056. State = [[-0.14914428 -0.16592154]]. Action = [[ 0.19229281 -0.00360949 -0.13898431  0.40913296]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1056 is [True, False, False, True, False, False]
Current timestep = 1057. State = [[-0.12605278 -0.174929  ]]. Action = [[ 0.11089152 -0.11819093  0.03827411 -0.3568344 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1057 is [True, False, False, True, False, False]
Current timestep = 1058. State = [[-0.11791205 -0.19022942]]. Action = [[-0.09948087 -0.13055018 -0.0109093   0.13094819]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1058 is [True, False, False, True, False, False]
Current timestep = 1059. State = [[-0.11771793 -0.20531294]]. Action = [[ 0.10399157 -0.09333961 -0.02540193 -0.6840319 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1059 is [True, False, False, True, False, False]
Current timestep = 1060. State = [[-0.10838831 -0.21221194]]. Action = [[ 0.07008415  0.04112563 -0.2265766   0.35454965]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1060 is [True, False, False, True, False, False]
Current timestep = 1061. State = [[-0.10082279 -0.20778902]]. Action = [[-0.01933654  0.10298765 -0.24691847  0.85307026]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1061 is [True, False, False, True, False, False]
Scene graph at timestep 1061 is [True, False, False, True, False, False]
State prediction error at timestep 1061 is tensor(2.5330e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1061 of -1
Current timestep = 1062. State = [[-0.10481314 -0.20046324]]. Action = [[-0.24230215  0.04791552  0.02003255  0.14030647]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1062 is [True, False, False, True, False, False]
Scene graph at timestep 1062 is [True, False, False, True, False, False]
State prediction error at timestep 1062 is tensor(3.6731e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1062 of -1
Current timestep = 1063. State = [[-0.11363456 -0.18472685]]. Action = [[ 0.02130815  0.23584509 -0.23719594  0.05077434]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1063 is [True, False, False, True, False, False]
Current timestep = 1064. State = [[-0.11339086 -0.16953218]]. Action = [[ 0.09174657 -0.06214383  0.11442006  0.0767647 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1064 is [True, False, False, True, False, False]
Scene graph at timestep 1064 is [True, False, False, True, False, False]
State prediction error at timestep 1064 is tensor(2.4355e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1064 of 1
Current timestep = 1065. State = [[-0.11208057 -0.17184179]]. Action = [[ 0.02806494 -0.06540515  0.10690767 -0.01599431]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1065 is [True, False, False, True, False, False]
Current timestep = 1066. State = [[-0.11361731 -0.16413741]]. Action = [[-0.16332695  0.19851601  0.06661439  0.48487902]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1066 is [True, False, False, True, False, False]
Current timestep = 1067. State = [[-0.11648985 -0.1513662 ]]. Action = [[ 0.05616623  0.03692076 -0.17951393  0.26175618]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1067 is [True, False, False, True, False, False]
Current timestep = 1068. State = [[-0.1232768  -0.14445715]]. Action = [[-0.20357138  0.04938871 -0.12918594 -0.5249106 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1068 is [True, False, False, True, False, False]
Current timestep = 1069. State = [[-0.12974599 -0.15271306]]. Action = [[ 0.2205056  -0.2394333   0.17739874 -0.5927483 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1069 is [True, False, False, True, False, False]
Current timestep = 1070. State = [[-0.12268222 -0.16118494]]. Action = [[ 0.11004794  0.05682099 -0.14582522 -0.41670263]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1070 is [True, False, False, True, False, False]
Current timestep = 1071. State = [[-0.11799323 -0.15046136]]. Action = [[-0.1663461   0.20866275  0.00599861 -0.5886015 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1071 is [True, False, False, True, False, False]
Current timestep = 1072. State = [[-0.11926758 -0.13884808]]. Action = [[ 0.06966588 -0.02113672 -0.21131024  0.78913283]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1072 is [True, False, False, True, False, False]
Current timestep = 1073. State = [[-0.12402688 -0.14681074]]. Action = [[-0.17054346 -0.14174697 -0.09673494  0.30614948]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1073 is [True, False, False, True, False, False]
Scene graph at timestep 1073 is [True, False, False, True, False, False]
State prediction error at timestep 1073 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1073 of 0
Current timestep = 1074. State = [[-0.13874769 -0.15925954]]. Action = [[-0.10695256 -0.04729545  0.1577869   0.58034015]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1074 is [True, False, False, True, False, False]
Current timestep = 1075. State = [[-0.14101441 -0.15449686]]. Action = [[ 0.12626868  0.15335119 -0.10020959 -0.7579041 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1075 is [True, False, False, True, False, False]
Current timestep = 1076. State = [[-0.13106596 -0.14490633]]. Action = [[ 0.22250408  0.00690535 -0.12938513 -0.12464052]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1076 is [True, False, False, True, False, False]
Current timestep = 1077. State = [[-0.12453683 -0.14662829]]. Action = [[-0.21512479 -0.07195474  0.2256417  -0.514337  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1077 is [True, False, False, True, False, False]
Current timestep = 1078. State = [[-0.12458239 -0.16219556]]. Action = [[ 0.16356835 -0.23429252 -0.13845739 -0.18891919]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1078 is [True, False, False, True, False, False]
Current timestep = 1079. State = [[-0.11340962 -0.18698741]]. Action = [[ 0.21227497 -0.16836634 -0.13700855  0.90058255]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1079 is [True, False, False, True, False, False]
Scene graph at timestep 1079 is [True, False, False, True, False, False]
State prediction error at timestep 1079 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1079 of -1
Current timestep = 1080. State = [[-0.08624534 -0.19503184]]. Action = [[ 0.1574046   0.14769176 -0.04418193  0.3744346 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1080 is [True, False, False, True, False, False]
Current timestep = 1081. State = [[-0.06296479 -0.19026671]]. Action = [[ 0.2258684  -0.03277446  0.22673303 -0.5632019 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1081 is [True, False, False, True, False, False]
Scene graph at timestep 1081 is [True, False, False, True, False, False]
State prediction error at timestep 1081 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1081 of 1
Current timestep = 1082. State = [[-0.04035568 -0.1798227 ]]. Action = [[-0.1409966   0.199579    0.08050078  0.33954298]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1082 is [True, False, False, True, False, False]
Current timestep = 1083. State = [[-0.03738464 -0.17853333]]. Action = [[ 0.21381772 -0.22148901 -0.16781007  0.9041269 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1083 is [False, True, False, True, False, False]
Current timestep = 1084. State = [[-0.02762648 -0.186644  ]]. Action = [[ 0.0518899   0.02530769 -0.13049012  0.61454034]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1084 is [False, True, False, True, False, False]
Current timestep = 1085. State = [[-0.01844793 -0.18001589]]. Action = [[ 0.0782364   0.132402   -0.23805065 -0.3626907 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1085 is [False, True, False, True, False, False]
Scene graph at timestep 1085 is [False, True, False, True, False, False]
State prediction error at timestep 1085 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1085 of 1
Current timestep = 1086. State = [[-0.00518944 -0.16612689]]. Action = [[0.10996842 0.10598692 0.11766869 0.78342676]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1086 is [False, True, False, True, False, False]
Current timestep = 1087. State = [[ 0.01326787 -0.17062978]]. Action = [[ 0.15889889 -0.21767944  0.1301834   0.91796625]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1087 is [False, True, False, True, False, False]
Current timestep = 1088. State = [[ 0.03590147 -0.18598226]]. Action = [[ 0.15896887 -0.07850905 -0.15957686  0.86034954]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1088 is [False, True, False, True, False, False]
Current timestep = 1089. State = [[ 0.04790135 -0.18625368]]. Action = [[-0.20141616  0.15450126 -0.2116612  -0.8755242 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1089 is [False, True, False, True, False, False]
Scene graph at timestep 1089 is [False, True, False, True, False, False]
State prediction error at timestep 1089 is tensor(8.2662e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1089 of 0
Current timestep = 1090. State = [[ 0.04741983 -0.18072142]]. Action = [[0.19164836 0.00078121 0.18755922 0.14685595]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1090 is [False, True, False, True, False, False]
Current timestep = 1091. State = [[ 0.04505726 -0.19126196]]. Action = [[-0.04498601 -0.17333275  0.22259918  0.09973466]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1091 is [False, True, False, True, False, False]
Scene graph at timestep 1091 is [False, True, False, True, False, False]
State prediction error at timestep 1091 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1091 of -1
Current timestep = 1092. State = [[ 0.04150611 -0.20195282]]. Action = [[ 0.2182681  -0.07384475  0.01238793  0.57415664]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1092 is [False, True, False, True, False, False]
Current timestep = 1093. State = [[ 0.0402289  -0.20988503]]. Action = [[-0.01982039 -0.11487857  0.06364042 -0.80218035]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1093 is [False, True, False, True, False, False]
Scene graph at timestep 1093 is [False, True, False, True, False, False]
State prediction error at timestep 1093 is tensor(4.6126e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1093 of -1
Current timestep = 1094. State = [[ 0.03861462 -0.21139579]]. Action = [[ 0.0384692   0.11940092  0.14245433 -0.9691992 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1094 is [False, True, False, True, False, False]
Scene graph at timestep 1094 is [False, True, False, True, False, False]
State prediction error at timestep 1094 is tensor(8.6372e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1094 of 0
Current timestep = 1095. State = [[ 0.03454094 -0.20764953]]. Action = [[-0.22978927  0.00286859 -0.05280268 -0.3444047 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1095 is [False, True, False, True, False, False]
Current timestep = 1096. State = [[ 0.02798785 -0.20300013]]. Action = [[0.13619435 0.06044036 0.05719703 0.28785872]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1096 is [False, True, False, True, False, False]
Current timestep = 1097. State = [[ 0.02635855 -0.18590091]]. Action = [[-0.16221255  0.21472141  0.24169677 -0.67255235]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1097 is [False, True, False, True, False, False]
Current timestep = 1098. State = [[ 0.0232426  -0.16090044]]. Action = [[0.18493134 0.11084878 0.21755755 0.7101493 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1098 is [False, True, False, True, False, False]
Current timestep = 1099. State = [[ 0.0252983  -0.13880746]]. Action = [[-0.03663263  0.1676194  -0.17796999  0.59274745]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1099 is [False, True, False, True, False, False]
Current timestep = 1100. State = [[ 0.02571148 -0.13539281]]. Action = [[ 0.04307094 -0.17703938  0.09850866  0.25712192]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1100 is [False, True, False, True, False, False]
Current timestep = 1101. State = [[ 0.02064916 -0.1445526 ]]. Action = [[-0.23346156 -0.0019092   0.07074845  0.53701437]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1101 is [False, True, False, True, False, False]
Scene graph at timestep 1101 is [False, True, False, True, False, False]
State prediction error at timestep 1101 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1101 of 1
Current timestep = 1102. State = [[ 0.0115041  -0.15019754]]. Action = [[ 0.03646442 -0.00141935 -0.12018134 -0.5391607 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1102 is [False, True, False, True, False, False]
Current timestep = 1103. State = [[ 0.01137924 -0.14962168]]. Action = [[-0.03837556  0.01569217 -0.18587482 -0.7635825 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1103 is [False, True, False, True, False, False]
Current timestep = 1104. State = [[ 0.01528993 -0.15324844]]. Action = [[ 0.24309987 -0.11786011 -0.24236538  0.73556757]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1104 is [False, True, False, True, False, False]
Current timestep = 1105. State = [[ 0.01407274 -0.17010187]]. Action = [[-0.15988939 -0.20522106  0.23598075 -0.9909654 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1105 is [False, True, False, True, False, False]
Current timestep = 1106. State = [[ 0.01394257 -0.18124221]]. Action = [[ 0.13406384  0.07263559 -0.14810555 -0.85459065]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1106 is [False, True, False, True, False, False]
Current timestep = 1107. State = [[ 0.01509793 -0.1791669 ]]. Action = [[-0.12988539  0.03998923  0.11748803  0.8997102 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1107 is [False, True, False, True, False, False]
Current timestep = 1108. State = [[ 0.01003191 -0.16753596]]. Action = [[-0.18838927  0.20659739  0.00220054 -0.7944578 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1108 is [False, True, False, True, False, False]
Current timestep = 1109. State = [[-0.00455807 -0.15579137]]. Action = [[-0.07252344 -0.05706286 -0.03877318  0.7049279 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1109 is [False, True, False, True, False, False]
Current timestep = 1110. State = [[-0.01348948 -0.14513293]]. Action = [[-0.09301028  0.20193657 -0.20003638  0.75302005]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1110 is [False, True, False, True, False, False]
Current timestep = 1111. State = [[-0.02066783 -0.12518588]]. Action = [[ 0.14540094  0.06158996 -0.14769804 -0.33035684]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1111 is [False, True, False, True, False, False]
Scene graph at timestep 1111 is [False, True, False, True, False, False]
State prediction error at timestep 1111 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1111 of 1
Current timestep = 1112. State = [[-0.01104307 -0.12570189]]. Action = [[ 0.23268205 -0.17127839  0.07088929 -0.2997148 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1112 is [False, True, False, True, False, False]
Current timestep = 1113. State = [[-0.00127955 -0.12130477]]. Action = [[-0.01573783  0.23917449 -0.00877285 -0.36644816]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1113 is [False, True, False, True, False, False]
Scene graph at timestep 1113 is [False, True, False, False, True, False]
State prediction error at timestep 1113 is tensor(6.9944e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1113 of 1
Current timestep = 1114. State = [[ 0.00242    -0.10986432]]. Action = [[ 0.11894089 -0.08256277  0.03051528 -0.18733704]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1114 is [False, True, False, False, True, False]
Scene graph at timestep 1114 is [False, True, False, False, True, False]
State prediction error at timestep 1114 is tensor(5.2990e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1114 of 1
Current timestep = 1115. State = [[-0.19372983 -0.0478639 ]]. Action = [[-0.04057825  0.23313498  0.10767531  0.04673707]]. Reward = [100.]
Curr episode timestep = 70
Scene graph at timestep 1115 is [False, True, False, False, True, False]
Current timestep = 1116. State = [[-0.18304864 -0.04916402]]. Action = [[-0.08415198  0.11032262 -0.10635293 -0.5760045 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1116 is [True, False, False, False, True, False]
Current timestep = 1117. State = [[-0.18358275 -0.04605151]]. Action = [[ 0.07272094 -0.03820463  0.22859374  0.7582264 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1117 is [True, False, False, False, True, False]
Current timestep = 1118. State = [[-0.1855946  -0.05180183]]. Action = [[-0.13513184 -0.08928494  0.16068134  0.08055019]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1118 is [True, False, False, False, True, False]
Current timestep = 1119. State = [[-0.18638898 -0.0702432 ]]. Action = [[ 0.11228684 -0.20531389  0.22684658 -0.10487747]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1119 is [True, False, False, False, True, False]
Current timestep = 1120. State = [[-0.19064033 -0.07670864]]. Action = [[-0.2049096   0.14835972  0.10450572 -0.565641  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1120 is [True, False, False, False, True, False]
Current timestep = 1121. State = [[-0.195829   -0.06818473]]. Action = [[-0.03972583  0.06856042 -0.16021162 -0.61120087]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1121 is [True, False, False, False, True, False]
Current timestep = 1122. State = [[-0.1999422  -0.06006188]]. Action = [[ 0.00458702  0.0308806   0.22066095 -0.7942973 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1122 is [True, False, False, False, True, False]
Current timestep = 1123. State = [[-0.20836027 -0.06887804]]. Action = [[-0.13272595 -0.18187208 -0.02736078  0.18257761]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1123 is [True, False, False, False, True, False]
Current timestep = 1124. State = [[-0.22781044 -0.08415616]]. Action = [[-0.17114656 -0.06261611 -0.10745436 -0.44622338]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1124 is [True, False, False, False, True, False]
Current timestep = 1125. State = [[-0.238405   -0.09540368]]. Action = [[ 0.12629053 -0.08845443 -0.0881314  -0.22334868]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1125 is [True, False, False, False, True, False]
Scene graph at timestep 1125 is [True, False, False, False, True, False]
State prediction error at timestep 1125 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1125 of -1
Current timestep = 1126. State = [[-0.23297736 -0.10781382]]. Action = [[ 0.09762025 -0.12259233 -0.1344762   0.06506038]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1126 is [True, False, False, False, True, False]
Current timestep = 1127. State = [[-0.2167092 -0.119775 ]]. Action = [[ 0.24846548 -0.03047228 -0.00435516 -0.93438095]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1127 is [True, False, False, False, True, False]
Current timestep = 1128. State = [[-0.18956451 -0.1340318 ]]. Action = [[ 0.17972475 -0.18251337  0.14457747  0.16901505]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1128 is [True, False, False, False, True, False]
Current timestep = 1129. State = [[-0.16995661 -0.14595088]]. Action = [[ 0.10984531  0.04307795 -0.1534445   0.3825016 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1129 is [True, False, False, True, False, False]
Scene graph at timestep 1129 is [True, False, False, True, False, False]
State prediction error at timestep 1129 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1129 of 1
Current timestep = 1130. State = [[-0.15578422 -0.15666586]]. Action = [[ 0.00443459 -0.18122466  0.14100754 -0.32658225]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1130 is [True, False, False, True, False, False]
Current timestep = 1131. State = [[-0.15392962 -0.16107413]]. Action = [[ 0.01288235  0.14629233 -0.21481255 -0.470532  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1131 is [True, False, False, True, False, False]
Current timestep = 1132. State = [[-0.15195014 -0.1514278 ]]. Action = [[0.02833235 0.07509765 0.1313734  0.44686222]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1132 is [True, False, False, True, False, False]
Current timestep = 1133. State = [[-0.1498072  -0.14642332]]. Action = [[-0.00264269 -0.00990322  0.09760502  0.6797662 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1133 is [True, False, False, True, False, False]
Scene graph at timestep 1133 is [True, False, False, True, False, False]
State prediction error at timestep 1133 is tensor(1.5474e-08, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1133 of 0
Current timestep = 1134. State = [[-0.13867146 -0.14597376]]. Action = [[ 0.22319686 -0.01914263  0.17821878 -0.908204  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1134 is [True, False, False, True, False, False]
Current timestep = 1135. State = [[-0.11443064 -0.15724076]]. Action = [[ 0.12882441 -0.17713173  0.1176548   0.06513321]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1135 is [True, False, False, True, False, False]
Current timestep = 1136. State = [[-0.08948097 -0.17039105]]. Action = [[ 0.22547862 -0.06975904  0.07493889  0.3998289 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1136 is [True, False, False, True, False, False]
Current timestep = 1137. State = [[-0.06763863 -0.16830882]]. Action = [[0.01128542 0.17198172 0.09842041 0.14563644]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1137 is [True, False, False, True, False, False]
Scene graph at timestep 1137 is [True, False, False, True, False, False]
State prediction error at timestep 1137 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1137 of 1
Current timestep = 1138. State = [[-0.0621257  -0.16316508]]. Action = [[-0.05211569 -0.06532434  0.05920872 -0.61757815]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1138 is [True, False, False, True, False, False]
Current timestep = 1139. State = [[-0.0639355  -0.17240632]]. Action = [[-0.04769124 -0.10433662 -0.09765792  0.5804446 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1139 is [True, False, False, True, False, False]
Scene graph at timestep 1139 is [True, False, False, True, False, False]
State prediction error at timestep 1139 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1139 of -1
Current timestep = 1140. State = [[-0.07330155 -0.19183853]]. Action = [[-0.19316244 -0.15284738 -0.08623666 -0.06215882]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1140 is [True, False, False, True, False, False]
Current timestep = 1141. State = [[-0.08026043 -0.20480147]]. Action = [[ 0.05061895 -0.02914956  0.18279618 -0.2522248 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1141 is [True, False, False, True, False, False]
Scene graph at timestep 1141 is [True, False, False, True, False, False]
State prediction error at timestep 1141 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1141 of -1
Current timestep = 1142. State = [[-0.07909212 -0.21249163]]. Action = [[ 0.11355397 -0.09215134  0.12219995  0.4360994 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1142 is [True, False, False, True, False, False]
Current timestep = 1143. State = [[-0.07048846 -0.23029137]]. Action = [[ 0.18817198 -0.20789152  0.09549618  0.15656865]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1143 is [True, False, False, True, False, False]
Current timestep = 1144. State = [[-0.05130108 -0.235219  ]]. Action = [[ 0.08541712  0.2050519  -0.085179    0.97606385]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1144 is [True, False, False, True, False, False]
Current timestep = 1145. State = [[-0.04520255 -0.2293138 ]]. Action = [[-0.05104639 -0.02038987 -0.13349795  0.8108964 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1145 is [True, False, False, True, False, False]
Current timestep = 1146. State = [[-0.04561995 -0.22151208]]. Action = [[-0.13064998  0.13745779 -0.01321429 -0.22409463]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1146 is [False, True, False, True, False, False]
Current timestep = 1147. State = [[-0.04323471 -0.2236102 ]]. Action = [[ 0.18563044 -0.20903057 -0.2128478  -0.2533645 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1147 is [False, True, False, True, False, False]
Current timestep = 1148. State = [[-0.03138471 -0.24281226]]. Action = [[ 0.21580619 -0.20567153 -0.2109531   0.3717053 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1148 is [False, True, False, True, False, False]
Scene graph at timestep 1148 is [False, True, False, True, False, False]
State prediction error at timestep 1148 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1148 of -1
Current timestep = 1149. State = [[-0.00259504 -0.27630323]]. Action = [[ 0.12119001 -0.24826713  0.07036549  0.71804523]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1149 is [False, True, False, True, False, False]
Scene graph at timestep 1149 is [False, True, False, True, False, False]
State prediction error at timestep 1149 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1149 of -1
Current timestep = 1150. State = [[ 0.01671238 -0.29066417]]. Action = [[0.07742611 0.17422518 0.13207048 0.89129686]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1150 is [False, True, False, True, False, False]
Scene graph at timestep 1150 is [False, True, False, True, False, False]
State prediction error at timestep 1150 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1150 of 0
Current timestep = 1151. State = [[ 0.02536217 -0.2821007 ]]. Action = [[ 0.20683408 -0.22094978 -0.07862177 -0.42025423]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1151 is [False, True, False, True, False, False]
Current timestep = 1152. State = [[ 0.03550643 -0.26807255]]. Action = [[0.20267719 0.21987033 0.24126333 0.82797647]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1152 is [False, True, False, True, False, False]
Current timestep = 1153. State = [[ 0.04813566 -0.245916  ]]. Action = [[-0.08952284  0.10883421  0.1122475   0.8973205 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1153 is [False, True, False, True, False, False]
Current timestep = 1154. State = [[ 0.04912125 -0.23677807]]. Action = [[ 0.24723929  0.14261734 -0.07172933 -0.69687635]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1154 is [False, True, False, True, False, False]
Current timestep = 1155. State = [[ 0.04920833 -0.23568356]]. Action = [[ 0.20172077  0.05106527 -0.19873296 -0.38038945]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1155 is [False, True, False, True, False, False]
Current timestep = 1156. State = [[ 0.04918386 -0.23534952]]. Action = [[ 0.23395127 -0.18676865  0.02585635 -0.42008787]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1156 is [False, True, False, True, False, False]
Current timestep = 1157. State = [[ 0.04918386 -0.23534952]]. Action = [[ 0.20823026  0.12780589  0.0552392  -0.25088847]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1157 is [False, True, False, True, False, False]
Current timestep = 1158. State = [[ 0.05005837 -0.24606511]]. Action = [[ 0.10135642 -0.22367942  0.18000245  0.32575953]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1158 is [False, True, False, True, False, False]
Current timestep = 1159. State = [[ 0.06011786 -0.24636924]]. Action = [[ 0.04155988  0.22091132 -0.01747155 -0.72736156]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1159 is [False, False, True, True, False, False]
Scene graph at timestep 1159 is [False, False, True, True, False, False]
State prediction error at timestep 1159 is tensor(2.8672e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1159 of -1
Current timestep = 1160. State = [[ 0.06196268 -0.23476006]]. Action = [[-0.12736002  0.02277067  0.18772626  0.73242176]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1160 is [False, False, True, True, False, False]
Current timestep = 1161. State = [[ 0.06160203 -0.2345278 ]]. Action = [[0.14916411 0.03197911 0.06323567 0.10261154]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1161 is [False, False, True, True, False, False]
Current timestep = 1162. State = [[ 0.0612567 -0.2345529]]. Action = [[ 0.18702894  0.10854566 -0.22422631  0.12107635]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1162 is [False, False, True, True, False, False]
Current timestep = 1163. State = [[ 0.06107109 -0.23426214]]. Action = [[ 0.17714345  0.03380424 -0.18355279 -0.45318103]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1163 is [False, False, True, True, False, False]
Current timestep = 1164. State = [[ 0.05806641 -0.24643008]]. Action = [[-0.01529755 -0.22267047  0.02610412 -0.17625427]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1164 is [False, False, True, True, False, False]
Current timestep = 1165. State = [[ 0.05464481 -0.2574649 ]]. Action = [[-0.06427595  0.03516173 -0.1910895  -0.3236413 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1165 is [False, False, True, True, False, False]
Scene graph at timestep 1165 is [False, False, True, True, False, False]
State prediction error at timestep 1165 is tensor(4.1694e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1165 of -1
Current timestep = 1166. State = [[ 0.05314051 -0.25891274]]. Action = [[ 0.17445576 -0.17954066  0.04018837 -0.5727471 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1166 is [False, False, True, True, False, False]
Current timestep = 1167. State = [[ 0.05290367 -0.25895536]]. Action = [[ 0.22238734  0.18098295 -0.01900376 -0.35156167]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1167 is [False, False, True, True, False, False]
Current timestep = 1168. State = [[ 0.05257325 -0.25895092]]. Action = [[ 0.10747892  0.23438478  0.18894857 -0.00860733]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1168 is [False, False, True, True, False, False]
Scene graph at timestep 1168 is [False, False, True, True, False, False]
State prediction error at timestep 1168 is tensor(2.9201e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1168 of -1
Current timestep = 1169. State = [[ 0.05247901 -0.25894964]]. Action = [[ 0.24339896 -0.0581836   0.24705023  0.7289169 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1169 is [False, False, True, True, False, False]
Current timestep = 1170. State = [[ 0.04832276 -0.26566276]]. Action = [[-0.11393498 -0.09110609 -0.17612706  0.32607186]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1170 is [False, False, True, True, False, False]
Scene graph at timestep 1170 is [False, True, False, True, False, False]
State prediction error at timestep 1170 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1170 of -1
Current timestep = 1171. State = [[ 0.04261094 -0.28247476]]. Action = [[ 0.09784421 -0.16519625  0.20007354 -0.96027017]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1171 is [False, True, False, True, False, False]
Scene graph at timestep 1171 is [False, True, False, True, False, False]
State prediction error at timestep 1171 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1171 of -1
Current timestep = 1172. State = [[ 0.04024403 -0.2840433 ]]. Action = [[-0.17991178  0.19554764  0.19127637 -0.60548943]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1172 is [False, True, False, True, False, False]
Current timestep = 1173. State = [[ 0.02566163 -0.27474698]]. Action = [[-0.23906331  0.05181587  0.13073763 -0.5851113 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1173 is [False, True, False, True, False, False]
Current timestep = 1174. State = [[ 0.01219588 -0.26164678]]. Action = [[ 0.22165409  0.05314332 -0.05202648  0.24048865]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1174 is [False, True, False, True, False, False]
Scene graph at timestep 1174 is [False, True, False, True, False, False]
State prediction error at timestep 1174 is tensor(8.4722e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1174 of 0
Current timestep = 1175. State = [[ 0.01450382 -0.26517367]]. Action = [[ 0.06922424 -0.21270697 -0.1464781   0.14227653]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1175 is [False, True, False, True, False, False]
Current timestep = 1176. State = [[ 0.01898263 -0.28062022]]. Action = [[ 0.11495602 -0.09173906  0.12579292 -0.8561648 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1176 is [False, True, False, True, False, False]
Scene graph at timestep 1176 is [False, True, False, True, False, False]
State prediction error at timestep 1176 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1176 of -1
Current timestep = 1177. State = [[ 0.02837725 -0.29064402]]. Action = [[-0.06569345 -0.00279497  0.08561271 -0.18028641]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1177 is [False, True, False, True, False, False]
Current timestep = 1178. State = [[ 0.0274421  -0.29775986]]. Action = [[ 0.0508042  -0.10201797 -0.14590687 -0.03934354]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1178 is [False, True, False, True, False, False]
Current timestep = 1179. State = [[ 0.02621488 -0.3037306 ]]. Action = [[-0.22318636 -0.21003462  0.06522188 -0.20091999]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1179 is [False, True, False, True, False, False]
Current timestep = 1180. State = [[ 0.03115948 -0.3041154 ]]. Action = [[ 0.13043731  0.00878492  0.13189542 -0.74337983]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1180 is [False, True, False, True, False, False]
Current timestep = 1181. State = [[ 0.04263168 -0.2959946 ]]. Action = [[-0.0808676   0.18352601 -0.08533397 -0.6430221 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1181 is [False, True, False, True, False, False]
Current timestep = 1182. State = [[ 0.04412879 -0.2883046 ]]. Action = [[ 0.17243859 -0.23067963  0.01310155  0.03331554]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1182 is [False, True, False, True, False, False]
Scene graph at timestep 1182 is [False, True, False, True, False, False]
State prediction error at timestep 1182 is tensor(1.7312e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1182 of -1
Current timestep = 1183. State = [[ 0.04413126 -0.28617314]]. Action = [[-0.1401698   0.04170099 -0.12329543 -0.97721255]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1183 is [False, True, False, True, False, False]
Current timestep = 1184. State = [[ 0.04378532 -0.2852398 ]]. Action = [[ 0.00816557 -0.14447825  0.10406435  0.3384025 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1184 is [False, True, False, True, False, False]
Scene graph at timestep 1184 is [False, True, False, True, False, False]
State prediction error at timestep 1184 is tensor(8.8237e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1184 of -1
Current timestep = 1185. State = [[ 0.03768044 -0.27449942]]. Action = [[-0.17057331  0.18384403 -0.22747827 -0.34190607]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1185 is [False, True, False, True, False, False]
Scene graph at timestep 1185 is [False, True, False, True, False, False]
State prediction error at timestep 1185 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1185 of 0
Current timestep = 1186. State = [[ 0.01581828 -0.25652805]]. Action = [[-0.10229993  0.02713004 -0.10209131  0.5708163 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1186 is [False, True, False, True, False, False]
Current timestep = 1187. State = [[ 0.0127052 -0.242952 ]]. Action = [[0.23364341 0.16935277 0.18842667 0.9682298 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1187 is [False, True, False, True, False, False]
Current timestep = 1188. State = [[ 0.01701437 -0.21961148]]. Action = [[ 0.01702422  0.09746692 -0.16782488  0.05274796]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1188 is [False, True, False, True, False, False]
Current timestep = 1189. State = [[ 0.02797024 -0.19744092]]. Action = [[ 0.2244415   0.20693627 -0.11007881  0.6267457 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1189 is [False, True, False, True, False, False]
Current timestep = 1190. State = [[ 0.04239274 -0.18066505]]. Action = [[ 0.24147648 -0.07899217 -0.0170587  -0.66566885]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1190 is [False, True, False, True, False, False]
Current timestep = 1191. State = [[ 0.04380709 -0.17787103]]. Action = [[ 0.17254657  0.06310204 -0.03356358 -0.19280124]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1191 is [False, True, False, True, False, False]
Current timestep = 1192. State = [[ 0.04381401 -0.1773113 ]]. Action = [[ 0.200245   -0.09026206  0.22006685  0.6670177 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1192 is [False, True, False, True, False, False]
Scene graph at timestep 1192 is [False, True, False, True, False, False]
State prediction error at timestep 1192 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1192 of -1
Current timestep = 1193. State = [[ 0.04233984 -0.18102482]]. Action = [[-0.1693932  -0.05195552 -0.07075641 -0.27730775]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1193 is [False, True, False, True, False, False]
Scene graph at timestep 1193 is [False, True, False, True, False, False]
State prediction error at timestep 1193 is tensor(8.8361e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1193 of -1
Current timestep = 1194. State = [[ 0.03995317 -0.1835404 ]]. Action = [[ 0.18828344 -0.12543915 -0.02742153 -0.05383885]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1194 is [False, True, False, True, False, False]
Current timestep = 1195. State = [[ 0.03982109 -0.17039813]]. Action = [[-0.04306012  0.23321325  0.1477313   0.94724536]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1195 is [False, True, False, True, False, False]
Current timestep = 1196. State = [[ 0.03899253 -0.14841996]]. Action = [[0.03318363 0.08713168 0.03248453 0.15004098]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1196 is [False, True, False, True, False, False]
Current timestep = 1197. State = [[ 0.03920145 -0.14038345]]. Action = [[ 0.24526602 -0.13698615 -0.10300888 -0.8660839 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1197 is [False, True, False, True, False, False]
Current timestep = 1198. State = [[ 0.04027873 -0.15238075]]. Action = [[ 0.12206292 -0.2484682   0.16883767  0.8291974 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1198 is [False, True, False, True, False, False]
Current timestep = 1199. State = [[ 0.0420208  -0.17472716]]. Action = [[ 0.08549768 -0.14409547  0.0510453  -0.21438587]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1199 is [False, True, False, True, False, False]
Scene graph at timestep 1199 is [False, True, False, True, False, False]
State prediction error at timestep 1199 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1199 of 0
Current timestep = 1200. State = [[ 0.0476129  -0.18828239]]. Action = [[ 0.19801694  0.09604689  0.04365698 -0.58732885]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 1200 is [False, True, False, True, False, False]
Current timestep = 1201. State = [[ 0.04592967 -0.1872838 ]]. Action = [[-0.2084394   0.07131523  0.23392928  0.09189343]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1201 is [False, True, False, True, False, False]
Scene graph at timestep 1201 is [False, True, False, True, False, False]
State prediction error at timestep 1201 is tensor(3.0652e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1201 of 0
Current timestep = 1202. State = [[ 0.04116923 -0.18729419]]. Action = [[-0.04378659 -0.00725597  0.16884172  0.18292665]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1202 is [False, True, False, True, False, False]
Current timestep = 1203. State = [[ 0.03819465 -0.19822426]]. Action = [[ 0.06750089 -0.17877999 -0.04123481  0.17755055]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1203 is [False, True, False, True, False, False]
Current timestep = 1204. State = [[ 0.03718258 -0.20329386]]. Action = [[-0.05443937  0.06294462  0.22926098 -0.8900853 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1204 is [False, True, False, True, False, False]
Current timestep = 1205. State = [[ 0.03618114 -0.21146798]]. Action = [[ 0.02119982 -0.14282142  0.17811209 -0.9327078 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 1205 is [False, True, False, True, False, False]
Current timestep = 1206. State = [[ 0.02808233 -0.2223597 ]]. Action = [[-0.20995246 -0.01487191 -0.21793756 -0.54482275]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1206 is [False, True, False, True, False, False]
Current timestep = 1207. State = [[ 0.01698887 -0.23785137]]. Action = [[ 0.07914999 -0.20109531 -0.03940006  0.52632356]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1207 is [False, True, False, True, False, False]
Current timestep = 1208. State = [[ 0.01515899 -0.25279427]]. Action = [[ 0.04812393 -0.04661807  0.03577954  0.67132926]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 1208 is [False, True, False, True, False, False]
Scene graph at timestep 1208 is [False, True, False, True, False, False]
State prediction error at timestep 1208 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1208 of -1
Current timestep = 1209. State = [[ 0.01863572 -0.2644732 ]]. Action = [[ 0.15642247 -0.1262053   0.20292515  0.9571128 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1209 is [False, True, False, True, False, False]
Current timestep = 1210. State = [[ 0.03101665 -0.28073755]]. Action = [[ 0.14506584 -0.13641444 -0.20392448  0.1373601 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1210 is [False, True, False, True, False, False]
Scene graph at timestep 1210 is [False, True, False, True, False, False]
State prediction error at timestep 1210 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1210 of -1
Current timestep = 1211. State = [[ 0.04622107 -0.28939608]]. Action = [[-0.05902171  0.11481619 -0.11506215  0.06287134]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 1211 is [False, True, False, True, False, False]
Current timestep = 1212. State = [[ 0.04805855 -0.278538  ]]. Action = [[-0.05502157  0.14227962 -0.21814907 -0.79593784]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 1212 is [False, True, False, True, False, False]
Current timestep = 1213. State = [[ 0.04872089 -0.2660664 ]]. Action = [[-0.06590667  0.06143057  0.20208353  0.95919037]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 1213 is [False, True, False, True, False, False]
Current timestep = 1214. State = [[ 0.04686326 -0.26350367]]. Action = [[-0.04222663 -0.04571809  0.21792561 -0.50683284]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1214 is [False, True, False, True, False, False]
Current timestep = 1215. State = [[ 0.04698881 -0.25475606]]. Action = [[ 0.09121689  0.17642313 -0.04278632 -0.48548698]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1215 is [False, True, False, True, False, False]
Current timestep = 1216. State = [[ 0.03968593 -0.25640744]]. Action = [[-0.15969518 -0.24024548  0.13445854 -0.5878648 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1216 is [False, True, False, True, False, False]
Current timestep = 1217. State = [[ 0.0302547  -0.26717648]]. Action = [[ 0.16383153  0.15018088 -0.00279897 -0.03595859]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1217 is [False, True, False, True, False, False]
Scene graph at timestep 1217 is [False, True, False, True, False, False]
State prediction error at timestep 1217 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1217 of -1
Current timestep = 1218. State = [[ 0.02752531 -0.27599755]]. Action = [[ 0.09861678 -0.12385985 -0.0404916   0.54667616]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1218 is [False, True, False, True, False, False]
Scene graph at timestep 1218 is [False, True, False, True, False, False]
State prediction error at timestep 1218 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1218 of -1
Current timestep = 1219. State = [[ 0.03473936 -0.2813711 ]]. Action = [[ 0.21698296  0.02089623 -0.1114659   0.41217744]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1219 is [False, True, False, True, False, False]
Scene graph at timestep 1219 is [False, True, False, True, False, False]
State prediction error at timestep 1219 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1219 of -1
Current timestep = 1220. State = [[ 0.05029149 -0.27869198]]. Action = [[ 0.19199449 -0.19060984 -0.01150912 -0.39840436]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1220 is [False, True, False, True, False, False]
Current timestep = 1221. State = [[ 0.05029149 -0.27869198]]. Action = [[ 0.24264693 -0.08279161 -0.00896341  0.5096611 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1221 is [False, False, True, True, False, False]
Current timestep = 1222. State = [[ 0.04774476 -0.2795761 ]]. Action = [[-0.22598608  0.05370462 -0.03291173 -0.9284143 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1222 is [False, False, True, True, False, False]
Current timestep = 1223. State = [[ 0.04302317 -0.27353278]]. Action = [[-0.10470563  0.15104383 -0.19978346  0.52138066]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1223 is [False, True, False, True, False, False]
Scene graph at timestep 1223 is [False, True, False, True, False, False]
State prediction error at timestep 1223 is tensor(2.6528e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1223 of -1
Current timestep = 1224. State = [[ 0.03030898 -0.26092595]]. Action = [[ 0.2120797  -0.1000365   0.14563167  0.36695623]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1224 is [False, True, False, True, False, False]
Current timestep = 1225. State = [[ 0.03204849 -0.24781306]]. Action = [[ 0.01968172  0.23316199 -0.0313271   0.7286949 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1225 is [False, True, False, True, False, False]
Current timestep = 1226. State = [[ 0.023003   -0.24341175]]. Action = [[-0.16784903 -0.21025427  0.08638078 -0.11678076]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1226 is [False, True, False, True, False, False]
Current timestep = 1227. State = [[ 0.01222008 -0.2444413 ]]. Action = [[ 0.00585604  0.12838778 -0.120377   -0.8124229 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1227 is [False, True, False, True, False, False]
Current timestep = 1228. State = [[ 0.00436187 -0.23994762]]. Action = [[-0.20302288  0.02044418  0.18987042 -0.9637726 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1228 is [False, True, False, True, False, False]
Current timestep = 1229. State = [[-0.0209548  -0.22969042]]. Action = [[-0.18100365  0.11429676 -0.15511575  0.17092574]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1229 is [False, True, False, True, False, False]
Current timestep = 1230. State = [[-0.04612199 -0.2066672 ]]. Action = [[-0.14970388  0.23457104  0.04878134  0.2563355 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1230 is [False, True, False, True, False, False]
Current timestep = 1231. State = [[-0.06110315 -0.18340544]]. Action = [[0.12114394 0.03348368 0.07535541 0.9430547 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1231 is [False, True, False, True, False, False]
Current timestep = 1232. State = [[-0.06048486 -0.17507057]]. Action = [[-0.0803981   0.04864684 -0.14295243 -0.91048664]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1232 is [True, False, False, True, False, False]
Current timestep = 1233. State = [[-0.06054404 -0.16783951]]. Action = [[ 0.06437597  0.04422492 -0.1877941  -0.95208985]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1233 is [True, False, False, True, False, False]
Current timestep = 1234. State = [[-0.05394788 -0.16818115]]. Action = [[ 0.21554548 -0.11662376  0.17493814  0.47209692]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1234 is [True, False, False, True, False, False]
Current timestep = 1235. State = [[-0.05419279 -0.16289085]]. Action = [[-0.21389636  0.17196637  0.08517602 -0.23779291]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1235 is [True, False, False, True, False, False]
Scene graph at timestep 1235 is [True, False, False, True, False, False]
State prediction error at timestep 1235 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1235 of 1
Current timestep = 1236. State = [[-0.05532916 -0.15407555]]. Action = [[ 0.02892497  0.01592317 -0.20951034 -0.9659235 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1236 is [True, False, False, True, False, False]
Scene graph at timestep 1236 is [True, False, False, True, False, False]
State prediction error at timestep 1236 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1236 of 0
Current timestep = 1237. State = [[-0.05212273 -0.15220861]]. Action = [[ 0.17509386 -0.02725445 -0.15074974 -0.41256642]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1237 is [True, False, False, True, False, False]
Current timestep = 1238. State = [[-0.04701354 -0.158756  ]]. Action = [[ 0.10304642 -0.13650969 -0.18208031  0.11721754]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1238 is [True, False, False, True, False, False]
Current timestep = 1239. State = [[-0.03098903 -0.16621639]]. Action = [[ 0.23143235 -0.0229454   0.05262175 -0.98185277]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1239 is [False, True, False, True, False, False]
Current timestep = 1240. State = [[-0.01404508 -0.18136382]]. Action = [[-0.05923697 -0.19496757  0.16462383 -0.08859378]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1240 is [False, True, False, True, False, False]
Scene graph at timestep 1240 is [False, True, False, True, False, False]
State prediction error at timestep 1240 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1240 of -1
Current timestep = 1241. State = [[-0.00715324 -0.19206621]]. Action = [[ 0.14190206  0.11543348 -0.19024688  0.85973716]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1241 is [False, True, False, True, False, False]
Current timestep = 1242. State = [[-0.19008279  0.17155612]]. Action = [[-0.22059393  0.06338733 -0.03760448  0.5449381 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1242 is [False, True, False, True, False, False]
Current timestep = 1243. State = [[-0.1821302   0.20005748]]. Action = [[-0.19044432  0.07666558  0.24435061  0.79007757]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1243 is [True, False, False, False, False, True]
Current timestep = 1244. State = [[-0.20191705  0.21985354]]. Action = [[-0.22726597  0.17121014  0.03575826 -0.85560906]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1244 is [True, False, False, False, False, True]
Scene graph at timestep 1244 is [True, False, False, False, False, True]
State prediction error at timestep 1244 is tensor(6.4986e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1244 of -1
Current timestep = 1245. State = [[-0.21940315  0.23568527]]. Action = [[ 0.1425153   0.00245333 -0.2004834   0.8668642 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1245 is [True, False, False, False, False, True]
Current timestep = 1246. State = [[-0.21914606  0.23607168]]. Action = [[-0.10263318  0.0092335  -0.01628082 -0.85976404]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1246 is [True, False, False, False, False, True]
Scene graph at timestep 1246 is [True, False, False, False, False, True]
State prediction error at timestep 1246 is tensor(4.0356e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1246 of 0
Current timestep = 1247. State = [[-0.21827213  0.23231977]]. Action = [[ 0.0183453  -0.09326316  0.1399216   0.19185066]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1247 is [True, False, False, False, False, True]
Current timestep = 1248. State = [[-0.22116888  0.23315212]]. Action = [[-0.06515887  0.11473909 -0.00942604  0.8059288 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1248 is [True, False, False, False, False, True]
Current timestep = 1249. State = [[-0.22653905  0.2396558 ]]. Action = [[-0.03880575 -0.00392799  0.09462303 -0.44577736]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1249 is [True, False, False, False, False, True]
