Current timestep = 0. State = [[-0.15941443  0.03485462]]. Action = [[-0.20755002  0.00063261 -0.00492862  0.29468012]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0169, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 0 of 1
Current timestep = 1. State = [[-0.16218683  0.04235329]]. Action = [[ 0.1733715   0.10429537 -0.21961524  0.6307514 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0128, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1 of 0
Current timestep = 2. State = [[-0.16513722  0.06336321]]. Action = [[-0.13159777  0.24621934 -0.04360297  0.06597912]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
Current timestep = 3. State = [[-0.18143357  0.09584266]]. Action = [[-0.24133767  0.19968805 -0.05156907  0.28423953]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0231, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3 of -1
Current timestep = 4. State = [[-0.20740627  0.12549256]]. Action = [[-0.20507562  0.05917001  0.13234723 -0.38591516]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
Scene graph at timestep 4 is [True, False, False, False, False, True]
State prediction error at timestep 4 is tensor(0.0297, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.21903685  0.14164792]]. Action = [[ 0.22937715  0.22044587 -0.01847056 -0.5835399 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, False, True]
Current timestep = 6. State = [[-0.20223114  0.14333317]]. Action = [[ 0.15373212 -0.1854106   0.04175514  0.9783077 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, False, True]
Current timestep = 7. State = [[-0.18663739  0.13524789]]. Action = [[ 0.12163904 -0.01121986  0.17862722 -0.82725275]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, False, True]
Scene graph at timestep 7 is [True, False, False, False, False, True]
State prediction error at timestep 7 is tensor(0.0168, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of 1
Current timestep = 8. State = [[-0.17914273  0.14149843]]. Action = [[-0.21861838  0.1486058  -0.05716029 -0.7452661 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, False, True]
Current timestep = 9. State = [[-0.182179    0.14559048]]. Action = [[ 0.1780603  -0.10875989  0.09468699 -0.6389792 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, False, True]
Current timestep = 10. State = [[-0.18372591  0.14973666]]. Action = [[-0.09053165  0.16047907 -0.09968707  0.510748  ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, False, True]
Scene graph at timestep 10 is [True, False, False, False, False, True]
State prediction error at timestep 10 is tensor(0.0172, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 10 of -1
Current timestep = 11. State = [[-0.19261682  0.16529265]]. Action = [[-0.15904735  0.07116503 -0.23565538 -0.2541672 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, False, True]
Scene graph at timestep 11 is [True, False, False, False, False, True]
State prediction error at timestep 11 is tensor(0.0167, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 11 of -1
Current timestep = 12. State = [[-0.20936146  0.18493001]]. Action = [[-0.22485189  0.18368804 -0.16085137  0.7215384 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, False, True]
Scene graph at timestep 12 is [True, False, False, False, False, True]
State prediction error at timestep 12 is tensor(0.0181, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.22836147  0.19023381]]. Action = [[-0.07451864 -0.23953708 -0.02953255  0.22267854]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, False, True]
Scene graph at timestep 13 is [True, False, False, False, False, True]
State prediction error at timestep 13 is tensor(0.0179, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.24356908  0.17427328]]. Action = [[-0.19957803  0.05639517  0.17227238 -0.9272402 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, False, True]
Scene graph at timestep 14 is [True, False, False, False, False, True]
State prediction error at timestep 14 is tensor(0.0090, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 14 of -1
Current timestep = 15. State = [[-0.2594095   0.17992824]]. Action = [[-0.14464894 -0.04302479 -0.09889233  0.80222857]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, False, True]
Current timestep = 16. State = [[-0.255677    0.17756397]]. Action = [[ 0.16654772 -0.03154337 -0.12456474  0.04933774]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, False, True]
Current timestep = 17. State = [[-0.25586677  0.18022531]]. Action = [[-0.05218075  0.13178083  0.09075433 -0.06875658]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, False, True]
Current timestep = 18. State = [[-0.2543893  0.1740031]]. Action = [[-0.00602031 -0.23297678  0.10287499  0.9180136 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, False, True]
Scene graph at timestep 18 is [True, False, False, False, False, True]
State prediction error at timestep 18 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 18 of -1
Current timestep = 19. State = [[-0.25103664  0.16116278]]. Action = [[ 0.01964051  0.05322459 -0.16460472 -0.9102668 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, False, True]
Current timestep = 20. State = [[-0.25125828  0.16173024]]. Action = [[-0.15936828  0.05116981 -0.21346079 -0.22423297]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, False, True]
Current timestep = 21. State = [[-0.2513689   0.16209388]]. Action = [[-0.18640408 -0.22125559 -0.24247864  0.0348053 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, False, True]
Current timestep = 22. State = [[-0.2514328   0.16235115]]. Action = [[-0.21540068 -0.22399688  0.11777014 -0.22218114]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, False, True]
Current timestep = 23. State = [[-0.24510594  0.17273025]]. Action = [[ 0.20581025  0.19323868 -0.18966098  0.785552  ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, False, True]
Current timestep = 24. State = [[-0.24148712  0.19806583]]. Action = [[-0.08739632  0.24855483 -0.23311602 -0.83815205]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, False, True]
Scene graph at timestep 24 is [True, False, False, False, False, True]
State prediction error at timestep 24 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 24 of -1
Current timestep = 25. State = [[-0.25558433  0.23422556]]. Action = [[-0.15659867  0.20170695  0.08742028  0.7873244 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, False, True]
Current timestep = 26. State = [[-0.2639982   0.24398477]]. Action = [[-0.01394661 -0.10684189  0.0345847   0.66801476]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, False, True]
Scene graph at timestep 26 is [True, False, False, False, False, True]
State prediction error at timestep 26 is tensor(0.0101, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 26 of -1
Current timestep = 27. State = [[-0.26379418  0.2433906 ]]. Action = [[-0.08373541  0.18732804 -0.12289983 -0.09759915]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, False, True]
Scene graph at timestep 27 is [True, False, False, False, False, True]
State prediction error at timestep 27 is tensor(0.0137, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 27 of -1
Current timestep = 28. State = [[-0.26116768  0.23645526]]. Action = [[-0.01096702 -0.1320445  -0.19848448  0.86304164]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, False, True]
Current timestep = 29. State = [[-0.25851637  0.2293853 ]]. Action = [[-0.11072698  0.19592807  0.07887018  0.42647207]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, False, True]
Current timestep = 30. State = [[-0.25237134  0.22987111]]. Action = [[ 0.24372423  0.06669152 -0.05651081  0.8131511 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, False, True]
Current timestep = 31. State = [[-0.23452145  0.23498021]]. Action = [[ 0.22031668  0.07413289  0.12619224 -0.51385975]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, False, True]
Current timestep = 32. State = [[-0.20327966  0.22881356]]. Action = [[ 0.21458644 -0.17306258  0.05854079 -0.02858078]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, False, True]
Current timestep = 33. State = [[-0.17458446  0.21870947]]. Action = [[ 0.21193793 -0.00183606 -0.04379326 -0.05189872]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, False, True]
Current timestep = 34. State = [[-0.14516284  0.22837232]]. Action = [[0.22321069 0.20744276 0.1146484  0.4842111 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, False, True]
Scene graph at timestep 34 is [True, False, False, False, False, True]
State prediction error at timestep 34 is tensor(0.0099, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.10567368  0.23295972]]. Action = [[ 0.20725328 -0.2027537   0.22807765 -0.87883574]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, False, True]
Current timestep = 36. State = [[-0.08376909  0.21573903]]. Action = [[ 0.09155059 -0.0923543   0.1490857  -0.88020545]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, False, True]
Scene graph at timestep 36 is [True, False, False, False, False, True]
State prediction error at timestep 36 is tensor(0.0123, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 36 of 1
Current timestep = 37. State = [[-0.06640822  0.21004583]]. Action = [[ 0.12668532  0.0623883  -0.05381921 -0.21828538]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, False, True]
Current timestep = 38. State = [[-0.05225545  0.22356877]]. Action = [[ 0.08475643  0.17966998 -0.07762593  0.7877816 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, False, True]
Scene graph at timestep 38 is [True, False, False, False, False, True]
State prediction error at timestep 38 is tensor(0.0173, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 38 of -1
Current timestep = 39. State = [[-0.03673616  0.23198143]]. Action = [[-0.2386465  -0.1907356   0.20787239  0.95386434]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, False, True]
Current timestep = 40. State = [[-0.04853238  0.23778059]]. Action = [[-0.23430139  0.20442349 -0.09577024 -0.04863244]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [False, True, False, False, False, True]
Current timestep = 41. State = [[-0.07375114  0.24590118]]. Action = [[-0.19438587 -0.1336793  -0.20913644  0.20874166]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [False, True, False, False, False, True]
Current timestep = 42. State = [[-0.09353054  0.24628174]]. Action = [[ 0.12328824  0.17377996 -0.18808463  0.7156577 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, False, True]
Current timestep = 43. State = [[-0.10198245  0.2644854 ]]. Action = [[-0.1829855   0.2034049   0.17384434 -0.33746302]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, False, True]
Current timestep = 44. State = [[-0.11966016  0.2763448 ]]. Action = [[-0.15794216 -0.15092157 -0.13580778  0.8480387 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, False, True]
Current timestep = 45. State = [[-0.13290454  0.25977635]]. Action = [[-0.16314523 -0.23157227  0.24590155 -0.6183024 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, False, True]
Current timestep = 46. State = [[-0.14346826  0.23280317]]. Action = [[ 0.16404691 -0.13573551 -0.21045657 -0.00022787]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, False, True]
Current timestep = 47. State = [[-0.1476884   0.20654914]]. Action = [[-0.1997117  -0.21955623  0.00182155 -0.7329455 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, False, True]
Scene graph at timestep 47 is [True, False, False, False, False, True]
State prediction error at timestep 47 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.17230046  0.18950173]]. Action = [[-0.10782327  0.15128988 -0.22431405  0.6293082 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, False, True]
Scene graph at timestep 48 is [True, False, False, False, False, True]
State prediction error at timestep 48 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 48 of 0
Current timestep = 49. State = [[-0.1890561   0.19443153]]. Action = [[-0.23780906 -0.13494901 -0.23899834  0.1047734 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, False, True]
Current timestep = 50. State = [[-0.21578899  0.19161022]]. Action = [[-0.05080278  0.1439515  -0.05334972  0.06692874]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, False, True]
Scene graph at timestep 50 is [True, False, False, False, False, True]
State prediction error at timestep 50 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 50 of -1
Current timestep = 51. State = [[-0.22994943  0.1891955 ]]. Action = [[-0.11324108 -0.21938378 -0.22511896  0.23558998]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, False, True]
Current timestep = 52. State = [[-0.23518364  0.18098395]]. Action = [[0.2250889  0.15070295 0.22736385 0.07035446]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, False, True]
Current timestep = 53. State = [[-0.22547925  0.19458635]]. Action = [[0.15410364 0.20766765 0.05466133 0.29154956]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, False, True]
Scene graph at timestep 53 is [True, False, False, False, False, True]
State prediction error at timestep 53 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 53 of -1
Current timestep = 54. State = [[-0.21102902  0.20612869]]. Action = [[ 0.1032722  -0.05746804 -0.10793328 -0.7864983 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, False, True]
Scene graph at timestep 54 is [True, False, False, False, False, True]
State prediction error at timestep 54 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 54 of -1
Current timestep = 55. State = [[-0.20318387  0.20723853]]. Action = [[ 0.01570594  0.05946374  0.09693769 -0.98765033]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, False, True]
Current timestep = 56. State = [[-0.19132604  0.21608925]]. Action = [[ 0.20859283  0.10780334  0.23055482 -0.16614956]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, False, True]
Current timestep = 57. State = [[-0.16559812  0.22932404]]. Action = [[0.23210856 0.05563724 0.05879685 0.6361432 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, False, True]
Current timestep = 58. State = [[-0.13261189  0.22677708]]. Action = [[ 0.24527067 -0.13226187 -0.08103365  0.8811426 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, False, True]
Current timestep = 59. State = [[-0.10252973  0.21648465]]. Action = [[ 0.11006171 -0.06076601 -0.09799042 -0.5493943 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, False, True]
Current timestep = 60. State = [[-0.08463193  0.21485753]]. Action = [[ 0.08373809  0.05759147 -0.01701361 -0.43932378]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, False, True]
Current timestep = 61. State = [[-0.07975534  0.2200441 ]]. Action = [[-0.23825608 -0.01599836 -0.12850405 -0.7000831 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, False, True]
Current timestep = 62. State = [[-0.08551228  0.22087453]]. Action = [[-0.11556859 -0.02155586  0.00571546 -0.28793877]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, False, True]
Current timestep = 63. State = [[-0.09933927  0.20770913]]. Action = [[-0.22081247 -0.22021447  0.04251558  0.4608122 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, False, True]
Current timestep = 64. State = [[-0.1305437   0.20627318]]. Action = [[-0.20340651  0.22293854 -0.12538901 -0.416973  ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, False, True]
Current timestep = 65. State = [[-0.15535226  0.22202885]]. Action = [[-0.05543843  0.04265195  0.22628453  0.509233  ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, False, True]
Current timestep = 66. State = [[-0.17209707  0.24009952]]. Action = [[-0.20708829  0.1924982   0.05514652 -0.8028915 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, False, True]
Current timestep = 67. State = [[-0.18485084  0.2525804 ]]. Action = [[ 0.21070242  0.00600275  0.11273748 -0.28275716]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, False, True]
Current timestep = 68. State = [[-0.17509063  0.25379655]]. Action = [[ 0.20963329  0.04871076 -0.09596062  0.39329505]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, False, True]
Scene graph at timestep 68 is [True, False, False, False, False, True]
State prediction error at timestep 68 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 68 of 0
Current timestep = 69. State = [[-0.15592125  0.24696052]]. Action = [[ 0.11072364 -0.17651197 -0.04123776 -0.6731534 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, False, True]
Current timestep = 70. State = [[-0.13924035  0.22052869]]. Action = [[ 0.13022077 -0.228764   -0.12258646 -0.85455   ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, False, True]
Current timestep = 71. State = [[-0.1333763   0.20345229]]. Action = [[-0.22683005 -0.00250317 -0.11041731 -0.43486607]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, False, True]
Scene graph at timestep 71 is [True, False, False, False, False, True]
State prediction error at timestep 71 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 71 of 1
Current timestep = 72. State = [[-0.13937847  0.20804296]]. Action = [[ 0.09128326  0.17460442 -0.02410078  0.84232795]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, False, True]
Current timestep = 73. State = [[-0.14314206  0.20807596]]. Action = [[-0.1317371  -0.17667666  0.18727428  0.8774276 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, False, True]
Scene graph at timestep 73 is [True, False, False, False, False, True]
State prediction error at timestep 73 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of 1
Current timestep = 74. State = [[-0.14310057  0.1900004 ]]. Action = [[ 0.02956516 -0.2218017  -0.13278635 -0.44977456]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, False, True]
Current timestep = 75. State = [[-0.14902292  0.1797196 ]]. Action = [[-0.19110993  0.06632957  0.01632848 -0.8559945 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, False, True]
Current timestep = 76. State = [[-0.16295385  0.1854727 ]]. Action = [[-0.05791891  0.07991427  0.02980319  0.5163399 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, False, True]
Current timestep = 77. State = [[-0.16359715  0.19378945]]. Action = [[ 0.20401168  0.10985023  0.07959783 -0.61688435]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, False, True]
Current timestep = 78. State = [[-0.16164975  0.1903529 ]]. Action = [[-0.07301232 -0.16888417  0.21950224  0.22997689]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, False, True]
Current timestep = 79. State = [[-0.16648044  0.1924541 ]]. Action = [[-0.10953373  0.13056827  0.10231915  0.70913696]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, False, True]
Current timestep = 80. State = [[-0.16812362  0.20644735]]. Action = [[ 0.15349373  0.19474584 -0.0820173  -0.81071115]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, False, True]
Scene graph at timestep 80 is [True, False, False, False, False, True]
State prediction error at timestep 80 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 80 of 0
Current timestep = 81. State = [[-0.17085041  0.20986404]]. Action = [[-0.20742232 -0.21415424 -0.07307249 -0.8936675 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, False, True]
Current timestep = 82. State = [[-0.17601666  0.19904616]]. Action = [[-0.03882039 -0.04072556 -0.22312361 -0.8980347 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, False, True]
Current timestep = 83. State = [[-0.16925143  0.1810896 ]]. Action = [[ 0.21241584 -0.22931948 -0.04985422  0.63667226]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, False, True]
Current timestep = 84. State = [[-0.15975507  0.1547813 ]]. Action = [[ 0.05590641 -0.13393956  0.21171504 -0.95005894]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, False, True]
Scene graph at timestep 84 is [True, False, False, False, False, True]
State prediction error at timestep 84 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of 1
Current timestep = 85. State = [[-0.15320355  0.13465752]]. Action = [[ 0.02248427 -0.06135568 -0.20037533  0.7971779 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, False, True]
Scene graph at timestep 85 is [True, False, False, False, False, True]
State prediction error at timestep 85 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.15726346  0.11751822]]. Action = [[-0.2111542  -0.20232813 -0.01953536  0.87038827]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, False, True]
Scene graph at timestep 86 is [True, False, False, False, True, False]
State prediction error at timestep 86 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 86 of -1
Current timestep = 87. State = [[-0.16375963  0.10607126]]. Action = [[ 0.23180422  0.1695233  -0.13291644 -0.7551383 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
Scene graph at timestep 87 is [True, False, False, False, True, False]
State prediction error at timestep 87 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 87 of 1
Current timestep = 88. State = [[-0.15191728  0.12140135]]. Action = [[ 0.1169022   0.11744004  0.21096447 -0.6934861 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
Current timestep = 89. State = [[-0.15204102  0.13890077]]. Action = [[-0.20533563  0.13999635  0.17649692 -0.5048282 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
Current timestep = 90. State = [[-0.16815813  0.15608388]]. Action = [[-0.22258762  0.03900942  0.1511775  -0.66827065]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, False, True]
Current timestep = 91. State = [[-0.17546616  0.15593128]]. Action = [[ 0.23004463 -0.10137914  0.22147593 -0.43236274]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, False, True]
Current timestep = 92. State = [[-0.17012835  0.14982297]]. Action = [[ 0.05544162  0.01167956 -0.17094813  0.30516362]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, False, True]
Scene graph at timestep 92 is [True, False, False, False, False, True]
State prediction error at timestep 92 is tensor(3.8672e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 92 of -1
Current timestep = 93. State = [[-0.16243957  0.14217974]]. Action = [[ 0.11617464 -0.10906857 -0.15795313  0.27255845]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, False, True]
Current timestep = 94. State = [[-0.15565847  0.14767729]]. Action = [[-0.13100204  0.2349793  -0.19154973 -0.14673954]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, False, True]
Scene graph at timestep 94 is [True, False, False, False, False, True]
State prediction error at timestep 94 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 0
Current timestep = 95. State = [[-0.16826876  0.17385213]]. Action = [[-0.19987017  0.20197749  0.23134446 -0.37722802]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, False, True]
Current timestep = 96. State = [[-0.17370239  0.2029823 ]]. Action = [[0.2130647  0.21269196 0.21020287 0.73553264]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, False, True]
Current timestep = 97. State = [[-0.15696225  0.22986756]]. Action = [[ 0.210002    0.1983009   0.23356336 -0.26015723]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, False, True]
Current timestep = 98. State = [[-0.14255771  0.24875133]]. Action = [[-0.02057482 -0.03025578 -0.00912192 -0.32860768]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, False, True]
Current timestep = 99. State = [[-0.13525169  0.23893286]]. Action = [[ 0.09929895 -0.22324395 -0.12016943 -0.86077696]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, False, True]
Current timestep = 100. State = [[-0.1151813   0.21475786]]. Action = [[ 0.23970085 -0.1196844  -0.12770627 -0.9389183 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, False, True]
Current timestep = 101. State = [[-0.1012297   0.20809707]]. Action = [[-0.13864215  0.05187425 -0.09645197 -0.9273581 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, False, True]
Current timestep = 102. State = [[-0.11134999  0.22145672]]. Action = [[-0.22105095  0.18409249 -0.1287394  -0.77639943]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, False, True]
Current timestep = 103. State = [[-0.11658057  0.2324478 ]]. Action = [[ 0.22255594 -0.02537061  0.19917852 -0.7953204 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, False, True]
Current timestep = 104. State = [[-0.11383047  0.2205246 ]]. Action = [[-0.10713333 -0.2069662   0.21265787  0.9651015 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, False, True]
Current timestep = 105. State = [[-0.11851464  0.20570613]]. Action = [[-0.12838247 -0.07569325 -0.1102241   0.44386315]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, False, True]
Current timestep = 106. State = [[-0.13108183  0.19501072]]. Action = [[-0.19100846 -0.05228008 -0.18534555  0.30383968]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, False, True]
Current timestep = 107. State = [[-0.13977997  0.18923196]]. Action = [[ 0.19130966 -0.00196482  0.18346983  0.33862364]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, False, True]
Current timestep = 108. State = [[-0.13880257  0.18439294]]. Action = [[-0.06239861 -0.06525807  0.07219845  0.54843533]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, False, True]
Current timestep = 109. State = [[-0.1415794   0.16805193]]. Action = [[-0.06745616 -0.21372546 -0.04717064 -0.7309623 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, False, True]
Current timestep = 110. State = [[-0.14551136  0.14772889]]. Action = [[ 0.03694928 -0.07566309 -0.0414449   0.3468461 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, False, True]
Current timestep = 111. State = [[-0.15234601  0.14799054]]. Action = [[-1.8210506e-01  1.3826579e-01 -4.0081143e-04  6.6728926e-01]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, False, True]
Current timestep = 112. State = [[-0.1535681   0.15385711]]. Action = [[ 0.22910148  0.01697266  0.18390268 -0.8580987 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, False, True]
Current timestep = 113. State = [[-0.15207545  0.1583856 ]]. Action = [[-0.04617177  0.08283809 -0.06661819 -0.05094224]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, False, True]
Current timestep = 114. State = [[-0.15493803  0.16626965]]. Action = [[-0.14427371  0.03342491  0.01180503  0.62227476]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, False, True]
Current timestep = 115. State = [[-0.16185415  0.17688054]]. Action = [[-0.05310476  0.07924792 -0.04628707  0.04339862]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, False, True]
Current timestep = 116. State = [[-0.16875757  0.18378103]]. Action = [[-0.06879427 -0.00284545  0.07751369  0.95140624]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, False, True]
Current timestep = 117. State = [[-0.17036481  0.1826667 ]]. Action = [[ 0.16196096 -0.03173777  0.2280156   0.2623725 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, False, True]
Current timestep = 118. State = [[-0.17485549  0.18760431]]. Action = [[-0.18744606  0.09765434 -0.13899674  0.472417  ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, False, True]
Current timestep = 119. State = [[-0.17565373  0.18340603]]. Action = [[ 0.10278544 -0.19454636 -0.19713905 -0.3814413 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, False, True]
Current timestep = 120. State = [[-0.17939562  0.18676287]]. Action = [[-0.24223925  0.21177882 -0.12263113  0.0637995 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, False, True]
Current timestep = 121. State = [[-0.19658065  0.20488045]]. Action = [[-0.00127172  0.12483603  0.1987598   0.00231802]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, False, True]
Scene graph at timestep 121 is [True, False, False, False, False, True]
State prediction error at timestep 121 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 121 of 1
Current timestep = 122. State = [[-0.2115119   0.20479088]]. Action = [[-0.22524895 -0.17455979  0.22168708 -0.6714256 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, False, True]
Scene graph at timestep 122 is [True, False, False, False, False, True]
State prediction error at timestep 122 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 122 of -1
Current timestep = 123. State = [[-0.2330184   0.19124645]]. Action = [[-4.6029270e-02 -5.8721006e-04  8.6667776e-02 -8.7190783e-01]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, False, True]
Scene graph at timestep 123 is [True, False, False, False, False, True]
State prediction error at timestep 123 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of -1
Current timestep = 124. State = [[-0.24514507  0.18693343]]. Action = [[-0.20669664 -0.09722635  0.10058719 -0.8026688 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, False, True]
Current timestep = 125. State = [[-0.26275247  0.19071013]]. Action = [[ 0.08212855  0.20037317 -0.05684268  0.5617683 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, False, True]
Current timestep = 126. State = [[-0.19503629 -0.17781295]]. Action = [[ 0.20508868 -0.16514315  0.04040796  0.19038975]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, False, True]
Current timestep = 127. State = [[-0.19376864 -0.20969082]]. Action = [[-0.10739508 -0.18422212  0.16915661  0.19165802]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 127 is [True, False, False, True, False, False]
Scene graph at timestep 127 is [True, False, False, True, False, False]
State prediction error at timestep 127 is tensor(0.0473, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 127 of -1
Current timestep = 128. State = [[-0.19169365 -0.22781014]]. Action = [[ 0.207829   -0.03365307 -0.19958887  0.6854328 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 128 is [True, False, False, True, False, False]
Current timestep = 129. State = [[-0.17675905 -0.22214264]]. Action = [[ 0.0086416   0.18391776  0.1350553  -0.36020303]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 129 is [True, False, False, True, False, False]
Current timestep = 130. State = [[-0.17220595 -0.21390897]]. Action = [[ 0.03598881 -0.03675513 -0.10041802 -0.89166445]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 130 is [True, False, False, True, False, False]
Current timestep = 131. State = [[-0.17653783 -0.2050968 ]]. Action = [[-0.2283464   0.17041141 -0.04329747 -0.39504254]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 131 is [True, False, False, True, False, False]
Scene graph at timestep 131 is [True, False, False, True, False, False]
State prediction error at timestep 131 is tensor(0.0421, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.19424148 -0.20013775]]. Action = [[-0.20816463 -0.06704202 -0.18382102  0.48384333]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 132 is [True, False, False, True, False, False]
Scene graph at timestep 132 is [True, False, False, True, False, False]
State prediction error at timestep 132 is tensor(0.0452, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 132 of -1
Current timestep = 133. State = [[-0.21161479 -0.20713447]]. Action = [[ 0.06529981 -0.10717183 -0.03402705 -0.21983081]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 133 is [True, False, False, True, False, False]
Scene graph at timestep 133 is [True, False, False, True, False, False]
State prediction error at timestep 133 is tensor(0.0412, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 133 of -1
Current timestep = 134. State = [[-0.20171987 -0.20068099]]. Action = [[ 0.23797107  0.20571184  0.04998499 -0.95119727]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 134 is [True, False, False, True, False, False]
Current timestep = 135. State = [[-0.19007784 -0.19047187]]. Action = [[-0.02286105  0.00063217 -0.1494792  -0.44239807]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 135 is [True, False, False, True, False, False]
Current timestep = 136. State = [[-0.18199448 -0.1762266 ]]. Action = [[ 0.12351695  0.17919385 -0.22519906 -0.55648017]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 136 is [True, False, False, True, False, False]
Scene graph at timestep 136 is [True, False, False, True, False, False]
State prediction error at timestep 136 is tensor(0.0321, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.17160521 -0.15142833]]. Action = [[ 0.03556117  0.1431858  -0.035119    0.5346565 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 137 is [True, False, False, True, False, False]
Current timestep = 138. State = [[-0.17074008 -0.1392667 ]]. Action = [[-0.01664419  0.0424825  -0.06523645 -0.3747381 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 138 is [True, False, False, True, False, False]
Current timestep = 139. State = [[-0.17653604 -0.14183155]]. Action = [[-0.20849408 -0.10665104  0.1629082  -0.7610431 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 139 is [True, False, False, True, False, False]
Current timestep = 140. State = [[-0.18048921 -0.14103621]]. Action = [[0.07674932 0.10868239 0.10393134 0.826519  ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 140 is [True, False, False, True, False, False]
Current timestep = 141. State = [[-0.18830429 -0.12629929]]. Action = [[-0.24290276  0.17497548 -0.1924065   0.00642073]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 141 is [True, False, False, True, False, False]
Scene graph at timestep 141 is [True, False, False, True, False, False]
State prediction error at timestep 141 is tensor(0.0192, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of -1
Current timestep = 142. State = [[-0.2074869  -0.09522749]]. Action = [[-0.02358955  0.23884502 -0.1531933  -0.9281612 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 142 is [True, False, False, True, False, False]
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0155, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 142 of -1
Current timestep = 143. State = [[-0.20642582 -0.0681919 ]]. Action = [[0.18654835 0.08088142 0.08784151 0.44707727]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 143 is [True, False, False, False, True, False]
Current timestep = 144. State = [[-0.1916066  -0.05983881]]. Action = [[ 0.19479632  0.05730072 -0.15623462 -0.43437648]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 144 is [True, False, False, False, True, False]
Current timestep = 145. State = [[-0.16465846 -0.04889877]]. Action = [[ 0.24261183  0.09355462  0.07648048 -0.63693905]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 145 is [True, False, False, False, True, False]
Current timestep = 146. State = [[-0.14395955 -0.05283377]]. Action = [[-0.03597364 -0.22674896 -0.09505132 -0.65680146]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 146 is [True, False, False, False, True, False]
Current timestep = 147. State = [[-0.13819288 -0.06081213]]. Action = [[ 0.10132915  0.03700784  0.13047272 -0.9433272 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 147 is [True, False, False, False, True, False]
Current timestep = 148. State = [[-0.13663697 -0.06781077]]. Action = [[-0.20638505 -0.08865553 -0.02536182  0.2197423 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 148 is [True, False, False, False, True, False]
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0124, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of 1
Current timestep = 149. State = [[-0.14546815 -0.06952189]]. Action = [[-0.09670207  0.13261628 -0.00578324  0.7017169 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 149 is [True, False, False, False, True, False]
Current timestep = 150. State = [[-0.16001931 -0.05359081]]. Action = [[-0.22444086  0.10144955  0.0138844  -0.6428971 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 150 is [True, False, False, False, True, False]
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0102, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 150 of -1
Current timestep = 151. State = [[-0.18323708 -0.05491423]]. Action = [[ 0.01060426 -0.17995448 -0.02867335  0.47616673]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 151 is [True, False, False, False, True, False]
Current timestep = 152. State = [[-0.1836715  -0.05266278]]. Action = [[ 0.09605449  0.22418994 -0.11320141 -0.06021899]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 152 is [True, False, False, False, True, False]
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(0.0089, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 152 of -1
Current timestep = 153. State = [[-0.1845664  -0.02641052]]. Action = [[-0.05890904  0.22032514  0.00645167  0.60166335]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 153 is [True, False, False, False, True, False]
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 153 of 1
Current timestep = 154. State = [[-0.19399007 -0.0019896 ]]. Action = [[-0.20542559  0.04997098  0.2188471   0.11892056]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 154 is [True, False, False, False, True, False]
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of -1
Current timestep = 155. State = [[-0.20279549  0.00491107]]. Action = [[ 0.17517146  0.03509623 -0.17198543  0.6551235 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 155 is [True, False, False, False, True, False]
Current timestep = 156. State = [[-0.19458848  0.01712874]]. Action = [[ 0.0853287   0.19114101 -0.19638419 -0.86225396]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 156 is [True, False, False, False, True, False]
Current timestep = 157. State = [[-0.19292498  0.04618788]]. Action = [[-0.05366248  0.23472154  0.16560623 -0.05549532]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 157 is [True, False, False, False, True, False]
Current timestep = 158. State = [[-0.20113456  0.06850268]]. Action = [[-0.22883084  0.0190756   0.1818183   0.2996372 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 158 is [True, False, False, False, True, False]
Current timestep = 159. State = [[-0.22040592  0.08211048]]. Action = [[-0.18701062  0.08421642  0.09403181  0.52983594]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 159 is [True, False, False, False, True, False]
Current timestep = 160. State = [[-0.22979453  0.08429232]]. Action = [[ 0.05829898 -0.08188431 -0.22391698 -0.15894789]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 160 is [True, False, False, False, True, False]
Current timestep = 161. State = [[-0.22998871  0.07775974]]. Action = [[-0.02677056 -0.0559016  -0.19569269  0.44724894]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 161 is [True, False, False, False, True, False]
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.2403127   0.07736246]]. Action = [[-0.10815749  0.10883424  0.14086622 -0.49488485]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 162 is [True, False, False, False, True, False]
Current timestep = 163. State = [[-0.2472188   0.08544195]]. Action = [[-0.22950211  0.23580688  0.02451012  0.82706094]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 163 is [True, False, False, False, True, False]
Current timestep = 164. State = [[-0.24525918  0.08525439]]. Action = [[ 0.13178173 -0.0287126  -0.05054399 -0.05486512]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 164 is [True, False, False, False, True, False]
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 164 of -1
Current timestep = 165. State = [[-0.2375231   0.07591256]]. Action = [[ 0.08782375 -0.11947744  0.19628355 -0.6971649 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 165 is [True, False, False, False, True, False]
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of 1
Current timestep = 166. State = [[-0.23335059  0.06437869]]. Action = [[-0.07112244 -0.02918532  0.23537537 -0.77202845]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 166 is [True, False, False, False, True, False]
Current timestep = 167. State = [[-0.24053353  0.06572501]]. Action = [[-0.1517893   0.05947694 -0.18168813  0.31264758]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 167 is [True, False, False, False, True, False]
Current timestep = 168. State = [[-0.24998988  0.07295627]]. Action = [[ 0.0210478   0.08436856 -0.06442581  0.28485346]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 168 is [True, False, False, False, True, False]
Current timestep = 169. State = [[-0.25218892  0.07765885]]. Action = [[-0.2442341  -0.21441782 -0.01034042  0.48339415]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 169 is [True, False, False, False, True, False]
Current timestep = 170. State = [[-0.2516534   0.06728643]]. Action = [[-0.00925764 -0.20466487  0.07485163  0.01015007]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 170 is [True, False, False, False, True, False]
Current timestep = 171. State = [[-0.2518476   0.05692651]]. Action = [[-0.20933357 -0.22486313  0.14385402 -0.8490946 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 171 is [True, False, False, False, True, False]
Current timestep = 172. State = [[-0.2552694   0.06508579]]. Action = [[-0.02527492  0.19274792  0.10627583  0.9337503 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 172 is [True, False, False, False, True, False]
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 172 of -1
Current timestep = 173. State = [[-0.25882897  0.08848967]]. Action = [[ 0.06534186  0.22621411 -0.06195973  0.56494164]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 173 is [True, False, False, False, True, False]
Current timestep = 174. State = [[-0.2535904   0.09295935]]. Action = [[ 0.14120984 -0.17345369  0.21663177 -0.86389065]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 174 is [True, False, False, False, True, False]
Current timestep = 175. State = [[-0.24396734  0.0899667 ]]. Action = [[-0.19475049  0.2150439   0.04562667  0.57639885]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 175 is [True, False, False, False, True, False]
Current timestep = 176. State = [[-0.23181815  0.07647807]]. Action = [[ 0.2139458  -0.19696873 -0.15811364  0.03786981]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 176 is [True, False, False, False, True, False]
Current timestep = 177. State = [[-0.21949576  0.07341849]]. Action = [[-0.11547764  0.17623112  0.11953753  0.6446662 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 177 is [True, False, False, False, True, False]
Current timestep = 178. State = [[-0.21438308  0.07488636]]. Action = [[ 0.18422353 -0.08712378 -0.01254082  0.24763918]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 178 is [True, False, False, False, True, False]
Current timestep = 179. State = [[-0.2054647   0.06758307]]. Action = [[-0.0815156  -0.11395958 -0.16949442  0.44442725]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 179 is [True, False, False, False, True, False]
Current timestep = 180. State = [[-0.20324986  0.06327519]]. Action = [[0.09802514 0.07556301 0.15965644 0.5931144 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 180 is [True, False, False, False, True, False]
Current timestep = 181. State = [[-0.2048744   0.07577188]]. Action = [[-0.09309311  0.20256507 -0.20666173 -0.72552085]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 181 is [True, False, False, False, True, False]
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.20101187  0.09543673]]. Action = [[ 0.16933915  0.07438409 -0.22925429 -0.67417854]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 182 is [True, False, False, False, True, False]
Current timestep = 183. State = [[-0.19377388  0.09235061]]. Action = [[-0.1615864  -0.1902625  -0.17675318 -0.15698665]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 183 is [True, False, False, False, True, False]
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of 1
Current timestep = 184. State = [[-0.19939992  0.08442249]]. Action = [[-0.0836692   0.05173647 -0.10033958  0.65881205]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 184 is [True, False, False, False, True, False]
Current timestep = 185. State = [[-0.21116048  0.07349114]]. Action = [[-0.14725964 -0.22597592 -0.00264704 -0.7447868 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 185 is [True, False, False, False, True, False]
Current timestep = 186. State = [[-0.23061551  0.06351782]]. Action = [[-0.15546642  0.06727031  0.03500134  0.47333407]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 186 is [True, False, False, False, True, False]
Current timestep = 187. State = [[-0.2379327   0.06523214]]. Action = [[ 0.17096156  0.04676372 -0.15639567  0.6056881 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 187 is [True, False, False, False, True, False]
Current timestep = 188. State = [[-0.23035122  0.07815268]]. Action = [[ 0.11642542  0.19558701 -0.04685178 -0.24596268]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 188 is [True, False, False, False, True, False]
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(1.1755e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of -1
Current timestep = 189. State = [[-0.22400515  0.09659689]]. Action = [[-0.10874408  0.02468932  0.01369125 -0.91217905]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 189 is [True, False, False, False, True, False]
Current timestep = 190. State = [[-0.2239887   0.08795715]]. Action = [[ 0.03883082 -0.21331765 -0.10759816 -0.9951274 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 190 is [True, False, False, False, True, False]
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of 1
Current timestep = 191. State = [[-0.22100152  0.07011446]]. Action = [[ 0.05909154 -0.08152238  0.00859526 -0.81001323]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 191 is [True, False, False, False, True, False]
Current timestep = 192. State = [[-0.22578906  0.04952781]]. Action = [[-0.14222799 -0.24058744  0.19529653 -0.81160164]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 192 is [True, False, False, False, True, False]
Current timestep = 193. State = [[-0.22461317  0.03207817]]. Action = [[0.18397856 0.01131448 0.21822369 0.8564508 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 193 is [True, False, False, False, True, False]
Current timestep = 194. State = [[-0.22634171  0.01879661]]. Action = [[-0.20397986 -0.15266371  0.11762968  0.44507575]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 194 is [True, False, False, False, True, False]
Current timestep = 195. State = [[-0.23422708  0.00956204]]. Action = [[-0.02790925  0.03715149 -0.12607388  0.7595047 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 195 is [True, False, False, False, True, False]
Current timestep = 196. State = [[-0.24041459 -0.00185143]]. Action = [[-0.07993698 -0.17573692  0.21105945 -0.44284344]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 196 is [True, False, False, False, True, False]
Current timestep = 197. State = [[-0.23519355 -0.01696952]]. Action = [[ 0.24051881 -0.05460769 -0.05782115 -0.02139449]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 197 is [True, False, False, False, True, False]
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of -1
Current timestep = 198. State = [[-0.21781333 -0.03814838]]. Action = [[ 0.21612    -0.22805084 -0.20737535  0.68320775]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 198 is [True, False, False, False, True, False]
Scene graph at timestep 198 is [True, False, False, False, True, False]
State prediction error at timestep 198 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 198 of 1
Current timestep = 199. State = [[-0.19578665 -0.07074203]]. Action = [[ 0.14035928 -0.20341475 -0.00542131 -0.48775268]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 199 is [True, False, False, False, True, False]
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of 1
Current timestep = 200. State = [[-0.17099755 -0.10065857]]. Action = [[ 0.21890569 -0.22865313 -0.13603368 -0.13805276]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 200 is [True, False, False, False, True, False]
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 200 of 1
Current timestep = 201. State = [[-0.14376311 -0.11181937]]. Action = [[ 0.10989082  0.18047386 -0.10618433 -0.12748283]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 201 is [True, False, False, False, True, False]
Current timestep = 202. State = [[-0.13066536 -0.11063115]]. Action = [[ 0.06300068 -0.14781308  0.1322496  -0.05160171]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 202 is [True, False, False, False, True, False]
Current timestep = 203. State = [[-0.12315376 -0.1206107 ]]. Action = [[ 0.05007052 -0.07792869 -0.01371659 -0.75844175]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 203 is [True, False, False, False, True, False]
Current timestep = 204. State = [[-0.10790471 -0.11922798]]. Action = [[ 0.18864256  0.13858157 -0.18914582  0.15568507]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 204 is [True, False, False, False, True, False]
Current timestep = 205. State = [[-0.09338316 -0.11365458]]. Action = [[-0.0498013  -0.03462453 -0.0267729   0.05773139]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 205 is [True, False, False, False, True, False]
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of 1
Current timestep = 206. State = [[-0.08272941 -0.10435453]]. Action = [[ 0.19761491  0.17153645 -0.11644375  0.08016384]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 206 is [True, False, False, False, True, False]
Current timestep = 207. State = [[-0.05953355 -0.08415196]]. Action = [[ 0.14604843  0.18282297  0.0843443  -0.7171961 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 207 is [True, False, False, False, True, False]
Current timestep = 208. State = [[-0.04057159 -0.0651379 ]]. Action = [[ 0.09693256  0.07722008 -0.19516785 -0.6800616 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 208 is [True, False, False, False, True, False]
Current timestep = 209. State = [[-0.16241576 -0.19511078]]. Action = [[ 0.16420135 -0.19649239  0.19320858 -0.720212  ]]. Reward = [100.]
Curr episode timestep = 82
Scene graph at timestep 209 is [False, True, False, False, True, False]
Current timestep = 210. State = [[-0.14789419 -0.2225376 ]]. Action = [[-0.05145344 -0.06034909  0.08532083  0.67509127]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 210 is [True, False, False, True, False, False]
Scene graph at timestep 210 is [True, False, False, True, False, False]
State prediction error at timestep 210 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[-0.15219854 -0.22777024]]. Action = [[-0.11719128  0.06317347 -0.24459346  0.803247  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 211 is [True, False, False, True, False, False]
Current timestep = 212. State = [[-0.15385668 -0.2291767 ]]. Action = [[ 0.12113708 -0.08960727  0.24429959  0.11429763]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 212 is [True, False, False, True, False, False]
Current timestep = 213. State = [[-0.1520979 -0.2313028]]. Action = [[ 0.03970829  0.00388113  0.14046645 -0.47017705]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 213 is [True, False, False, True, False, False]
Current timestep = 214. State = [[-0.1434727  -0.24298927]]. Action = [[ 0.11898291 -0.18740438  0.13354492  0.03449094]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 214 is [True, False, False, True, False, False]
Current timestep = 215. State = [[-0.12109543 -0.24344228]]. Action = [[ 0.24253145  0.22933757  0.05725032 -0.8327104 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 215 is [True, False, False, True, False, False]
Current timestep = 216. State = [[-0.09169149 -0.22809482]]. Action = [[0.17666525 0.07689989 0.15523964 0.1240356 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 216 is [True, False, False, True, False, False]
Scene graph at timestep 216 is [True, False, False, True, False, False]
State prediction error at timestep 216 is tensor(0.0072, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 216 of 1
Current timestep = 217. State = [[-0.07305615 -0.21339737]]. Action = [[-0.04289606  0.12496847 -0.23327757  0.98533106]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 217 is [True, False, False, True, False, False]
Scene graph at timestep 217 is [True, False, False, True, False, False]
State prediction error at timestep 217 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 217 of 1
Current timestep = 218. State = [[-0.06635033 -0.19130033]]. Action = [[0.15776247 0.19845927 0.24099696 0.3340515 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 218 is [True, False, False, True, False, False]
Current timestep = 219. State = [[-0.0592692  -0.16462502]]. Action = [[-0.11165354  0.20426959 -0.14522582  0.19747567]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 219 is [True, False, False, True, False, False]
Scene graph at timestep 219 is [True, False, False, True, False, False]
State prediction error at timestep 219 is tensor(0.0075, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of 1
Current timestep = 220. State = [[-0.06402879 -0.16035369]]. Action = [[-0.08322169 -0.22921509 -0.22779335 -0.3718189 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 220 is [True, False, False, True, False, False]
Current timestep = 221. State = [[-0.07705437 -0.16888721]]. Action = [[-0.22570734  0.09072909  0.15143418 -0.4923978 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 221 is [True, False, False, True, False, False]
Current timestep = 222. State = [[-0.09008235 -0.15792067]]. Action = [[-0.06808591  0.16596389 -0.03305581  0.00918221]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 222 is [True, False, False, True, False, False]
Current timestep = 223. State = [[-0.10785822 -0.13861358]]. Action = [[-0.19886298  0.1132575   0.12405413 -0.46321696]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 223 is [True, False, False, True, False, False]
Current timestep = 224. State = [[-0.12156749 -0.11764138]]. Action = [[0.07883912 0.16013926 0.12061447 0.93188393]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 224 is [True, False, False, True, False, False]
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of -1
Current timestep = 225. State = [[-0.12206483 -0.09071881]]. Action = [[-0.00504377  0.18514091 -0.04451719 -0.7565868 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 225 is [True, False, False, False, True, False]
Current timestep = 226. State = [[-0.11865494 -0.08985495]]. Action = [[ 0.162694   -0.24010941  0.08073589 -0.4369725 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 226 is [True, False, False, False, True, False]
Current timestep = 227. State = [[-0.10866126 -0.09660634]]. Action = [[ 0.13590097  0.0656178   0.04234785 -0.8946719 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 227 is [True, False, False, False, True, False]
Current timestep = 228. State = [[-0.10116471 -0.09519337]]. Action = [[-0.01016514  0.02801746 -0.1292815  -0.8060217 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 228 is [True, False, False, False, True, False]
Current timestep = 229. State = [[-0.10366528 -0.08528999]]. Action = [[-0.18352573  0.14283076  0.0250887   0.88439715]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 229 is [True, False, False, False, True, False]
Scene graph at timestep 229 is [True, False, False, False, True, False]
State prediction error at timestep 229 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 229 of 1
Current timestep = 230. State = [[-0.10554679 -0.08693127]]. Action = [[ 0.10031009 -0.22792459 -0.23525323  0.0492425 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 230 is [True, False, False, False, True, False]
Current timestep = 231. State = [[-0.10554224 -0.11447678]]. Action = [[ 0.00493634 -0.24503322 -0.23261449  0.5714816 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 231 is [True, False, False, False, True, False]
Current timestep = 232. State = [[-0.096522   -0.14293706]]. Action = [[ 0.23088166 -0.16234039 -0.02522449  0.4810846 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 232 is [True, False, False, False, True, False]
Current timestep = 233. State = [[-0.0798633  -0.14465587]]. Action = [[ 0.01815203  0.22061306 -0.00968698  0.9704696 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 233 is [True, False, False, True, False, False]
Current timestep = 234. State = [[-0.07619303 -0.1341221 ]]. Action = [[0.031995   0.03261936 0.11773407 0.2907617 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 234 is [True, False, False, True, False, False]
Current timestep = 235. State = [[-0.07446229 -0.13304095]]. Action = [[-0.1213302  -0.05474252 -0.17270468  0.51870966]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 235 is [True, False, False, True, False, False]
Current timestep = 236. State = [[-0.07138822 -0.14125146]]. Action = [[ 0.1790362  -0.13974081 -0.00883424  0.44138634]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 236 is [True, False, False, True, False, False]
Current timestep = 237. State = [[-0.05714692 -0.14858471]]. Action = [[ 0.18217152 -0.00405161  0.19737309 -0.46449006]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 237 is [True, False, False, True, False, False]
Current timestep = 238. State = [[-0.04454754 -0.16402927]]. Action = [[-0.06165031 -0.21365191  0.2114209   0.3813324 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 238 is [True, False, False, True, False, False]
Scene graph at timestep 238 is [False, True, False, True, False, False]
State prediction error at timestep 238 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 238 of 0
Current timestep = 239. State = [[-0.03977171 -0.18464327]]. Action = [[ 0.13241509 -0.05488709  0.03486043 -0.6743768 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 239 is [False, True, False, True, False, False]
Current timestep = 240. State = [[-0.03305982 -0.18690334]]. Action = [[-0.20398931  0.07624808 -0.15075125 -0.14938384]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 240 is [False, True, False, True, False, False]
Current timestep = 241. State = [[-0.04031993 -0.1827017 ]]. Action = [[-0.18829097  0.09301937  0.1628811   0.29934764]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 241 is [False, True, False, True, False, False]
Current timestep = 242. State = [[-0.06486573 -0.19072251]]. Action = [[-0.23694266 -0.20351644 -0.0413717   0.189286  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 242 is [False, True, False, True, False, False]
Scene graph at timestep 242 is [True, False, False, True, False, False]
State prediction error at timestep 242 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 242 of -1
Current timestep = 243. State = [[-0.09546089 -0.19496925]]. Action = [[-0.13162567  0.17849296  0.18424645  0.38513136]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 243 is [True, False, False, True, False, False]
Current timestep = 244. State = [[-0.10723548 -0.188076  ]]. Action = [[ 0.03913766 -0.10728526 -0.1771127   0.9358964 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 244 is [True, False, False, True, False, False]
Scene graph at timestep 244 is [True, False, False, True, False, False]
State prediction error at timestep 244 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of -1
Current timestep = 245. State = [[-0.10500093 -0.18399478]]. Action = [[ 0.15201753  0.11057436  0.15383637 -0.909547  ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 245 is [True, False, False, True, False, False]
Current timestep = 246. State = [[-0.10647097 -0.18902485]]. Action = [[-0.15233368 -0.16199401  0.18288851 -0.9015452 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 246 is [True, False, False, True, False, False]
Current timestep = 247. State = [[-0.11685152 -0.20405798]]. Action = [[-0.18797232 -0.09223334  0.08595234 -0.14075637]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 247 is [True, False, False, True, False, False]
Scene graph at timestep 247 is [True, False, False, True, False, False]
State prediction error at timestep 247 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 247 of -1
Current timestep = 248. State = [[-0.13495131 -0.22425675]]. Action = [[-0.03350082 -0.14737675  0.1382032   0.27417767]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 248 is [True, False, False, True, False, False]
Current timestep = 249. State = [[-0.13689896 -0.23449609]]. Action = [[ 0.07599148 -0.03389437  0.01974285  0.42334378]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 249 is [True, False, False, True, False, False]
Current timestep = 250. State = [[-0.13608131 -0.24942495]]. Action = [[ 0.00678805 -0.18589507  0.01085454  0.64724135]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 250 is [True, False, False, True, False, False]
Current timestep = 251. State = [[-0.14090045 -0.27216864]]. Action = [[-0.08638474 -0.11316893 -0.04255944 -0.69175303]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 251 is [True, False, False, True, False, False]
Current timestep = 252. State = [[-0.1511487  -0.28445128]]. Action = [[-0.16469595  0.01453736  0.22503549  0.57264566]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 252 is [True, False, False, True, False, False]
Scene graph at timestep 252 is [True, False, False, True, False, False]
State prediction error at timestep 252 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.15986338 -0.27893904]]. Action = [[ 0.19300508  0.12654439  0.12861365 -0.8098755 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 253 is [True, False, False, True, False, False]
Scene graph at timestep 253 is [True, False, False, True, False, False]
State prediction error at timestep 253 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 253 of 1
Current timestep = 254. State = [[-0.15622655 -0.2752075 ]]. Action = [[-0.06790945 -0.12742323 -0.24038418 -0.7150516 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 254 is [True, False, False, True, False, False]
Current timestep = 255. State = [[-0.15252316 -0.28684455]]. Action = [[ 0.15166533 -0.12854269  0.18037957 -0.74864995]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 255 is [True, False, False, True, False, False]
Current timestep = 256. State = [[-0.14096856 -0.28933352]]. Action = [[ 0.10398167  0.11906254  0.2391431  -0.7005272 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 256 is [True, False, False, True, False, False]
Current timestep = 257. State = [[-0.1405322 -0.2918206]]. Action = [[-0.21542631 -0.06995615  0.20200685  0.33319092]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 257 is [True, False, False, True, False, False]
Current timestep = 258. State = [[-0.15631498 -0.29190943]]. Action = [[-0.24196012  0.10497022 -0.1225379   0.42152452]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 258 is [True, False, False, True, False, False]
Current timestep = 259. State = [[-0.16797374 -0.28044704]]. Action = [[ 0.03424358  0.140347    0.12195438 -0.8980245 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 259 is [True, False, False, True, False, False]
Current timestep = 260. State = [[-0.16040756 -0.26427823]]. Action = [[0.22248891 0.06032938 0.04451111 0.07119083]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 260 is [True, False, False, True, False, False]
Scene graph at timestep 260 is [True, False, False, True, False, False]
State prediction error at timestep 260 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 260 of 0
Current timestep = 261. State = [[-0.14432995 -0.24767111]]. Action = [[ 0.22154456  0.08028606  0.10046899 -0.6927237 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 261 is [True, False, False, True, False, False]
Current timestep = 262. State = [[-0.13090217 -0.2530079 ]]. Action = [[ 0.01914597 -0.22906607  0.02210227  0.50789523]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 262 is [True, False, False, True, False, False]
Scene graph at timestep 262 is [True, False, False, True, False, False]
State prediction error at timestep 262 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 262 of 0
Current timestep = 263. State = [[-0.12410242 -0.27723694]]. Action = [[ 0.0913963  -0.2158619   0.13279057  0.03541386]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 263 is [True, False, False, True, False, False]
Current timestep = 264. State = [[-0.11451618 -0.29344013]]. Action = [[ 0.102763   -0.18934195 -0.12639955 -0.608683  ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 264 is [True, False, False, True, False, False]
Scene graph at timestep 264 is [True, False, False, True, False, False]
State prediction error at timestep 264 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 264 of -1
Current timestep = 265. State = [[-0.10140711 -0.29369727]]. Action = [[ 0.24010187  0.04760849 -0.15630603 -0.6016958 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 265 is [True, False, False, True, False, False]
Current timestep = 266. State = [[-0.08885117 -0.2926676 ]]. Action = [[-0.16912203  0.05310345 -0.1605654  -0.4886859 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 266 is [True, False, False, True, False, False]
Scene graph at timestep 266 is [True, False, False, True, False, False]
State prediction error at timestep 266 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 266 of 0
Current timestep = 267. State = [[-0.08153102 -0.27908036]]. Action = [[ 0.22706908  0.2003839  -0.11980963  0.83354926]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 267 is [True, False, False, True, False, False]
Current timestep = 268. State = [[-0.06417605 -0.25978392]]. Action = [[ 0.19270954  0.01528206  0.14315498 -0.6576555 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 268 is [True, False, False, True, False, False]
Current timestep = 269. State = [[-0.05033595 -0.24541335]]. Action = [[-0.11928974  0.2182771  -0.15736485 -0.93950677]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 269 is [True, False, False, True, False, False]
Current timestep = 270. State = [[-0.04395213 -0.22220495]]. Action = [[0.14227337 0.10957828 0.20269686 0.43096387]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 270 is [True, False, False, True, False, False]
Current timestep = 271. State = [[-0.03077625 -0.22194722]]. Action = [[ 0.22345191 -0.21230857 -0.02494995  0.3591174 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 271 is [False, True, False, True, False, False]
Current timestep = 272. State = [[-0.00923684 -0.24009056]]. Action = [[ 0.06370509 -0.14239442 -0.06449611 -0.25897712]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 272 is [False, True, False, True, False, False]
Current timestep = 273. State = [[ 0.00696    -0.26392642]]. Action = [[ 0.1628274  -0.22371125  0.17678335  0.738436  ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 273 is [False, True, False, True, False, False]
Current timestep = 274. State = [[ 0.02614476 -0.28422976]]. Action = [[ 0.0380522  -0.02610409  0.01351425  0.07338953]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 274 is [False, True, False, True, False, False]
Current timestep = 275. State = [[ 0.03329633 -0.28904265]]. Action = [[-0.17032336 -0.17834297  0.21718144 -0.7313629 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 275 is [False, True, False, True, False, False]
Current timestep = 276. State = [[ 0.03195109 -0.29471278]]. Action = [[-0.1040349  -0.03401899  0.17907292 -0.19720185]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 276 is [False, True, False, True, False, False]
Current timestep = 277. State = [[ 0.0268422 -0.2926491]]. Action = [[-0.17703657  0.16391611 -0.01427545 -0.877139  ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 277 is [False, True, False, True, False, False]
Current timestep = 278. State = [[ 0.02351733 -0.2821632 ]]. Action = [[ 0.0691725   0.05962268 -0.00551131 -0.73416495]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 278 is [False, True, False, True, False, False]
Current timestep = 279. State = [[ 0.01999807 -0.26597992]]. Action = [[-0.1582958   0.20197314 -0.18637195  0.93921995]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 279 is [False, True, False, True, False, False]
Scene graph at timestep 279 is [False, True, False, True, False, False]
State prediction error at timestep 279 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 279 of 1
Current timestep = 280. State = [[ 0.01130687 -0.24393262]]. Action = [[ 0.1777221   0.01456562  0.15934998 -0.64363384]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 280 is [False, True, False, True, False, False]
Current timestep = 281. State = [[ 0.01421837 -0.23044688]]. Action = [[-0.03433809  0.17249924 -0.12790516 -0.63009053]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 281 is [False, True, False, True, False, False]
Scene graph at timestep 281 is [False, True, False, True, False, False]
State prediction error at timestep 281 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 281 of 1
Current timestep = 282. State = [[ 0.00895252 -0.22308312]]. Action = [[-0.1975317  -0.09899482 -0.20987388  0.5781095 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 282 is [False, True, False, True, False, False]
Current timestep = 283. State = [[ 0.00274438 -0.22539799]]. Action = [[ 0.03850174  0.07286674  0.01032385 -0.819763  ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 283 is [False, True, False, True, False, False]
Scene graph at timestep 283 is [False, True, False, True, False, False]
State prediction error at timestep 283 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 283 of 1
Current timestep = 284. State = [[-0.00349187 -0.21171321]]. Action = [[-0.24142775  0.23590672  0.24505326 -0.5949131 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 284 is [False, True, False, True, False, False]
Scene graph at timestep 284 is [False, True, False, True, False, False]
State prediction error at timestep 284 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 284 of 1
Current timestep = 285. State = [[-0.03651682 -0.19872178]]. Action = [[-0.1321046  -0.19290687 -0.15355957  0.27959824]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 285 is [False, True, False, True, False, False]
Current timestep = 286. State = [[-0.05092352 -0.20006841]]. Action = [[-0.13042307  0.17977422 -0.05980951 -0.90005904]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 286 is [False, True, False, True, False, False]
Current timestep = 287. State = [[-0.05897537 -0.18529674]]. Action = [[ 0.05316132  0.06331399 -0.11855419  0.29509246]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 287 is [True, False, False, True, False, False]
Current timestep = 288. State = [[-0.0704691  -0.18943262]]. Action = [[-0.18610223 -0.14102222 -0.12008688  0.04235339]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 288 is [True, False, False, True, False, False]
Current timestep = 289. State = [[-0.09639367 -0.18792021]]. Action = [[-0.23439044  0.148556   -0.21185184 -0.03923637]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 289 is [True, False, False, True, False, False]
Current timestep = 290. State = [[-0.11932146 -0.17334722]]. Action = [[-0.00376855  0.08109325  0.20830613  0.03514063]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 290 is [True, False, False, True, False, False]
Scene graph at timestep 290 is [True, False, False, True, False, False]
State prediction error at timestep 290 is tensor(5.8525e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 290 of -1
Current timestep = 291. State = [[-0.13820556 -0.17516851]]. Action = [[-0.19491789 -0.14178957  0.15809897  0.55672526]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 291 is [True, False, False, True, False, False]
Current timestep = 292. State = [[-0.15606424 -0.17445232]]. Action = [[-0.07474884  0.14278686  0.18581617  0.58176804]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 292 is [True, False, False, True, False, False]
Scene graph at timestep 292 is [True, False, False, True, False, False]
State prediction error at timestep 292 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 292 of -1
Current timestep = 293. State = [[-0.16861045 -0.15842436]]. Action = [[-0.13935354  0.13847476 -0.24879777  0.5826616 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 293 is [True, False, False, True, False, False]
Current timestep = 294. State = [[-0.18532448 -0.1483098 ]]. Action = [[-0.03060429 -0.0216919  -0.17338523  0.30103314]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 294 is [True, False, False, True, False, False]
Current timestep = 295. State = [[-0.1995939 -0.1380857]]. Action = [[-0.17936158  0.17370325 -0.22589695  0.839267  ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 295 is [True, False, False, True, False, False]
Current timestep = 296. State = [[-0.22908717 -0.1394045 ]]. Action = [[-0.23828006 -0.22613278 -0.11199845  0.658375  ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 296 is [True, False, False, True, False, False]
Current timestep = 297. State = [[-0.24380125 -0.15251268]]. Action = [[ 0.20625839 -0.04516628 -0.15648659  0.11141324]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 297 is [True, False, False, True, False, False]
Current timestep = 298. State = [[-0.2384144  -0.15637256]]. Action = [[ 0.05511597 -0.02510221  0.11262432  0.77174187]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 298 is [True, False, False, True, False, False]
Current timestep = 299. State = [[-0.24335779 -0.16319294]]. Action = [[-0.18880314 -0.04596138  0.11335865  0.9587188 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 299 is [True, False, False, True, False, False]
Scene graph at timestep 299 is [True, False, False, True, False, False]
State prediction error at timestep 299 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 299 of -1
Current timestep = 300. State = [[-0.2602821  -0.16901526]]. Action = [[-0.11869036  0.00226766  0.17421922 -0.18846768]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 300 is [True, False, False, True, False, False]
Current timestep = 301. State = [[-0.2605608  -0.15873396]]. Action = [[ 0.12106854  0.21728939 -0.24473318 -0.34438664]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 301 is [True, False, False, True, False, False]
Current timestep = 302. State = [[-0.2598543  -0.14410211]]. Action = [[-0.23711525  0.10520321 -0.03516011 -0.9084014 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 302 is [True, False, False, True, False, False]
Current timestep = 303. State = [[-0.24696702 -0.13139978]]. Action = [[ 0.24214604  0.18461496  0.24331188 -0.7615023 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 303 is [True, False, False, True, False, False]
Current timestep = 304. State = [[-0.2267921  -0.12522942]]. Action = [[ 0.16066754 -0.17666784  0.22458434 -0.7279155 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 304 is [True, False, False, True, False, False]
Current timestep = 305. State = [[-0.20280609 -0.13719818]]. Action = [[ 0.22780794 -0.11507183  0.22411036  0.60422885]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 305 is [True, False, False, True, False, False]
Current timestep = 306. State = [[-0.16768152 -0.13185886]]. Action = [[ 0.24612817  0.23767862  0.21495223 -0.56876075]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 306 is [True, False, False, True, False, False]
Current timestep = 307. State = [[-0.1385631 -0.1200024]]. Action = [[ 0.15721488 -0.01640028 -0.1644164  -0.56329614]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 307 is [True, False, False, True, False, False]
Current timestep = 308. State = [[-0.12199333 -0.11510766]]. Action = [[ 0.04444343  0.00729373 -0.06602958 -0.7018555 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 308 is [True, False, False, False, True, False]
Current timestep = 309. State = [[-0.12249053 -0.12378544]]. Action = [[-0.158039   -0.1607396  -0.11869019  0.25466287]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 309 is [True, False, False, False, True, False]
Current timestep = 310. State = [[-0.11581971 -0.12533213]]. Action = [[ 0.21092331  0.17116886  0.11987612 -0.83102614]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 310 is [True, False, False, False, True, False]
Current timestep = 311. State = [[-0.10741674 -0.11920927]]. Action = [[-0.06518781 -0.01987199 -0.11839592  0.48294342]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 311 is [True, False, False, True, False, False]
Current timestep = 312. State = [[-0.11758549 -0.12636603]]. Action = [[-0.24599062 -0.09992853  0.11325157  0.70901704]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 312 is [True, False, False, False, True, False]
Current timestep = 313. State = [[-0.13754474 -0.13454567]]. Action = [[-0.15445684 -0.00859329 -0.23703097 -0.25452244]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 313 is [True, False, False, True, False, False]
Current timestep = 314. State = [[-0.14809641 -0.1487102 ]]. Action = [[ 0.0843139  -0.21861596  0.08532572 -0.6643821 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 314 is [True, False, False, True, False, False]
Scene graph at timestep 314 is [True, False, False, True, False, False]
State prediction error at timestep 314 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 314 of 1
Current timestep = 315. State = [[-0.16156958 -0.17747745]]. Action = [[-0.20736456 -0.2135914  -0.24490292 -0.79785585]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 315 is [True, False, False, True, False, False]
Current timestep = 316. State = [[-0.17729491 -0.18236557]]. Action = [[-0.11179301  0.20501554 -0.07960159 -0.7433787 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 316 is [True, False, False, True, False, False]
Current timestep = 317. State = [[-0.1820283  -0.18152018]]. Action = [[ 0.16942388 -0.18087582 -0.03939307 -0.988541  ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 317 is [True, False, False, True, False, False]
Scene graph at timestep 317 is [True, False, False, True, False, False]
State prediction error at timestep 317 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 317 of -1
Current timestep = 318. State = [[-0.19117965 -0.20361552]]. Action = [[-0.23573104 -0.22587757 -0.02904215 -0.2556286 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 318 is [True, False, False, True, False, False]
Current timestep = 319. State = [[-0.19906433 -0.23334876]]. Action = [[ 0.1285454  -0.21535182  0.00141582  0.45396984]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 319 is [True, False, False, True, False, False]
Current timestep = 320. State = [[-0.1839245  -0.25198472]]. Action = [[ 0.2229299  -0.00392762 -0.01134093  0.61293745]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 320 is [True, False, False, True, False, False]
Current timestep = 321. State = [[-0.161236  -0.2634449]]. Action = [[ 0.14788866 -0.16036399 -0.23308432  0.4893067 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 321 is [True, False, False, True, False, False]
Current timestep = 322. State = [[-0.14642754 -0.28153253]]. Action = [[-0.00071992 -0.08384943  0.24215811  0.25562906]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 322 is [True, False, False, True, False, False]
Current timestep = 323. State = [[-0.13768078 -0.2891037 ]]. Action = [[ 0.12179369 -0.01873127 -0.12465057  0.68549156]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 323 is [True, False, False, True, False, False]
Current timestep = 324. State = [[-0.13320099 -0.285921  ]]. Action = [[-0.20665146  0.18705809 -0.0071101   0.37847185]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 324 is [True, False, False, True, False, False]
Current timestep = 325. State = [[-0.13781695 -0.27882266]]. Action = [[-0.08927789 -0.20011416 -0.23953669 -0.4416408 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 325 is [True, False, False, True, False, False]
Scene graph at timestep 325 is [True, False, False, True, False, False]
State prediction error at timestep 325 is tensor(4.3228e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 325 of 0
Current timestep = 326. State = [[-0.13932186 -0.27778193]]. Action = [[ 0.11606216 -0.19319052  0.08922333  0.22145224]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 326 is [True, False, False, True, False, False]
Current timestep = 327. State = [[-0.13962741 -0.27625695]]. Action = [[-0.03286907  0.03771314 -0.0017878   0.9609997 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 327 is [True, False, False, True, False, False]
Scene graph at timestep 327 is [True, False, False, True, False, False]
State prediction error at timestep 327 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 327 of 0
Current timestep = 328. State = [[-0.14557993 -0.27140903]]. Action = [[ 0.17844784 -0.21779913 -0.13801007  0.43613338]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 328 is [True, False, False, True, False, False]
Current timestep = 329. State = [[-0.14550874 -0.25957334]]. Action = [[-0.03654425  0.21851316  0.22916466 -0.24193376]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 329 is [True, False, False, True, False, False]
Current timestep = 330. State = [[-0.13660352 -0.23300585]]. Action = [[ 0.24405748  0.14450705  0.22778967 -0.52424675]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 330 is [True, False, False, True, False, False]
Current timestep = 331. State = [[-0.11775059 -0.20572312]]. Action = [[ 0.14392668  0.22484022 -0.07151695  0.29117537]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 331 is [True, False, False, True, False, False]
Current timestep = 332. State = [[-0.11149058 -0.17770235]]. Action = [[-0.06926964  0.143453    0.09523779 -0.99338514]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 332 is [True, False, False, True, False, False]
Current timestep = 333. State = [[-0.10953744 -0.17198355]]. Action = [[ 0.09360948 -0.1460432  -0.18045393 -0.45893788]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 333 is [True, False, False, True, False, False]
Current timestep = 334. State = [[-0.09984237 -0.16570994]]. Action = [[ 0.10700142  0.17725962  0.1158264  -0.7475796 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 334 is [True, False, False, True, False, False]
Current timestep = 335. State = [[-0.08576066 -0.15902139]]. Action = [[ 0.11497605 -0.05535915 -0.00565411 -0.4931575 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 335 is [True, False, False, True, False, False]
Scene graph at timestep 335 is [True, False, False, True, False, False]
State prediction error at timestep 335 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 335 of 1
Current timestep = 336. State = [[-0.14958157  0.20799527]]. Action = [[-0.21183886 -0.18752089  0.0180282  -0.59091383]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 336 is [True, False, False, True, False, False]
Scene graph at timestep 336 is [True, False, False, False, False, True]
State prediction error at timestep 336 is tensor(0.0691, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of 1
Current timestep = 337. State = [[-0.12612493  0.22964591]]. Action = [[ 0.0567947  -0.07120606  0.09509346 -0.85409415]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 337 is [True, False, False, False, False, True]
Scene graph at timestep 337 is [True, False, False, False, False, True]
State prediction error at timestep 337 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 337 of 1
Current timestep = 338. State = [[-0.12452339  0.21317802]]. Action = [[-0.19449924 -0.22657658  0.14058411 -0.8230806 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 338 is [True, False, False, False, False, True]
Scene graph at timestep 338 is [True, False, False, False, False, True]
State prediction error at timestep 338 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 338 of 1
Current timestep = 339. State = [[-0.12907325  0.20088826]]. Action = [[ 0.07488683  0.08268988 -0.04883815 -0.0624525 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 339 is [True, False, False, False, False, True]
Current timestep = 340. State = [[-0.12336672  0.20727597]]. Action = [[ 0.19667214  0.0968999  -0.06386417 -0.8399436 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 340 is [True, False, False, False, False, True]
Current timestep = 341. State = [[-0.11189238  0.22550093]]. Action = [[ 0.0194999   0.215518   -0.15161195  0.9654105 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 341 is [True, False, False, False, False, True]
Current timestep = 342. State = [[-0.10822783  0.24094461]]. Action = [[-0.0817     -0.07658249  0.01539376 -0.24283367]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 342 is [True, False, False, False, False, True]
Current timestep = 343. State = [[-0.10323767  0.23480238]]. Action = [[ 0.15650406 -0.07822457 -0.24524195  0.26965177]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 343 is [True, False, False, False, False, True]
Current timestep = 344. State = [[-0.08601985  0.23123479]]. Action = [[ 0.24841511  0.04294491 -0.04501194  0.21937692]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 344 is [True, False, False, False, False, True]
Scene graph at timestep 344 is [True, False, False, False, False, True]
State prediction error at timestep 344 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 344 of 1
Current timestep = 345. State = [[-0.05092675  0.22842856]]. Action = [[-0.0249259  -0.1427886  -0.22045699  0.85784554]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 345 is [True, False, False, False, False, True]
Current timestep = 346. State = [[-0.04491213  0.20810962]]. Action = [[ 0.02121988 -0.20623216  0.15507695  0.2869637 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 346 is [True, False, False, False, False, True]
Current timestep = 347. State = [[-0.03680735  0.18492168]]. Action = [[ 0.09569991 -0.15797047  0.19510943  0.97828317]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 347 is [False, True, False, False, False, True]
Current timestep = 348. State = [[-0.03530497  0.16933477]]. Action = [[-0.16773297 -0.06214903  0.11643437  0.20836306]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 348 is [False, True, False, False, False, True]
Current timestep = 349. State = [[-0.04420473  0.15477444]]. Action = [[-0.140131   -0.13720824  0.22833115  0.65954566]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 349 is [False, True, False, False, False, True]
Current timestep = 350. State = [[-0.05287858  0.13511074]]. Action = [[ 0.02756229 -0.15059234 -0.03156346 -0.21987611]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 350 is [False, True, False, False, False, True]
Current timestep = 351. State = [[-0.04810037  0.1112323 ]]. Action = [[ 0.14076743 -0.19099306 -0.09971482  0.7089399 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 351 is [True, False, False, False, False, True]
Scene graph at timestep 351 is [False, True, False, False, True, False]
State prediction error at timestep 351 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 351 of 1
Current timestep = 352. State = [[-0.04311611  0.10062442]]. Action = [[ 0.06449258  0.19436184 -0.01690859 -0.2712    ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 352 is [False, True, False, False, True, False]
Current timestep = 353. State = [[-0.04184063  0.09840571]]. Action = [[-0.03164674 -0.20076843 -0.1687681   0.91672444]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 353 is [False, True, False, False, True, False]
Scene graph at timestep 353 is [False, True, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of 1
Current timestep = 354. State = [[-0.19998771 -0.22072008]]. Action = [[ 0.22614002 -0.11703949  0.18483177  0.562917  ]]. Reward = [100.]
Curr episode timestep = 17
Scene graph at timestep 354 is [False, True, False, False, True, False]
Current timestep = 355. State = [[-0.1982941  -0.25376263]]. Action = [[-0.21202856 -0.09406082  0.10437125 -0.6930147 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 355 is [True, False, False, True, False, False]
Current timestep = 356. State = [[-0.20150325 -0.25315678]]. Action = [[ 0.13787311  0.1695537   0.05657297 -0.9485015 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 356 is [True, False, False, True, False, False]
Scene graph at timestep 356 is [True, False, False, True, False, False]
State prediction error at timestep 356 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 356 of -1
Current timestep = 357. State = [[-0.19898587 -0.25075698]]. Action = [[-0.00553173 -0.13605581  0.05478188 -0.370098  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 357 is [True, False, False, True, False, False]
Scene graph at timestep 357 is [True, False, False, True, False, False]
State prediction error at timestep 357 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 357 of -1
Current timestep = 358. State = [[-0.20017132 -0.25260156]]. Action = [[-0.0804639   0.11710149  0.14056921 -0.9492026 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 358 is [True, False, False, True, False, False]
Current timestep = 359. State = [[-0.20237514 -0.25761384]]. Action = [[-0.01293491 -0.14540668  0.24643904 -0.24863017]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 359 is [True, False, False, True, False, False]
Current timestep = 360. State = [[-0.20608547 -0.2724748 ]]. Action = [[ 0.03826752 -0.19064327  0.22232366  0.2960658 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 360 is [True, False, False, True, False, False]
Current timestep = 361. State = [[-0.21477859 -0.29794464]]. Action = [[-0.12499791 -0.16976652  0.06092796  0.34684038]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 361 is [True, False, False, True, False, False]
Current timestep = 362. State = [[-0.21002617 -0.30948165]]. Action = [[0.23639464 0.03308582 0.18427685 0.32889676]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 362 is [True, False, False, True, False, False]
Scene graph at timestep 362 is [True, False, False, True, False, False]
State prediction error at timestep 362 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 362 of 1
Current timestep = 363. State = [[-0.19458254 -0.31330138]]. Action = [[-0.01927339 -0.10475343 -0.08663665 -0.37915695]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 363 is [True, False, False, True, False, False]
Scene graph at timestep 363 is [True, False, False, True, False, False]
State prediction error at timestep 363 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 363 of 0
Current timestep = 364. State = [[-0.19272746 -0.30660108]]. Action = [[-0.03514132  0.14603361 -0.01506881  0.79552746]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 364 is [True, False, False, True, False, False]
Scene graph at timestep 364 is [True, False, False, True, False, False]
State prediction error at timestep 364 is tensor(5.0730e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 364 of 1
Current timestep = 365. State = [[-0.19516127 -0.30106205]]. Action = [[-0.13218884 -0.01924297  0.05763346 -0.50825393]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 365 is [True, False, False, True, False, False]
Current timestep = 366. State = [[-0.20919473 -0.2962432 ]]. Action = [[-0.18405001  0.12149394  0.04422456 -0.78395647]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 366 is [True, False, False, True, False, False]
Current timestep = 367. State = [[-0.23056592 -0.28316587]]. Action = [[-0.13010785  0.08944398  0.21065515  0.21905065]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 367 is [True, False, False, True, False, False]
Current timestep = 368. State = [[-0.24752752 -0.28320408]]. Action = [[-0.07565886 -0.12435886 -0.2082197  -0.3979438 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 368 is [True, False, False, True, False, False]
Current timestep = 369. State = [[-0.24493961 -0.27567846]]. Action = [[ 0.243536    0.20704624 -0.15673053  0.2732488 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 369 is [True, False, False, True, False, False]
Scene graph at timestep 369 is [True, False, False, True, False, False]
State prediction error at timestep 369 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 369 of -1
Current timestep = 370. State = [[-0.22769733 -0.25782228]]. Action = [[ 0.1846413  -0.03569633 -0.19696885 -0.93903226]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 370 is [True, False, False, True, False, False]
Scene graph at timestep 370 is [True, False, False, True, False, False]
State prediction error at timestep 370 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 370 of 1
Current timestep = 371. State = [[-0.20788404 -0.27077648]]. Action = [[ 0.06741056 -0.21732913  0.01279983  0.18018866]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 371 is [True, False, False, True, False, False]
Current timestep = 372. State = [[-0.19310828 -0.28180814]]. Action = [[ 0.21267444  0.00954628 -0.17631322  0.14920878]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 372 is [True, False, False, True, False, False]
Current timestep = 373. State = [[-0.16675332 -0.2854135 ]]. Action = [[0.19071382 0.00285903 0.1001803  0.29305792]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 373 is [True, False, False, True, False, False]
Current timestep = 374. State = [[-0.15717764 -0.27812108]]. Action = [[-0.2338617   0.19803745  0.08439434  0.36708474]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 374 is [True, False, False, True, False, False]
Current timestep = 375. State = [[-0.16533728 -0.27803126]]. Action = [[-0.0831157  -0.15790336 -0.10258681 -0.8953219 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 375 is [True, False, False, True, False, False]
Current timestep = 376. State = [[-0.16968437 -0.2837711 ]]. Action = [[ 0.01958656  0.0373086  -0.11179891 -0.9623962 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 376 is [True, False, False, True, False, False]
Scene graph at timestep 376 is [True, False, False, True, False, False]
State prediction error at timestep 376 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 376 of 1
Current timestep = 377. State = [[-0.16971962 -0.28349784]]. Action = [[-0.1497732  -0.17488368  0.0208542   0.25628912]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 377 is [True, False, False, True, False, False]
Scene graph at timestep 377 is [True, False, False, True, False, False]
State prediction error at timestep 377 is tensor(4.4937e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of 1
Current timestep = 378. State = [[-0.16971962 -0.28349784]]. Action = [[ 0.07555085 -0.15579443  0.22597444  0.42310536]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 378 is [True, False, False, True, False, False]
Scene graph at timestep 378 is [True, False, False, True, False, False]
State prediction error at timestep 378 is tensor(9.6002e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 378 of 1
Current timestep = 379. State = [[-0.16613974 -0.27450627]]. Action = [[ 0.13033128  0.12116554 -0.16238536  0.9882686 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 379 is [True, False, False, True, False, False]
Current timestep = 380. State = [[-0.16948538 -0.2546357 ]]. Action = [[-0.21495552  0.20743498 -0.16209553  0.22007918]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 380 is [True, False, False, True, False, False]
Current timestep = 381. State = [[-0.17390057 -0.22801223]]. Action = [[0.17206171 0.15655324 0.2362991  0.17995536]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 381 is [True, False, False, True, False, False]
Current timestep = 382. State = [[-0.16162409 -0.20516184]]. Action = [[ 0.17903191  0.0940645  -0.01238173 -0.36646914]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 382 is [True, False, False, True, False, False]
Current timestep = 383. State = [[-0.15300393 -0.1856082 ]]. Action = [[-0.07568784  0.19176614 -0.0089457   0.04404354]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 383 is [True, False, False, True, False, False]
Scene graph at timestep 383 is [True, False, False, True, False, False]
State prediction error at timestep 383 is tensor(4.5896e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 383 of 1
Current timestep = 384. State = [[-0.15309002 -0.17318827]]. Action = [[-0.01402105 -0.11455804  0.02027893  0.4119743 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 384 is [True, False, False, True, False, False]
Current timestep = 385. State = [[-0.15612234 -0.19217345]]. Action = [[ 0.00186601 -0.2312299  -0.05020759 -0.6005242 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 385 is [True, False, False, True, False, False]
Current timestep = 386. State = [[-0.15457201 -0.19743304]]. Action = [[0.08688176 0.1771627  0.1274969  0.6739421 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 386 is [True, False, False, True, False, False]
Scene graph at timestep 386 is [True, False, False, True, False, False]
State prediction error at timestep 386 is tensor(5.2795e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of 0
Current timestep = 387. State = [[-0.14856233 -0.19152805]]. Action = [[ 0.04058006 -0.05003756  0.16457635 -0.9213976 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 387 is [True, False, False, True, False, False]
Scene graph at timestep 387 is [True, False, False, True, False, False]
State prediction error at timestep 387 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 387 of 0
Current timestep = 388. State = [[-0.14275618 -0.18419738]]. Action = [[0.07463062 0.15552998 0.02448165 0.8020072 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 388 is [True, False, False, True, False, False]
Scene graph at timestep 388 is [True, False, False, True, False, False]
State prediction error at timestep 388 is tensor(7.9936e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 388 of 1
Current timestep = 389. State = [[-0.12934175 -0.18381046]]. Action = [[ 0.1370576  -0.20441328  0.06964621 -0.33041388]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 389 is [True, False, False, True, False, False]
Current timestep = 390. State = [[-0.11195054 -0.20595321]]. Action = [[ 0.09302431 -0.19861445 -0.24216731 -0.47094476]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 390 is [True, False, False, True, False, False]
Scene graph at timestep 390 is [True, False, False, True, False, False]
State prediction error at timestep 390 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 390 of 0
Current timestep = 391. State = [[-0.10151891 -0.23113443]]. Action = [[-0.02459896 -0.09380241 -0.22484206  0.9018123 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 391 is [True, False, False, True, False, False]
Scene graph at timestep 391 is [True, False, False, True, False, False]
State prediction error at timestep 391 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 391 of -1
Current timestep = 392. State = [[-0.10138017 -0.22838435]]. Action = [[-0.01468086  0.21038085 -0.033948   -0.42619163]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 392 is [True, False, False, True, False, False]
Current timestep = 393. State = [[-0.10670001 -0.2297604 ]]. Action = [[-0.16257702 -0.16749907 -0.06612024  0.7809551 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 393 is [True, False, False, True, False, False]
Current timestep = 394. State = [[-0.11818632 -0.22665001]]. Action = [[-0.1702286   0.21298084  0.17476541  0.0193367 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 394 is [True, False, False, True, False, False]
Scene graph at timestep 394 is [True, False, False, True, False, False]
State prediction error at timestep 394 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 394 of -1
Current timestep = 395. State = [[-0.13747722 -0.22388148]]. Action = [[-0.05069789 -0.16841705  0.06838059  0.09236455]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 395 is [True, False, False, True, False, False]
Current timestep = 396. State = [[-0.13989036 -0.21994442]]. Action = [[ 0.02678943  0.22128141 -0.03297383  0.5378301 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 396 is [True, False, False, True, False, False]
Scene graph at timestep 396 is [True, False, False, True, False, False]
State prediction error at timestep 396 is tensor(3.3948e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 396 of 0
Current timestep = 397. State = [[-0.13885239 -0.20610873]]. Action = [[ 0.03494123  0.00855079 -0.24779198 -0.6710945 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 397 is [True, False, False, True, False, False]
Current timestep = 398. State = [[-0.12923573 -0.19449116]]. Action = [[ 0.23558846  0.13165477 -0.09842959 -0.56905454]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 398 is [True, False, False, True, False, False]
Current timestep = 399. State = [[-0.12161235 -0.18746032]]. Action = [[-0.01311408 -0.06811179 -0.17632157  0.08845091]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 399 is [True, False, False, True, False, False]
Scene graph at timestep 399 is [True, False, False, True, False, False]
State prediction error at timestep 399 is tensor(7.2811e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 399 of 1
Current timestep = 400. State = [[-0.11956672 -0.19260457]]. Action = [[-0.0099389  -0.07812592  0.02391648 -0.7674467 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 400 is [True, False, False, True, False, False]
Current timestep = 401. State = [[-0.12586103 -0.21072306]]. Action = [[-0.17696555 -0.20570642 -0.13323054 -0.90966284]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 401 is [True, False, False, True, False, False]
Current timestep = 402. State = [[-0.139325   -0.21415803]]. Action = [[-0.20821767  0.23582658 -0.19382004  0.517429  ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 402 is [True, False, False, True, False, False]
Scene graph at timestep 402 is [True, False, False, True, False, False]
State prediction error at timestep 402 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 402 of -1
Current timestep = 403. State = [[-0.15821943 -0.20528364]]. Action = [[-0.07357574 -0.01996878 -0.09085658  0.85853124]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 403 is [True, False, False, True, False, False]
Current timestep = 404. State = [[-0.17149135 -0.198794  ]]. Action = [[-0.15621562  0.1285243  -0.04383354 -0.08210975]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 404 is [True, False, False, True, False, False]
Current timestep = 405. State = [[-0.18182142 -0.1877096 ]]. Action = [[ 0.09842095  0.01585138  0.07460716 -0.82283497]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 405 is [True, False, False, True, False, False]
Current timestep = 406. State = [[-0.18840417 -0.17219546]]. Action = [[-0.19746181  0.22705346  0.22615486  0.42109585]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 406 is [True, False, False, True, False, False]
Current timestep = 407. State = [[-0.21034044 -0.16327251]]. Action = [[-0.23252492 -0.10049398 -0.11572377  0.68664193]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 407 is [True, False, False, True, False, False]
Scene graph at timestep 407 is [True, False, False, True, False, False]
State prediction error at timestep 407 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 407 of -1
Current timestep = 408. State = [[-0.23206535 -0.17719324]]. Action = [[ 0.19623655 -0.22310388 -0.18786985 -0.67559165]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 408 is [True, False, False, True, False, False]
Current timestep = 409. State = [[-0.21550748 -0.18188336]]. Action = [[ 0.22441542  0.09557769 -0.20028673 -0.25956064]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 409 is [True, False, False, True, False, False]
Current timestep = 410. State = [[-0.20759913 -0.18927087]]. Action = [[-0.19536214 -0.13740425 -0.1856558   0.9728867 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 410 is [True, False, False, True, False, False]
Current timestep = 411. State = [[-0.20683165 -0.20131557]]. Action = [[ 0.16320401 -0.1009395   0.07254362  0.08559835]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 411 is [True, False, False, True, False, False]
Current timestep = 412. State = [[-0.20394671 -0.19710681]]. Action = [[-0.07032606  0.24806401 -0.11091301  0.04173422]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 412 is [True, False, False, True, False, False]
Current timestep = 413. State = [[-0.1953963  -0.17754988]]. Action = [[ 0.21654847  0.08305344 -0.03088279  0.2850263 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 413 is [True, False, False, True, False, False]
Current timestep = 414. State = [[-0.16985586 -0.15296179]]. Action = [[ 0.24522856  0.24736011  0.19955456 -0.6200344 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 414 is [True, False, False, True, False, False]
Current timestep = 415. State = [[-0.15405747 -0.12646806]]. Action = [[-0.06356472  0.14141428  0.14560407 -0.83370584]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 415 is [True, False, False, True, False, False]
Current timestep = 416. State = [[-0.14839603 -0.10183078]]. Action = [[0.1235528  0.1903699  0.14191389 0.45297205]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 416 is [True, False, False, True, False, False]
Current timestep = 417. State = [[-0.14234653 -0.09751347]]. Action = [[-0.05955908 -0.2047477   0.21655142  0.38129544]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 417 is [True, False, False, False, True, False]
Current timestep = 418. State = [[-0.14905176 -0.09224728]]. Action = [[-0.2126075   0.22440267  0.00675198  0.13416147]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 418 is [True, False, False, False, True, False]
Current timestep = 419. State = [[-0.15132229 -0.09392104]]. Action = [[ 0.19534165 -0.23589931 -0.08211356  0.9245312 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 419 is [True, False, False, False, True, False]
Current timestep = 420. State = [[-0.15105578 -0.0937352 ]]. Action = [[-0.15547681  0.20211458 -0.05166019 -0.8967282 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 420 is [True, False, False, False, True, False]
Current timestep = 421. State = [[-0.14960812 -0.09605359]]. Action = [[ 0.17120326 -0.21597756  0.18872619  0.04524827]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 421 is [True, False, False, False, True, False]
Current timestep = 422. State = [[-0.13603921 -0.10463107]]. Action = [[ 0.23914686  0.02112928 -0.2353644   0.85495615]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 422 is [True, False, False, False, True, False]
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 422 of 1
Current timestep = 423. State = [[-0.1044892  -0.09563201]]. Action = [[ 0.19193807  0.16480988  0.22317308 -0.9132345 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 423 is [True, False, False, False, True, False]
Scene graph at timestep 423 is [True, False, False, False, True, False]
State prediction error at timestep 423 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 423 of 1
Current timestep = 424. State = [[-0.08112862 -0.08902793]]. Action = [[ 0.08017093 -0.06936029  0.01709574  0.96506774]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 424 is [True, False, False, False, True, False]
Current timestep = 425. State = [[-0.06625595 -0.08445132]]. Action = [[0.21728206 0.12237662 0.24433878 0.48020518]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 425 is [True, False, False, False, True, False]
Current timestep = 426. State = [[-0.04517642 -0.06987093]]. Action = [[-0.03817818  0.15635282  0.19736952 -0.3929574 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 426 is [True, False, False, False, True, False]
Current timestep = 427. State = [[-0.04607635 -0.05596877]]. Action = [[-0.09444878  0.04355294  0.05226493 -0.64990604]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 427 is [False, True, False, False, True, False]
Current timestep = 428. State = [[-0.04432712 -0.04406023]]. Action = [[0.13057238 0.10103571 0.1479573  0.5957658 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 428 is [False, True, False, False, True, False]
Scene graph at timestep 428 is [False, True, False, False, True, False]
State prediction error at timestep 428 is tensor(1.7411e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 428 of 1
Current timestep = 429. State = [[-0.15863064  0.0653688 ]]. Action = [[ 0.14954802  0.00572771 -0.0471351   0.18312359]]. Reward = [100.]
Curr episode timestep = 74
Scene graph at timestep 429 is [False, True, False, False, True, False]
Current timestep = 430. State = [[-0.13629964  0.06925989]]. Action = [[ 0.14327747 -0.09790997 -0.11926828 -0.34258556]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 430 is [True, False, False, False, True, False]
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 430 of 1
Current timestep = 431. State = [[-0.11910641  0.06515272]]. Action = [[ 0.07082376  0.02268007  0.13396335 -0.92338425]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 431 is [True, False, False, False, True, False]
Current timestep = 432. State = [[-0.11863077  0.07903761]]. Action = [[-0.10978574  0.21878088  0.06633863 -0.0763914 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 432 is [True, False, False, False, True, False]
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 432 of -1
Current timestep = 433. State = [[-0.11384736  0.10167675]]. Action = [[ 0.24798149  0.12388733  0.00725868 -0.92828953]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 433 is [True, False, False, False, True, False]
Current timestep = 434. State = [[-0.08821722  0.12288436]]. Action = [[0.20336425 0.21511096 0.19245827 0.52044296]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 434 is [True, False, False, False, True, False]
Scene graph at timestep 434 is [True, False, False, False, True, False]
State prediction error at timestep 434 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 434 of 0
Current timestep = 435. State = [[-0.05259334  0.14309669]]. Action = [[ 0.17617002 -0.0506946   0.11443031  0.2895372 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 435 is [True, False, False, False, True, False]
Current timestep = 436. State = [[-0.03560501  0.14808935]]. Action = [[ 0.05445513  0.10254937  0.19865718 -0.50479573]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 436 is [True, False, False, False, False, True]
Current timestep = 437. State = [[-0.02849481  0.14472204]]. Action = [[-0.1723001  -0.16999032  0.24888363  0.12326074]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 437 is [False, True, False, False, False, True]
Scene graph at timestep 437 is [False, True, False, False, False, True]
State prediction error at timestep 437 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 437 of 1
Current timestep = 438. State = [[-0.03220801  0.12214649]]. Action = [[-0.1371806  -0.2313833   0.24181652  0.5392133 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 438 is [False, True, False, False, False, True]
Current timestep = 439. State = [[-0.03783625  0.0966729 ]]. Action = [[-0.0624913  -0.1754402   0.09733722 -0.27864385]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 439 is [False, True, False, False, True, False]
Current timestep = 440. State = [[-0.04061416  0.06865641]]. Action = [[ 0.10396063 -0.21661167  0.11536056 -0.5975133 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 440 is [False, True, False, False, True, False]
Current timestep = 441. State = [[-0.17467064  0.04014859]]. Action = [[ 0.23374513  0.07375014  0.07203814 -0.0770241 ]]. Reward = [100.]
Curr episode timestep = 11
Scene graph at timestep 441 is [False, True, False, False, True, False]
Current timestep = 442. State = [[-0.15898569  0.03670083]]. Action = [[-0.08125427 -0.19458285 -0.187579   -0.6880405 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 442 is [True, False, False, False, True, False]
Current timestep = 443. State = [[-0.15923522  0.04096615]]. Action = [[0.09219456 0.24824479 0.18648836 0.77816   ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 443 is [True, False, False, False, True, False]
Current timestep = 444. State = [[-0.15395775  0.05684217]]. Action = [[0.13808906 0.08266687 0.10899138 0.9545293 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 444 is [True, False, False, False, True, False]
Current timestep = 445. State = [[-0.15004072  0.07393751]]. Action = [[-0.23307419  0.13556194  0.22499698  0.11104977]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 445 is [True, False, False, False, True, False]
Scene graph at timestep 445 is [True, False, False, False, True, False]
State prediction error at timestep 445 is tensor(2.7594e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 445 of -1
Current timestep = 446. State = [[-0.15279835  0.10383864]]. Action = [[ 0.22405109  0.2376175  -0.09698261  0.90519965]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 446 is [True, False, False, False, True, False]
Current timestep = 447. State = [[-0.14258535  0.11225718]]. Action = [[-0.05523692 -0.11858892 -0.00184821 -0.63792247]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 447 is [True, False, False, False, True, False]
Current timestep = 448. State = [[-0.14284012  0.10809914]]. Action = [[-0.09221712 -0.03521684 -0.08019367  0.99176145]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 448 is [True, False, False, False, True, False]
Current timestep = 449. State = [[-0.13646242  0.09587044]]. Action = [[ 0.22586787 -0.14502482 -0.21964405  0.6697656 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 449 is [True, False, False, False, True, False]
Scene graph at timestep 449 is [True, False, False, False, True, False]
State prediction error at timestep 449 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 449 of 1
Current timestep = 450. State = [[-0.12827179  0.08724124]]. Action = [[ 0.0500111   0.1030567  -0.08769466  0.7194109 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 450 is [True, False, False, False, True, False]
Current timestep = 451. State = [[-0.12667742  0.08896012]]. Action = [[-0.21796192 -0.08541334 -0.14526026 -0.91023284]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 451 is [True, False, False, False, True, False]
Scene graph at timestep 451 is [True, False, False, False, True, False]
State prediction error at timestep 451 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 451 of 1
Current timestep = 452. State = [[-0.13166697  0.07611833]]. Action = [[-0.02884899 -0.17793757  0.02520496  0.39631593]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 452 is [True, False, False, False, True, False]
Current timestep = 453. State = [[-0.14283153  0.06074666]]. Action = [[-0.19178644 -0.06106196 -0.02693456 -0.82745624]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 453 is [True, False, False, False, True, False]
Current timestep = 454. State = [[-0.15088196  0.05139669]]. Action = [[ 0.19928735 -0.04433572 -0.03785788 -0.44851232]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 454 is [True, False, False, False, True, False]
Current timestep = 455. State = [[-0.13954414  0.04160241]]. Action = [[ 0.20489812 -0.10130274  0.07147354  0.4311874 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 455 is [True, False, False, False, True, False]
Scene graph at timestep 455 is [True, False, False, False, True, False]
State prediction error at timestep 455 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 455 of 1
Current timestep = 456. State = [[-0.11936864  0.0378007 ]]. Action = [[ 0.16105235  0.11753777  0.20064107 -0.04165012]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 456 is [True, False, False, False, True, False]
Current timestep = 457. State = [[-0.09772087  0.03098815]]. Action = [[ 0.16139638 -0.22979672  0.04444546 -0.741417  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 457 is [True, False, False, False, True, False]
Current timestep = 458. State = [[-0.08406227  0.01232661]]. Action = [[-0.06185108 -0.09660825  0.11982709  0.75544715]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 458 is [True, False, False, False, True, False]
Scene graph at timestep 458 is [True, False, False, False, True, False]
State prediction error at timestep 458 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 458 of 1
Current timestep = 459. State = [[-0.0789198   0.00319254]]. Action = [[ 0.1259216   0.0544627  -0.20813152  0.27316105]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 459 is [True, False, False, False, True, False]
Current timestep = 460. State = [[-0.07362159  0.0025971 ]]. Action = [[-0.10994646 -0.05587426  0.09477189 -0.23285604]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 460 is [True, False, False, False, True, False]
Scene graph at timestep 460 is [True, False, False, False, True, False]
State prediction error at timestep 460 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of 1
Current timestep = 461. State = [[-0.08252015 -0.01265327]]. Action = [[-0.24281347 -0.18978104  0.20600075 -0.7438417 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 461 is [True, False, False, False, True, False]
Current timestep = 462. State = [[-0.09970185 -0.01324201]]. Action = [[-0.15548472  0.20974359  0.16529709 -0.9279335 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 462 is [True, False, False, False, True, False]
Scene graph at timestep 462 is [True, False, False, False, True, False]
State prediction error at timestep 462 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 462 of -1
Current timestep = 463. State = [[-0.12443245 -0.01074619]]. Action = [[-0.18525426 -0.19075939  0.16174656  0.0417726 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 463 is [True, False, False, False, True, False]
Current timestep = 464. State = [[-0.13454066 -0.02138959]]. Action = [[ 0.18384653  0.00881353 -0.12015179 -0.722015  ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 464 is [True, False, False, False, True, False]
Scene graph at timestep 464 is [True, False, False, False, True, False]
State prediction error at timestep 464 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 464 of -1
Current timestep = 465. State = [[-0.13322084 -0.01037521]]. Action = [[-0.01828381  0.2178948  -0.12723646 -0.11405069]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 465 is [True, False, False, False, True, False]
Current timestep = 466. State = [[-0.13524206 -0.00042579]]. Action = [[-0.03985007 -0.02169402  0.22612125 -0.849668  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 466 is [True, False, False, False, True, False]
Scene graph at timestep 466 is [True, False, False, False, True, False]
State prediction error at timestep 466 is tensor(5.8339e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 466 of -1
Current timestep = 467. State = [[-0.14145479 -0.00746907]]. Action = [[-0.18528649 -0.14788733  0.05226162  0.6799798 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 467 is [True, False, False, False, True, False]
Scene graph at timestep 467 is [True, False, False, False, True, False]
State prediction error at timestep 467 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 467 of -1
Current timestep = 468. State = [[-0.15064697 -0.01337325]]. Action = [[0.10112917 0.09501773 0.07412112 0.4093554 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 468 is [True, False, False, False, True, False]
Current timestep = 469. State = [[-0.14266887 -0.00093691]]. Action = [[0.20938438 0.13825655 0.07094687 0.74683297]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 469 is [True, False, False, False, True, False]
Current timestep = 470. State = [[-0.13526078 -0.00358834]]. Action = [[-0.16997574 -0.23442674  0.04731661  0.5993812 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 470 is [True, False, False, False, True, False]
Current timestep = 471. State = [[-0.14702804 -0.01855825]]. Action = [[-0.23714426 -0.08722143 -0.12977202  0.01361156]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 471 is [True, False, False, False, True, False]
Current timestep = 472. State = [[-0.1612126  -0.01800872]]. Action = [[ 0.06939101  0.17019439 -0.21710475 -0.811823  ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 472 is [True, False, False, False, True, False]
Current timestep = 473. State = [[-0.16460715 -0.0007342 ]]. Action = [[-0.05614382  0.14625901  0.01880616  0.23563683]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 473 is [True, False, False, False, True, False]
Current timestep = 474. State = [[-0.1703821   0.01899725]]. Action = [[-0.06771484  0.1146335   0.23699912  0.83504295]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 474 is [True, False, False, False, True, False]
Current timestep = 475. State = [[-0.18323527  0.02675288]]. Action = [[-0.21744226 -0.06655842  0.12713033  0.53033984]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 475 is [True, False, False, False, True, False]
Current timestep = 476. State = [[-0.20433621  0.02128012]]. Action = [[-0.08335665 -0.03873348 -0.11830291  0.82629144]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 476 is [True, False, False, False, True, False]
Scene graph at timestep 476 is [True, False, False, False, True, False]
State prediction error at timestep 476 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 476 of -1
Current timestep = 477. State = [[-0.21354099  0.01537384]]. Action = [[ 0.18614012 -0.04888789  0.13766468 -0.06448632]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 477 is [True, False, False, False, True, False]
Current timestep = 478. State = [[-0.2090049   0.02391694]]. Action = [[0.02498823 0.20627168 0.10056719 0.36217272]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 478 is [True, False, False, False, True, False]
Current timestep = 479. State = [[-0.2088883   0.04105794]]. Action = [[ 0.03053674  0.13107097  0.02500749 -0.5044807 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 479 is [True, False, False, False, True, False]
Current timestep = 480. State = [[-0.20196564  0.04179323]]. Action = [[ 0.09039786 -0.20344357  0.10458684  0.7610481 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 480 is [True, False, False, False, True, False]
Current timestep = 481. State = [[-0.20008875  0.043314  ]]. Action = [[-0.17833228  0.17933965 -0.07503538 -0.6796222 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 481 is [True, False, False, False, True, False]
Scene graph at timestep 481 is [True, False, False, False, True, False]
State prediction error at timestep 481 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 481 of 1
Current timestep = 482. State = [[-0.20848939  0.06324235]]. Action = [[-0.06195787  0.13993081  0.16148755 -0.33980227]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 482 is [True, False, False, False, True, False]
Current timestep = 483. State = [[-0.21725303  0.06508712]]. Action = [[-0.11203334 -0.1515012   0.11465591  0.09344077]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 483 is [True, False, False, False, True, False]
Current timestep = 484. State = [[-0.22323179  0.06415712]]. Action = [[0.09045181 0.1378352  0.07849827 0.69581914]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 484 is [True, False, False, False, True, False]
Current timestep = 485. State = [[-0.21912032  0.06165756]]. Action = [[ 0.11912093 -0.160848    0.12217787  0.71516633]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 485 is [True, False, False, False, True, False]
Current timestep = 486. State = [[-0.21441069  0.06654403]]. Action = [[0.00822306 0.22148994 0.13975257 0.8781401 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 486 is [True, False, False, False, True, False]
Current timestep = 487. State = [[-0.2024018   0.06347348]]. Action = [[ 0.22931981 -0.23829655 -0.15157184  0.9781647 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 487 is [True, False, False, False, True, False]
Current timestep = 488. State = [[-0.18933256  0.05309485]]. Action = [[-0.15993711 -0.00090484  0.15872312  0.34104896]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 488 is [True, False, False, False, True, False]
Current timestep = 489. State = [[-0.18274961  0.0389172 ]]. Action = [[ 0.24251622 -0.18486571 -0.01353629  0.3839233 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 489 is [True, False, False, False, True, False]
Current timestep = 490. State = [[-0.16341996  0.01067891]]. Action = [[ 0.21399432 -0.23793052  0.17955348 -0.9434803 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 490 is [True, False, False, False, True, False]
Current timestep = 491. State = [[-0.13881513 -0.01555626]]. Action = [[ 0.08218783 -0.10166833 -0.0444587  -0.15295243]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 491 is [True, False, False, False, True, False]
Current timestep = 492. State = [[-0.12040141 -0.02861892]]. Action = [[ 0.20618734 -0.04757136 -0.06398427 -0.826079  ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 492 is [True, False, False, False, True, False]
Current timestep = 493. State = [[-0.11023839 -0.03843112]]. Action = [[-0.24166676 -0.08477914  0.15908366  0.9140279 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 493 is [True, False, False, False, True, False]
Current timestep = 494. State = [[-0.11338846 -0.04991781]]. Action = [[ 0.04603487 -0.06624417  0.22595477 -0.8029217 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 494 is [True, False, False, False, True, False]
Current timestep = 495. State = [[-0.11195707 -0.06624149]]. Action = [[ 0.04510516 -0.1836404  -0.00202185  0.71270025]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 495 is [True, False, False, False, True, False]
Scene graph at timestep 495 is [True, False, False, False, True, False]
State prediction error at timestep 495 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 495 of 1
Current timestep = 496. State = [[-0.11258743 -0.09303848]]. Action = [[-0.07976633 -0.147734    0.14029849 -0.23919463]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 496 is [True, False, False, False, True, False]
Scene graph at timestep 496 is [True, False, False, False, True, False]
State prediction error at timestep 496 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 496 of -1
Current timestep = 497. State = [[-0.10984447 -0.09628986]]. Action = [[ 0.2130251   0.17798093 -0.12425056  0.04434383]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 497 is [True, False, False, False, True, False]
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(5.7015e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of 1
Current timestep = 498. State = [[-0.10436109 -0.09821755]]. Action = [[-0.1551152  -0.21785355  0.03772625 -0.82243264]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 498 is [True, False, False, False, True, False]
Current timestep = 499. State = [[-0.10473192 -0.12324518]]. Action = [[ 0.08224571 -0.22133282 -0.17350928  0.4616835 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 499 is [True, False, False, False, True, False]
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of -1
Current timestep = 500. State = [[-0.10294075 -0.15571365]]. Action = [[ 0.05945098 -0.17214952  0.24681965 -0.7027404 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 500 is [True, False, False, False, True, False]
Current timestep = 501. State = [[-0.10433476 -0.16096084]]. Action = [[-0.1525685   0.15713573 -0.08126539  0.7299086 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 501 is [True, False, False, True, False, False]
Scene graph at timestep 501 is [True, False, False, True, False, False]
State prediction error at timestep 501 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[-0.10755092 -0.15953685]]. Action = [[-0.04658176 -0.07160956  0.20980275  0.4251337 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 502 is [True, False, False, True, False, False]
Current timestep = 503. State = [[-0.10701329 -0.17345408]]. Action = [[ 0.19750395 -0.22184227  0.23112208 -0.8942767 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 503 is [True, False, False, True, False, False]
Current timestep = 504. State = [[-0.10061007 -0.20091464]]. Action = [[ 0.08643168 -0.23644365  0.09875363  0.44877434]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 504 is [True, False, False, True, False, False]
Scene graph at timestep 504 is [True, False, False, True, False, False]
State prediction error at timestep 504 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 504 of -1
Current timestep = 505. State = [[-0.08212929 -0.21348692]]. Action = [[ 0.18296474  0.19079995 -0.19690779  0.44991255]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 505 is [True, False, False, True, False, False]
Current timestep = 506. State = [[-0.06084124 -0.19307433]]. Action = [[0.08574027 0.19948834 0.08631164 0.11982572]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 506 is [True, False, False, True, False, False]
Current timestep = 507. State = [[-0.04598365 -0.17586672]]. Action = [[0.09870583 0.03398478 0.14354873 0.0168848 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 507 is [True, False, False, True, False, False]
Current timestep = 508. State = [[-0.04228996 -0.16803086]]. Action = [[-0.2398456   0.07097924 -0.07275954 -0.6106274 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 508 is [False, True, False, True, False, False]
Current timestep = 509. State = [[-0.04194722 -0.17414582]]. Action = [[ 0.2376616  -0.24265659  0.08262616 -0.6380366 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 509 is [False, True, False, True, False, False]
Scene graph at timestep 509 is [False, True, False, True, False, False]
State prediction error at timestep 509 is tensor(8.1372e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 509 of 1
Current timestep = 510. State = [[-0.04217375 -0.18185177]]. Action = [[-0.23785155  0.1445694  -0.18156888 -0.47171652]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 510 is [False, True, False, True, False, False]
Current timestep = 511. State = [[-0.04591129 -0.16501562]]. Action = [[ 0.00578696  0.20619866  0.14247859 -0.86757576]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 511 is [False, True, False, True, False, False]
Current timestep = 512. State = [[-0.04948243 -0.13672045]]. Action = [[-0.09069112  0.19584796 -0.08774972 -0.76460254]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 512 is [False, True, False, True, False, False]
Scene graph at timestep 512 is [False, True, False, True, False, False]
State prediction error at timestep 512 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 512 of 1
Current timestep = 513. State = [[-0.05917616 -0.10380714]]. Action = [[-0.02171639  0.23906523  0.06720856 -0.58845395]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 513 is [False, True, False, True, False, False]
Current timestep = 514. State = [[-0.06944493 -0.07159952]]. Action = [[-0.20044628  0.2013601   0.12307787 -0.03293407]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 514 is [True, False, False, False, True, False]
Current timestep = 515. State = [[-0.09565365 -0.03872994]]. Action = [[-0.19562653  0.24111491 -0.01447147 -0.5442494 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 515 is [True, False, False, False, True, False]
Current timestep = 516. State = [[-0.12190177 -0.02852744]]. Action = [[-0.15044257 -0.15676945 -0.18534797  0.7244308 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 516 is [True, False, False, False, True, False]
Current timestep = 517. State = [[-0.1402285  -0.04176325]]. Action = [[-0.07893431 -0.11785915  0.03478983  0.9464841 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 517 is [True, False, False, False, True, False]
Current timestep = 518. State = [[-0.14046185 -0.04908679]]. Action = [[ 0.2246575   0.02014166 -0.17985538  0.7510972 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 518 is [True, False, False, False, True, False]
Current timestep = 519. State = [[-0.14034946 -0.04878262]]. Action = [[-0.20009406  0.01391736  0.08094275 -0.4771697 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 519 is [True, False, False, False, True, False]
Scene graph at timestep 519 is [True, False, False, False, True, False]
State prediction error at timestep 519 is tensor(6.2035e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 519 of -1
Current timestep = 520. State = [[-0.14619423 -0.05647003]]. Action = [[ 0.04385597 -0.13042925  0.1257667  -0.95360374]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 520 is [True, False, False, False, True, False]
Current timestep = 521. State = [[-0.15006238 -0.06479225]]. Action = [[-0.06016624 -0.03339422 -0.01487926 -0.7725721 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 521 is [True, False, False, False, True, False]
Current timestep = 522. State = [[-0.15569334 -0.05783842]]. Action = [[-0.05892299  0.2164374   0.1501405  -0.07385314]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 522 is [True, False, False, False, True, False]
Current timestep = 523. State = [[-0.16017334 -0.04173984]]. Action = [[ 0.07869524  0.04690209  0.24042797 -0.9080545 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 523 is [True, False, False, False, True, False]
Scene graph at timestep 523 is [True, False, False, False, True, False]
State prediction error at timestep 523 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 523 of 0
Current timestep = 524. State = [[-0.1501341 -0.0295299]]. Action = [[ 0.2344895   0.10872844 -0.05435941  0.7713363 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 524 is [True, False, False, False, True, False]
Current timestep = 525. State = [[-0.12696089 -0.00743659]]. Action = [[0.1908688  0.22749826 0.21561724 0.90163183]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 525 is [True, False, False, False, True, False]
Current timestep = 526. State = [[-0.10387772 -0.00154608]]. Action = [[ 0.22185338 -0.2010542   0.19806558  0.7937603 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 526 is [True, False, False, False, True, False]
Current timestep = 527. State = [[-0.08103259 -0.00416508]]. Action = [[0.10026523 0.06284803 0.06403738 0.34361744]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 527 is [True, False, False, False, True, False]
Current timestep = 528. State = [[-0.07642662 -0.01113467]]. Action = [[-0.19846268 -0.16001152 -0.01741669  0.4495752 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 528 is [True, False, False, False, True, False]
Current timestep = 529. State = [[-0.08082832 -0.03239191]]. Action = [[ 0.01056921 -0.20429426  0.07506672  0.53392506]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 529 is [True, False, False, False, True, False]
Current timestep = 530. State = [[-0.08329827 -0.05721384]]. Action = [[-0.0685994  -0.12559386 -0.11225785 -0.15245765]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 530 is [True, False, False, False, True, False]
Current timestep = 531. State = [[-0.08141503 -0.07485409]]. Action = [[ 0.15730989 -0.10276717 -0.0820543   0.52196383]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 531 is [True, False, False, False, True, False]
Current timestep = 532. State = [[-0.07792605 -0.07173219]]. Action = [[-0.01217543  0.24651802  0.00810921 -0.85616994]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 532 is [True, False, False, False, True, False]
Scene graph at timestep 532 is [True, False, False, False, True, False]
State prediction error at timestep 532 is tensor(8.5318e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 532 of 1
Current timestep = 533. State = [[-0.0686854  -0.05250756]]. Action = [[ 0.23813611  0.03926444 -0.18294525 -0.4928046 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 533 is [True, False, False, False, True, False]
Current timestep = 534. State = [[-0.05485591 -0.05821295]]. Action = [[-0.08902068 -0.1853049  -0.0773726  -0.6625178 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 534 is [True, False, False, False, True, False]
Current timestep = 535. State = [[-0.06224751 -0.08172803]]. Action = [[-0.22528246 -0.17500824  0.0836249  -0.47219652]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 535 is [True, False, False, False, True, False]
Current timestep = 536. State = [[-0.07521809 -0.09437405]]. Action = [[-0.09195548  0.03026518  0.02435541  0.03445196]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 536 is [True, False, False, False, True, False]
Current timestep = 537. State = [[-0.08539235 -0.08737173]]. Action = [[-0.08045977  0.14912826  0.14721757 -0.07470065]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 537 is [True, False, False, False, True, False]
Current timestep = 538. State = [[-0.0849694  -0.08710192]]. Action = [[ 0.21836805 -0.20455872  0.0081366   0.74740195]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 538 is [True, False, False, False, True, False]
Scene graph at timestep 538 is [True, False, False, False, True, False]
State prediction error at timestep 538 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 538 of -1
Current timestep = 539. State = [[-0.08126219 -0.11403237]]. Action = [[-0.03945309 -0.22268832 -0.11970446  0.6618874 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 539 is [True, False, False, False, True, False]
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 539 of -1
Current timestep = 540. State = [[-0.08461797 -0.14489399]]. Action = [[-0.08947375 -0.1744789  -0.14835262 -0.8968659 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 540 is [True, False, False, False, True, False]
Current timestep = 541. State = [[-0.09974055 -0.15961745]]. Action = [[-0.23956048  0.01698476  0.2165891  -0.3632269 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 541 is [True, False, False, True, False, False]
Scene graph at timestep 541 is [True, False, False, True, False, False]
State prediction error at timestep 541 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 541 of -1
Current timestep = 542. State = [[-0.11711489 -0.16478251]]. Action = [[ 0.0366624  -0.03560425  0.02990189  0.97015786]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 542 is [True, False, False, True, False, False]
Current timestep = 543. State = [[-0.12263376 -0.15475084]]. Action = [[-0.19350196  0.23885071  0.00988194  0.18444586]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 543 is [True, False, False, True, False, False]
Current timestep = 544. State = [[-0.1380171  -0.12683833]]. Action = [[-0.13434026  0.22440374 -0.2477888   0.62462115]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 544 is [True, False, False, True, False, False]
Current timestep = 545. State = [[-0.15514667 -0.09915926]]. Action = [[-0.02312769  0.11168545 -0.17453587 -0.27887148]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 545 is [True, False, False, True, False, False]
Scene graph at timestep 545 is [True, False, False, False, True, False]
State prediction error at timestep 545 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 545 of -1
Current timestep = 546. State = [[-0.16890392 -0.08530498]]. Action = [[-0.22732925  0.02424413  0.06658846  0.6941383 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 546 is [True, False, False, False, True, False]
Current timestep = 547. State = [[-0.18295799 -0.0815003 ]]. Action = [[ 0.16276658  0.01390317  0.01822719 -0.6960162 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 547 is [True, False, False, False, True, False]
Scene graph at timestep 547 is [True, False, False, False, True, False]
State prediction error at timestep 547 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 547 of -1
Current timestep = 548. State = [[-0.19213969 -0.08734124]]. Action = [[-0.23977351 -0.10668896  0.11805108 -0.14970219]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 548 is [True, False, False, False, True, False]
Current timestep = 549. State = [[-0.20868951 -0.09569564]]. Action = [[-0.10661578 -0.03456445 -0.1349159  -0.42007476]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 549 is [True, False, False, False, True, False]
Current timestep = 550. State = [[-0.23044291 -0.11536287]]. Action = [[-0.15278074 -0.24297065  0.17333788 -0.90526354]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 550 is [True, False, False, False, True, False]
Current timestep = 551. State = [[-0.25687745 -0.12481748]]. Action = [[-0.21063079  0.1321198  -0.07592295 -0.78715694]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 551 is [True, False, False, False, True, False]
Current timestep = 552. State = [[-0.27079922 -0.1307486 ]]. Action = [[ 0.09216669 -0.19172224  0.13489172  0.7384343 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 552 is [True, False, False, False, True, False]
Current timestep = 553. State = [[-0.2703291  -0.13504738]]. Action = [[-0.1406747   0.1859591  -0.22903256  0.16367483]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 553 is [True, False, False, True, False, False]
Current timestep = 554. State = [[-0.27022108 -0.12220549]]. Action = [[0.04632428 0.06076005 0.09225971 0.6230974 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 554 is [True, False, False, True, False, False]
Current timestep = 555. State = [[-0.26965487 -0.11858425]]. Action = [[-0.222271   -0.20176879 -0.09350568 -0.24693751]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 555 is [True, False, False, False, True, False]
Current timestep = 556. State = [[-0.26265365 -0.12228158]]. Action = [[0.19097036 0.01156107 0.18875712 0.05774176]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 556 is [True, False, False, False, True, False]
Current timestep = 557. State = [[-0.24426727 -0.12740962]]. Action = [[ 0.19587606  0.05012986 -0.08466676  0.4785843 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 557 is [True, False, False, False, True, False]
Current timestep = 558. State = [[-0.23709647 -0.13776171]]. Action = [[-0.17693613 -0.21594097 -0.05254596  0.8888097 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 558 is [True, False, False, True, False, False]
Scene graph at timestep 558 is [True, False, False, True, False, False]
State prediction error at timestep 558 is tensor(6.1656e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 558 of -1
Current timestep = 559. State = [[-0.23973024 -0.14567363]]. Action = [[ 0.05954868 -0.1649297  -0.12897666 -0.9115682 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 559 is [True, False, False, True, False, False]
Current timestep = 560. State = [[-0.24400958 -0.1581483 ]]. Action = [[-0.1603885  -0.00076373  0.19901389  0.289495  ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 560 is [True, False, False, True, False, False]
Scene graph at timestep 560 is [True, False, False, True, False, False]
State prediction error at timestep 560 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 560 of -1
Current timestep = 561. State = [[-0.2429947  -0.15707856]]. Action = [[ 0.22605523  0.10009465  0.02682143 -0.77727664]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 561 is [True, False, False, True, False, False]
Current timestep = 562. State = [[-0.22669582 -0.15133023]]. Action = [[ 0.12593484  0.23203832 -0.15586445  0.88614464]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 562 is [True, False, False, True, False, False]
Current timestep = 563. State = [[-0.20309272 -0.12517077]]. Action = [[ 0.19231129  0.21691132 -0.0259794  -0.8059053 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 563 is [True, False, False, True, False, False]
Current timestep = 564. State = [[-0.19355312 -0.09732745]]. Action = [[-0.15767701  0.19300824 -0.14309728  0.9027382 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 564 is [True, False, False, True, False, False]
Current timestep = 565. State = [[-0.18830115 -0.08646864]]. Action = [[ 0.24091017 -0.13999584  0.05347729  0.2967577 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 565 is [True, False, False, False, True, False]
Scene graph at timestep 565 is [True, False, False, False, True, False]
State prediction error at timestep 565 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 565 of -1
Current timestep = 566. State = [[-0.16657408 -0.09243162]]. Action = [[ 0.20460206 -0.0409885   0.14548355 -0.03871042]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 566 is [True, False, False, False, True, False]
Current timestep = 567. State = [[-0.14286277 -0.09105159]]. Action = [[ 0.11142522  0.06350249  0.23051637 -0.0256893 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 567 is [True, False, False, False, True, False]
Current timestep = 568. State = [[-0.1379615  -0.13538186]]. Action = [[ 0.21017817 -0.07274891  0.04903361 -0.5263052 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 568 is [True, False, False, False, True, False]
Current timestep = 569. State = [[-0.122858   -0.14408247]]. Action = [[-0.1368077   0.14720613  0.08967203 -0.35364795]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 569 is [True, False, False, True, False, False]
Current timestep = 570. State = [[-0.1238308  -0.13549802]]. Action = [[ 0.03078732  0.06092629 -0.16996963  0.5356599 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 570 is [True, False, False, True, False, False]
Current timestep = 571. State = [[-0.12065491 -0.12132388]]. Action = [[0.14694142 0.1300987  0.08300224 0.7965797 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 571 is [True, False, False, True, False, False]
Scene graph at timestep 571 is [True, False, False, False, True, False]
State prediction error at timestep 571 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 571 of 1
Current timestep = 572. State = [[-0.10754415 -0.09393625]]. Action = [[ 0.18058115  0.24838099  0.04265782 -0.9665939 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 572 is [True, False, False, False, True, False]
Current timestep = 573. State = [[-0.09739187 -0.0762258 ]]. Action = [[-0.18176153  0.00365099  0.22639781  0.97588253]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 573 is [True, False, False, False, True, False]
Current timestep = 574. State = [[-0.09213383 -0.08491196]]. Action = [[ 0.23107451 -0.21709046 -0.15334833 -0.72737896]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 574 is [True, False, False, False, True, False]
Current timestep = 575. State = [[-0.08174751 -0.09284857]]. Action = [[ 0.01154378  0.05039901 -0.2195008  -0.9489104 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 575 is [True, False, False, False, True, False]
Current timestep = 576. State = [[-0.07019752 -0.09335286]]. Action = [[ 0.23046434 -0.00729175  0.03770688  0.3157128 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 576 is [True, False, False, False, True, False]
Current timestep = 577. State = [[-0.04987044 -0.10566741]]. Action = [[ 0.00881904 -0.17752871  0.0558041  -0.79274297]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 577 is [True, False, False, False, True, False]
Current timestep = 578. State = [[-0.04612539 -0.1308445 ]]. Action = [[-0.03130212 -0.2402767   0.05238694  0.20947778]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 578 is [False, True, False, False, True, False]
Current timestep = 579. State = [[-0.04500718 -0.13935654]]. Action = [[ 0.04848242  0.1631679   0.19451338 -0.39991903]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 579 is [False, True, False, True, False, False]
Current timestep = 580. State = [[-0.03384694 -0.14703202]]. Action = [[ 0.20349485 -0.23170507 -0.10959911 -0.23899835]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 580 is [False, True, False, True, False, False]
Current timestep = 581. State = [[-0.01102966 -0.16762066]]. Action = [[ 0.0899336  -0.12229517 -0.03224024 -0.39470387]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 581 is [False, True, False, True, False, False]
Current timestep = 582. State = [[-0.00037533 -0.16621943]]. Action = [[-0.05768435  0.23175824  0.1785213   0.4117422 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 582 is [False, True, False, True, False, False]
Current timestep = 583. State = [[ 0.00619824 -0.16647375]]. Action = [[ 0.18779916 -0.2114285  -0.00116915  0.22921503]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 583 is [False, True, False, True, False, False]
Scene graph at timestep 583 is [False, True, False, True, False, False]
State prediction error at timestep 583 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 583 of -1
Current timestep = 584. State = [[ 0.03116399 -0.1914814 ]]. Action = [[ 0.1996187  -0.23492254 -0.19625084 -0.66947776]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 584 is [False, True, False, True, False, False]
Scene graph at timestep 584 is [False, True, False, True, False, False]
State prediction error at timestep 584 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 584 of -1
Current timestep = 585. State = [[ 0.05742088 -0.21165954]]. Action = [[ 0.10949862 -0.20241334 -0.22056842 -0.2930581 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 585 is [False, True, False, True, False, False]
Current timestep = 586. State = [[ 0.05524582 -0.22385378]]. Action = [[-0.0396117  -0.1645027   0.15664142  0.53911567]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 586 is [False, False, True, True, False, False]
Scene graph at timestep 586 is [False, False, True, True, False, False]
State prediction error at timestep 586 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 586 of -1
Current timestep = 587. State = [[ 0.05303777 -0.23657599]]. Action = [[ 0.21111733  0.1210967   0.22483897 -0.68874323]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 587 is [False, False, True, True, False, False]
Current timestep = 588. State = [[ 0.05303777 -0.23657599]]. Action = [[ 0.14245105 -0.15068787 -0.1398681   0.7810416 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 588 is [False, False, True, True, False, False]
Scene graph at timestep 588 is [False, False, True, True, False, False]
State prediction error at timestep 588 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 588 of -1
Current timestep = 589. State = [[ 0.05302886 -0.236654  ]]. Action = [[ 0.23379338  0.205598   -0.11817816 -0.15064782]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 589 is [False, False, True, True, False, False]
Current timestep = 590. State = [[ 0.05393763 -0.2303022 ]]. Action = [[ 0.04328251  0.13001826  0.01158231 -0.887772  ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 590 is [False, False, True, True, False, False]
Scene graph at timestep 590 is [False, False, True, True, False, False]
State prediction error at timestep 590 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 590 of 1
Current timestep = 591. State = [[ 0.05217945 -0.23513767]]. Action = [[-0.19626391 -0.14555992  0.2401526  -0.88424915]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 591 is [False, False, True, True, False, False]
Current timestep = 592. State = [[ 0.04860459 -0.24443883]]. Action = [[ 0.16698363 -0.02070941  0.11009628  0.42503405]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 592 is [False, False, True, True, False, False]
Current timestep = 593. State = [[ 0.04813723 -0.24541654]]. Action = [[ 0.208581    0.17705178 -0.06677942  0.6638104 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 593 is [False, True, False, True, False, False]
Current timestep = 594. State = [[ 0.03685696 -0.2615829 ]]. Action = [[-0.22376005 -0.22375092 -0.00743894  0.40916955]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 594 is [False, True, False, True, False, False]
Current timestep = 595. State = [[ 0.01968087 -0.26675728]]. Action = [[-0.17998086  0.20678324  0.11579362 -0.93977356]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 595 is [False, True, False, True, False, False]
Current timestep = 596. State = [[-0.00360995 -0.25115043]]. Action = [[-0.2129754   0.12184858 -0.03911912  0.71515656]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 596 is [False, True, False, True, False, False]
Current timestep = 597. State = [[-0.01978779 -0.23350452]]. Action = [[ 0.13574857  0.08440363 -0.14114319  0.8162986 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 597 is [False, True, False, True, False, False]
Current timestep = 598. State = [[-0.01694801 -0.2145826 ]]. Action = [[ 0.02154076  0.17550242 -0.21255788 -0.29910582]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 598 is [False, True, False, True, False, False]
Scene graph at timestep 598 is [False, True, False, True, False, False]
State prediction error at timestep 598 is tensor(7.4777e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 598 of 1
Current timestep = 599. State = [[-0.01802464 -0.18981117]]. Action = [[-0.1584523   0.1285342  -0.15660089 -0.60274565]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 599 is [False, True, False, True, False, False]
Current timestep = 600. State = [[-0.02773967 -0.17328781]]. Action = [[-0.08885208  0.10556602 -0.21324256  0.5438503 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 600 is [False, True, False, True, False, False]
Scene graph at timestep 600 is [False, True, False, True, False, False]
State prediction error at timestep 600 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 600 of 1
Current timestep = 601. State = [[-0.04125457 -0.14907104]]. Action = [[ 0.19577512  0.20625347  0.00181243 -0.8220346 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 601 is [False, True, False, True, False, False]
Current timestep = 602. State = [[-0.04041589 -0.12909062]]. Action = [[-0.16965875  0.05703801 -0.20823044 -0.5869362 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 602 is [False, True, False, True, False, False]
Current timestep = 603. State = [[-0.04136655 -0.1335711 ]]. Action = [[ 0.1459952  -0.21123773  0.04855311 -0.6312104 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 603 is [False, True, False, True, False, False]
Current timestep = 604. State = [[-0.0414664  -0.13581505]]. Action = [[-0.12039596  0.14931852  0.03086439 -0.47060966]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 604 is [False, True, False, True, False, False]
Current timestep = 605. State = [[-0.04547329 -0.11883434]]. Action = [[-0.07892404  0.206483    0.24599731 -0.6151638 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 605 is [False, True, False, True, False, False]
Current timestep = 606. State = [[-0.04801412 -0.09715272]]. Action = [[0.13021907 0.06529802 0.16035491 0.5745683 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 606 is [False, True, False, False, True, False]
Current timestep = 607. State = [[-0.04925786 -0.0883297 ]]. Action = [[-0.15073532  0.0351018  -0.20657372  0.9699607 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 607 is [False, True, False, False, True, False]
Current timestep = 608. State = [[-0.05056999 -0.09236834]]. Action = [[ 0.11669511 -0.14957274 -0.03161195 -0.36948705]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 608 is [False, True, False, False, True, False]
Current timestep = 609. State = [[-0.04729913 -0.08922487]]. Action = [[0.11920345 0.16333753 0.11631861 0.56811786]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 609 is [True, False, False, False, True, False]
Current timestep = 610. State = [[-0.04679835 -0.07421368]]. Action = [[-0.09173176  0.15711027  0.19559893  0.9494357 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 610 is [False, True, False, False, True, False]
Scene graph at timestep 610 is [False, True, False, False, True, False]
State prediction error at timestep 610 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 610 of 1
Current timestep = 611. State = [[-0.04894143 -0.04506283]]. Action = [[ 0.04924807  0.23326534 -0.12274334  0.38844323]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 611 is [False, True, False, False, True, False]
Current timestep = 612. State = [[-0.04832172 -0.03933987]]. Action = [[ 0.04868042 -0.20860402  0.12622699  0.98843   ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 612 is [False, True, False, False, True, False]
Current timestep = 613. State = [[-0.04794009 -0.038541  ]]. Action = [[-0.02985834  0.1532971  -0.05754545  0.9614868 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 613 is [False, True, False, False, True, False]
Current timestep = 614. State = [[-0.04836396 -0.03642861]]. Action = [[-0.00762455 -0.08335684 -0.13746543 -0.70386744]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 614 is [False, True, False, False, True, False]
Current timestep = 615. State = [[-0.04393231 -0.04953315]]. Action = [[ 0.1437971  -0.19256234 -0.1732539  -0.07792503]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 615 is [False, True, False, False, True, False]
Current timestep = 616. State = [[-0.27564305  0.12255402]]. Action = [[ 0.09274393 -0.17503403 -0.11892687  0.03272927]]. Reward = [100.]
Curr episode timestep = 47
Scene graph at timestep 616 is [False, True, False, False, True, False]
Current timestep = 617. State = [[-0.2745467   0.13487045]]. Action = [[-0.17843686  0.00039351 -0.15773793  0.19768941]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 617 is [True, False, False, False, True, False]
Scene graph at timestep 617 is [True, False, False, False, False, True]
State prediction error at timestep 617 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 617 of -1
Current timestep = 618. State = [[-0.27439857  0.14324145]]. Action = [[ 0.02146456  0.12588674 -0.07130423  0.50976014]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 618 is [True, False, False, False, False, True]
Current timestep = 619. State = [[-0.27139372  0.15303041]]. Action = [[-0.22407113 -0.09365569 -0.10917741  0.12132001]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 619 is [True, False, False, False, False, True]
Current timestep = 620. State = [[-0.26623133  0.15620555]]. Action = [[ 0.12247699  0.0624229   0.23954901 -0.2513665 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 620 is [True, False, False, False, False, True]
Current timestep = 621. State = [[-0.24286798  0.15739419]]. Action = [[ 0.23884189 -0.0927608  -0.18030877 -0.51667815]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 621 is [True, False, False, False, False, True]
Current timestep = 622. State = [[-0.22306882  0.15618396]]. Action = [[-0.23278214  0.19786358  0.06973484 -0.9352489 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 622 is [True, False, False, False, False, True]
Scene graph at timestep 622 is [True, False, False, False, False, True]
State prediction error at timestep 622 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of 1
Current timestep = 623. State = [[-0.21183386  0.13999823]]. Action = [[ 0.13980675 -0.23615682 -0.19427301 -0.6286741 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 623 is [True, False, False, False, False, True]
Current timestep = 624. State = [[-0.1937009  0.1259606]]. Action = [[ 0.19881743  0.00469279  0.09343719 -0.9343993 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 624 is [True, False, False, False, False, True]
Scene graph at timestep 624 is [True, False, False, False, False, True]
State prediction error at timestep 624 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 624 of 1
Current timestep = 625. State = [[-0.1672125   0.12498382]]. Action = [[ 0.07783294  0.05945176 -0.06120521  0.7982968 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 625 is [True, False, False, False, False, True]
Scene graph at timestep 625 is [True, False, False, False, True, False]
State prediction error at timestep 625 is tensor(2.9917e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 625 of 1
Current timestep = 626. State = [[-0.15806445  0.1214075 ]]. Action = [[-0.20303194 -0.17204718 -0.19217336 -0.07870269]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 626 is [True, False, False, False, True, False]
Scene graph at timestep 626 is [True, False, False, False, True, False]
State prediction error at timestep 626 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 626 of 1
Current timestep = 627. State = [[-0.17216358  0.10046271]]. Action = [[-0.1914424  -0.15393752  0.07542551 -0.93412155]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 627 is [True, False, False, False, True, False]
Current timestep = 628. State = [[-0.18382671  0.08958986]]. Action = [[ 0.09687093  0.00254869 -0.2386088  -0.47969085]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 628 is [True, False, False, False, True, False]
Scene graph at timestep 628 is [True, False, False, False, True, False]
State prediction error at timestep 628 is tensor(8.5175e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 628 of 1
Current timestep = 629. State = [[-0.18335699  0.07503087]]. Action = [[-0.06879883 -0.21594353  0.19935691  0.00137472]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 629 is [True, False, False, False, True, False]
Current timestep = 630. State = [[-0.18235816  0.04726044]]. Action = [[ 0.08356684 -0.21004103 -0.12546073 -0.950168  ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 630 is [True, False, False, False, True, False]
Scene graph at timestep 630 is [True, False, False, False, True, False]
State prediction error at timestep 630 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 630 of 1
Current timestep = 631. State = [[-0.17270213  0.02266655]]. Action = [[ 0.243426   -0.05158429  0.15387684 -0.7449423 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 631 is [True, False, False, False, True, False]
Current timestep = 632. State = [[-0.16201644  0.01916823]]. Action = [[-0.01901424  0.03366336 -0.20687112 -0.50501347]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 632 is [True, False, False, False, True, False]
Current timestep = 633. State = [[-0.15173964  0.0073818 ]]. Action = [[ 0.22736907 -0.22582565  0.1426875  -0.7124105 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 633 is [True, False, False, False, True, False]
Scene graph at timestep 633 is [True, False, False, False, True, False]
State prediction error at timestep 633 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 633 of 1
Current timestep = 634. State = [[-0.12397674 -0.00144174]]. Action = [[0.13445044 0.19358754 0.00751835 0.70908475]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 634 is [True, False, False, False, True, False]
Current timestep = 635. State = [[-0.10614806  0.01567553]]. Action = [[ 0.16536745  0.13394779  0.07815135 -0.70109695]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 635 is [True, False, False, False, True, False]
Current timestep = 636. State = [[-0.08643619  0.03296931]]. Action = [[ 0.09179059  0.10071784  0.00679052 -0.4247923 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 636 is [True, False, False, False, True, False]
Scene graph at timestep 636 is [True, False, False, False, True, False]
State prediction error at timestep 636 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 636 of 1
Current timestep = 637. State = [[-0.07502981  0.03360113]]. Action = [[-0.16208886 -0.20328909  0.05548602  0.44391298]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 637 is [True, False, False, False, True, False]
Scene graph at timestep 637 is [True, False, False, False, True, False]
State prediction error at timestep 637 is tensor(1.2033e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 637 of 1
Current timestep = 638. State = [[-0.08448647  0.02339187]]. Action = [[-0.1982769   0.0244135   0.08326429 -0.67992145]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 638 is [True, False, False, False, True, False]
Scene graph at timestep 638 is [True, False, False, False, True, False]
State prediction error at timestep 638 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 638 of -1
Current timestep = 639. State = [[-0.10842029  0.00967838]]. Action = [[-0.20017739 -0.19510354  0.20645627  0.4414016 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 639 is [True, False, False, False, True, False]
Scene graph at timestep 639 is [True, False, False, False, True, False]
State prediction error at timestep 639 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 639 of -1
Current timestep = 640. State = [[-0.13464569  0.00465405]]. Action = [[-0.19181076  0.15251672  0.09115893 -0.94301915]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 640 is [True, False, False, False, True, False]
Current timestep = 641. State = [[-0.16477221  0.028582  ]]. Action = [[-0.2469485   0.23926017  0.03657565  0.7069502 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 641 is [True, False, False, False, True, False]
Current timestep = 642. State = [[-0.19731979  0.04544507]]. Action = [[-0.21559311 -0.05330899 -0.2083797  -0.86173964]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 642 is [True, False, False, False, True, False]
Current timestep = 643. State = [[-0.22413236  0.04681099]]. Action = [[-0.08947295  0.02079272  0.05617738 -0.16614938]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 643 is [True, False, False, False, True, False]
Current timestep = 644. State = [[-0.23285092  0.03576693]]. Action = [[ 0.04176116 -0.19337483 -0.19599341 -0.11827362]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 644 is [True, False, False, False, True, False]
Current timestep = 645. State = [[-0.23770669  0.02838918]]. Action = [[-0.1157186   0.05864653 -0.12125456  0.13348699]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 645 is [True, False, False, False, True, False]
Scene graph at timestep 645 is [True, False, False, False, True, False]
State prediction error at timestep 645 is tensor(8.1332e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 645 of -1
Current timestep = 646. State = [[-0.25409555  0.0183059 ]]. Action = [[-0.0881363  -0.14045592 -0.17662753  0.7497885 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 646 is [True, False, False, False, True, False]
Current timestep = 647. State = [[-0.2573914   0.00914613]]. Action = [[-0.2396637  -0.02432048 -0.11600232 -0.17071211]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 647 is [True, False, False, False, True, False]
Scene graph at timestep 647 is [True, False, False, False, True, False]
State prediction error at timestep 647 is tensor(4.2881e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 647 of -1
Current timestep = 648. State = [[-0.25790745  0.00729484]]. Action = [[-0.19549456  0.1481519   0.17527059  0.652421  ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 648 is [True, False, False, False, True, False]
Current timestep = 649. State = [[-0.26109517  0.00673802]]. Action = [[-0.07632208 -0.00832467 -0.17812313 -0.6273053 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 649 is [True, False, False, False, True, False]
Current timestep = 650. State = [[-0.27464554  0.01050026]]. Action = [[-0.0154373   0.09508163  0.12568718  0.9581262 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 650 is [True, False, False, False, True, False]
Current timestep = 651. State = [[-0.27573448  0.02436378]]. Action = [[ 0.10882637  0.14877859 -0.14759305  0.7209859 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 651 is [True, False, False, False, True, False]
Current timestep = 652. State = [[-0.27499464  0.03351745]]. Action = [[-0.10948378  0.01781467 -0.21727277 -0.00600982]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 652 is [True, False, False, False, True, False]
Current timestep = 653. State = [[-0.2750783   0.03459713]]. Action = [[-0.10495372  0.19996011 -0.17790394  0.414083  ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 653 is [True, False, False, False, True, False]
Current timestep = 654. State = [[-0.2749896   0.03486306]]. Action = [[-0.09890845  0.14205992  0.12642291  0.6974163 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 654 is [True, False, False, False, True, False]
Current timestep = 655. State = [[-0.27498972  0.03492353]]. Action = [[-0.11905703 -0.08202906  0.14443028  0.9518701 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 655 is [True, False, False, False, True, False]
Current timestep = 656. State = [[-0.26847568  0.03064504]]. Action = [[ 0.14667594 -0.11191276  0.03217453 -0.67976344]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 656 is [True, False, False, False, True, False]
Current timestep = 657. State = [[-0.26215118  0.03250624]]. Action = [[-0.00664467  0.13252121 -0.2453286   0.54173756]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 657 is [True, False, False, False, True, False]
Current timestep = 658. State = [[-0.26248094  0.03661006]]. Action = [[-0.1943434   0.12481862 -0.01533996 -0.34875524]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 658 is [True, False, False, False, True, False]
Scene graph at timestep 658 is [True, False, False, False, True, False]
State prediction error at timestep 658 is tensor(3.8663e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 658 of -1
Current timestep = 659. State = [[-0.24918146  0.04499951]]. Action = [[ 0.24302769  0.06958675 -0.20014367  0.07736945]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 659 is [True, False, False, False, True, False]
Current timestep = 660. State = [[-0.2262736   0.05531464]]. Action = [[ 0.13093829  0.09856951 -0.07551751  0.30867684]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 660 is [True, False, False, False, True, False]
Scene graph at timestep 660 is [True, False, False, False, True, False]
State prediction error at timestep 660 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 660 of 1
Current timestep = 661. State = [[-0.21731633  0.0728268 ]]. Action = [[-0.15842976  0.11251682 -0.12598151 -0.8582155 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 661 is [True, False, False, False, True, False]
Current timestep = 662. State = [[-0.22594579  0.09223164]]. Action = [[-0.04042216  0.19318122  0.15813783 -0.7271377 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 662 is [True, False, False, False, True, False]
Current timestep = 663. State = [[-0.23007587  0.0947074 ]]. Action = [[ 0.01982585 -0.2149424   0.12346596 -0.6048454 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 663 is [True, False, False, False, True, False]
Current timestep = 664. State = [[-0.22989857  0.07584034]]. Action = [[-0.05118304 -0.17410731 -0.10905443 -0.42313623]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 664 is [True, False, False, False, True, False]
Current timestep = 665. State = [[-0.23016861  0.0641777 ]]. Action = [[0.01135653 0.01598874 0.20811698 0.89397776]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 665 is [True, False, False, False, True, False]
Scene graph at timestep 665 is [True, False, False, False, True, False]
State prediction error at timestep 665 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 665 of -1
Current timestep = 666. State = [[-0.22292322  0.0739776 ]]. Action = [[ 0.22932118  0.23584193  0.24391454 -0.63160074]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 666 is [True, False, False, False, True, False]
Scene graph at timestep 666 is [True, False, False, False, True, False]
State prediction error at timestep 666 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 666 of 1
Current timestep = 667. State = [[-0.20976047  0.08926022]]. Action = [[-0.11566339 -0.03675593 -0.02665934  0.7653322 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 667 is [True, False, False, False, True, False]
Scene graph at timestep 667 is [True, False, False, False, True, False]
State prediction error at timestep 667 is tensor(5.9212e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 667 of 1
Current timestep = 668. State = [[-0.2201137  0.0897314]]. Action = [[-0.21381609 -0.00114939 -0.00825492 -0.9275378 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 668 is [True, False, False, False, True, False]
Current timestep = 669. State = [[-0.2273297   0.07464243]]. Action = [[ 0.20075363 -0.23123741  0.06584671 -0.10623586]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 669 is [True, False, False, False, True, False]
Current timestep = 670. State = [[-0.22246958  0.07023398]]. Action = [[ 0.03079739  0.20166591 -0.10050252 -0.8854506 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 670 is [True, False, False, False, True, False]
Scene graph at timestep 670 is [True, False, False, False, True, False]
State prediction error at timestep 670 is tensor(4.3564e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 670 of 1
Current timestep = 671. State = [[-0.21654893  0.06662416]]. Action = [[ 0.12317127 -0.24136777  0.01403528  0.5278835 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 671 is [True, False, False, False, True, False]
Current timestep = 672. State = [[-0.1965994   0.05157582]]. Action = [[ 0.24027982  0.01374447  0.07328483 -0.33169878]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 672 is [True, False, False, False, True, False]
Current timestep = 673. State = [[-0.17636631  0.0444635 ]]. Action = [[ 0.03738827 -0.11257641 -0.10437292 -0.25017977]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 673 is [True, False, False, False, True, False]
Current timestep = 674. State = [[-0.17647561  0.04281878]]. Action = [[-0.2264386   0.11439762  0.05549324  0.4819243 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 674 is [True, False, False, False, True, False]
Current timestep = 675. State = [[-0.17977168  0.05861086]]. Action = [[0.11397842 0.18404481 0.11574391 0.3369032 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 675 is [True, False, False, False, True, False]
Current timestep = 676. State = [[-0.17815079  0.08113182]]. Action = [[ 0.07661334  0.18379363 -0.12292451  0.66451955]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 676 is [True, False, False, False, True, False]
Current timestep = 677. State = [[-0.17016473  0.10006198]]. Action = [[ 0.05213669  0.09537506 -0.16624294 -0.1298104 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 677 is [True, False, False, False, True, False]
Current timestep = 678. State = [[-0.16458358  0.12165305]]. Action = [[-0.02545302  0.19495925  0.14449897 -0.41640246]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 678 is [True, False, False, False, True, False]
Current timestep = 679. State = [[-0.17457315  0.14648199]]. Action = [[-0.12046802  0.16571793  0.24392974  0.851516  ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 679 is [True, False, False, False, True, False]
Current timestep = 680. State = [[-0.17670333  0.17554356]]. Action = [[0.13899472 0.23622069 0.21075255 0.54544175]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 680 is [True, False, False, False, False, True]
Scene graph at timestep 680 is [True, False, False, False, False, True]
State prediction error at timestep 680 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 680 of 1
Current timestep = 681. State = [[-0.15571058  0.18661979]]. Action = [[ 0.15305528 -0.21722293  0.14888161  0.7439467 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 681 is [True, False, False, False, False, True]
Scene graph at timestep 681 is [True, False, False, False, False, True]
State prediction error at timestep 681 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 681 of 1
Current timestep = 682. State = [[-0.15308495  0.17996068]]. Action = [[-0.18109022  0.09850991 -0.18999732  0.34676707]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 682 is [True, False, False, False, False, True]
Current timestep = 683. State = [[-0.16047463  0.18510272]]. Action = [[-0.12602636 -0.03341341 -0.00553988  0.2673056 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 683 is [True, False, False, False, False, True]
Current timestep = 684. State = [[-0.16309592  0.18893884]]. Action = [[ 0.1441027   0.10289657 -0.16286501 -0.7219383 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 684 is [True, False, False, False, False, True]
Current timestep = 685. State = [[-0.15443751  0.19677196]]. Action = [[ 0.22962803  0.09232575 -0.16441773 -0.39959407]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 685 is [True, False, False, False, False, True]
Current timestep = 686. State = [[-0.12901483  0.20209754]]. Action = [[ 0.19377482 -0.09145381 -0.01243801 -0.71161366]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 686 is [True, False, False, False, False, True]
Scene graph at timestep 686 is [True, False, False, False, False, True]
State prediction error at timestep 686 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 686 of 1
Current timestep = 687. State = [[-0.11068628  0.19117564]]. Action = [[-0.17814392 -0.15475951  0.1785869  -0.7159559 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 687 is [True, False, False, False, False, True]
Current timestep = 688. State = [[-0.10523126  0.16951887]]. Action = [[ 0.20542383 -0.19306408  0.18816322  0.5819137 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 688 is [True, False, False, False, False, True]
Current timestep = 689. State = [[-0.09020389  0.1426384 ]]. Action = [[ 0.18111658 -0.17073332  0.22741097 -0.18176496]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 689 is [True, False, False, False, False, True]
Current timestep = 690. State = [[-0.08014219  0.12899289]]. Action = [[-0.0652597   0.01564643  0.12910178  0.05934823]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 690 is [True, False, False, False, False, True]
Current timestep = 691. State = [[-0.07312634  0.12712283]]. Action = [[ 0.22624657  0.00979987 -0.21874453  0.18251526]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 691 is [True, False, False, False, False, True]
Current timestep = 692. State = [[-0.04914777  0.13747373]]. Action = [[ 0.15042746  0.17231452  0.03491133 -0.8420596 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 692 is [True, False, False, False, False, True]
Scene graph at timestep 692 is [False, True, False, False, False, True]
State prediction error at timestep 692 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 692 of 1
Current timestep = 693. State = [[-0.03007798  0.15854548]]. Action = [[-0.03651826  0.10185629 -0.10756999 -0.54429066]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 693 is [False, True, False, False, False, True]
Current timestep = 694. State = [[-0.03251959  0.15385103]]. Action = [[-0.18836057 -0.18823619  0.01016954  0.15025902]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 694 is [False, True, False, False, False, True]
Current timestep = 695. State = [[-0.03516418  0.15045431]]. Action = [[ 0.01444638  0.08509046 -0.18873973 -0.78222626]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 695 is [False, True, False, False, False, True]
Scene graph at timestep 695 is [False, True, False, False, False, True]
State prediction error at timestep 695 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 695 of 1
Current timestep = 696. State = [[-0.03288045  0.14727946]]. Action = [[ 0.17885697 -0.09016034 -0.04773672 -0.29552865]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 696 is [False, True, False, False, False, True]
Current timestep = 697. State = [[-0.03284045  0.14685747]]. Action = [[-0.16473141  0.07218432  0.04200768  0.5297675 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 697 is [False, True, False, False, False, True]
Current timestep = 698. State = [[-0.03037748  0.15570606]]. Action = [[ 0.22443646  0.12616932 -0.1504981  -0.12488675]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 698 is [False, True, False, False, False, True]
Scene graph at timestep 698 is [False, True, False, False, False, True]
State prediction error at timestep 698 is tensor(2.3641e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 698 of 0
Current timestep = 699. State = [[-0.02140889  0.15547931]]. Action = [[-0.14418513 -0.2380673  -0.08689404 -0.8437128 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 699 is [False, True, False, False, False, True]
Current timestep = 700. State = [[-0.02763648  0.12703858]]. Action = [[-0.1206999  -0.23717593 -0.23010266  0.7538688 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 700 is [False, True, False, False, False, True]
Current timestep = 701. State = [[-0.04233765  0.11807761]]. Action = [[-0.22617193  0.15282351 -0.12046382 -0.604199  ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 701 is [False, True, False, False, False, True]
Current timestep = 702. State = [[-0.05810607  0.12305529]]. Action = [[ 0.09439442 -0.02303886 -0.14788051 -0.18603551]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 702 is [False, True, False, False, True, False]
Current timestep = 703. State = [[-0.05327409  0.13329619]]. Action = [[0.1981036  0.23274565 0.13929573 0.51655424]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 703 is [True, False, False, False, True, False]
Scene graph at timestep 703 is [True, False, False, False, False, True]
State prediction error at timestep 703 is tensor(4.7440e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 703 of 1
Current timestep = 704. State = [[-0.05063776  0.1581888 ]]. Action = [[-0.19840841  0.15880418  0.19525284 -0.67119   ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 704 is [True, False, False, False, False, True]
Current timestep = 705. State = [[-0.06469164  0.1857595 ]]. Action = [[-0.1025935   0.20490628 -0.17160296 -0.242818  ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 705 is [True, False, False, False, False, True]
Current timestep = 706. State = [[-0.07717895  0.19131072]]. Action = [[-0.10859959 -0.21371703  0.00144076  0.44855905]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 706 is [True, False, False, False, False, True]
Current timestep = 707. State = [[-0.08838779  0.19288865]]. Action = [[-0.16168544  0.16166967 -0.2021848  -0.1796261 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 707 is [True, False, False, False, False, True]
Current timestep = 708. State = [[-0.10312135  0.20197435]]. Action = [[ 0.06931621  0.00410056 -0.17088816  0.2223419 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 708 is [True, False, False, False, False, True]
Current timestep = 709. State = [[-0.10892594  0.20709196]]. Action = [[-0.14876229  0.04766655 -0.15036356  0.18312931]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 709 is [True, False, False, False, False, True]
Current timestep = 710. State = [[-0.1177077   0.19980754]]. Action = [[-0.04619449 -0.2006131  -0.01852235 -0.9637491 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 710 is [True, False, False, False, False, True]
Current timestep = 711. State = [[-0.13116686  0.17335233]]. Action = [[-0.21195349 -0.21093972 -0.13513513  0.18343627]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 711 is [True, False, False, False, False, True]
Current timestep = 712. State = [[-0.14753059  0.15368798]]. Action = [[ 0.12822622 -0.03294188  0.16424835  0.55034566]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 712 is [True, False, False, False, False, True]
Current timestep = 713. State = [[-0.139508    0.13778348]]. Action = [[ 0.15217853 -0.18459617 -0.06236766 -0.8460758 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 713 is [True, False, False, False, False, True]
Current timestep = 714. State = [[-0.12475821  0.11081872]]. Action = [[ 0.14198685 -0.19521649  0.10602453 -0.81727386]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 714 is [True, False, False, False, False, True]
Current timestep = 715. State = [[-0.11093988  0.08591249]]. Action = [[ 0.14256275 -0.06242581  0.18839455 -0.01447129]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 715 is [True, False, False, False, True, False]
Current timestep = 716. State = [[-0.09327892  0.07605967]]. Action = [[ 2.3950744e-01  6.1810017e-05 -9.3620703e-02 -6.1565572e-01]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 716 is [True, False, False, False, True, False]
Current timestep = 717. State = [[-0.07517964  0.06314939]]. Action = [[-0.0825012  -0.21628508 -0.15997466  0.24638021]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 717 is [True, False, False, False, True, False]
Scene graph at timestep 717 is [True, False, False, False, True, False]
State prediction error at timestep 717 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 717 of 1
Current timestep = 718. State = [[-0.06950071  0.05909472]]. Action = [[ 0.04028249  0.23416865  0.21809775 -0.14934355]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 718 is [True, False, False, False, True, False]
Current timestep = 719. State = [[-0.06334294  0.07954253]]. Action = [[ 0.17114675  0.15255195  0.01942834 -0.72842795]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 719 is [True, False, False, False, True, False]
Current timestep = 720. State = [[-0.05018315  0.08773764]]. Action = [[ 0.01732197 -0.11192146  0.17804408 -0.96663254]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 720 is [True, False, False, False, True, False]
Current timestep = 721. State = [[-0.04703373  0.08744938]]. Action = [[0.03496552 0.06712267 0.23692966 0.06714261]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 721 is [True, False, False, False, True, False]
Scene graph at timestep 721 is [False, True, False, False, True, False]
State prediction error at timestep 721 is tensor(4.9947e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 721 of 1
Current timestep = 722. State = [[-0.0439082   0.09259144]]. Action = [[-0.10696068  0.00136068 -0.1617199  -0.92464274]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 722 is [False, True, False, False, True, False]
Current timestep = 723. State = [[-0.04408014  0.09284767]]. Action = [[ 0.06546128  0.00578687 -0.2402268  -0.5251891 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 723 is [False, True, False, False, True, False]
Current timestep = 724. State = [[-0.04105106  0.08895735]]. Action = [[ 0.09987444 -0.06336406  0.0667882  -0.533785  ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 724 is [False, True, False, False, True, False]
Scene graph at timestep 724 is [False, True, False, False, True, False]
State prediction error at timestep 724 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 724 of 1
Current timestep = 725. State = [[-0.17389205  0.21209663]]. Action = [[ 0.05873501  0.16092229 -0.01997122 -0.38415718]]. Reward = [100.]
Curr episode timestep = 108
Scene graph at timestep 725 is [False, True, False, False, True, False]
Current timestep = 726. State = [[-0.14813747  0.23946592]]. Action = [[ 0.2076956   0.04334092 -0.04811731 -0.246683  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 726 is [True, False, False, False, False, True]
Current timestep = 727. State = [[-0.12197265  0.2387562 ]]. Action = [[ 0.14148015 -0.1043804  -0.1808551   0.23559427]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 727 is [True, False, False, False, False, True]
Current timestep = 728. State = [[-0.0983229   0.22339953]]. Action = [[ 0.1602282  -0.16617785 -0.07417658  0.5194446 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 728 is [True, False, False, False, False, True]
Current timestep = 729. State = [[-0.0785939  0.2101238]]. Action = [[ 0.09237969 -0.03292841  0.1578359   0.824445  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 729 is [True, False, False, False, False, True]
Scene graph at timestep 729 is [True, False, False, False, False, True]
State prediction error at timestep 729 is tensor(1.4733e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 729 of 1
Current timestep = 730. State = [[-0.06774352  0.1965487 ]]. Action = [[-0.18360756 -0.19380942 -0.10753059  0.0053544 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 730 is [True, False, False, False, False, True]
Current timestep = 731. State = [[-0.06544494  0.19152385]]. Action = [[ 0.22016531  0.15499696  0.17476067 -0.06598121]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 731 is [True, False, False, False, False, True]
Current timestep = 732. State = [[-0.06634424  0.205096  ]]. Action = [[-0.17779943  0.11846399 -0.09183583 -0.6067787 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 732 is [True, False, False, False, False, True]
Current timestep = 733. State = [[-0.06417678  0.20746234]]. Action = [[ 0.24050704 -0.11927804 -0.06745708 -0.9288214 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 733 is [True, False, False, False, False, True]
Current timestep = 734. State = [[-0.05412027  0.20127223]]. Action = [[-0.05227838 -0.04355478 -0.04087184  0.9948857 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 734 is [True, False, False, False, False, True]
Current timestep = 735. State = [[-0.04628906  0.19013117]]. Action = [[ 0.18461943 -0.09208797 -0.07208978  0.04758787]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 735 is [True, False, False, False, False, True]
Current timestep = 736. State = [[-0.0387348   0.18688658]]. Action = [[-0.17561086  0.01976851 -0.22752768  0.04217041]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 736 is [False, True, False, False, False, True]
Current timestep = 737. State = [[-0.0421374   0.17836624]]. Action = [[-0.14703058 -0.16460642 -0.05169472 -0.03231913]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 737 is [False, True, False, False, False, True]
Current timestep = 738. State = [[-0.0421031   0.17975911]]. Action = [[0.24669725 0.22203422 0.23821893 0.60083985]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 738 is [False, True, False, False, False, True]
Current timestep = 739. State = [[-0.0432886   0.18943657]]. Action = [[-0.06875271  0.03894398  0.1846776   0.4978999 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 739 is [False, True, False, False, False, True]
Current timestep = 740. State = [[-0.04222036  0.20790464]]. Action = [[ 0.15332869  0.24113184 -0.00893015  0.95531464]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 740 is [False, True, False, False, False, True]
Scene graph at timestep 740 is [False, True, False, False, False, True]
State prediction error at timestep 740 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 740 of -1
Current timestep = 741. State = [[-0.01980201  0.23129001]]. Action = [[ 0.13663673  0.01259926  0.1261869  -0.846576  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 741 is [False, True, False, False, False, True]
Current timestep = 742. State = [[-0.00047696  0.22222893]]. Action = [[ 0.17512879 -0.209374   -0.00460124  0.8288121 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 742 is [False, True, False, False, False, True]
Scene graph at timestep 742 is [False, True, False, False, False, True]
State prediction error at timestep 742 is tensor(1.6985e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 742 of -1
Current timestep = 743. State = [[0.02672902 0.20354828]]. Action = [[ 0.2179634  -0.06718443 -0.03242956 -0.58751434]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 743 is [False, True, False, False, False, True]
Current timestep = 744. State = [[0.05142798 0.1979675 ]]. Action = [[0.18763328 0.02539963 0.22783369 0.20177579]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 744 is [False, True, False, False, False, True]
Current timestep = 745. State = [[0.07110625 0.20069197]]. Action = [[ 0.15150702  0.03149334  0.03560507 -0.05928618]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 745 is [False, False, True, False, False, True]
Current timestep = 746. State = [[0.07876708 0.2034434 ]]. Action = [[0.22047174 0.19477487 0.21100011 0.7551868 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 746 is [False, False, True, False, False, True]
Current timestep = 747. State = [[0.08017559 0.203867  ]]. Action = [[ 0.2451872  -0.19483218  0.13520697  0.53116834]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 747 is [False, False, True, False, False, True]
Scene graph at timestep 747 is [False, False, True, False, False, True]
State prediction error at timestep 747 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 747 of -1
Current timestep = 748. State = [[0.08006651 0.20384707]]. Action = [[-0.01958293 -0.19903971  0.13389993 -0.93298376]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 748 is [False, False, True, False, False, True]
Current timestep = 749. State = [[0.08003934 0.20384209]]. Action = [[-2.5865436e-04 -4.8982114e-02  2.2422320e-01  9.3411183e-01]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 749 is [False, False, True, False, False, True]
Current timestep = 750. State = [[0.07984859 0.20380722]]. Action = [[ 0.20882052  0.0345397   0.01622465 -0.24684882]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 750 is [False, False, True, False, False, True]
Current timestep = 751. State = [[0.08009615 0.20288068]]. Action = [[-0.11448562 -0.0254142   0.18036902  0.29217827]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 751 is [False, False, True, False, False, True]
Current timestep = 752. State = [[0.08029706 0.20188217]]. Action = [[ 0.07875407 -0.04651043 -0.16118452  0.6420965 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 752 is [False, False, True, False, False, True]
Current timestep = 753. State = [[0.08035279 0.20133512]]. Action = [[ 0.19100296 -0.12866016 -0.141816   -0.09386063]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 753 is [False, False, True, False, False, True]
Scene graph at timestep 753 is [False, False, True, False, False, True]
State prediction error at timestep 753 is tensor(9.0455e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 753 of 1
Current timestep = 754. State = [[0.0804425  0.20045854]]. Action = [[ 0.23488885  0.15283716 -0.07253809 -0.6331725 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 754 is [False, False, True, False, False, True]
Current timestep = 755. State = [[0.0804425  0.20045854]]. Action = [[ 0.1733008  -0.15046291 -0.16679211 -0.9032583 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 755 is [False, False, True, False, False, True]
Current timestep = 756. State = [[0.08045103 0.20016538]]. Action = [[ 0.12680513 -0.12163523  0.20125216  0.2993977 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 756 is [False, False, True, False, False, True]
Scene graph at timestep 756 is [False, False, True, False, False, True]
State prediction error at timestep 756 is tensor(4.9057e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 756 of 1
Current timestep = 757. State = [[0.08045103 0.20016538]]. Action = [[ 0.09196609 -0.20518818  0.22329408 -0.6361071 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 757 is [False, False, True, False, False, True]
Current timestep = 758. State = [[0.08045103 0.20016538]]. Action = [[ 0.1642392  -0.18887016  0.08668214 -0.7704462 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 758 is [False, False, True, False, False, True]
Current timestep = 759. State = [[0.08045103 0.20016538]]. Action = [[ 0.09760952 -0.08090046  0.09765387 -0.02252507]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 759 is [False, False, True, False, False, True]
Current timestep = 760. State = [[0.08045103 0.20016538]]. Action = [[ 0.1969916  -0.18472493 -0.09200582 -0.02698374]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 760 is [False, False, True, False, False, True]
Current timestep = 761. State = [[0.07790925 0.20517913]]. Action = [[-0.1484678   0.03867143  0.20840555  0.87448025]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 761 is [False, False, True, False, False, True]
Scene graph at timestep 761 is [False, False, True, False, False, True]
State prediction error at timestep 761 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 761 of 0
Current timestep = 762. State = [[0.07382447 0.21117927]]. Action = [[-0.05640373 -0.00696985 -0.10560408 -0.4735247 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 762 is [False, False, True, False, False, True]
Current timestep = 763. State = [[0.07241758 0.2116059 ]]. Action = [[ 0.1224536  -0.14143944  0.21298552 -0.71692926]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 763 is [False, False, True, False, False, True]
Current timestep = 764. State = [[0.07210201 0.21160005]]. Action = [[ 0.12904671 -0.15692376 -0.21410702 -0.8574385 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 764 is [False, False, True, False, False, True]
Current timestep = 765. State = [[0.07190412 0.21189408]]. Action = [[ 0.211871    0.00869274 -0.05737361 -0.7566293 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 765 is [False, False, True, False, False, True]
Scene graph at timestep 765 is [False, False, True, False, False, True]
State prediction error at timestep 765 is tensor(2.6342e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 765 of 0
Current timestep = 766. State = [[0.0713869  0.21168981]]. Action = [[ 0.20760176 -0.14571011 -0.12903033  0.20582545]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 766 is [False, False, True, False, False, True]
Current timestep = 767. State = [[0.0713869  0.21168981]]. Action = [[0.06889653 0.17991066 0.01180252 0.74535453]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 767 is [False, False, True, False, False, True]
Scene graph at timestep 767 is [False, False, True, False, False, True]
State prediction error at timestep 767 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 767 of 0
Current timestep = 768. State = [[0.07122531 0.21153595]]. Action = [[ 0.22188616  0.1762585   0.14400405 -0.50046265]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 768 is [False, False, True, False, False, True]
Current timestep = 769. State = [[0.07122531 0.21153595]]. Action = [[ 0.1416167  -0.22837377 -0.21351787  0.21468365]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 769 is [False, False, True, False, False, True]
Current timestep = 770. State = [[0.07122531 0.21153595]]. Action = [[ 0.18148726 -0.03903908  0.1924456   0.34793484]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 770 is [False, False, True, False, False, True]
Current timestep = 771. State = [[0.07122531 0.21153595]]. Action = [[0.22112769 0.18868795 0.16259328 0.42565584]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 771 is [False, False, True, False, False, True]
Current timestep = 772. State = [[0.07122531 0.21153595]]. Action = [[ 0.10742739  0.17190468  0.13380653 -0.83324784]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 772 is [False, False, True, False, False, True]
Current timestep = 773. State = [[0.07122531 0.21153595]]. Action = [[ 0.18331915  0.16923827 -0.04512727 -0.32360286]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 773 is [False, False, True, False, False, True]
Current timestep = 774. State = [[0.07122531 0.21153595]]. Action = [[ 0.24587902  0.17394218 -0.21640423 -0.5069454 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 774 is [False, False, True, False, False, True]
Current timestep = 775. State = [[0.07122531 0.21153595]]. Action = [[ 0.22958612 -0.14065875  0.11426753  0.28097975]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 775 is [False, False, True, False, False, True]
Current timestep = 776. State = [[0.07122531 0.21153595]]. Action = [[ 0.21135852 -0.21188426  0.18487406  0.33601284]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 776 is [False, False, True, False, False, True]
Current timestep = 777. State = [[0.07122531 0.21153595]]. Action = [[ 0.04050586  0.02297619  0.02142638 -0.86442155]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 777 is [False, False, True, False, False, True]
Scene graph at timestep 777 is [False, False, True, False, False, True]
State prediction error at timestep 777 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 777 of 0
Current timestep = 778. State = [[0.0698115  0.21134168]]. Action = [[-0.09805746 -0.02052854  0.15214455 -0.14941573]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 778 is [False, False, True, False, False, True]
Current timestep = 779. State = [[0.06238962 0.2108778 ]]. Action = [[ 0.0490841  -0.2155015   0.22388941  0.4315759 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 779 is [False, False, True, False, False, True]
Current timestep = 780. State = [[0.06105516 0.21093822]]. Action = [[ 0.11245024  0.18329903 -0.21905823 -0.52931523]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 780 is [False, False, True, False, False, True]
Current timestep = 781. State = [[0.06144093 0.19938077]]. Action = [[-0.01508512 -0.20958237 -0.1574778   0.34283042]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 781 is [False, False, True, False, False, True]
Current timestep = 782. State = [[0.05301091 0.19303188]]. Action = [[-0.13713972  0.09533253  0.03802058  0.6453726 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 782 is [False, False, True, False, False, True]
Scene graph at timestep 782 is [False, False, True, False, False, True]
State prediction error at timestep 782 is tensor(4.5671e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 782 of 1
Current timestep = 783. State = [[0.04169068 0.19491924]]. Action = [[ 0.0749734   0.00254953  0.02758199 -0.8153704 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 783 is [False, False, True, False, False, True]
Current timestep = 784. State = [[0.03960282 0.20012699]]. Action = [[-0.06878251  0.08339682 -0.10576065  0.89771485]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 784 is [False, True, False, False, False, True]
Current timestep = 785. State = [[0.03448032 0.19385834]]. Action = [[-0.20335947 -0.24192078 -0.11574012  0.40819502]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 785 is [False, True, False, False, False, True]
Current timestep = 786. State = [[0.02100616 0.18911295]]. Action = [[0.13491312 0.20484394 0.05599409 0.09340942]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 786 is [False, True, False, False, False, True]
Scene graph at timestep 786 is [False, True, False, False, False, True]
State prediction error at timestep 786 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 786 of 1
Current timestep = 787. State = [[0.01586684 0.20268981]]. Action = [[0.01146019 0.11781868 0.13438454 0.675473  ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 787 is [False, True, False, False, False, True]
Scene graph at timestep 787 is [False, True, False, False, False, True]
State prediction error at timestep 787 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 787 of 0
Current timestep = 788. State = [[0.00678013 0.22426435]]. Action = [[-0.16764888  0.22947937  0.19561541 -0.6083512 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 788 is [False, True, False, False, False, True]
Current timestep = 789. State = [[-0.01099087  0.25139204]]. Action = [[-0.22085798  0.0400188   0.24338594  0.9832051 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 789 is [False, True, False, False, False, True]
Current timestep = 790. State = [[-0.02245955  0.26536065]]. Action = [[ 0.04398593  0.06129196  0.21678531 -0.8587192 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 790 is [False, True, False, False, False, True]
Scene graph at timestep 790 is [False, True, False, False, False, True]
State prediction error at timestep 790 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 790 of -1
Current timestep = 791. State = [[-0.02979344  0.2789768 ]]. Action = [[-0.04580896  0.17125896 -0.09152237  0.5217885 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 791 is [False, True, False, False, False, True]
Scene graph at timestep 791 is [False, True, False, False, False, True]
State prediction error at timestep 791 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 791 of -1
Current timestep = 792. State = [[-0.0326128   0.28628126]]. Action = [[ 0.13313198 -0.09457797  0.12669474 -0.17705458]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 792 is [False, True, False, False, False, True]
Scene graph at timestep 792 is [False, True, False, False, False, True]
State prediction error at timestep 792 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 792 of 1
Current timestep = 793. State = [[-0.0350373  0.2863091]]. Action = [[-0.22458954  0.03477153 -0.1062867   0.16288602]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 793 is [False, True, False, False, False, True]
Current timestep = 794. State = [[-0.04057789  0.28910786]]. Action = [[ 0.19609189  0.10342088 -0.11064178  0.6859894 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 794 is [False, True, False, False, False, True]
Current timestep = 795. State = [[-0.04110872  0.2895086 ]]. Action = [[-0.18905933  0.16402936  0.09728763 -0.44425964]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 795 is [False, True, False, False, False, True]
Current timestep = 796. State = [[-0.04724595  0.28962097]]. Action = [[-0.15222365 -0.02815163  0.14211637 -0.593197  ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 796 is [False, True, False, False, False, True]
Current timestep = 797. State = [[-0.06092301  0.28719765]]. Action = [[-0.19287883  0.11301303 -0.16591752  0.691674  ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 797 is [False, True, False, False, False, True]
Current timestep = 798. State = [[-0.05742853  0.27377018]]. Action = [[ 0.1674149  -0.20259969  0.2358023   0.33131337]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 798 is [True, False, False, False, False, True]
Scene graph at timestep 798 is [True, False, False, False, False, True]
State prediction error at timestep 798 is tensor(1.0750e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 798 of 1
Current timestep = 799. State = [[-0.0550053  0.2570327]]. Action = [[ 0.0824517   0.12997442 -0.14148584 -0.89370525]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 799 is [True, False, False, False, False, True]
Scene graph at timestep 799 is [True, False, False, False, False, True]
State prediction error at timestep 799 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 799 of -1
Current timestep = 800. State = [[-0.05881838  0.26342505]]. Action = [[-0.18960074 -0.0045213  -0.15731902 -0.8429978 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 800 is [True, False, False, False, False, True]
Scene graph at timestep 800 is [True, False, False, False, False, True]
State prediction error at timestep 800 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 800 of -1
Current timestep = 801. State = [[-0.05601592  0.2592805 ]]. Action = [[ 0.21477586 -0.142836   -0.22857434  0.32687044]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 801 is [True, False, False, False, False, True]
Current timestep = 802. State = [[-0.0411796   0.23719215]]. Action = [[ 0.2218714  -0.18172699 -0.17954953  0.5297756 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 802 is [True, False, False, False, False, True]
Current timestep = 803. State = [[-0.02316735  0.21930553]]. Action = [[0.15831888 0.06208071 0.22481143 0.68521905]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 803 is [False, True, False, False, False, True]
Current timestep = 804. State = [[-0.00999289  0.21263629]]. Action = [[ 0.02727541 -0.12173027 -0.01782557 -0.44982576]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 804 is [False, True, False, False, False, True]
Current timestep = 805. State = [[0.00032808 0.20643228]]. Action = [[ 0.14021361  0.0486156  -0.23348024  0.6228566 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 805 is [False, True, False, False, False, True]
Scene graph at timestep 805 is [False, True, False, False, False, True]
State prediction error at timestep 805 is tensor(6.1119e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 805 of 1
Current timestep = 806. State = [[0.0102067  0.22067162]]. Action = [[-0.11116326  0.219726   -0.10472625  0.35898232]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 806 is [False, True, False, False, False, True]
Scene graph at timestep 806 is [False, True, False, False, False, True]
State prediction error at timestep 806 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 806 of -1
Current timestep = 807. State = [[-1.8969080e-05  2.4001886e-01]]. Action = [[-0.10124338  0.05675894 -0.08241221 -0.10087174]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 807 is [False, True, False, False, False, True]
Scene graph at timestep 807 is [False, True, False, False, False, True]
State prediction error at timestep 807 is tensor(7.0945e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 807 of -1
Current timestep = 808. State = [[-0.00319148  0.24685289]]. Action = [[ 0.13401827  0.01283801 -0.09463933 -0.8377708 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 808 is [False, True, False, False, False, True]
Current timestep = 809. State = [[-0.00482773  0.25035742]]. Action = [[-0.18419847  0.01608279  0.22804087  0.9484254 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 809 is [False, True, False, False, False, True]
Current timestep = 810. State = [[-0.00598248  0.25237462]]. Action = [[ 0.0760304  -0.00682008  0.11900732 -0.456322  ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 810 is [False, True, False, False, False, True]
Current timestep = 811. State = [[-0.00225449  0.2412813 ]]. Action = [[ 0.04229054 -0.23079698  0.1687549   0.5946548 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 811 is [False, True, False, False, False, True]
Current timestep = 812. State = [[0.00161784 0.23415427]]. Action = [[-0.02570817  0.14888272  0.09091729 -0.6121792 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 812 is [False, True, False, False, False, True]
Scene graph at timestep 812 is [False, True, False, False, False, True]
State prediction error at timestep 812 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 812 of 1
Current timestep = 813. State = [[-0.00761528  0.2502882 ]]. Action = [[-0.2359906   0.18292528  0.21185881  0.47723818]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 813 is [False, True, False, False, False, True]
Current timestep = 814. State = [[-0.01513041  0.25445488]]. Action = [[ 0.09240735 -0.22986194  0.10781798 -0.45427358]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 814 is [False, True, False, False, False, True]
Current timestep = 815. State = [[-0.00853992  0.24531934]]. Action = [[ 0.23713523  0.06686831  0.00442678 -0.5138576 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 815 is [False, True, False, False, False, True]
Current timestep = 816. State = [[-0.00121905  0.2400349 ]]. Action = [[-0.1531736  -0.11019579 -0.10073578  0.22988892]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 816 is [False, True, False, False, False, True]
Current timestep = 817. State = [[0.00251497 0.23154749]]. Action = [[ 0.0920521  -0.06685862  0.18076283  0.7376956 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 817 is [False, True, False, False, False, True]
Current timestep = 818. State = [[0.0114773  0.22132643]]. Action = [[ 0.23769927 -0.00436386  0.21842033 -0.42535502]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 818 is [False, True, False, False, False, True]
Scene graph at timestep 818 is [False, True, False, False, False, True]
State prediction error at timestep 818 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 818 of 0
Current timestep = 819. State = [[0.02230299 0.22852777]]. Action = [[-0.09419012  0.21235487 -0.2441221  -0.33129406]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 819 is [False, True, False, False, False, True]
Current timestep = 820. State = [[0.0271609  0.22709808]]. Action = [[ 0.2033543  -0.22032627  0.17753446  0.38969266]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 820 is [False, True, False, False, False, True]
Scene graph at timestep 820 is [False, True, False, False, False, True]
State prediction error at timestep 820 is tensor(2.9241e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 820 of -1
Current timestep = 821. State = [[0.03879848 0.2248334 ]]. Action = [[-0.16006054  0.14190483  0.08529666 -0.97400045]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 821 is [False, True, False, False, False, True]
Current timestep = 822. State = [[0.03451718 0.2322876 ]]. Action = [[ 0.2095114  -0.05639285  0.20510784 -0.8148195 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 822 is [False, True, False, False, False, True]
Current timestep = 823. State = [[0.03581678 0.22888587]]. Action = [[-0.01677445 -0.12158063  0.10370606 -0.35343724]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 823 is [False, True, False, False, False, True]
Current timestep = 824. State = [[0.04000917 0.21950482]]. Action = [[ 0.11150137 -0.0874615  -0.17116927 -0.70749176]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 824 is [False, True, False, False, False, True]
Current timestep = 825. State = [[0.04080703 0.2181368 ]]. Action = [[-0.02769035  0.13429987 -0.21207479  0.13514614]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 825 is [False, True, False, False, False, True]
Current timestep = 826. State = [[0.04275495 0.2147477 ]]. Action = [[ 0.11925459 -0.09810549 -0.03540485 -0.00854439]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 826 is [False, True, False, False, False, True]
Current timestep = 827. State = [[0.04757874 0.21551563]]. Action = [[ 0.11632547  0.08600122  0.1323671  -0.33030295]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 827 is [False, True, False, False, False, True]
Current timestep = 828. State = [[0.06076037 0.22163068]]. Action = [[ 0.21859938 -0.18567584  0.18884283 -0.04486102]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 828 is [False, True, False, False, False, True]
Current timestep = 829. State = [[0.06317126 0.22323838]]. Action = [[ 0.19487411  0.04965732  0.24388027 -0.89371604]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 829 is [False, False, True, False, False, True]
Current timestep = 830. State = [[0.06319235 0.2230413 ]]. Action = [[ 0.1621055  -0.13833311  0.06479251  0.9006219 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 830 is [False, False, True, False, False, True]
Current timestep = 831. State = [[0.06275646 0.22326653]]. Action = [[-0.10800278 -0.0128092  -0.02471505 -0.72428674]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 831 is [False, False, True, False, False, True]
Scene graph at timestep 831 is [False, False, True, False, False, True]
State prediction error at timestep 831 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 831 of -1
Current timestep = 832. State = [[0.06249667 0.22334205]]. Action = [[ 0.15867198 -0.2031609   0.09222355  0.31691062]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 832 is [False, False, True, False, False, True]
Current timestep = 833. State = [[0.06249667 0.22334205]]. Action = [[ 0.06054878 -0.1214928  -0.1736167   0.24267077]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 833 is [False, False, True, False, False, True]
Current timestep = 834. State = [[0.05970475 0.23092881]]. Action = [[0.00161818 0.1497128  0.15304267 0.2696104 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 834 is [False, False, True, False, False, True]
Current timestep = 835. State = [[0.05592339 0.22861102]]. Action = [[-0.2081496  -0.21597865 -0.1713893  -0.08391178]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 835 is [False, False, True, False, False, True]
Current timestep = 836. State = [[0.05432226 0.22072056]]. Action = [[ 0.14115345 -0.06165273  0.21100974  0.79272616]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 836 is [False, False, True, False, False, True]
Current timestep = 837. State = [[0.05045689 0.22943157]]. Action = [[ 0.02478319  0.19625473  0.12451261 -0.01467431]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 837 is [False, False, True, False, False, True]
Current timestep = 838. State = [[0.04123086 0.25419012]]. Action = [[-0.01935807  0.24059972 -0.18283063  0.09716415]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 838 is [False, False, True, False, False, True]
Current timestep = 839. State = [[0.03513959 0.26596564]]. Action = [[-0.05958387 -0.13452126  0.20428395  0.194309  ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 839 is [False, True, False, False, False, True]
Scene graph at timestep 839 is [False, True, False, False, False, True]
State prediction error at timestep 839 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 839 of -1
Current timestep = 840. State = [[0.03542972 0.2653108 ]]. Action = [[ 0.24538004 -0.14643593  0.1724233  -0.23575062]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 840 is [False, True, False, False, False, True]
Current timestep = 841. State = [[0.0361331 0.268239 ]]. Action = [[ 0.15855509  0.12233639 -0.2203306  -0.4766729 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 841 is [False, True, False, False, False, True]
Current timestep = 842. State = [[0.03927386 0.2605169 ]]. Action = [[-0.06885327 -0.24558493  0.00174859  0.9670783 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 842 is [False, True, False, False, False, True]
Current timestep = 843. State = [[0.04127603 0.2460498 ]]. Action = [[-0.13295011 -0.0763998   0.1972326   0.69543326]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 843 is [False, True, False, False, False, True]
Current timestep = 844. State = [[0.03764346 0.23762499]]. Action = [[ 0.2056086   0.17410037  0.066719   -0.94880855]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 844 is [False, True, False, False, False, True]
Current timestep = 845. State = [[0.03704267 0.23661608]]. Action = [[ 0.24238583  0.11636454  0.09965152 -0.7034999 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 845 is [False, True, False, False, False, True]
Scene graph at timestep 845 is [False, True, False, False, False, True]
State prediction error at timestep 845 is tensor(1.5787e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 845 of -1
Current timestep = 846. State = [[0.03702202 0.23540826]]. Action = [[ 0.22477508 -0.14257285  0.0483169   0.93881905]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 846 is [False, True, False, False, False, True]
Current timestep = 847. State = [[0.03128606 0.24614626]]. Action = [[-0.14900735  0.18189403  0.01878387 -0.02918243]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 847 is [False, True, False, False, False, True]
Current timestep = 848. State = [[0.01156453 0.2759056 ]]. Action = [[-0.22401935  0.22585699 -0.01138151  0.39195335]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 848 is [False, True, False, False, False, True]
Current timestep = 849. State = [[-0.01367217  0.29414237]]. Action = [[-0.23172174  0.1755853   0.00317636  0.26527274]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 849 is [False, True, False, False, False, True]
Scene graph at timestep 849 is [False, True, False, False, False, True]
State prediction error at timestep 849 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 849 of -1
Current timestep = 850. State = [[-0.02300002  0.29986876]]. Action = [[-0.16472007  0.00633726 -0.2085371  -0.04195642]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 850 is [False, True, False, False, False, True]
Scene graph at timestep 850 is [False, True, False, False, False, True]
State prediction error at timestep 850 is tensor(7.3084e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 850 of -1
Current timestep = 851. State = [[-0.03665797  0.3019021 ]]. Action = [[ 0.03207222  0.2016252   0.11001843 -0.30328643]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 851 is [False, True, False, False, False, True]
Scene graph at timestep 851 is [False, True, False, False, False, True]
State prediction error at timestep 851 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 851 of -1
Current timestep = 852. State = [[-0.2494444   0.02118882]]. Action = [[-0.17856167  0.20892555  0.19047475 -0.8931623 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 852 is [False, True, False, False, False, True]
Scene graph at timestep 852 is [True, False, False, False, True, False]
State prediction error at timestep 852 is tensor(0.0657, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 852 of 1
Current timestep = 853. State = [[-0.25142     0.02711866]]. Action = [[-0.09790134  0.02601483  0.17066193  0.48891377]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 853 is [True, False, False, False, True, False]
Current timestep = 854. State = [[-0.2545254   0.02933194]]. Action = [[-0.08052048 -0.04321162  0.09570783 -0.83128464]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 854 is [True, False, False, False, True, False]
Current timestep = 855. State = [[-0.25824896  0.02380289]]. Action = [[-0.03542551 -0.08292267  0.12942839  0.7109256 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 855 is [True, False, False, False, True, False]
Current timestep = 856. State = [[-0.2550108   0.01377011]]. Action = [[ 0.2176857  -0.09060603  0.1629973  -0.90147597]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 856 is [True, False, False, False, True, False]
Current timestep = 857. State = [[-0.24735089  0.01881946]]. Action = [[0.06781766 0.2203297  0.18125808 0.3714993 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 857 is [True, False, False, False, True, False]
Current timestep = 858. State = [[-0.23962794  0.01850303]]. Action = [[ 0.00235403 -0.18699214 -0.22918965  0.17444146]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 858 is [True, False, False, False, True, False]
Current timestep = 859. State = [[-0.22917499  0.00902563]]. Action = [[ 0.19434726 -0.05511403  0.02059099  0.40286875]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 859 is [True, False, False, False, True, False]
Scene graph at timestep 859 is [True, False, False, False, True, False]
State prediction error at timestep 859 is tensor(2.4662e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 859 of 1
Current timestep = 860. State = [[-0.21771704 -0.01051793]]. Action = [[-0.22047138 -0.20360224 -0.1783426  -0.36238408]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 860 is [True, False, False, False, True, False]
Current timestep = 861. State = [[-0.23583171 -0.02066816]]. Action = [[-0.2262629   0.07444802 -0.22013588 -0.6673531 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 861 is [True, False, False, False, True, False]
Current timestep = 862. State = [[-0.24713875 -0.00998328]]. Action = [[ 0.17156464  0.1659385  -0.1929664   0.7034507 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 862 is [True, False, False, False, True, False]
Current timestep = 863. State = [[-0.24085365  0.00691082]]. Action = [[ 0.13258708  0.12321025 -0.24484758  0.89703584]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 863 is [True, False, False, False, True, False]
Current timestep = 864. State = [[-0.22066684  0.00667748]]. Action = [[ 0.2147972  -0.19477035  0.20061952  0.37121558]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 864 is [True, False, False, False, True, False]
Scene graph at timestep 864 is [True, False, False, False, True, False]
State prediction error at timestep 864 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 864 of 1
Current timestep = 865. State = [[-0.19262831 -0.01310359]]. Action = [[ 0.16722256 -0.18256015 -0.14949702 -0.4390489 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 865 is [True, False, False, False, True, False]
Current timestep = 866. State = [[-0.16949736 -0.01471197]]. Action = [[0.217085   0.19708064 0.18922436 0.2576425 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 866 is [True, False, False, False, True, False]
Current timestep = 867. State = [[-0.14876647 -0.00990485]]. Action = [[ 0.01021251 -0.04520839 -0.19536427  0.53400254]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 867 is [True, False, False, False, True, False]
Scene graph at timestep 867 is [True, False, False, False, True, False]
State prediction error at timestep 867 is tensor(7.5693e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 867 of 1
Current timestep = 868. State = [[-0.14589523 -0.0183912 ]]. Action = [[-0.13467576 -0.14287016  0.19387054  0.287562  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 868 is [True, False, False, False, True, False]
Current timestep = 869. State = [[-0.14174004 -0.02768799]]. Action = [[ 0.23517603 -0.01744896  0.17405781  0.8487661 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 869 is [True, False, False, False, True, False]
Current timestep = 870. State = [[-0.11884759 -0.03765618]]. Action = [[ 0.23557162 -0.12733372 -0.09860539  0.50366044]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 870 is [True, False, False, False, True, False]
Current timestep = 871. State = [[-0.10331488 -0.04898809]]. Action = [[-0.13002056 -0.04210043  0.17184272 -0.78152686]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 871 is [True, False, False, False, True, False]
Current timestep = 872. State = [[-0.09940245 -0.04392945]]. Action = [[0.17119414 0.1842367  0.05227643 0.76163936]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 872 is [True, False, False, False, True, False]
Scene graph at timestep 872 is [True, False, False, False, True, False]
State prediction error at timestep 872 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 872 of 1
Current timestep = 873. State = [[-0.08977962 -0.02232699]]. Action = [[-0.03595924  0.1816231   0.0969277  -0.8391601 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 873 is [True, False, False, False, True, False]
Current timestep = 874. State = [[-0.09277364 -0.00877192]]. Action = [[-0.12641753  0.02380589 -0.12183207  0.6892625 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 874 is [True, False, False, False, True, False]
Current timestep = 875. State = [[-0.09174056  0.00837521]]. Action = [[ 0.17270747  0.22351992 -0.14572066 -0.11951077]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 875 is [True, False, False, False, True, False]
Current timestep = 876. State = [[-0.08184883  0.02037043]]. Action = [[ 0.12747896 -0.07488576  0.03214166 -0.6696207 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 876 is [True, False, False, False, True, False]
Current timestep = 877. State = [[-0.0638179   0.02213513]]. Action = [[ 0.1660583   0.05958971 -0.11748715  0.71450126]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 877 is [True, False, False, False, True, False]
Current timestep = 878. State = [[-0.03888272  0.02364702]]. Action = [[ 0.19062722 -0.03886585 -0.2015816   0.07228756]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 878 is [True, False, False, False, True, False]
Current timestep = 879. State = [[-0.2612998  0.1916421]]. Action = [[ 0.16342843  0.06403661 -0.18011193 -0.41063946]]. Reward = [100.]
Curr episode timestep = 26
Scene graph at timestep 879 is [False, True, False, False, True, False]
Current timestep = 880. State = [[-0.25339034  0.21934077]]. Action = [[ 0.08494496  0.12003219  0.03246278 -0.8096696 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 880 is [True, False, False, False, False, True]
Current timestep = 881. State = [[-0.24445072  0.2294942 ]]. Action = [[-0.21216577  0.15313673 -0.1998133   0.67462325]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 881 is [True, False, False, False, False, True]
Current timestep = 882. State = [[-0.24848375  0.23963131]]. Action = [[-0.02100883  0.15963292  0.059057   -0.23927695]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 882 is [True, False, False, False, False, True]
Current timestep = 883. State = [[-0.23960963  0.25586   ]]. Action = [[0.23413336 0.12899715 0.09248519 0.26436138]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 883 is [True, False, False, False, False, True]
Current timestep = 884. State = [[-0.20527413  0.26907855]]. Action = [[ 0.22791415 -0.02923891 -0.00747098 -0.29388028]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 884 is [True, False, False, False, False, True]
Current timestep = 885. State = [[-0.18569086  0.26389584]]. Action = [[-0.07390898 -0.17160112 -0.01479754  0.15278292]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 885 is [True, False, False, False, False, True]
Current timestep = 886. State = [[-0.17627075  0.24991325]]. Action = [[ 0.167449   -0.04595608 -0.23471315 -0.8609911 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 886 is [True, False, False, False, False, True]
Current timestep = 887. State = [[-0.1680973  0.2333635]]. Action = [[-0.08106986 -0.21692984 -0.24046141 -0.29883575]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 887 is [True, False, False, False, False, True]
Current timestep = 888. State = [[-0.16161928  0.2165575 ]]. Action = [[ 0.14661616 -0.02577229 -0.19856608  0.31107068]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 888 is [True, False, False, False, False, True]
Current timestep = 889. State = [[-0.15230101  0.20510869]]. Action = [[ 0.13128895 -0.09500736  0.23892507 -0.93310726]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 889 is [True, False, False, False, False, True]
Current timestep = 890. State = [[-0.13650802  0.19061749]]. Action = [[ 0.07084239 -0.12429239 -0.0911057   0.6602404 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 890 is [True, False, False, False, False, True]
Current timestep = 891. State = [[-0.13111651  0.18042341]]. Action = [[-0.1127221  -0.03371909 -0.18480958  0.5464835 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 891 is [True, False, False, False, False, True]
Current timestep = 892. State = [[-0.13784802  0.18956062]]. Action = [[-0.16610177  0.19446984  0.20853019 -0.34408236]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 892 is [True, False, False, False, False, True]
Current timestep = 893. State = [[-0.14536953  0.2015477 ]]. Action = [[0.0967012  0.03409788 0.11635211 0.7741177 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 893 is [True, False, False, False, False, True]
Current timestep = 894. State = [[-0.14516024  0.19978994]]. Action = [[-0.0656777  -0.11132953 -0.09338886  0.3100953 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 894 is [True, False, False, False, False, True]
Current timestep = 895. State = [[-0.14838246  0.19093579]]. Action = [[-0.1360511  -0.10392478  0.04495168  0.08625877]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 895 is [True, False, False, False, False, True]
Current timestep = 896. State = [[-0.16071002  0.19224483]]. Action = [[-0.10299277  0.11863494 -0.1594221  -0.25351465]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 896 is [True, False, False, False, False, True]
Current timestep = 897. State = [[-0.1663973   0.20029113]]. Action = [[ 0.16579658  0.09618098 -0.10502636 -0.34107137]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 897 is [True, False, False, False, False, True]
Current timestep = 898. State = [[-0.17365348  0.21644555]]. Action = [[-0.19938302  0.19836694 -0.10122818 -0.81323487]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 898 is [True, False, False, False, False, True]
Current timestep = 899. State = [[-0.18386163  0.2402247 ]]. Action = [[ 0.09699336  0.14393634  0.23252243 -0.5053058 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 899 is [True, False, False, False, False, True]
Current timestep = 900. State = [[-0.18879722  0.25267527]]. Action = [[-0.16154766 -0.00251807 -0.16534328 -0.21887565]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 900 is [True, False, False, False, False, True]
Current timestep = 901. State = [[-0.18691033  0.24544011]]. Action = [[ 0.11175585 -0.21528639 -0.24763668  0.15518904]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 901 is [True, False, False, False, False, True]
Current timestep = 902. State = [[-0.18641013  0.24049598]]. Action = [[-0.07215789  0.12318289  0.18760902  0.65942526]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 902 is [True, False, False, False, False, True]
Current timestep = 903. State = [[-0.18301451  0.24754308]]. Action = [[ 0.23514944  0.09518066 -0.23201334  0.52630377]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 903 is [True, False, False, False, False, True]
Current timestep = 904. State = [[-0.17373106  0.25459838]]. Action = [[ 0.00280285  0.03775567 -0.17942302  0.04594612]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 904 is [True, False, False, False, False, True]
Scene graph at timestep 904 is [True, False, False, False, False, True]
State prediction error at timestep 904 is tensor(9.6029e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 904 of 0
Current timestep = 905. State = [[-0.17892018  0.26360017]]. Action = [[-0.17098786  0.11377543  0.14678898  0.07931197]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 905 is [True, False, False, False, False, True]
Current timestep = 906. State = [[-0.18313746  0.27606106]]. Action = [[0.14371547 0.09171641 0.22314614 0.36927783]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 906 is [True, False, False, False, False, True]
Current timestep = 907. State = [[-0.17126682  0.27101046]]. Action = [[ 0.03267938 -0.2482703   0.07368332  0.6278775 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 907 is [True, False, False, False, False, True]
Current timestep = 908. State = [[-0.15681307  0.24881607]]. Action = [[ 0.22932506 -0.18499906 -0.1525091  -0.13339943]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 908 is [True, False, False, False, False, True]
Current timestep = 909. State = [[-0.15055363  0.24655107]]. Action = [[-0.17534864  0.22320399 -0.23038152 -0.3403532 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 909 is [True, False, False, False, False, True]
Current timestep = 910. State = [[-0.16193014  0.26124346]]. Action = [[-0.12593773  0.06188014 -0.04330117 -0.03806436]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 910 is [True, False, False, False, False, True]
Current timestep = 911. State = [[-0.16230679  0.25912595]]. Action = [[ 0.10886696 -0.17758264 -0.19404623  0.21342254]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 911 is [True, False, False, False, False, True]
Current timestep = 912. State = [[-0.1608393   0.24844922]]. Action = [[-0.13249457 -0.08006191  0.10843503 -0.42888856]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 912 is [True, False, False, False, False, True]
Current timestep = 913. State = [[-0.15614201  0.23945162]]. Action = [[ 0.22089303 -0.02505064  0.04771084 -0.07814759]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 913 is [True, False, False, False, False, True]
Current timestep = 914. State = [[-0.14971447  0.22618991]]. Action = [[-0.10798207 -0.16925928 -0.23381695  0.6851275 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 914 is [True, False, False, False, False, True]
Current timestep = 915. State = [[-0.15651147  0.20605078]]. Action = [[-0.24069071 -0.1909632  -0.21247295 -0.37314355]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 915 is [True, False, False, False, False, True]
Current timestep = 916. State = [[-0.18030308  0.17864875]]. Action = [[-0.19405223 -0.2143138   0.1986726   0.6345849 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 916 is [True, False, False, False, False, True]
Current timestep = 917. State = [[-0.1975901   0.16036294]]. Action = [[0.01444429 0.01258165 0.13768995 0.45867634]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 917 is [True, False, False, False, False, True]
Current timestep = 918. State = [[-0.19747134  0.15966326]]. Action = [[0.06744096 0.07326546 0.1490959  0.608547  ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 918 is [True, False, False, False, False, True]
Current timestep = 919. State = [[-0.19789727  0.17268169]]. Action = [[0.05533296 0.20645964 0.1317237  0.9224286 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 919 is [True, False, False, False, False, True]
Scene graph at timestep 919 is [True, False, False, False, False, True]
State prediction error at timestep 919 is tensor(8.9014e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 919 of -1
Current timestep = 920. State = [[-0.19827893  0.19697952]]. Action = [[ 0.07439098  0.18317515 -0.04703924 -0.9839988 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 920 is [True, False, False, False, False, True]
Current timestep = 921. State = [[-0.19722538  0.20665611]]. Action = [[-0.10338983 -0.07993233 -0.214303   -0.7695256 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 921 is [True, False, False, False, False, True]
Current timestep = 922. State = [[-0.199936    0.20589738]]. Action = [[-0.09750861 -0.03272524 -0.14241087  0.3282932 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 922 is [True, False, False, False, False, True]
Current timestep = 923. State = [[-0.20074746  0.20712876]]. Action = [[ 0.18005103  0.09416944 -0.23688783  0.42130923]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 923 is [True, False, False, False, False, True]
Current timestep = 924. State = [[-0.19628182  0.20850047]]. Action = [[ 0.07375908 -0.02670686  0.02664739 -0.35634744]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 924 is [True, False, False, False, False, True]
Current timestep = 925. State = [[-0.19585925  0.19820836]]. Action = [[-0.18383174 -0.22193617  0.0732612  -0.09767115]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 925 is [True, False, False, False, False, True]
Current timestep = 926. State = [[-0.1921434   0.17652684]]. Action = [[ 0.12855059 -0.15284438 -0.14435418 -0.76107645]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 926 is [True, False, False, False, False, True]
Current timestep = 927. State = [[-0.1808992   0.17212811]]. Action = [[ 0.23814708  0.17171144  0.04889551 -0.8529352 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 927 is [True, False, False, False, False, True]
Scene graph at timestep 927 is [True, False, False, False, False, True]
State prediction error at timestep 927 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 927 of 1
Current timestep = 928. State = [[-0.16356364  0.1705771 ]]. Action = [[-0.05366638 -0.18730037 -0.03697869  0.02038467]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 928 is [True, False, False, False, False, True]
Current timestep = 929. State = [[-0.15711498  0.15864456]]. Action = [[ 0.16357923  0.01038626 -0.21380374  0.36892533]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 929 is [True, False, False, False, False, True]
Scene graph at timestep 929 is [True, False, False, False, False, True]
State prediction error at timestep 929 is tensor(7.0270e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 929 of 1
Current timestep = 930. State = [[-0.14186028  0.1609541 ]]. Action = [[ 0.1985786   0.12331524  0.08885154 -0.65919566]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 930 is [True, False, False, False, False, True]
Current timestep = 931. State = [[-0.13208573  0.17321202]]. Action = [[-1.4684738e-01  8.8499755e-02 -2.0184220e-01 -5.0842762e-05]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 931 is [True, False, False, False, False, True]
Scene graph at timestep 931 is [True, False, False, False, False, True]
State prediction error at timestep 931 is tensor(9.4598e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 931 of -1
Current timestep = 932. State = [[-0.13300249  0.17151412]]. Action = [[-0.02529337 -0.18261434 -0.23242079 -0.70612466]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 932 is [True, False, False, False, False, True]
Current timestep = 933. State = [[-0.13825823  0.15225084]]. Action = [[-0.20275134 -0.22450556 -0.0413956  -0.39992142]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 933 is [True, False, False, False, False, True]
Current timestep = 934. State = [[-0.1492432   0.14121807]]. Action = [[-0.01582424  0.11678967  0.14171407 -0.41264123]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 934 is [True, False, False, False, False, True]
Current timestep = 935. State = [[-0.14497153  0.13410933]]. Action = [[ 0.20246089 -0.16771044 -0.24381113  0.5422299 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 935 is [True, False, False, False, False, True]
Current timestep = 936. State = [[-0.1348963   0.13174208]]. Action = [[ 0.12585658  0.16753456  0.22800362 -0.7696266 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 936 is [True, False, False, False, False, True]
Current timestep = 937. State = [[-0.12603828  0.12813742]]. Action = [[ 0.01914442 -0.17712137 -0.23682633  0.68555427]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 937 is [True, False, False, False, False, True]
Scene graph at timestep 937 is [True, False, False, False, False, True]
State prediction error at timestep 937 is tensor(3.3984e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 937 of 1
Current timestep = 938. State = [[-0.11863047  0.11563568]]. Action = [[ 0.05092338 -0.05380937 -0.05677408 -0.7924732 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 938 is [True, False, False, False, False, True]
Scene graph at timestep 938 is [True, False, False, False, True, False]
State prediction error at timestep 938 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 938 of 1
Current timestep = 939. State = [[-0.11765841  0.09897549]]. Action = [[-0.11120723 -0.20463939 -0.220391    0.43129027]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 939 is [True, False, False, False, True, False]
Scene graph at timestep 939 is [True, False, False, False, True, False]
State prediction error at timestep 939 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 939 of 1
Current timestep = 940. State = [[-0.11825197  0.06807242]]. Action = [[ 0.04549766 -0.22689512 -0.04662609  0.77249646]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 940 is [True, False, False, False, True, False]
Scene graph at timestep 940 is [True, False, False, False, True, False]
State prediction error at timestep 940 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 940 of 0
Current timestep = 941. State = [[-0.10848745  0.05917206]]. Action = [[ 0.24520135  0.24796051  0.09463751 -0.79473495]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 941 is [True, False, False, False, True, False]
Current timestep = 942. State = [[-0.0878899   0.07955011]]. Action = [[ 0.17312565  0.13244921  0.23371637 -0.931823  ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 942 is [True, False, False, False, True, False]
Scene graph at timestep 942 is [True, False, False, False, True, False]
State prediction error at timestep 942 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 942 of 1
Current timestep = 943. State = [[-0.07103858  0.10664409]]. Action = [[-0.12620535  0.19658521 -0.182142   -0.48609138]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 943 is [True, False, False, False, True, False]
Current timestep = 944. State = [[-0.07500298  0.11721899]]. Action = [[ 0.06218892 -0.06187691  0.19852465 -0.62313795]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 944 is [True, False, False, False, True, False]
Current timestep = 945. State = [[-0.07462881  0.11649666]]. Action = [[-0.03686804 -0.01685311 -0.15767376  0.5799849 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 945 is [True, False, False, False, True, False]
Current timestep = 946. State = [[-0.07107398  0.10801114]]. Action = [[ 0.07936752 -0.13794912  0.1778472  -0.42164743]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 946 is [True, False, False, False, True, False]
Current timestep = 947. State = [[-0.06856935  0.10181338]]. Action = [[-0.08216409  0.02924815 -0.11985201 -0.8361867 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 947 is [True, False, False, False, True, False]
Scene graph at timestep 947 is [True, False, False, False, True, False]
State prediction error at timestep 947 is tensor(6.3234e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 947 of 1
Current timestep = 948. State = [[-0.06338987  0.09013764]]. Action = [[ 0.19083786 -0.19786538 -0.04030728  0.1853764 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 948 is [True, False, False, False, True, False]
Current timestep = 949. State = [[-0.0512073   0.08155545]]. Action = [[ 0.19731915  0.12382078  0.21579847 -0.17541039]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 949 is [True, False, False, False, True, False]
Current timestep = 950. State = [[-0.1674919  -0.05137393]]. Action = [[ 0.11681849 -0.0572342  -0.13369183 -0.59424525]]. Reward = [100.]
Curr episode timestep = 70
Scene graph at timestep 950 is [True, False, False, False, True, False]
Current timestep = 951. State = [[-0.15783699 -0.0563613 ]]. Action = [[-0.19195391  0.02528253  0.22487837 -0.3762464 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 951 is [True, False, False, False, True, False]
Scene graph at timestep 951 is [True, False, False, False, True, False]
State prediction error at timestep 951 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 951 of -1
Current timestep = 952. State = [[-0.15498975 -0.0698724 ]]. Action = [[ 0.23205203 -0.21348958 -0.10039783 -0.6144428 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 952 is [True, False, False, False, True, False]
Current timestep = 953. State = [[-0.1442672 -0.0753799]]. Action = [[ 0.11226103  0.13427031 -0.13576506  0.69318366]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 953 is [True, False, False, False, True, False]
Current timestep = 954. State = [[-0.12856989 -0.07607205]]. Action = [[ 0.15336299 -0.07678697 -0.06289423 -0.20925426]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 954 is [True, False, False, False, True, False]
Current timestep = 955. State = [[-0.10965782 -0.07864821]]. Action = [[ 0.03855354  0.01245394  0.05384591 -0.54650056]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 955 is [True, False, False, False, True, False]
Current timestep = 956. State = [[-0.09464828 -0.09236859]]. Action = [[ 0.18597472 -0.23216031 -0.19249843  0.5913439 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 956 is [True, False, False, False, True, False]
Scene graph at timestep 956 is [True, False, False, False, True, False]
State prediction error at timestep 956 is tensor(8.7065e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 956 of 1
Current timestep = 957. State = [[-0.0695898  -0.12384524]]. Action = [[ 0.1209057  -0.23130602  0.14080477 -0.94396603]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 957 is [True, False, False, False, True, False]
Scene graph at timestep 957 is [True, False, False, False, True, False]
State prediction error at timestep 957 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 957 of -1
Current timestep = 958. State = [[-0.05427833 -0.14787427]]. Action = [[ 0.07382655 -0.07715034  0.16399264  0.51152277]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 958 is [True, False, False, False, True, False]
Current timestep = 959. State = [[-0.05276057 -0.15306632]]. Action = [[-0.14014181  0.01679423  0.05971617 -0.24833429]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 959 is [True, False, False, True, False, False]
Current timestep = 960. State = [[-0.05484421 -0.15664211]]. Action = [[-0.05777372 -0.03447755  0.228214    0.40049446]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 960 is [True, False, False, True, False, False]
Scene graph at timestep 960 is [True, False, False, True, False, False]
State prediction error at timestep 960 is tensor(6.8342e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 960 of -1
Current timestep = 961. State = [[-0.05671846 -0.15654145]]. Action = [[ 0.10316437  0.09705794 -0.15649329 -0.8260351 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 961 is [True, False, False, True, False, False]
Current timestep = 962. State = [[-0.05436836 -0.16250873]]. Action = [[ 0.08311957 -0.19937946 -0.01843238 -0.49906033]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 962 is [True, False, False, True, False, False]
Current timestep = 963. State = [[-0.04242755 -0.16208255]]. Action = [[ 0.18334061  0.14870852  0.2019301  -0.7026945 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 963 is [True, False, False, True, False, False]
Current timestep = 964. State = [[-0.02296364 -0.16994953]]. Action = [[ 0.07097936 -0.2269048  -0.02518015  0.4218905 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 964 is [False, True, False, True, False, False]
Current timestep = 965. State = [[-0.00641202 -0.16985074]]. Action = [[ 0.14621758  0.20804623 -0.23131531  0.86988723]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 965 is [False, True, False, True, False, False]
Current timestep = 966. State = [[ 0.00271135 -0.15711947]]. Action = [[-0.15451865  0.09324807  0.11886233 -0.85439384]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 966 is [False, True, False, True, False, False]
Scene graph at timestep 966 is [False, True, False, True, False, False]
State prediction error at timestep 966 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 966 of -1
Current timestep = 967. State = [[ 0.0036547  -0.14414632]]. Action = [[ 0.11458585  0.07147691 -0.03953847 -0.9212791 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 967 is [False, True, False, True, False, False]
Current timestep = 968. State = [[ 0.01139762 -0.14495313]]. Action = [[ 0.16961992 -0.13518815 -0.17496686  0.67575467]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 968 is [False, True, False, True, False, False]
Current timestep = 969. State = [[ 0.0221625  -0.14840315]]. Action = [[-0.20449714  0.05701566  0.06439403 -0.67331475]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 969 is [False, True, False, True, False, False]
Current timestep = 970. State = [[ 0.02146738 -0.13869965]]. Action = [[ 0.09713987  0.17455372 -0.10961366  0.35272455]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 970 is [False, True, False, True, False, False]
Scene graph at timestep 970 is [False, True, False, True, False, False]
State prediction error at timestep 970 is tensor(1.0563e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 970 of 1
Current timestep = 971. State = [[ 0.02709617 -0.11370406]]. Action = [[ 0.17030662  0.16424212  0.22841695 -0.33062816]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 971 is [False, True, False, True, False, False]
Scene graph at timestep 971 is [False, True, False, False, True, False]
State prediction error at timestep 971 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 971 of -1
Current timestep = 972. State = [[ 0.04250432 -0.10032133]]. Action = [[ 0.1568231   0.06768402  0.13780886 -0.858191  ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 972 is [False, True, False, False, True, False]
Current timestep = 973. State = [[ 0.04250432 -0.10032133]]. Action = [[ 0.15585333  0.21219987  0.00374204 -0.23181874]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 973 is [False, True, False, False, True, False]
Current timestep = 974. State = [[ 0.04577922 -0.08616455]]. Action = [[ 0.09354699  0.2179389  -0.20159435  0.6064069 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 974 is [False, True, False, False, True, False]
Current timestep = 975. State = [[ 0.05502512 -0.08068319]]. Action = [[ 0.08964884 -0.17847326 -0.03427574  0.02205908]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 975 is [False, True, False, False, True, False]
Current timestep = 976. State = [[ 0.07018884 -0.09299946]]. Action = [[ 0.05524513 -0.09017795  0.20798042  0.41392267]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 976 is [False, False, True, False, True, False]
Current timestep = 977. State = [[ 0.07827257 -0.09946334]]. Action = [[0.16068375 0.05419523 0.07078567 0.7172773 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 977 is [False, False, True, False, True, False]
Current timestep = 978. State = [[ 0.08094551 -0.10057684]]. Action = [[-0.05065924  0.13609523  0.14947522  0.42105663]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 978 is [False, False, True, False, True, False]
Current timestep = 979. State = [[ 0.07906801 -0.11407687]]. Action = [[-0.09759665 -0.1915436  -0.17561598  0.835155  ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 979 is [False, False, True, False, True, False]
Current timestep = 980. State = [[ 0.0768842  -0.12571792]]. Action = [[ 0.03598341 -0.07121548 -0.02722606  0.4555409 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 980 is [False, False, True, False, True, False]
Current timestep = 981. State = [[ 0.07623995 -0.12819502]]. Action = [[ 0.00939074 -0.22953758  0.07827371 -0.5803712 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 981 is [False, False, True, True, False, False]
Current timestep = 982. State = [[ 0.07504387 -0.13565566]]. Action = [[-0.11847694 -0.09125131 -0.02114145 -0.19729787]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 982 is [False, False, True, True, False, False]
Current timestep = 983. State = [[ 0.07319558 -0.14319204]]. Action = [[ 0.06862041 -0.20157911 -0.23670653 -0.20841914]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 983 is [False, False, True, True, False, False]
Scene graph at timestep 983 is [False, False, True, True, False, False]
State prediction error at timestep 983 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 983 of -1
Current timestep = 984. State = [[ 0.06837012 -0.15777922]]. Action = [[-0.11966342 -0.19480295 -0.24164774 -0.51528424]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 984 is [False, False, True, True, False, False]
Scene graph at timestep 984 is [False, False, True, True, False, False]
State prediction error at timestep 984 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 984 of -1
Current timestep = 985. State = [[ 0.05928968 -0.17422651]]. Action = [[ 0.21066585  0.23617142 -0.09443648 -0.6139895 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 985 is [False, False, True, True, False, False]
Current timestep = 986. State = [[ 0.05927237 -0.17428869]]. Action = [[ 0.23108098  0.19644678  0.23448017 -0.16875362]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 986 is [False, False, True, True, False, False]
Current timestep = 987. State = [[ 0.05927237 -0.17428869]]. Action = [[ 0.21153963  0.03412369 -0.09568994 -0.7489745 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 987 is [False, False, True, True, False, False]
Current timestep = 988. State = [[ 0.05385584 -0.16500069]]. Action = [[-0.19978258  0.18276244  0.22921425 -0.20952857]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 988 is [False, False, True, True, False, False]
Current timestep = 989. State = [[ 0.04507985 -0.15452522]]. Action = [[ 0.11378038  0.1617853  -0.13000342 -0.28721946]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 989 is [False, False, True, True, False, False]
Scene graph at timestep 989 is [False, True, False, True, False, False]
State prediction error at timestep 989 is tensor(4.1368e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 989 of 1
Current timestep = 990. State = [[ 0.03858987 -0.15639722]]. Action = [[-0.0957756  -0.0620032   0.01320836  0.52333283]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 990 is [False, True, False, True, False, False]
Current timestep = 991. State = [[ 0.02484824 -0.15587461]]. Action = [[-0.18516432  0.09131819 -0.09408396  0.595989  ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 991 is [False, True, False, True, False, False]
Scene graph at timestep 991 is [False, True, False, True, False, False]
State prediction error at timestep 991 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 991 of 1
Current timestep = 992. State = [[ 0.00160671 -0.1346174 ]]. Action = [[ 0.05151892  0.24283168 -0.02350812 -0.2193563 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 992 is [False, True, False, True, False, False]
Current timestep = 993. State = [[ 0.00128206 -0.11607867]]. Action = [[ 0.0039936  -0.01741727  0.2287817   0.35753465]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 993 is [False, True, False, True, False, False]
Scene graph at timestep 993 is [False, True, False, False, True, False]
State prediction error at timestep 993 is tensor(3.2566e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 993 of 1
Current timestep = 994. State = [[ 0.00017866 -0.11567127]]. Action = [[-0.07233973 -0.02594586 -0.1765349  -0.15154481]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 994 is [False, True, False, False, True, False]
Current timestep = 995. State = [[-0.00554976 -0.13217074]]. Action = [[-0.10778999 -0.24726772  0.05671036  0.34543633]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 995 is [False, True, False, False, True, False]
Current timestep = 996. State = [[-0.02146562 -0.16326585]]. Action = [[-0.07767414 -0.22488783  0.11708641 -0.6687877 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 996 is [False, True, False, True, False, False]
Scene graph at timestep 996 is [False, True, False, True, False, False]
State prediction error at timestep 996 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 996 of -1
Current timestep = 997. State = [[-0.03569542 -0.18031292]]. Action = [[ 0.10665968  0.10137838 -0.21813504  0.7659503 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 997 is [False, True, False, True, False, False]
Current timestep = 998. State = [[-0.03762255 -0.17802757]]. Action = [[-0.17440061 -0.02960669  0.1144169  -0.6884935 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 998 is [False, True, False, True, False, False]
Current timestep = 999. State = [[-0.03900944 -0.19154906]]. Action = [[ 0.13196918 -0.2406936   0.13930076 -0.26979935]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 999 is [False, True, False, True, False, False]
Current timestep = 1000. State = [[-0.03772468 -0.19785178]]. Action = [[-0.02997571  0.16052741  0.14848113 -0.6136408 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1000 is [False, True, False, True, False, False]
Current timestep = 1001. State = [[-0.04592026 -0.2056727 ]]. Action = [[-0.18776345 -0.19900812  0.0996294  -0.8598894 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1001 is [False, True, False, True, False, False]
Scene graph at timestep 1001 is [False, True, False, True, False, False]
State prediction error at timestep 1001 is tensor(3.2232e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1001 of -1
Current timestep = 1002. State = [[-0.06387717 -0.2212148 ]]. Action = [[-0.08563419  0.01165307 -0.22212014 -0.808326  ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1002 is [False, True, False, True, False, False]
Current timestep = 1003. State = [[-0.06865707 -0.22949617]]. Action = [[-0.0113913  -0.1406598  -0.15434848  0.35919893]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1003 is [True, False, False, True, False, False]
Current timestep = 1004. State = [[-0.08324887 -0.22817409]]. Action = [[-0.17019626  0.1715084  -0.0553724  -0.0096696 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1004 is [True, False, False, True, False, False]
Scene graph at timestep 1004 is [True, False, False, True, False, False]
State prediction error at timestep 1004 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1004 of -1
Current timestep = 1005. State = [[-0.10087811 -0.2218028 ]]. Action = [[ 0.11200657 -0.10712503 -0.03823063 -0.36985862]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1005 is [True, False, False, True, False, False]
Current timestep = 1006. State = [[-0.09389476 -0.22330531]]. Action = [[ 0.1733264  -0.00886473 -0.13793235  0.7266371 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1006 is [True, False, False, True, False, False]
Current timestep = 1007. State = [[-0.08029816 -0.22867273]]. Action = [[ 0.20722917 -0.10402535 -0.17113529 -0.57338554]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1007 is [True, False, False, True, False, False]
Current timestep = 1008. State = [[-0.05732979 -0.22828157]]. Action = [[ 0.14411312  0.13783509 -0.02649578  0.6070931 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1008 is [True, False, False, True, False, False]
Current timestep = 1009. State = [[-0.03611093 -0.21278907]]. Action = [[ 0.12883165  0.19090801 -0.15035859  0.08507895]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1009 is [True, False, False, True, False, False]
Current timestep = 1010. State = [[-0.02537417 -0.20302592]]. Action = [[ 0.04526642 -0.10824141  0.03440982 -0.2413398 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1010 is [False, True, False, True, False, False]
Current timestep = 1011. State = [[-0.01162188 -0.19207707]]. Action = [[0.24352202 0.19266543 0.23778939 0.8926358 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1011 is [False, True, False, True, False, False]
Scene graph at timestep 1011 is [False, True, False, True, False, False]
State prediction error at timestep 1011 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1011 of -1
Current timestep = 1012. State = [[ 0.01735636 -0.1918487 ]]. Action = [[-0.11761194 -0.19333725  0.08857036 -0.58070946]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1012 is [False, True, False, True, False, False]
Current timestep = 1013. State = [[ 0.01669457 -0.19594863]]. Action = [[ 0.02806851  0.15547076 -0.23662648  0.1579125 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1013 is [False, True, False, True, False, False]
Current timestep = 1014. State = [[ 0.01652731 -0.1830658 ]]. Action = [[-0.12041549  0.15172455 -0.1007051  -0.8922006 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1014 is [False, True, False, True, False, False]
Scene graph at timestep 1014 is [False, True, False, True, False, False]
State prediction error at timestep 1014 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1014 of 1
Current timestep = 1015. State = [[ 0.0186161  -0.17665014]]. Action = [[ 0.24482334 -0.16725935  0.20187366  0.578547  ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1015 is [False, True, False, True, False, False]
Current timestep = 1016. State = [[ 0.0224461  -0.18594424]]. Action = [[ 0.00457013 -0.05370659  0.13831782 -0.05278885]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1016 is [False, True, False, True, False, False]
Current timestep = 1017. State = [[ 0.03025526 -0.18176274]]. Action = [[ 0.13810003  0.15026304 -0.1663955   0.4045912 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1017 is [False, True, False, True, False, False]
Scene graph at timestep 1017 is [False, True, False, True, False, False]
State prediction error at timestep 1017 is tensor(5.5472e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1017 of -1
Current timestep = 1018. State = [[ 0.04857111 -0.1700136 ]]. Action = [[-0.20108707  0.09194291 -0.22564064 -0.42648244]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1018 is [False, True, False, True, False, False]
Current timestep = 1019. State = [[ 0.04299696 -0.17169861]]. Action = [[-0.10034813 -0.06814216 -0.13319546  0.49157012]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1019 is [False, True, False, True, False, False]
Current timestep = 1020. State = [[ 0.03784756 -0.1779465 ]]. Action = [[ 0.07625777 -0.06664526 -0.09127678 -0.15232456]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1020 is [False, True, False, True, False, False]
Current timestep = 1021. State = [[ 0.03766947 -0.18191436]]. Action = [[ 0.04631647 -0.04248792 -0.11920446 -0.6542919 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1021 is [False, True, False, True, False, False]
Current timestep = 1022. State = [[ 0.03370097 -0.18270108]]. Action = [[-0.16279684  0.07367983  0.05345416  0.8084414 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1022 is [False, True, False, True, False, False]
Current timestep = 1023. State = [[ 0.02384946 -0.18996693]]. Action = [[-0.06759968 -0.14148423  0.09985894  0.6093235 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1023 is [False, True, False, True, False, False]
Scene graph at timestep 1023 is [False, True, False, True, False, False]
State prediction error at timestep 1023 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1023 of 1
Current timestep = 1024. State = [[ 0.01889456 -0.19750334]]. Action = [[0.1553604  0.02528518 0.02926534 0.63269997]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1024 is [False, True, False, True, False, False]
Current timestep = 1025. State = [[ 0.01990893 -0.19205621]]. Action = [[-0.1233061   0.08943009 -0.16073513 -0.813214  ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1025 is [False, True, False, True, False, False]
Current timestep = 1026. State = [[ 0.02460055 -0.18674196]]. Action = [[ 0.24229652 -0.03111346  0.12112033 -0.20117474]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1026 is [False, True, False, True, False, False]
Current timestep = 1027. State = [[ 0.02865141 -0.17266692]]. Action = [[-0.05454919  0.23723882 -0.08148327  0.43161595]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1027 is [False, True, False, True, False, False]
Current timestep = 1028. State = [[ 0.03355238 -0.15045245]]. Action = [[0.16313776 0.07301438 0.03875911 0.7632005 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1028 is [False, True, False, True, False, False]
Scene graph at timestep 1028 is [False, True, False, True, False, False]
State prediction error at timestep 1028 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1028 of 1
Current timestep = 1029. State = [[ 0.04761798 -0.13010919]]. Action = [[ 0.03966027  0.19974172 -0.24244018  0.03531778]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1029 is [False, True, False, True, False, False]
Current timestep = 1030. State = [[ 0.04793168 -0.10485364]]. Action = [[-0.01559694  0.17257518  0.224549    0.682899  ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1030 is [False, True, False, True, False, False]
Scene graph at timestep 1030 is [False, True, False, False, True, False]
State prediction error at timestep 1030 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1030 of 1
Current timestep = 1031. State = [[ 0.04664817 -0.0869633 ]]. Action = [[-0.03341562  0.02129725  0.00640264 -0.7943188 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1031 is [False, True, False, False, True, False]
Current timestep = 1032. State = [[ 0.04572674 -0.07616205]]. Action = [[-0.0633193   0.14447185 -0.11896706 -0.37150097]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1032 is [False, True, False, False, True, False]
Scene graph at timestep 1032 is [False, True, False, False, True, False]
State prediction error at timestep 1032 is tensor(2.4347e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1032 of 1
Current timestep = 1033. State = [[ 0.04426014 -0.06305238]]. Action = [[ 0.20582533  0.07508618  0.01758611 -0.1361351 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1033 is [False, True, False, False, True, False]
Current timestep = 1034. State = [[ 0.04173557 -0.05066816]]. Action = [[-0.1509728   0.22012937 -0.01777229  0.587227  ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1034 is [False, True, False, False, True, False]
Current timestep = 1035. State = [[-0.24990489  0.11128554]]. Action = [[ 0.11491629 -0.1476114   0.12222031 -0.43839192]]. Reward = [100.]
Curr episode timestep = 84
Scene graph at timestep 1035 is [False, True, False, False, True, False]
Current timestep = 1036. State = [[-0.24427451  0.1291581 ]]. Action = [[ 0.08199352  0.11700714  0.14584434 -0.42292118]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1036 is [True, False, False, False, True, False]
Current timestep = 1037. State = [[-0.22431411  0.13695732]]. Action = [[ 0.20402712 -0.0630396   0.16435945 -0.31449097]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1037 is [True, False, False, False, False, True]
Scene graph at timestep 1037 is [True, False, False, False, False, True]
State prediction error at timestep 1037 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1037 of 1
Current timestep = 1038. State = [[-0.2020495   0.13916276]]. Action = [[ 0.04393172  0.06909797  0.03208256 -0.45270038]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1038 is [True, False, False, False, False, True]
Current timestep = 1039. State = [[-0.2035933   0.15391754]]. Action = [[-0.07978469  0.18418038  0.11761281 -0.4546517 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1039 is [True, False, False, False, False, True]
Scene graph at timestep 1039 is [True, False, False, False, False, True]
State prediction error at timestep 1039 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1039 of -1
Current timestep = 1040. State = [[-0.20236152  0.17760071]]. Action = [[ 0.18557218  0.15822119 -0.09963161  0.05931652]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1040 is [True, False, False, False, False, True]
Current timestep = 1041. State = [[-0.1801064   0.19746938]]. Action = [[ 0.13025358  0.13176    -0.0181862   0.07254326]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1041 is [True, False, False, False, False, True]
Current timestep = 1042. State = [[-0.15988283  0.21429497]]. Action = [[ 0.14599481  0.09666258 -0.1489254   0.55349946]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1042 is [True, False, False, False, False, True]
Current timestep = 1043. State = [[-0.15000957  0.22534956]]. Action = [[-0.09298953  0.00465     0.07967645  0.18369448]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1043 is [True, False, False, False, False, True]
Current timestep = 1044. State = [[-0.14284863  0.22509825]]. Action = [[ 0.20910695 -0.04410015  0.1143702  -0.66019195]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1044 is [True, False, False, False, False, True]
Scene graph at timestep 1044 is [True, False, False, False, False, True]
State prediction error at timestep 1044 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1044 of -1
Current timestep = 1045. State = [[-0.11508528  0.21985406]]. Action = [[ 0.2431534  -0.0438489  -0.1555949  -0.23413098]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1045 is [True, False, False, False, False, True]
Current timestep = 1046. State = [[-0.08805206  0.2255174 ]]. Action = [[ 0.21919602  0.16859707 -0.1782713  -0.15347093]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1046 is [True, False, False, False, False, True]
Current timestep = 1047. State = [[-0.05391201  0.22595572]]. Action = [[ 0.14561754 -0.1957558   0.01719648 -0.01288718]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1047 is [True, False, False, False, False, True]
Current timestep = 1048. State = [[-0.04025627  0.23082143]]. Action = [[-0.06915702  0.20384783 -0.22430901  0.71644104]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1048 is [True, False, False, False, False, True]
Current timestep = 1049. State = [[-0.04215864  0.25228208]]. Action = [[ 0.04731071  0.20119166 -0.02419196  0.06888187]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1049 is [False, True, False, False, False, True]
Scene graph at timestep 1049 is [False, True, False, False, False, True]
State prediction error at timestep 1049 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1049 of -1
Current timestep = 1050. State = [[-0.02714401  0.27633268]]. Action = [[ 0.1669825   0.09172267  0.06067145 -0.29275918]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1050 is [False, True, False, False, False, True]
Current timestep = 1051. State = [[-0.01264721  0.27614295]]. Action = [[-0.10780728 -0.1864238   0.03486469  0.68355536]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1051 is [False, True, False, False, False, True]
Current timestep = 1052. State = [[-0.00854955  0.26651323]]. Action = [[ 0.13901228 -0.00252174 -0.1789424  -0.3854974 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1052 is [False, True, False, False, False, True]
Current timestep = 1053. State = [[-0.00569676  0.2611783 ]]. Action = [[-1.09313905e-01 -9.81044769e-02 -5.58808446e-04  6.76270723e-01]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1053 is [False, True, False, False, False, True]
Current timestep = 1054. State = [[0.00095618 0.25239608]]. Action = [[ 0.23538178  0.00248107  0.16087848 -0.07108831]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1054 is [False, True, False, False, False, True]
Scene graph at timestep 1054 is [False, True, False, False, False, True]
State prediction error at timestep 1054 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1054 of 1
Current timestep = 1055. State = [[0.01675181 0.25095662]]. Action = [[ 0.12345159  0.03287706 -0.00640951  0.6132679 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1055 is [False, True, False, False, False, True]
Current timestep = 1056. State = [[0.03568961 0.24943528]]. Action = [[ 0.17937064 -0.04659672 -0.20669484  0.85591817]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1056 is [False, True, False, False, False, True]
Current timestep = 1057. State = [[0.05878818 0.24428882]]. Action = [[ 0.16429627 -0.0555689  -0.04469864  0.15465188]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1057 is [False, True, False, False, False, True]
Scene graph at timestep 1057 is [False, False, True, False, False, True]
State prediction error at timestep 1057 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1057 of -1
Current timestep = 1058. State = [[0.07489125 0.23349258]]. Action = [[ 0.17382929  0.11559337  0.14486635 -0.21450502]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1058 is [False, False, True, False, False, True]
Current timestep = 1059. State = [[0.07489125 0.23349258]]. Action = [[-0.02056424  0.11002812  0.09697595 -0.44052553]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1059 is [False, False, True, False, False, True]
Current timestep = 1060. State = [[0.07076642 0.24039552]]. Action = [[-0.14349584  0.06820846 -0.11124043 -0.33825493]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1060 is [False, False, True, False, False, True]
Current timestep = 1061. State = [[0.06097027 0.25732023]]. Action = [[-0.15797243  0.17379075  0.07524627 -0.65666837]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1061 is [False, False, True, False, False, True]
Current timestep = 1062. State = [[0.05223059 0.27273196]]. Action = [[ 0.16133505 -0.15046564  0.14944053 -0.8051375 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1062 is [False, False, True, False, False, True]
Current timestep = 1063. State = [[0.05099709 0.27486807]]. Action = [[ 0.24632925 -0.23279713  0.23617035  0.7148657 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1063 is [False, False, True, False, False, True]
Current timestep = 1064. State = [[0.05064305 0.27535668]]. Action = [[ 0.0391598   0.02260089 -0.13858837  0.45313096]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1064 is [False, False, True, False, False, True]
Current timestep = 1065. State = [[0.05064305 0.27535668]]. Action = [[ 0.19619527 -0.10434145  0.02045894 -0.54062694]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1065 is [False, False, True, False, False, True]
Current timestep = 1066. State = [[0.05274518 0.2692484 ]]. Action = [[-0.08646604 -0.19578707  0.11205277 -0.90962285]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1066 is [False, False, True, False, False, True]
Current timestep = 1067. State = [[0.05409729 0.26455456]]. Action = [[ 0.15838245 -0.01989636 -0.00425416  0.509789  ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1067 is [False, False, True, False, False, True]
Current timestep = 1068. State = [[0.05200838 0.26517144]]. Action = [[-0.10285953  0.01447973  0.0306817   0.23801255]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1068 is [False, False, True, False, False, True]
Current timestep = 1069. State = [[0.05009854 0.266055  ]]. Action = [[ 0.14660811 -0.15449695 -0.16725115 -0.14391893]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1069 is [False, False, True, False, False, True]
Current timestep = 1070. State = [[0.04994204 0.26606974]]. Action = [[ 0.13529259 -0.01716764  0.15562958  0.7920146 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1070 is [False, False, True, False, False, True]
Current timestep = 1071. State = [[0.04994204 0.26606974]]. Action = [[ 0.14652064  0.00632361  0.03296161 -0.7082599 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1071 is [False, True, False, False, False, True]
Scene graph at timestep 1071 is [False, True, False, False, False, True]
State prediction error at timestep 1071 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1071 of -1
Current timestep = 1072. State = [[0.051697   0.26229116]]. Action = [[ 0.03908154 -0.07536232 -0.11975817  0.28320932]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1072 is [False, True, False, False, False, True]
Scene graph at timestep 1072 is [False, False, True, False, False, True]
State prediction error at timestep 1072 is tensor(6.2397e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1072 of -1
Current timestep = 1073. State = [[0.05291637 0.25973564]]. Action = [[ 0.11467957 -0.15788828  0.02387428  0.45926428]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1073 is [False, False, True, False, False, True]
Scene graph at timestep 1073 is [False, False, True, False, False, True]
State prediction error at timestep 1073 is tensor(6.1146e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1073 of 0
Current timestep = 1074. State = [[0.05273909 0.25413075]]. Action = [[-0.21366993 -0.20391968 -0.01548587 -0.15235329]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1074 is [False, False, True, False, False, True]
Current timestep = 1075. State = [[0.05315309 0.24948338]]. Action = [[0.1602208  0.04766911 0.02221236 0.94097376]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1075 is [False, False, True, False, False, True]
Scene graph at timestep 1075 is [False, False, True, False, False, True]
State prediction error at timestep 1075 is tensor(5.8123e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1075 of 1
Current timestep = 1076. State = [[0.0532819  0.24815735]]. Action = [[ 0.12261274 -0.00363386 -0.02588345  0.6665679 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1076 is [False, False, True, False, False, True]
Current timestep = 1077. State = [[0.05167258 0.24942052]]. Action = [[-0.07434654  0.01873511 -0.24188346 -0.17053515]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1077 is [False, False, True, False, False, True]
Scene graph at timestep 1077 is [False, False, True, False, False, True]
State prediction error at timestep 1077 is tensor(2.1159e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1077 of 0
Current timestep = 1078. State = [[0.05035258 0.24986683]]. Action = [[ 0.22916424  0.0656645  -0.09144926 -0.3740766 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1078 is [False, False, True, False, False, True]
Current timestep = 1079. State = [[0.04659565 0.2570547 ]]. Action = [[-0.01616427  0.12805867 -0.18343294 -0.575255  ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1079 is [False, False, True, False, False, True]
Scene graph at timestep 1079 is [False, True, False, False, False, True]
State prediction error at timestep 1079 is tensor(2.5294e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1079 of -1
Current timestep = 1080. State = [[0.0423287  0.26472545]]. Action = [[ 0.1743679  -0.14537068 -0.06562351 -0.9276998 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1080 is [False, True, False, False, False, True]
Current timestep = 1081. State = [[0.04780596 0.25323153]]. Action = [[ 0.11137488 -0.21724768  0.01138806 -0.31233078]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1081 is [False, True, False, False, False, True]
Current timestep = 1082. State = [[0.05488927 0.23925443]]. Action = [[ 0.17239842 -0.1421286   0.20951867  0.19438767]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1082 is [False, True, False, False, False, True]
Current timestep = 1083. State = [[0.05320046 0.23962426]]. Action = [[-0.1968068  -0.02204542  0.20254207  0.31472003]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1083 is [False, False, True, False, False, True]
Current timestep = 1084. State = [[0.05191513 0.2405631 ]]. Action = [[ 0.16005242 -0.08663936  0.13683611 -0.93593913]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1084 is [False, False, True, False, False, True]
Scene graph at timestep 1084 is [False, False, True, False, False, True]
State prediction error at timestep 1084 is tensor(3.5004e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1084 of 1
Current timestep = 1085. State = [[0.05170844 0.2405903 ]]. Action = [[ 0.09252954 -0.17076792  0.00942782 -0.31287885]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1085 is [False, False, True, False, False, True]
Scene graph at timestep 1085 is [False, False, True, False, False, True]
State prediction error at timestep 1085 is tensor(2.1103e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1085 of 0
Current timestep = 1086. State = [[0.05034945 0.24226692]]. Action = [[-0.07438079 -0.00417368  0.01631477  0.7575946 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1086 is [False, False, True, False, False, True]
Current timestep = 1087. State = [[0.04903229 0.24357863]]. Action = [[ 0.04702339  0.02643421 -0.1883944  -0.06435716]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1087 is [False, False, True, False, False, True]
Scene graph at timestep 1087 is [False, True, False, False, False, True]
State prediction error at timestep 1087 is tensor(5.9238e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1087 of 0
Current timestep = 1088. State = [[0.04873806 0.24394473]]. Action = [[ 0.11941934 -0.21188506  0.06373772 -0.9142843 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1088 is [False, True, False, False, False, True]
Scene graph at timestep 1088 is [False, True, False, False, False, True]
State prediction error at timestep 1088 is tensor(3.7411e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1088 of 0
Current timestep = 1089. State = [[0.04873806 0.24394473]]. Action = [[ 0.18675768 -0.0762001   0.15755814  0.06471717]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1089 is [False, True, False, False, False, True]
Current timestep = 1090. State = [[0.04251038 0.25629058]]. Action = [[-0.06993502  0.22093415  0.04866251  0.30112505]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1090 is [False, True, False, False, False, True]
Current timestep = 1091. State = [[0.03068671 0.273202  ]]. Action = [[-0.24310097 -0.1281156  -0.17908189 -0.48555017]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1091 is [False, True, False, False, False, True]
Scene graph at timestep 1091 is [False, True, False, False, False, True]
State prediction error at timestep 1091 is tensor(3.4815e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1091 of -1
Current timestep = 1092. State = [[0.01936206 0.28623262]]. Action = [[-0.14881139  0.11202317 -0.09774533  0.66451573]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1092 is [False, True, False, False, False, True]
Current timestep = 1093. State = [[0.016312   0.28933275]]. Action = [[ 0.16583163 -0.12763956 -0.1109734  -0.03960299]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1093 is [False, True, False, False, False, True]
Current timestep = 1094. State = [[0.02752937 0.26767835]]. Action = [[ 0.22440511 -0.17729916 -0.18249407  0.5518129 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1094 is [False, True, False, False, False, True]
Current timestep = 1095. State = [[0.04671731 0.23284012]]. Action = [[ 0.15979952 -0.18564971 -0.23923936 -0.41850996]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1095 is [False, True, False, False, False, True]
Scene graph at timestep 1095 is [False, True, False, False, False, True]
State prediction error at timestep 1095 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1095 of 1
Current timestep = 1096. State = [[0.06025139 0.20631641]]. Action = [[-0.01979649  0.05423534  0.05628651  0.71183777]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1096 is [False, True, False, False, False, True]
Scene graph at timestep 1096 is [False, False, True, False, False, True]
State prediction error at timestep 1096 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1096 of 0
Current timestep = 1097. State = [[0.05427248 0.21991944]]. Action = [[-0.18135634  0.17696434  0.20834988  0.07297432]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1097 is [False, False, True, False, False, True]
Current timestep = 1098. State = [[0.03884557 0.2505161 ]]. Action = [[-0.08939874  0.20839196  0.18355966  0.72396517]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1098 is [False, False, True, False, False, True]
Current timestep = 1099. State = [[0.02762659 0.27063146]]. Action = [[-0.09579399 -0.06844293 -0.2332349  -0.49430597]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1099 is [False, True, False, False, False, True]
Current timestep = 1100. State = [[0.02873759 0.26827312]]. Action = [[ 0.11785227 -0.06874865 -0.1289655  -0.20182723]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1100 is [False, True, False, False, False, True]
Scene graph at timestep 1100 is [False, True, False, False, False, True]
State prediction error at timestep 1100 is tensor(2.5883e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1100 of 0
Current timestep = 1101. State = [[0.03467166 0.25686717]]. Action = [[-0.04522812 -0.15508775 -0.23772627  0.76054287]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1101 is [False, True, False, False, False, True]
Scene graph at timestep 1101 is [False, True, False, False, False, True]
State prediction error at timestep 1101 is tensor(7.2665e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1101 of 1
Current timestep = 1102. State = [[0.03701254 0.25131086]]. Action = [[-0.14805263  0.03199828 -0.20179485  0.5226886 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1102 is [False, True, False, False, False, True]
Current timestep = 1103. State = [[0.03501985 0.25377235]]. Action = [[ 0.11730313  0.06257772  0.20113254 -0.48375487]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1103 is [False, True, False, False, False, True]
Scene graph at timestep 1103 is [False, True, False, False, False, True]
State prediction error at timestep 1103 is tensor(3.3494e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1103 of 1
Current timestep = 1104. State = [[0.03001281 0.26528984]]. Action = [[ 0.00133201  0.22291833 -0.14960526 -0.68498844]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1104 is [False, True, False, False, False, True]
Current timestep = 1105. State = [[0.0163241  0.29002166]]. Action = [[-0.08371559  0.15133113 -0.08708262 -0.28306925]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1105 is [False, True, False, False, False, True]
Current timestep = 1106. State = [[0.01263201 0.296355  ]]. Action = [[ 0.14867264 -0.09486409 -0.05953018 -0.591132  ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1106 is [False, True, False, False, False, True]
Current timestep = 1107. State = [[0.02058871 0.2828386 ]]. Action = [[ 0.16467896 -0.11440322  0.11957815  0.6320839 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1107 is [False, True, False, False, False, True]
Scene graph at timestep 1107 is [False, True, False, False, False, True]
State prediction error at timestep 1107 is tensor(4.6452e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1107 of -1
Current timestep = 1108. State = [[0.03175137 0.26610154]]. Action = [[ 0.21541965 -0.15437607 -0.02797404 -0.79922575]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1108 is [False, True, False, False, False, True]
Current timestep = 1109. State = [[0.03175137 0.26610154]]. Action = [[ 0.24427962  0.16956526 -0.10125193  0.04644418]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1109 is [False, True, False, False, False, True]
Current timestep = 1110. State = [[0.03135313 0.26700762]]. Action = [[-0.01442951  0.03810567 -0.17044158 -0.03739619]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1110 is [False, True, False, False, False, True]
Current timestep = 1111. State = [[0.02535612 0.27808523]]. Action = [[-0.1853579   0.11221963  0.1911841  -0.33696902]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1111 is [False, True, False, False, False, True]
Current timestep = 1112. State = [[0.01256956 0.29886988]]. Action = [[-0.23019595  0.02395838  0.08801445  0.905046  ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1112 is [False, True, False, False, False, True]
Current timestep = 1113. State = [[0.00439433 0.31072855]]. Action = [[-0.0197428   0.14467597 -0.13930795 -0.9021765 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1113 is [False, True, False, False, False, True]
Current timestep = 1114. State = [[0.00311767 0.312641  ]]. Action = [[-0.18948103  0.11156791 -0.18130606 -0.8820375 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1114 is [False, True, False, False, False, True]
Scene graph at timestep 1114 is [False, True, False, False, False, True]
State prediction error at timestep 1114 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1114 of 1
Current timestep = 1115. State = [[0.00354491 0.3118493 ]]. Action = [[ 0.01304594 -0.0636245   0.19871122 -0.67634165]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1115 is [False, True, False, False, False, True]
Current timestep = 1116. State = [[0.00718963 0.30562538]]. Action = [[ 0.00836393 -0.09752804 -0.13863976 -0.09422505]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1116 is [False, True, False, False, False, True]
Current timestep = 1117. State = [[0.01175598 0.2949027 ]]. Action = [[-0.13475364 -0.18924682 -0.16354036  0.31011844]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1117 is [False, True, False, False, False, True]
Current timestep = 1118. State = [[0.01606099 0.28473786]]. Action = [[ 0.0075877  -0.04451172 -0.08657125  0.71344626]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1118 is [False, True, False, False, False, True]
Current timestep = 1119. State = [[0.01612696 0.28105596]]. Action = [[-0.09898064 -0.00760502  0.14182988 -0.70855343]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1119 is [False, True, False, False, False, True]
Scene graph at timestep 1119 is [False, True, False, False, False, True]
State prediction error at timestep 1119 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1119 of 1
Current timestep = 1120. State = [[0.01398553 0.2802398 ]]. Action = [[-0.06141284 -0.02735427 -0.13297336  0.7735052 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 1120 is [False, True, False, False, False, True]
Current timestep = 1121. State = [[0.0180787  0.26945361]]. Action = [[ 0.05779678 -0.19363335 -0.16033354 -0.9270134 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1121 is [False, True, False, False, False, True]
Current timestep = 1122. State = [[0.03046669 0.24443953]]. Action = [[ 0.05786833 -0.1835142   0.17479849  0.8041631 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1122 is [False, True, False, False, False, True]
Current timestep = 1123. State = [[0.0386322  0.22910172]]. Action = [[0.03202838 0.11493433 0.01576796 0.43114758]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1123 is [False, True, False, False, False, True]
Scene graph at timestep 1123 is [False, True, False, False, False, True]
State prediction error at timestep 1123 is tensor(5.5249e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1123 of 1
Current timestep = 1124. State = [[0.0334912  0.23923552]]. Action = [[-0.11899777  0.13879639 -0.0294487  -0.42106867]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1124 is [False, True, False, False, False, True]
Scene graph at timestep 1124 is [False, True, False, False, False, True]
State prediction error at timestep 1124 is tensor(5.1262e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1124 of -1
Current timestep = 1125. State = [[0.02871352 0.24787982]]. Action = [[ 0.17439434 -0.06120425  0.08352602  0.8268975 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 1125 is [False, True, False, False, False, True]
Current timestep = 1126. State = [[0.02943723 0.24676153]]. Action = [[-0.14101222  0.04560161 -0.08924466  0.700511  ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1126 is [False, True, False, False, False, True]
Current timestep = 1127. State = [[0.02071897 0.2621223 ]]. Action = [[-0.19714327  0.16600755  0.2051096  -0.6714793 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1127 is [False, True, False, False, False, True]
Scene graph at timestep 1127 is [False, True, False, False, False, True]
State prediction error at timestep 1127 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1127 of -1
Current timestep = 1128. State = [[0.00471771 0.2865567 ]]. Action = [[-0.0822061  -0.14285159 -0.12671062  0.30488002]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 1128 is [False, True, False, False, False, True]
Current timestep = 1129. State = [[0.00864558 0.27706635]]. Action = [[ 0.08545464 -0.08893117 -0.02243009  0.01789963]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1129 is [False, True, False, False, False, True]
Current timestep = 1130. State = [[0.01429535 0.2638919 ]]. Action = [[-0.12541416 -0.12054056 -0.19838959 -0.88587093]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1130 is [False, True, False, False, False, True]
Current timestep = 1131. State = [[0.01490227 0.25867474]]. Action = [[-0.05546449 -0.01074426 -0.02443264 -0.22557306]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 1131 is [False, True, False, False, False, True]
Current timestep = 1132. State = [[0.01442238 0.25111842]]. Action = [[-0.15486956 -0.19875456  0.07725066 -0.26092082]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 1132 is [False, True, False, False, False, True]
Scene graph at timestep 1132 is [False, True, False, False, False, True]
State prediction error at timestep 1132 is tensor(6.2912e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1132 of 1
Current timestep = 1133. State = [[0.0138719  0.24514818]]. Action = [[ 0.08002281  0.21284881 -0.11166996  0.05845392]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 1133 is [False, True, False, False, False, True]
Current timestep = 1134. State = [[0.00307375 0.26462898]]. Action = [[-0.21999772  0.11363566  0.09413379  0.69299936]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1134 is [False, True, False, False, False, True]
Current timestep = 1135. State = [[-0.00809494  0.27999306]]. Action = [[-0.1334433  -0.20777701 -0.14311482  0.46977234]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1135 is [False, True, False, False, False, True]
Current timestep = 1136. State = [[0.0069471  0.25529832]]. Action = [[ 0.22789383 -0.22561806 -0.1963569   0.25284183]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1136 is [False, True, False, False, False, True]
Scene graph at timestep 1136 is [False, True, False, False, False, True]
State prediction error at timestep 1136 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1136 of -1
Current timestep = 1137. State = [[-0.20384906  0.07535414]]. Action = [[-0.23650564 -0.24251162 -0.16024227  0.04405427]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1137 is [False, True, False, False, False, True]
Current timestep = 1138. State = [[-0.23323093  0.04945577]]. Action = [[-0.12896837 -0.11947221  0.01399985  0.72568345]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1138 is [True, False, False, False, True, False]
Current timestep = 1139. State = [[-0.25091767  0.04834229]]. Action = [[-0.07211193  0.202099   -0.03779772 -0.39397573]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1139 is [True, False, False, False, True, False]
Current timestep = 1140. State = [[-0.25800166  0.06660663]]. Action = [[ 0.04092106  0.11158776 -0.07018967  0.4954288 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1140 is [True, False, False, False, True, False]
Current timestep = 1141. State = [[-0.2530426   0.06805464]]. Action = [[ 0.1786176  -0.12246895  0.06177658 -0.56474644]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1141 is [True, False, False, False, True, False]
Current timestep = 1142. State = [[-0.24862152  0.0656781 ]]. Action = [[-0.09638134  0.05348939 -0.04888295  0.58667076]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1142 is [True, False, False, False, True, False]
Current timestep = 1143. State = [[-0.24589449  0.06944811]]. Action = [[ 0.13913828  0.06757295 -0.23179637 -0.9720687 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1143 is [True, False, False, False, True, False]
Current timestep = 1144. State = [[-0.24494585  0.07231735]]. Action = [[-0.2380428   0.08006835  0.2100333   0.5921519 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1144 is [True, False, False, False, True, False]
Current timestep = 1145. State = [[-0.2345713  0.0596645]]. Action = [[ 0.19196302 -0.24267586  0.04765174 -0.2882017 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1145 is [True, False, False, False, True, False]
Current timestep = 1146. State = [[-0.22436547  0.04559398]]. Action = [[-0.01107965  0.02466333  0.09567297 -0.9349312 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1146 is [True, False, False, False, True, False]
Current timestep = 1147. State = [[-0.223038    0.04270843]]. Action = [[ 0.02361715 -0.02942491 -0.19627486  0.05125582]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1147 is [True, False, False, False, True, False]
Current timestep = 1148. State = [[-0.21240368  0.0268348 ]]. Action = [[ 0.17294833 -0.24052165  0.07362729 -0.5419334 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1148 is [True, False, False, False, True, False]
Current timestep = 1149. State = [[-0.18749288  0.00517592]]. Action = [[ 0.22408617 -0.0376669  -0.20520525  0.2315489 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1149 is [True, False, False, False, True, False]
Current timestep = 1150. State = [[-0.16935183 -0.00402227]]. Action = [[-0.03075787 -0.07065573 -0.01577902  0.19943511]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1150 is [True, False, False, False, True, False]
Current timestep = 1151. State = [[-0.16933602 -0.00488421]]. Action = [[-0.07264051  0.08871773 -0.04913212 -0.8543081 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1151 is [True, False, False, False, True, False]
Current timestep = 1152. State = [[-0.16541566 -0.01235549]]. Action = [[ 0.13968587 -0.17194384  0.23851079 -0.4233073 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1152 is [True, False, False, False, True, False]
Scene graph at timestep 1152 is [True, False, False, False, True, False]
State prediction error at timestep 1152 is tensor(7.1047e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1152 of 1
Current timestep = 1153. State = [[-0.15636267 -0.03127433]]. Action = [[ 0.08428347 -0.14859657 -0.24819416  0.11532545]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1153 is [True, False, False, False, True, False]
Current timestep = 1154. State = [[-0.14636518 -0.04297227]]. Action = [[ 0.03122589 -0.01275885 -0.03505477  0.4902227 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1154 is [True, False, False, False, True, False]
Scene graph at timestep 1154 is [True, False, False, False, True, False]
State prediction error at timestep 1154 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1154 of 1
Current timestep = 1155. State = [[-0.13635309 -0.03892076]]. Action = [[ 0.12600583  0.12879527 -0.15268324  0.78400266]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1155 is [True, False, False, False, True, False]
Scene graph at timestep 1155 is [True, False, False, False, True, False]
State prediction error at timestep 1155 is tensor(2.2484e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1155 of 1
Current timestep = 1156. State = [[-0.11801721 -0.03712635]]. Action = [[ 0.14099026 -0.14780094 -0.16033687  0.41725755]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1156 is [True, False, False, False, True, False]
Scene graph at timestep 1156 is [True, False, False, False, True, False]
State prediction error at timestep 1156 is tensor(4.9529e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1156 of 1
Current timestep = 1157. State = [[-0.10438614 -0.05337743]]. Action = [[-0.07045993 -0.09798476  0.0853287  -0.1313029 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1157 is [True, False, False, False, True, False]
Scene graph at timestep 1157 is [True, False, False, False, True, False]
State prediction error at timestep 1157 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1157 of 1
Current timestep = 1158. State = [[-0.1077265  -0.05101345]]. Action = [[-0.11246929  0.19940531 -0.08699855 -0.509721  ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1158 is [True, False, False, False, True, False]
Current timestep = 1159. State = [[-0.10560824 -0.02841584]]. Action = [[ 0.17398351  0.20568848  0.0573805  -0.8343405 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1159 is [True, False, False, False, True, False]
Current timestep = 1160. State = [[-0.09948642 -0.01381763]]. Action = [[ 0.08496276 -0.01998916 -0.05824183  0.6537398 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1160 is [True, False, False, False, True, False]
Scene graph at timestep 1160 is [True, False, False, False, True, False]
State prediction error at timestep 1160 is tensor(7.8269e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1160 of 1
Current timestep = 1161. State = [[-0.08414452  0.00223151]]. Action = [[0.14092976 0.20419627 0.20601958 0.4043572 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1161 is [True, False, False, False, True, False]
Scene graph at timestep 1161 is [True, False, False, False, True, False]
State prediction error at timestep 1161 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1161 of 1
Current timestep = 1162. State = [[-0.141841   -0.02113716]]. Action = [[ 0.06599519  0.22178009  0.1608181  -0.82622826]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1162 is [True, False, False, False, True, False]
Scene graph at timestep 1162 is [True, False, False, False, True, False]
State prediction error at timestep 1162 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1162 of 1
Current timestep = 1163. State = [[-0.11253905 -0.03565151]]. Action = [[ 0.20143712 -0.16081426  0.13866246  0.5030577 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1163 is [True, False, False, False, True, False]
Current timestep = 1164. State = [[-0.10303119 -0.04530104]]. Action = [[-0.21174082  0.02603531 -0.08528742  0.44748402]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1164 is [True, False, False, False, True, False]
Current timestep = 1165. State = [[-0.10617427 -0.06162322]]. Action = [[-0.04473308 -0.23326318  0.11824796  0.93293834]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1165 is [True, False, False, False, True, False]
Current timestep = 1166. State = [[-0.10882529 -0.06385   ]]. Action = [[0.11753371 0.23829776 0.04352999 0.48138392]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1166 is [True, False, False, False, True, False]
Scene graph at timestep 1166 is [True, False, False, False, True, False]
State prediction error at timestep 1166 is tensor(7.2957e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1166 of 0
Current timestep = 1167. State = [[-0.11218204 -0.03582564]]. Action = [[-0.17644629  0.23396528  0.21815601 -0.11425668]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1167 is [True, False, False, False, True, False]
Current timestep = 1168. State = [[-0.12591138 -0.00799364]]. Action = [[-0.18922897  0.14915785  0.02182668 -0.13665521]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1168 is [True, False, False, False, True, False]
Current timestep = 1169. State = [[-0.13493457 -0.00371559]]. Action = [[ 0.2120232  -0.1500537  -0.1077438   0.49269307]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1169 is [True, False, False, False, True, False]
Scene graph at timestep 1169 is [True, False, False, False, True, False]
State prediction error at timestep 1169 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1169 of 1
Current timestep = 1170. State = [[-0.13451478 -0.00342444]]. Action = [[-0.06054825  0.13644788  0.08330548  0.8575809 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1170 is [True, False, False, False, True, False]
Current timestep = 1171. State = [[-0.13485679  0.00640646]]. Action = [[ 0.06936374  0.06265047 -0.23024897  0.18520069]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1171 is [True, False, False, False, True, False]
Current timestep = 1172. State = [[-0.12687697  0.01363546]]. Action = [[0.1504333  0.05540839 0.12351441 0.19400942]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1172 is [True, False, False, False, True, False]
Current timestep = 1173. State = [[-0.106411    0.02343971]]. Action = [[ 0.20197254  0.07779139  0.09778374 -0.3023033 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1173 is [True, False, False, False, True, False]
Current timestep = 1174. State = [[-0.09182445  0.02887787]]. Action = [[-0.07888538 -0.02629724 -0.21781598  0.5964637 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1174 is [True, False, False, False, True, False]
Current timestep = 1175. State = [[-0.09253981  0.02189033]]. Action = [[-0.12963295 -0.14492583 -0.0387408  -0.75529236]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1175 is [True, False, False, False, True, False]
Scene graph at timestep 1175 is [True, False, False, False, True, False]
State prediction error at timestep 1175 is tensor(3.4649e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1175 of 1
Current timestep = 1176. State = [[-0.09733371  0.00696432]]. Action = [[-0.09952976 -0.08295579  0.23861802 -0.63049483]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1176 is [True, False, False, False, True, False]
Current timestep = 1177. State = [[-0.10115003 -0.00357019]]. Action = [[ 0.08234933 -0.07181814  0.16988176 -0.8749637 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1177 is [True, False, False, False, True, False]
Current timestep = 1178. State = [[-0.10339571 -0.00702621]]. Action = [[-0.07589851  0.02279335 -0.05145513  0.0649271 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1178 is [True, False, False, False, True, False]
Current timestep = 1179. State = [[-0.10625738  0.00487155]]. Action = [[ 0.00752774  0.21290678  0.18701118 -0.7976074 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1179 is [True, False, False, False, True, False]
Current timestep = 1180. State = [[-0.11539345  0.00857961]]. Action = [[-0.19121012 -0.17177509 -0.02616429 -0.7886578 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1180 is [True, False, False, False, True, False]
Scene graph at timestep 1180 is [True, False, False, False, True, False]
State prediction error at timestep 1180 is tensor(5.1165e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1180 of -1
Current timestep = 1181. State = [[-0.12460822 -0.01438488]]. Action = [[ 0.12712035 -0.21910998  0.1349271   0.7956698 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1181 is [True, False, False, False, True, False]
Current timestep = 1182. State = [[-0.12336969 -0.02456332]]. Action = [[-0.06482403  0.0576224  -0.15422904 -0.8538611 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1182 is [True, False, False, False, True, False]
Current timestep = 1183. State = [[-0.1211588  -0.03866255]]. Action = [[ 0.13106048 -0.2321974  -0.1432889  -0.5460126 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1183 is [True, False, False, False, True, False]
Current timestep = 1184. State = [[-0.11154179 -0.06074515]]. Action = [[ 0.15316314 -0.08047074 -0.16804948  0.76019204]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1184 is [True, False, False, False, True, False]
Current timestep = 1185. State = [[-0.09134078 -0.05777737]]. Action = [[ 0.23311704  0.18038869 -0.12038325  0.6291754 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1185 is [True, False, False, False, True, False]
Current timestep = 1186. State = [[-0.07830036 -0.0572056 ]]. Action = [[-0.19183524 -0.10790715 -0.03730221  0.8277317 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1186 is [True, False, False, False, True, False]
Current timestep = 1187. State = [[-0.07846098 -0.07400684]]. Action = [[ 0.06805843 -0.21409178 -0.12612551 -0.07426953]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1187 is [True, False, False, False, True, False]
Current timestep = 1188. State = [[-0.06896981 -0.07851302]]. Action = [[0.22511262 0.15430215 0.0973987  0.242517  ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1188 is [True, False, False, False, True, False]
Current timestep = 1189. State = [[-0.05935507 -0.06455482]]. Action = [[-0.03492188  0.14257675  0.03042966 -0.8067457 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1189 is [True, False, False, False, True, False]
Current timestep = 1190. State = [[-0.05093457 -0.04809861]]. Action = [[ 0.20592183  0.11689848  0.23045868 -0.40297282]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1190 is [True, False, False, False, True, False]
Current timestep = 1191. State = [[-0.14717543 -0.20416877]]. Action = [[ 0.11236119 -0.07611495 -0.21167141  0.5733855 ]]. Reward = [100.]
Curr episode timestep = 28
Scene graph at timestep 1191 is [True, False, False, False, True, False]
Scene graph at timestep 1191 is [True, False, False, True, False, False]
State prediction error at timestep 1191 is tensor(0.0188, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1191 of 1
Current timestep = 1192. State = [[-0.12644432 -0.24114439]]. Action = [[ 0.04588899 -0.21234398  0.20546287 -0.598262  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1192 is [True, False, False, True, False, False]
Current timestep = 1193. State = [[-0.13016602 -0.27021897]]. Action = [[-0.13634466 -0.19667095 -0.14255024 -0.5513247 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1193 is [True, False, False, True, False, False]
Current timestep = 1194. State = [[-0.13867539 -0.29862502]]. Action = [[-0.00281967 -0.20520861  0.10363612 -0.36797905]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1194 is [True, False, False, True, False, False]
Scene graph at timestep 1194 is [True, False, False, True, False, False]
State prediction error at timestep 1194 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1194 of -1
Current timestep = 1195. State = [[-0.1477259 -0.3090056]]. Action = [[-0.19413698  0.2327156  -0.24228331 -0.5774329 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1195 is [True, False, False, True, False, False]
Current timestep = 1196. State = [[-0.14561963 -0.28764424]]. Action = [[ 0.11490291  0.2377041  -0.14621629 -0.67860305]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1196 is [True, False, False, True, False, False]
Current timestep = 1197. State = [[-0.1351365 -0.2588629]]. Action = [[ 0.18069267  0.10561433  0.01747003 -0.04674059]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1197 is [True, False, False, True, False, False]
Current timestep = 1198. State = [[-0.11833081 -0.23541732]]. Action = [[ 0.22017032  0.1370951   0.01266307 -0.79423094]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1198 is [True, False, False, True, False, False]
Current timestep = 1199. State = [[-0.09399687 -0.21309146]]. Action = [[ 0.13035059  0.18028638 -0.09911036  0.924404  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1199 is [True, False, False, True, False, False]
Current timestep = 1200. State = [[-0.08505252 -0.20395619]]. Action = [[-0.08819366 -0.09541184  0.21382606  0.6827241 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1200 is [True, False, False, True, False, False]
Scene graph at timestep 1200 is [True, False, False, True, False, False]
State prediction error at timestep 1200 is tensor(6.4370e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1200 of 1
Current timestep = 1201. State = [[-0.08899343 -0.21843691]]. Action = [[-0.02080655 -0.19723676  0.09581318 -0.41088033]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1201 is [True, False, False, True, False, False]
Current timestep = 1202. State = [[-0.08078574 -0.22115222]]. Action = [[ 0.22567433  0.15666097 -0.07657352 -0.03153884]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1202 is [True, False, False, True, False, False]
Current timestep = 1203. State = [[-0.05748339 -0.2246559 ]]. Action = [[ 0.16536671 -0.16491455 -0.21274494 -0.20225585]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1203 is [True, False, False, True, False, False]
Current timestep = 1204. State = [[-0.03205676 -0.23371495]]. Action = [[ 0.17813164 -0.04779816  0.2340459  -0.08486819]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1204 is [True, False, False, True, False, False]
Scene graph at timestep 1204 is [False, True, False, True, False, False]
State prediction error at timestep 1204 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1204 of 0
Current timestep = 1205. State = [[-0.00404995 -0.23887026]]. Action = [[ 0.1662333   0.02854785 -0.19934036  0.55175567]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1205 is [False, True, False, True, False, False]
Current timestep = 1206. State = [[ 0.0078509 -0.2376928]]. Action = [[-0.13715774  0.06179658 -0.08774868 -0.16653222]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1206 is [False, True, False, True, False, False]
Current timestep = 1207. State = [[ 0.00904153 -0.22373249]]. Action = [[ 0.05098346  0.2081582  -0.01568078  0.9368875 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1207 is [False, True, False, True, False, False]
Current timestep = 1208. State = [[ 0.01579743 -0.21717092]]. Action = [[ 0.1869142  -0.20138459 -0.19663875  0.7108494 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1208 is [False, True, False, True, False, False]
Current timestep = 1209. State = [[ 0.02663636 -0.21755123]]. Action = [[-0.07967143  0.15347475 -0.17003351 -0.62158257]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1209 is [False, True, False, True, False, False]
Current timestep = 1210. State = [[ 0.02598444 -0.20065093]]. Action = [[-0.13301669  0.22863334 -0.0169615   0.25032055]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1210 is [False, True, False, True, False, False]
Current timestep = 1211. State = [[ 0.02470325 -0.19252141]]. Action = [[ 0.13471413 -0.17026512  0.06797889  0.5437585 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1211 is [False, True, False, True, False, False]
Current timestep = 1212. State = [[ 0.0317557  -0.20810843]]. Action = [[ 0.2138705  -0.21123804  0.06893459  0.16322756]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1212 is [False, True, False, True, False, False]
Current timestep = 1213. State = [[ 0.05279058 -0.21751489]]. Action = [[ 0.10773307  0.08882481  0.15703973 -0.64935607]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1213 is [False, True, False, True, False, False]
Current timestep = 1214. State = [[ 0.06730919 -0.21605712]]. Action = [[ 0.11003423  0.04742488  0.18031025 -0.00068772]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1214 is [False, False, True, True, False, False]
Scene graph at timestep 1214 is [False, False, True, True, False, False]
State prediction error at timestep 1214 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1214 of -1
Current timestep = 1215. State = [[ 0.06951462 -0.2041139 ]]. Action = [[-0.00272618  0.22159535 -0.2446607   0.07431245]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1215 is [False, False, True, True, False, False]
Current timestep = 1216. State = [[ 0.06953842 -0.19448788]]. Action = [[-0.07883759 -0.06422231  0.17393106  0.16377735]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1216 is [False, False, True, True, False, False]
Current timestep = 1217. State = [[ 0.06878456 -0.19682613]]. Action = [[ 0.21605301  0.14341289 -0.16606241  0.67857575]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1217 is [False, False, True, True, False, False]
Current timestep = 1218. State = [[ 0.06858941 -0.19741337]]. Action = [[-0.00327599 -0.11727583 -0.10091999  0.6931164 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1218 is [False, False, True, True, False, False]
Current timestep = 1219. State = [[ 0.06849578 -0.19743893]]. Action = [[ 0.18806344  0.09696013 -0.17000826 -0.6592606 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1219 is [False, False, True, True, False, False]
Current timestep = 1220. State = [[ 0.06849578 -0.19743893]]. Action = [[ 0.2328574  -0.16345668 -0.20381305  0.34602845]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1220 is [False, False, True, True, False, False]
Scene graph at timestep 1220 is [False, False, True, True, False, False]
State prediction error at timestep 1220 is tensor(3.2671e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1220 of 0
Current timestep = 1221. State = [[ 0.06548192 -0.20791051]]. Action = [[-0.12561226 -0.14937782 -0.00472991 -0.42597365]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1221 is [False, False, True, True, False, False]
Scene graph at timestep 1221 is [False, False, True, True, False, False]
State prediction error at timestep 1221 is tensor(9.0474e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1221 of -1
Current timestep = 1222. State = [[ 0.06247424 -0.20652708]]. Action = [[ 0.01324865  0.217816    0.1658347  -0.5913262 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1222 is [False, False, True, True, False, False]
Scene graph at timestep 1222 is [False, False, True, True, False, False]
State prediction error at timestep 1222 is tensor(7.9189e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1222 of 0
Current timestep = 1223. State = [[ 0.06319778 -0.19081055]]. Action = [[ 0.2180717   0.1093874  -0.08107345 -0.0187093 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1223 is [False, False, True, True, False, False]
Current timestep = 1224. State = [[ 0.06319778 -0.19081055]]. Action = [[ 0.05271411 -0.00494313  0.0464206  -0.90572345]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1224 is [False, False, True, True, False, False]
Current timestep = 1225. State = [[ 0.0632937  -0.18329105]]. Action = [[-0.04354538  0.11389521  0.19018489 -0.80835426]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1225 is [False, False, True, True, False, False]
Current timestep = 1226. State = [[ 0.06285148 -0.17567931]]. Action = [[ 0.19386947 -0.23031056 -0.03685644  0.6961322 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1226 is [False, False, True, True, False, False]
Current timestep = 1227. State = [[ 0.0628439  -0.17449324]]. Action = [[ 0.1293253   0.13253343 -0.16975385  0.16448414]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1227 is [False, False, True, True, False, False]
Scene graph at timestep 1227 is [False, False, True, True, False, False]
State prediction error at timestep 1227 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1227 of 1
Current timestep = 1228. State = [[ 0.06275095 -0.17450015]]. Action = [[ 0.22880799 -0.0021418   0.15575475  0.11158216]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1228 is [False, False, True, True, False, False]
Current timestep = 1229. State = [[ 0.06270431 -0.17450361]]. Action = [[ 0.19726619 -0.06158593  0.07807508  0.9151801 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1229 is [False, False, True, True, False, False]
Current timestep = 1230. State = [[ 0.06270431 -0.17450361]]. Action = [[ 0.19649252 -0.04119368  0.01453519  0.23082697]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1230 is [False, False, True, True, False, False]
Current timestep = 1231. State = [[ 0.05862626 -0.16645558]]. Action = [[-0.12433437  0.13396269  0.24831295 -0.72170854]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1231 is [False, False, True, True, False, False]
Current timestep = 1232. State = [[ 0.05335486 -0.1465462 ]]. Action = [[0.05187625 0.12713656 0.16186237 0.14008725]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1232 is [False, False, True, True, False, False]
Scene graph at timestep 1232 is [False, False, True, True, False, False]
State prediction error at timestep 1232 is tensor(4.8091e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1232 of 1
Current timestep = 1233. State = [[ 0.05304032 -0.13413121]]. Action = [[ 0.03832722 -0.00706367 -0.11416355 -0.1868707 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1233 is [False, False, True, True, False, False]
Scene graph at timestep 1233 is [False, False, True, True, False, False]
State prediction error at timestep 1233 is tensor(1.3272e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1233 of 1
Current timestep = 1234. State = [[ 0.0532244  -0.13395151]]. Action = [[ 0.20786572 -0.17432481  0.21328855 -0.03930795]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1234 is [False, False, True, True, False, False]
Current timestep = 1235. State = [[ 0.05354827 -0.13436663]]. Action = [[ 0.05253899 -0.03582346  0.14499176 -0.69123715]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1235 is [False, False, True, True, False, False]
Current timestep = 1236. State = [[ 0.05352866 -0.13467719]]. Action = [[ 0.1826902  -0.03406939 -0.01783074 -0.5848066 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1236 is [False, False, True, True, False, False]
Current timestep = 1237. State = [[ 0.05352866 -0.13467719]]. Action = [[0.18828022 0.20825875 0.0117811  0.78791404]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1237 is [False, False, True, True, False, False]
Current timestep = 1238. State = [[ 0.05352866 -0.13467719]]. Action = [[0.08576202 0.03913105 0.19223478 0.18251753]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1238 is [False, False, True, True, False, False]
Scene graph at timestep 1238 is [False, False, True, True, False, False]
State prediction error at timestep 1238 is tensor(5.2006e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1238 of 1
Current timestep = 1239. State = [[ 0.05352866 -0.13467719]]. Action = [[ 0.09699589 -0.10756522 -0.1470914   0.91310024]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1239 is [False, False, True, True, False, False]
Scene graph at timestep 1239 is [False, False, True, True, False, False]
State prediction error at timestep 1239 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1239 of 1
Current timestep = 1240. State = [[ 0.05345377 -0.12696074]]. Action = [[-0.03471002  0.14300036  0.07520595  0.5080373 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1240 is [False, False, True, True, False, False]
Current timestep = 1241. State = [[ 0.04629534 -0.13177197]]. Action = [[-0.20628877 -0.18799789 -0.00164716  0.06813431]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1241 is [False, False, True, True, False, False]
Current timestep = 1242. State = [[ 0.03737935 -0.14282942]]. Action = [[ 0.21119002  0.18385088 -0.20219919 -0.71948975]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1242 is [False, True, False, True, False, False]
Current timestep = 1243. State = [[ 0.03213458 -0.15144481]]. Action = [[-0.06479786 -0.09421121 -0.2287453   0.7048342 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1243 is [False, True, False, True, False, False]
Current timestep = 1244. State = [[ 0.03062479 -0.1465018 ]]. Action = [[0.18040934 0.21937096 0.16332984 0.20693624]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1244 is [False, True, False, True, False, False]
Scene graph at timestep 1244 is [False, True, False, True, False, False]
State prediction error at timestep 1244 is tensor(1.0663e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1244 of 1
Current timestep = 1245. State = [[ 0.04058351 -0.11435327]]. Action = [[ 0.17553735  0.20904529  0.20136845 -0.48817825]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1245 is [False, True, False, True, False, False]
Current timestep = 1246. State = [[ 0.04632175 -0.09885111]]. Action = [[ 0.18286407 -0.01214275 -0.06424835  0.8618331 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1246 is [False, True, False, False, True, False]
Current timestep = 1247. State = [[ 0.04914635 -0.08549868]]. Action = [[ 0.03896895  0.18007436 -0.09320548  0.29456878]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1247 is [False, True, False, False, True, False]
Current timestep = 1248. State = [[ 0.05370769 -0.07089474]]. Action = [[ 0.13115475 -0.02285703  0.03562081 -0.8008211 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1248 is [False, True, False, False, True, False]
Scene graph at timestep 1248 is [False, False, True, False, True, False]
State prediction error at timestep 1248 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1248 of 1
Current timestep = 1249. State = [[ 0.05454341 -0.06874213]]. Action = [[ 0.11794031  0.19869474 -0.01712017 -0.90372837]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1249 is [False, False, True, False, True, False]
