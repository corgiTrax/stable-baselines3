Current timestep = 0. State = [[-0.20357977  0.22999147]]. Action = [[ 0.06299114 -0.07998601  0.1021077   0.6932514 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Current timestep = 1. State = [[-0.21054037  0.2375194 ]]. Action = [[-0.2329391   0.16465718 -0.17286141 -0.9050274 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, False, True]
Scene graph at timestep 1 is [True, False, False, False, False, True]
State prediction error at timestep 1 is tensor(0.0601, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.21850206  0.24085867]]. Action = [[ 0.11275035 -0.17566247  0.23866415 -0.8213498 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, False, True]
Current timestep = 3. State = [[-0.21602239  0.21835305]]. Action = [[-0.15694897 -0.21790281 -0.08748233 -0.94051874]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, False, True]
Scene graph at timestep 3 is [True, False, False, False, False, True]
State prediction error at timestep 3 is tensor(0.0296, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3 of 1
Current timestep = 4. State = [[-0.22782883  0.19834986]]. Action = [[-0.22496887 -0.08034375 -0.01220685 -0.9660438 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, False, True]
Current timestep = 5. State = [[-0.25281352  0.20584738]]. Action = [[-0.15817     0.22565845 -0.17806064 -0.8143887 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, False, True]
Current timestep = 6. State = [[-0.2687916   0.22169468]]. Action = [[-0.22982308 -0.23628718 -0.01327302 -0.9404466 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, False, True]
Current timestep = 7. State = [[-0.262916    0.21152796]]. Action = [[ 0.24866676 -0.16096264  0.10772052 -0.43771684]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, False, True]
Scene graph at timestep 7 is [True, False, False, False, False, True]
State prediction error at timestep 7 is tensor(0.0186, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.25423595  0.1993901 ]]. Action = [[-0.12280184 -0.00727075  0.23059183  0.6920028 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, False, True]
Current timestep = 9. State = [[-0.2558773   0.20925897]]. Action = [[0.11643142 0.22311422 0.18114626 0.40296865]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, False, True]
Scene graph at timestep 9 is [True, False, False, False, False, True]
State prediction error at timestep 9 is tensor(0.0232, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9 of -1
Current timestep = 10. State = [[-0.2576277   0.21906613]]. Action = [[-0.1154575  -0.11681288 -0.12393151 -0.8623228 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, False, True]
Scene graph at timestep 10 is [True, False, False, False, False, True]
State prediction error at timestep 10 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 10 of -1
Current timestep = 11. State = [[-0.2543122   0.22093695]]. Action = [[ 0.09638867  0.01169002 -0.1869633  -0.25790083]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, False, True]
Current timestep = 12. State = [[-0.24039268  0.22064593]]. Action = [[ 0.13120109 -0.05235311  0.2440505  -0.14235401]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, False, True]
Scene graph at timestep 12 is [True, False, False, False, False, True]
State prediction error at timestep 12 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 12 of 1
Current timestep = 13. State = [[-0.2307291  0.2182774]]. Action = [[-0.16041227 -0.07791942 -0.11028329 -0.09639537]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, False, True]
Scene graph at timestep 13 is [True, False, False, False, False, True]
State prediction error at timestep 13 is tensor(0.0083, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of 0
Current timestep = 14. State = [[-0.23970823  0.2097995 ]]. Action = [[-0.1948994  -0.09049852  0.20018348 -0.14339006]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, False, True]
Current timestep = 15. State = [[-0.25178596  0.20409526]]. Action = [[-0.23052488 -0.15175709 -0.2289426  -0.0836314 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, False, True]
Current timestep = 16. State = [[-0.25563762  0.20399125]]. Action = [[ 0.02983895  0.05126569 -0.15928535 -0.69111896]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, False, True]
Scene graph at timestep 16 is [True, False, False, False, False, True]
State prediction error at timestep 16 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of -1
Current timestep = 17. State = [[-0.25177532  0.21142572]]. Action = [[ 0.17593664  0.1575582   0.24485326 -0.9516652 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, False, True]
Current timestep = 18. State = [[-0.24234577  0.2145294 ]]. Action = [[ 0.09592223 -0.0786024   0.18916541  0.5762372 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, False, True]
Current timestep = 19. State = [[-0.22713093  0.21070756]]. Action = [[ 0.19591445 -0.04575925 -0.21415243  0.80007076]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, False, True]
Scene graph at timestep 19 is [True, False, False, False, False, True]
State prediction error at timestep 19 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 19 of 1
Current timestep = 20. State = [[-0.19908035  0.205535  ]]. Action = [[ 0.19702232 -0.02232911 -0.20242174  0.00102615]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, False, True]
Current timestep = 21. State = [[-0.17683329  0.20705113]]. Action = [[0.17274463 0.07057807 0.24114186 0.7103584 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, False, True]
Current timestep = 22. State = [[-0.16073291  0.20170647]]. Action = [[-0.10603704 -0.20992558  0.10126397  0.85412955]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, False, True]
Current timestep = 23. State = [[-0.15923785  0.19326748]]. Action = [[ 0.02302521  0.04965425 -0.02515706 -0.910111  ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, False, True]
Scene graph at timestep 23 is [True, False, False, False, False, True]
State prediction error at timestep 23 is tensor(0.0100, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 23 of 1
Current timestep = 24. State = [[-0.16089346  0.18674415]]. Action = [[-0.14971867 -0.1461225  -0.01206546  0.40052462]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, False, True]
Current timestep = 25. State = [[-0.16887636  0.1672192 ]]. Action = [[-0.11605197 -0.18586805  0.16705674  0.683061  ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, False, True]
Current timestep = 26. State = [[-0.18564074  0.14165875]]. Action = [[-0.18502787 -0.18133485 -0.03079091 -0.29262257]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, False, True]
Scene graph at timestep 26 is [True, False, False, False, False, True]
State prediction error at timestep 26 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 26 of -1
Current timestep = 27. State = [[-0.20759854  0.11372562]]. Action = [[-0.11470839 -0.14264889 -0.17453988  0.7011161 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, False, True]
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 27 of -1
Current timestep = 28. State = [[-0.22732902  0.10648605]]. Action = [[-0.19362155  0.07968533  0.10513929  0.9689052 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
Current timestep = 29. State = [[-0.2455562   0.10619928]]. Action = [[-0.02048369 -0.07589945 -0.20151421 -0.19303441]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
Current timestep = 30. State = [[-0.25433373  0.10503837]]. Action = [[-0.13519092  0.02002528  0.20684332  0.45992672]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
Current timestep = 31. State = [[-0.26879576  0.09255558]]. Action = [[ 0.00208434 -0.17233321 -0.10666311 -0.97067887]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
Current timestep = 32. State = [[-0.266878   0.0904633]]. Action = [[0.14406586 0.20221853 0.20734179 0.6762588 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
Current timestep = 33. State = [[-0.26516902  0.09652532]]. Action = [[-0.06591684 -0.17768939 -0.07548828 -0.524211  ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
Current timestep = 34. State = [[-0.26388326  0.08882283]]. Action = [[-0.05571708 -0.1934398   0.01505733 -0.179605  ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
Current timestep = 35. State = [[-0.26343605  0.0918559 ]]. Action = [[0.0659551  0.2050038  0.18712544 0.64422464]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
Current timestep = 36. State = [[-0.25482574  0.08886697]]. Action = [[ 0.18217677 -0.17316766  0.0272381  -0.79851604]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
Current timestep = 37. State = [[-0.2353922   0.08882953]]. Action = [[ 0.22988892  0.16458824 -0.10274896 -0.4427516 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
Current timestep = 38. State = [[-0.21744873  0.08495184]]. Action = [[-0.04621463 -0.2195774  -0.11757979 -0.70103765]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
Current timestep = 39. State = [[-0.20936859  0.07233825]]. Action = [[ 0.13512987 -0.02204596 -0.09840125  0.30343938]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
Current timestep = 40. State = [[-0.1989383   0.05684777]]. Action = [[ 0.08722281 -0.1774906  -0.12306759  0.4860556 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
Current timestep = 41. State = [[-0.19101368  0.03355774]]. Action = [[-0.04887094 -0.15075305  0.2461128  -0.23762107]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.1912534   0.01765124]]. Action = [[-0.13126563 -0.02160195 -0.09736693 -0.6637713 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of 0
Current timestep = 43. State = [[-0.19428577  0.02289064]]. Action = [[ 0.0973036   0.1456719  -0.15411685 -0.7885568 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
Current timestep = 44. State = [[-0.1951382   0.04030962]]. Action = [[-0.00407575  0.17703927  0.0540117  -0.86468047]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 44 of 0
Current timestep = 45. State = [[-0.20059745  0.05986238]]. Action = [[-0.14759757  0.03787076 -0.24269103 -0.49174798]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 45 of -1
Current timestep = 46. State = [[-0.21115737  0.07425348]]. Action = [[-0.15426838  0.1282489   0.15308493  0.24327719]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of -1
Current timestep = 47. State = [[-0.23025885  0.08850469]]. Action = [[-0.13087378  0.02050763 -0.06895782  0.69170356]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
Current timestep = 48. State = [[-0.23450384  0.08012342]]. Action = [[ 0.15423852 -0.14695501 -0.19683632  0.60153866]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
Current timestep = 49. State = [[-0.2391403   0.06748464]]. Action = [[-0.19781162 -0.05786973 -0.23975784 -0.72052896]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
Current timestep = 50. State = [[-0.24495086  0.05937583]]. Action = [[-0.03940111 -0.0307337   0.17405617 -0.612953  ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
Current timestep = 51. State = [[-0.24714243  0.05634903]]. Action = [[-0.23216768 -0.19794223  0.09597993  0.50811887]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 51 of -1
Current timestep = 52. State = [[-0.24242823  0.04359637]]. Action = [[ 0.1638028  -0.2156191  -0.22166194 -0.29574656]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
Current timestep = 53. State = [[-0.23172617  0.03556504]]. Action = [[0.15915838 0.13234437 0.19454277 0.7946633 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
Current timestep = 54. State = [[-0.22288992  0.02615167]]. Action = [[ 0.07363528 -0.21741964  0.15036497 -0.02982748]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
Current timestep = 55. State = [[-0.21900095  0.02448422]]. Action = [[-0.05827151  0.23536813  0.01808783  0.02052271]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
Current timestep = 56. State = [[-0.21127117  0.04423332]]. Action = [[0.23178291 0.13817132 0.22842193 0.5828531 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of 1
Current timestep = 57. State = [[-0.19474074  0.05342365]]. Action = [[-0.04437174 -0.13358162  0.09351355 -0.9059424 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
Current timestep = 58. State = [[-0.18837713  0.03632155]]. Action = [[ 0.12497193 -0.19204985 -0.16386025  0.9186673 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 58 of 1
Current timestep = 59. State = [[-0.18072237  0.01519185]]. Action = [[ 0.02922246 -0.04242198 -0.01555136  0.91042995]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
Current timestep = 60. State = [[-0.16974011  0.01520359]]. Action = [[0.21489483 0.05808735 0.02044186 0.2929926 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
Current timestep = 61. State = [[-0.15703475  0.0110529 ]]. Action = [[-0.16002432 -0.13178147  0.09690547 -0.08370566]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
Current timestep = 62. State = [[-0.15073204 -0.00531102]]. Action = [[ 0.23135543 -0.15212183 -0.00261816  0.40564   ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
Current timestep = 63. State = [[-0.1428609  -0.02363089]]. Action = [[-0.11245522 -0.06549814 -0.19752221  0.87373376]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
Current timestep = 64. State = [[-0.14419186 -0.0337925 ]]. Action = [[-0.0309701  -0.06828257  0.2095908  -0.53540635]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
Current timestep = 65. State = [[-0.1429364  -0.03931211]]. Action = [[ 1.5381885e-01 -3.4563243e-04 -2.3665883e-01 -6.1603487e-01]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
Current timestep = 66. State = [[-0.14319623 -0.04646267]]. Action = [[-0.1573951  -0.08774555 -0.23236305 -0.80643326]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(0.0132, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 66 of 1
Current timestep = 67. State = [[-0.15258227 -0.05512916]]. Action = [[-0.2001618  -0.01193093 -0.21790312 -0.35527456]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0091, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.16963208 -0.04791525]]. Action = [[-0.11528672  0.23363137  0.12418193  0.85107803]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
Current timestep = 69. State = [[-0.17517202 -0.03277491]]. Action = [[ 0.20112672 -0.04378456  0.18174553  0.18226504]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
Current timestep = 70. State = [[-0.16930856 -0.02756351]]. Action = [[ 0.05215478  0.09643486 -0.12532026 -0.18616015]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of 0
Current timestep = 71. State = [[-0.17081495 -0.00911576]]. Action = [[-0.22173572  0.18951035 -0.23749608  0.4931414 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
Current timestep = 72. State = [[-0.17288965  0.0040413 ]]. Action = [[ 0.20071548 -0.03631231 -0.12185422  0.6287627 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
Current timestep = 73. State = [[-0.17278345  0.00091218]]. Action = [[-0.15377283 -0.06896102 -0.0556128   0.08666515]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
Current timestep = 74. State = [[-0.1715212  -0.00502891]]. Action = [[ 0.16581103 -0.04130678 -0.12584615  0.20809829]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
Current timestep = 75. State = [[-0.16333543 -0.00809187]]. Action = [[0.12705746 0.00674951 0.04727161 0.9874699 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
Current timestep = 76. State = [[-0.15511467 -0.02205686]]. Action = [[-0.11142462 -0.23876503 -0.04135534  0.5729294 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
Current timestep = 77. State = [[-0.14964981 -0.0373232 ]]. Action = [[ 0.14109182  0.02168471 -0.09212917 -0.02043653]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
Current timestep = 78. State = [[-0.14299347 -0.04438095]]. Action = [[ 0.12701094 -0.07499228 -0.08459747  0.34485245]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
Current timestep = 79. State = [[-0.13710435 -0.04441926]]. Action = [[-0.1900694   0.10472882  0.22693259 -0.33679867]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of 0
Current timestep = 80. State = [[-0.13611265 -0.05240636]]. Action = [[ 0.12646562 -0.2195519   0.23353297 -0.8010603 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
Current timestep = 81. State = [[-0.13117065 -0.06621572]]. Action = [[ 0.08455399 -0.01382607  0.16575381 -0.50052726]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
Current timestep = 82. State = [[-0.12235658 -0.06489265]]. Action = [[0.09748289 0.08420566 0.23622945 0.03965485]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
Current timestep = 83. State = [[-0.11516277 -0.06068139]]. Action = [[-0.08654904  0.01488444  0.24371538 -0.64669836]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
Current timestep = 84. State = [[-0.12174304 -0.05013845]]. Action = [[-0.21161732  0.1545521  -0.04170756 -0.6584152 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
Current timestep = 85. State = [[-0.12943773 -0.04253661]]. Action = [[ 0.07853985 -0.0712145   0.17076498 -0.5269921 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 85 of 0
Current timestep = 86. State = [[-0.13108775 -0.03475382]]. Action = [[-0.02430612  0.16783056 -0.16852649  0.17912674]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
Current timestep = 87. State = [[-0.13747913 -0.01449781]]. Action = [[-0.15905276  0.1778512  -0.22182074  0.20821106]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
Current timestep = 88. State = [[-0.1428872  -0.00064147]]. Action = [[ 0.08200282 -0.01906662  0.21266279 -0.35716033]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of -1
Current timestep = 89. State = [[-0.14373554  0.00833775]]. Action = [[ 0.01742026  0.13099217  0.09698719 -0.683086  ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 89 of -1
Current timestep = 90. State = [[-0.13814478  0.01321657]]. Action = [[ 0.1893433  -0.09336811 -0.03843041 -0.9378988 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of 1
Current timestep = 91. State = [[-0.13497059  0.00194491]]. Action = [[-0.18465564 -0.13258724  0.09485364 -0.21607864]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
Current timestep = 92. State = [[-0.13705204 -0.00516863]]. Action = [[ 0.01412669 -0.01618031  0.12927318  0.8912897 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
Current timestep = 93. State = [[-0.13378173 -0.01774026]]. Action = [[ 0.11077029 -0.15260921  0.22441465  0.19944382]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
Current timestep = 94. State = [[-0.12672013 -0.02415843]]. Action = [[0.11884934 0.12418848 0.21744347 0.24159694]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 0
Current timestep = 95. State = [[-0.1217709  -0.02284192]]. Action = [[-0.15805963 -0.08273958 -0.11456515  0.8282392 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
Current timestep = 96. State = [[-0.13552202 -0.03937833]]. Action = [[-0.20464544 -0.20217678 -0.07010075 -0.7238252 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
Current timestep = 97. State = [[-0.14558762 -0.05005182]]. Action = [[ 0.10922799  0.08086079 -0.1698318  -0.6521768 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
Current timestep = 98. State = [[-0.15066843 -0.03700468]]. Action = [[-0.19906676  0.21484962 -0.2176789  -0.16799688]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
Current timestep = 99. State = [[-0.16409694 -0.01122053]]. Action = [[-0.06224367  0.1409271   0.17270666 -0.51801693]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
Current timestep = 100. State = [[-0.17740226 -0.00642706]]. Action = [[-0.15744218 -0.11600907 -0.02131894 -0.46658194]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 100 of -1
Current timestep = 101. State = [[-0.18446624 -0.02290676]]. Action = [[ 0.23933673 -0.17473766 -0.16853827  0.62273526]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
Current timestep = 102. State = [[-0.17999884 -0.03244715]]. Action = [[-0.08736283  0.01179248  0.14878505 -0.8432932 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
Current timestep = 103. State = [[-0.18191479 -0.04235196]]. Action = [[-0.04717556 -0.13412452 -0.19287166  0.03903282]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
Current timestep = 104. State = [[-0.17538448 -0.04998539]]. Action = [[ 0.2405979   0.02922401 -0.20538516 -0.7923396 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
Current timestep = 105. State = [[-0.16101503 -0.04730299]]. Action = [[ 0.13053918  0.07062855 -0.22049344 -0.80972844]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
Current timestep = 106. State = [[-0.15366253 -0.03165535]]. Action = [[-0.14356549  0.21502656 -0.04836179 -0.01414341]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 106 of 1
Current timestep = 107. State = [[-0.15528086 -0.02518101]]. Action = [[ 0.05886146 -0.20929255 -0.22958983  0.24883628]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
Current timestep = 108. State = [[-0.15316264 -0.03407456]]. Action = [[0.01998207 0.0476234  0.19958833 0.96364474]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
Current timestep = 109. State = [[-0.15083233 -0.0386848 ]]. Action = [[ 0.08292577 -0.08483116  0.11970741 -0.8265075 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
Current timestep = 110. State = [[-0.15172213 -0.05516504]]. Action = [[-0.130335   -0.1929487  -0.1951768  -0.05449545]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
Current timestep = 111. State = [[-0.14493279 -0.07116316]]. Action = [[ 0.1994595  -0.02301678  0.1743283  -0.9247449 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
Current timestep = 112. State = [[-0.14029777 -0.07875431]]. Action = [[-0.1244418  -0.05893859  0.0655458   0.15650105]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
Current timestep = 113. State = [[-0.13367003 -0.08896857]]. Action = [[ 0.24441421 -0.09344915 -0.21551536  0.55186343]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0109, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.11247662 -0.10291381]]. Action = [[ 0.14625531 -0.08266389 -0.11621466  0.556492  ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
Current timestep = 115. State = [[-0.10195458 -0.11650767]]. Action = [[-0.06522478 -0.1151076  -0.2168472   0.93270767]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
Current timestep = 116. State = [[-0.09301118 -0.12058324]]. Action = [[ 0.18360484  0.07282519 -0.24194494 -0.06610823]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
Current timestep = 117. State = [[-0.08428774 -0.11053996]]. Action = [[-0.11553115  0.13774362  0.04679012  0.19405735]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
Current timestep = 118. State = [[-0.08576857 -0.10047133]]. Action = [[-0.00322083  0.0527927   0.1618368  -0.10446024]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
Current timestep = 119. State = [[-0.07779534 -0.10355049]]. Action = [[ 0.23577407 -0.159055   -0.11857733  0.03262067]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0113, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.05968457 -0.10163976]]. Action = [[0.04573089 0.17871734 0.11934477 0.8582897 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
Current timestep = 121. State = [[-0.0460474  -0.10422076]]. Action = [[ 0.19767582 -0.22158766  0.16991952  0.961738  ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
Current timestep = 122. State = [[-0.02487586 -0.12048534]]. Action = [[ 0.09561342 -0.08177176 -0.21830074  0.8797684 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [False, True, False, False, True, False]
Current timestep = 123. State = [[-0.01540094 -0.13306026]]. Action = [[-0.11528763 -0.0640347   0.12298325 -0.4864701 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [False, True, False, False, True, False]
Current timestep = 124. State = [[-0.01071873 -0.1262083 ]]. Action = [[ 0.13972056  0.20786065 -0.15163888  0.57012236]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [False, True, False, True, False, False]
Scene graph at timestep 124 is [False, True, False, True, False, False]
State prediction error at timestep 124 is tensor(0.0198, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.14700735 -0.22388348]]. Action = [[ 0.20440409  0.21171534 -0.12989174 -0.9269219 ]]. Reward = [100.]
Curr episode timestep = 125
Scene graph at timestep 125 is [False, True, False, True, False, False]
Current timestep = 126. State = [[-0.13583392 -0.2566038 ]]. Action = [[-0.2399979  -0.03006712  0.09393448  0.2669027 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 126 is [True, False, False, True, False, False]
Scene graph at timestep 126 is [True, False, False, True, False, False]
State prediction error at timestep 126 is tensor(0.0359, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of -1
Current timestep = 127. State = [[-0.14108382 -0.2573954 ]]. Action = [[ 0.1806091   0.12051088  0.22361839 -0.74840856]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 127 is [True, False, False, True, False, False]
Current timestep = 128. State = [[-0.13322324 -0.24288319]]. Action = [[0.08223444 0.09478688 0.12257549 0.3044355 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 128 is [True, False, False, True, False, False]
Current timestep = 129. State = [[-0.12813365 -0.22404236]]. Action = [[-0.02826811  0.16638783  0.01331061  0.6681609 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 129 is [True, False, False, True, False, False]
Current timestep = 130. State = [[-0.11850286 -0.2195526 ]]. Action = [[ 0.20282507 -0.16774079 -0.14397326 -0.0406338 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 130 is [True, False, False, True, False, False]
Current timestep = 131. State = [[-0.10990122 -0.23538001]]. Action = [[-0.15566522 -0.13785149 -0.03155608 -0.7998623 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 131 is [True, False, False, True, False, False]
Scene graph at timestep 131 is [True, False, False, True, False, False]
State prediction error at timestep 131 is tensor(0.0277, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 0
Current timestep = 132. State = [[-0.11482922 -0.24676368]]. Action = [[-0.01710358  0.01227024  0.23430473 -0.84337777]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 132 is [True, False, False, True, False, False]
Current timestep = 133. State = [[-0.11470594 -0.2554927 ]]. Action = [[ 0.07980624 -0.15664303  0.19927633 -0.9383561 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 133 is [True, False, False, True, False, False]
Current timestep = 134. State = [[-0.10847259 -0.27530438]]. Action = [[ 0.14015257 -0.19585702  0.05092058 -0.67591524]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 134 is [True, False, False, True, False, False]
Scene graph at timestep 134 is [True, False, False, True, False, False]
State prediction error at timestep 134 is tensor(0.0346, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of -1
Current timestep = 135. State = [[-0.09411867 -0.29389355]]. Action = [[-0.01344009  0.02454239  0.00357181 -0.58385634]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 135 is [True, False, False, True, False, False]
Current timestep = 136. State = [[-0.0871994  -0.28522003]]. Action = [[0.13400841 0.15354687 0.16417772 0.21134996]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 136 is [True, False, False, True, False, False]
Scene graph at timestep 136 is [True, False, False, True, False, False]
State prediction error at timestep 136 is tensor(0.0345, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.07542065 -0.27847242]]. Action = [[ 0.04449242 -0.05968645 -0.067917   -0.5475714 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 137 is [True, False, False, True, False, False]
Current timestep = 138. State = [[-0.07458822 -0.2849998 ]]. Action = [[ 0.01562446 -0.10730799 -0.11827213  0.79062486]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 138 is [True, False, False, True, False, False]
Current timestep = 139. State = [[-0.07117201 -0.28398073]]. Action = [[-0.14053424  0.1874165  -0.19256254 -0.75327104]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 139 is [True, False, False, True, False, False]
Scene graph at timestep 139 is [True, False, False, True, False, False]
State prediction error at timestep 139 is tensor(0.0338, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 139 of 0
Current timestep = 140. State = [[-0.07379651 -0.26737216]]. Action = [[-0.10979456  0.20593032 -0.12500295 -0.5932611 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 140 is [True, False, False, True, False, False]
Current timestep = 141. State = [[-0.07639215 -0.2550991 ]]. Action = [[ 0.12332869 -0.05430517  0.12413135 -0.5151169 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 141 is [True, False, False, True, False, False]
Current timestep = 142. State = [[-0.0771213 -0.2595157]]. Action = [[-0.03645955 -0.10474694 -0.1419807  -0.4670745 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 142 is [True, False, False, True, False, False]
Current timestep = 143. State = [[-0.08868284 -0.27380246]]. Action = [[-0.23069638 -0.1221901   0.02453163  0.27263737]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 143 is [True, False, False, True, False, False]
Current timestep = 144. State = [[-0.10064451 -0.29298082]]. Action = [[-0.00865395 -0.13186057 -0.02373573  0.38496256]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 144 is [True, False, False, True, False, False]
Current timestep = 145. State = [[-0.09895495 -0.29127976]]. Action = [[0.1827302  0.18541288 0.20145544 0.5656984 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 145 is [True, False, False, True, False, False]
Scene graph at timestep 145 is [True, False, False, True, False, False]
State prediction error at timestep 145 is tensor(0.0344, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of 0
Current timestep = 146. State = [[-0.09287699 -0.27881283]]. Action = [[-0.17191929 -0.21844193  0.17743367  0.58981586]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 146 is [True, False, False, True, False, False]
Current timestep = 147. State = [[-0.09372272 -0.28310266]]. Action = [[-0.04302546 -0.0898145  -0.17388038  0.16590023]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 147 is [True, False, False, True, False, False]
Current timestep = 148. State = [[-0.09960444 -0.2968172 ]]. Action = [[-0.12984174 -0.13819157  0.10573488  0.79038   ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 148 is [True, False, False, True, False, False]
Scene graph at timestep 148 is [True, False, False, True, False, False]
State prediction error at timestep 148 is tensor(0.0381, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.10470506 -0.31004256]]. Action = [[ 0.00688401 -0.13817577 -0.08968246  0.636451  ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 149 is [True, False, False, True, False, False]
Current timestep = 150. State = [[-0.10069646 -0.2977389 ]]. Action = [[0.10991731 0.22900769 0.18425024 0.48436642]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 150 is [True, False, False, True, False, False]
Current timestep = 151. State = [[-0.10311601 -0.27202815]]. Action = [[-0.21787666  0.23524195  0.149176    0.7687495 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 151 is [True, False, False, True, False, False]
Current timestep = 152. State = [[-0.11893633 -0.24140367]]. Action = [[-0.11757913  0.19155663 -0.01198761 -0.8047205 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 152 is [True, False, False, True, False, False]
Current timestep = 153. State = [[-0.12431216 -0.21369153]]. Action = [[ 1.4752573e-01  1.4464998e-01 -3.9944053e-04  8.6146927e-01]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 153 is [True, False, False, True, False, False]
Current timestep = 154. State = [[-0.12808594 -0.20939384]]. Action = [[-0.15596426 -0.13996366 -0.20995463  0.77802324]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 154 is [True, False, False, True, False, False]
Scene graph at timestep 154 is [True, False, False, True, False, False]
State prediction error at timestep 154 is tensor(0.0180, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of 1
Current timestep = 155. State = [[-0.1336788  -0.22340794]]. Action = [[ 0.075872   -0.14042798  0.03886011  0.7925811 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 155 is [True, False, False, True, False, False]
Scene graph at timestep 155 is [True, False, False, True, False, False]
State prediction error at timestep 155 is tensor(0.0214, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of 0
Current timestep = 156. State = [[-0.12315167 -0.21921614]]. Action = [[ 0.24582386  0.20874822  0.23036495 -0.9971514 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 156 is [True, False, False, True, False, False]
Current timestep = 157. State = [[-0.11621965 -0.21166664]]. Action = [[-0.15394671 -0.05288936  0.0912573   0.6233783 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 157 is [True, False, False, True, False, False]
Current timestep = 158. State = [[-0.1140017  -0.20250317]]. Action = [[ 0.138636    0.1581546   0.04340398 -0.8623456 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 158 is [True, False, False, True, False, False]
Scene graph at timestep 158 is [True, False, False, True, False, False]
State prediction error at timestep 158 is tensor(0.0094, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of 1
Current timestep = 159. State = [[-0.10027162 -0.18437262]]. Action = [[ 0.2298575   0.07394278 -0.01131819 -0.46921968]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 159 is [True, False, False, True, False, False]
Scene graph at timestep 159 is [True, False, False, True, False, False]
State prediction error at timestep 159 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 159 of 1
Current timestep = 160. State = [[-0.08118939 -0.17238276]]. Action = [[ 0.00844157  0.07445741 -0.18156713 -0.7933067 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 160 is [True, False, False, True, False, False]
Scene graph at timestep 160 is [True, False, False, True, False, False]
State prediction error at timestep 160 is tensor(0.0080, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of 1
Current timestep = 161. State = [[-0.07836509 -0.15364352]]. Action = [[ 0.00231251  0.21079212  0.12007564 -0.77791524]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 161 is [True, False, False, True, False, False]
Current timestep = 162. State = [[-0.08181502 -0.14394365]]. Action = [[-0.19270338 -0.05630888 -0.24437179  0.63204885]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 162 is [True, False, False, True, False, False]
Scene graph at timestep 162 is [True, False, False, True, False, False]
State prediction error at timestep 162 is tensor(0.0072, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of 1
Current timestep = 163. State = [[-0.083092   -0.13518079]]. Action = [[ 0.1377339   0.16316193 -0.17922305  0.07549715]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 163 is [True, False, False, True, False, False]
Current timestep = 164. State = [[-0.08329698 -0.11647323]]. Action = [[-0.05612604  0.10778123  0.04375756 -0.68406284]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 164 is [True, False, False, True, False, False]
Current timestep = 165. State = [[-0.08040894 -0.09324498]]. Action = [[ 0.10996479  0.22254151 -0.21600804 -0.5199518 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 165 is [True, False, False, False, True, False]
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of 1
Current timestep = 166. State = [[-0.08194726 -0.07712474]]. Action = [[-0.16638073 -0.07396449 -0.20186351 -0.4429211 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 166 is [True, False, False, False, True, False]
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of 0
Current timestep = 167. State = [[-0.08136087 -0.07862423]]. Action = [[ 0.16787118  0.05990943  0.03820816 -0.07650793]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 167 is [True, False, False, False, True, False]
Current timestep = 168. State = [[-0.080787   -0.07523506]]. Action = [[-0.08341634  0.01676238 -0.0228838   0.63246346]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 168 is [True, False, False, False, True, False]
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of 0
Current timestep = 169. State = [[-0.07960014 -0.05864589]]. Action = [[ 0.07392523  0.22411633 -0.16527617 -0.84856033]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 169 is [True, False, False, False, True, False]
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of 1
Current timestep = 170. State = [[-0.06933707 -0.04307496]]. Action = [[ 0.23071098 -0.0921683  -0.06110191  0.9108665 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 170 is [True, False, False, False, True, False]
Current timestep = 171. State = [[-0.05286848 -0.03618546]]. Action = [[-0.01635048  0.16589281  0.05021417  0.89701986]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 171 is [True, False, False, False, True, False]
Current timestep = 172. State = [[-0.04910177 -0.02051266]]. Action = [[ 0.06859928  0.14117444  0.11959246 -0.9548941 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 172 is [True, False, False, False, True, False]
Current timestep = 173. State = [[-0.16116267  0.1464173 ]]. Action = [[ 0.14691216 -0.19740999 -0.12911505  0.66734445]]. Reward = [100.]
Curr episode timestep = 47
Scene graph at timestep 173 is [False, True, False, False, True, False]
Current timestep = 174. State = [[-0.13808487  0.15739714]]. Action = [[ 0.08117312 -0.13376111  0.15549147 -0.9612504 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 174 is [True, False, False, False, False, True]
Current timestep = 175. State = [[-0.13893872  0.14610152]]. Action = [[-0.2216295  -0.09016588 -0.13378179 -0.92040426]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 175 is [True, False, False, False, False, True]
Scene graph at timestep 175 is [True, False, False, False, False, True]
State prediction error at timestep 175 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.14414418  0.14557913]]. Action = [[ 0.15370697  0.16094264  0.24464214 -0.69689476]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 176 is [True, False, False, False, False, True]
Scene graph at timestep 176 is [True, False, False, False, False, True]
State prediction error at timestep 176 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of 0
Current timestep = 177. State = [[-0.13250723  0.15306486]]. Action = [[ 0.19684124 -0.02776141  0.1848172  -0.17773438]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 177 is [True, False, False, False, False, True]
Current timestep = 178. State = [[-0.11771578  0.14569752]]. Action = [[-0.05902228 -0.1573915  -0.00943942 -0.66577655]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 178 is [True, False, False, False, False, True]
Current timestep = 179. State = [[-0.11764251  0.13656664]]. Action = [[-0.10821109 -0.00889976 -0.22305383  0.60395646]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 179 is [True, False, False, False, False, True]
Current timestep = 180. State = [[-0.12672465  0.14495338]]. Action = [[-0.19922382  0.14594749 -0.0110589   0.02625704]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 180 is [True, False, False, False, False, True]
Current timestep = 181. State = [[-0.13892019  0.14522591]]. Action = [[-0.00118484 -0.1573728   0.01648301  0.09050059]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 181 is [True, False, False, False, False, True]
Current timestep = 182. State = [[-0.1433916   0.14679265]]. Action = [[-0.02415453  0.16711047 -0.10737833  0.08762193]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 182 is [True, False, False, False, False, True]
Current timestep = 183. State = [[-0.15355706  0.15350482]]. Action = [[-0.1568889  -0.06575069 -0.03136577 -0.68444824]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 183 is [True, False, False, False, False, True]
Scene graph at timestep 183 is [True, False, False, False, False, True]
State prediction error at timestep 183 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of 0
Current timestep = 184. State = [[-0.1774367   0.15320677]]. Action = [[-0.22365764  0.01089063  0.19635439  0.7642603 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 184 is [True, False, False, False, False, True]
Current timestep = 185. State = [[-0.19391556  0.16622257]]. Action = [[ 0.11539486  0.23424357 -0.12016331  0.8356638 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 185 is [True, False, False, False, False, True]
Current timestep = 186. State = [[-0.19328482  0.17139916]]. Action = [[ 0.05666971 -0.1270471   0.01940325  0.3942697 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 186 is [True, False, False, False, False, True]
Current timestep = 187. State = [[-0.18420744  0.15824725]]. Action = [[ 0.16283703 -0.12933089  0.14147091  0.99114835]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 187 is [True, False, False, False, False, True]
Scene graph at timestep 187 is [True, False, False, False, False, True]
State prediction error at timestep 187 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 187 of 0
Current timestep = 188. State = [[-0.1653198   0.13755599]]. Action = [[ 0.16923153 -0.13228749 -0.13378194 -0.36575747]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 188 is [True, False, False, False, False, True]
Scene graph at timestep 188 is [True, False, False, False, False, True]
State prediction error at timestep 188 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of 1
Current timestep = 189. State = [[-0.14435077  0.13682477]]. Action = [[ 0.17588401  0.20730904 -0.23569205  0.10714364]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 189 is [True, False, False, False, False, True]
Current timestep = 190. State = [[-0.12585348  0.14215377]]. Action = [[ 0.04485112 -0.1165258   0.003088    0.89750016]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 190 is [True, False, False, False, False, True]
Current timestep = 191. State = [[-0.11225193  0.13500729]]. Action = [[ 0.24588382 -0.05369282  0.02725628  0.58741856]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 191 is [True, False, False, False, False, True]
Current timestep = 192. State = [[-0.08504158  0.1360071 ]]. Action = [[ 0.13865617  0.07746565 -0.02096963 -0.9606135 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 192 is [True, False, False, False, False, True]
Scene graph at timestep 192 is [True, False, False, False, False, True]
State prediction error at timestep 192 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 192 of 1
Current timestep = 193. State = [[-0.05905967  0.14014347]]. Action = [[ 0.10543591 -0.04126316  0.02042836  0.66052306]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 193 is [True, False, False, False, False, True]
Current timestep = 194. State = [[-0.04326814  0.14686899]]. Action = [[ 0.19657418  0.18566337  0.18727022 -0.76846886]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 194 is [True, False, False, False, False, True]
Current timestep = 195. State = [[-0.02438828  0.15422098]]. Action = [[-0.06828323 -0.09422559 -0.20924772 -0.93582934]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 195 is [False, True, False, False, False, True]
Current timestep = 196. State = [[-0.02295138  0.14795814]]. Action = [[-0.11638594 -0.09013198  0.17696092  0.6750369 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 196 is [False, True, False, False, False, True]
Current timestep = 197. State = [[-0.0171878   0.13120191]]. Action = [[ 0.23322642 -0.14403658 -0.1698391  -0.12193751]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 197 is [False, True, False, False, False, True]
Scene graph at timestep 197 is [False, True, False, False, False, True]
State prediction error at timestep 197 is tensor(0.0118, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.00991257  0.12920678]]. Action = [[ 0.00827637  0.22346908 -0.12283784 -0.7170155 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 198 is [False, True, False, False, False, True]
Current timestep = 199. State = [[-0.01403646  0.15035824]]. Action = [[-0.24039869  0.09000173 -0.22849356 -0.40960085]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 199 is [False, True, False, False, False, True]
Current timestep = 200. State = [[-0.0163692   0.15073173]]. Action = [[ 0.14927995 -0.17113285  0.04715082  0.38392925]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 200 is [False, True, False, False, False, True]
Current timestep = 201. State = [[-0.00952743  0.13052724]]. Action = [[ 0.05087245 -0.19357824  0.18559092 -0.54893833]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 201 is [False, True, False, False, False, True]
Current timestep = 202. State = [[-0.16482498 -0.09868751]]. Action = [[-0.06286873 -0.18464608 -0.20293473  0.30511975]]. Reward = [100.]
Curr episode timestep = 28
Scene graph at timestep 202 is [False, True, False, False, False, True]
Current timestep = 203. State = [[-0.15050064 -0.09799865]]. Action = [[0.00558788 0.20707637 0.00622225 0.2728145 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 203 is [True, False, False, False, True, False]
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of 1
Current timestep = 204. State = [[-0.14007235 -0.07175182]]. Action = [[ 0.21123493  0.2194894  -0.21745357  0.8687973 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 204 is [True, False, False, False, True, False]
Current timestep = 205. State = [[-0.1223216  -0.05838427]]. Action = [[-0.00286624 -0.00589111 -0.03194226  0.0503124 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 205 is [True, False, False, False, True, False]
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of 1
Current timestep = 206. State = [[-0.11770737 -0.05479345]]. Action = [[ 0.00751361  0.02229172 -0.04378623 -0.81455153]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 206 is [True, False, False, False, True, False]
Current timestep = 207. State = [[-0.1116515  -0.04722252]]. Action = [[ 0.15884951  0.09591281 -0.2285318   0.74104905]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 207 is [True, False, False, False, True, False]
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 207 of 1
Current timestep = 208. State = [[-0.09156823 -0.02771564]]. Action = [[ 0.0901691   0.16966987 -0.15248013 -0.95680887]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 208 is [True, False, False, False, True, False]
Current timestep = 209. State = [[-0.0868761 -0.0166462]]. Action = [[-0.19159588 -0.02042462 -0.21147783 -0.9762291 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 209 is [True, False, False, False, True, False]
Current timestep = 210. State = [[-0.091182   -0.01902767]]. Action = [[-0.08001146 -0.06493887 -0.04229322 -0.40617514]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 210 is [True, False, False, False, True, False]
Current timestep = 211. State = [[-0.09277018 -0.02922677]]. Action = [[ 0.12795836 -0.10799617 -0.10609335  0.49943244]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 211 is [True, False, False, False, True, False]
Current timestep = 212. State = [[-0.08634855 -0.04875409]]. Action = [[ 0.1640906  -0.22938152 -0.19632944 -0.43657225]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 212 is [True, False, False, False, True, False]
Current timestep = 213. State = [[-0.06853333 -0.06744713]]. Action = [[ 0.20847869 -0.03541745  0.1637325  -0.04456294]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 213 is [True, False, False, False, True, False]
Current timestep = 214. State = [[-0.04135936 -0.07074121]]. Action = [[ 0.22283274  0.04007891 -0.06671943 -0.4103198 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 214 is [True, False, False, False, True, False]
Current timestep = 215. State = [[-0.26739067  0.08692386]]. Action = [[-0.1326931   0.22769496  0.18486801 -0.83525544]]. Reward = [100.]
Curr episode timestep = 12
Scene graph at timestep 215 is [False, True, False, False, True, False]
Current timestep = 216. State = [[-0.26836413  0.09438347]]. Action = [[-0.08242467  0.14983726  0.08495054  0.05830348]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 216 is [True, False, False, False, True, False]
Current timestep = 217. State = [[-0.26836413  0.09438347]]. Action = [[-0.1652934  -0.19528079 -0.14546421  0.08518708]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 217 is [True, False, False, False, True, False]
Current timestep = 218. State = [[-0.27136728  0.10684512]]. Action = [[-0.03277399  0.18581328  0.10009688 -0.23147249]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 218 is [True, False, False, False, True, False]
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of -1
Current timestep = 219. State = [[-0.27412352  0.12454148]]. Action = [[-0.00845943  0.04371855 -0.14090686 -0.84834456]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 219 is [True, False, False, False, True, False]
Current timestep = 220. State = [[-0.269311    0.11601035]]. Action = [[ 0.09988537 -0.22883476  0.05862308  0.18629849]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 220 is [True, False, False, False, True, False]
Current timestep = 221. State = [[-0.26406676  0.10519364]]. Action = [[-0.16613494 -0.08120164  0.0542796  -0.76573265]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 221 is [True, False, False, False, True, False]
Current timestep = 222. State = [[-0.26093775  0.08943694]]. Action = [[-0.06319007 -0.23300388 -0.13108326  0.8229313 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 222 is [True, False, False, False, True, False]
Current timestep = 223. State = [[-0.2601331   0.07420978]]. Action = [[-0.11038741  0.21467072 -0.05030371 -0.56359285]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 223 is [True, False, False, False, True, False]
Current timestep = 224. State = [[-0.26185912  0.07603248]]. Action = [[ 0.00770047  0.11494735 -0.1795237  -0.89591223]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 224 is [True, False, False, False, True, False]
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of 0
Current timestep = 225. State = [[-0.26737988  0.08770429]]. Action = [[-0.0449618   0.10663897  0.20863941  0.10852408]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 225 is [True, False, False, False, True, False]
Current timestep = 226. State = [[-0.27079     0.09413005]]. Action = [[-0.09097078 -0.13151458 -0.07517679  0.02377129]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 226 is [True, False, False, False, True, False]
Current timestep = 227. State = [[-0.2635729   0.08373642]]. Action = [[ 0.14047676 -0.21756342  0.1631976  -0.27620494]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 227 is [True, False, False, False, True, False]
Current timestep = 228. State = [[-0.2531402   0.06802915]]. Action = [[ 0.03803599 -0.08040503 -0.11948648 -0.89150524]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 228 is [True, False, False, False, True, False]
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 228 of 0
Current timestep = 229. State = [[-0.24745898  0.06030166]]. Action = [[-0.17518097  0.06871161 -0.24143207  0.21868229]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 229 is [True, False, False, False, True, False]
Current timestep = 230. State = [[-0.23590557  0.04669259]]. Action = [[ 0.20267165 -0.20362243 -0.12373438 -0.666298  ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 230 is [True, False, False, False, True, False]
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 230 of 1
Current timestep = 231. State = [[-0.2159882   0.03123665]]. Action = [[ 0.0551002   0.00408968 -0.05070163 -0.08485365]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 231 is [True, False, False, False, True, False]
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of 1
Current timestep = 232. State = [[-0.21846202  0.03882295]]. Action = [[-0.21649675  0.13550758  0.09439215 -0.17016411]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 232 is [True, False, False, False, True, False]
Current timestep = 233. State = [[-0.2274628   0.05862258]]. Action = [[-0.02659231  0.20509207  0.21162891  0.62704945]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 233 is [True, False, False, False, True, False]
Current timestep = 234. State = [[-0.23060177  0.06473658]]. Action = [[ 0.08834705 -0.14257424  0.06317109 -0.2190389 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 234 is [True, False, False, False, True, False]
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 234 of 0
Current timestep = 235. State = [[-0.22543424  0.04920155]]. Action = [[ 0.07976362 -0.16489355 -0.1382921  -0.79958993]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 235 is [True, False, False, False, True, False]
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 235 of 1
Current timestep = 236. State = [[-0.22561763  0.02332998]]. Action = [[-0.222462   -0.21961221 -0.05245999 -0.22565055]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 236 is [True, False, False, False, True, False]
Current timestep = 237. State = [[-0.24213251  0.01645011]]. Action = [[-0.15509224  0.15963411  0.0072189  -0.34341478]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 237 is [True, False, False, False, True, False]
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 237 of -1
Current timestep = 238. State = [[-0.25065503  0.03651104]]. Action = [[ 0.19034159  0.22329268 -0.21503343 -0.3376007 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 238 is [True, False, False, False, True, False]
Current timestep = 239. State = [[-0.24171877  0.05199282]]. Action = [[ 0.09538621  0.04989854 -0.06826644 -0.44101143]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 239 is [True, False, False, False, True, False]
Current timestep = 240. State = [[-0.22864155  0.06596851]]. Action = [[ 0.11243105  0.12983829 -0.17816338  0.9790411 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 240 is [True, False, False, False, True, False]
Current timestep = 241. State = [[-0.21588679  0.07602485]]. Action = [[ 0.06552482 -0.0076374  -0.07139535  0.24802446]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 241 is [True, False, False, False, True, False]
Current timestep = 242. State = [[-0.19833934  0.08483247]]. Action = [[ 0.24119648  0.1226328  -0.17452139 -0.08137572]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 242 is [True, False, False, False, True, False]
Current timestep = 243. State = [[-0.17713377  0.10035598]]. Action = [[ 0.01508051  0.12415296 -0.1282078  -0.5308819 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 243 is [True, False, False, False, True, False]
Current timestep = 244. State = [[-0.16362189  0.11436118]]. Action = [[ 0.17060941  0.07848942  0.10858113 -0.6521914 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 244 is [True, False, False, False, True, False]
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of 1
Current timestep = 245. State = [[-0.1521465   0.13957395]]. Action = [[-0.14668554  0.22970915 -0.21269798 -0.93319285]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 245 is [True, False, False, False, True, False]
Current timestep = 246. State = [[-0.16412497  0.15838926]]. Action = [[-0.16782825 -0.01116115 -0.07889265 -0.28836554]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 246 is [True, False, False, False, False, True]
Scene graph at timestep 246 is [True, False, False, False, False, True]
State prediction error at timestep 246 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 246 of -1
Current timestep = 247. State = [[-0.17704286  0.17286582]]. Action = [[-0.10219267  0.13846809 -0.08937511  0.30776942]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 247 is [True, False, False, False, False, True]
Current timestep = 248. State = [[-0.18111691  0.16992085]]. Action = [[ 0.06034386 -0.22069927  0.19828862 -0.97321963]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 248 is [True, False, False, False, False, True]
Current timestep = 249. State = [[-0.18643516  0.17027357]]. Action = [[-0.13424024  0.21551925 -0.16547821 -0.4354515 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 249 is [True, False, False, False, False, True]
Current timestep = 250. State = [[-0.18907613  0.16959094]]. Action = [[ 0.14259535 -0.19011098 -0.10894388 -0.44112456]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 250 is [True, False, False, False, False, True]
Current timestep = 251. State = [[-0.1951126   0.15421407]]. Action = [[-0.23735811 -0.11830306  0.20966566  0.910679  ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 251 is [True, False, False, False, False, True]
Current timestep = 252. State = [[-0.21071284  0.13529356]]. Action = [[-0.17071053 -0.15299568  0.1305368  -0.5114918 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 252 is [True, False, False, False, False, True]
Scene graph at timestep 252 is [True, False, False, False, False, True]
State prediction error at timestep 252 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.23769587  0.10589728]]. Action = [[-0.18045412 -0.24258336  0.08483964 -0.23912454]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 253 is [True, False, False, False, False, True]
Current timestep = 254. State = [[-0.2632274   0.08353622]]. Action = [[-0.20126404 -0.07548513  0.20759225  0.71070755]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 254 is [True, False, False, False, True, False]
Current timestep = 255. State = [[-0.2733073   0.07901594]]. Action = [[ 0.24078631  0.11388865 -0.16941476  0.6507057 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 255 is [True, False, False, False, True, False]
Current timestep = 256. State = [[-0.26988018  0.08195747]]. Action = [[-0.22136134 -0.20545864  0.03932551 -0.55005103]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 256 is [True, False, False, False, True, False]
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 256 of -1
Current timestep = 257. State = [[-0.26871297  0.08271953]]. Action = [[-0.1269845   0.20192903  0.17646852 -0.6852479 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 257 is [True, False, False, False, True, False]
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of -1
Current timestep = 258. State = [[-0.25929713  0.07439724]]. Action = [[ 0.19889009 -0.15660258 -0.11329162 -0.7752446 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 258 is [True, False, False, False, True, False]
Current timestep = 259. State = [[-0.24530311  0.06560984]]. Action = [[ 0.07742372  0.02769729 -0.19010454  0.44184756]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 259 is [True, False, False, False, True, False]
Current timestep = 260. State = [[-0.23507284  0.07843084]]. Action = [[0.03854895 0.22161198 0.03712395 0.32145262]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 260 is [True, False, False, False, True, False]
Current timestep = 261. State = [[-0.22657737  0.0935263 ]]. Action = [[0.04225585 0.05880618 0.06060073 0.08363032]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 261 is [True, False, False, False, True, False]
Current timestep = 262. State = [[-0.21319245  0.08974638]]. Action = [[ 0.208633   -0.18298931  0.02239791  0.7114625 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 262 is [True, False, False, False, True, False]
Current timestep = 263. State = [[-0.20379676  0.08796403]]. Action = [[-0.18576568  0.08120647 -0.18354756  0.00051522]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 263 is [True, False, False, False, True, False]
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(6.8496e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 263 of 1
Current timestep = 264. State = [[-0.21556725  0.08641024]]. Action = [[-0.2429386  -0.09558824  0.15998137 -0.84815276]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 264 is [True, False, False, False, True, False]
Current timestep = 265. State = [[-0.23056251  0.07321176]]. Action = [[ 0.01017118 -0.12505953  0.14741087  0.9560466 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 265 is [True, False, False, False, True, False]
Current timestep = 266. State = [[-0.23874284  0.07366189]]. Action = [[-0.14801484  0.16299051  0.09720331  0.75192356]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 266 is [True, False, False, False, True, False]
Current timestep = 267. State = [[-0.24602766  0.09401275]]. Action = [[ 0.17827863  0.20926821 -0.2176022  -0.8561273 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 267 is [True, False, False, False, True, False]
Current timestep = 268. State = [[-0.23798795  0.11122197]]. Action = [[0.16746509 0.10620505 0.06350246 0.18500876]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 268 is [True, False, False, False, True, False]
Current timestep = 269. State = [[-0.22950082  0.12174923]]. Action = [[-0.1557846  -0.0262004  -0.19260752 -0.5572248 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 269 is [True, False, False, False, True, False]
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of 0
Current timestep = 270. State = [[-0.22507156  0.1163887 ]]. Action = [[ 0.21282396 -0.14585254  0.14466864 -0.2744075 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 270 is [True, False, False, False, True, False]
Current timestep = 271. State = [[-0.22008939  0.10888971]]. Action = [[-0.03551015  0.00152761  0.07783267 -0.04349613]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 271 is [True, False, False, False, True, False]
Current timestep = 272. State = [[-0.21839248  0.0999197 ]]. Action = [[-0.03294165 -0.13965401 -0.06056511 -0.9729515 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 272 is [True, False, False, False, True, False]
Current timestep = 273. State = [[-0.21509337  0.08462121]]. Action = [[ 0.02483898 -0.10972568  0.16742206  0.85473037]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 273 is [True, False, False, False, True, False]
Current timestep = 274. State = [[-0.21384314  0.06031808]]. Action = [[-0.01719911 -0.21664235  0.23136902 -0.56634945]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 274 is [True, False, False, False, True, False]
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 274 of 0
Current timestep = 275. State = [[-0.21613461  0.04620317]]. Action = [[ 0.03689554  0.1368764   0.12599501 -0.6925264 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 275 is [True, False, False, False, True, False]
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 275 of 0
Current timestep = 276. State = [[-0.22302788  0.06376492]]. Action = [[-0.18808815  0.1978642   0.02659225 -0.7643548 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 276 is [True, False, False, False, True, False]
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(5.7692e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of -1
Current timestep = 277. State = [[-0.2282579   0.08768387]]. Action = [[ 0.16280001  0.0956026   0.10099274 -0.8324251 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 277 is [True, False, False, False, True, False]
Current timestep = 278. State = [[-0.21442251  0.10151771]]. Action = [[ 0.22579014  0.13870889 -0.22987743  0.37256682]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 278 is [True, False, False, False, True, False]
Current timestep = 279. State = [[-0.19860987  0.10809065]]. Action = [[-0.06004013 -0.11599603 -0.16731034 -0.9766189 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 279 is [True, False, False, False, True, False]
Current timestep = 280. State = [[-0.19227742  0.10117044]]. Action = [[ 0.15227222 -0.0642437  -0.2483609  -0.35328138]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 280 is [True, False, False, False, True, False]
Current timestep = 281. State = [[-0.18667303  0.10139862]]. Action = [[-0.072442    0.1291132  -0.21915741  0.3771491 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 281 is [True, False, False, False, True, False]
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 281 of 1
Current timestep = 282. State = [[-0.18014793  0.09732167]]. Action = [[ 0.16933799 -0.17596504 -0.03230484  0.06681609]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 282 is [True, False, False, False, True, False]
Current timestep = 283. State = [[-0.16784753  0.08938807]]. Action = [[ 0.0649384   0.01261616 -0.13315275 -0.33431363]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 283 is [True, False, False, False, True, False]
Current timestep = 284. State = [[-0.16388355  0.07578169]]. Action = [[-0.16739728 -0.23570283  0.10386378  0.7038326 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 284 is [True, False, False, False, True, False]
Current timestep = 285. State = [[-0.1718602   0.07255281]]. Action = [[-0.1372332   0.19633853 -0.16706915  0.76945734]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 285 is [True, False, False, False, True, False]
Current timestep = 286. State = [[-0.17553352  0.08374177]]. Action = [[ 0.16689682  0.08636439 -0.12254554 -0.04232621]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 286 is [True, False, False, False, True, False]
Current timestep = 287. State = [[-0.17340805  0.10238415]]. Action = [[ 0.00874659  0.22611374 -0.23875363  0.8864614 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 287 is [True, False, False, False, True, False]
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 287 of 0
Current timestep = 288. State = [[-0.1726774  0.1275302]]. Action = [[ 0.01017639  0.0936031   0.23805216 -0.8316193 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 288 is [True, False, False, False, True, False]
Current timestep = 289. State = [[-0.16878022  0.12431062]]. Action = [[ 0.09855676 -0.18258655 -0.02118489  0.93497896]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 289 is [True, False, False, False, False, True]
Current timestep = 290. State = [[-0.16162132  0.10561628]]. Action = [[-0.03441641 -0.19436282 -0.09572592  0.34013295]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 290 is [True, False, False, False, True, False]
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 290 of 1
Current timestep = 291. State = [[-0.15702754  0.10273159]]. Action = [[ 0.07598722  0.2475493   0.06725505 -0.3042704 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 291 is [True, False, False, False, True, False]
Current timestep = 292. State = [[-0.15017994  0.126171  ]]. Action = [[ 0.05417845  0.22354952 -0.2036452   0.7035346 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 292 is [True, False, False, False, True, False]
Current timestep = 293. State = [[-0.13537863  0.15011592]]. Action = [[ 0.22416925  0.08882019 -0.23992747  0.02355731]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 293 is [True, False, False, False, False, True]
Current timestep = 294. State = [[-0.11355207  0.16652164]]. Action = [[ 0.09520158  0.0491161  -0.01178133 -0.13074052]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 294 is [True, False, False, False, False, True]
Current timestep = 295. State = [[-0.11112718  0.18531114]]. Action = [[-0.225164    0.2082339  -0.21936516 -0.5219244 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 295 is [True, False, False, False, False, True]
Current timestep = 296. State = [[-0.12440617  0.20030586]]. Action = [[-0.19237332 -0.06108429 -0.17657569 -0.01063204]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 296 is [True, False, False, False, False, True]
Current timestep = 297. State = [[-0.1302206   0.20014277]]. Action = [[ 0.17951983  0.00803369 -0.21551183  0.3625176 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 297 is [True, False, False, False, False, True]
Scene graph at timestep 297 is [True, False, False, False, False, True]
State prediction error at timestep 297 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 297 of -1
Current timestep = 298. State = [[-0.13241655  0.20580374]]. Action = [[-0.04272738  0.13391158 -0.17240387 -0.7933775 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 298 is [True, False, False, False, False, True]
Scene graph at timestep 298 is [True, False, False, False, False, True]
State prediction error at timestep 298 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 298 of -1
Current timestep = 299. State = [[-0.13064113  0.20072708]]. Action = [[ 0.03847483 -0.23556066 -0.20624551 -0.21681392]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 299 is [True, False, False, False, False, True]
Current timestep = 300. State = [[-0.1308984   0.19812271]]. Action = [[-0.11093383  0.1471802  -0.01308624 -0.8123662 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 300 is [True, False, False, False, False, True]
Current timestep = 301. State = [[-0.1261184   0.19063511]]. Action = [[ 0.22837502 -0.22636166  0.22190669 -0.894039  ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 301 is [True, False, False, False, False, True]
Current timestep = 302. State = [[-0.11029527  0.16770662]]. Action = [[ 0.16975045 -0.12050483 -0.23553872  0.34077108]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 302 is [True, False, False, False, False, True]
Current timestep = 303. State = [[-0.09983028  0.15864654]]. Action = [[-0.1138916   0.01597467  0.20957035 -0.8638397 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 303 is [True, False, False, False, False, True]
Current timestep = 304. State = [[-0.1066141  0.1672512]]. Action = [[-0.21547367  0.1200729  -0.06262007  0.5904292 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 304 is [True, False, False, False, False, True]
Scene graph at timestep 304 is [True, False, False, False, False, True]
State prediction error at timestep 304 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 304 of 1
Current timestep = 305. State = [[-0.11311365  0.1675757 ]]. Action = [[ 0.07925719 -0.1808145  -0.19800165 -0.5846623 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 305 is [True, False, False, False, False, True]
Scene graph at timestep 305 is [True, False, False, False, False, True]
State prediction error at timestep 305 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of 1
Current timestep = 306. State = [[-0.10728819  0.16442186]]. Action = [[ 0.24554682  0.23764265 -0.22370036  0.56696546]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 306 is [True, False, False, False, False, True]
Current timestep = 307. State = [[-0.08547632  0.17550524]]. Action = [[ 0.17682713 -0.04263505  0.0049375   0.36021197]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 307 is [True, False, False, False, False, True]
Current timestep = 308. State = [[-0.07250822  0.18197922]]. Action = [[-0.04244043  0.0919694  -0.12372792  0.27960777]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 308 is [True, False, False, False, False, True]
Current timestep = 309. State = [[-0.06312586  0.1849387 ]]. Action = [[ 0.16076249 -0.04813403  0.19334847 -0.73625255]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 309 is [True, False, False, False, False, True]
Current timestep = 310. State = [[-0.05847119  0.19568323]]. Action = [[-0.15329963  0.19534513  0.23234701  0.8803725 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 310 is [True, False, False, False, False, True]
Current timestep = 311. State = [[-0.0632297   0.20218095]]. Action = [[-0.05988874 -0.11571462  0.22373831 -0.37359738]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 311 is [True, False, False, False, False, True]
Scene graph at timestep 311 is [True, False, False, False, False, True]
State prediction error at timestep 311 is tensor(0.0075, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 311 of -1
Current timestep = 312. State = [[-0.05884889  0.20287398]]. Action = [[ 0.22165188  0.08936754 -0.08381644 -0.8848209 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 312 is [True, False, False, False, False, True]
Scene graph at timestep 312 is [True, False, False, False, False, True]
State prediction error at timestep 312 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 312 of -1
Current timestep = 313. State = [[-0.05030639  0.21141014]]. Action = [[-0.20038024 -0.01560825 -0.24850957 -0.6248247 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 313 is [True, False, False, False, False, True]
Scene graph at timestep 313 is [True, False, False, False, False, True]
State prediction error at timestep 313 is tensor(0.0083, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 313 of 0
Current timestep = 314. State = [[-0.06007773  0.22226699]]. Action = [[-0.18218051  0.15781325 -0.12297422  0.70195293]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 314 is [True, False, False, False, False, True]
Current timestep = 315. State = [[-0.06902906  0.23827764]]. Action = [[ 0.21740466  0.13822055 -0.14281641 -0.01738334]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 315 is [True, False, False, False, False, True]
Current timestep = 316. State = [[-0.06142029  0.2465594 ]]. Action = [[ 0.1480655   0.02801394 -0.24527931  0.3137256 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 316 is [True, False, False, False, False, True]
Current timestep = 317. State = [[-0.05178961  0.25490496]]. Action = [[-0.0099923   0.02841529 -0.09192954  0.5627636 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 317 is [True, False, False, False, False, True]
Current timestep = 318. State = [[-0.05348726  0.26227194]]. Action = [[-0.13702367  0.05122778 -0.20179535  0.3857801 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 318 is [True, False, False, False, False, True]
Scene graph at timestep 318 is [True, False, False, False, False, True]
State prediction error at timestep 318 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 318 of -1
Current timestep = 319. State = [[-0.05850149  0.26967403]]. Action = [[-0.02385621  0.0651657  -0.08000121 -0.5666255 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 319 is [True, False, False, False, False, True]
Current timestep = 320. State = [[-0.06429897  0.27727178]]. Action = [[-0.14780672 -0.00372839 -0.08088914  0.2823205 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 320 is [True, False, False, False, False, True]
Current timestep = 321. State = [[-0.06853269  0.28082895]]. Action = [[ 0.19579738  0.22357213 -0.15423234 -0.97470254]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 321 is [True, False, False, False, False, True]
Current timestep = 322. State = [[-0.06360473  0.271184  ]]. Action = [[ 0.21537036 -0.13603161  0.09704396  0.8235352 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 322 is [True, False, False, False, False, True]
Current timestep = 323. State = [[-0.05958218  0.25655535]]. Action = [[-0.15881732 -0.12443599  0.02647594 -0.8188199 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 323 is [True, False, False, False, False, True]
Current timestep = 324. State = [[-0.05570104  0.23937689]]. Action = [[ 0.20379657 -0.10042831 -0.11476959  0.52926207]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 324 is [True, False, False, False, False, True]
Scene graph at timestep 324 is [True, False, False, False, False, True]
State prediction error at timestep 324 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 324 of 0
Current timestep = 325. State = [[-0.0496624   0.23622811]]. Action = [[ 0.03565782  0.18994159 -0.18591179  0.42281485]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 325 is [True, False, False, False, False, True]
Current timestep = 326. State = [[-0.05264084  0.23438749]]. Action = [[-0.23425433 -0.22999454 -0.19349389 -0.56323   ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 326 is [False, True, False, False, False, True]
Current timestep = 327. State = [[-0.06370893  0.22873287]]. Action = [[-0.10899225  0.03403634  0.16450185  0.9635148 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 327 is [True, False, False, False, False, True]
Current timestep = 328. State = [[-0.06722932  0.21724093]]. Action = [[ 0.13029152 -0.18751898 -0.23150088  0.61886823]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 328 is [True, False, False, False, False, True]
Current timestep = 329. State = [[-0.06508794  0.2073191 ]]. Action = [[ 0.0878334   0.0496611  -0.04781587 -0.94601566]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 329 is [True, False, False, False, False, True]
Current timestep = 330. State = [[-0.06750473  0.19275701]]. Action = [[-0.20777525 -0.24468105  0.05713215 -0.8713075 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 330 is [True, False, False, False, False, True]
Current timestep = 331. State = [[-0.06785468  0.16582485]]. Action = [[ 0.18327242 -0.1661794  -0.23527108 -0.7341193 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 331 is [True, False, False, False, False, True]
Current timestep = 332. State = [[-0.05999723  0.13834706]]. Action = [[-0.05894174 -0.21060085 -0.15700112 -0.40509236]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 332 is [True, False, False, False, False, True]
Current timestep = 333. State = [[-0.05817087  0.11708316]]. Action = [[ 0.10592735 -0.0620988   0.00787568 -0.24009353]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 333 is [True, False, False, False, False, True]
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 333 of 1
Current timestep = 334. State = [[-0.05496616  0.09789827]]. Action = [[-0.05095357 -0.14425927 -0.20120938  0.8506274 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 334 is [True, False, False, False, True, False]
Current timestep = 335. State = [[-0.04973968  0.0761736 ]]. Action = [[ 0.10294899 -0.20450842  0.1828137   0.7711172 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 335 is [True, False, False, False, True, False]
Current timestep = 336. State = [[-0.03660815  0.05047383]]. Action = [[ 0.21618122 -0.09082878 -0.14932519  0.5124934 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 336 is [False, True, False, False, True, False]
Scene graph at timestep 336 is [False, True, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of 1
Current timestep = 337. State = [[-0.15354858  0.12344261]]. Action = [[-0.09131423  0.19829902  0.197389   -0.23867846]]. Reward = [100.]
Curr episode timestep = 121
Scene graph at timestep 337 is [False, True, False, False, True, False]
Current timestep = 338. State = [[-0.14109196  0.12807405]]. Action = [[-0.23294699 -0.2100856   0.08131787  0.39389062]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 338 is [True, False, False, False, True, False]
Current timestep = 339. State = [[-0.14997591  0.1204159 ]]. Action = [[-0.06355405  0.03478318 -0.13658953 -0.96529466]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 339 is [True, False, False, False, False, True]
Current timestep = 340. State = [[-0.16460754  0.13352105]]. Action = [[-0.14666927  0.18363124 -0.06470013 -0.48349476]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 340 is [True, False, False, False, True, False]
Current timestep = 341. State = [[-0.17228855  0.14097716]]. Action = [[ 0.12548941 -0.06252959 -0.1287694   0.86808884]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 341 is [True, False, False, False, False, True]
Scene graph at timestep 341 is [True, False, False, False, False, True]
State prediction error at timestep 341 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 341 of -1
Current timestep = 342. State = [[-0.16554122  0.13390575]]. Action = [[ 0.1273287  -0.06219301 -0.00214368 -0.5192667 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 342 is [True, False, False, False, False, True]
Scene graph at timestep 342 is [True, False, False, False, False, True]
State prediction error at timestep 342 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 342 of 0
Current timestep = 343. State = [[-0.16200922  0.12743379]]. Action = [[-0.08801344 -0.01617248  0.18468368 -0.853348  ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 343 is [True, False, False, False, False, True]
Scene graph at timestep 343 is [True, False, False, False, False, True]
State prediction error at timestep 343 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 343 of 0
Current timestep = 344. State = [[-0.16789769  0.13959184]]. Action = [[-0.09320292  0.22235256 -0.16529956 -0.23928273]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 344 is [True, False, False, False, False, True]
Current timestep = 345. State = [[-0.1688171   0.15579659]]. Action = [[ 0.21918905  0.05780512 -0.17969154  0.2300632 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 345 is [True, False, False, False, False, True]
Current timestep = 346. State = [[-0.16200095  0.17054568]]. Action = [[-0.04709798  0.17346644 -0.06327294 -0.59636116]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 346 is [True, False, False, False, False, True]
Current timestep = 347. State = [[-0.16501279  0.18400122]]. Action = [[-0.05491307 -0.01834229 -0.12457028  0.1944015 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 347 is [True, False, False, False, False, True]
Current timestep = 348. State = [[-0.16269137  0.18743774]]. Action = [[ 0.12141156  0.06346214 -0.06372711 -0.9146321 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 348 is [True, False, False, False, False, True]
Current timestep = 349. State = [[-0.14580198  0.19505414]]. Action = [[ 0.23506814  0.06122673 -0.23487946  0.4140805 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 349 is [True, False, False, False, False, True]
Current timestep = 350. State = [[-0.11846063  0.21098836]]. Action = [[ 0.18454278  0.17485136 -0.02216415  0.89166665]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 350 is [True, False, False, False, False, True]
Current timestep = 351. State = [[-0.09435473  0.23771033]]. Action = [[ 0.14107591  0.24697095  0.00380176 -0.44535065]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 351 is [True, False, False, False, False, True]
Current timestep = 352. State = [[-0.0756296   0.26652506]]. Action = [[ 0.02306464  0.09441692 -0.24188091  0.11775684]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 352 is [True, False, False, False, False, True]
Current timestep = 353. State = [[-0.07000342  0.27768785]]. Action = [[-0.07685539 -0.02658987  0.06675404  0.93384314]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 353 is [True, False, False, False, False, True]
Scene graph at timestep 353 is [True, False, False, False, False, True]
State prediction error at timestep 353 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of -1
Current timestep = 354. State = [[-0.07773823  0.28586698]]. Action = [[-0.19247578  0.05499697  0.13065362 -0.2153033 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 354 is [True, False, False, False, False, True]
Current timestep = 355. State = [[-0.09313909  0.28635177]]. Action = [[-0.22394033 -0.15468085  0.01047418  0.9545758 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 355 is [True, False, False, False, False, True]
Scene graph at timestep 355 is [True, False, False, False, False, True]
State prediction error at timestep 355 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 355 of 0
Current timestep = 356. State = [[-0.1125725   0.27691442]]. Action = [[0.22321516 0.22164997 0.0144591  0.01086545]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 356 is [True, False, False, False, False, True]
Current timestep = 357. State = [[-0.10960156  0.27666348]]. Action = [[0.19677186 0.03912866 0.17306718 0.84920716]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 357 is [True, False, False, False, False, True]
Current timestep = 358. State = [[-0.1030056   0.26746285]]. Action = [[-0.00548765 -0.15036213 -0.03651538 -0.3460257 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 358 is [True, False, False, False, False, True]
Scene graph at timestep 358 is [True, False, False, False, False, True]
State prediction error at timestep 358 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 358 of 0
Current timestep = 359. State = [[-0.10068224  0.26259527]]. Action = [[ 0.00122848  0.12449238  0.14315343 -0.19416016]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 359 is [True, False, False, False, False, True]
Current timestep = 360. State = [[-0.10795187  0.27356076]]. Action = [[-0.1596319   0.05849501 -0.14553948  0.2814207 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 360 is [True, False, False, False, False, True]
Current timestep = 361. State = [[-0.11136536  0.27236593]]. Action = [[ 0.02215523 -0.17188063  0.03142589 -0.9234156 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 361 is [True, False, False, False, False, True]
Current timestep = 362. State = [[-0.10736135  0.25065014]]. Action = [[ 0.01620606 -0.24321795 -0.09222263 -0.47820926]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 362 is [True, False, False, False, False, True]
Current timestep = 363. State = [[-0.1076329   0.22613718]]. Action = [[-0.03413767 -0.10713588 -0.07732102  0.5112666 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 363 is [True, False, False, False, False, True]
Current timestep = 364. State = [[-0.10664368  0.21204701]]. Action = [[ 0.02713791 -0.06024179 -0.11675397  0.78519106]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 364 is [True, False, False, False, False, True]
Current timestep = 365. State = [[-0.10639117  0.20808728]]. Action = [[ 0.02982983  0.04438159  0.10693735 -0.5207699 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 365 is [True, False, False, False, False, True]
Current timestep = 366. State = [[-0.10766723  0.21020745]]. Action = [[-0.02255471  0.05260473 -0.0495851   0.7487726 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 366 is [True, False, False, False, False, True]
Current timestep = 367. State = [[-0.11688212  0.22580376]]. Action = [[-0.22709154  0.23475122 -0.19252907  0.30015385]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 367 is [True, False, False, False, False, True]
Current timestep = 368. State = [[-0.12319912  0.23406726]]. Action = [[ 0.21728921 -0.17926498  0.14526594 -0.29385465]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 368 is [True, False, False, False, False, True]
Scene graph at timestep 368 is [True, False, False, False, False, True]
State prediction error at timestep 368 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of 0
Current timestep = 369. State = [[-0.12166633  0.2310265 ]]. Action = [[-0.15499626  0.08622068  0.03981349  0.8583256 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 369 is [True, False, False, False, False, True]
Current timestep = 370. State = [[-0.12706245  0.2385346 ]]. Action = [[ 0.07790035  0.10489422 -0.19679001  0.02546442]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 370 is [True, False, False, False, False, True]
Current timestep = 371. State = [[-0.12068816  0.25007227]]. Action = [[ 0.24379477  0.15154967  0.07671839 -0.8014113 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 371 is [True, False, False, False, False, True]
Current timestep = 372. State = [[-0.10085139  0.27334186]]. Action = [[ 0.16689146  0.20429564 -0.13201538  0.16755593]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 372 is [True, False, False, False, False, True]
Current timestep = 373. State = [[-0.08567996  0.27968216]]. Action = [[-0.08692144 -0.21178454  0.24701792 -0.28329337]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 373 is [True, False, False, False, False, True]
Current timestep = 374. State = [[-0.0892192   0.27008563]]. Action = [[-0.2022156  -0.07035786 -0.21286586 -0.82580423]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 374 is [True, False, False, False, False, True]
Current timestep = 375. State = [[-0.0951618  0.2652904]]. Action = [[ 0.06198102  0.21785116 -0.07816988  0.02401876]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 375 is [True, False, False, False, False, True]
Scene graph at timestep 375 is [True, False, False, False, False, True]
State prediction error at timestep 375 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of 1
Current timestep = 376. State = [[-0.09625031  0.2547351 ]]. Action = [[ 0.00405407 -0.1703794   0.04389277  0.4871452 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 376 is [True, False, False, False, False, True]
Current timestep = 377. State = [[-0.10182838  0.2520534 ]]. Action = [[-0.0751045   0.1407988  -0.10313544 -0.7006201 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 377 is [True, False, False, False, False, True]
Current timestep = 378. State = [[-0.10954388  0.24553142]]. Action = [[-0.12965229 -0.23835303 -0.19093241 -0.76053226]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 378 is [True, False, False, False, False, True]
Current timestep = 379. State = [[-0.12281439  0.21685891]]. Action = [[-0.03744163 -0.21716107  0.06888103  0.71892476]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 379 is [True, False, False, False, False, True]
Current timestep = 380. State = [[-0.12521082  0.20509478]]. Action = [[0.1866945  0.15669322 0.20477206 0.50083673]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 380 is [True, False, False, False, False, True]
Current timestep = 381. State = [[-0.11492855  0.2185519 ]]. Action = [[0.21736535 0.21016452 0.24263626 0.6173973 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 381 is [True, False, False, False, False, True]
Current timestep = 382. State = [[-0.10563274  0.23065162]]. Action = [[-0.17945486 -0.08950488  0.09484452  0.74871814]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 382 is [True, False, False, False, False, True]
Current timestep = 383. State = [[-0.10571108  0.23109472]]. Action = [[ 0.08771634  0.05869588  0.20380464 -0.41342103]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 383 is [True, False, False, False, False, True]
Current timestep = 384. State = [[-0.10845561  0.23504433]]. Action = [[-0.12464388  0.02083099 -0.16498818 -0.07761914]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 384 is [True, False, False, False, False, True]
Current timestep = 385. State = [[-0.1189127   0.23551922]]. Action = [[-0.21147312 -0.09334016 -0.04019749  0.88650846]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 385 is [True, False, False, False, False, True]
Current timestep = 386. State = [[-0.12949659  0.23719645]]. Action = [[ 0.21049184  0.176436   -0.18174878  0.07773995]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 386 is [True, False, False, False, False, True]
Scene graph at timestep 386 is [True, False, False, False, False, True]
State prediction error at timestep 386 is tensor(2.2049e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of 0
Current timestep = 387. State = [[-0.12752382  0.24501848]]. Action = [[-0.09694062 -0.0374684   0.23236835 -0.8715086 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 387 is [True, False, False, False, False, True]
Current timestep = 388. State = [[-0.13331404  0.24839129]]. Action = [[-0.14564915  0.01308697  0.21562934  0.94717824]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 388 is [True, False, False, False, False, True]
Current timestep = 389. State = [[-0.13768914  0.25420454]]. Action = [[ 0.21344203  0.09995887 -0.02395359  0.03218007]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 389 is [True, False, False, False, False, True]
Current timestep = 390. State = [[-0.13518229  0.25568378]]. Action = [[-0.02118364 -0.04196391 -0.18509674 -0.73977387]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 390 is [True, False, False, False, False, True]
Current timestep = 391. State = [[-0.13247252  0.25881457]]. Action = [[ 0.08822432  0.08886677 -0.24507093 -0.8495186 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 391 is [True, False, False, False, False, True]
Current timestep = 392. State = [[-0.13062777  0.25717306]]. Action = [[-0.12317893 -0.14714733  0.11124644  0.37920105]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 392 is [True, False, False, False, False, True]
Current timestep = 393. State = [[-0.12635735  0.246836  ]]. Action = [[ 0.06866702 -0.1092027   0.0259749  -0.9264853 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 393 is [True, False, False, False, False, True]
Current timestep = 394. State = [[-0.1280219   0.23371585]]. Action = [[-0.18101798 -0.0868583   0.19249672 -0.87215185]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 394 is [True, False, False, False, False, True]
Scene graph at timestep 394 is [True, False, False, False, False, True]
State prediction error at timestep 394 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 394 of 0
Current timestep = 395. State = [[-0.13611215  0.2266941 ]]. Action = [[-0.09795091 -0.00431956 -0.14258839 -0.92493963]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 395 is [True, False, False, False, False, True]
Current timestep = 396. State = [[-0.14857298  0.22036834]]. Action = [[-0.06565884 -0.07369924 -0.07285926 -0.86535543]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 396 is [True, False, False, False, False, True]
Current timestep = 397. State = [[-0.15999702  0.20184009]]. Action = [[-0.19463633 -0.22718886 -0.22372048 -0.00527114]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 397 is [True, False, False, False, False, True]
Current timestep = 398. State = [[-0.17883347  0.19284157]]. Action = [[ 0.11831498  0.19932461 -0.1773183   0.52805054]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 398 is [True, False, False, False, False, True]
Current timestep = 399. State = [[-0.17669345  0.19080925]]. Action = [[ 0.03237745 -0.17570429 -0.14721645  0.09928775]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 399 is [True, False, False, False, False, True]
Current timestep = 400. State = [[-0.18195902  0.18856873]]. Action = [[-0.21419679  0.05612952  0.08292085 -0.4265647 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 400 is [True, False, False, False, False, True]
Current timestep = 401. State = [[-0.18860134  0.17578955]]. Action = [[ 0.18010831 -0.18841316  0.18359905 -0.7475998 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 401 is [True, False, False, False, False, True]
Scene graph at timestep 401 is [True, False, False, False, False, True]
State prediction error at timestep 401 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 401 of -1
Current timestep = 402. State = [[-0.17976363  0.1727742 ]]. Action = [[ 0.08913878  0.24625134  0.18434572 -0.3395158 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 402 is [True, False, False, False, False, True]
Current timestep = 403. State = [[-0.1762555   0.19659035]]. Action = [[ 0.07772166  0.23349848  0.14052862 -0.52624476]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 403 is [True, False, False, False, False, True]
Current timestep = 404. State = [[-0.17629993  0.214844  ]]. Action = [[-0.16306582 -0.01397708 -0.21151955  0.97687733]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 404 is [True, False, False, False, False, True]
Current timestep = 405. State = [[-0.17908464  0.22471905]]. Action = [[ 0.13203603  0.11683407  0.24145648 -0.7330185 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 405 is [True, False, False, False, False, True]
Scene graph at timestep 405 is [True, False, False, False, False, True]
State prediction error at timestep 405 is tensor(3.2760e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of -1
Current timestep = 406. State = [[-0.17750484  0.23338774]]. Action = [[-0.08725801 -0.02503423  0.20968336  0.37680697]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 406 is [True, False, False, False, False, True]
Scene graph at timestep 406 is [True, False, False, False, False, True]
State prediction error at timestep 406 is tensor(6.7746e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 406 of -1
Current timestep = 407. State = [[-0.18378167  0.2337354 ]]. Action = [[-0.15762804 -0.04298642 -0.18515891 -0.3427683 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 407 is [True, False, False, False, False, True]
Current timestep = 408. State = [[-0.20127349  0.2439882 ]]. Action = [[-0.22346304  0.16122106 -0.08052123  0.86819434]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 408 is [True, False, False, False, False, True]
Current timestep = 409. State = [[-0.23211329  0.2639739 ]]. Action = [[-0.22182788  0.08693412 -0.1447163   0.27762687]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 409 is [True, False, False, False, False, True]
Current timestep = 410. State = [[-0.25434914  0.27810737]]. Action = [[ 0.07251081  0.16731527 -0.10618785  0.04745114]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 410 is [True, False, False, False, False, True]
Current timestep = 411. State = [[-0.25231776  0.2875963 ]]. Action = [[ 0.18960547  0.05792844 -0.12417576  0.29758406]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 411 is [True, False, False, False, False, True]
Current timestep = 412. State = [[-0.23432209  0.2887621 ]]. Action = [[ 0.24549901 -0.06862801  0.14198297 -0.56344277]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 412 is [True, False, False, False, False, True]
Current timestep = 413. State = [[-0.20392275  0.27335122]]. Action = [[ 0.21866298 -0.20808208  0.06884074 -0.8340816 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 413 is [True, False, False, False, False, True]
Scene graph at timestep 413 is [True, False, False, False, False, True]
State prediction error at timestep 413 is tensor(8.4907e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 413 of -1
Current timestep = 414. State = [[-0.18454923  0.2566694 ]]. Action = [[-0.2264576  -0.04117942 -0.19004382 -0.10157883]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 414 is [True, False, False, False, False, True]
Scene graph at timestep 414 is [True, False, False, False, False, True]
State prediction error at timestep 414 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 414 of -1
Current timestep = 415. State = [[-0.1989818  0.2676764]]. Action = [[-0.16810809  0.20314425 -0.24371783 -0.6820367 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 415 is [True, False, False, False, False, True]
Scene graph at timestep 415 is [True, False, False, False, False, True]
State prediction error at timestep 415 is tensor(6.3501e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of -1
Current timestep = 416. State = [[-0.21021181  0.27161333]]. Action = [[ 0.1527695 -0.2338304 -0.0864673 -0.8487892]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 416 is [True, False, False, False, False, True]
Scene graph at timestep 416 is [True, False, False, False, False, True]
State prediction error at timestep 416 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 416 of -1
Current timestep = 417. State = [[-0.19599174  0.2592567 ]]. Action = [[ 0.17338598  0.16009003  0.00911257 -0.935563  ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 417 is [True, False, False, False, False, True]
Current timestep = 418. State = [[-0.19126146  0.25514492]]. Action = [[-0.10928768 -0.23596124 -0.15787645 -0.8140349 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 418 is [True, False, False, False, False, True]
Current timestep = 419. State = [[-0.18620297  0.25128925]]. Action = [[ 0.19975722  0.15553644 -0.03378552 -0.48955643]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 419 is [True, False, False, False, False, True]
Scene graph at timestep 419 is [True, False, False, False, False, True]
State prediction error at timestep 419 is tensor(9.9363e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 419 of 0
Current timestep = 420. State = [[-0.17550012  0.25551584]]. Action = [[-0.10808724 -0.08921078 -0.0613801  -0.5461469 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 420 is [True, False, False, False, False, True]
Current timestep = 421. State = [[-0.17715655  0.2390935 ]]. Action = [[-0.04143372 -0.23766418  0.13835287 -0.75794226]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 421 is [True, False, False, False, False, True]
Current timestep = 422. State = [[-0.17232496  0.21787141]]. Action = [[ 0.20810246 -0.0572091   0.05391866  0.6660167 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 422 is [True, False, False, False, False, True]
Scene graph at timestep 422 is [True, False, False, False, False, True]
State prediction error at timestep 422 is tensor(8.7179e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 422 of 1
Current timestep = 423. State = [[-0.15805167  0.19270058]]. Action = [[ 0.02307817 -0.2164128  -0.2028207  -0.29595387]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 423 is [True, False, False, False, False, True]
Current timestep = 424. State = [[-0.14185254  0.18093236]]. Action = [[ 0.22938019  0.07512271  0.01581308 -0.5377509 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 424 is [True, False, False, False, False, True]
Current timestep = 425. State = [[-0.1350859   0.18099558]]. Action = [[-0.17682119 -0.01075864 -0.06539004 -0.86318505]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 425 is [True, False, False, False, False, True]
Scene graph at timestep 425 is [True, False, False, False, False, True]
State prediction error at timestep 425 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 425 of 1
Current timestep = 426. State = [[-0.14512394  0.1931845 ]]. Action = [[-0.18108834  0.19545844 -0.20472112  0.8336246 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 426 is [True, False, False, False, False, True]
Current timestep = 427. State = [[-0.16681975  0.20863973]]. Action = [[-0.23777887 -0.02549854  0.16467059  0.03420055]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 427 is [True, False, False, False, False, True]
Scene graph at timestep 427 is [True, False, False, False, False, True]
State prediction error at timestep 427 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 427 of -1
Current timestep = 428. State = [[-0.18933494  0.20994693]]. Action = [[0.08654994 0.01713282 0.0652107  0.38619113]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 428 is [True, False, False, False, False, True]
Current timestep = 429. State = [[-0.18014246  0.21506058]]. Action = [[ 0.24286568  0.11205813 -0.14121507  0.06304157]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 429 is [True, False, False, False, False, True]
Current timestep = 430. State = [[-0.16465001  0.21547957]]. Action = [[ 0.10677531 -0.0684891   0.13982052  0.2063129 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 430 is [True, False, False, False, False, True]
Scene graph at timestep 430 is [True, False, False, False, False, True]
State prediction error at timestep 430 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 430 of 1
Current timestep = 431. State = [[-0.1549908   0.20630066]]. Action = [[-0.13981937 -0.13971603  0.00765207 -0.73202527]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 431 is [True, False, False, False, False, True]
Current timestep = 432. State = [[-0.15585881  0.21168636]]. Action = [[ 0.05333883  0.2202617   0.22080484 -0.01654118]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 432 is [True, False, False, False, False, True]
Scene graph at timestep 432 is [True, False, False, False, False, True]
State prediction error at timestep 432 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 432 of 0
Current timestep = 433. State = [[-0.14912346  0.23524158]]. Action = [[ 0.21433169  0.23809579 -0.16060214 -0.25535417]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 433 is [True, False, False, False, False, True]
Current timestep = 434. State = [[-0.13061896  0.265318  ]]. Action = [[ 0.11945483  0.17556185 -0.03945574 -0.18464684]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 434 is [True, False, False, False, False, True]
Current timestep = 435. State = [[-0.12360828  0.28043032]]. Action = [[-0.21223217 -0.05449539 -0.19676174  0.7645681 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 435 is [True, False, False, False, False, True]
Scene graph at timestep 435 is [True, False, False, False, False, True]
State prediction error at timestep 435 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 435 of -1
Current timestep = 436. State = [[-0.12228128  0.28544977]]. Action = [[0.18020245 0.08502701 0.18657327 0.30472958]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 436 is [True, False, False, False, False, True]
Scene graph at timestep 436 is [True, False, False, False, False, True]
State prediction error at timestep 436 is tensor(9.5127e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 436 of 0
Current timestep = 437. State = [[-0.12051024  0.28787905]]. Action = [[ 0.22319245  0.15382445 -0.23374155  0.16959035]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 437 is [True, False, False, False, False, True]
Current timestep = 438. State = [[-0.12051024  0.28787905]]. Action = [[-0.15023878  0.23981619 -0.03354931 -0.92720366]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 438 is [True, False, False, False, False, True]
Current timestep = 439. State = [[-0.12051024  0.28787905]]. Action = [[-0.21423769  0.20412177 -0.04678451 -0.01284671]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 439 is [True, False, False, False, False, True]
Current timestep = 440. State = [[-0.12576212  0.29443857]]. Action = [[-0.17817125  0.07800838 -0.0830915   0.3165846 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 440 is [True, False, False, False, False, True]
Current timestep = 441. State = [[-0.13088006  0.30107978]]. Action = [[ 0.08280641  0.22994149  0.19560763 -0.3560965 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 441 is [True, False, False, False, False, True]
Current timestep = 442. State = [[-0.13158904  0.30171025]]. Action = [[-0.1684495   0.11131623 -0.11809352 -0.05603689]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 442 is [True, False, False, False, False, True]
Current timestep = 443. State = [[-0.13227439  0.3007001 ]]. Action = [[-0.04761383 -0.04609993 -0.0246868   0.396163  ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 443 is [True, False, False, False, False, True]
Current timestep = 444. State = [[-0.13307546  0.29942155]]. Action = [[ 0.0749189   0.03569871 -0.1113269  -0.36493307]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 444 is [True, False, False, False, False, True]
Current timestep = 445. State = [[-0.13358438  0.2993084 ]]. Action = [[-0.18891191  0.21547747  0.10923332  0.90760636]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 445 is [True, False, False, False, False, True]
Current timestep = 446. State = [[-0.13078822  0.2910358 ]]. Action = [[ 0.05827746 -0.14217834 -0.04846555 -0.51525605]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 446 is [True, False, False, False, False, True]
Current timestep = 447. State = [[-0.12896359  0.28469372]]. Action = [[0.0638606  0.11862415 0.14784974 0.6831136 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 447 is [True, False, False, False, False, True]
Current timestep = 448. State = [[-0.12878731  0.28373685]]. Action = [[ 0.15136838  0.20794952  0.07481784 -0.2037518 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 448 is [True, False, False, False, False, True]
Current timestep = 449. State = [[-0.12370962  0.28403333]]. Action = [[0.1959638  0.07539418 0.06128436 0.67542315]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 449 is [True, False, False, False, False, True]
Current timestep = 450. State = [[-0.11852078  0.28003493]]. Action = [[-0.07550642 -0.09527551 -0.21484652 -0.93959713]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 450 is [True, False, False, False, False, True]
Current timestep = 451. State = [[-0.11397424  0.2710949 ]]. Action = [[ 0.08827972 -0.08686939 -0.08810647 -0.33581626]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 451 is [True, False, False, False, False, True]
Current timestep = 452. State = [[-0.10389663  0.26545352]]. Action = [[ 0.16197124  0.06796527 -0.2040512  -0.3226546 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 452 is [True, False, False, False, False, True]
Current timestep = 453. State = [[-0.09979373  0.28101897]]. Action = [[-0.22737738  0.20300466 -0.05394033  0.3455087 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 453 is [True, False, False, False, False, True]
Current timestep = 454. State = [[-0.10801168  0.28389624]]. Action = [[-0.03844875 -0.23484237 -0.16950785  0.5202429 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 454 is [True, False, False, False, False, True]
Scene graph at timestep 454 is [True, False, False, False, False, True]
State prediction error at timestep 454 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 454 of 0
Current timestep = 455. State = [[-0.10743401  0.27210152]]. Action = [[ 0.15607518  0.0437668  -0.10219826  0.5255457 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 455 is [True, False, False, False, False, True]
Current timestep = 456. State = [[-0.1051042  0.2746939]]. Action = [[-0.02531202  0.05817476  0.05346271 -0.30553216]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 456 is [True, False, False, False, False, True]
Current timestep = 457. State = [[-0.09978443  0.28617513]]. Action = [[ 0.10753009  0.13242358 -0.19435829  0.8327253 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 457 is [True, False, False, False, False, True]
Current timestep = 458. State = [[-0.09422052  0.29762477]]. Action = [[ 0.00946996  0.02650094 -0.14684157  0.7228384 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 458 is [True, False, False, False, False, True]
Current timestep = 459. State = [[-0.09216585  0.3015576 ]]. Action = [[ 0.20619622  0.2169171  -0.16807447 -0.18497998]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 459 is [True, False, False, False, False, True]
Current timestep = 460. State = [[-0.09243816  0.29441884]]. Action = [[-0.13627093 -0.18273117  0.02258068  0.41517115]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 460 is [True, False, False, False, False, True]
Scene graph at timestep 460 is [True, False, False, False, False, True]
State prediction error at timestep 460 is tensor(5.1626e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of 0
Current timestep = 461. State = [[-0.10238614  0.27696055]]. Action = [[-0.2226346  -0.10069577  0.00511587 -0.6760397 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 461 is [True, False, False, False, False, True]
Current timestep = 462. State = [[-0.12076718  0.25573292]]. Action = [[-0.14135617 -0.23869759 -0.11297005  0.5616785 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 462 is [True, False, False, False, False, True]
Current timestep = 463. State = [[-0.14061028  0.24530412]]. Action = [[-0.0930337   0.1055136   0.19171214  0.2648412 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 463 is [True, False, False, False, False, True]
Current timestep = 464. State = [[-0.2276679  -0.13888879]]. Action = [[ 0.19888246  0.11458504 -0.21855615 -0.43471098]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 464 is [True, False, False, False, False, True]
Current timestep = 465. State = [[-0.21165113 -0.15392119]]. Action = [[ 0.24089116  0.00544623  0.18307832 -0.3457904 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 465 is [True, False, False, True, False, False]
Scene graph at timestep 465 is [True, False, False, True, False, False]
State prediction error at timestep 465 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of 1
Current timestep = 466. State = [[-0.1787205 -0.1598772]]. Action = [[ 0.2469039  -0.03645393 -0.10774127  0.83775413]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 466 is [True, False, False, True, False, False]
Scene graph at timestep 466 is [True, False, False, True, False, False]
State prediction error at timestep 466 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 466 of 1
Current timestep = 467. State = [[-0.15695815 -0.15617341]]. Action = [[-0.06418031  0.14273071 -0.06724733 -0.13835281]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 467 is [True, False, False, True, False, False]
Current timestep = 468. State = [[-0.15741284 -0.15510173]]. Action = [[ 0.00517735 -0.14003344  0.0140036  -0.889192  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 468 is [True, False, False, True, False, False]
Current timestep = 469. State = [[-0.15182218 -0.15020978]]. Action = [[ 0.157944    0.15368965  0.20814443 -0.05309641]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 469 is [True, False, False, True, False, False]
Scene graph at timestep 469 is [True, False, False, True, False, False]
State prediction error at timestep 469 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 469 of 1
Current timestep = 470. State = [[-0.13043518 -0.13388722]]. Action = [[ 0.19815588  0.1559164  -0.04268065 -0.4351101 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 470 is [True, False, False, True, False, False]
Current timestep = 471. State = [[-0.11180086 -0.12959243]]. Action = [[ 0.03359669 -0.13129926 -0.1421451  -0.38623345]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 471 is [True, False, False, True, False, False]
Current timestep = 472. State = [[-0.09664468 -0.13037713]]. Action = [[ 0.18454272  0.06602144 -0.19042061  0.9361384 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 472 is [True, False, False, True, False, False]
Current timestep = 473. State = [[-0.08620946 -0.1294013 ]]. Action = [[-0.19073667  0.0193094  -0.08539519 -0.75822014]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 473 is [True, False, False, True, False, False]
Scene graph at timestep 473 is [True, False, False, True, False, False]
State prediction error at timestep 473 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 473 of 1
Current timestep = 474. State = [[-0.08467996 -0.12781923]]. Action = [[ 0.2065829   0.01351601 -0.08153948  0.48780334]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 474 is [True, False, False, True, False, False]
Current timestep = 475. State = [[-0.06588032 -0.11164661]]. Action = [[ 0.2300658   0.23490888 -0.03013495  0.92019486]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 475 is [True, False, False, True, False, False]
Current timestep = 476. State = [[-0.04781226 -0.10562532]]. Action = [[-0.03006136 -0.19484946 -0.23209497 -0.40973508]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 476 is [True, False, False, False, True, False]
Current timestep = 477. State = [[-0.04609111 -0.112475  ]]. Action = [[ 0.0024935  -0.01305825 -0.09697255 -0.63202244]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 477 is [False, True, False, False, True, False]
Current timestep = 478. State = [[-0.04354649 -0.11707301]]. Action = [[ 0.06582946 -0.04370119 -0.15751614 -0.3077736 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 478 is [False, True, False, False, True, False]
Current timestep = 479. State = [[-0.03797204 -0.11925461]]. Action = [[0.01597002 0.04489729 0.09458229 0.15677917]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 479 is [False, True, False, False, True, False]
Current timestep = 480. State = [[-0.03637115 -0.10944224]]. Action = [[ 0.01847926  0.15086052 -0.12869477  0.48125112]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 480 is [False, True, False, False, True, False]
Current timestep = 481. State = [[-0.17136963  0.07143233]]. Action = [[-0.16100734  0.12074393 -0.09437202  0.94058216]]. Reward = [100.]
Curr episode timestep = 16
Scene graph at timestep 481 is [False, True, False, False, True, False]
Scene graph at timestep 481 is [True, False, False, False, True, False]
State prediction error at timestep 481 is tensor(0.0230, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 481 of 1
Current timestep = 482. State = [[-0.14713775  0.08668532]]. Action = [[ 0.21024644  0.12466338 -0.07204291 -0.63277835]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 482 is [True, False, False, False, True, False]
Current timestep = 483. State = [[-0.11931576  0.11082204]]. Action = [[ 0.23681253  0.23484886 -0.23182556 -0.907217  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 483 is [True, False, False, False, True, False]
Current timestep = 484. State = [[-0.10304174  0.12645459]]. Action = [[-0.17501596 -0.0811891  -0.20451213  0.08352852]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 484 is [True, False, False, False, True, False]
Current timestep = 485. State = [[-0.10500986  0.12700337]]. Action = [[-0.03937232  0.00765368 -0.1561846   0.9581764 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 485 is [True, False, False, False, False, True]
Current timestep = 486. State = [[-0.10495129  0.12419096]]. Action = [[ 0.10692731 -0.04683393  0.11441785  0.43079388]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 486 is [True, False, False, False, False, True]
Current timestep = 487. State = [[-0.10855096  0.11155897]]. Action = [[-0.21890058 -0.1628104   0.00483531  0.9611602 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 487 is [True, False, False, False, True, False]
Current timestep = 488. State = [[-0.10683031  0.09349038]]. Action = [[ 0.22172236 -0.11426336  0.02185008  0.6740241 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 488 is [True, False, False, False, True, False]
Current timestep = 489. State = [[-0.10155527  0.07627138]]. Action = [[ 0.03258887 -0.11611429  0.1180253   0.56665444]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 489 is [True, False, False, False, True, False]
Current timestep = 490. State = [[-0.09717666  0.07790747]]. Action = [[ 0.07490307  0.18792284 -0.1509382  -0.71622354]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 490 is [True, False, False, False, True, False]
Scene graph at timestep 490 is [True, False, False, False, True, False]
State prediction error at timestep 490 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 490 of 1
Current timestep = 491. State = [[-0.08231581  0.09802488]]. Action = [[ 0.19967389  0.18915385 -0.08493784  0.8712208 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 491 is [True, False, False, False, True, False]
Scene graph at timestep 491 is [True, False, False, False, True, False]
State prediction error at timestep 491 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 491 of 1
Current timestep = 492. State = [[-0.06044243  0.10427856]]. Action = [[-0.17293301 -0.2086855  -0.23246548  0.4856707 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 492 is [True, False, False, False, True, False]
Current timestep = 493. State = [[-0.07060789  0.08573575]]. Action = [[-0.20605487 -0.12088242  0.06772515  0.5330851 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 493 is [True, False, False, False, True, False]
Scene graph at timestep 493 is [True, False, False, False, True, False]
State prediction error at timestep 493 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 493 of -1
Current timestep = 494. State = [[-0.08458673  0.06830351]]. Action = [[ 0.05863175 -0.0854381   0.05114633  0.18434525]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 494 is [True, False, False, False, True, False]
Current timestep = 495. State = [[-0.08273689  0.07291598]]. Action = [[ 0.14444059  0.19460505  0.1183697  -0.9708194 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 495 is [True, False, False, False, True, False]
Current timestep = 496. State = [[-0.08404011  0.0821959 ]]. Action = [[-0.14886019 -0.01603962 -0.16741234 -0.00048029]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 496 is [True, False, False, False, True, False]
Current timestep = 497. State = [[-0.09281902  0.07575722]]. Action = [[-0.18565252 -0.15786316  0.20783967 -0.67447686]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 497 is [True, False, False, False, True, False]
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(2.7987e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of -1
Current timestep = 498. State = [[-0.10297944  0.07619703]]. Action = [[0.12793499 0.20266467 0.0033032  0.25020587]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 498 is [True, False, False, False, True, False]
Current timestep = 499. State = [[-0.10790589  0.10055953]]. Action = [[-0.03613691  0.22890109 -0.17949758  0.64524007]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 499 is [True, False, False, False, True, False]
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of -1
Current timestep = 500. State = [[-0.1180112   0.11503514]]. Action = [[-0.23307045 -0.15362363 -0.03613625 -0.07924032]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 500 is [True, False, False, False, True, False]
Current timestep = 501. State = [[-0.13641965  0.12065827]]. Action = [[-0.22993228  0.17999908  0.2451764   0.24708533]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 501 is [True, False, False, False, True, False]
Scene graph at timestep 501 is [True, False, False, False, True, False]
State prediction error at timestep 501 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[-0.16179991  0.12749518]]. Action = [[-0.09261757 -0.12624411 -0.02873728  0.966815  ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 502 is [True, False, False, False, True, False]
Current timestep = 503. State = [[-0.1710775   0.12613207]]. Action = [[ 0.01790476  0.12733841 -0.09928334 -0.11424309]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 503 is [True, False, False, False, False, True]
Current timestep = 504. State = [[-0.16870633  0.13321035]]. Action = [[ 0.19663018  0.0203979  -0.07422216  0.9028094 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 504 is [True, False, False, False, False, True]
Scene graph at timestep 504 is [True, False, False, False, False, True]
State prediction error at timestep 504 is tensor(4.2655e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 504 of 0
Current timestep = 505. State = [[-0.16744994  0.14443205]]. Action = [[-0.05103612  0.1800172   0.19354975  0.160977  ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 505 is [True, False, False, False, False, True]
Current timestep = 506. State = [[-0.17839882  0.16594236]]. Action = [[-0.20040941  0.20559683  0.22144881  0.83195055]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 506 is [True, False, False, False, False, True]
Scene graph at timestep 506 is [True, False, False, False, False, True]
State prediction error at timestep 506 is tensor(5.8554e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 506 of -1
Current timestep = 507. State = [[-0.18635562  0.19988829]]. Action = [[ 0.21421188  0.21627754 -0.19001168 -0.03153038]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 507 is [True, False, False, False, False, True]
Current timestep = 508. State = [[-0.16512519  0.21786018]]. Action = [[ 0.23808366  0.09686455 -0.1050639   0.7465086 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 508 is [True, False, False, False, False, True]
Scene graph at timestep 508 is [True, False, False, False, False, True]
State prediction error at timestep 508 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 508 of 0
Current timestep = 509. State = [[-0.13711555  0.23267515]]. Action = [[0.16924697 0.02968639 0.11340317 0.0060606 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 509 is [True, False, False, False, False, True]
Current timestep = 510. State = [[-0.11564602  0.24739635]]. Action = [[0.18308157 0.20694381 0.19463533 0.17624402]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 510 is [True, False, False, False, False, True]
Scene graph at timestep 510 is [True, False, False, False, False, True]
State prediction error at timestep 510 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 510 of 0
Current timestep = 511. State = [[-0.07749649  0.25397778]]. Action = [[ 0.24726665 -0.22674032 -0.14884217  0.16563213]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 511 is [True, False, False, False, False, True]
Current timestep = 512. State = [[-0.06157687  0.23132105]]. Action = [[-0.22565126 -0.20397782 -0.20641506 -0.38773417]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 512 is [True, False, False, False, False, True]
Current timestep = 513. State = [[-0.05645219  0.21009538]]. Action = [[ 0.17838722 -0.10165501  0.08554539 -0.1390475 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 513 is [True, False, False, False, False, True]
Current timestep = 514. State = [[-0.05349332  0.20671387]]. Action = [[ 0.01887724  0.13485211  0.09827581 -0.0741486 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 514 is [True, False, False, False, False, True]
Current timestep = 515. State = [[-0.05712415  0.20274912]]. Action = [[-0.23905542 -0.17196901 -0.17830335 -0.35097146]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 515 is [True, False, False, False, False, True]
Current timestep = 516. State = [[-0.07031281  0.20177726]]. Action = [[-0.17427091  0.05786702  0.24045777  0.07545686]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 516 is [True, False, False, False, False, True]
Current timestep = 517. State = [[-0.08992777  0.19072747]]. Action = [[-0.2163741  -0.2452063  -0.1330676   0.69166636]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 517 is [True, False, False, False, False, True]
Scene graph at timestep 517 is [True, False, False, False, False, True]
State prediction error at timestep 517 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 517 of 0
Current timestep = 518. State = [[-0.12242509  0.15775767]]. Action = [[-0.17934133 -0.21745907  0.00944665  0.02924287]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 518 is [True, False, False, False, False, True]
Scene graph at timestep 518 is [True, False, False, False, False, True]
State prediction error at timestep 518 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 518 of -1
Current timestep = 519. State = [[-0.14190744  0.14129505]]. Action = [[ 0.22552556  0.162242   -0.20889752 -0.17937338]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 519 is [True, False, False, False, False, True]
Current timestep = 520. State = [[-0.12929805  0.15426293]]. Action = [[ 0.21441603  0.11820462  0.00872588 -0.3360318 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 520 is [True, False, False, False, False, True]
Current timestep = 521. State = [[-0.10788208  0.16895817]]. Action = [[ 0.15275264  0.05980235 -0.01462311  0.25502396]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 521 is [True, False, False, False, False, True]
Current timestep = 522. State = [[-0.09793185  0.16917606]]. Action = [[-0.16762462 -0.1474997   0.10035354  0.601123  ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 522 is [True, False, False, False, False, True]
Current timestep = 523. State = [[-0.09656996  0.15209322]]. Action = [[ 0.05124363 -0.18015794 -0.05779813 -0.4950887 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 523 is [True, False, False, False, False, True]
Current timestep = 524. State = [[-0.08831009  0.14622808]]. Action = [[0.17952299 0.15207008 0.03425628 0.91610575]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 524 is [True, False, False, False, False, True]
Current timestep = 525. State = [[-0.0858019   0.13776974]]. Action = [[-0.1616755  -0.23639369  0.20398295  0.33253813]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 525 is [True, False, False, False, False, True]
Current timestep = 526. State = [[-0.07952968  0.11707731]]. Action = [[ 0.2166403  -0.14108899  0.2096296  -0.53364193]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 526 is [True, False, False, False, False, True]
Current timestep = 527. State = [[-0.06398726  0.11112396]]. Action = [[ 0.2349191   0.18574804  0.14514065 -0.1215784 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 527 is [True, False, False, False, True, False]
Scene graph at timestep 527 is [True, False, False, False, True, False]
State prediction error at timestep 527 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 527 of 1
Current timestep = 528. State = [[-0.21135803 -0.15481898]]. Action = [[ 0.20399934 -0.07331491 -0.2470715  -0.94772977]]. Reward = [100.]
Curr episode timestep = 46
Scene graph at timestep 528 is [True, False, False, False, True, False]
Scene graph at timestep 528 is [True, False, False, True, False, False]
State prediction error at timestep 528 is tensor(0.0427, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 528 of -1
Current timestep = 529. State = [[-0.20444828 -0.181624  ]]. Action = [[-0.00372495 -0.15704753  0.16295427  0.99356246]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 529 is [True, False, False, True, False, False]
Scene graph at timestep 529 is [True, False, False, True, False, False]
State prediction error at timestep 529 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 529 of -1
Current timestep = 530. State = [[-0.21037541 -0.19344154]]. Action = [[-0.19001411  0.07781398  0.06761116  0.5919485 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 530 is [True, False, False, True, False, False]
Scene graph at timestep 530 is [True, False, False, True, False, False]
State prediction error at timestep 530 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 530 of 0
Current timestep = 531. State = [[-0.22178435 -0.19298688]]. Action = [[-0.14638057  0.01681915  0.00095055  0.86999226]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 531 is [True, False, False, True, False, False]
Current timestep = 532. State = [[-0.23837347 -0.20425352]]. Action = [[-0.08671585 -0.19251785  0.02046373  0.74795485]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 532 is [True, False, False, True, False, False]
Current timestep = 533. State = [[-0.25082368 -0.20669436]]. Action = [[-0.06013486  0.16141778 -0.0976208  -0.7821505 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 533 is [True, False, False, True, False, False]
Scene graph at timestep 533 is [True, False, False, True, False, False]
State prediction error at timestep 533 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 533 of -1
Current timestep = 534. State = [[-0.24840471 -0.21034652]]. Action = [[ 0.20581922 -0.22304681 -0.23933616  0.7629448 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 534 is [True, False, False, True, False, False]
Current timestep = 535. State = [[-0.2384056  -0.21989277]]. Action = [[ 0.11835021 -0.03226432  0.24628276 -0.81597006]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 535 is [True, False, False, True, False, False]
Current timestep = 536. State = [[-0.2266539  -0.22794247]]. Action = [[ 0.03272009 -0.06531142 -0.20414807 -0.32130086]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 536 is [True, False, False, True, False, False]
Scene graph at timestep 536 is [True, False, False, True, False, False]
State prediction error at timestep 536 is tensor(2.5995e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 536 of 0
Current timestep = 537. State = [[-0.21190307 -0.24685791]]. Action = [[ 0.2147063  -0.21923533 -0.15102816 -0.07524109]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 537 is [True, False, False, True, False, False]
Current timestep = 538. State = [[-0.18236434 -0.27125657]]. Action = [[ 0.24477571 -0.14487031  0.11076292 -0.7860357 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 538 is [True, False, False, True, False, False]
Current timestep = 539. State = [[-0.14978407 -0.27178103]]. Action = [[ 0.2124573   0.2309503   0.07288617 -0.66886395]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 539 is [True, False, False, True, False, False]
Scene graph at timestep 539 is [True, False, False, True, False, False]
State prediction error at timestep 539 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 539 of 0
Current timestep = 540. State = [[-0.11711054 -0.2466972 ]]. Action = [[ 0.22793359  0.24728212 -0.03036845 -0.64774984]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 540 is [True, False, False, True, False, False]
Current timestep = 541. State = [[-0.10389727 -0.22412184]]. Action = [[-0.23189633  0.11049032 -0.03398852 -0.51201195]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 541 is [True, False, False, True, False, False]
Current timestep = 542. State = [[-0.1075069  -0.20922665]]. Action = [[-0.03994775  0.09605718  0.05123216  0.8956027 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 542 is [True, False, False, True, False, False]
Scene graph at timestep 542 is [True, False, False, True, False, False]
State prediction error at timestep 542 is tensor(5.2689e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 542 of 1
Current timestep = 543. State = [[-0.12191409 -0.19130188]]. Action = [[-0.24523698  0.14499122  0.16152269 -0.04028523]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 543 is [True, False, False, True, False, False]
Scene graph at timestep 543 is [True, False, False, True, False, False]
State prediction error at timestep 543 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 543 of -1
Current timestep = 544. State = [[-0.1419121  -0.17657007]]. Action = [[ 0.10192937  0.03619036 -0.06403126  0.6945282 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 544 is [True, False, False, True, False, False]
Current timestep = 545. State = [[-0.14293465 -0.18493181]]. Action = [[-0.0349732  -0.21114899 -0.12139809  0.8609755 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 545 is [True, False, False, True, False, False]
Current timestep = 546. State = [[-0.1550622  -0.19982812]]. Action = [[-0.23948166 -0.04451118  0.06112298  0.4032675 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 546 is [True, False, False, True, False, False]
Current timestep = 547. State = [[-0.16312996 -0.21600918]]. Action = [[ 0.18071574 -0.1867922  -0.23598325 -0.568256  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 547 is [True, False, False, True, False, False]
Current timestep = 548. State = [[-0.15733682 -0.23579465]]. Action = [[ 0.08763865 -0.15400964  0.1285395  -0.85251313]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 548 is [True, False, False, True, False, False]
Current timestep = 549. State = [[-0.15403953 -0.24572039]]. Action = [[-0.06243624  0.04856083 -0.00166468  0.49905753]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 549 is [True, False, False, True, False, False]
Scene graph at timestep 549 is [True, False, False, True, False, False]
State prediction error at timestep 549 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 549 of -1
Current timestep = 550. State = [[-0.15927157 -0.24563704]]. Action = [[-0.1710551   0.04744089 -0.17295168 -0.68329555]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 550 is [True, False, False, True, False, False]
Current timestep = 551. State = [[-0.1691538  -0.25928536]]. Action = [[-0.03523606 -0.23833705  0.11279589  0.4244057 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 551 is [True, False, False, True, False, False]
Current timestep = 552. State = [[-0.18232323 -0.27460057]]. Action = [[-0.12910753 -0.00246087  0.16348714  0.7211821 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 552 is [True, False, False, True, False, False]
Scene graph at timestep 552 is [True, False, False, True, False, False]
State prediction error at timestep 552 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 552 of -1
Current timestep = 553. State = [[-0.20157146 -0.2745018 ]]. Action = [[-0.22605972  0.15312502 -0.23961778  0.40464485]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 553 is [True, False, False, True, False, False]
Current timestep = 554. State = [[-0.21713237 -0.26627687]]. Action = [[ 0.12882596 -0.21615925  0.01240471  0.25367248]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 554 is [True, False, False, True, False, False]
Current timestep = 555. State = [[-0.21480519 -0.25239754]]. Action = [[ 0.05386126  0.2398453  -0.08383298 -0.30338717]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 555 is [True, False, False, True, False, False]
Current timestep = 556. State = [[-0.22188252 -0.22458546]]. Action = [[-0.20220564  0.1515243   0.21158585 -0.88207865]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 556 is [True, False, False, True, False, False]
Scene graph at timestep 556 is [True, False, False, True, False, False]
State prediction error at timestep 556 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 556 of 0
Current timestep = 557. State = [[-0.24423514 -0.2087133 ]]. Action = [[ 0.04714537 -0.10426576 -0.07838604  0.52401304]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 557 is [True, False, False, True, False, False]
Current timestep = 558. State = [[-0.2450177  -0.21214423]]. Action = [[-0.0628283   0.0234924  -0.17654942  0.4923947 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 558 is [True, False, False, True, False, False]
Current timestep = 559. State = [[-0.2451902  -0.21393532]]. Action = [[ 0.13417476 -0.02465954  0.14979935 -0.89183265]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 559 is [True, False, False, True, False, False]
Current timestep = 560. State = [[-0.23406976 -0.21654907]]. Action = [[ 0.20303589 -0.06401563 -0.16554634  0.8125603 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 560 is [True, False, False, True, False, False]
Current timestep = 561. State = [[-0.22417288 -0.21434909]]. Action = [[-0.127452    0.13158321 -0.18065844  0.08234823]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 561 is [True, False, False, True, False, False]
Current timestep = 562. State = [[-0.21955635 -0.21063627]]. Action = [[ 0.17814991 -0.04736499 -0.03728959  0.1295147 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 562 is [True, False, False, True, False, False]
Current timestep = 563. State = [[-0.20880577 -0.20372751]]. Action = [[ 0.07874984  0.08919963  0.10355866 -0.04966873]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 563 is [True, False, False, True, False, False]
Current timestep = 564. State = [[-0.19383287 -0.19100769]]. Action = [[ 0.16847458  0.09526867 -0.17829834 -0.8728359 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 564 is [True, False, False, True, False, False]
Current timestep = 565. State = [[-0.17367812 -0.19729783]]. Action = [[ 0.1227603  -0.22870892 -0.08476952 -0.41805875]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 565 is [True, False, False, True, False, False]
Current timestep = 566. State = [[-0.14767763 -0.19507955]]. Action = [[ 0.24370599  0.22411239 -0.00740562 -0.13617504]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 566 is [True, False, False, True, False, False]
Scene graph at timestep 566 is [True, False, False, True, False, False]
State prediction error at timestep 566 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 566 of 1
Current timestep = 567. State = [[-0.11347017 -0.16876027]]. Action = [[0.21478277 0.24947172 0.04207823 0.9030249 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 567 is [True, False, False, True, False, False]
Current timestep = 568. State = [[-0.08619341 -0.1436843 ]]. Action = [[ 0.22323349  0.11674899 -0.2352509  -0.6390605 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 568 is [True, False, False, True, False, False]
Current timestep = 569. State = [[-0.0708992  -0.11956581]]. Action = [[-0.18407714  0.23878565  0.14444977  0.3014481 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 569 is [True, False, False, True, False, False]
Current timestep = 570. State = [[-0.08415686 -0.11060494]]. Action = [[-0.245877   -0.15435776 -0.01592296  0.9627987 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 570 is [True, False, False, False, True, False]
Current timestep = 571. State = [[-0.09497882 -0.12387584]]. Action = [[ 0.06014308 -0.11621229 -0.11057083 -0.7989275 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 571 is [True, False, False, False, True, False]
Current timestep = 572. State = [[-0.09042354 -0.12075521]]. Action = [[ 0.20022798  0.17583418 -0.22487058 -0.949109  ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 572 is [True, False, False, False, True, False]
Current timestep = 573. State = [[-0.09192053 -0.11736625]]. Action = [[-0.2485011  -0.08622095 -0.1733331  -0.5315935 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 573 is [True, False, False, False, True, False]
Current timestep = 574. State = [[-0.10069273 -0.11088326]]. Action = [[-0.09570655  0.16477048 -0.01212117 -0.82872164]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 574 is [True, False, False, False, True, False]
Current timestep = 575. State = [[-0.10660039 -0.08681915]]. Action = [[ 0.04573461  0.22729295  0.01050553 -0.125597  ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 575 is [True, False, False, False, True, False]
Current timestep = 576. State = [[-0.10879933 -0.0579465 ]]. Action = [[0.02925891 0.16268909 0.18556416 0.867574  ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 576 is [True, False, False, False, True, False]
Current timestep = 577. State = [[-0.1114241  -0.03412141]]. Action = [[-0.06815025  0.12854576 -0.10014686  0.31557608]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 577 is [True, False, False, False, True, False]
Current timestep = 578. State = [[-0.12385555 -0.02766007]]. Action = [[-0.22975388 -0.06816229 -0.20921378  0.6110929 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 578 is [True, False, False, False, True, False]
Current timestep = 579. State = [[-0.14254853 -0.01844022]]. Action = [[-0.09169567  0.17162949  0.11188152 -0.64831936]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 579 is [True, False, False, False, True, False]
Current timestep = 580. State = [[-0.14794534 -0.01658284]]. Action = [[ 0.16872022 -0.17467329  0.20035431  0.07695246]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 580 is [True, False, False, False, True, False]
Current timestep = 581. State = [[-0.14691967 -0.01197862]]. Action = [[-0.01900016  0.21573651 -0.14924332 -0.98140925]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 581 is [True, False, False, False, True, False]
Current timestep = 582. State = [[-0.15531838  0.00945129]]. Action = [[-0.21085909  0.18065184  0.01698479 -0.5994877 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 582 is [True, False, False, False, True, False]
Current timestep = 583. State = [[-0.16502494  0.03483108]]. Action = [[0.10317737 0.15832677 0.21308273 0.05980444]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 583 is [True, False, False, False, True, False]
Scene graph at timestep 583 is [True, False, False, False, True, False]
State prediction error at timestep 583 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 583 of 0
Current timestep = 584. State = [[-0.16509217  0.04910517]]. Action = [[0.07694197 0.0050841  0.10358357 0.7760626 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 584 is [True, False, False, False, True, False]
Current timestep = 585. State = [[-0.16820543  0.05960208]]. Action = [[-0.22988997  0.12624139 -0.19755535  0.82158923]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 585 is [True, False, False, False, True, False]
Scene graph at timestep 585 is [True, False, False, False, True, False]
State prediction error at timestep 585 is tensor(6.4797e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 585 of 0
Current timestep = 586. State = [[-0.17110854  0.05990731]]. Action = [[ 0.19161218 -0.21192257  0.1775656  -0.8647039 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 586 is [True, False, False, False, True, False]
Current timestep = 587. State = [[-0.15829504  0.03722752]]. Action = [[ 0.17770213 -0.16869566 -0.23290862 -0.33646905]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 587 is [True, False, False, False, True, False]
Current timestep = 588. State = [[-0.14333536  0.00900514]]. Action = [[ 0.01621169 -0.22738315  0.1256969   0.84783554]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 588 is [True, False, False, False, True, False]
Current timestep = 589. State = [[-0.1392007   0.00296635]]. Action = [[-0.02600068  0.23005342  0.02453351 -0.393713  ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 589 is [True, False, False, False, True, False]
Scene graph at timestep 589 is [True, False, False, False, True, False]
State prediction error at timestep 589 is tensor(1.0761e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 589 of 1
Current timestep = 590. State = [[-0.14498134  0.02627034]]. Action = [[-0.20472483  0.19162536 -0.22437894 -0.781105  ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 590 is [True, False, False, False, True, False]
Current timestep = 591. State = [[-0.15055476  0.04077194]]. Action = [[ 0.12314597 -0.01157005  0.14446011 -0.8253837 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 591 is [True, False, False, False, True, False]
Scene graph at timestep 591 is [True, False, False, False, True, False]
State prediction error at timestep 591 is tensor(2.8907e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 591 of -1
Current timestep = 592. State = [[-0.14392646  0.02944099]]. Action = [[ 0.13220036 -0.22646259  0.11910665 -0.88259304]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 592 is [True, False, False, False, True, False]
Scene graph at timestep 592 is [True, False, False, False, True, False]
State prediction error at timestep 592 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 592 of 1
Current timestep = 593. State = [[-0.13680968  0.01211142]]. Action = [[-0.04617128 -0.04692641 -0.15093175  0.04773581]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 593 is [True, False, False, False, True, False]
Current timestep = 594. State = [[-0.13175073  0.0179566 ]]. Action = [[ 0.15094182  0.18109363 -0.01627305  0.9152808 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 594 is [True, False, False, False, True, False]
Current timestep = 595. State = [[-0.12363634  0.01364601]]. Action = [[-0.07660463 -0.22690418 -0.1655294  -0.7907823 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 595 is [True, False, False, False, True, False]
Current timestep = 596. State = [[-0.12904821  0.00263682]]. Action = [[-0.16916032 -0.0082614  -0.19702649  0.83018017]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 596 is [True, False, False, False, True, False]
Current timestep = 597. State = [[-0.14493956 -0.01632751]]. Action = [[-0.16579415 -0.22624713 -0.2443835  -0.23131603]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 597 is [True, False, False, False, True, False]
Current timestep = 598. State = [[-0.16732    -0.01810611]]. Action = [[-0.20989527  0.2286976  -0.15598972 -0.345227  ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 598 is [True, False, False, False, True, False]
Scene graph at timestep 598 is [True, False, False, False, True, False]
State prediction error at timestep 598 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 598 of -1
Current timestep = 599. State = [[-0.1892129  -0.00279117]]. Action = [[ 0.03208554  0.03548425  0.04168671 -0.5839462 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 599 is [True, False, False, False, True, False]
Current timestep = 600. State = [[-0.18625067 -0.00750328]]. Action = [[ 0.11036611 -0.13304847 -0.1588925  -0.920316  ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 600 is [True, False, False, False, True, False]
Scene graph at timestep 600 is [True, False, False, False, True, False]
State prediction error at timestep 600 is tensor(3.7260e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 600 of 1
Current timestep = 601. State = [[-0.1878392  -0.00890279]]. Action = [[-0.21486185  0.08561581 -0.05823348 -0.7888006 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 601 is [True, False, False, False, True, False]
Current timestep = 602. State = [[-0.20580631 -0.02177835]]. Action = [[-0.24642967 -0.24382211 -0.0193508  -0.76782423]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 602 is [True, False, False, False, True, False]
Current timestep = 603. State = [[-0.22287792 -0.02597246]]. Action = [[ 0.15392596  0.20739809 -0.0722128   0.17418802]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 603 is [True, False, False, False, True, False]
Current timestep = 604. State = [[-0.2145374 -0.005337 ]]. Action = [[ 0.18980485  0.17836624  0.17854977 -0.3177731 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 604 is [True, False, False, False, True, False]
Current timestep = 605. State = [[-0.202239    0.00424737]]. Action = [[ 0.1121603  -0.07482244 -0.1015453   0.80729306]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 605 is [True, False, False, False, True, False]
Current timestep = 606. State = [[-0.19281133  0.01534265]]. Action = [[-1.7642975e-04  1.9540650e-01 -4.2249605e-02  4.4271147e-01]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 606 is [True, False, False, False, True, False]
Current timestep = 607. State = [[-0.1848362   0.01335621]]. Action = [[ 0.15409768 -0.23762722  0.17590892 -0.0614031 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 607 is [True, False, False, False, True, False]
Current timestep = 608. State = [[-0.1673323  -0.00803656]]. Action = [[ 0.1226683  -0.17665406  0.07589966 -0.31058103]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 608 is [True, False, False, False, True, False]
Scene graph at timestep 608 is [True, False, False, False, True, False]
State prediction error at timestep 608 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 608 of 1
Current timestep = 609. State = [[-0.15323667 -0.01374191]]. Action = [[-0.03595929  0.20707452  0.13196671  0.9475579 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 609 is [True, False, False, False, True, False]
Scene graph at timestep 609 is [True, False, False, False, True, False]
State prediction error at timestep 609 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 609 of 1
Current timestep = 610. State = [[-0.15683152  0.001508  ]]. Action = [[-0.0539363   0.04045218 -0.07852376 -0.91932094]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 610 is [True, False, False, False, True, False]
Current timestep = 611. State = [[-0.1607231   0.01337703]]. Action = [[-0.0653851   0.13629538  0.12321886 -0.62613755]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 611 is [True, False, False, False, True, False]
Current timestep = 612. State = [[-0.16136363  0.01826474]]. Action = [[ 0.1252752  -0.10925838  0.11184675  0.8932395 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 612 is [True, False, False, False, True, False]
Current timestep = 613. State = [[-0.15120666  0.02614379]]. Action = [[0.18814477 0.22003329 0.12858868 0.05968714]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 613 is [True, False, False, False, True, False]
Current timestep = 614. State = [[-0.12178291  0.03306755]]. Action = [[ 0.24830627 -0.12763396  0.06313461 -0.9258483 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 614 is [True, False, False, False, True, False]
Current timestep = 615. State = [[-0.101654    0.01647436]]. Action = [[-0.05105805 -0.22246166  0.18133056 -0.868366  ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 615 is [True, False, False, False, True, False]
Current timestep = 616. State = [[-0.09167854 -0.01210847]]. Action = [[ 0.20088744 -0.23121601  0.10190564  0.82812107]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 616 is [True, False, False, False, True, False]
Current timestep = 617. State = [[-0.06653409 -0.01892965]]. Action = [[0.1638962  0.22456396 0.15223736 0.40204048]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 617 is [True, False, False, False, True, False]
Current timestep = 618. State = [[-0.05043563 -0.01974817]]. Action = [[ 0.04032692 -0.21668606 -0.00089033  0.22113812]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 618 is [True, False, False, False, True, False]
Current timestep = 619. State = [[-0.04660644 -0.0389065 ]]. Action = [[-0.16141777 -0.09444275 -0.13577674  0.9397032 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 619 is [True, False, False, False, True, False]
Current timestep = 620. State = [[-0.05002117 -0.04507815]]. Action = [[-0.00925478  0.03411451 -0.2408209   0.37256145]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 620 is [False, True, False, False, True, False]
Current timestep = 621. State = [[-0.05206407 -0.05345012]]. Action = [[-0.02986185 -0.13586064 -0.14574207 -0.63651544]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 621 is [True, False, False, False, True, False]
Scene graph at timestep 621 is [True, False, False, False, True, False]
State prediction error at timestep 621 is tensor(1.0385e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 621 of 1
Current timestep = 622. State = [[-0.0579211  -0.07408563]]. Action = [[-0.02503987 -0.18896686  0.22018677  0.12914336]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 622 is [True, False, False, False, True, False]
Scene graph at timestep 622 is [True, False, False, False, True, False]
State prediction error at timestep 622 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of -1
Current timestep = 623. State = [[-0.06061538 -0.0906705 ]]. Action = [[-0.09369802  0.02564034  0.18741679  0.06249428]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 623 is [True, False, False, False, True, False]
Scene graph at timestep 623 is [True, False, False, False, True, False]
State prediction error at timestep 623 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 623 of 0
Current timestep = 624. State = [[-0.05871897 -0.10256218]]. Action = [[ 0.2409656  -0.22419916 -0.11775827  0.3099388 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 624 is [True, False, False, False, True, False]
Current timestep = 625. State = [[-0.04599358 -0.10502777]]. Action = [[0.11827874 0.23043907 0.02806553 0.79044604]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 625 is [True, False, False, False, True, False]
Current timestep = 626. State = [[-0.15634249  0.18060115]]. Action = [[ 0.07940903  0.10332361  0.13827974 -0.22016537]]. Reward = [100.]
Curr episode timestep = 97
Scene graph at timestep 626 is [False, True, False, False, True, False]
Scene graph at timestep 626 is [True, False, False, False, False, True]
State prediction error at timestep 626 is tensor(0.0484, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 626 of 0
Current timestep = 627. State = [[-0.14430691  0.20542036]]. Action = [[-0.21511036 -0.01077141  0.01281577  0.86447024]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 627 is [True, False, False, False, False, True]
Scene graph at timestep 627 is [True, False, False, False, False, True]
State prediction error at timestep 627 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 627 of -1
Current timestep = 628. State = [[-0.16152969  0.20038898]]. Action = [[-0.20474625 -0.14896235 -0.10748973  0.31971824]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 628 is [True, False, False, False, False, True]
Current timestep = 629. State = [[-0.1739147   0.20078763]]. Action = [[ 0.05729985  0.15463331 -0.01170446 -0.27056742]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 629 is [True, False, False, False, False, True]
Current timestep = 630. State = [[-0.1764008   0.19455342]]. Action = [[-0.07265323 -0.22850294 -0.20948648  0.97997475]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 630 is [True, False, False, False, False, True]
Current timestep = 631. State = [[-0.18093273  0.17773157]]. Action = [[-0.08037817 -0.08348542  0.15556735  0.8952663 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 631 is [True, False, False, False, False, True]
Current timestep = 632. State = [[-0.19414313  0.17247969]]. Action = [[-0.19173408  0.00662225 -0.11975223  0.4698031 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 632 is [True, False, False, False, False, True]
Current timestep = 633. State = [[-0.21149167  0.16595262]]. Action = [[-0.00442794 -0.05692182 -0.10618645  0.46626806]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 633 is [True, False, False, False, False, True]
Scene graph at timestep 633 is [True, False, False, False, False, True]
State prediction error at timestep 633 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 633 of -1
Current timestep = 634. State = [[-0.22351022  0.1645101 ]]. Action = [[-0.1347393   0.03192145  0.10307586 -0.1350745 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 634 is [True, False, False, False, False, True]
Scene graph at timestep 634 is [True, False, False, False, False, True]
State prediction error at timestep 634 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of -1
Current timestep = 635. State = [[-0.22891045  0.17563517]]. Action = [[ 0.2267319   0.21220028  0.10774568 -0.8346017 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 635 is [True, False, False, False, False, True]
Scene graph at timestep 635 is [True, False, False, False, False, True]
State prediction error at timestep 635 is tensor(9.6591e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 635 of 0
Current timestep = 636. State = [[-0.21679261  0.17552541]]. Action = [[ 0.12829578 -0.18675277 -0.23091511 -0.8473784 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 636 is [True, False, False, False, False, True]
Scene graph at timestep 636 is [True, False, False, False, False, True]
State prediction error at timestep 636 is tensor(1.3250e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 636 of 1
Current timestep = 637. State = [[-0.21032816  0.16897433]]. Action = [[-0.14176261  0.05285197 -0.0050692   0.84404016]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 637 is [True, False, False, False, False, True]
Scene graph at timestep 637 is [True, False, False, False, False, True]
State prediction error at timestep 637 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 637 of 0
Current timestep = 638. State = [[-0.20896424  0.1801467 ]]. Action = [[0.1716212  0.16309384 0.02843592 0.77428436]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 638 is [True, False, False, False, False, True]
Current timestep = 639. State = [[-0.1985613   0.20113762]]. Action = [[ 0.11867803  0.21730715  0.08755916 -0.9391265 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 639 is [True, False, False, False, False, True]
Current timestep = 640. State = [[-0.19252726  0.20800184]]. Action = [[-0.18611388 -0.21927373  0.18568176  0.5711299 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 640 is [True, False, False, False, False, True]
Current timestep = 641. State = [[-0.20407327  0.21073519]]. Action = [[-0.19337025  0.14766318 -0.18912207 -0.108307  ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 641 is [True, False, False, False, False, True]
Current timestep = 642. State = [[-0.21307583  0.23064756]]. Action = [[ 0.2289809   0.23566923  0.23787564 -0.9148718 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 642 is [True, False, False, False, False, True]
Current timestep = 643. State = [[-0.19924492  0.23413742]]. Action = [[ 0.19502085 -0.16239221 -0.109772    0.9096904 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 643 is [True, False, False, False, False, True]
Scene graph at timestep 643 is [True, False, False, False, False, True]
State prediction error at timestep 643 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 643 of 0
Current timestep = 644. State = [[-0.17756581  0.22513564]]. Action = [[ 0.14103478 -0.0259119  -0.09909625  0.9694487 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 644 is [True, False, False, False, False, True]
Current timestep = 645. State = [[-0.16876793  0.2176045 ]]. Action = [[-0.14112905 -0.13225324 -0.07582295  0.9223912 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 645 is [True, False, False, False, False, True]
Current timestep = 646. State = [[-0.1788954   0.21455413]]. Action = [[-0.23147725  0.03130108  0.18841693  0.9177021 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 646 is [True, False, False, False, False, True]
Current timestep = 647. State = [[-0.19473955  0.21910763]]. Action = [[-0.10219577  0.03275368 -0.1192093  -0.26682746]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 647 is [True, False, False, False, False, True]
Current timestep = 648. State = [[-0.21604924  0.23045084]]. Action = [[-0.24467064  0.11782417  0.18737179  0.18764293]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 648 is [True, False, False, False, False, True]
Current timestep = 649. State = [[-0.24402781  0.24973206]]. Action = [[-0.02605981  0.23578548  0.1805116  -0.9259146 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 649 is [True, False, False, False, False, True]
Current timestep = 650. State = [[-0.24684958  0.25252414]]. Action = [[ 0.10669434 -0.20255208  0.08223322 -0.21602362]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 650 is [True, False, False, False, False, True]
Scene graph at timestep 650 is [True, False, False, False, False, True]
State prediction error at timestep 650 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 650 of -1
Current timestep = 651. State = [[-0.23786923  0.245337  ]]. Action = [[ 0.12281793  0.11032677  0.23058838 -0.8672011 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 651 is [True, False, False, False, False, True]
Scene graph at timestep 651 is [True, False, False, False, False, True]
State prediction error at timestep 651 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 651 of 0
Current timestep = 652. State = [[-0.24250346  0.2553581 ]]. Action = [[-0.23252793  0.04408929  0.06116778  0.8814422 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 652 is [True, False, False, False, False, True]
Current timestep = 653. State = [[-0.24193843  0.24650812]]. Action = [[ 0.1971063  -0.24667488  0.07501689  0.98520947]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 653 is [True, False, False, False, False, True]
Current timestep = 654. State = [[-0.23507431  0.23849355]]. Action = [[ 0.05792493  0.1528722  -0.18105854  0.70222926]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 654 is [True, False, False, False, False, True]
Current timestep = 655. State = [[-0.22545369  0.25024152]]. Action = [[0.15965784 0.15978777 0.10890925 0.15797234]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 655 is [True, False, False, False, False, True]
Current timestep = 656. State = [[-0.21678732  0.25401083]]. Action = [[-0.184707   -0.19953725 -0.1547136   0.10405219]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 656 is [True, False, False, False, False, True]
Scene graph at timestep 656 is [True, False, False, False, False, True]
State prediction error at timestep 656 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 656 of 0
Current timestep = 657. State = [[-0.21348456  0.23576117]]. Action = [[ 0.06823832 -0.18264799 -0.15332069 -0.01139945]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 657 is [True, False, False, False, False, True]
Current timestep = 658. State = [[-0.20947503  0.22818603]]. Action = [[ 0.11484551  0.10526776  0.16784629 -0.40637594]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 658 is [True, False, False, False, False, True]
Scene graph at timestep 658 is [True, False, False, False, False, True]
State prediction error at timestep 658 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 658 of 1
Current timestep = 659. State = [[-0.21273707  0.22090298]]. Action = [[-0.21050172 -0.19981422 -0.0440215  -0.27538556]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 659 is [True, False, False, False, False, True]
Scene graph at timestep 659 is [True, False, False, False, False, True]
State prediction error at timestep 659 is tensor(2.6090e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 659 of 0
Current timestep = 660. State = [[-0.21408151  0.19372956]]. Action = [[ 0.14819044 -0.20688652  0.21720508  0.44207346]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 660 is [True, False, False, False, False, True]
Current timestep = 661. State = [[-0.2140273   0.18956894]]. Action = [[-0.17911847  0.1625019  -0.15487134  0.5346446 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 661 is [True, False, False, False, False, True]
Current timestep = 662. State = [[-0.21156275  0.18455029]]. Action = [[ 0.24060297 -0.18618883  0.04285225  0.6510309 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 662 is [True, False, False, False, False, True]
Current timestep = 663. State = [[-0.20390904  0.1761543 ]]. Action = [[ 0.00403425  0.09530845 -0.05932295  0.80026865]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 663 is [True, False, False, False, False, True]
Scene graph at timestep 663 is [True, False, False, False, False, True]
State prediction error at timestep 663 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 663 of 0
Current timestep = 664. State = [[-0.20202184  0.17453553]]. Action = [[ 0.02448505 -0.07025796  0.15950316 -0.88280714]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 664 is [True, False, False, False, False, True]
Current timestep = 665. State = [[-0.19006933  0.17273174]]. Action = [[ 0.23348004  0.03880128 -0.1663752   0.93572307]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 665 is [True, False, False, False, False, True]
Current timestep = 666. State = [[-0.17946342  0.16246915]]. Action = [[-0.16877545 -0.2167437  -0.11214626  0.19124258]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 666 is [True, False, False, False, False, True]
Current timestep = 667. State = [[-0.17784359  0.1545835 ]]. Action = [[0.15443063 0.09929854 0.07606539 0.33025074]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 667 is [True, False, False, False, False, True]
Scene graph at timestep 667 is [True, False, False, False, False, True]
State prediction error at timestep 667 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 667 of 1
Current timestep = 668. State = [[-0.16626187  0.14439638]]. Action = [[ 0.18332946 -0.20054634  0.08166501 -0.36669457]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 668 is [True, False, False, False, False, True]
Current timestep = 669. State = [[-0.13949627  0.12744284]]. Action = [[ 0.22154927 -0.01407109 -0.02496587 -0.93205184]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 669 is [True, False, False, False, False, True]
Current timestep = 670. State = [[-0.11498393  0.12722678]]. Action = [[0.13100529 0.04343304 0.01576409 0.11329496]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 670 is [True, False, False, False, False, True]
Current timestep = 671. State = [[-0.09764866  0.14321281]]. Action = [[ 0.06947029  0.23501167 -0.22966316  0.34348094]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 671 is [True, False, False, False, False, True]
Current timestep = 672. State = [[-0.07965118  0.14892218]]. Action = [[ 0.15528372 -0.17763919  0.16164303 -0.12226242]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 672 is [True, False, False, False, False, True]
Current timestep = 673. State = [[-0.06574704  0.13093549]]. Action = [[-0.09943089 -0.19183974  0.12923467 -0.9469875 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 673 is [True, False, False, False, False, True]
Current timestep = 674. State = [[-0.06050064  0.11163112]]. Action = [[ 0.11970648 -0.11278051  0.16141346 -0.62036794]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 674 is [True, False, False, False, False, True]
Current timestep = 675. State = [[-0.05251995  0.10940722]]. Action = [[ 0.10709944  0.18701214 -0.06023508  0.39110625]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 675 is [True, False, False, False, True, False]
Current timestep = 676. State = [[-0.03655674  0.10887422]]. Action = [[ 0.16473106 -0.16375256 -0.2275646   0.44443488]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 676 is [True, False, False, False, True, False]
Current timestep = 677. State = [[-0.26634616 -0.12423526]]. Action = [[ 0.23572108 -0.01624642 -0.14503211 -0.25834918]]. Reward = [100.]
Curr episode timestep = 50
Scene graph at timestep 677 is [False, True, False, False, True, False]
Current timestep = 678. State = [[-0.2588542  -0.14444225]]. Action = [[ 0.17589676 -0.10079503 -0.17783403  0.19874394]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 678 is [True, False, False, False, True, False]
Scene graph at timestep 678 is [True, False, False, True, False, False]
State prediction error at timestep 678 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 678 of 0
Current timestep = 679. State = [[-0.24622166 -0.15175496]]. Action = [[ 0.02525628  0.00190651 -0.1129908  -0.93382794]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 679 is [True, False, False, True, False, False]
Scene graph at timestep 679 is [True, False, False, True, False, False]
State prediction error at timestep 679 is tensor(3.2932e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 679 of 0
Current timestep = 680. State = [[-0.24061653 -0.14132231]]. Action = [[ 0.03151932  0.22285187 -0.22709021  0.0804193 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 680 is [True, False, False, True, False, False]
Current timestep = 681. State = [[-0.23848167 -0.1409356 ]]. Action = [[-0.00735021 -0.21135874 -0.17878662 -0.93242794]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 681 is [True, False, False, True, False, False]
Scene graph at timestep 681 is [True, False, False, True, False, False]
State prediction error at timestep 681 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 681 of 0
Current timestep = 682. State = [[-0.23904939 -0.15759373]]. Action = [[-0.04278675 -0.10036495  0.12842983 -0.97889197]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 682 is [True, False, False, True, False, False]
Scene graph at timestep 682 is [True, False, False, True, False, False]
State prediction error at timestep 682 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 682 of -1
Current timestep = 683. State = [[-0.23762599 -0.17441933]]. Action = [[ 0.09338933 -0.15494575 -0.06729513  0.53188634]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 683 is [True, False, False, True, False, False]
Scene graph at timestep 683 is [True, False, False, True, False, False]
State prediction error at timestep 683 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 683 of -1
Current timestep = 684. State = [[-0.22344932 -0.17509621]]. Action = [[ 0.21691877  0.21783641 -0.10980821  0.4996339 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 684 is [True, False, False, True, False, False]
Scene graph at timestep 684 is [True, False, False, True, False, False]
State prediction error at timestep 684 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 684 of 1
Current timestep = 685. State = [[-0.20401554 -0.15635687]]. Action = [[-0.0252229   0.13016623 -0.23022577 -0.4875028 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 685 is [True, False, False, True, False, False]
Current timestep = 686. State = [[-0.2091727  -0.13746043]]. Action = [[-0.17124793  0.15602005 -0.17459266 -0.5664616 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 686 is [True, False, False, True, False, False]
Current timestep = 687. State = [[-0.21764567 -0.11413894]]. Action = [[-0.09964505  0.18963885 -0.22581014 -0.79649836]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 687 is [True, False, False, True, False, False]
Current timestep = 688. State = [[-0.2261753 -0.0842509]]. Action = [[-0.00447541  0.23724076  0.19713417  0.08760107]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 688 is [True, False, False, False, True, False]
Scene graph at timestep 688 is [True, False, False, False, True, False]
State prediction error at timestep 688 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 688 of 0
Current timestep = 689. State = [[-0.2360193  -0.06416196]]. Action = [[-0.12092525 -0.06149539  0.19565725 -0.9681763 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 689 is [True, False, False, False, True, False]
Current timestep = 690. State = [[-0.23987126 -0.08026857]]. Action = [[ 0.1135444  -0.22467043  0.03113759  0.48284364]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 690 is [True, False, False, False, True, False]
Current timestep = 691. State = [[-0.23437576 -0.08737627]]. Action = [[ 0.12966451  0.10268393 -0.09001675  0.16894424]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 691 is [True, False, False, False, True, False]
Current timestep = 692. State = [[-0.22542183 -0.08331709]]. Action = [[ 0.08958665  0.04835695  0.07975382 -0.7122097 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 692 is [True, False, False, False, True, False]
Current timestep = 693. State = [[-0.22278151 -0.09052207]]. Action = [[-0.20687032 -0.2058171   0.18289363 -0.87241554]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 693 is [True, False, False, False, True, False]
Current timestep = 694. State = [[-0.23846716 -0.09965407]]. Action = [[-0.16397098  0.05080527 -0.17777552  0.4418571 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 694 is [True, False, False, False, True, False]
Current timestep = 695. State = [[-0.24299005 -0.08829067]]. Action = [[ 0.20337895  0.21047503 -0.22544983  0.17786252]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 695 is [True, False, False, False, True, False]
Current timestep = 696. State = [[-0.24119022 -0.07497962]]. Action = [[-0.23014814  0.21197969 -0.0050531  -0.43498313]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 696 is [True, False, False, False, True, False]
Current timestep = 697. State = [[-0.2324796  -0.06118606]]. Action = [[0.14588779 0.17214185 0.1497007  0.27794302]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 697 is [True, False, False, False, True, False]
Current timestep = 698. State = [[-0.22899975 -0.0628088 ]]. Action = [[-0.2243357  -0.23662934  0.20962939 -0.10101217]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 698 is [True, False, False, False, True, False]
Current timestep = 699. State = [[-0.22950889 -0.07081839]]. Action = [[ 0.20790568  0.0678066  -0.16658635  0.44283223]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 699 is [True, False, False, False, True, False]
Current timestep = 700. State = [[-0.21649587 -0.07240401]]. Action = [[ 0.0966019  -0.06233084  0.06429857  0.08395135]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 700 is [True, False, False, False, True, False]
Current timestep = 701. State = [[-0.20783639 -0.08743997]]. Action = [[ 0.05243382 -0.23874177 -0.18600902  0.2920072 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 701 is [True, False, False, False, True, False]
Current timestep = 702. State = [[-0.18840337 -0.09026288]]. Action = [[ 0.2412343   0.23622498 -0.06139363 -0.25295937]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 702 is [True, False, False, False, True, False]
Current timestep = 703. State = [[-0.15822522 -0.08001333]]. Action = [[ 0.23248443  0.0226818   0.14321873 -0.8162239 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 703 is [True, False, False, False, True, False]
Scene graph at timestep 703 is [True, False, False, False, True, False]
State prediction error at timestep 703 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 703 of 1
Current timestep = 704. State = [[-0.13761535 -0.06363709]]. Action = [[-0.22502568  0.2042836   0.1455633  -0.20937377]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 704 is [True, False, False, False, True, False]
Scene graph at timestep 704 is [True, False, False, False, True, False]
State prediction error at timestep 704 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 704 of 0
Current timestep = 705. State = [[-0.14544243 -0.0439315 ]]. Action = [[ 0.03278202  0.03455684  0.22409111 -0.4816563 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 705 is [True, False, False, False, True, False]
Current timestep = 706. State = [[-0.14141981 -0.03375551]]. Action = [[0.16079938 0.12885126 0.06267697 0.5132115 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 706 is [True, False, False, False, True, False]
Current timestep = 707. State = [[-0.13381    -0.02276054]]. Action = [[ 0.05469507  0.0534108   0.14118809 -0.43778658]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 707 is [True, False, False, False, True, False]
Current timestep = 708. State = [[-0.11799622 -0.02197766]]. Action = [[ 0.20971811 -0.11103551 -0.09127429  0.92182803]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 708 is [True, False, False, False, True, False]
Current timestep = 709. State = [[-0.1082022 -0.0152502]]. Action = [[-0.20422259  0.17043713  0.03423283 -0.34630787]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 709 is [True, False, False, False, True, False]
Scene graph at timestep 709 is [True, False, False, False, True, False]
State prediction error at timestep 709 is tensor(2.1297e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 709 of 0
Current timestep = 710. State = [[-0.10632177 -0.0026336 ]]. Action = [[ 0.20216608  0.05111206 -0.12778883 -0.67443   ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 710 is [True, False, False, False, True, False]
Current timestep = 711. State = [[-0.09013021  0.01383763]]. Action = [[ 0.15900958  0.2192308  -0.19988039  0.97439027]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 711 is [True, False, False, False, True, False]
Current timestep = 712. State = [[-0.06641077  0.03709568]]. Action = [[ 0.23349345  0.10741124  0.0683195  -0.70629746]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 712 is [True, False, False, False, True, False]
Current timestep = 713. State = [[-0.03691581  0.05026999]]. Action = [[ 0.16062391  0.03914821 -0.14338952 -0.32654428]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 713 is [True, False, False, False, True, False]
Current timestep = 714. State = [[-0.26623446 -0.1730667 ]]. Action = [[ 0.10500959 -0.15663566 -0.20271364  0.3393185 ]]. Reward = [100.]
Curr episode timestep = 36
Scene graph at timestep 714 is [False, True, False, False, True, False]
Current timestep = 715. State = [[-0.25863448 -0.20353945]]. Action = [[ 0.15706259 -0.19611967 -0.1494245   0.00823069]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 715 is [True, False, False, True, False, False]
Scene graph at timestep 715 is [True, False, False, True, False, False]
State prediction error at timestep 715 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 715 of -1
Current timestep = 716. State = [[-0.23568434 -0.21782823]]. Action = [[ 2.2380987e-01  5.4955095e-02 -6.1079860e-05 -1.5993619e-01]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 716 is [True, False, False, True, False, False]
Scene graph at timestep 716 is [True, False, False, True, False, False]
State prediction error at timestep 716 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 716 of 1
Current timestep = 717. State = [[-0.21493669 -0.21275102]]. Action = [[-0.039516    0.09459621 -0.01902412 -0.13288462]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 717 is [True, False, False, True, False, False]
Current timestep = 718. State = [[-0.20878272 -0.21896319]]. Action = [[ 0.16327906 -0.22197863 -0.05696563 -0.38650346]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 718 is [True, False, False, True, False, False]
Current timestep = 719. State = [[-0.20176296 -0.24000208]]. Action = [[-0.03738968 -0.18952504 -0.20266289 -0.57545644]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 719 is [True, False, False, True, False, False]
Current timestep = 720. State = [[-0.20123473 -0.2695627 ]]. Action = [[ 0.01262602 -0.23122863 -0.05612147 -0.9124003 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 720 is [True, False, False, True, False, False]
Scene graph at timestep 720 is [True, False, False, True, False, False]
State prediction error at timestep 720 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 720 of -1
Current timestep = 721. State = [[-0.19189051 -0.2804177 ]]. Action = [[ 0.1965918   0.19603151 -0.15524262  0.6838832 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 721 is [True, False, False, True, False, False]
Current timestep = 722. State = [[-0.17800231 -0.2802608 ]]. Action = [[-0.00349005 -0.15365481 -0.07260105 -0.21731806]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 722 is [True, False, False, True, False, False]
Current timestep = 723. State = [[-0.16533893 -0.2778049 ]]. Action = [[ 0.17292488  0.14563203  0.03379014 -0.17095059]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 723 is [True, False, False, True, False, False]
Current timestep = 724. State = [[-0.15861098 -0.28036335]]. Action = [[-0.14616425 -0.12036988 -0.12434378  0.5110142 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 724 is [True, False, False, True, False, False]
Current timestep = 725. State = [[-0.16599628 -0.29489747]]. Action = [[-0.04214858 -0.16140768  0.04404524 -0.91128415]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 725 is [True, False, False, True, False, False]
Current timestep = 726. State = [[-0.16118571 -0.30473286]]. Action = [[ 0.20666093  0.00706172 -0.22693563  0.6915233 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 726 is [True, False, False, True, False, False]
Current timestep = 727. State = [[-0.13851976 -0.3093309 ]]. Action = [[ 0.20037118 -0.0134683  -0.13681543 -0.6738269 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 727 is [True, False, False, True, False, False]
Current timestep = 728. State = [[-0.11739288 -0.3051231 ]]. Action = [[0.03918883 0.12307569 0.22775406 0.7363602 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 728 is [True, False, False, True, False, False]
Scene graph at timestep 728 is [True, False, False, True, False, False]
State prediction error at timestep 728 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of 0
Current timestep = 729. State = [[-0.11187904 -0.2841348 ]]. Action = [[-0.08560565  0.23931316  0.09028846  0.8678415 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 729 is [True, False, False, True, False, False]
Current timestep = 730. State = [[-0.10944941 -0.27221778]]. Action = [[ 0.04330093 -0.06055602  0.12626022  0.7127451 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 730 is [True, False, False, True, False, False]
Current timestep = 731. State = [[-0.10620949 -0.25944394]]. Action = [[ 0.04440349  0.21491736  0.13528037 -0.48828983]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 731 is [True, False, False, True, False, False]
Current timestep = 732. State = [[-0.09398246 -0.23041844]]. Action = [[ 0.22004086  0.20526463 -0.06324558 -0.9053584 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 732 is [True, False, False, True, False, False]
Current timestep = 733. State = [[-0.08325604 -0.22814366]]. Action = [[-0.03504741 -0.24260698 -0.2265743   0.8304037 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 733 is [True, False, False, True, False, False]
Scene graph at timestep 733 is [True, False, False, True, False, False]
State prediction error at timestep 733 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 733 of 1
Current timestep = 734. State = [[-0.08099893 -0.23002416]]. Action = [[ 0.05432084  0.14248851 -0.17799841 -0.94565946]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 734 is [True, False, False, True, False, False]
Current timestep = 735. State = [[-0.06897878 -0.2249259 ]]. Action = [[ 0.1955691  -0.02148801 -0.21338952 -0.19437349]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 735 is [True, False, False, True, False, False]
Current timestep = 736. State = [[-0.04476506 -0.23262464]]. Action = [[ 0.15092623 -0.13672683 -0.1362362  -0.7170306 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 736 is [True, False, False, True, False, False]
Current timestep = 737. State = [[-0.01783703 -0.2419074 ]]. Action = [[ 0.22666436 -0.06193    -0.19037525  0.8423326 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 737 is [False, True, False, True, False, False]
Scene graph at timestep 737 is [False, True, False, True, False, False]
State prediction error at timestep 737 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 737 of 0
Current timestep = 738. State = [[ 0.01159858 -0.24596676]]. Action = [[-0.05484344  0.11370122 -0.10760306 -0.08781195]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 738 is [False, True, False, True, False, False]
Current timestep = 739. State = [[ 0.01734727 -0.22831008]]. Action = [[ 0.15765193  0.22033188  0.13746607 -0.906787  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 739 is [False, True, False, True, False, False]
Current timestep = 740. State = [[ 0.02535907 -0.19648753]]. Action = [[ 0.02483267  0.24349883  0.16393167 -0.9275103 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 740 is [False, True, False, True, False, False]
Current timestep = 741. State = [[ 0.03367973 -0.16420303]]. Action = [[ 0.10715422  0.15983433 -0.0219364   0.36415887]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 741 is [False, True, False, True, False, False]
Current timestep = 742. State = [[ 0.04079058 -0.16010308]]. Action = [[-0.09017542 -0.15691875 -0.00358889  0.10236478]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 742 is [False, True, False, True, False, False]
Current timestep = 743. State = [[ 0.03942605 -0.15209669]]. Action = [[-0.00906801  0.24769074  0.17643893  0.12566566]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 743 is [False, True, False, True, False, False]
Current timestep = 744. State = [[ 0.0379724 -0.1501358]]. Action = [[-0.01659913 -0.21010141  0.08938497  0.9859879 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 744 is [False, True, False, True, False, False]
Current timestep = 745. State = [[ 0.040391   -0.14584978]]. Action = [[ 0.14670223  0.20917267  0.09205937 -0.7420016 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 745 is [False, True, False, True, False, False]
Current timestep = 746. State = [[ 0.05062735 -0.13671799]]. Action = [[ 0.19705537  0.10912406  0.19376811 -0.8972813 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 746 is [False, True, False, True, False, False]
Current timestep = 747. State = [[ 0.05276453 -0.13532208]]. Action = [[ 0.23184958 -0.05636907  0.09711856 -0.26537693]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 747 is [False, False, True, True, False, False]
Current timestep = 748. State = [[ 0.05166291 -0.14052427]]. Action = [[-0.10824856 -0.09304649  0.02827042 -0.1389302 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 748 is [False, False, True, True, False, False]
Current timestep = 749. State = [[ 0.04520342 -0.13459694]]. Action = [[-0.24385776  0.20997477 -0.13157049  0.01088929]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 749 is [False, False, True, True, False, False]
Current timestep = 750. State = [[ 0.03489759 -0.12136821]]. Action = [[ 0.22048318  0.23084718  0.00587916 -0.970466  ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 750 is [False, True, False, True, False, False]
Current timestep = 751. State = [[ 0.03409362 -0.1308658 ]]. Action = [[ 0.17736542 -0.21966569 -0.15427445  0.4478575 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 751 is [False, True, False, False, True, False]
Current timestep = 752. State = [[ 0.03278152 -0.13668996]]. Action = [[-0.20099367  0.11237323 -0.05081257 -0.85087264]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 752 is [False, True, False, True, False, False]
Current timestep = 753. State = [[ 0.03178442 -0.14107364]]. Action = [[ 0.18321273 -0.12340683 -0.10554318 -0.59534395]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 753 is [False, True, False, True, False, False]
Current timestep = 754. State = [[ 0.0332263  -0.14455186]]. Action = [[ 0.20404369  0.22868478  0.08686897 -0.332116  ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 754 is [False, True, False, True, False, False]
Current timestep = 755. State = [[ 0.03205451 -0.13608123]]. Action = [[-0.16669187  0.18622485 -0.09426294 -0.76868814]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 755 is [False, True, False, True, False, False]
Current timestep = 756. State = [[ 0.02773961 -0.11383939]]. Action = [[-0.04802009  0.20750186 -0.01415122  0.15814435]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 756 is [False, True, False, True, False, False]
Current timestep = 757. State = [[-0.2094636  -0.19562091]]. Action = [[ 0.10066438  0.16828716 -0.057868    0.43880904]]. Reward = [100.]
Curr episode timestep = 42
Scene graph at timestep 757 is [False, True, False, False, True, False]
Current timestep = 758. State = [[-0.19612423 -0.21043961]]. Action = [[ 0.14130026  0.12315416 -0.12255335  0.4912969 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 758 is [True, False, False, True, False, False]
Scene graph at timestep 758 is [True, False, False, True, False, False]
State prediction error at timestep 758 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 758 of 1
Current timestep = 759. State = [[-0.17874494 -0.19334878]]. Action = [[ 0.06727317  0.18465117 -0.19881766  0.4405278 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 759 is [True, False, False, True, False, False]
Current timestep = 760. State = [[-0.1690611  -0.17643683]]. Action = [[ 0.11875635  0.09280211 -0.2228246  -0.52254266]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 760 is [True, False, False, True, False, False]
Current timestep = 761. State = [[-0.15529753 -0.1541175 ]]. Action = [[0.12107578 0.21727988 0.12100133 0.7324724 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 761 is [True, False, False, True, False, False]
Current timestep = 762. State = [[-0.1321132 -0.1359261]]. Action = [[ 0.2262882   0.04499325  0.0674057  -0.97414666]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 762 is [True, False, False, True, False, False]
Current timestep = 763. State = [[-0.10272291 -0.13598122]]. Action = [[ 0.14474553 -0.11335447  0.19570735 -0.961432  ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 763 is [True, False, False, True, False, False]
Scene graph at timestep 763 is [True, False, False, True, False, False]
State prediction error at timestep 763 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 763 of 1
Current timestep = 764. State = [[-0.07253683 -0.15452518]]. Action = [[ 0.24759346 -0.22135364  0.23784149 -0.82479256]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 764 is [True, False, False, True, False, False]
Current timestep = 765. State = [[-0.04197695 -0.15450852]]. Action = [[ 0.20996186  0.24675381 -0.20170717 -0.45131707]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 765 is [True, False, False, True, False, False]
Current timestep = 766. State = [[-0.01700016 -0.13617934]]. Action = [[ 0.09007406  0.11404347 -0.07970864 -0.19981688]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 766 is [False, True, False, True, False, False]
Current timestep = 767. State = [[-0.00654563 -0.1401695 ]]. Action = [[-0.04020515 -0.22084285 -0.18489523  0.22755921]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 767 is [False, True, False, True, False, False]
Current timestep = 768. State = [[-0.00521275 -0.1559776 ]]. Action = [[ 0.09990829 -0.11089268 -0.05796985 -0.8448174 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 768 is [False, True, False, True, False, False]
Scene graph at timestep 768 is [False, True, False, True, False, False]
State prediction error at timestep 768 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 768 of 0
Current timestep = 769. State = [[ 0.00437762 -0.16271107]]. Action = [[-0.19544263  0.12637144 -0.02406707  0.8111346 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 769 is [False, True, False, True, False, False]
Current timestep = 770. State = [[ 0.00331547 -0.14449006]]. Action = [[ 0.18328834  0.21879345 -0.10599864  0.5885844 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 770 is [False, True, False, True, False, False]
Current timestep = 771. State = [[ 0.01354966 -0.12420399]]. Action = [[ 0.23587531  0.03345412  0.21927565 -0.45443815]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 771 is [False, True, False, True, False, False]
Scene graph at timestep 771 is [False, True, False, False, True, False]
State prediction error at timestep 771 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 771 of 0
Current timestep = 772. State = [[ 0.04257436 -0.11921705]]. Action = [[ 0.01219621 -0.02112792  0.02207768 -0.3056218 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 772 is [False, True, False, False, True, False]
Scene graph at timestep 772 is [False, True, False, False, True, False]
State prediction error at timestep 772 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 772 of 0
Current timestep = 773. State = [[ 0.0452932  -0.10753222]]. Action = [[ 0.08308604  0.22376949 -0.22582628 -0.79383063]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 773 is [False, True, False, False, True, False]
Current timestep = 774. State = [[ 0.04752475 -0.07700765]]. Action = [[-0.21759777  0.24786088 -0.00938021 -0.72077686]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 774 is [False, True, False, False, True, False]
Current timestep = 775. State = [[ 0.04002151 -0.04021801]]. Action = [[-0.08134979  0.24436933 -0.16339758 -0.37481868]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 775 is [False, True, False, False, True, False]
Scene graph at timestep 775 is [False, True, False, False, True, False]
State prediction error at timestep 775 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 775 of 1
Current timestep = 776. State = [[-0.25967157  0.00621452]]. Action = [[-0.08243933 -0.13760868  0.11109483 -0.30924797]]. Reward = [100.]
Curr episode timestep = 18
Scene graph at timestep 776 is [False, True, False, False, True, False]
Current timestep = 777. State = [[-0.26092315  0.01150703]]. Action = [[-0.04281834  0.05392301  0.05691269 -0.02165848]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 777 is [True, False, False, False, True, False]
Current timestep = 778. State = [[-0.25405     0.01105243]]. Action = [[ 0.18718448 -0.10797596  0.22463071 -0.03329456]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 778 is [True, False, False, False, True, False]
Current timestep = 779. State = [[-0.23726958  0.01312503]]. Action = [[ 0.17929429  0.13162792 -0.09189484  0.9129064 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 779 is [True, False, False, False, True, False]
Scene graph at timestep 779 is [True, False, False, False, True, False]
State prediction error at timestep 779 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 779 of 1
Current timestep = 780. State = [[-0.21079712  0.01121011]]. Action = [[ 0.14627752 -0.15380926 -0.08079155 -0.4700446 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 780 is [True, False, False, False, True, False]
Current timestep = 781. State = [[-0.20519476  0.00274307]]. Action = [[-0.11852527 -0.03268692 -0.15262161 -0.6517782 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 781 is [True, False, False, False, True, False]
Current timestep = 782. State = [[-0.2133373  -0.00423096]]. Action = [[-0.18228213 -0.03470698  0.21713692 -0.01860034]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 782 is [True, False, False, False, True, False]
Current timestep = 783. State = [[-0.21860857 -0.02211083]]. Action = [[ 0.09965304 -0.23356493 -0.24335076  0.95076656]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 783 is [True, False, False, False, True, False]
Current timestep = 784. State = [[-0.2227813  -0.05316969]]. Action = [[-0.12276128 -0.2477535  -0.1006411   0.12932599]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 784 is [True, False, False, False, True, False]
Current timestep = 785. State = [[-0.23467694 -0.08887353]]. Action = [[-0.15462507 -0.2307724  -0.14194453 -0.6163691 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 785 is [True, False, False, False, True, False]
Current timestep = 786. State = [[-0.24309854 -0.11462986]]. Action = [[ 0.10154432 -0.09638549  0.05406588 -0.23523867]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 786 is [True, False, False, False, True, False]
Current timestep = 787. State = [[-0.23838882 -0.12041611]]. Action = [[0.08529291 0.08330992 0.06594008 0.81595516]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 787 is [True, False, False, False, True, False]
Scene graph at timestep 787 is [True, False, False, False, True, False]
State prediction error at timestep 787 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 787 of -1
Current timestep = 788. State = [[-0.23468034 -0.11573169]]. Action = [[ 0.07532448  0.06070504 -0.14797774  0.6467986 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 788 is [True, False, False, False, True, False]
Current timestep = 789. State = [[-0.2332797  -0.11428997]]. Action = [[-0.11882259 -0.0139838   0.140814    0.92247605]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 789 is [True, False, False, False, True, False]
Current timestep = 790. State = [[-0.23313773 -0.11243413]]. Action = [[0.09149045 0.04303214 0.24354005 0.88378453]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 790 is [True, False, False, False, True, False]
Current timestep = 791. State = [[-0.23932578 -0.09576556]]. Action = [[-0.2224692   0.23412108  0.18512496  0.80011797]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 791 is [True, False, False, False, True, False]
Current timestep = 792. State = [[-0.24533121 -0.07425606]]. Action = [[0.03922665 0.05731428 0.17513835 0.09431458]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 792 is [True, False, False, False, True, False]
Current timestep = 793. State = [[-0.24643064 -0.05832337]]. Action = [[ 0.00264129  0.12640125 -0.08903134  0.54962087]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 793 is [True, False, False, False, True, False]
Current timestep = 794. State = [[-0.24572618 -0.03400691]]. Action = [[ 0.08576271  0.23212272 -0.1708706  -0.7042503 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 794 is [True, False, False, False, True, False]
Current timestep = 795. State = [[-0.23356406 -0.02714164]]. Action = [[ 0.19872895 -0.20279205 -0.19594172 -0.95265687]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 795 is [True, False, False, False, True, False]
Current timestep = 796. State = [[-0.21241887 -0.02520964]]. Action = [[ 0.13712418  0.16738242 -0.16966258 -0.64318925]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 796 is [True, False, False, False, True, False]
Current timestep = 797. State = [[-0.20254114 -0.02293525]]. Action = [[-0.06708938 -0.07929157 -0.10257739 -0.66062313]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 797 is [True, False, False, False, True, False]
Current timestep = 798. State = [[-0.20325491 -0.03389907]]. Action = [[-0.07019123 -0.14387843  0.22390532  0.47190642]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 798 is [True, False, False, False, True, False]
Current timestep = 799. State = [[-0.19806425 -0.05504198]]. Action = [[ 0.21885094 -0.22785541 -0.23543796 -0.91432405]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 799 is [True, False, False, False, True, False]
Current timestep = 800. State = [[-0.19082636 -0.06575978]]. Action = [[-0.08837458  0.13384998 -0.07475749  0.18985188]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 800 is [True, False, False, False, True, False]
Current timestep = 801. State = [[-0.18858506 -0.05601979]]. Action = [[ 0.11933538  0.12040997 -0.07008483  0.36213565]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 801 is [True, False, False, False, True, False]
Current timestep = 802. State = [[-0.18283689 -0.05146613]]. Action = [[ 0.05396268 -0.06305534 -0.22280315 -0.03929663]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 802 is [True, False, False, False, True, False]
Current timestep = 803. State = [[-0.18142647 -0.04823925]]. Action = [[-0.1902971   0.07344675 -0.12387079  0.13033724]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 803 is [True, False, False, False, True, False]
Current timestep = 804. State = [[-0.19073696 -0.03916933]]. Action = [[-0.11786549  0.07799163 -0.23070332  0.83044195]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 804 is [True, False, False, False, True, False]
Current timestep = 805. State = [[-0.20832765 -0.03780076]]. Action = [[-0.20867263 -0.07867467 -0.11004771  0.7775047 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 805 is [True, False, False, False, True, False]
Scene graph at timestep 805 is [True, False, False, False, True, False]
State prediction error at timestep 805 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 805 of 0
Current timestep = 806. State = [[-0.22788827 -0.05712783]]. Action = [[ 0.04179054 -0.24347532  0.13419783  0.01793742]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 806 is [True, False, False, False, True, False]
Current timestep = 807. State = [[-0.22076954 -0.06296469]]. Action = [[ 0.23017699  0.15865254 -0.22988714 -0.6064943 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 807 is [True, False, False, False, True, False]
Current timestep = 808. State = [[-0.20362799 -0.04944101]]. Action = [[ 0.13106251  0.12743145  0.06510735 -0.09719437]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 808 is [True, False, False, False, True, False]
Current timestep = 809. State = [[-0.19993278 -0.03719941]]. Action = [[-0.24516161  0.05461758 -0.15832946  0.03646159]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 809 is [True, False, False, False, True, False]
Current timestep = 810. State = [[-0.20114416 -0.03019031]]. Action = [[0.22566926 0.01292464 0.18517119 0.954968  ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 810 is [True, False, False, False, True, False]
Current timestep = 811. State = [[-0.19907776 -0.04162289]]. Action = [[-0.16790685 -0.23802306 -0.18398313 -0.58051264]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 811 is [True, False, False, False, True, False]
Current timestep = 812. State = [[-0.20596647 -0.05069002]]. Action = [[-0.07449989  0.07006553 -0.09521195  0.44232643]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 812 is [True, False, False, False, True, False]
Scene graph at timestep 812 is [True, False, False, False, True, False]
State prediction error at timestep 812 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 812 of 0
Current timestep = 813. State = [[-0.20873639 -0.045894  ]]. Action = [[ 0.12439057  0.08914489  0.06488252 -0.6628323 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 813 is [True, False, False, False, True, False]
Current timestep = 814. State = [[-0.21059667 -0.02716279]]. Action = [[-0.11815372  0.24575987 -0.24086796 -0.51906395]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 814 is [True, False, False, False, True, False]
Current timestep = 815. State = [[-0.20595895 -0.0199811 ]]. Action = [[ 0.2332499  -0.22080319 -0.02374291  0.16386223]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 815 is [True, False, False, False, True, False]
Scene graph at timestep 815 is [True, False, False, False, True, False]
State prediction error at timestep 815 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 815 of 0
Current timestep = 816. State = [[-0.18880935 -0.01737958]]. Action = [[ 0.15188473  0.24835658  0.12364858 -0.1980961 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 816 is [True, False, False, False, True, False]
Current timestep = 817. State = [[-0.17693295  0.00630492]]. Action = [[-0.03096323  0.16243106 -0.09437363 -0.97318465]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 817 is [True, False, False, False, True, False]
Scene graph at timestep 817 is [True, False, False, False, True, False]
State prediction error at timestep 817 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 817 of 1
Current timestep = 818. State = [[-0.16883509  0.03701037]]. Action = [[ 0.1524384   0.2290746  -0.1271876  -0.40262437]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 818 is [True, False, False, False, True, False]
Current timestep = 819. State = [[-0.15826039  0.03892718]]. Action = [[-0.06428927 -0.24813649  0.02431247  0.59310937]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 819 is [True, False, False, False, True, False]
Current timestep = 820. State = [[-0.15041585  0.03024192]]. Action = [[0.19522429 0.01479155 0.07814425 0.86844444]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 820 is [True, False, False, False, True, False]
Current timestep = 821. State = [[-0.14520289  0.04061797]]. Action = [[-0.12302408  0.21537781 -0.1961399  -0.8704669 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 821 is [True, False, False, False, True, False]
Current timestep = 822. State = [[-0.14890957  0.04341215]]. Action = [[-0.07270148 -0.17284243  0.00752306 -0.8851374 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 822 is [True, False, False, False, True, False]
Current timestep = 823. State = [[-0.15463606  0.03140477]]. Action = [[-0.13255204 -0.08207318 -0.14389975 -0.2880553 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 823 is [True, False, False, False, True, False]
Current timestep = 824. State = [[-0.16701648  0.01403199]]. Action = [[-0.14140496 -0.15457387  0.22575423 -0.06064671]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 824 is [True, False, False, False, True, False]
Current timestep = 825. State = [[-0.18463473  0.01510162]]. Action = [[-0.16109024  0.23531973  0.07243484 -0.11722499]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 825 is [True, False, False, False, True, False]
Current timestep = 826. State = [[-0.19262597  0.02934949]]. Action = [[ 0.22643301 -0.02163009 -0.18502223  0.6213561 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 826 is [True, False, False, False, True, False]
Scene graph at timestep 826 is [True, False, False, False, True, False]
State prediction error at timestep 826 is tensor(8.7015e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 826 of 0
Current timestep = 827. State = [[-0.17839347  0.01742313]]. Action = [[ 0.22396344 -0.22475271 -0.16336     0.11917377]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 827 is [True, False, False, False, True, False]
Current timestep = 828. State = [[-0.15820156 -0.00572043]]. Action = [[ 0.11653233 -0.12496763 -0.02024958  0.4122492 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 828 is [True, False, False, False, True, False]
Current timestep = 829. State = [[-0.13997655 -0.01034896]]. Action = [[ 0.14325935  0.11936119 -0.1344549  -0.9956836 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 829 is [True, False, False, False, True, False]
Current timestep = 830. State = [[-0.12626335 -0.00461259]]. Action = [[-0.02237098  0.01173431  0.20871484 -0.12318498]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 830 is [True, False, False, False, True, False]
Current timestep = 831. State = [[-0.12467203 -0.00300252]]. Action = [[0.02304631 0.02921093 0.13212642 0.151407  ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 831 is [True, False, False, False, True, False]
Current timestep = 832. State = [[-0.12908107 -0.01354126]]. Action = [[-0.19833171 -0.22487563  0.22239548 -0.6549599 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 832 is [True, False, False, False, True, False]
Current timestep = 833. State = [[-0.13003139 -0.03166861]]. Action = [[ 0.10314944 -0.10885423 -0.08748087  0.88721347]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 833 is [True, False, False, False, True, False]
Current timestep = 834. State = [[-0.1348491  -0.04340028]]. Action = [[-0.21260144 -0.02549283 -0.13119435  0.41478062]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 834 is [True, False, False, False, True, False]
Current timestep = 835. State = [[-0.14235921 -0.05029324]]. Action = [[ 0.0233238  -0.01195538  0.20490062  0.0493418 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 835 is [True, False, False, False, True, False]
Current timestep = 836. State = [[-0.14187442 -0.05062995]]. Action = [[0.12882763 0.03399962 0.20945078 0.85033536]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 836 is [True, False, False, False, True, False]
Current timestep = 837. State = [[-0.1435539  -0.05109626]]. Action = [[-0.14959073 -0.04024038 -0.22820479 -0.9604484 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 837 is [True, False, False, False, True, False]
Scene graph at timestep 837 is [True, False, False, False, True, False]
State prediction error at timestep 837 is tensor(2.0674e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 837 of 1
Current timestep = 838. State = [[-0.14651641 -0.04503523]]. Action = [[ 0.12033918  0.14211991  0.24245602 -0.45913708]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 838 is [True, False, False, False, True, False]
Current timestep = 839. State = [[-0.13512124 -0.02562293]]. Action = [[ 0.23585027  0.20098233  0.20723969 -0.694891  ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 839 is [True, False, False, False, True, False]
Current timestep = 840. State = [[-1.0966317e-01  6.2472820e-05]]. Action = [[ 0.1756118   0.16577017  0.07245195 -0.16982108]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 840 is [True, False, False, False, True, False]
Current timestep = 841. State = [[-0.09924439  0.02464086]]. Action = [[-0.17619354  0.16953337  0.01365438 -0.48210943]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 841 is [True, False, False, False, True, False]
Current timestep = 842. State = [[-0.10805685  0.05101319]]. Action = [[-0.11805236  0.14560929 -0.04391804  0.06047022]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 842 is [True, False, False, False, True, False]
Current timestep = 843. State = [[-0.11366431  0.06649698]]. Action = [[0.02086192 0.03633872 0.12564403 0.45972776]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 843 is [True, False, False, False, True, False]
Current timestep = 844. State = [[-0.11055045  0.06136479]]. Action = [[ 0.15475357 -0.16827312 -0.22454453  0.15667772]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 844 is [True, False, False, False, True, False]
Current timestep = 845. State = [[-0.10698173  0.05235343]]. Action = [[-0.10227552 -0.00723134 -0.12415236 -0.6565313 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 845 is [True, False, False, False, True, False]
Current timestep = 846. State = [[-0.11110271  0.04365391]]. Action = [[-0.12330249 -0.10918552  0.04113334  0.7828903 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 846 is [True, False, False, False, True, False]
Current timestep = 847. State = [[-0.12119527  0.04870735]]. Action = [[-0.06244645  0.22468114  0.13261706  0.57475364]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 847 is [True, False, False, False, True, False]
Current timestep = 848. State = [[-0.12635124  0.04755628]]. Action = [[ 0.04429054 -0.2297382  -0.08269453 -0.55747414]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 848 is [True, False, False, False, True, False]
Current timestep = 849. State = [[-0.1235045   0.02748121]]. Action = [[ 0.12973768 -0.16594511 -0.02468844 -0.9145617 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 849 is [True, False, False, False, True, False]
Current timestep = 850. State = [[-0.12232222 -0.00103654]]. Action = [[-0.12846039 -0.21530758 -0.16772005 -0.5490233 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 850 is [True, False, False, False, True, False]
Current timestep = 851. State = [[-0.11860651 -0.00574128]]. Action = [[0.18564999 0.20681462 0.24151266 0.8454348 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 851 is [True, False, False, False, True, False]
Scene graph at timestep 851 is [True, False, False, False, True, False]
State prediction error at timestep 851 is tensor(1.4587e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 851 of 0
Current timestep = 852. State = [[-0.1196451   0.01226628]]. Action = [[-0.13761233  0.19131243  0.17555386 -0.6780159 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 852 is [True, False, False, False, True, False]
Scene graph at timestep 852 is [True, False, False, False, True, False]
State prediction error at timestep 852 is tensor(6.9932e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 852 of -1
Current timestep = 853. State = [[-0.12859595  0.02145252]]. Action = [[-0.13149348 -0.1600645  -0.17185988 -0.01260936]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 853 is [True, False, False, False, True, False]
Current timestep = 854. State = [[-0.12936714  0.01073229]]. Action = [[ 0.10137179 -0.05870584 -0.20056884  0.6126522 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 854 is [True, False, False, False, True, False]
Current timestep = 855. State = [[-0.12719585  0.01346181]]. Action = [[ 0.06676137  0.15084374 -0.12323166  0.8023814 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 855 is [True, False, False, False, True, False]
Current timestep = 856. State = [[-0.12620477  0.01647975]]. Action = [[-0.0116545  -0.0318453  -0.13800429 -0.89910483]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 856 is [True, False, False, False, True, False]
Current timestep = 857. State = [[-0.11931902  0.02067955]]. Action = [[ 0.17057812  0.07471618 -0.22618602 -0.4002179 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 857 is [True, False, False, False, True, False]
Current timestep = 858. State = [[-0.10973489  0.01839698]]. Action = [[-0.05759111 -0.14058864 -0.23897779 -0.41630256]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 858 is [True, False, False, False, True, False]
Scene graph at timestep 858 is [True, False, False, False, True, False]
State prediction error at timestep 858 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 858 of 0
Current timestep = 859. State = [[-0.10341188  0.00351329]]. Action = [[ 0.15647396 -0.14327593  0.02563864  0.8002274 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 859 is [True, False, False, False, True, False]
Current timestep = 860. State = [[-0.09020891 -0.02080099]]. Action = [[ 0.19468194 -0.22993194 -0.05828962 -0.23644364]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 860 is [True, False, False, False, True, False]
Current timestep = 861. State = [[-0.06457151 -0.03517976]]. Action = [[ 0.12312129  0.07047278 -0.07119864  0.03945374]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 861 is [True, False, False, False, True, False]
Current timestep = 862. State = [[-0.05662167 -0.02078464]]. Action = [[-0.18767118  0.23226482 -0.14340462 -0.7199938 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 862 is [True, False, False, False, True, False]
Current timestep = 863. State = [[-0.0546431  -0.01992745]]. Action = [[ 0.19763654 -0.2417036  -0.18062216 -0.1842593 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 863 is [True, False, False, False, True, False]
Current timestep = 864. State = [[-0.04919066 -0.0277662 ]]. Action = [[-0.04133889  0.08702567  0.22520727 -0.27918988]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 864 is [True, False, False, False, True, False]
Scene graph at timestep 864 is [False, True, False, False, True, False]
State prediction error at timestep 864 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 864 of 1
Current timestep = 865. State = [[-0.0495388  -0.02655031]]. Action = [[-0.09123856 -0.0126249   0.0281713  -0.05863327]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 865 is [False, True, False, False, True, False]
Current timestep = 866. State = [[-0.04996054 -0.02692738]]. Action = [[-0.06180154 -0.01679592  0.07380992  0.17076707]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 866 is [False, True, False, False, True, False]
Current timestep = 867. State = [[-0.05980574 -0.04042002]]. Action = [[-0.17119583 -0.19676277  0.01480061  0.5650945 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 867 is [False, True, False, False, True, False]
Current timestep = 868. State = [[-0.07133105 -0.0550398 ]]. Action = [[-0.00249317 -0.02073571 -0.2249502  -0.15344143]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 868 is [True, False, False, False, True, False]
Scene graph at timestep 868 is [True, False, False, False, True, False]
State prediction error at timestep 868 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 868 of -1
Current timestep = 869. State = [[-0.06763516 -0.04570577]]. Action = [[ 0.20781893  0.23178875 -0.1650524  -0.70737267]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 869 is [True, False, False, False, True, False]
Current timestep = 870. State = [[-0.05641764 -0.0221204 ]]. Action = [[0.22640797 0.17456934 0.21813875 0.98855495]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 870 is [True, False, False, False, True, False]
Current timestep = 871. State = [[-0.03701197  0.00409669]]. Action = [[-0.0127922   0.19869524 -0.096986    0.30611408]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 871 is [True, False, False, False, True, False]
Current timestep = 872. State = [[-0.18083192  0.06506773]]. Action = [[ 0.11114424  0.10441819 -0.19203074  0.6808009 ]]. Reward = [100.]
Curr episode timestep = 95
Scene graph at timestep 872 is [False, True, False, False, True, False]
Scene graph at timestep 872 is [True, False, False, False, True, False]
State prediction error at timestep 872 is tensor(0.0137, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 872 of 1
Current timestep = 873. State = [[-0.16450208  0.06440885]]. Action = [[ 0.02624008 -0.17992508  0.17906195 -0.8352957 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 873 is [True, False, False, False, True, False]
Scene graph at timestep 873 is [True, False, False, False, True, False]
State prediction error at timestep 873 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 873 of 1
Current timestep = 874. State = [[-0.1643907   0.06652984]]. Action = [[ 0.00730351  0.24470726 -0.18859877  0.6439533 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 874 is [True, False, False, False, True, False]
Current timestep = 875. State = [[-0.15785241  0.0718467 ]]. Action = [[ 0.15886015 -0.1415589   0.10319552 -0.88630015]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 875 is [True, False, False, False, True, False]
Current timestep = 876. State = [[-0.14468125  0.0580061 ]]. Action = [[-0.10745904 -0.1809453  -0.23383361 -0.8787288 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 876 is [True, False, False, False, True, False]
Scene graph at timestep 876 is [True, False, False, False, True, False]
State prediction error at timestep 876 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 876 of 0
Current timestep = 877. State = [[-0.13740285  0.03217372]]. Action = [[ 0.22794652 -0.17621501  0.21712092  0.5918108 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 877 is [True, False, False, False, True, False]
Current timestep = 878. State = [[-0.12590846  0.00541056]]. Action = [[-0.04941712 -0.23719284  0.10498035  0.59842086]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 878 is [True, False, False, False, True, False]
Scene graph at timestep 878 is [True, False, False, False, True, False]
State prediction error at timestep 878 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 878 of 0
Current timestep = 879. State = [[-0.13080105 -0.01648526]]. Action = [[-0.22550637  0.00246528  0.22478047 -0.7743339 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 879 is [True, False, False, False, True, False]
Current timestep = 880. State = [[-0.13865453 -0.01949282]]. Action = [[-0.00873749 -0.03168879 -0.1688889   0.97591317]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 880 is [True, False, False, False, True, False]
Current timestep = 881. State = [[-0.13843215 -0.02171186]]. Action = [[ 0.13379082  0.0137825   0.0327346  -0.6548607 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 881 is [True, False, False, False, True, False]
Current timestep = 882. State = [[-0.13889785 -0.01368989]]. Action = [[-0.00984162  0.15039593 -0.08205333  0.24554217]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 882 is [True, False, False, False, True, False]
Scene graph at timestep 882 is [True, False, False, False, True, False]
State prediction error at timestep 882 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 882 of 0
Current timestep = 883. State = [[-0.13160117  0.01217782]]. Action = [[ 0.18315566  0.23441467 -0.22243166  0.60489976]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 883 is [True, False, False, False, True, False]
Current timestep = 884. State = [[-0.12030466  0.03616686]]. Action = [[-0.01170093  0.14410132  0.24144626 -0.7984739 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 884 is [True, False, False, False, True, False]
Current timestep = 885. State = [[-0.11968636  0.05716102]]. Action = [[ 0.09121948  0.14288557 -0.05597402  0.52876043]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 885 is [True, False, False, False, True, False]
Current timestep = 886. State = [[-0.10167906  0.05872458]]. Action = [[ 0.1389502  -0.19194792 -0.18710202  0.95836854]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 886 is [True, False, False, False, True, False]
Scene graph at timestep 886 is [True, False, False, False, True, False]
State prediction error at timestep 886 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 886 of 1
Current timestep = 887. State = [[-0.08071628  0.05151105]]. Action = [[ 0.1488288   0.06122193 -0.17391415  0.49554956]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 887 is [True, False, False, False, True, False]
Current timestep = 888. State = [[-0.06462157  0.04499848]]. Action = [[ 0.04881662 -0.16948535 -0.21145214  0.8446512 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 888 is [True, False, False, False, True, False]
Current timestep = 889. State = [[-0.04877945  0.02240759]]. Action = [[ 0.24443641 -0.22529848  0.19694191  0.442142  ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 889 is [True, False, False, False, True, False]
Current timestep = 890. State = [[-0.24939665 -0.126523  ]]. Action = [[ 0.00949228  0.10118252  0.07970834 -0.38060343]]. Reward = [100.]
Curr episode timestep = 17
Scene graph at timestep 890 is [False, True, False, False, True, False]
Current timestep = 891. State = [[-0.24756223 -0.13940182]]. Action = [[-0.17240837  0.1901254   0.21537954 -0.83824134]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 891 is [True, False, False, True, False, False]
Current timestep = 892. State = [[-0.25100005 -0.14660116]]. Action = [[-0.15586397 -0.0850147  -0.19306567  0.40724683]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 892 is [True, False, False, True, False, False]
Current timestep = 893. State = [[-0.2561899  -0.16434138]]. Action = [[ 0.04852688 -0.15702301  0.15802905 -0.18814176]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 893 is [True, False, False, True, False, False]
Current timestep = 894. State = [[-0.25046092 -0.16681592]]. Action = [[ 0.16483569  0.1543535  -0.09150633 -0.8479328 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 894 is [True, False, False, True, False, False]
Scene graph at timestep 894 is [True, False, False, True, False, False]
State prediction error at timestep 894 is tensor(4.9292e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 894 of 0
Current timestep = 895. State = [[-0.22787262 -0.16904835]]. Action = [[ 0.23590454 -0.15922453 -0.23152626  0.5920615 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 895 is [True, False, False, True, False, False]
Current timestep = 896. State = [[-0.21506539 -0.18136093]]. Action = [[-0.17888725 -0.05323896  0.04140571  0.34710443]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 896 is [True, False, False, True, False, False]
Current timestep = 897. State = [[-0.22136334 -0.18440752]]. Action = [[-0.09369424  0.07039553  0.11695343  0.39289594]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 897 is [True, False, False, True, False, False]
Scene graph at timestep 897 is [True, False, False, True, False, False]
State prediction error at timestep 897 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 897 of 0
Current timestep = 898. State = [[-0.2359625  -0.17288072]]. Action = [[-0.16821192  0.17239696  0.17360616  0.49473405]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 898 is [True, False, False, True, False, False]
Current timestep = 899. State = [[-0.24559622 -0.16279908]]. Action = [[-0.2305771   0.2323015   0.00093922 -0.33190733]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 899 is [True, False, False, True, False, False]
Current timestep = 900. State = [[-0.2459802  -0.14805482]]. Action = [[ 0.0623953   0.20967054 -0.2063926  -0.9217406 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 900 is [True, False, False, True, False, False]
Current timestep = 901. State = [[-0.25087798 -0.13848636]]. Action = [[-0.14694417 -0.08650111  0.16902888  0.24432826]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 901 is [True, False, False, True, False, False]
Current timestep = 902. State = [[-0.26476604 -0.15034804]]. Action = [[-0.11445111 -0.14213933 -0.18071245  0.8917661 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 902 is [True, False, False, True, False, False]
Current timestep = 903. State = [[-0.26761255 -0.1447033 ]]. Action = [[ 0.23069364  0.2228756  -0.09276974  0.16150093]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 903 is [True, False, False, True, False, False]
Current timestep = 904. State = [[-0.2522451 -0.1306189]]. Action = [[ 0.19887573  0.01497671  0.12150663 -0.8141805 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 904 is [True, False, False, True, False, False]
Current timestep = 905. State = [[-0.24006972 -0.13645418]]. Action = [[-0.04017255 -0.19543494  0.12019777 -0.5149605 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 905 is [True, False, False, True, False, False]
Current timestep = 906. State = [[-0.23837183 -0.15912399]]. Action = [[ 0.01974607 -0.22292784 -0.1317679  -0.7868189 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 906 is [True, False, False, True, False, False]
Current timestep = 907. State = [[-0.24157906 -0.16739058]]. Action = [[-0.16241845  0.19062138 -0.22329007 -0.34212565]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 907 is [True, False, False, True, False, False]
Current timestep = 908. State = [[-0.24484542 -0.148699  ]]. Action = [[ 0.0063695   0.2371426   0.11982027 -0.00248134]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 908 is [True, False, False, True, False, False]
Scene graph at timestep 908 is [True, False, False, True, False, False]
State prediction error at timestep 908 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 908 of 0
Current timestep = 909. State = [[-0.24950212 -0.13951185]]. Action = [[-0.07091945 -0.21963634  0.165501   -0.8611677 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 909 is [True, False, False, True, False, False]
Current timestep = 910. State = [[-0.2481701  -0.15956777]]. Action = [[ 0.19202492 -0.16929051  0.10652974  0.76768005]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 910 is [True, False, False, True, False, False]
Current timestep = 911. State = [[-0.24547128 -0.17710567]]. Action = [[-0.096123   -0.06166567 -0.23596331 -0.16349244]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 911 is [True, False, False, True, False, False]
Current timestep = 912. State = [[-0.2466027  -0.18393543]]. Action = [[-0.2213874   0.23260021  0.07438484 -0.82679605]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 912 is [True, False, False, True, False, False]
Current timestep = 913. State = [[-0.24228531 -0.19213338]]. Action = [[ 0.17588556 -0.13784255 -0.22507003 -0.3730741 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 913 is [True, False, False, True, False, False]
Current timestep = 914. State = [[-0.22443387 -0.19105677]]. Action = [[0.16874704 0.19895908 0.2100183  0.71811104]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 914 is [True, False, False, True, False, False]
Current timestep = 915. State = [[-0.21720503 -0.17736843]]. Action = [[-0.1934922   0.12180865 -0.23857284 -0.9686841 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 915 is [True, False, False, True, False, False]
Current timestep = 916. State = [[-0.22358155 -0.15603335]]. Action = [[-0.11137587  0.2055336  -0.1420935   0.060058  ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 916 is [True, False, False, True, False, False]
Current timestep = 917. State = [[-0.22320238 -0.12530768]]. Action = [[ 0.23589188  0.21487027 -0.18775684  0.7535393 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 917 is [True, False, False, True, False, False]
Current timestep = 918. State = [[-0.20508131 -0.09787253]]. Action = [[ 0.24215737  0.13113213 -0.08481415  0.9126222 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 918 is [True, False, False, True, False, False]
Current timestep = 919. State = [[-0.1906154  -0.09019363]]. Action = [[-0.05284868 -0.07787329 -0.07868889  0.86417866]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 919 is [True, False, False, False, True, False]
Current timestep = 920. State = [[-0.1898504  -0.10369792]]. Action = [[ 0.00410467 -0.21257105  0.04400149  0.6297896 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 920 is [True, False, False, False, True, False]
Current timestep = 921. State = [[-0.19235368 -0.11117991]]. Action = [[-0.14642158  0.10943067 -0.17199042 -0.9022283 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 921 is [True, False, False, False, True, False]
Current timestep = 922. State = [[-0.19506986 -0.11849116]]. Action = [[ 0.10058326 -0.16799092 -0.24239106 -0.9723127 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 922 is [True, False, False, False, True, False]
Current timestep = 923. State = [[-0.20118107 -0.12208258]]. Action = [[-0.22174904  0.08939475  0.1427714  -0.27770483]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 923 is [True, False, False, False, True, False]
Current timestep = 924. State = [[-0.21872236 -0.11647832]]. Action = [[-0.17986517  0.0842655   0.01000339 -0.6509867 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 924 is [True, False, False, False, True, False]
Current timestep = 925. State = [[-0.23887065 -0.09975481]]. Action = [[-0.15648335  0.1765635  -0.23970254  0.8550198 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 925 is [True, False, False, False, True, False]
Current timestep = 926. State = [[-0.24776766 -0.09773728]]. Action = [[ 0.20610353 -0.20438449 -0.08421785 -0.82547766]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 926 is [True, False, False, False, True, False]
Current timestep = 927. State = [[-0.24321026 -0.10384507]]. Action = [[-0.20065671  0.09643617  0.1935826   0.41907954]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 927 is [True, False, False, False, True, False]
Current timestep = 928. State = [[-0.24435355 -0.11631658]]. Action = [[-0.05044937 -0.16982454 -0.15523593 -0.08056444]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 928 is [True, False, False, False, True, False]
Current timestep = 929. State = [[-0.2457448  -0.12923573]]. Action = [[ 0.0141528  -0.01685765 -0.10014471  0.8537159 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 929 is [True, False, False, False, True, False]
Scene graph at timestep 929 is [True, False, False, True, False, False]
State prediction error at timestep 929 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 929 of 0
Current timestep = 930. State = [[-0.2444539  -0.14271112]]. Action = [[ 0.04643932 -0.13274108  0.16916996 -0.8172909 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 930 is [True, False, False, True, False, False]
Current timestep = 931. State = [[-0.2378015  -0.15328075]]. Action = [[ 0.13207132 -0.02604496 -0.12694107  0.13306808]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 931 is [True, False, False, True, False, False]
Current timestep = 932. State = [[-0.2213718 -0.1547541]]. Action = [[ 0.13732696  0.03525802 -0.10019703 -0.6370522 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 932 is [True, False, False, True, False, False]
Scene graph at timestep 932 is [True, False, False, True, False, False]
State prediction error at timestep 932 is tensor(7.0353e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 932 of 1
Current timestep = 933. State = [[-0.21004951 -0.14072813]]. Action = [[-0.05710301  0.23603642  0.13308012  0.11023438]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 933 is [True, False, False, True, False, False]
Scene graph at timestep 933 is [True, False, False, True, False, False]
State prediction error at timestep 933 is tensor(9.1355e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 933 of 1
Current timestep = 934. State = [[-0.21011633 -0.11437514]]. Action = [[-0.06723192  0.18102366 -0.22365834 -0.25658387]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 934 is [True, False, False, True, False, False]
Current timestep = 935. State = [[-0.21260232 -0.1113947 ]]. Action = [[ 0.00554919 -0.18665908 -0.1445534   0.4544921 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 935 is [True, False, False, False, True, False]
Current timestep = 936. State = [[-0.20562702 -0.12212398]]. Action = [[ 0.2243594  -0.08082865  0.18878967 -0.35160625]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 936 is [True, False, False, False, True, False]
Current timestep = 937. State = [[-0.18535265 -0.13046835]]. Action = [[ 0.200759   -0.04704039  0.17274421  0.04178476]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 937 is [True, False, False, False, True, False]
Current timestep = 938. State = [[-0.15720357 -0.1372209 ]]. Action = [[ 0.23755872 -0.05590738  0.05420315 -0.1039651 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 938 is [True, False, False, True, False, False]
Current timestep = 939. State = [[-0.12607524 -0.13461286]]. Action = [[ 0.15091476  0.14412946 -0.19533716  0.16778445]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 939 is [True, False, False, True, False, False]
Current timestep = 940. State = [[-0.10226509 -0.13822939]]. Action = [[ 0.16726038 -0.16508755  0.06671041 -0.3412512 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 940 is [True, False, False, True, False, False]
Current timestep = 941. State = [[-0.07910427 -0.15528315]]. Action = [[ 0.14349198 -0.17722782 -0.06761631  0.75779164]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 941 is [True, False, False, True, False, False]
Current timestep = 942. State = [[-0.06477663 -0.1741    ]]. Action = [[-0.03940679 -0.06471518 -0.22891296 -0.75260556]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 942 is [True, False, False, True, False, False]
Current timestep = 943. State = [[-0.0696786  -0.18302743]]. Action = [[-0.23159951  0.01646036 -0.18913762 -0.44999075]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 943 is [True, False, False, True, False, False]
Current timestep = 944. State = [[-0.07607972 -0.17635255]]. Action = [[-0.02101769  0.17972058  0.07615793  0.45005035]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 944 is [True, False, False, True, False, False]
Current timestep = 945. State = [[-0.07658318 -0.15780056]]. Action = [[ 0.01612332  0.10558373 -0.12619987  0.01749611]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 945 is [True, False, False, True, False, False]
Scene graph at timestep 945 is [True, False, False, True, False, False]
State prediction error at timestep 945 is tensor(9.4103e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 945 of 1
Current timestep = 946. State = [[-0.08251357 -0.15831248]]. Action = [[-0.10763875 -0.16646077  0.07178938  0.887745  ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 946 is [True, False, False, True, False, False]
Current timestep = 947. State = [[-0.1027041  -0.17201908]]. Action = [[-0.24071527 -0.0572831   0.21131533 -0.42486918]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 947 is [True, False, False, True, False, False]
Scene graph at timestep 947 is [True, False, False, True, False, False]
State prediction error at timestep 947 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 947 of -1
Current timestep = 948. State = [[-0.13547604 -0.18949926]]. Action = [[-0.17741099 -0.13427755  0.07119679 -0.03155535]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 948 is [True, False, False, True, False, False]
Current timestep = 949. State = [[-0.14353816 -0.2040645 ]]. Action = [[ 0.1757425  -0.13564424  0.23454973 -0.83700985]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 949 is [True, False, False, True, False, False]
Current timestep = 950. State = [[-0.14219055 -0.22286147]]. Action = [[-0.04320922 -0.17270786 -0.19644    -0.05549341]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 950 is [True, False, False, True, False, False]
Current timestep = 951. State = [[-0.1513137  -0.23324737]]. Action = [[-0.1972353   0.1162613  -0.09446287 -0.01394808]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 951 is [True, False, False, True, False, False]
Current timestep = 952. State = [[-0.16641407 -0.23345678]]. Action = [[-0.1591929   0.01603928  0.22886765  0.6966882 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 952 is [True, False, False, True, False, False]
Current timestep = 953. State = [[-0.19063179 -0.23522858]]. Action = [[-0.17374024 -0.05343996 -0.16714977  0.45838928]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 953 is [True, False, False, True, False, False]
Current timestep = 954. State = [[-0.20458865 -0.2446523 ]]. Action = [[ 0.0626924  -0.13531005 -0.15475318  0.6401653 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 954 is [True, False, False, True, False, False]
Current timestep = 955. State = [[-0.20369168 -0.25051537]]. Action = [[ 0.13565016 -0.00885597 -0.1112237   0.43096423]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 955 is [True, False, False, True, False, False]
Current timestep = 956. State = [[-0.20694359 -0.24645194]]. Action = [[-0.21711478  0.13282305  0.09284979  0.88503027]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 956 is [True, False, False, True, False, False]
Scene graph at timestep 956 is [True, False, False, True, False, False]
State prediction error at timestep 956 is tensor(5.2956e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 956 of -1
Current timestep = 957. State = [[-0.21808125 -0.2302351 ]]. Action = [[-0.17186971  0.23090535 -0.00143927  0.96202564]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 957 is [True, False, False, True, False, False]
Current timestep = 958. State = [[-0.22947656 -0.21445478]]. Action = [[ 0.17759287 -0.05499236 -0.04230121 -0.25636077]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 958 is [True, False, False, True, False, False]
Current timestep = 959. State = [[-0.21981421 -0.2166914 ]]. Action = [[ 0.184986   -0.09061281 -0.0503729   0.65628564]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 959 is [True, False, False, True, False, False]
Current timestep = 960. State = [[-0.20861983 -0.2301689 ]]. Action = [[ 0.07987273 -0.2054939   0.03081694 -0.34294283]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 960 is [True, False, False, True, False, False]
Current timestep = 961. State = [[-0.20141345 -0.23358   ]]. Action = [[-0.08704773  0.22601682 -0.15160324  0.11263597]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 961 is [True, False, False, True, False, False]
Current timestep = 962. State = [[-0.19459867 -0.21340998]]. Action = [[ 0.13391706  0.19726557 -0.16710284 -0.60484993]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 962 is [True, False, False, True, False, False]
Current timestep = 963. State = [[-0.17928043 -0.19449022]]. Action = [[ 0.22195423 -0.00148083  0.20223922 -0.9227038 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 963 is [True, False, False, True, False, False]
Current timestep = 964. State = [[-0.17333426 -0.20192508]]. Action = [[-0.19706494 -0.20468155 -0.10514531  0.53932977]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 964 is [True, False, False, True, False, False]
Scene graph at timestep 964 is [True, False, False, True, False, False]
State prediction error at timestep 964 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 964 of 0
Current timestep = 965. State = [[-0.17245844 -0.2090042 ]]. Action = [[ 0.13804239  0.12622476 -0.20875213  0.6746049 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 965 is [True, False, False, True, False, False]
Scene graph at timestep 965 is [True, False, False, True, False, False]
State prediction error at timestep 965 is tensor(7.5059e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 965 of 1
Current timestep = 966. State = [[-0.16290955 -0.18788768]]. Action = [[ 0.06862158  0.24517673 -0.01804313 -0.3648699 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 966 is [True, False, False, True, False, False]
Scene graph at timestep 966 is [True, False, False, True, False, False]
State prediction error at timestep 966 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 966 of 1
Current timestep = 967. State = [[-0.15219979 -0.1745727 ]]. Action = [[ 0.1078158  -0.15676446 -0.20768043  0.12473011]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 967 is [True, False, False, True, False, False]
Current timestep = 968. State = [[-0.13838674 -0.17878321]]. Action = [[ 0.16822028  0.0451616  -0.22035015  0.5533333 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 968 is [True, False, False, True, False, False]
Current timestep = 969. State = [[-0.12384839 -0.18833905]]. Action = [[ 0.00420964 -0.18315473 -0.05526669 -0.77030295]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 969 is [True, False, False, True, False, False]
Current timestep = 970. State = [[-0.11837701 -0.20158868]]. Action = [[ 0.05358553 -0.04441835  0.06839538 -0.13516366]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 970 is [True, False, False, True, False, False]
Scene graph at timestep 970 is [True, False, False, True, False, False]
State prediction error at timestep 970 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 970 of 0
Current timestep = 971. State = [[-0.10513205 -0.19819997]]. Action = [[ 0.15890187  0.16950786  0.14937675 -0.33300847]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 971 is [True, False, False, True, False, False]
Scene graph at timestep 971 is [True, False, False, True, False, False]
State prediction error at timestep 971 is tensor(3.5496e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 971 of 1
Current timestep = 972. State = [[-0.08244108 -0.18541022]]. Action = [[ 0.08832404  0.07274222 -0.23426831  0.1917206 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 972 is [True, False, False, True, False, False]
Current timestep = 973. State = [[-0.07586243 -0.18927124]]. Action = [[ 0.0925867  -0.18393204  0.02907026 -0.42388147]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 973 is [True, False, False, True, False, False]
Current timestep = 974. State = [[-0.06468869 -0.187017  ]]. Action = [[-0.04982667  0.21700439 -0.19230087  0.95309186]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 974 is [True, False, False, True, False, False]
Current timestep = 975. State = [[-0.0698107  -0.16664125]]. Action = [[-0.23999967  0.2123051   0.05929071  0.76272845]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 975 is [True, False, False, True, False, False]
Current timestep = 976. State = [[-0.08790936 -0.15434772]]. Action = [[-0.20968471 -0.04383916 -0.1985938   0.29540694]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 976 is [True, False, False, True, False, False]
Scene graph at timestep 976 is [True, False, False, True, False, False]
State prediction error at timestep 976 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 976 of 0
Current timestep = 977. State = [[-0.10488038 -0.13955253]]. Action = [[ 0.07946286  0.23956633 -0.12385294 -0.84362674]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 977 is [True, False, False, True, False, False]
Current timestep = 978. State = [[-0.095723   -0.11229174]]. Action = [[ 0.21250963  0.11714923 -0.13027668 -0.5529529 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 978 is [True, False, False, True, False, False]
Current timestep = 979. State = [[-0.09158999 -0.11264022]]. Action = [[-0.05910178 -0.20687354 -0.12816697 -0.20716214]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 979 is [True, False, False, False, True, False]
Current timestep = 980. State = [[-0.08789233 -0.13205953]]. Action = [[ 0.12848777 -0.21110328  0.2365036   0.92946506]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 980 is [True, False, False, False, True, False]
Scene graph at timestep 980 is [True, False, False, True, False, False]
State prediction error at timestep 980 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 980 of 0
Current timestep = 981. State = [[-0.08485237 -0.14414686]]. Action = [[-0.16647783  0.16674966 -0.24277127 -0.7590215 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 981 is [True, False, False, True, False, False]
Current timestep = 982. State = [[-0.09312376 -0.1269192 ]]. Action = [[-0.17286901  0.18159968 -0.22993457 -0.72531956]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 982 is [True, False, False, True, False, False]
Scene graph at timestep 982 is [True, False, False, True, False, False]
State prediction error at timestep 982 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 982 of 0
Current timestep = 983. State = [[-0.1061469  -0.11899038]]. Action = [[ 0.02971858 -0.17154458  0.02768227 -0.8224575 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 983 is [True, False, False, True, False, False]
Current timestep = 984. State = [[-0.11070897 -0.1403637 ]]. Action = [[ 0.01950595 -0.22782508 -0.19275586  0.6496272 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 984 is [True, False, False, False, True, False]
Current timestep = 985. State = [[-0.10301633 -0.15099274]]. Action = [[ 0.2141299   0.10450324 -0.04079227 -0.7399114 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 985 is [True, False, False, True, False, False]
Current timestep = 986. State = [[-0.08813397 -0.1353717 ]]. Action = [[ 0.05719936  0.23155004  0.24445441 -0.431516  ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 986 is [True, False, False, True, False, False]
Current timestep = 987. State = [[-0.07835823 -0.10702454]]. Action = [[ 0.09854293  0.23135412  0.18011513 -0.07670945]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 987 is [True, False, False, True, False, False]
Current timestep = 988. State = [[-0.06124322 -0.09838013]]. Action = [[ 0.22264972 -0.20658363 -0.02889283  0.14799643]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 988 is [True, False, False, False, True, False]
Current timestep = 989. State = [[-0.04672613 -0.10598817]]. Action = [[-0.08644164 -0.0100513  -0.17137176  0.26885915]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 989 is [True, False, False, False, True, False]
Scene graph at timestep 989 is [False, True, False, False, True, False]
State prediction error at timestep 989 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 989 of 1
Current timestep = 990. State = [[-0.04812536 -0.10403161]]. Action = [[-0.11699902  0.09536722  0.11180034 -0.5424728 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 990 is [False, True, False, False, True, False]
Current timestep = 991. State = [[-0.05523864 -0.10401165]]. Action = [[-0.14834146 -0.02299859 -0.10107404  0.63363266]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 991 is [False, True, False, False, True, False]
Scene graph at timestep 991 is [True, False, False, False, True, False]
State prediction error at timestep 991 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 991 of -1
Current timestep = 992. State = [[-0.0698697  -0.09260025]]. Action = [[-0.22737765  0.22711635  0.01800779  0.5837679 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 992 is [True, False, False, False, True, False]
Current timestep = 993. State = [[-0.08489747 -0.0770013 ]]. Action = [[ 0.21134639 -0.04769766  0.11457843  0.7486117 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 993 is [True, False, False, False, True, False]
Current timestep = 994. State = [[-0.07796455 -0.06555223]]. Action = [[0.10191828 0.1838539  0.10404211 0.4539976 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 994 is [True, False, False, False, True, False]
Current timestep = 995. State = [[-0.07325971 -0.03976284]]. Action = [[-0.03206688  0.24722955  0.23238903 -0.86024827]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 995 is [True, False, False, False, True, False]
Scene graph at timestep 995 is [True, False, False, False, True, False]
State prediction error at timestep 995 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 995 of 0
Current timestep = 996. State = [[-0.0809835  -0.00357954]]. Action = [[-0.18591551  0.19503489  0.01435125 -0.24065363]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 996 is [True, False, False, False, True, False]
Current timestep = 997. State = [[-0.08656739  0.02121065]]. Action = [[ 0.10758242  0.12805325  0.05688006 -0.6902699 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 997 is [True, False, False, False, True, False]
Current timestep = 998. State = [[-0.07904109  0.01905432]]. Action = [[ 0.24060649 -0.23339038  0.00274736  0.5031891 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 998 is [True, False, False, False, True, False]
Current timestep = 999. State = [[-0.05418442  0.00333025]]. Action = [[ 0.16647339 -0.06419268 -0.15984619 -0.4788847 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 999 is [True, False, False, False, True, False]
Current timestep = 1000. State = [[-0.18928134  0.19156837]]. Action = [[ 0.17805865  0.15780365 -0.04587492 -0.54016936]]. Reward = [100.]
Curr episode timestep = 109
Scene graph at timestep 1000 is [True, False, False, False, True, False]
Scene graph at timestep 1000 is [True, False, False, False, False, True]
State prediction error at timestep 1000 is tensor(0.0284, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1000 of -1
Current timestep = 1001. State = [[-0.16375972  0.20182331]]. Action = [[ 0.21950883 -0.19723934  0.03721789  0.20294511]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1001 is [True, False, False, False, False, True]
Current timestep = 1002. State = [[-0.14673105  0.20140204]]. Action = [[0.04208419 0.1871185  0.20573938 0.28787708]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1002 is [True, False, False, False, False, True]
Current timestep = 1003. State = [[-0.13586396  0.21935733]]. Action = [[ 0.08108309  0.14574054 -0.13223429  0.11173189]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1003 is [True, False, False, False, False, True]
Current timestep = 1004. State = [[-0.11931005  0.23459767]]. Action = [[ 0.18359989  0.08875218 -0.02230291  0.79386425]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1004 is [True, False, False, False, False, True]
Current timestep = 1005. State = [[-0.09300783  0.23092122]]. Action = [[ 0.14390975 -0.22875962  0.16930234 -0.4355116 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1005 is [True, False, False, False, False, True]
Current timestep = 1006. State = [[-0.06631294  0.20635726]]. Action = [[ 0.24208462 -0.21403839  0.04332441  0.6742072 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1006 is [True, False, False, False, False, True]
Scene graph at timestep 1006 is [True, False, False, False, False, True]
State prediction error at timestep 1006 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1006 of 1
Current timestep = 1007. State = [[-0.03088154  0.17579359]]. Action = [[ 0.2156454  -0.20562574 -0.1644139  -0.07208318]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1007 is [True, False, False, False, False, True]
Scene graph at timestep 1007 is [False, True, False, False, False, True]
State prediction error at timestep 1007 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1007 of 1
Current timestep = 1008. State = [[-0.00311861  0.16424835]]. Action = [[0.09547794 0.07628149 0.20701817 0.5831008 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1008 is [False, True, False, False, False, True]
Current timestep = 1009. State = [[0.0102408  0.15503772]]. Action = [[ 0.08386153 -0.23567484 -0.09419347  0.5391047 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1009 is [False, True, False, False, False, True]
Current timestep = 1010. State = [[0.02679132 0.14038463]]. Action = [[ 0.16766801  0.00941098  0.21204096 -0.06755704]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1010 is [False, True, False, False, False, True]
Current timestep = 1011. State = [[0.03906081 0.142134  ]]. Action = [[-0.06191055  0.04511136 -0.24003516  0.4666642 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1011 is [False, True, False, False, False, True]
Current timestep = 1012. State = [[0.04031339 0.14338858]]. Action = [[ 0.19093543  0.05627465  0.10087931 -0.43867612]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1012 is [False, True, False, False, False, True]
Current timestep = 1013. State = [[0.04054141 0.14348766]]. Action = [[0.22029787 0.02108374 0.14612788 0.35678995]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1013 is [False, True, False, False, False, True]
Scene graph at timestep 1013 is [False, True, False, False, False, True]
State prediction error at timestep 1013 is tensor(5.9213e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1013 of 1
Current timestep = 1014. State = [[0.04058092 0.14350459]]. Action = [[-0.00349863  0.00235131  0.04832888  0.8215041 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1014 is [False, True, False, False, False, True]
Current timestep = 1015. State = [[0.0415488  0.13755567]]. Action = [[-0.12001544 -0.14038421  0.20370346 -0.69490105]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1015 is [False, True, False, False, False, True]
Scene graph at timestep 1015 is [False, True, False, False, False, True]
State prediction error at timestep 1015 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1015 of 1
Current timestep = 1016. State = [[0.04345001 0.12697452]]. Action = [[ 0.06528211 -0.05495054  0.06137574 -0.21312726]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1016 is [False, True, False, False, False, True]
Scene graph at timestep 1016 is [False, True, False, False, False, True]
State prediction error at timestep 1016 is tensor(4.6824e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1016 of 1
Current timestep = 1017. State = [[0.04475905 0.12335607]]. Action = [[ 0.13992915  0.24331087 -0.04732585 -0.5942884 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1017 is [False, True, False, False, False, True]
Current timestep = 1018. State = [[0.04348554 0.13169138]]. Action = [[0.1088185  0.20011687 0.2231667  0.95655584]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1018 is [False, True, False, False, True, False]
Scene graph at timestep 1018 is [False, True, False, False, False, True]
State prediction error at timestep 1018 is tensor(5.9808e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1018 of -1
Current timestep = 1019. State = [[0.04771884 0.14067088]]. Action = [[ 0.08109468 -0.02005047 -0.23778056 -0.3273207 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1019 is [False, True, False, False, False, True]
Current timestep = 1020. State = [[0.05004686 0.15517744]]. Action = [[-0.0782731   0.22683382 -0.21880133 -0.82482076]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1020 is [False, True, False, False, False, True]
Scene graph at timestep 1020 is [False, False, True, False, False, True]
State prediction error at timestep 1020 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1020 of -1
Current timestep = 1021. State = [[0.04779471 0.1722553 ]]. Action = [[-0.02144219  0.01169538 -0.02189256  0.18842518]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1021 is [False, False, True, False, False, True]
Scene graph at timestep 1021 is [False, True, False, False, False, True]
State prediction error at timestep 1021 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1021 of -1
Current timestep = 1022. State = [[0.04414507 0.18253402]]. Action = [[-0.01339652  0.1435715  -0.18324436 -0.24738246]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1022 is [False, True, False, False, False, True]
Current timestep = 1023. State = [[0.03433983 0.20399739]]. Action = [[-0.19757774  0.15910608  0.0173507   0.8489418 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1023 is [False, True, False, False, False, True]
Current timestep = 1024. State = [[0.01897282 0.2324026 ]]. Action = [[-0.21843871  0.16491205 -0.21459828  0.4874091 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1024 is [False, True, False, False, False, True]
Scene graph at timestep 1024 is [False, True, False, False, False, True]
State prediction error at timestep 1024 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1024 of -1
Current timestep = 1025. State = [[0.00471695 0.25458694]]. Action = [[ 0.10251328  0.06200796  0.13441753 -0.6737206 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1025 is [False, True, False, False, False, True]
Current timestep = 1026. State = [[-0.00123738  0.26542792]]. Action = [[-0.18144128  0.14191285  0.09048113 -0.23210567]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1026 is [False, True, False, False, False, True]
Scene graph at timestep 1026 is [False, True, False, False, False, True]
State prediction error at timestep 1026 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1026 of -1
Current timestep = 1027. State = [[-0.01317562  0.28434986]]. Action = [[-0.05221316  0.0170089   0.20313054 -0.11179948]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1027 is [False, True, False, False, False, True]
Current timestep = 1028. State = [[-0.01322464  0.27949035]]. Action = [[-8.8699162e-04 -1.3521816e-01 -1.7650490e-01  9.3451881e-01]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1028 is [False, True, False, False, False, True]
Scene graph at timestep 1028 is [False, True, False, False, False, True]
State prediction error at timestep 1028 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1028 of 0
Current timestep = 1029. State = [[-0.01497445  0.27750805]]. Action = [[-0.01958877  0.1100944  -0.13402864  0.8096707 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1029 is [False, True, False, False, False, True]
Scene graph at timestep 1029 is [False, True, False, False, False, True]
State prediction error at timestep 1029 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1029 of -1
Current timestep = 1030. State = [[-0.01554023  0.2809271 ]]. Action = [[ 0.17200857 -0.00074817 -0.0125536  -0.3687427 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1030 is [False, True, False, False, False, True]
Current timestep = 1031. State = [[-0.00747667  0.26879075]]. Action = [[ 0.08137411 -0.1927259  -0.1416876  -0.91263586]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1031 is [False, True, False, False, False, True]
Scene graph at timestep 1031 is [False, True, False, False, False, True]
State prediction error at timestep 1031 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1031 of 1
Current timestep = 1032. State = [[0.00824755 0.24140486]]. Action = [[ 0.22106612 -0.17228264 -0.06395286 -0.68187726]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1032 is [False, True, False, False, False, True]
Current timestep = 1033. State = [[0.02902845 0.23166029]]. Action = [[0.20771545 0.11518767 0.04038045 0.5844461 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1033 is [False, True, False, False, False, True]
Scene graph at timestep 1033 is [False, True, False, False, False, True]
State prediction error at timestep 1033 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1033 of 0
Current timestep = 1034. State = [[0.05601389 0.23934637]]. Action = [[-0.05478899 -0.0460254   0.08746964 -0.03309333]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1034 is [False, True, False, False, False, True]
Scene graph at timestep 1034 is [False, False, True, False, False, True]
State prediction error at timestep 1034 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1034 of 0
Current timestep = 1035. State = [[0.05353067 0.2438074 ]]. Action = [[-0.07148647  0.10020316  0.11651462  0.8719468 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1035 is [False, False, True, False, False, True]
Current timestep = 1036. State = [[0.05584722 0.23812686]]. Action = [[ 0.06292889 -0.17694479 -0.14020495  0.5738654 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1036 is [False, False, True, False, False, True]
Scene graph at timestep 1036 is [False, False, True, False, False, True]
State prediction error at timestep 1036 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1036 of 0
Current timestep = 1037. State = [[0.05931533 0.22842719]]. Action = [[ 0.22560155  0.08025104 -0.08975023  0.7655879 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1037 is [False, False, True, False, False, True]
Current timestep = 1038. State = [[0.06121176 0.2162716 ]]. Action = [[-0.08917974 -0.23008999 -0.20055507 -0.27225626]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1038 is [False, False, True, False, False, True]
Scene graph at timestep 1038 is [False, False, True, False, False, True]
State prediction error at timestep 1038 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1038 of 1
Current timestep = 1039. State = [[0.06333578 0.19938704]]. Action = [[ 0.17260003 -0.00965776  0.19301963 -0.7024732 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1039 is [False, False, True, False, False, True]
Scene graph at timestep 1039 is [False, False, True, False, False, True]
State prediction error at timestep 1039 is tensor(8.7581e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1039 of 1
Current timestep = 1040. State = [[0.06383438 0.19553354]]. Action = [[ 0.00732929 -0.06737894  0.1423791   0.9912183 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1040 is [False, False, True, False, False, True]
Current timestep = 1041. State = [[0.06424519 0.19212492]]. Action = [[ 0.15535432 -0.20830241 -0.1927035   0.2708001 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1041 is [False, False, True, False, False, True]
Current timestep = 1042. State = [[0.06312296 0.19308911]]. Action = [[-0.07258582  0.0428611  -0.14915292 -0.5049787 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1042 is [False, False, True, False, False, True]
Current timestep = 1043. State = [[0.0551355  0.20102334]]. Action = [[-0.24532506  0.07211027  0.01561001  0.8545046 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1043 is [False, False, True, False, False, True]
Current timestep = 1044. State = [[0.03214585 0.19747499]]. Action = [[-0.10326687 -0.1646123  -0.2070828   0.00667739]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1044 is [False, False, True, False, False, True]
Current timestep = 1045. State = [[0.01997881 0.1731545 ]]. Action = [[-0.07170494 -0.22190168  0.15130231  0.5850477 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1045 is [False, True, False, False, False, True]
Current timestep = 1046. State = [[0.01416058 0.16326791]]. Action = [[ 0.22343642  0.1660937  -0.15206254  0.5677221 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1046 is [False, True, False, False, False, True]
Scene graph at timestep 1046 is [False, True, False, False, False, True]
State prediction error at timestep 1046 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1046 of 1
Current timestep = 1047. State = [[0.01260509 0.1766691 ]]. Action = [[-0.01947923  0.17184526  0.07814971 -0.29443675]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1047 is [False, True, False, False, False, True]
Current timestep = 1048. State = [[0.00754233 0.17778717]]. Action = [[-0.19541834 -0.18865332 -0.19200543 -0.93289566]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1048 is [False, True, False, False, False, True]
Current timestep = 1049. State = [[0.0026185  0.15881322]]. Action = [[-0.0862366  -0.21765068  0.07837856 -0.11098838]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1049 is [False, True, False, False, False, True]
Scene graph at timestep 1049 is [False, True, False, False, False, True]
State prediction error at timestep 1049 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1049 of 1
Current timestep = 1050. State = [[-0.01431318  0.13338234]]. Action = [[-0.14451611 -0.03885522 -0.18849476 -0.02323318]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1050 is [False, True, False, False, False, True]
Current timestep = 1051. State = [[-0.02658104  0.14395139]]. Action = [[0.06603804 0.21698612 0.03135476 0.71013486]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1051 is [False, True, False, False, False, True]
Current timestep = 1052. State = [[-0.02574681  0.16540316]]. Action = [[0.2301543  0.1897729  0.04089597 0.7826977 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1052 is [False, True, False, False, False, True]
Current timestep = 1053. State = [[-0.01367018  0.16541725]]. Action = [[ 0.18984842 -0.19384123  0.1893447   0.74011445]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1053 is [False, True, False, False, False, True]
Current timestep = 1054. State = [[0.00523459 0.15754989]]. Action = [[0.1372261  0.03644812 0.09245372 0.60477376]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1054 is [False, True, False, False, False, True]
Current timestep = 1055. State = [[0.02491777 0.14645286]]. Action = [[ 0.19216469 -0.20107512  0.21867144  0.6507659 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1055 is [False, True, False, False, False, True]
Scene graph at timestep 1055 is [False, True, False, False, False, True]
State prediction error at timestep 1055 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1055 of -1
Current timestep = 1056. State = [[0.04666422 0.13019715]]. Action = [[ 0.2025322  -0.01683013 -0.16962884  0.33879614]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1056 is [False, True, False, False, False, True]
Current timestep = 1057. State = [[0.04666422 0.13019715]]. Action = [[ 0.13054204 -0.13116789  0.00565028 -0.4359374 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1057 is [False, True, False, False, False, True]
Current timestep = 1058. State = [[0.04814578 0.12322902]]. Action = [[-0.08502203 -0.14251854 -0.04600574  0.05043507]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1058 is [False, True, False, False, False, True]
Current timestep = 1059. State = [[0.04956695 0.11568373]]. Action = [[ 0.12464696 -0.1038993  -0.06167537 -0.7395134 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1059 is [False, True, False, False, True, False]
Current timestep = 1060. State = [[0.04996025 0.11477957]]. Action = [[ 0.09217283  0.03976333 -0.2362655   0.27800274]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1060 is [False, True, False, False, True, False]
Current timestep = 1061. State = [[0.05012123 0.12004441]]. Action = [[ 0.00143218  0.10673201 -0.24851044  0.8363521 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1061 is [False, True, False, False, True, False]
Scene graph at timestep 1061 is [False, False, True, False, True, False]
State prediction error at timestep 1061 is tensor(5.9136e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1061 of 0
Current timestep = 1062. State = [[0.04924631 0.13257784]]. Action = [[-0.24224773  0.04920334 -0.01246884  0.05177748]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1062 is [False, False, True, False, True, False]
Scene graph at timestep 1062 is [False, True, False, False, False, True]
State prediction error at timestep 1062 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1062 of -1
Current timestep = 1063. State = [[0.04217142 0.15220988]]. Action = [[ 0.0408307   0.2387681  -0.24284737 -0.04020417]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1063 is [False, True, False, False, False, True]
Current timestep = 1064. State = [[0.03842862 0.16250333]]. Action = [[ 0.10814765 -0.07171473  0.09930336 -0.011913  ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1064 is [False, True, False, False, False, True]
Scene graph at timestep 1064 is [False, True, False, False, False, True]
State prediction error at timestep 1064 is tensor(3.4043e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1064 of -1
Current timestep = 1065. State = [[0.04095945 0.15638272]]. Action = [[ 0.04351673 -0.07493377  0.09327772 -0.1011883 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1065 is [False, True, False, False, False, True]
Current timestep = 1066. State = [[0.03815892 0.16483146]]. Action = [[-0.1577332   0.20383245  0.04706296  0.43177307]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1066 is [False, True, False, False, False, True]
Current timestep = 1067. State = [[0.03355001 0.17659378]]. Action = [[ 0.0699378   0.03372225 -0.19962631  0.19020092]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1067 is [False, True, False, False, False, True]
Current timestep = 1068. State = [[0.02928539 0.18534496]]. Action = [[-0.20163618  0.04588571 -0.15448779 -0.59557253]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1068 is [False, True, False, False, False, True]
Current timestep = 1069. State = [[0.02501105 0.19307299]]. Action = [[ 0.22396278 -0.24259059  0.1778875  -0.6586714 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1069 is [False, True, False, False, False, True]
Current timestep = 1070. State = [[0.02455628 0.19435078]]. Action = [[ 0.11883181  0.04718283 -0.16843773 -0.5032697 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1070 is [False, True, False, False, False, True]
Current timestep = 1071. State = [[0.02011566 0.2073507 ]]. Action = [[-0.1626079   0.21006984 -0.01381415 -0.66365725]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1071 is [False, True, False, False, False, True]
Current timestep = 1072. State = [[0.01243024 0.22257055]]. Action = [[ 0.07814795 -0.04041278 -0.22257638  0.76821136]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1072 is [False, True, False, False, False, True]
Current timestep = 1073. State = [[0.01052027 0.21561761]]. Action = [[-0.16760422 -0.16128895 -0.11638963  0.23275232]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1073 is [False, True, False, False, False, True]
Scene graph at timestep 1073 is [False, True, False, False, False, True]
State prediction error at timestep 1073 is tensor(1.9887e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1073 of -1
Current timestep = 1074. State = [[0.00720693 0.20368752]]. Action = [[-0.10170802 -0.07027106  0.15843245  0.5304533 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1074 is [False, True, False, False, False, True]
Current timestep = 1075. State = [[0.0026262  0.20558916]]. Action = [[ 0.13295686  0.14675581 -0.12148035 -0.8132919 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1075 is [False, True, False, False, False, True]
Current timestep = 1076. State = [[0.00851615 0.2089917 ]]. Action = [[ 0.22480333 -0.01485522 -0.14869745 -0.23103297]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1076 is [False, True, False, False, False, True]
Current timestep = 1077. State = [[0.01442965 0.20751148]]. Action = [[-0.21416682 -0.09272555  0.2286346  -0.59474784]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1077 is [False, True, False, False, False, True]
Current timestep = 1078. State = [[0.01776991 0.18896197]]. Action = [[ 0.1708456  -0.2388385  -0.15508279 -0.28168142]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1078 is [False, True, False, False, False, True]
Current timestep = 1079. State = [[0.03107034 0.15947938]]. Action = [[ 0.21716195 -0.18190852 -0.15439172  0.8927939 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1079 is [False, True, False, False, False, True]
Scene graph at timestep 1079 is [False, True, False, False, False, True]
State prediction error at timestep 1079 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1079 of 1
Current timestep = 1080. State = [[0.04272459 0.13510916]]. Action = [[ 0.16900605  0.1486412  -0.06395799  0.29835045]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1080 is [False, True, False, False, False, True]
Current timestep = 1081. State = [[0.04272459 0.13510916]]. Action = [[ 0.2313101  -0.03426556  0.22818136 -0.63729626]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1081 is [False, True, False, False, False, True]
Scene graph at timestep 1081 is [False, True, False, False, False, True]
State prediction error at timestep 1081 is tensor(1.4443e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1081 of 1
Current timestep = 1082. State = [[0.0401246  0.14667627]]. Action = [[-0.13328744  0.20624793  0.07220232  0.2803725 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1082 is [False, True, False, False, False, True]
Current timestep = 1083. State = [[0.03534003 0.15925546]]. Action = [[ 0.22189501 -0.22342795 -0.1802505   0.9010675 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1083 is [False, True, False, False, False, True]
Current timestep = 1084. State = [[0.03644278 0.16307242]]. Action = [[ 0.07232881  0.03408223 -0.14326465  0.5824206 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1084 is [False, True, False, False, False, True]
Current timestep = 1085. State = [[0.04168389 0.17519665]]. Action = [[ 0.09957108  0.1435942  -0.24095955 -0.4363553 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 1085 is [False, True, False, False, False, True]
Scene graph at timestep 1085 is [False, True, False, False, False, True]
State prediction error at timestep 1085 is tensor(3.5550e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1085 of -1
Current timestep = 1086. State = [[0.05038977 0.19141541]]. Action = [[0.13079947 0.11962479 0.11754501 0.7673998 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1086 is [False, True, False, False, False, True]
Current timestep = 1087. State = [[0.05030734 0.19142494]]. Action = [[ 0.17613971 -0.21815512  0.13311318  0.9162116 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1087 is [False, False, True, False, False, True]
Current timestep = 1088. State = [[0.05030734 0.19142494]]. Action = [[ 0.17676774 -0.06659585 -0.16741854  0.8555403 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1088 is [False, False, True, False, False, True]
Current timestep = 1089. State = [[0.04544087 0.20212618]]. Action = [[-0.19762419  0.16949892 -0.21651861 -0.8959029 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1089 is [False, False, True, False, False, True]
Scene graph at timestep 1089 is [False, True, False, False, False, True]
State prediction error at timestep 1089 is tensor(6.3410e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1089 of -1
Current timestep = 1090. State = [[0.03869071 0.21498205]]. Action = [[0.20516592 0.01716036 0.19254622 0.0920347 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 1090 is [False, True, False, False, False, True]
Current timestep = 1091. State = [[0.04003367 0.20752537]]. Action = [[-0.02384903 -0.17147504  0.22628763  0.04167306]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1091 is [False, True, False, False, False, True]
Scene graph at timestep 1091 is [False, True, False, False, False, True]
State prediction error at timestep 1091 is tensor(7.9922e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1091 of 1
Current timestep = 1092. State = [[0.03905402 0.1964945 ]]. Action = [[ 0.22633746 -0.06522605  0.01229671  0.5375525 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1092 is [False, True, False, False, False, True]
Current timestep = 1093. State = [[0.03931536 0.18972921]]. Action = [[ 0.00262383 -0.11116831  0.06379142 -0.8358284 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 1093 is [False, True, False, False, False, True]
Scene graph at timestep 1093 is [False, True, False, False, False, True]
State prediction error at timestep 1093 is tensor(6.2392e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1093 of 1
Current timestep = 1094. State = [[0.03908545 0.18881954]]. Action = [[ 0.06223738  0.13289064  0.14424631 -0.97624314]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1094 is [False, True, False, False, False, True]
Scene graph at timestep 1094 is [False, True, False, False, False, True]
State prediction error at timestep 1094 is tensor(4.0372e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1094 of -1
Current timestep = 1095. State = [[0.03470695 0.1972081 ]]. Action = [[-0.22895046  0.01164901 -0.06005543 -0.42653644]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1095 is [False, True, False, False, False, True]
Current timestep = 1096. State = [[0.02767401 0.20234892]]. Action = [[0.15424556 0.06906781 0.05484274 0.20818436]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 1096 is [False, True, False, False, False, True]
Current timestep = 1097. State = [[0.02415295 0.21611436]]. Action = [[-0.15434279  0.2203115   0.24275684 -0.7299072 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 1097 is [False, True, False, False, False, True]
Current timestep = 1098. State = [[0.01838093 0.24139418]]. Action = [[0.19595999 0.11702704 0.22041813 0.6737914 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 1098 is [False, True, False, False, False, True]
Current timestep = 1099. State = [[0.02037191 0.2610463 ]]. Action = [[-0.02113698  0.17383155 -0.18329348  0.54820824]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1099 is [False, True, False, False, False, True]
Current timestep = 1100. State = [[0.02539095 0.264215  ]]. Action = [[ 0.05913475 -0.1866561   0.10507029  0.19630992]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1100 is [False, True, False, False, False, True]
Current timestep = 1101. State = [[0.0258928 0.2592956]]. Action = [[-0.23264465 -0.0084049   0.07592067  0.4982704 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1101 is [False, True, False, False, False, True]
Scene graph at timestep 1101 is [False, True, False, False, False, True]
State prediction error at timestep 1101 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1101 of -1
Current timestep = 1102. State = [[0.02098079 0.25797284]]. Action = [[ 0.05339843 -0.01237401 -0.12530285 -0.60070276]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1102 is [False, True, False, False, False, True]
Current timestep = 1103. State = [[0.02104081 0.25776863]]. Action = [[-0.02228384  0.00119469 -0.19149014 -0.8057095 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1103 is [False, True, False, False, False, True]
Current timestep = 1104. State = [[0.02718289 0.24830416]]. Action = [[ 0.24442875 -0.1382408  -0.24381137  0.7094822 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1104 is [False, True, False, False, False, True]
Current timestep = 1105. State = [[0.03524898 0.22473551]]. Action = [[-0.15026075 -0.21660526  0.23783892 -0.99343765]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1105 is [False, True, False, False, False, True]
Current timestep = 1106. State = [[0.0359702  0.21241371]]. Action = [[ 0.15067959  0.0603143  -0.15679078 -0.8845076 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1106 is [False, True, False, False, False, True]
Current timestep = 1107. State = [[0.03644606 0.21323732]]. Action = [[-0.11494721  0.02318636  0.11614615  0.89274406]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1107 is [False, True, False, False, False, True]
Current timestep = 1108. State = [[0.03051998 0.22672799]]. Action = [[-0.18001598  0.20688808 -0.00363247 -0.832543  ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1108 is [False, True, False, False, False, True]
Current timestep = 1109. State = [[0.01671079 0.23885845]]. Action = [[-0.05234218 -0.07916893 -0.04357851  0.6868223 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1109 is [False, True, False, False, False, True]
Current timestep = 1110. State = [[0.00952744 0.24901788]]. Action = [[-0.07520871  0.20150736 -0.20493811  0.74187875]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1110 is [False, True, False, False, False, True]
Current timestep = 1111. State = [[0.00235871 0.26320088]]. Action = [[ 0.1593633   0.0460141  -0.15203984 -0.38487434]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1111 is [False, True, False, False, False, True]
Scene graph at timestep 1111 is [False, True, False, False, False, True]
State prediction error at timestep 1111 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1111 of -1
Current timestep = 1112. State = [[0.01032074 0.25578654]]. Action = [[ 0.23549336 -0.18779974  0.07991081 -0.3416463 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1112 is [False, True, False, False, False, True]
Current timestep = 1113. State = [[0.02629188 0.25591192]]. Action = [[ 0.00391835  0.24074924 -0.00258534 -0.39974892]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1113 is [False, True, False, False, False, True]
Scene graph at timestep 1113 is [False, True, False, False, False, True]
State prediction error at timestep 1113 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1113 of -1
Current timestep = 1114. State = [[0.03362479 0.2642513 ]]. Action = [[ 0.13611013 -0.09507304  0.04016203 -0.20268029]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1114 is [False, True, False, False, False, True]
Scene graph at timestep 1114 is [False, True, False, False, False, True]
State prediction error at timestep 1114 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1114 of -1
Current timestep = 1115. State = [[0.04665028 0.2731246 ]]. Action = [[-0.02151313  0.23618057  0.12148923  0.05622602]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1115 is [False, True, False, False, False, True]
Current timestep = 1116. State = [[0.04355002 0.29617187]]. Action = [[-0.07550064  0.11667514 -0.11602232 -0.6032172 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1116 is [False, True, False, False, False, True]
Current timestep = 1117. State = [[0.04384213 0.3024192 ]]. Action = [[ 0.08545533 -0.04580878  0.23301393  0.78449035]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1117 is [False, True, False, False, False, True]
Current timestep = 1118. State = [[0.04555666 0.30118752]]. Action = [[-0.13111201 -0.09875523  0.16910362  0.08936846]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1118 is [False, True, False, False, False, True]
Current timestep = 1119. State = [[0.04571534 0.29834154]]. Action = [[ 0.12344775 -0.21407507  0.23121727 -0.11174673]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1119 is [False, True, False, False, False, True]
Current timestep = 1120. State = [[0.04576207 0.29751003]]. Action = [[-0.2054335   0.1588788   0.10834882 -0.6024374 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1120 is [False, True, False, False, False, True]
Current timestep = 1121. State = [[0.04562372 0.29750627]]. Action = [[-0.03304711  0.07509792 -0.16989557 -0.65265244]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1121 is [False, True, False, False, False, True]
Current timestep = 1122. State = [[0.04533805 0.29789233]]. Action = [[ 0.01220703  0.03665727  0.22441429 -0.82642055]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1122 is [False, True, False, False, False, True]
Current timestep = 1123. State = [[0.04415782 0.2888833 ]]. Action = [[-0.13272183 -0.19045629 -0.03486542  0.1783942 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1123 is [False, True, False, False, False, True]
Current timestep = 1124. State = [[0.03348119 0.2761504 ]]. Action = [[-0.1731852  -0.06442326 -0.1203451  -0.49329454]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1124 is [False, True, False, False, False, True]
Current timestep = 1125. State = [[0.02287827 0.2628569 ]]. Action = [[ 0.13445741 -0.09622347 -0.10392034 -0.28130007]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1125 is [False, True, False, False, False, True]
Scene graph at timestep 1125 is [False, True, False, False, False, True]
State prediction error at timestep 1125 is tensor(1.8478e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1125 of 1
Current timestep = 1126. State = [[0.02746033 0.24438605]]. Action = [[ 0.10465574 -0.1350955  -0.15097244  0.00677204]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1126 is [False, True, False, False, False, True]
Current timestep = 1127. State = [[-0.2248507  -0.12860096]]. Action = [[ 0.24887073 -0.03545772 -0.02359244 -0.9543578 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1127 is [False, True, False, False, False, True]
Current timestep = 1128. State = [[-0.20966923 -0.15302327]]. Action = [[ 0.17700756 -0.18835922  0.13052082  0.02992427]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1128 is [True, False, False, True, False, False]
Current timestep = 1129. State = [[-0.19092697 -0.16518   ]]. Action = [[ 0.10645431  0.06225947 -0.1777941   0.26776814]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1129 is [True, False, False, True, False, False]
Scene graph at timestep 1129 is [True, False, False, True, False, False]
State prediction error at timestep 1129 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1129 of 1
Current timestep = 1130. State = [[-0.17944795 -0.17654893]]. Action = [[ 0.00050581 -0.1807834   0.12309629 -0.45900905]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1130 is [True, False, False, True, False, False]
Current timestep = 1131. State = [[-0.17828046 -0.1776556 ]]. Action = [[ 0.01100779  0.16749567 -0.22708933 -0.5897986 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1131 is [True, False, False, True, False, False]
Current timestep = 1132. State = [[-0.17618416 -0.16648746]]. Action = [[0.02760142 0.09815443 0.10838243 0.3390863 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1132 is [True, False, False, True, False, False]
Current timestep = 1133. State = [[-0.1737102  -0.15865341]]. Action = [[-0.00379772  0.00842196  0.06969029  0.61729383]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1133 is [True, False, False, True, False, False]
Scene graph at timestep 1133 is [True, False, False, True, False, False]
State prediction error at timestep 1133 is tensor(6.0099e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1133 of 0
Current timestep = 1134. State = [[-0.16339083 -0.15573584]]. Action = [[ 0.22471943 -0.00300932  0.16572869 -0.93838525]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1134 is [True, False, False, True, False, False]
Current timestep = 1135. State = [[-0.13850798 -0.16516629]]. Action = [[ 0.13262415 -0.17472847  0.09327465 -0.06294721]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1135 is [True, False, False, True, False, False]
Current timestep = 1136. State = [[-0.1140632  -0.17503025]]. Action = [[ 0.22878629 -0.0513819   0.04254657  0.31805146]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1136 is [True, False, False, True, False, False]
Current timestep = 1137. State = [[-0.0906288  -0.17177217]]. Action = [[0.02174562 0.18931729 0.06837609 0.0546484 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1137 is [True, False, False, True, False, False]
Scene graph at timestep 1137 is [True, False, False, True, False, False]
State prediction error at timestep 1137 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1137 of 1
Current timestep = 1138. State = [[-0.08418845 -0.16224182]]. Action = [[-0.04161236 -0.03670764  0.02448875 -0.6866626 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1138 is [True, False, False, True, False, False]
Current timestep = 1139. State = [[-0.08546712 -0.16849913]]. Action = [[-0.0351243  -0.0805921  -0.13427456  0.5662482 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1139 is [True, False, False, True, False, False]
Scene graph at timestep 1139 is [True, False, False, True, False, False]
State prediction error at timestep 1139 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1139 of -1
Current timestep = 1140. State = [[-0.09437489 -0.18624212]]. Action = [[-0.19234698 -0.13781895 -0.12507918 -0.13228929]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1140 is [True, False, False, True, False, False]
Current timestep = 1141. State = [[-0.10064532 -0.19552253]]. Action = [[ 0.06961524 -0.00761721  0.16856033 -0.3429073 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1141 is [True, False, False, True, False, False]
Scene graph at timestep 1141 is [True, False, False, True, False, False]
State prediction error at timestep 1141 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1141 of -1
Current timestep = 1142. State = [[-0.09797545 -0.19973946]]. Action = [[ 0.13218305 -0.07939267  0.09148413  0.3874426 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1142 is [True, False, False, True, False, False]
Current timestep = 1143. State = [[-0.08491035 -0.21462788]]. Action = [[ 0.20113122 -0.20589213  0.05841491  0.07929802]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1143 is [True, False, False, True, False, False]
Current timestep = 1144. State = [[-0.06133416 -0.21944903]]. Action = [[ 0.10913557  0.21139026 -0.1307109   0.9796363 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1144 is [True, False, False, True, False, False]
Current timestep = 1145. State = [[-0.05311174 -0.21336663]]. Action = [[-0.02970365 -0.00326824 -0.17136902  0.81934404]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1145 is [True, False, False, True, False, False]
Current timestep = 1146. State = [[-0.05382553 -0.20438036]]. Action = [[-0.11906505  0.15043765 -0.06237921 -0.29120123]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1146 is [True, False, False, True, False, False]
Current timestep = 1147. State = [[-0.04919048 -0.20417663]]. Action = [[ 0.20346871 -0.20605302 -0.22826818 -0.31544685]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1147 is [True, False, False, True, False, False]
Current timestep = 1148. State = [[-0.03248994 -0.22238165]]. Action = [[ 0.22731745 -0.2026149  -0.22714901  0.3625517 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1148 is [False, True, False, True, False, False]
Scene graph at timestep 1148 is [False, True, False, True, False, False]
State prediction error at timestep 1148 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1148 of 0
Current timestep = 1149. State = [[-0.00053908 -0.25590008]]. Action = [[ 0.15045282 -0.2481641   0.02626622  0.73382473]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1149 is [False, True, False, True, False, False]
Scene graph at timestep 1149 is [False, True, False, True, False, False]
State prediction error at timestep 1149 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1149 of -1
Current timestep = 1150. State = [[ 0.02422289 -0.26796114]]. Action = [[0.11323175 0.18638998 0.09587047 0.9060657 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1150 is [False, True, False, True, False, False]
Scene graph at timestep 1150 is [False, True, False, True, False, False]
State prediction error at timestep 1150 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1150 of 1
Current timestep = 1151. State = [[ 0.03502211 -0.25979346]]. Action = [[ 0.22419947 -0.21418601 -0.12985231 -0.47721517]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1151 is [False, True, False, True, False, False]
Current timestep = 1152. State = [[ 0.0352083  -0.25982463]]. Action = [[0.22155848 0.22602952 0.23972246 0.8475785 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1152 is [False, True, False, True, False, False]
Current timestep = 1153. State = [[ 0.03606369 -0.25236255]]. Action = [[-0.06804103  0.13397658  0.07455835  0.9138882 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1153 is [False, True, False, True, False, False]
Current timestep = 1154. State = [[ 0.03694132 -0.24393094]]. Action = [[ 0.248871    0.16530961 -0.11822084 -0.7348518 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1154 is [False, True, False, True, False, False]
Current timestep = 1155. State = [[ 0.03691439 -0.24279064]]. Action = [[ 0.22040734  0.08562285 -0.2193688  -0.41703045]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1155 is [False, True, False, True, False, False]
Current timestep = 1156. State = [[ 0.03692392 -0.2427143 ]]. Action = [[ 0.24159002 -0.16896614 -0.01909959 -0.46413374]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1156 is [False, True, False, True, False, False]
Current timestep = 1157. State = [[ 0.03693353 -0.24263723]]. Action = [[ 0.22448316  0.15404639  0.01356441 -0.29570556]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1157 is [False, True, False, True, False, False]
Current timestep = 1158. State = [[ 0.03835595 -0.25305718]]. Action = [[ 0.13320363 -0.21593598  0.16580588  0.31576788]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1158 is [False, True, False, True, False, False]
Current timestep = 1159. State = [[ 0.04833588 -0.25101206]]. Action = [[ 0.07185721  0.22866619 -0.0669333  -0.78036165]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1159 is [False, True, False, True, False, False]
Scene graph at timestep 1159 is [False, True, False, True, False, False]
State prediction error at timestep 1159 is tensor(1.4221e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1159 of 0
Current timestep = 1160. State = [[ 0.05539758 -0.23637067]]. Action = [[-0.12062864  0.06151655  0.17494512  0.74087644]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1160 is [False, True, False, True, False, False]
Current timestep = 1161. State = [[ 0.05538884 -0.23365048]]. Action = [[0.1768409  0.07159489 0.02353865 0.06587851]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1161 is [False, False, True, True, False, False]
Current timestep = 1162. State = [[ 0.05532077 -0.2329981 ]]. Action = [[ 0.20818853  0.13977823 -0.23636755  0.08225441]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1162 is [False, False, True, True, False, False]
Current timestep = 1163. State = [[ 0.05526824 -0.2329522 ]]. Action = [[ 0.20025462  0.07253516 -0.21001597 -0.52141684]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1163 is [False, False, True, True, False, False]
Current timestep = 1164. State = [[ 0.05275605 -0.24441485]]. Action = [[ 0.00764176 -0.21347733 -0.01942618 -0.23915386]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1164 is [False, False, True, True, False, False]
Current timestep = 1165. State = [[ 0.05078419 -0.2511896 ]]. Action = [[-0.04893495  0.07301462 -0.21591619 -0.39816344]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1165 is [False, False, True, True, False, False]
Scene graph at timestep 1165 is [False, False, True, True, False, False]
State prediction error at timestep 1165 is tensor(7.9316e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1165 of -1
Current timestep = 1166. State = [[ 0.05041736 -0.25164455]]. Action = [[ 0.19817716 -0.15907678 -0.00871857 -0.64573574]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1166 is [False, False, True, True, False, False]
Current timestep = 1167. State = [[ 0.05041736 -0.25164455]]. Action = [[ 0.23421562  0.19752419 -0.07239059 -0.42681468]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1167 is [False, False, True, True, False, False]
Current timestep = 1168. State = [[ 0.05041736 -0.25164455]]. Action = [[ 0.13833743  0.23848087  0.17511496 -0.05989146]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1168 is [False, False, True, True, False, False]
Scene graph at timestep 1168 is [False, False, True, True, False, False]
State prediction error at timestep 1168 is tensor(9.6815e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1168 of -1
Current timestep = 1169. State = [[ 0.05041736 -0.25164455]]. Action = [[ 0.24700493 -0.01901627  0.24726722  0.7449043 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1169 is [False, False, True, True, False, False]
Current timestep = 1170. State = [[ 0.0469779  -0.25596708]]. Action = [[-0.10657424 -0.05322495 -0.20678234  0.3247192 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1170 is [False, False, True, True, False, False]
Scene graph at timestep 1170 is [False, True, False, True, False, False]
State prediction error at timestep 1170 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1170 of -1
Current timestep = 1171. State = [[ 0.0425652 -0.2669696]]. Action = [[ 0.13046777 -0.13893889  0.18884218 -0.9725601 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1171 is [False, True, False, True, False, False]
Scene graph at timestep 1171 is [False, True, False, True, False, False]
State prediction error at timestep 1171 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1171 of -1
Current timestep = 1172. State = [[ 0.04118519 -0.2635282 ]]. Action = [[-0.18220806  0.20886624  0.17531747 -0.6561175 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1172 is [False, True, False, True, False, False]
Current timestep = 1173. State = [[ 0.03095278 -0.25122288]]. Action = [[-0.24201345  0.08823404  0.09496093 -0.63013375]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1173 is [False, True, False, True, False, False]
Current timestep = 1174. State = [[ 0.01186735 -0.24288467]]. Action = [[ 0.23452032  0.08834022 -0.10822436  0.2541772 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1174 is [False, True, False, True, False, False]
Scene graph at timestep 1174 is [False, True, False, True, False, False]
State prediction error at timestep 1174 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1174 of 1
Current timestep = 1175. State = [[ 0.0096517 -0.2511158]]. Action = [[ 0.10241443 -0.20069583 -0.1867575   0.1573627 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1175 is [False, True, False, True, False, False]
Current timestep = 1176. State = [[ 0.01241868 -0.26153147]]. Action = [[ 0.14903337 -0.05583726  0.08944669 -0.88114893]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1176 is [False, True, False, True, False, False]
Scene graph at timestep 1176 is [False, True, False, True, False, False]
State prediction error at timestep 1176 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1176 of -1
Current timestep = 1177. State = [[ 0.01468096 -0.26444048]]. Action = [[-0.0453198   0.03658888  0.03611773 -0.1837995 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1177 is [False, True, False, True, False, False]
Current timestep = 1178. State = [[ 0.01543108 -0.26580328]]. Action = [[ 0.08926439 -0.0655849  -0.18758754 -0.02526778]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1178 is [False, True, False, True, False, False]
Current timestep = 1179. State = [[ 0.00884698 -0.28202948]]. Action = [[-0.22709408 -0.19580953  0.00898653 -0.19926566]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1179 is [False, True, False, True, False, False]
Current timestep = 1180. State = [[ 0.00737764 -0.2951437 ]]. Action = [[ 0.16805387  0.04856309  0.08885384 -0.7701845 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1180 is [False, True, False, True, False, False]
Current timestep = 1181. State = [[ 0.010659   -0.28473604]]. Action = [[-0.05852038  0.20020393 -0.14242047 -0.667751  ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1181 is [False, True, False, True, False, False]
Current timestep = 1182. State = [[ 0.01257662 -0.27453962]]. Action = [[ 0.20349944 -0.22219646 -0.04906076  0.06384337]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1182 is [False, True, False, True, False, False]
Scene graph at timestep 1182 is [False, True, False, True, False, False]
State prediction error at timestep 1182 is tensor(8.3645e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1182 of -1
Current timestep = 1183. State = [[ 0.01135696 -0.26900923]]. Action = [[-0.1293181   0.08203673 -0.16989632 -0.9827675 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1183 is [False, True, False, True, False, False]
Current timestep = 1184. State = [[ 0.01049478 -0.26909477]]. Action = [[ 0.04970968 -0.11207712  0.05570656  0.3906144 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1184 is [False, True, False, True, False, False]
Scene graph at timestep 1184 is [False, True, False, True, False, False]
State prediction error at timestep 1184 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1184 of -1
Current timestep = 1185. State = [[ 0.00754148 -0.2631791 ]]. Action = [[-0.165577    0.2004146  -0.23852013 -0.33838797]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1185 is [False, True, False, True, False, False]
Scene graph at timestep 1185 is [False, True, False, True, False, False]
State prediction error at timestep 1185 is tensor(6.3667e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1185 of 0
Current timestep = 1186. State = [[-0.00447297 -0.24768475]]. Action = [[-0.08139676  0.06821066 -0.15213612  0.62225676]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1186 is [False, True, False, True, False, False]
Current timestep = 1187. State = [[-0.00306002 -0.2305512 ]]. Action = [[0.24257877 0.18890914 0.16725138 0.97797406]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1187 is [False, True, False, True, False, False]
Current timestep = 1188. State = [[ 0.00296805 -0.20287159]]. Action = [[ 0.05610275  0.12969968 -0.2003293   0.08729112]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1188 is [False, True, False, True, False, False]
Current timestep = 1189. State = [[ 0.01473071 -0.17706147]]. Action = [[ 0.23715287  0.21841478 -0.15417726  0.68352604]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1189 is [False, True, False, True, False, False]
Current timestep = 1190. State = [[ 0.0394148  -0.15924838]]. Action = [[ 0.2463927  -0.03822118 -0.06456134 -0.66499186]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1190 is [False, True, False, True, False, False]
Current timestep = 1191. State = [[ 0.06019194 -0.15566751]]. Action = [[ 0.20243055  0.10582995 -0.07776994 -0.12748551]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1191 is [False, True, False, True, False, False]
Current timestep = 1192. State = [[ 0.06435618 -0.15512404]]. Action = [[ 0.22254449 -0.04166426  0.21659562  0.7406999 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1192 is [False, False, True, True, False, False]
Scene graph at timestep 1192 is [False, False, True, True, False, False]
State prediction error at timestep 1192 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1192 of 0
Current timestep = 1193. State = [[ 0.0634629  -0.15723486]]. Action = [[-0.16471186  0.00041938 -0.11375198 -0.21770471]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1193 is [False, False, True, True, False, False]
Scene graph at timestep 1193 is [False, False, True, True, False, False]
State prediction error at timestep 1193 is tensor(7.0482e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1193 of -1
Current timestep = 1194. State = [[ 0.06196878 -0.160179  ]]. Action = [[ 0.21380675 -0.08136445 -0.07216379  0.01785624]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1194 is [False, False, True, True, False, False]
Current timestep = 1195. State = [[ 0.06187233 -0.1469181 ]]. Action = [[-0.01239461  0.23863012  0.12602577  0.96391916]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1195 is [False, False, True, True, False, False]
Current timestep = 1196. State = [[ 0.06097618 -0.12908462]]. Action = [[ 0.07207114  0.13009864 -0.00598499  0.23214877]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1196 is [False, False, True, True, False, False]
Current timestep = 1197. State = [[ 0.06091331 -0.12686981]]. Action = [[ 0.2481029  -0.09275617 -0.13869126 -0.8673568 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1197 is [False, False, True, True, False, False]
Current timestep = 1198. State = [[ 0.0609062  -0.12663266]]. Action = [[ 0.1572451  -0.2474829   0.16105732  0.8760834 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1198 is [False, False, True, True, False, False]
Current timestep = 1199. State = [[ 0.0609062  -0.12663266]]. Action = [[ 0.12223902 -0.10123357  0.02265963 -0.15043545]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1199 is [False, False, True, True, False, False]
Scene graph at timestep 1199 is [False, False, True, True, False, False]
State prediction error at timestep 1199 is tensor(2.6779e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1199 of 1
Current timestep = 1200. State = [[ 0.0609062  -0.12663266]]. Action = [[ 0.21836647  0.13570565  0.01272368 -0.575138  ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1200 is [False, False, True, True, False, False]
Current timestep = 1201. State = [[ 0.05553672 -0.11990736]]. Action = [[-0.21031141  0.11348313  0.2341643   0.14933777]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1201 is [False, False, True, True, False, False]
Scene graph at timestep 1201 is [False, False, True, False, True, False]
State prediction error at timestep 1201 is tensor(4.7252e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1201 of 1
Current timestep = 1202. State = [[ 0.04345756 -0.10668405]]. Action = [[-0.01868859  0.03975162  0.16046208  0.23769021]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1202 is [False, False, True, False, True, False]
Current timestep = 1203. State = [[ 0.04436627 -0.11280035]]. Action = [[ 0.10001266 -0.1546072  -0.07244939  0.21939385]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1203 is [False, True, False, False, True, False]
Current timestep = 1204. State = [[ 0.0445281  -0.11373642]]. Action = [[-0.03242406  0.09808406  0.2296676  -0.9017999 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1204 is [False, True, False, False, True, False]
Current timestep = 1205. State = [[ 0.04488173 -0.11730647]]. Action = [[ 0.05109525 -0.11349408  0.17264298 -0.9422328 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1205 is [False, True, False, False, True, False]
Current timestep = 1206. State = [[ 0.03785737 -0.12198553]]. Action = [[-0.21110813  0.02159101 -0.2290741  -0.56237674]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1206 is [False, True, False, False, True, False]
Current timestep = 1207. State = [[ 0.03195166 -0.13440333]]. Action = [[ 0.1099706  -0.18725342 -0.07150182  0.5428288 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1207 is [False, True, False, False, True, False]
Current timestep = 1208. State = [[ 0.03406208 -0.14476708]]. Action = [[ 0.07827762 -0.01777449  0.00393984  0.67965674]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1208 is [False, True, False, True, False, False]
Scene graph at timestep 1208 is [False, True, False, True, False, False]
State prediction error at timestep 1208 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1208 of -1
Current timestep = 1209. State = [[ 0.03934106 -0.1539477 ]]. Action = [[ 0.18169978 -0.10333532  0.19605002  0.961702  ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1209 is [False, True, False, True, False, False]
Current timestep = 1210. State = [[ 0.04640293 -0.16056441]]. Action = [[ 0.1728611  -0.11440116 -0.22021037  0.0999521 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1210 is [False, True, False, True, False, False]
Scene graph at timestep 1210 is [False, True, False, True, False, False]
State prediction error at timestep 1210 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1210 of -1
Current timestep = 1211. State = [[ 0.04686001 -0.15550119]]. Action = [[-0.03443307  0.13456258 -0.15082082  0.01578736]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1211 is [False, True, False, True, False, False]
Current timestep = 1212. State = [[ 0.04754053 -0.14194962]]. Action = [[-0.02814274  0.15775228 -0.2298391  -0.82827485]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 1212 is [False, True, False, True, False, False]
Current timestep = 1213. State = [[ 0.04714378 -0.12348256]]. Action = [[-0.03948745  0.08402729  0.19221824  0.9615698 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1213 is [False, True, False, True, False, False]
Current timestep = 1214. State = [[ 0.04687763 -0.11701745]]. Action = [[-0.01138838 -0.02152757  0.21258637 -0.5538324 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1214 is [False, True, False, False, True, False]
Current timestep = 1215. State = [[ 0.04687427 -0.11694009]]. Action = [[ 0.12838274  0.18602175 -0.08157691 -0.5348768 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1215 is [False, True, False, False, True, False]
Current timestep = 1216. State = [[ 0.0405313 -0.1312832]]. Action = [[-0.14882113 -0.23776601  0.11103076 -0.63169193]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1216 is [False, True, False, False, True, False]
Current timestep = 1217. State = [[ 0.03508479 -0.14458542]]. Action = [[ 0.19221538  0.16449338 -0.04477662 -0.09034616]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 1217 is [False, True, False, True, False, False]
Scene graph at timestep 1217 is [False, True, False, True, False, False]
State prediction error at timestep 1217 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1217 of -1
Current timestep = 1218. State = [[ 0.03581476 -0.15311888]]. Action = [[ 0.13666546 -0.1047731  -0.08585927  0.5132854 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1218 is [False, True, False, True, False, False]
Scene graph at timestep 1218 is [False, True, False, True, False, False]
State prediction error at timestep 1218 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1218 of -1
Current timestep = 1219. State = [[ 0.0361473  -0.16026284]]. Action = [[ 0.23141778  0.04371092 -0.15129144  0.3591081 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1219 is [False, True, False, True, False, False]
Scene graph at timestep 1219 is [False, True, False, True, False, False]
State prediction error at timestep 1219 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1219 of -1
Current timestep = 1220. State = [[ 0.0361473  -0.16026284]]. Action = [[ 0.21481872 -0.17984597 -0.06155255 -0.47593892]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 1220 is [False, True, False, True, False, False]
Current timestep = 1221. State = [[ 0.0361473  -0.16026284]]. Action = [[ 0.24671444 -0.06029576 -0.05812369  0.46443462]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1221 is [False, True, False, True, False, False]
Current timestep = 1222. State = [[ 0.03318494 -0.15871848]]. Action = [[-0.22647437  0.0755384  -0.08229071 -0.9448409 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1222 is [False, True, False, True, False, False]
Current timestep = 1223. State = [[ 0.02652914 -0.14813054]]. Action = [[-0.08062305  0.16425228 -0.21875708  0.47295976]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 1223 is [False, True, False, True, False, False]
Scene graph at timestep 1223 is [False, True, False, True, False, False]
State prediction error at timestep 1223 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1223 of 1
Current timestep = 1224. State = [[ 0.01956097 -0.13612854]]. Action = [[ 0.22865939 -0.08558756  0.11424279  0.30353594]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 1224 is [False, True, False, True, False, False]
Current timestep = 1225. State = [[ 0.02552376 -0.1253226 ]]. Action = [[ 0.05915135  0.23565847 -0.08105084  0.70326567]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 1225 is [False, True, False, True, False, False]
Current timestep = 1226. State = [[ 0.02529293 -0.12496126]]. Action = [[-0.15913473 -0.20742154  0.04537335 -0.19388902]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1226 is [False, True, False, True, False, False]
Current timestep = 1227. State = [[ 0.02433587 -0.12513375]]. Action = [[ 0.04354712  0.14037079 -0.16038443 -0.8497632 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1227 is [False, True, False, False, True, False]
Current timestep = 1228. State = [[ 0.01934399 -0.11949232]]. Action = [[-0.20155694  0.03263125  0.17240542 -0.97392213]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1228 is [False, True, False, True, False, False]
Current timestep = 1229. State = [[ 0.00772993 -0.10889563]]. Action = [[-0.17642094  0.12442768 -0.1889883   0.10269892]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1229 is [False, True, False, False, True, False]
Current timestep = 1230. State = [[-0.23555079 -0.21350372]]. Action = [[-0.1407252   0.2368232  -0.00050487  0.18886566]]. Reward = [100.]
Curr episode timestep = 102
Scene graph at timestep 1230 is [False, True, False, False, True, False]
Current timestep = 1231. State = [[-0.22526112 -0.23699465]]. Action = [[0.12948269 0.01843715 0.03655785 0.92066693]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1231 is [True, False, False, True, False, False]
Current timestep = 1232. State = [[-0.21717003 -0.23826237]]. Action = [[-0.0748274   0.04534006 -0.17769201 -0.94215274]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1232 is [True, False, False, True, False, False]
Current timestep = 1233. State = [[-0.21552148 -0.23396741]]. Action = [[ 0.07284513  0.04589143 -0.21165712 -0.96949553]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1233 is [True, False, False, True, False, False]
Current timestep = 1234. State = [[-0.20361213 -0.23541708]]. Action = [[ 0.21906155 -0.11794901  0.16191262  0.3547212 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1234 is [True, False, False, True, False, False]
Current timestep = 1235. State = [[-0.19310656 -0.23376472]]. Action = [[-0.21530008  0.1804727   0.05380219 -0.37934887]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1235 is [True, False, False, True, False, False]
Scene graph at timestep 1235 is [True, False, False, True, False, False]
State prediction error at timestep 1235 is tensor(2.7620e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1235 of 1
Current timestep = 1236. State = [[-0.19522135 -0.22462894]]. Action = [[ 0.03363499  0.03125721 -0.22678801 -0.9781125 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1236 is [True, False, False, True, False, False]
Scene graph at timestep 1236 is [True, False, False, True, False, False]
State prediction error at timestep 1236 is tensor(4.8284e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1236 of 0
Current timestep = 1237. State = [[-0.1889679  -0.22026367]]. Action = [[ 0.17914146 -0.0115142  -0.18230985 -0.5243483 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1237 is [True, False, False, True, False, False]
Current timestep = 1238. State = [[-0.17971778 -0.22337523]]. Action = [[ 0.10596213 -0.12542029 -0.20634325  0.01205373]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1238 is [True, False, False, True, False, False]
Current timestep = 1239. State = [[-0.16162021 -0.22775084]]. Action = [[ 2.3324066e-01  6.3049793e-04  2.7321607e-02 -9.8839027e-01]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1239 is [True, False, False, True, False, False]
Current timestep = 1240. State = [[-0.1422847  -0.24335916]]. Action = [[-0.06112349 -0.18408012  0.1615076  -0.17676044]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1240 is [True, False, False, True, False, False]
Scene graph at timestep 1240 is [True, False, False, True, False, False]
State prediction error at timestep 1240 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1240 of 0
Current timestep = 1241. State = [[-0.13582423 -0.24658096]]. Action = [[ 0.14812773  0.14454967 -0.21161759  0.8611938 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1241 is [True, False, False, True, False, False]
Current timestep = 1242. State = [[-0.12988232 -0.23962693]]. Action = [[-0.22381167  0.10046095 -0.06863075  0.5234623 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1242 is [True, False, False, True, False, False]
Current timestep = 1243. State = [[-0.14147416 -0.22577134]]. Action = [[-0.20322776  0.14995545  0.24782115  0.84545255]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1243 is [True, False, False, True, False, False]
Current timestep = 1244. State = [[-0.16830276 -0.20053563]]. Action = [[-0.23372214  0.21068329  0.04627293 -0.85127187]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1244 is [True, False, False, True, False, False]
Scene graph at timestep 1244 is [True, False, False, True, False, False]
State prediction error at timestep 1244 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1244 of 0
Current timestep = 1245. State = [[-0.19132341 -0.17358053]]. Action = [[ 0.13927868  0.08299127 -0.21649131  0.90180755]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1245 is [True, False, False, True, False, False]
Current timestep = 1246. State = [[-0.19061595 -0.16243362]]. Action = [[-0.11973989  0.08237517 -0.00900538 -0.8601567 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1246 is [True, False, False, True, False, False]
Scene graph at timestep 1246 is [True, False, False, True, False, False]
State prediction error at timestep 1246 is tensor(9.6183e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1246 of 1
Current timestep = 1247. State = [[-0.19244608 -0.15558559]]. Action = [[ 0.00334805 -0.02503207  0.16797113  0.26981664]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1247 is [True, False, False, True, False, False]
Current timestep = 1248. State = [[-0.19512226 -0.14679454]]. Action = [[-0.08105761  0.16799095  0.00168633  0.8471534 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1248 is [True, False, False, True, False, False]
Current timestep = 1249. State = [[-0.20293263 -0.13120833]]. Action = [[-0.05345222  0.06879392  0.12186638 -0.40529418]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1249 is [True, False, False, True, False, False]
