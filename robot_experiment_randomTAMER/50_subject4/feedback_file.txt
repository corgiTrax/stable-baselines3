Current timestep = 0. State = [[-0.22171146  0.00541267]]. Action = [[-0.06814426 -0.18124709 -0.22208488  0.4302361 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Current timestep = 1. State = [[-0.21385531  0.00198933]]. Action = [[ 0.23785892  0.10886368 -0.19513394 -0.0658505 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
Current timestep = 2. State = [[-0.20032653  0.00056087]]. Action = [[-0.0272152  -0.09777817  0.11796829  0.03991604]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
Current timestep = 3. State = [[-0.18807536  0.001482  ]]. Action = [[ 0.23987466  0.09152105  0.18610996 -0.47685552]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0143, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3 of 1
Current timestep = 4. State = [[-0.1650036   0.00221675]]. Action = [[-0.05968414 -0.07776591  0.23938963 -0.395908  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
Current timestep = 5. State = [[-0.16858675 -0.01114003]]. Action = [[-0.11781912 -0.15833494 -0.135357    0.7717104 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
Current timestep = 6. State = [[-0.17417398 -0.01041965]]. Action = [[-0.00956023  0.21504253  0.15955889  0.7496271 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
Current timestep = 7. State = [[-0.18128046  0.0037667 ]]. Action = [[-0.1320599   0.04056519  0.02199471  0.22947109]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.19007048  0.02233974]]. Action = [[-0.05408037  0.20782539  0.09839526  0.60121846]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.20728885  0.03104316]]. Action = [[-0.19491518 -0.15942848 -0.23568285 -0.83021396]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9 of -1
Current timestep = 10. State = [[-0.22629485  0.02003113]]. Action = [[-0.10086176  0.02950558 -0.22601056  0.22289622]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
Current timestep = 11. State = [[-0.24026851  0.03521429]]. Action = [[-0.09640585  0.20737135  0.0089992  -0.04647833]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 11 of -1
Current timestep = 12. State = [[-0.2565483  0.0643404]]. Action = [[-0.03418586  0.17850697  0.02607834  0.7912977 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
Current timestep = 13. State = [[-0.253622    0.08790671]]. Action = [[ 0.23758358  0.21533778 -0.1382918  -0.7454057 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0097, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.245794    0.09640186]]. Action = [[-0.14207827 -0.17509258  0.01236716  0.20718884]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
Current timestep = 15. State = [[-0.25299639  0.10018691]]. Action = [[-0.0956917   0.18279973 -0.19677082 -0.66279024]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
Current timestep = 16. State = [[-0.26205486  0.1149516 ]]. Action = [[-0.00332132  0.05938822 -0.21067776 -0.05010092]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0089, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of -1
Current timestep = 17. State = [[-0.26521716  0.12029466]]. Action = [[-0.23086706  0.00149179 -0.02322541 -0.02148581]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
Current timestep = 18. State = [[-0.26259893  0.13015378]]. Action = [[0.13620815 0.18288982 0.09107476 0.30512595]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
Scene graph at timestep 18 is [True, False, False, False, False, True]
State prediction error at timestep 18 is tensor(0.0083, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 18 of -1
Current timestep = 19. State = [[-0.26093736  0.13821287]]. Action = [[-0.03777803 -0.08280522 -0.04415049  0.86727   ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, False, True]
Current timestep = 20. State = [[-0.26050052  0.13642238]]. Action = [[-0.1022431   0.06631494 -0.19343531 -0.538363  ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, False, True]
Current timestep = 21. State = [[-0.26045218  0.136176  ]]. Action = [[-0.15214975  0.07591939  0.15312642  0.5698302 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, False, True]
Scene graph at timestep 21 is [True, False, False, False, False, True]
State prediction error at timestep 21 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 21 of -1
Current timestep = 22. State = [[-0.2578709   0.14747512]]. Action = [[ 0.1134885   0.22574157 -0.10240674 -0.7781005 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, False, True]
Current timestep = 23. State = [[-0.24980366  0.1594525 ]]. Action = [[-0.20579945  0.19943422  0.19633651  0.89721215]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, False, True]
Current timestep = 24. State = [[-0.24912947  0.15249091]]. Action = [[-0.10404077 -0.2044357  -0.13995034  0.93672657]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, False, True]
Current timestep = 25. State = [[-0.24253705  0.1359454 ]]. Action = [[ 0.20368236 -0.12251917  0.19610903 -0.90133417]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, False, True]
Scene graph at timestep 25 is [True, False, False, False, False, True]
State prediction error at timestep 25 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 25 of 0
Current timestep = 26. State = [[-0.23749523  0.11089137]]. Action = [[-0.1546973  -0.20503898  0.03250128 -0.04823184]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, False, True]
Current timestep = 27. State = [[-0.23611838  0.10580336]]. Action = [[ 0.16679999  0.15904039 -0.13281979 -0.8727645 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
Current timestep = 28. State = [[-0.23602174  0.10496343]]. Action = [[-0.14775746 -0.1434767   0.10245013  0.43458807]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
Current timestep = 29. State = [[-0.23819351  0.09549651]]. Action = [[-0.02690026 -0.06589389  0.00266433  0.42530966]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
Scene graph at timestep 29 is [True, False, False, False, True, False]
State prediction error at timestep 29 is tensor(8.1231e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 29 of 0
Current timestep = 30. State = [[-0.24012104  0.07733367]]. Action = [[ 0.05940422 -0.18101366  0.12112951 -0.88700265]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 30 of 0
Current timestep = 31. State = [[-0.23069674  0.06934709]]. Action = [[0.23771593 0.16944826 0.11729336 0.1023339 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
Current timestep = 32. State = [[-0.22574928  0.076842  ]]. Action = [[-0.22740173 -0.03215286 -0.06257968  0.544557  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
Scene graph at timestep 32 is [True, False, False, False, True, False]
State prediction error at timestep 32 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 32 of -1
Current timestep = 33. State = [[-0.23521657  0.08846816]]. Action = [[-0.11245029  0.16671348 -0.10644934 -0.89725226]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
Current timestep = 34. State = [[-0.24108282  0.09777355]]. Action = [[ 0.05970022 -0.06551726  0.16824698 -0.21658784]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.24172293  0.0900261 ]]. Action = [[-0.06835361 -0.10562743 -0.16716932  0.8656182 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
Current timestep = 36. State = [[-0.24559602  0.08086533]]. Action = [[-0.04971576 -0.05223559 -0.1807589  -0.8322392 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
Current timestep = 37. State = [[-0.2564593  0.0706181]]. Action = [[-0.11427513 -0.08076517  0.20705533  0.71229064]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
Current timestep = 38. State = [[-0.25975782  0.05420287]]. Action = [[ 0.17710793 -0.14275089 -0.2234109   0.6119797 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 38 of -1
Current timestep = 39. State = [[-0.2513513   0.04303061]]. Action = [[ 0.08073604  0.03221759 -0.06185481 -0.48797333]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
Scene graph at timestep 39 is [True, False, False, False, True, False]
State prediction error at timestep 39 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 39 of -1
Current timestep = 40. State = [[-0.25033343  0.04336168]]. Action = [[-0.17669813  0.02634937  0.09299445  0.8437247 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[-0.25033343  0.04336168]]. Action = [[-0.1656732  -0.15461726  0.17605782  0.36570668]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
Current timestep = 42. State = [[-0.24369758  0.04717932]]. Action = [[0.14288479 0.08165577 0.13361952 0.56300175]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.23209965  0.03941116]]. Action = [[-0.07662845 -0.23178323  0.1130105   0.6784966 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
Current timestep = 44. State = [[-0.22887957  0.03433951]]. Action = [[0.11692894 0.1627549  0.16917795 0.03896821]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 44 of -1
Current timestep = 45. State = [[-0.22718035  0.05342673]]. Action = [[ 0.00619504  0.19401461 -0.02610874  0.7458601 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
Current timestep = 46. State = [[-0.21439955  0.06015616]]. Action = [[ 0.21908355 -0.08060029 -0.02149804  0.67581844]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
Current timestep = 47. State = [[-0.18979816  0.0716765 ]]. Action = [[ 0.1664283   0.20906323 -0.02876943 -0.01679301]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(4.4091e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.16284171  0.09126784]]. Action = [[ 0.19249904  0.09326082 -0.07096297  0.6963208 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
Current timestep = 49. State = [[-0.14056276  0.09003679]]. Action = [[ 0.08355778 -0.15557235  0.20046675 -0.3949803 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
Current timestep = 50. State = [[-0.12425915  0.06999718]]. Action = [[ 0.11704093 -0.22519839  0.09911585  0.7310473 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
Current timestep = 51. State = [[-0.1214839   0.06087907]]. Action = [[-0.16623992  0.14087498 -0.18469024 -0.8691681 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 51 of 1
Current timestep = 52. State = [[-0.12044346  0.05431709]]. Action = [[ 0.15196064 -0.20407134 -0.02205995  0.67831135]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.11463749  0.03640243]]. Action = [[-0.04952326 -0.07229862 -0.08077312  0.740515  ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
Current timestep = 54. State = [[-0.10573285  0.04323744]]. Action = [[ 0.23425102  0.1989159  -0.22784913  0.7462503 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
Scene graph at timestep 54 is [True, False, False, False, True, False]
State prediction error at timestep 54 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 54 of 1
Current timestep = 55. State = [[-0.08060151  0.04144011]]. Action = [[ 0.15720323 -0.21763608  0.15346622  0.48900282]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
Current timestep = 56. State = [[-0.06774536  0.03255228]]. Action = [[0.02187228 0.07438624 0.06278849 0.5264332 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
Current timestep = 57. State = [[-0.05941893  0.02235481]]. Action = [[ 0.05893859 -0.22379795 -0.0088158  -0.08130765]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
Current timestep = 58. State = [[-0.04773717  0.01665159]]. Action = [[ 0.16438311  0.15556741  0.05276322 -0.19748843]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
Scene graph at timestep 58 is [False, True, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0118, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 58 of 1
Current timestep = 59. State = [[-0.20999385 -0.153953  ]]. Action = [[ 0.10540569 -0.12438187  0.21032912 -0.80983806]]. Reward = [100.]
Curr episode timestep = 59
Scene graph at timestep 59 is [False, True, False, False, True, False]
Current timestep = 60. State = [[-0.20333995 -0.17095424]]. Action = [[-0.11235759  0.00542235 -0.15784858 -0.09359407]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 60 is [True, False, False, True, False, False]
Current timestep = 61. State = [[-0.20866589 -0.17539705]]. Action = [[-0.08256204 -0.02484094  0.18989754 -0.03418827]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 61 is [True, False, False, True, False, False]
Scene graph at timestep 61 is [True, False, False, True, False, False]
State prediction error at timestep 61 is tensor(0.0270, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 61 of -1
Current timestep = 62. State = [[-0.22584902 -0.18547776]]. Action = [[-0.23905154 -0.06102882  0.08203191  0.8042958 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 62 is [True, False, False, True, False, False]
Scene graph at timestep 62 is [True, False, False, True, False, False]
State prediction error at timestep 62 is tensor(0.0297, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.24946782 -0.18274511]]. Action = [[-0.07757786  0.16605842 -0.04327145 -0.7063879 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 63 is [True, False, False, True, False, False]
Current timestep = 64. State = [[-0.25261098 -0.17317666]]. Action = [[-0.15943813  0.07742807 -0.0642591   0.6266155 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 64 is [True, False, False, True, False, False]
Current timestep = 65. State = [[-0.25301775 -0.17202729]]. Action = [[-0.14013778  0.23015389 -0.17084734  0.7924175 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 65 is [True, False, False, True, False, False]
Scene graph at timestep 65 is [True, False, False, True, False, False]
State prediction error at timestep 65 is tensor(0.0291, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 65 of -1
Current timestep = 66. State = [[-0.24956933 -0.1729667 ]]. Action = [[ 0.15266234 -0.07287008  0.1736573  -0.9561905 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 66 is [True, False, False, True, False, False]
Scene graph at timestep 66 is [True, False, False, True, False, False]
State prediction error at timestep 66 is tensor(0.0301, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 66 of -1
Current timestep = 67. State = [[-0.23699363 -0.16735397]]. Action = [[ 0.21368772  0.11782914  0.05238292 -0.8506642 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 67 is [True, False, False, True, False, False]
Current timestep = 68. State = [[-0.21468818 -0.17023532]]. Action = [[ 0.22389925 -0.18257394  0.14677542 -0.52326155]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 68 is [True, False, False, True, False, False]
Scene graph at timestep 68 is [True, False, False, True, False, False]
State prediction error at timestep 68 is tensor(0.0245, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 68 of 1
Current timestep = 69. State = [[-0.19526398 -0.18897934]]. Action = [[-0.10822389 -0.13900971  0.23833889  0.68681014]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 69 is [True, False, False, True, False, False]
Scene graph at timestep 69 is [True, False, False, True, False, False]
State prediction error at timestep 69 is tensor(0.0269, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of -1
Current timestep = 70. State = [[-0.19798869 -0.20081277]]. Action = [[ 0.03767073 -0.01572897  0.01641381  0.70195925]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 70 is [True, False, False, True, False, False]
Scene graph at timestep 70 is [True, False, False, True, False, False]
State prediction error at timestep 70 is tensor(0.0285, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of -1
Current timestep = 71. State = [[-0.20273447 -0.20010135]]. Action = [[-0.17976393  0.08161426 -0.10170889  0.08653307]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 71 is [True, False, False, True, False, False]
Current timestep = 72. State = [[-0.22255413 -0.211126  ]]. Action = [[-0.24152055 -0.19406416 -0.18325941 -0.915981  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 72 is [True, False, False, True, False, False]
Current timestep = 73. State = [[-0.24887997 -0.23063062]]. Action = [[-0.19914117 -0.07282886  0.11057463 -0.04950219]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 73 is [True, False, False, True, False, False]
Scene graph at timestep 73 is [True, False, False, True, False, False]
State prediction error at timestep 73 is tensor(0.0383, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-0.26425195 -0.22717682]]. Action = [[ 0.17215347  0.20574987 -0.23019196 -0.55977005]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 74 is [True, False, False, True, False, False]
Scene graph at timestep 74 is [True, False, False, True, False, False]
State prediction error at timestep 74 is tensor(0.0416, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 74 of 1
Current timestep = 75. State = [[-0.24965605 -0.21339093]]. Action = [[ 0.18979955 -0.06970659  0.09427369 -0.37656593]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 75 is [True, False, False, True, False, False]
Current timestep = 76. State = [[-0.24316399 -0.22405417]]. Action = [[-0.07888404 -0.14625673 -0.15176004 -0.01353419]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 76 is [True, False, False, True, False, False]
Current timestep = 77. State = [[-0.23876262 -0.2325325 ]]. Action = [[ 0.11526436 -0.0171424   0.10245106 -0.68001026]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 77 is [True, False, False, True, False, False]
Scene graph at timestep 77 is [True, False, False, True, False, False]
State prediction error at timestep 77 is tensor(0.0357, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 77 of -1
Current timestep = 78. State = [[-0.23696603 -0.24725401]]. Action = [[-0.08320349 -0.19894156 -0.08854151  0.4090407 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 78 is [True, False, False, True, False, False]
Current timestep = 79. State = [[-0.23994659 -0.26269934]]. Action = [[-0.23160014  0.08268014  0.07181296 -0.28463423]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 79 is [True, False, False, True, False, False]
Current timestep = 80. State = [[-0.23659436 -0.2571215 ]]. Action = [[0.07242978 0.15345669 0.16327924 0.5223726 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 80 is [True, False, False, True, False, False]
Scene graph at timestep 80 is [True, False, False, True, False, False]
State prediction error at timestep 80 is tensor(0.0347, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 80 of 1
Current timestep = 81. State = [[-0.21928613 -0.24711989]]. Action = [[ 0.2411645  -0.01973446 -0.03452504  0.6658797 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 81 is [True, False, False, True, False, False]
Current timestep = 82. State = [[-0.19539666 -0.25228682]]. Action = [[ 0.14502877 -0.09257188 -0.20099194  0.5844284 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 82 is [True, False, False, True, False, False]
Current timestep = 83. State = [[-0.18448956 -0.27237996]]. Action = [[-0.04491863 -0.24432456  0.1841796  -0.6243257 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 83 is [True, False, False, True, False, False]
Current timestep = 84. State = [[-0.1891381 -0.2819517]]. Action = [[-0.17832282  0.18575746 -0.2028229  -0.30875152]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 84 is [True, False, False, True, False, False]
Scene graph at timestep 84 is [True, False, False, True, False, False]
State prediction error at timestep 84 is tensor(0.0388, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of -1
Current timestep = 85. State = [[-0.19312987 -0.28509647]]. Action = [[ 0.05843017 -0.14637728  0.00765672  0.16904438]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 85 is [True, False, False, True, False, False]
Scene graph at timestep 85 is [True, False, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0343, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 85 of -1
Current timestep = 86. State = [[-0.19546355 -0.28945872]]. Action = [[ 0.23630583 -0.16025801 -0.00460649 -0.8454213 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 86 is [True, False, False, True, False, False]
Scene graph at timestep 86 is [True, False, False, True, False, False]
State prediction error at timestep 86 is tensor(0.0361, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 86 of -1
Current timestep = 87. State = [[-0.19564064 -0.28388417]]. Action = [[-0.09680896  0.1695945   0.05907425  0.56372714]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 87 is [True, False, False, True, False, False]
Scene graph at timestep 87 is [True, False, False, True, False, False]
State prediction error at timestep 87 is tensor(0.0330, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 87 of 1
Current timestep = 88. State = [[-0.19805972 -0.27364317]]. Action = [[ 0.02721646  0.05703169 -0.21343677 -0.28857088]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 88 is [True, False, False, True, False, False]
Scene graph at timestep 88 is [True, False, False, True, False, False]
State prediction error at timestep 88 is tensor(0.0327, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.18811244 -0.26588446]]. Action = [[ 0.24053556 -0.009444    0.02166072  0.92860854]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 89 is [True, False, False, True, False, False]
Scene graph at timestep 89 is [True, False, False, True, False, False]
State prediction error at timestep 89 is tensor(0.0250, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 89 of 1
Current timestep = 90. State = [[-0.17387088 -0.25978485]]. Action = [[ 0.10456786  0.01678687  0.23134309 -0.7322976 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 90 is [True, False, False, True, False, False]
Current timestep = 91. State = [[-0.1710434  -0.25005665]]. Action = [[-0.17358316  0.18961835  0.15234339 -0.16428769]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 91 is [True, False, False, True, False, False]
Current timestep = 92. State = [[-0.1659402 -0.2444208]]. Action = [[ 0.2430414  -0.13470438  0.09402078  0.24214816]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 92 is [True, False, False, True, False, False]
Current timestep = 93. State = [[-0.15648524 -0.25672817]]. Action = [[-0.0184626  -0.14692071  0.22954047 -0.49227297]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 93 is [True, False, False, True, False, False]
Current timestep = 94. State = [[-0.14964971 -0.258072  ]]. Action = [[0.14072186 0.12288937 0.24341947 0.7944522 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 94 is [True, False, False, True, False, False]
Current timestep = 95. State = [[-0.1431778  -0.26643363]]. Action = [[-0.07048154 -0.22521453  0.15010458  0.7272401 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 95 is [True, False, False, True, False, False]
Scene graph at timestep 95 is [True, False, False, True, False, False]
State prediction error at timestep 95 is tensor(0.0255, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 95 of -1
Current timestep = 96. State = [[-0.14460637 -0.2798818 ]]. Action = [[-0.15400746 -0.19975336 -0.06399845  0.47974062]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 96 is [True, False, False, True, False, False]
Current timestep = 97. State = [[-0.14030705 -0.27013883]]. Action = [[ 0.07269904  0.19389075  0.12020141 -0.7229286 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 97 is [True, False, False, True, False, False]
Scene graph at timestep 97 is [True, False, False, True, False, False]
State prediction error at timestep 97 is tensor(0.0274, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 97 of 1
Current timestep = 98. State = [[-0.13399641 -0.25094715]]. Action = [[-0.04530448  0.14865756  0.19046894  0.84921193]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 98 is [True, False, False, True, False, False]
Current timestep = 99. State = [[-0.12711734 -0.2474159 ]]. Action = [[ 0.19846964 -0.12406179 -0.0394752   0.8028661 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 99 is [True, False, False, True, False, False]
Current timestep = 100. State = [[-0.11113416 -0.25240752]]. Action = [[ 0.10492107 -0.01309751  0.07188502 -0.6330008 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 100 is [True, False, False, True, False, False]
Current timestep = 101. State = [[-0.09209147 -0.2500362 ]]. Action = [[0.15205026 0.0850305  0.09742475 0.72530246]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 101 is [True, False, False, True, False, False]
Scene graph at timestep 101 is [True, False, False, True, False, False]
State prediction error at timestep 101 is tensor(0.0199, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 101 of 1
Current timestep = 102. State = [[-0.06873932 -0.25334597]]. Action = [[ 0.17524332 -0.14847343 -0.01412198  0.20123303]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 102 is [True, False, False, True, False, False]
Scene graph at timestep 102 is [True, False, False, True, False, False]
State prediction error at timestep 102 is tensor(0.0207, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 102 of 1
Current timestep = 103. State = [[-0.05231408 -0.26796827]]. Action = [[-0.13276173 -0.0521853  -0.04548667 -0.8930715 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 103 is [True, False, False, True, False, False]
Scene graph at timestep 103 is [True, False, False, True, False, False]
State prediction error at timestep 103 is tensor(0.0289, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 103 of -1
Current timestep = 104. State = [[-0.05688119 -0.28281888]]. Action = [[ 0.09813422 -0.180605   -0.19369127  0.26892507]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 104 is [True, False, False, True, False, False]
Current timestep = 105. State = [[-0.05806032 -0.2869039 ]]. Action = [[-0.11167458  0.14916062  0.23535264 -0.1687898 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 105 is [True, False, False, True, False, False]
Scene graph at timestep 105 is [True, False, False, True, False, False]
State prediction error at timestep 105 is tensor(0.0285, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 105 of -1
Current timestep = 106. State = [[-0.05988234 -0.27498716]]. Action = [[-0.08837965  0.17876089 -0.15258746  0.63145006]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 106 is [True, False, False, True, False, False]
Current timestep = 107. State = [[-0.06757846 -0.2631911 ]]. Action = [[-0.1776493   0.02756426  0.04309568 -0.3587216 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 107 is [True, False, False, True, False, False]
Scene graph at timestep 107 is [True, False, False, True, False, False]
State prediction error at timestep 107 is tensor(0.0228, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 107 of -1
Current timestep = 108. State = [[-0.07763004 -0.24667597]]. Action = [[ 0.1574378   0.1732952  -0.18633598  0.13772416]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 108 is [True, False, False, True, False, False]
Scene graph at timestep 108 is [True, False, False, True, False, False]
State prediction error at timestep 108 is tensor(0.0141, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of 1
Current timestep = 109. State = [[-0.07345804 -0.22294267]]. Action = [[-0.04212612  0.10857022  0.19935435 -0.885976  ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 109 is [True, False, False, True, False, False]
Scene graph at timestep 109 is [True, False, False, True, False, False]
State prediction error at timestep 109 is tensor(0.0168, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 109 of 1
Current timestep = 110. State = [[-0.0845895  -0.22737631]]. Action = [[-0.24425952 -0.19676685  0.2428239   0.33479595]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 110 is [True, False, False, True, False, False]
Current timestep = 111. State = [[-0.10820731 -0.24892668]]. Action = [[-0.20096834 -0.1162011   0.11971793 -0.85228616]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 111 is [True, False, False, True, False, False]
Scene graph at timestep 111 is [True, False, False, True, False, False]
State prediction error at timestep 111 is tensor(0.0165, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 111 of -1
Current timestep = 112. State = [[-0.12680545 -0.267977  ]]. Action = [[ 0.07250839 -0.15822205  0.23938888  0.6192142 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 112 is [True, False, False, True, False, False]
Scene graph at timestep 112 is [True, False, False, True, False, False]
State prediction error at timestep 112 is tensor(0.0152, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 112 of -1
Current timestep = 113. State = [[-0.12089732 -0.2786831 ]]. Action = [[ 0.2026223  -0.05630164 -0.20384221 -0.36723053]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 113 is [True, False, False, True, False, False]
Scene graph at timestep 113 is [True, False, False, True, False, False]
State prediction error at timestep 113 is tensor(0.0175, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.11081998 -0.28296718]]. Action = [[ 0.1035538  -0.06438482  0.23543274 -0.0396409 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 114 is [True, False, False, True, False, False]
Current timestep = 115. State = [[-0.10108694 -0.2879544 ]]. Action = [[ 0.03084388 -0.20240775  0.04301974 -0.6761434 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 115 is [True, False, False, True, False, False]
Current timestep = 116. State = [[-0.10091476 -0.28868437]]. Action = [[ 0.1792208  -0.19516157  0.13014385  0.9750266 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 116 is [True, False, False, True, False, False]
Scene graph at timestep 116 is [True, False, False, True, False, False]
State prediction error at timestep 116 is tensor(0.0198, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 116 of -1
Current timestep = 117. State = [[-0.10044992 -0.2888313 ]]. Action = [[-0.18656379 -0.1871583  -0.09953955 -0.00521111]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 117 is [True, False, False, True, False, False]
Current timestep = 118. State = [[-0.10145641 -0.2953867 ]]. Action = [[-0.00956653 -0.09735107  0.11886466 -0.8501213 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 118 is [True, False, False, True, False, False]
Current timestep = 119. State = [[-0.09419956 -0.29417282]]. Action = [[0.15302086 0.1359722  0.18794581 0.52664363]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 119 is [True, False, False, True, False, False]
Scene graph at timestep 119 is [True, False, False, True, False, False]
State prediction error at timestep 119 is tensor(0.0142, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of -1
Current timestep = 120. State = [[-0.08919866 -0.2923466 ]]. Action = [[-0.24010304 -0.06125075  0.10306752  0.2355007 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 120 is [True, False, False, True, False, False]
Scene graph at timestep 120 is [True, False, False, True, False, False]
State prediction error at timestep 120 is tensor(0.0141, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 120 of -1
Current timestep = 121. State = [[-0.09240522 -0.29921404]]. Action = [[ 0.20188162 -0.02082281  0.07039595  0.9675214 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 121 is [True, False, False, True, False, False]
Scene graph at timestep 121 is [True, False, False, True, False, False]
State prediction error at timestep 121 is tensor(0.0178, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 121 of -1
Current timestep = 122. State = [[-0.08838566 -0.2980614 ]]. Action = [[-0.12548095 -0.23497765 -0.06987539 -0.901466  ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 122 is [True, False, False, True, False, False]
Scene graph at timestep 122 is [True, False, False, True, False, False]
State prediction error at timestep 122 is tensor(0.0172, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 122 of -1
Current timestep = 123. State = [[-0.07500059 -0.28860918]]. Action = [[0.24673885 0.1550084  0.17915958 0.5028262 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 123 is [True, False, False, True, False, False]
Scene graph at timestep 123 is [True, False, False, True, False, False]
State prediction error at timestep 123 is tensor(0.0111, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of -1
Current timestep = 124. State = [[-0.05444106 -0.28352213]]. Action = [[-0.15667029 -0.08298607 -0.04342183  0.16825831]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 124 is [True, False, False, True, False, False]
Current timestep = 125. State = [[-0.05166328 -0.27779454]]. Action = [[ 0.18596673  0.15718782 -0.10865079 -0.65127534]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 125 is [True, False, False, True, False, False]
Current timestep = 126. State = [[-0.04134506 -0.26472795]]. Action = [[ 0.14681911  0.05941021  0.21353221 -0.6132191 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 126 is [True, False, False, True, False, False]
Scene graph at timestep 126 is [False, True, False, True, False, False]
State prediction error at timestep 126 is tensor(0.0129, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of -1
Current timestep = 127. State = [[-0.01663987 -0.24790908]]. Action = [[ 0.14973861  0.19796273  0.16343042 -0.9981382 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 127 is [False, True, False, True, False, False]
Current timestep = 128. State = [[-0.00944089 -0.23008668]]. Action = [[-0.03412524  0.04908612  0.05031317  0.09119081]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 128 is [False, True, False, True, False, False]
Scene graph at timestep 128 is [False, True, False, True, False, False]
State prediction error at timestep 128 is tensor(0.0112, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 128 of 1
Current timestep = 129. State = [[-0.00838872 -0.21441302]]. Action = [[-0.08272889  0.18544656 -0.18345258  0.9187653 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 129 is [False, True, False, True, False, False]
Current timestep = 130. State = [[-0.00771963 -0.19629884]]. Action = [[-0.05646138  0.07828194  0.08574763  0.6570468 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 130 is [False, True, False, True, False, False]
Current timestep = 131. State = [[-0.00804636 -0.19113323]]. Action = [[ 0.09714222 -0.05732852  0.17104554  0.766008  ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 131 is [False, True, False, True, False, False]
Current timestep = 132. State = [[-0.00035342 -0.19474185]]. Action = [[ 0.22641021 -0.08383764 -0.08052993  0.49454284]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 132 is [False, True, False, True, False, False]
Current timestep = 133. State = [[ 0.02467856 -0.20312953]]. Action = [[ 0.2040107  -0.09502509  0.17676324 -0.31044102]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 133 is [False, True, False, True, False, False]
Scene graph at timestep 133 is [False, True, False, True, False, False]
State prediction error at timestep 133 is tensor(0.0140, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 133 of -1
Current timestep = 134. State = [[ 0.0521904  -0.21303855]]. Action = [[ 0.23150855 -0.00374463  0.17866334  0.00698721]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 134 is [False, True, False, True, False, False]
Current timestep = 135. State = [[ 0.0529768  -0.20700215]]. Action = [[ 0.01697963  0.12210816 -0.06479275 -0.4675461 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 135 is [False, False, True, True, False, False]
Current timestep = 136. State = [[ 0.05402438 -0.19778083]]. Action = [[-0.15430942  0.09792483  0.04644382 -0.9501833 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 136 is [False, False, True, True, False, False]
Scene graph at timestep 136 is [False, False, True, True, False, False]
State prediction error at timestep 136 is tensor(0.0182, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[ 0.05131153 -0.19527437]]. Action = [[ 0.05030721 -0.09640811  0.1782226  -0.9668367 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 137 is [False, False, True, True, False, False]
Scene graph at timestep 137 is [False, False, True, True, False, False]
State prediction error at timestep 137 is tensor(0.0188, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 137 of -1
Current timestep = 138. State = [[ 0.05036106 -0.20174497]]. Action = [[ 0.04502732 -0.05794498  0.19661772  0.54429376]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 138 is [False, False, True, True, False, False]
Current timestep = 139. State = [[ 0.05073372 -0.19856246]]. Action = [[-0.00424673  0.11214396 -0.05317648 -0.22861093]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 139 is [False, False, True, True, False, False]
Current timestep = 140. State = [[ 0.05088845 -0.19591427]]. Action = [[ 0.18642554 -0.07012838  0.1737858   0.507128  ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 140 is [False, False, True, True, False, False]
Current timestep = 141. State = [[ 0.05016726 -0.20468697]]. Action = [[-0.03968388 -0.17045018  0.19344482 -0.89220154]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 141 is [False, False, True, True, False, False]
Scene graph at timestep 141 is [False, False, True, True, False, False]
State prediction error at timestep 141 is tensor(0.0185, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of -1
Current timestep = 142. State = [[ 0.04929149 -0.21392271]]. Action = [[ 0.03319371  0.00466514  0.22331268 -0.91322964]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 142 is [False, False, True, True, False, False]
Current timestep = 143. State = [[ 0.04929149 -0.21392271]]. Action = [[ 0.20777899  0.14362586 -0.11888111  0.50608766]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 143 is [False, True, False, True, False, False]
Current timestep = 144. State = [[ 0.04929149 -0.21392271]]. Action = [[ 0.18005383 -0.16868113 -0.11811392 -0.02257705]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 144 is [False, True, False, True, False, False]
Current timestep = 145. State = [[ 0.04603137 -0.22590205]]. Action = [[-0.1472131  -0.16469656  0.00187308 -0.5870818 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 145 is [False, True, False, True, False, False]
Scene graph at timestep 145 is [False, True, False, True, False, False]
State prediction error at timestep 145 is tensor(0.0140, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[ 0.04271943 -0.23853911]]. Action = [[ 0.22652096 -0.00864266  0.11865765 -0.7809923 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 146 is [False, True, False, True, False, False]
Scene graph at timestep 146 is [False, True, False, True, False, False]
State prediction error at timestep 146 is tensor(0.0133, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 146 of -1
Current timestep = 147. State = [[ 0.04169229 -0.24418998]]. Action = [[-0.0089578  -0.07807769  0.19055769  0.17498076]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 147 is [False, True, False, True, False, False]
Current timestep = 148. State = [[ 0.0414352  -0.24576594]]. Action = [[ 0.08877528  0.0759407  -0.03199062 -0.7731919 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 148 is [False, True, False, True, False, False]
Current timestep = 149. State = [[ 0.03722004 -0.25387576]]. Action = [[-0.19738625 -0.14441173  0.07702577 -0.4780656 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 149 is [False, True, False, True, False, False]
Scene graph at timestep 149 is [False, True, False, True, False, False]
State prediction error at timestep 149 is tensor(0.0095, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 149 of -1
Current timestep = 150. State = [[ 0.02238357 -0.27318713]]. Action = [[-0.17367353 -0.13844338 -0.19178125 -0.3938725 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 150 is [False, True, False, True, False, False]
Scene graph at timestep 150 is [False, True, False, True, False, False]
State prediction error at timestep 150 is tensor(0.0072, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 150 of -1
Current timestep = 151. State = [[ 0.00289872 -0.29439694]]. Action = [[-0.13642246 -0.11641113  0.00101733  0.4275875 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 151 is [False, True, False, True, False, False]
Scene graph at timestep 151 is [False, True, False, True, False, False]
State prediction error at timestep 151 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 151 of -1
Current timestep = 152. State = [[-0.00449286 -0.3029718 ]]. Action = [[ 0.19762045 -0.03442392 -0.02764475 -0.62804806]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 152 is [False, True, False, True, False, False]
Current timestep = 153. State = [[-0.00196594 -0.30209115]]. Action = [[-0.21726075 -0.1615923  -0.15516213  0.61913335]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 153 is [False, True, False, True, False, False]
Current timestep = 154. State = [[-0.00060714 -0.29414847]]. Action = [[-0.11832082  0.19956708 -0.18992004  0.5121937 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 154 is [False, True, False, True, False, False]
Scene graph at timestep 154 is [False, True, False, True, False, False]
State prediction error at timestep 154 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of 1
Current timestep = 155. State = [[ 0.00459777 -0.27120653]]. Action = [[ 0.10079178  0.23502448  0.06437254 -0.11308527]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 155 is [False, True, False, True, False, False]
Current timestep = 156. State = [[ 0.01278588 -0.25928697]]. Action = [[ 0.22696638 -0.18783146  0.17688274 -0.9236271 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 156 is [False, True, False, True, False, False]
Scene graph at timestep 156 is [False, True, False, True, False, False]
State prediction error at timestep 156 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of 1
Current timestep = 157. State = [[ 0.02456306 -0.2525284 ]]. Action = [[ 0.01994854  0.23679769 -0.0521085  -0.631135  ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 157 is [False, True, False, True, False, False]
Scene graph at timestep 157 is [False, True, False, True, False, False]
State prediction error at timestep 157 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 157 of 1
Current timestep = 158. State = [[ 0.02130401 -0.25164184]]. Action = [[-0.1907649  -0.24929993  0.2138285  -0.6533145 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 158 is [False, True, False, True, False, False]
Current timestep = 159. State = [[ 0.01474762 -0.26005965]]. Action = [[-0.13335557  0.15042374  0.10762975 -0.11346531]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 159 is [False, True, False, True, False, False]
Current timestep = 160. State = [[ 0.00670623 -0.2535644 ]]. Action = [[-0.08617941  0.06714138 -0.12590218 -0.6896455 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 160 is [False, True, False, True, False, False]
Scene graph at timestep 160 is [False, True, False, True, False, False]
State prediction error at timestep 160 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of -1
Current timestep = 161. State = [[-0.00518538 -0.25574088]]. Action = [[-0.04157811 -0.15333453 -0.00398795  0.54760647]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 161 is [False, True, False, True, False, False]
Scene graph at timestep 161 is [False, True, False, True, False, False]
State prediction error at timestep 161 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.01810389 -0.25409   ]]. Action = [[-0.16201979  0.19806775 -0.00787842  0.01655376]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 162 is [False, True, False, True, False, False]
Current timestep = 163. State = [[-0.0258999  -0.24164987]]. Action = [[ 0.15793997 -0.01160876 -0.1097264  -0.38367873]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 163 is [False, True, False, True, False, False]
Current timestep = 164. State = [[-0.03129759 -0.25039035]]. Action = [[-0.18702027 -0.16852598 -0.16867034 -0.9245157 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 164 is [False, True, False, True, False, False]
Current timestep = 165. State = [[-0.04661411 -0.26532537]]. Action = [[-0.2053161  -0.06799728 -0.07323809  0.3668084 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 165 is [False, True, False, True, False, False]
Scene graph at timestep 165 is [False, True, False, True, False, False]
State prediction error at timestep 165 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.06909908 -0.27020654]]. Action = [[ 0.09831992  0.0580911  -0.15545052 -0.33599257]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 166 is [False, True, False, True, False, False]
Current timestep = 167. State = [[-0.06582665 -0.26551598]]. Action = [[ 0.09612292  0.00743675  0.1915555  -0.36764395]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 167 is [True, False, False, True, False, False]
Current timestep = 168. State = [[-0.06606105 -0.253054  ]]. Action = [[-0.19714092  0.20988035  0.23915938 -0.8356895 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 168 is [True, False, False, True, False, False]
Scene graph at timestep 168 is [True, False, False, True, False, False]
State prediction error at timestep 168 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of 1
Current timestep = 169. State = [[-0.06537659 -0.22690025]]. Action = [[ 0.0645431   0.1540213  -0.24322197 -0.68134135]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 169 is [True, False, False, True, False, False]
Current timestep = 170. State = [[-0.06692538 -0.20404977]]. Action = [[-0.15945849  0.19482374  0.15833357  0.23052633]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 170 is [True, False, False, True, False, False]
Current timestep = 171. State = [[-0.08608646 -0.17759356]]. Action = [[-0.088981    0.12852001 -0.19398686  0.72029746]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 171 is [True, False, False, True, False, False]
Scene graph at timestep 171 is [True, False, False, True, False, False]
State prediction error at timestep 171 is tensor(6.0232e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 171 of -1
Current timestep = 172. State = [[-0.10366884 -0.15760313]]. Action = [[-0.12446837  0.09185398 -0.00925592 -0.00066727]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 172 is [True, False, False, True, False, False]
Current timestep = 173. State = [[-0.11693958 -0.1490771 ]]. Action = [[ 0.01389888  0.02479613 -0.16013929 -0.7033849 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 173 is [True, False, False, True, False, False]
Current timestep = 174. State = [[-0.1168612  -0.14105853]]. Action = [[ 0.09144509  0.07956034 -0.09851846  0.9657719 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 174 is [True, False, False, True, False, False]
Current timestep = 175. State = [[-0.11767846 -0.1279083 ]]. Action = [[-0.09942479  0.09956834  0.01686433 -0.70884573]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 175 is [True, False, False, True, False, False]
Scene graph at timestep 175 is [True, False, False, True, False, False]
State prediction error at timestep 175 is tensor(8.9740e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.12784058 -0.12779987]]. Action = [[-0.18365087 -0.15204385  0.15519765 -0.14217311]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 176 is [True, False, False, True, False, False]
Scene graph at timestep 176 is [True, False, False, True, False, False]
State prediction error at timestep 176 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of -1
Current timestep = 177. State = [[-0.14352465 -0.15136738]]. Action = [[ 0.04414251 -0.23091292  0.21621287  0.249964  ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 177 is [True, False, False, True, False, False]
Current timestep = 178. State = [[-0.1387095  -0.16291258]]. Action = [[0.18991095 0.0346773  0.06339958 0.71028125]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 178 is [True, False, False, True, False, False]
Scene graph at timestep 178 is [True, False, False, True, False, False]
State prediction error at timestep 178 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 178 of -1
Current timestep = 179. State = [[-0.13528803 -0.15413049]]. Action = [[-0.10978135  0.17575422  0.14502844  0.7736486 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 179 is [True, False, False, True, False, False]
Current timestep = 180. State = [[-0.1417012  -0.13444258]]. Action = [[-0.18207495  0.20761758 -0.10432562 -0.8288292 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 180 is [True, False, False, True, False, False]
Scene graph at timestep 180 is [True, False, False, True, False, False]
State prediction error at timestep 180 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of 1
Current timestep = 181. State = [[-0.15679507 -0.10426775]]. Action = [[0.09522244 0.14959383 0.0949834  0.8525599 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 181 is [True, False, False, True, False, False]
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.15382862 -0.09015848]]. Action = [[0.09956282 0.00487638 0.14300522 0.40046442]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 182 is [True, False, False, False, True, False]
Current timestep = 183. State = [[-0.14072706 -0.09161383]]. Action = [[ 0.20216727 -0.06965087  0.10122514 -0.2746774 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 183 is [True, False, False, False, True, False]
Current timestep = 184. State = [[-0.11963362 -0.10125326]]. Action = [[ 0.1766395  -0.16116111  0.21168047 -0.54878485]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 184 is [True, False, False, False, True, False]
Current timestep = 185. State = [[-0.10380493 -0.11808524]]. Action = [[ 0.01316926 -0.07107973 -0.09811635 -0.07529682]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 185 is [True, False, False, False, True, False]
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(7.5461e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 185 of 1
Current timestep = 186. State = [[-0.2005238  0.1686424]]. Action = [[ 0.07678539  0.24232328 -0.10824394  0.6920426 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 186 is [True, False, False, False, True, False]
Scene graph at timestep 186 is [True, False, False, False, False, True]
State prediction error at timestep 186 is tensor(0.0377, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 186 of -1
Current timestep = 187. State = [[-0.18311915  0.17619744]]. Action = [[ 0.08484757 -0.21003981  0.12856525  0.34546888]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 187 is [True, False, False, False, False, True]
Current timestep = 188. State = [[-0.1715317   0.15405278]]. Action = [[ 0.12840152 -0.16016796 -0.19154124  0.5784813 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 188 is [True, False, False, False, False, True]
Scene graph at timestep 188 is [True, False, False, False, False, True]
State prediction error at timestep 188 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of 1
Current timestep = 189. State = [[-0.14768907  0.13961335]]. Action = [[ 0.23555177  0.00729126 -0.16734555  0.6818799 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 189 is [True, False, False, False, False, True]
Current timestep = 190. State = [[-0.11782253  0.13827105]]. Action = [[ 0.22401118 -0.02496804  0.21588585  0.5839541 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 190 is [True, False, False, False, False, True]
Current timestep = 191. State = [[-0.08658828  0.14185467]]. Action = [[ 0.22975442  0.07847404  0.15261406 -0.23241842]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 191 is [True, False, False, False, False, True]
Scene graph at timestep 191 is [True, False, False, False, False, True]
State prediction error at timestep 191 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of 1
Current timestep = 192. State = [[-0.06396439  0.16106282]]. Action = [[-0.11788145  0.21536031  0.04613122 -0.7771176 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 192 is [True, False, False, False, False, True]
Current timestep = 193. State = [[-0.07013385  0.1628547 ]]. Action = [[-0.15821443 -0.22760364 -0.03606784  0.4902215 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 193 is [True, False, False, False, False, True]
Scene graph at timestep 193 is [True, False, False, False, False, True]
State prediction error at timestep 193 is tensor(0.0085, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 193 of -1
Current timestep = 194. State = [[-0.06755764  0.13693608]]. Action = [[ 0.0809367  -0.24471459 -0.13955347 -0.6675154 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 194 is [True, False, False, False, False, True]
Scene graph at timestep 194 is [True, False, False, False, False, True]
State prediction error at timestep 194 is tensor(0.0097, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 194 of 1
Current timestep = 195. State = [[-0.0680353   0.13098584]]. Action = [[-0.01789491  0.21764386 -0.08211103  0.6206404 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 195 is [True, False, False, False, False, True]
Scene graph at timestep 195 is [True, False, False, False, False, True]
State prediction error at timestep 195 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 195 of -1
Current timestep = 196. State = [[-0.06998993  0.13617405]]. Action = [[ 0.03825361 -0.1277435  -0.01366176  0.30850148]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 196 is [True, False, False, False, False, True]
Current timestep = 197. State = [[-0.0633302   0.13592038]]. Action = [[ 0.208552    0.11392996  0.13500485 -0.23206961]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 197 is [True, False, False, False, False, True]
Scene graph at timestep 197 is [True, False, False, False, False, True]
State prediction error at timestep 197 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of -1
Current timestep = 198. State = [[-0.04724516  0.15342407]]. Action = [[ 0.17117971  0.224832   -0.12293276 -0.36432832]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 198 is [True, False, False, False, False, True]
Scene graph at timestep 198 is [False, True, False, False, False, True]
State prediction error at timestep 198 is tensor(0.0108, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 198 of -1
Current timestep = 199. State = [[-0.02075849  0.16142142]]. Action = [[-0.16841577 -0.2072467   0.20018908  0.95084643]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 199 is [False, True, False, False, False, True]
Scene graph at timestep 199 is [False, True, False, False, False, True]
State prediction error at timestep 199 is tensor(0.0113, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of 1
Current timestep = 200. State = [[-0.02098926  0.15269959]]. Action = [[0.01138619 0.05215845 0.12803608 0.7168796 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 200 is [False, True, False, False, False, True]
Scene graph at timestep 200 is [False, True, False, False, False, True]
State prediction error at timestep 200 is tensor(0.0099, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 200 of -1
Current timestep = 201. State = [[-0.01643457  0.14271379]]. Action = [[ 0.24318743 -0.19362558  0.11596575 -0.20392722]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 201 is [False, True, False, False, False, True]
Scene graph at timestep 201 is [False, True, False, False, False, True]
State prediction error at timestep 201 is tensor(0.0111, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 201 of 1
Current timestep = 202. State = [[-0.26911896 -0.19920199]]. Action = [[-0.19175486 -0.17591597  0.12319207 -0.80230844]]. Reward = [100.]
Curr episode timestep = 15
Scene graph at timestep 202 is [False, True, False, False, False, True]
Current timestep = 203. State = [[-0.26397467 -0.22167127]]. Action = [[ 0.10456327 -0.03393669  0.15368462  0.4566126 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 203 is [True, False, False, True, False, False]
Scene graph at timestep 203 is [True, False, False, True, False, False]
State prediction error at timestep 203 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of -1
Current timestep = 204. State = [[-0.25558135 -0.23090631]]. Action = [[-0.05265857 -0.0653791  -0.02596958 -0.11585391]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 204 is [True, False, False, True, False, False]
Scene graph at timestep 204 is [True, False, False, True, False, False]
State prediction error at timestep 204 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 204 of -1
Current timestep = 205. State = [[-0.24629669 -0.2264109 ]]. Action = [[ 0.23904467  0.16843408 -0.10739881 -0.9534903 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 205 is [True, False, False, True, False, False]
Scene graph at timestep 205 is [True, False, False, True, False, False]
State prediction error at timestep 205 is tensor(0.0069, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of 1
Current timestep = 206. State = [[-0.23314461 -0.21218684]]. Action = [[-0.03090355  0.02174655 -0.07812729 -0.5407485 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 206 is [True, False, False, True, False, False]
Scene graph at timestep 206 is [True, False, False, True, False, False]
State prediction error at timestep 206 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of 1
Current timestep = 207. State = [[-0.23542511 -0.21619451]]. Action = [[-0.1076622  -0.07779066 -0.19572538  0.2487067 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 207 is [True, False, False, True, False, False]
Current timestep = 208. State = [[-0.23455322 -0.20747386]]. Action = [[0.07932442 0.23992187 0.00451058 0.7484509 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 208 is [True, False, False, True, False, False]
Current timestep = 209. State = [[-0.22637598 -0.20048794]]. Action = [[ 0.205908   -0.10951823 -0.01451547 -0.79084086]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 209 is [True, False, False, True, False, False]
Scene graph at timestep 209 is [True, False, False, True, False, False]
State prediction error at timestep 209 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 209 of -1
Current timestep = 210. State = [[-0.21737444 -0.21359836]]. Action = [[-0.24429692 -0.11451353  0.01622051 -0.74971884]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 210 is [True, False, False, True, False, False]
Scene graph at timestep 210 is [True, False, False, True, False, False]
State prediction error at timestep 210 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[-0.23257123 -0.22408798]]. Action = [[-0.14080434  0.01124951 -0.0457989  -0.8788185 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 211 is [True, False, False, True, False, False]
Scene graph at timestep 211 is [True, False, False, True, False, False]
State prediction error at timestep 211 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 211 of -1
Current timestep = 212. State = [[-0.23922166 -0.22946274]]. Action = [[ 0.23246294 -0.1327737  -0.095357   -0.7589254 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 212 is [True, False, False, True, False, False]
Current timestep = 213. State = [[-0.21849884 -0.22946425]]. Action = [[ 0.23536456  0.08325171 -0.13888007  0.5826249 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 213 is [True, False, False, True, False, False]
Scene graph at timestep 213 is [True, False, False, True, False, False]
State prediction error at timestep 213 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 213 of 1
Current timestep = 214. State = [[-0.18990614 -0.22786175]]. Action = [[ 0.14147496 -0.05965464 -0.12903339 -0.41374385]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 214 is [True, False, False, True, False, False]
Current timestep = 215. State = [[-0.18303128 -0.22466193]]. Action = [[-0.14893077  0.16305053  0.23209321  0.8380518 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 215 is [True, False, False, True, False, False]
Current timestep = 216. State = [[-0.17939463 -0.21087092]]. Action = [[ 0.14815032  0.09148392 -0.1638424   0.01653314]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 216 is [True, False, False, True, False, False]
Scene graph at timestep 216 is [True, False, False, True, False, False]
State prediction error at timestep 216 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 216 of 1
Current timestep = 217. State = [[-0.16323599 -0.20123288]]. Action = [[ 0.22452515 -0.0352817  -0.02607715  0.30980027]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 217 is [True, False, False, True, False, False]
Current timestep = 218. State = [[-0.13750352 -0.19009773]]. Action = [[ 0.15580425  0.21563    -0.14251316  0.7834375 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 218 is [True, False, False, True, False, False]
Scene graph at timestep 218 is [True, False, False, True, False, False]
State prediction error at timestep 218 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of 1
Current timestep = 219. State = [[-0.12287941 -0.16966255]]. Action = [[-0.1293613   0.10800183 -0.07997246  0.7052257 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 219 is [True, False, False, True, False, False]
Current timestep = 220. State = [[-0.13048562 -0.17584921]]. Action = [[-0.12478873 -0.22450398 -0.13019681 -0.11298001]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 220 is [True, False, False, True, False, False]
Scene graph at timestep 220 is [True, False, False, True, False, False]
State prediction error at timestep 220 is tensor(2.8205e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 220 of -1
Current timestep = 221. State = [[-0.138737   -0.17928055]]. Action = [[ 0.02621749  0.2282533  -0.03702393 -0.16171372]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 221 is [True, False, False, True, False, False]
Current timestep = 222. State = [[-0.13204336 -0.1554434 ]]. Action = [[0.19991744 0.10271564 0.03230208 0.27718115]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 222 is [True, False, False, True, False, False]
Current timestep = 223. State = [[-0.12704477 -0.15531202]]. Action = [[-0.02132535 -0.1812718   0.05546927 -0.5275832 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 223 is [True, False, False, True, False, False]
Current timestep = 224. State = [[-0.1172842  -0.17217608]]. Action = [[ 0.18638372 -0.18704414  0.13417846 -0.19085336]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 224 is [True, False, False, True, False, False]
Scene graph at timestep 224 is [True, False, False, True, False, False]
State prediction error at timestep 224 is tensor(3.7848e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of -1
Current timestep = 225. State = [[-0.10425954 -0.19356276]]. Action = [[-0.20523207 -0.02882142 -0.14705764 -0.992453  ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 225 is [True, False, False, True, False, False]
Current timestep = 226. State = [[-0.10484444 -0.1876979 ]]. Action = [[0.16312611 0.19492674 0.00845796 0.06080151]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 226 is [True, False, False, True, False, False]
Scene graph at timestep 226 is [True, False, False, True, False, False]
State prediction error at timestep 226 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 226 of 1
Current timestep = 227. State = [[-0.10228785 -0.1846083 ]]. Action = [[ 0.04280749 -0.21063972 -0.24491099  0.96852994]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 227 is [True, False, False, True, False, False]
Current timestep = 228. State = [[-0.09874044 -0.18153724]]. Action = [[ 0.04110819  0.2403486  -0.04572909 -0.49347496]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 228 is [True, False, False, True, False, False]
Current timestep = 229. State = [[-0.09721841 -0.16205148]]. Action = [[-0.0138146   0.12821075 -0.2294237   0.6772413 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 229 is [True, False, False, True, False, False]
Scene graph at timestep 229 is [True, False, False, True, False, False]
State prediction error at timestep 229 is tensor(7.3828e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 229 of 1
Current timestep = 230. State = [[-0.09737236 -0.15981454]]. Action = [[-0.05724882 -0.18657768 -0.23582606 -0.7276163 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 230 is [True, False, False, True, False, False]
Scene graph at timestep 230 is [True, False, False, True, False, False]
State prediction error at timestep 230 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 230 of -1
Current timestep = 231. State = [[-0.09232805 -0.1814518 ]]. Action = [[ 0.18598494 -0.20735958 -0.05340558 -0.7092604 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 231 is [True, False, False, True, False, False]
Current timestep = 232. State = [[-0.08676539 -0.21146144]]. Action = [[-0.20415829 -0.20380601  0.01111242 -0.5685104 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 232 is [True, False, False, True, False, False]
Current timestep = 233. State = [[-0.09226263 -0.21747881]]. Action = [[-0.04092228  0.2323212   0.20692286 -0.91447186]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 233 is [True, False, False, True, False, False]
Current timestep = 234. State = [[-0.10065685 -0.21715088]]. Action = [[-0.17199366 -0.13488871 -0.11360209 -0.5435645 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 234 is [True, False, False, True, False, False]
Scene graph at timestep 234 is [True, False, False, True, False, False]
State prediction error at timestep 234 is tensor(1.7664e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 234 of -1
Current timestep = 235. State = [[-0.11416794 -0.23337276]]. Action = [[ 0.08678746 -0.16192815 -0.09508677 -0.78106034]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 235 is [True, False, False, True, False, False]
Scene graph at timestep 235 is [True, False, False, True, False, False]
State prediction error at timestep 235 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 235 of -1
Current timestep = 236. State = [[-0.10803147 -0.24395159]]. Action = [[ 0.17877185 -0.06409886  0.20455784 -0.9268576 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 236 is [True, False, False, True, False, False]
Scene graph at timestep 236 is [True, False, False, True, False, False]
State prediction error at timestep 236 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 236 of -1
Current timestep = 237. State = [[-0.09948349 -0.24824378]]. Action = [[-0.04677445 -0.00047979  0.1453844  -0.3581444 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 237 is [True, False, False, True, False, False]
Scene graph at timestep 237 is [True, False, False, True, False, False]
State prediction error at timestep 237 is tensor(2.5051e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 237 of -1
Current timestep = 238. State = [[-0.09049375 -0.26185584]]. Action = [[ 0.24040163 -0.23966603  0.19233674 -0.8395644 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 238 is [True, False, False, True, False, False]
Scene graph at timestep 238 is [True, False, False, True, False, False]
State prediction error at timestep 238 is tensor(9.6317e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 238 of -1
Current timestep = 239. State = [[-0.07298477 -0.2879962 ]]. Action = [[-0.06166241 -0.07622275  0.23867017 -0.4167161 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 239 is [True, False, False, True, False, False]
Scene graph at timestep 239 is [True, False, False, True, False, False]
State prediction error at timestep 239 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 239 of -1
Current timestep = 240. State = [[-0.07483593 -0.2945736 ]]. Action = [[-0.07917883  0.0627901   0.20552725 -0.0183183 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 240 is [True, False, False, True, False, False]
Scene graph at timestep 240 is [True, False, False, True, False, False]
State prediction error at timestep 240 is tensor(6.9522e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 240 of -1
Current timestep = 241. State = [[-0.07100457 -0.29323852]]. Action = [[ 0.1912319  -0.03001468 -0.2197167   0.481174  ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 241 is [True, False, False, True, False, False]
Scene graph at timestep 241 is [True, False, False, True, False, False]
State prediction error at timestep 241 is tensor(2.9768e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 241 of -1
Current timestep = 242. State = [[-0.06422836 -0.29260138]]. Action = [[ 0.07400364 -0.16094874  0.11697897  0.51293063]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 242 is [True, False, False, True, False, False]
Current timestep = 243. State = [[-0.06422836 -0.29260138]]. Action = [[-0.19942972 -0.2284324  -0.01813105  0.5414748 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 243 is [True, False, False, True, False, False]
Scene graph at timestep 243 is [True, False, False, True, False, False]
State prediction error at timestep 243 is tensor(7.2134e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 243 of -1
Current timestep = 244. State = [[-0.06433994 -0.29142082]]. Action = [[-0.07118806  0.05920035  0.16870424 -0.07359773]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 244 is [True, False, False, True, False, False]
Scene graph at timestep 244 is [True, False, False, True, False, False]
State prediction error at timestep 244 is tensor(2.0186e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of -1
Current timestep = 245. State = [[-0.06446829 -0.28999805]]. Action = [[-0.02017815 -0.19067    -0.23056982  0.9328258 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 245 is [True, False, False, True, False, False]
Current timestep = 246. State = [[-0.06674981 -0.29392916]]. Action = [[-0.10307115 -0.06335783  0.06032687 -0.70739186]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 246 is [True, False, False, True, False, False]
Scene graph at timestep 246 is [True, False, False, True, False, False]
State prediction error at timestep 246 is tensor(6.1374e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 246 of -1
Current timestep = 247. State = [[-0.07362116 -0.29365262]]. Action = [[-0.12033498  0.1008293  -0.02248812 -0.09284103]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 247 is [True, False, False, True, False, False]
Scene graph at timestep 247 is [True, False, False, True, False, False]
State prediction error at timestep 247 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 247 of -1
Current timestep = 248. State = [[-0.08800502 -0.29178813]]. Action = [[-0.19833559  0.00460386  0.20808992  0.54458857]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 248 is [True, False, False, True, False, False]
Current timestep = 249. State = [[-0.11214553 -0.27742922]]. Action = [[-0.15801463  0.2385984  -0.12273295  0.30359173]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 249 is [True, False, False, True, False, False]
Scene graph at timestep 249 is [True, False, False, True, False, False]
State prediction error at timestep 249 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 249 of -1
Current timestep = 250. State = [[-0.13226835 -0.25770718]]. Action = [[-0.04119813 -0.02376522  0.22042549 -0.22550964]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 250 is [True, False, False, True, False, False]
Current timestep = 251. State = [[-0.1284737  -0.24388947]]. Action = [[0.24092162 0.21585816 0.2275073  0.92116475]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 251 is [True, False, False, True, False, False]
Current timestep = 252. State = [[-0.12239981 -0.21425772]]. Action = [[-0.08537082  0.2066952   0.11863691 -0.15972894]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 252 is [True, False, False, True, False, False]
Scene graph at timestep 252 is [True, False, False, True, False, False]
State prediction error at timestep 252 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of 1
Current timestep = 253. State = [[-0.12723649 -0.20841007]]. Action = [[-0.11525649 -0.23355168 -0.11302751 -0.14563024]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 253 is [True, False, False, True, False, False]
Current timestep = 254. State = [[-0.13929006 -0.21421231]]. Action = [[-0.16503516  0.14250988  0.08572155  0.43207693]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 254 is [True, False, False, True, False, False]
Current timestep = 255. State = [[-0.15282065 -0.19713938]]. Action = [[-0.05819684  0.22100973  0.00858656  0.81975675]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 255 is [True, False, False, True, False, False]
Current timestep = 256. State = [[-0.15495089 -0.19053097]]. Action = [[ 0.17617625 -0.22756828 -0.13547432  0.32257462]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 256 is [True, False, False, True, False, False]
Scene graph at timestep 256 is [True, False, False, True, False, False]
State prediction error at timestep 256 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 256 of -1
Current timestep = 257. State = [[-0.14243856 -0.1935936 ]]. Action = [[ 0.21626195  0.08861959 -0.20895647  0.7998743 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 257 is [True, False, False, True, False, False]
Scene graph at timestep 257 is [True, False, False, True, False, False]
State prediction error at timestep 257 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of 1
Current timestep = 258. State = [[-0.13577648 -0.20361082]]. Action = [[-0.13360137 -0.23314406  0.04996228 -0.94513655]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 258 is [True, False, False, True, False, False]
Current timestep = 259. State = [[-0.1392649  -0.21794242]]. Action = [[-0.06008317  0.00179824 -0.13702798  0.5688461 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 259 is [True, False, False, True, False, False]
Current timestep = 260. State = [[-0.13942795 -0.2325531 ]]. Action = [[ 0.15450919 -0.22406888  0.1422475   0.23889828]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 260 is [True, False, False, True, False, False]
Current timestep = 261. State = [[-0.13250059 -0.24812818]]. Action = [[ 0.13672197 -0.00585265  0.02948901 -0.66648483]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 261 is [True, False, False, True, False, False]
Current timestep = 262. State = [[-0.12446412 -0.24894372]]. Action = [[-0.17446515  0.10128087 -0.18154718  0.44066238]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 262 is [True, False, False, True, False, False]
Current timestep = 263. State = [[-0.12012431 -0.23233795]]. Action = [[ 0.18440181  0.24262017  0.23573905 -0.00582778]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 263 is [True, False, False, True, False, False]
Scene graph at timestep 263 is [True, False, False, True, False, False]
State prediction error at timestep 263 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 263 of -1
Current timestep = 264. State = [[-0.11713116 -0.22420494]]. Action = [[-0.02866802 -0.22553669 -0.20511329  0.44949257]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 264 is [True, False, False, True, False, False]
Scene graph at timestep 264 is [True, False, False, True, False, False]
State prediction error at timestep 264 is tensor(4.9183e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 264 of -1
Current timestep = 265. State = [[-0.10707053 -0.2338567 ]]. Action = [[0.22955382 0.00179017 0.04664364 0.5120548 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 265 is [True, False, False, True, False, False]
Current timestep = 266. State = [[-0.09437246 -0.23057877]]. Action = [[-0.06615219  0.11067933 -0.13090771 -0.42059088]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 266 is [True, False, False, True, False, False]
Current timestep = 267. State = [[-0.10123441 -0.23573752]]. Action = [[-0.2180625  -0.15284261 -0.24093716 -0.88702637]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 267 is [True, False, False, True, False, False]
Scene graph at timestep 267 is [True, False, False, True, False, False]
State prediction error at timestep 267 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 267 of -1
Current timestep = 268. State = [[-0.11178525 -0.23656161]]. Action = [[-0.09641811  0.20761332  0.21433902  0.5882586 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 268 is [True, False, False, True, False, False]
Scene graph at timestep 268 is [True, False, False, True, False, False]
State prediction error at timestep 268 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 268 of 1
Current timestep = 269. State = [[-0.12935121 -0.22508672]]. Action = [[-0.17996335 -0.05687447  0.22456878  0.5198796 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 269 is [True, False, False, True, False, False]
Scene graph at timestep 269 is [True, False, False, True, False, False]
State prediction error at timestep 269 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[-0.14799736 -0.24242833]]. Action = [[ 0.01405343 -0.24919018 -0.15802604 -0.3596481 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 270 is [True, False, False, True, False, False]
Scene graph at timestep 270 is [True, False, False, True, False, False]
State prediction error at timestep 270 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 270 of -1
Current timestep = 271. State = [[-0.14840238 -0.25678045]]. Action = [[ 0.11692405  0.04019585  0.07341838 -0.39078355]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 271 is [True, False, False, True, False, False]
Current timestep = 272. State = [[-0.1427712  -0.24122906]]. Action = [[ 0.04415333  0.24898705  0.02793232 -0.8090328 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 272 is [True, False, False, True, False, False]
Scene graph at timestep 272 is [True, False, False, True, False, False]
State prediction error at timestep 272 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 272 of 1
Current timestep = 273. State = [[-0.12972987 -0.21510315]]. Action = [[ 0.15283474  0.07626897 -0.20670097  0.24253011]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 273 is [True, False, False, True, False, False]
Scene graph at timestep 273 is [True, False, False, True, False, False]
State prediction error at timestep 273 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 273 of 1
Current timestep = 274. State = [[-0.11654845 -0.20210423]]. Action = [[ 0.05889836  0.098719   -0.08719599  0.8809035 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 274 is [True, False, False, True, False, False]
Scene graph at timestep 274 is [True, False, False, True, False, False]
State prediction error at timestep 274 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 274 of 1
Current timestep = 275. State = [[-0.10282819 -0.19518843]]. Action = [[ 0.19458312 -0.03807385  0.20634323 -0.58540237]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 275 is [True, False, False, True, False, False]
Current timestep = 276. State = [[-0.08533154 -0.19046842]]. Action = [[ 0.05324078  0.10955948  0.02147451 -0.55635464]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 276 is [True, False, False, True, False, False]
Scene graph at timestep 276 is [True, False, False, True, False, False]
State prediction error at timestep 276 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of 1
Current timestep = 277. State = [[-0.07142734 -0.19247104]]. Action = [[ 0.18747362 -0.19808881 -0.21081166  0.6780679 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 277 is [True, False, False, True, False, False]
Current timestep = 278. State = [[-0.0648315 -0.2156773]]. Action = [[-0.23823233 -0.17603981  0.14853883 -0.08726853]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 278 is [True, False, False, True, False, False]
Scene graph at timestep 278 is [True, False, False, True, False, False]
State prediction error at timestep 278 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 278 of -1
Current timestep = 279. State = [[-0.07199207 -0.23073953]]. Action = [[-0.05167842  0.05744961 -0.01054722  0.64180756]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 279 is [True, False, False, True, False, False]
Scene graph at timestep 279 is [True, False, False, True, False, False]
State prediction error at timestep 279 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 279 of -1
Current timestep = 280. State = [[-0.07591293 -0.23834829]]. Action = [[-0.04065193 -0.13764161 -0.0414553   0.47186792]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 280 is [True, False, False, True, False, False]
Scene graph at timestep 280 is [True, False, False, True, False, False]
State prediction error at timestep 280 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 280 of -1
Current timestep = 281. State = [[-0.09444669 -0.2629146 ]]. Action = [[-0.19441546 -0.24263352  0.24049348 -0.861547  ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 281 is [True, False, False, True, False, False]
Scene graph at timestep 281 is [True, False, False, True, False, False]
State prediction error at timestep 281 is tensor(4.8032e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 281 of -1
Current timestep = 282. State = [[-0.10529228 -0.28341076]]. Action = [[ 0.19937873 -0.17404914  0.05888695 -0.6315773 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 282 is [True, False, False, True, False, False]
Current timestep = 283. State = [[-0.1030493 -0.2768876]]. Action = [[ 0.11705393  0.12224114 -0.10649413  0.98325694]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 283 is [True, False, False, True, False, False]
Current timestep = 284. State = [[-0.09259868 -0.2580469 ]]. Action = [[0.1427126  0.21376878 0.00178516 0.7555523 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 284 is [True, False, False, True, False, False]
Current timestep = 285. State = [[-0.07764139 -0.2445007 ]]. Action = [[ 0.20772994 -0.09176049  0.13272852 -0.16001815]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 285 is [True, False, False, True, False, False]
Scene graph at timestep 285 is [True, False, False, True, False, False]
State prediction error at timestep 285 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 285 of 1
Current timestep = 286. State = [[-0.06181364 -0.2459672 ]]. Action = [[-0.08000204  0.00707287 -0.2118452  -0.35480714]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 286 is [True, False, False, True, False, False]
Scene graph at timestep 286 is [True, False, False, True, False, False]
State prediction error at timestep 286 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 286 of -1
Current timestep = 287. State = [[-0.05540043 -0.2487033 ]]. Action = [[ 0.23409429 -0.08227988 -0.21699207 -0.62731373]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 287 is [True, False, False, True, False, False]
Scene graph at timestep 287 is [True, False, False, True, False, False]
State prediction error at timestep 287 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 287 of 1
Current timestep = 288. State = [[-0.03344055 -0.24219888]]. Action = [[0.0866673  0.19915515 0.0831688  0.49967372]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 288 is [True, False, False, True, False, False]
Current timestep = 289. State = [[-0.02851872 -0.23445691]]. Action = [[-0.04928653 -0.02166466 -0.2147951   0.9246621 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 289 is [False, True, False, True, False, False]
Current timestep = 290. State = [[-0.02806279 -0.22342922]]. Action = [[-0.06863192  0.2065742  -0.20754728 -0.92882365]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 290 is [False, True, False, True, False, False]
Scene graph at timestep 290 is [False, True, False, True, False, False]
State prediction error at timestep 290 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 290 of 1
Current timestep = 291. State = [[-0.02387468 -0.19716051]]. Action = [[ 0.12679839  0.15548152 -0.01058164 -0.8704691 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 291 is [False, True, False, True, False, False]
Current timestep = 292. State = [[-0.01372951 -0.17025787]]. Action = [[0.15556729 0.228205   0.10701674 0.5870172 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 292 is [False, True, False, True, False, False]
Scene graph at timestep 292 is [False, True, False, True, False, False]
State prediction error at timestep 292 is tensor(1.8246e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 292 of 1
Current timestep = 293. State = [[ 0.0073682  -0.13604715]]. Action = [[ 0.03711495  0.20752823  0.1504584  -0.13983917]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 293 is [False, True, False, True, False, False]
Scene graph at timestep 293 is [False, True, False, True, False, False]
State prediction error at timestep 293 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 293 of 1
Current timestep = 294. State = [[ 0.01508502 -0.10515727]]. Action = [[ 0.18296015  0.19553056 -0.17139761 -0.51477134]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 294 is [False, True, False, True, False, False]
Scene graph at timestep 294 is [False, True, False, False, True, False]
State prediction error at timestep 294 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 294 of -1
Current timestep = 295. State = [[-0.20785931  0.1143241 ]]. Action = [[-0.02343817 -0.18324974  0.15463495 -0.54383075]]. Reward = [100.]
Curr episode timestep = 92
Scene graph at timestep 295 is [False, True, False, False, True, False]
Current timestep = 296. State = [[-0.19858734  0.14016585]]. Action = [[-0.00350121  0.18514216  0.18296286  0.9179237 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 296 is [True, False, False, False, True, False]
Current timestep = 297. State = [[-0.20102993  0.16419308]]. Action = [[ 0.00485459  0.19051638 -0.04749832  0.26451588]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 297 is [True, False, False, False, False, True]
Current timestep = 298. State = [[-0.19462384  0.19410786]]. Action = [[ 0.10375437  0.22359091 -0.16494493 -0.95179176]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 298 is [True, False, False, False, False, True]
Scene graph at timestep 298 is [True, False, False, False, False, True]
State prediction error at timestep 298 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 298 of -1
Current timestep = 299. State = [[-0.17669067  0.23002887]]. Action = [[0.13633645 0.2436232  0.13720626 0.39175963]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 299 is [True, False, False, False, False, True]
Scene graph at timestep 299 is [True, False, False, False, False, True]
State prediction error at timestep 299 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 299 of -1
Current timestep = 300. State = [[-0.15113173  0.24249084]]. Action = [[ 0.1924048  -0.16846323  0.18184835  0.7677572 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 300 is [True, False, False, False, False, True]
Current timestep = 301. State = [[-0.1315331   0.23304819]]. Action = [[ 0.11404693 -0.01693183 -0.1568435  -0.03897303]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 301 is [True, False, False, False, False, True]
Scene graph at timestep 301 is [True, False, False, False, False, True]
State prediction error at timestep 301 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 301 of 1
Current timestep = 302. State = [[-0.10958552  0.21959656]]. Action = [[ 0.15747353 -0.14199689  0.08473757 -0.5427685 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 302 is [True, False, False, False, False, True]
Current timestep = 303. State = [[-0.09491599  0.21464999]]. Action = [[ 0.05706081  0.03491879 -0.10849053  0.25877643]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 303 is [True, False, False, False, False, True]
Current timestep = 304. State = [[-0.08692634  0.22113496]]. Action = [[ 0.00092384  0.10089567 -0.05046967 -0.81844634]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 304 is [True, False, False, False, False, True]
Scene graph at timestep 304 is [True, False, False, False, False, True]
State prediction error at timestep 304 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 304 of 1
Current timestep = 305. State = [[-0.09110472  0.23620908]]. Action = [[-0.22722124  0.11600298  0.02080989 -0.97486746]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 305 is [True, False, False, False, False, True]
Scene graph at timestep 305 is [True, False, False, False, False, True]
State prediction error at timestep 305 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of -1
Current timestep = 306. State = [[-0.09735206  0.2412233 ]]. Action = [[ 0.09957051 -0.15719005 -0.20116739 -0.82801867]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 306 is [True, False, False, False, False, True]
Scene graph at timestep 306 is [True, False, False, False, False, True]
State prediction error at timestep 306 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 306 of 1
Current timestep = 307. State = [[-0.08686716  0.22522348]]. Action = [[ 0.19728497 -0.03547087  0.24073774  0.38725793]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 307 is [True, False, False, False, False, True]
Current timestep = 308. State = [[-0.07135522  0.20621112]]. Action = [[ 0.05612254 -0.24600407 -0.22988874  0.31568575]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 308 is [True, False, False, False, False, True]
Current timestep = 309. State = [[-0.07118572  0.20296787]]. Action = [[-0.16384618  0.21727121  0.136581   -0.7342251 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 309 is [True, False, False, False, False, True]
Scene graph at timestep 309 is [True, False, False, False, False, True]
State prediction error at timestep 309 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 309 of -1
Current timestep = 310. State = [[-0.07255479  0.20408812]]. Action = [[ 0.15379614 -0.17917281  0.22359884 -0.04520738]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 310 is [True, False, False, False, False, True]
Current timestep = 311. State = [[-0.06874309  0.18879686]]. Action = [[-0.21735942 -0.13623634 -0.08806242  0.22850358]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 311 is [True, False, False, False, False, True]
Current timestep = 312. State = [[-0.06187373  0.16624784]]. Action = [[ 0.23123151 -0.19882262 -0.11545941  0.8183931 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 312 is [True, False, False, False, False, True]
Current timestep = 313. State = [[-0.05484054  0.14993791]]. Action = [[-0.08858001 -0.03676049 -0.14028846 -0.23686916]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 313 is [True, False, False, False, False, True]
Scene graph at timestep 313 is [True, False, False, False, False, True]
State prediction error at timestep 313 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 313 of 1
Current timestep = 314. State = [[-0.05663182  0.13076355]]. Action = [[-0.11347772 -0.22048713 -0.08674131  0.38826144]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 314 is [True, False, False, False, False, True]
Scene graph at timestep 314 is [True, False, False, False, False, True]
State prediction error at timestep 314 is tensor(1.4117e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 314 of 1
Current timestep = 315. State = [[-0.07244024  0.1001147 ]]. Action = [[-0.22430693 -0.20006903 -0.06493384  0.07362282]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 315 is [True, False, False, False, False, True]
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(1.9969e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 315 of -1
Current timestep = 316. State = [[-0.09637223  0.08792893]]. Action = [[0.11416465 0.13898319 0.23231548 0.28867602]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 316 is [True, False, False, False, True, False]
Current timestep = 317. State = [[-0.08844315  0.09569694]]. Action = [[ 0.20588005  0.06985876  0.08136526 -0.68671227]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 317 is [True, False, False, False, True, False]
Scene graph at timestep 317 is [True, False, False, False, True, False]
State prediction error at timestep 317 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 317 of 1
Current timestep = 318. State = [[-0.07179883  0.11097065]]. Action = [[ 0.13755098  0.1555013   0.18397525 -0.6925617 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 318 is [True, False, False, False, True, False]
Current timestep = 319. State = [[-0.06590003  0.13570578]]. Action = [[-0.07993785  0.22440049 -0.20272732 -0.02538997]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 319 is [True, False, False, False, True, False]
Scene graph at timestep 319 is [True, False, False, False, False, True]
State prediction error at timestep 319 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 319 of -1
Current timestep = 320. State = [[-0.06206552  0.15118963]]. Action = [[ 0.15879709 -0.0613967  -0.18980764  0.47429752]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 320 is [True, False, False, False, False, True]
Scene graph at timestep 320 is [True, False, False, False, False, True]
State prediction error at timestep 320 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 320 of 1
Current timestep = 321. State = [[-0.04073035  0.13951878]]. Action = [[ 0.1849728  -0.18299446 -0.02153903 -0.5852008 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 321 is [True, False, False, False, False, True]
Scene graph at timestep 321 is [False, True, False, False, False, True]
State prediction error at timestep 321 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 321 of 1
Current timestep = 322. State = [[-0.02698746  0.13476157]]. Action = [[-0.06642631  0.18589786 -0.15121801 -0.8579717 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 322 is [False, True, False, False, False, True]
Scene graph at timestep 322 is [False, True, False, False, False, True]
State prediction error at timestep 322 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 322 of -1
Current timestep = 323. State = [[-0.03165149  0.1580692 ]]. Action = [[-0.0803144   0.16074485  0.08981201 -0.7954664 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 323 is [False, True, False, False, False, True]
Scene graph at timestep 323 is [False, True, False, False, False, True]
State prediction error at timestep 323 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 323 of -1
Current timestep = 324. State = [[-0.0402267   0.16996159]]. Action = [[-0.21632627 -0.0824651   0.20704055 -0.3293473 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 324 is [False, True, False, False, False, True]
Scene graph at timestep 324 is [False, True, False, False, False, True]
State prediction error at timestep 324 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 324 of 1
Current timestep = 325. State = [[-0.04180091  0.16942509]]. Action = [[ 0.22310174  0.04510891  0.09997401 -0.77113086]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 325 is [False, True, False, False, False, True]
Scene graph at timestep 325 is [False, True, False, False, False, True]
State prediction error at timestep 325 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 325 of -1
Current timestep = 326. State = [[-0.04339149  0.16816555]]. Action = [[-0.23782007 -0.06784219  0.06803483  0.14024746]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 326 is [False, True, False, False, False, True]
Scene graph at timestep 326 is [False, True, False, False, False, True]
State prediction error at timestep 326 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of -1
Current timestep = 327. State = [[-0.03998264  0.15241712]]. Action = [[ 0.15902951 -0.20912202  0.11608183  0.58352745]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 327 is [False, True, False, False, False, True]
Scene graph at timestep 327 is [False, True, False, False, False, True]
State prediction error at timestep 327 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 327 of 1
Current timestep = 328. State = [[-0.02958614  0.12535612]]. Action = [[ 0.14556324 -0.16982286 -0.01171485  0.6961576 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 328 is [False, True, False, False, False, True]
Scene graph at timestep 328 is [False, True, False, False, False, True]
State prediction error at timestep 328 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 328 of 1
Current timestep = 329. State = [[-0.24458092  0.19539669]]. Action = [[-0.16106339 -0.23605798  0.13985354 -0.8154379 ]]. Reward = [100.]
Curr episode timestep = 33
Scene graph at timestep 329 is [False, True, False, False, False, True]
Current timestep = 330. State = [[-0.23741612  0.21489868]]. Action = [[-0.06339791 -0.12033933 -0.15927121  0.26025057]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 330 is [True, False, False, False, False, True]
Scene graph at timestep 330 is [True, False, False, False, False, True]
State prediction error at timestep 330 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 330 of 1
Current timestep = 331. State = [[-0.23053744  0.2081887 ]]. Action = [[ 0.18368274  0.00830942  0.19274747 -0.4958408 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 331 is [True, False, False, False, False, True]
Scene graph at timestep 331 is [True, False, False, False, False, True]
State prediction error at timestep 331 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 331 of 1
Current timestep = 332. State = [[-0.22141267  0.1972402 ]]. Action = [[-0.21965528 -0.24131103  0.0685823  -0.8243787 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 332 is [True, False, False, False, False, True]
Scene graph at timestep 332 is [True, False, False, False, False, True]
State prediction error at timestep 332 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 332 of 1
Current timestep = 333. State = [[-0.2397831   0.16638674]]. Action = [[-0.21816367 -0.23618102 -0.16552459  0.61674523]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 333 is [True, False, False, False, False, True]
Scene graph at timestep 333 is [True, False, False, False, False, True]
State prediction error at timestep 333 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 333 of -1
Current timestep = 334. State = [[-0.26008597  0.13317226]]. Action = [[-0.01500456 -0.20827629  0.20032912 -0.58960915]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 334 is [True, False, False, False, False, True]
Current timestep = 335. State = [[-0.2543709   0.12641473]]. Action = [[ 0.22883344  0.14048398 -0.24421383  0.59512377]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 335 is [True, False, False, False, False, True]
Current timestep = 336. State = [[-0.24960102  0.13028154]]. Action = [[-0.15690206 -0.05140413 -0.08713457 -0.97164714]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 336 is [True, False, False, False, False, True]
Scene graph at timestep 336 is [True, False, False, False, False, True]
State prediction error at timestep 336 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of -1
Current timestep = 337. State = [[-0.2505128  0.118302 ]]. Action = [[-0.14210044 -0.23317213 -0.1155505   0.9461267 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 337 is [True, False, False, False, False, True]
Current timestep = 338. State = [[-0.24425082  0.09209447]]. Action = [[ 0.2093671  -0.20350893 -0.00103137 -0.39712644]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 338 is [True, False, False, False, True, False]
Current timestep = 339. State = [[-0.23844987  0.07910278]]. Action = [[-0.12571014  0.06509632 -0.1934611  -0.88359267]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 339 is [True, False, False, False, True, False]
Current timestep = 340. State = [[-0.24349211  0.06645235]]. Action = [[-0.09982985 -0.22630109 -0.14900334 -0.98977613]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 340 is [True, False, False, False, True, False]
Current timestep = 341. State = [[-0.2526543   0.06414714]]. Action = [[0.04444432 0.24638677 0.00761345 0.18819845]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 341 is [True, False, False, False, True, False]
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 341 of -1
Current timestep = 342. State = [[-0.25721273  0.08713197]]. Action = [[0.00377902 0.16636813 0.21867901 0.44557607]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 342 is [True, False, False, False, True, False]
Current timestep = 343. State = [[-0.24947987  0.08525695]]. Action = [[ 0.19545174 -0.21366198  0.08904511 -0.9384203 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 343 is [True, False, False, False, True, False]
Current timestep = 344. State = [[-0.22846235  0.08891833]]. Action = [[ 0.20626423  0.22630048 -0.03334971  0.70650434]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 344 is [True, False, False, False, True, False]
Current timestep = 345. State = [[-0.22062792  0.11047142]]. Action = [[-0.22702853  0.17741844 -0.12793955  0.20754933]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 345 is [True, False, False, False, True, False]
Current timestep = 346. State = [[-0.22199059  0.11435948]]. Action = [[ 0.20472315 -0.20081167  0.03207779  0.4814911 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 346 is [True, False, False, False, True, False]
Scene graph at timestep 346 is [True, False, False, False, True, False]
State prediction error at timestep 346 is tensor(1.2619e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 346 of 1
Current timestep = 347. State = [[-0.20400624  0.0984713 ]]. Action = [[ 0.16447511 -0.06678835  0.19792563  0.1745007 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 347 is [True, False, False, False, True, False]
Scene graph at timestep 347 is [True, False, False, False, True, False]
State prediction error at timestep 347 is tensor(9.2266e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 347 of 1
Current timestep = 348. State = [[-0.18229936  0.08320861]]. Action = [[ 0.16364408 -0.15279396  0.02124664  0.32174265]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 348 is [True, False, False, False, True, False]
Scene graph at timestep 348 is [True, False, False, False, True, False]
State prediction error at timestep 348 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 348 of 1
Current timestep = 349. State = [[-0.1592762   0.05929109]]. Action = [[ 0.14110366 -0.1782016   0.08995518  0.644544  ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 349 is [True, False, False, False, True, False]
Scene graph at timestep 349 is [True, False, False, False, True, False]
State prediction error at timestep 349 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 349 of 1
Current timestep = 350. State = [[-0.1432418   0.04464653]]. Action = [[ 0.08633012 -0.00140299  0.15787584  0.32988405]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 350 is [True, False, False, False, True, False]
Current timestep = 351. State = [[-0.12286993  0.03892364]]. Action = [[ 0.23382193 -0.09290695  0.07297829 -0.7677097 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 351 is [True, False, False, False, True, False]
Scene graph at timestep 351 is [True, False, False, False, True, False]
State prediction error at timestep 351 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 351 of 1
Current timestep = 352. State = [[-0.09411682  0.01896593]]. Action = [[ 0.10676932 -0.19867624  0.06975925  0.01404154]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 352 is [True, False, False, False, True, False]
Current timestep = 353. State = [[-0.08098774  0.00733136]]. Action = [[0.07283896 0.06762028 0.18311888 0.76659894]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 353 is [True, False, False, False, True, False]
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of 1
Current timestep = 354. State = [[-0.07407391  0.00078249]]. Action = [[-0.14891648 -0.14403306 -0.18206881  0.7971046 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 354 is [True, False, False, False, True, False]
Current timestep = 355. State = [[-0.0689351  -0.00854259]]. Action = [[ 0.2410821  -0.01107275 -0.11582337 -0.23866445]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 355 is [True, False, False, False, True, False]
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(7.4617e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 355 of -1
Current timestep = 356. State = [[-0.06129088 -0.00662109]]. Action = [[-0.03633416  0.10757732  0.0517787  -0.23876464]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 356 is [True, False, False, False, True, False]
Current timestep = 357. State = [[-0.06613626  0.01170161]]. Action = [[-0.1813374   0.22895813 -0.07269767  0.2093029 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 357 is [True, False, False, False, True, False]
Scene graph at timestep 357 is [True, False, False, False, True, False]
State prediction error at timestep 357 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 357 of 1
Current timestep = 358. State = [[-0.06919409  0.03259578]]. Action = [[ 0.15739688 -0.01980725  0.09942806 -0.25968766]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 358 is [True, False, False, False, True, False]
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 358 of 1
Current timestep = 359. State = [[-0.06069082  0.02156793]]. Action = [[ 0.12789676 -0.18436797 -0.23507498  0.9116677 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 359 is [True, False, False, False, True, False]
Current timestep = 360. State = [[-0.05145947  0.00521635]]. Action = [[-0.10231681 -0.07618582  0.21171421 -0.69661355]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 360 is [True, False, False, False, True, False]
Current timestep = 361. State = [[-0.05490454 -0.00301747]]. Action = [[-0.19614567 -0.01839675 -0.00213432 -0.41950488]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 361 is [True, False, False, False, True, False]
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 361 of -1
Current timestep = 362. State = [[-0.06362959  0.00483186]]. Action = [[0.11211899 0.22934663 0.05310097 0.6270926 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 362 is [True, False, False, False, True, False]
Scene graph at timestep 362 is [True, False, False, False, True, False]
State prediction error at timestep 362 is tensor(9.3824e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 362 of 1
Current timestep = 363. State = [[-0.07260875  0.00750151]]. Action = [[-0.22977372 -0.2462115  -0.17143813  0.5422572 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 363 is [True, False, False, False, True, False]
Scene graph at timestep 363 is [True, False, False, False, True, False]
State prediction error at timestep 363 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 363 of -1
Current timestep = 364. State = [[-0.08126231 -0.0098586 ]]. Action = [[ 0.02441397 -0.02891076  0.2041831  -0.4894262 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 364 is [True, False, False, False, True, False]
Current timestep = 365. State = [[-0.07608108 -0.01134952]]. Action = [[ 0.23553962  0.0200147  -0.09706482 -0.654643  ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 365 is [True, False, False, False, True, False]
Scene graph at timestep 365 is [True, False, False, False, True, False]
State prediction error at timestep 365 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 365 of 1
Current timestep = 366. State = [[-0.06870542 -0.02227519]]. Action = [[-0.07120708 -0.18593319 -0.00863014 -0.82460386]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 366 is [True, False, False, False, True, False]
Scene graph at timestep 366 is [True, False, False, False, True, False]
State prediction error at timestep 366 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 366 of -1
Current timestep = 367. State = [[-0.06469917 -0.04701591]]. Action = [[ 0.05046809 -0.16484867  0.21693218 -0.62064934]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 367 is [True, False, False, False, True, False]
Current timestep = 368. State = [[-0.05744939 -0.07275014]]. Action = [[ 0.19636738 -0.21637292  0.0590986   0.34643006]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 368 is [True, False, False, False, True, False]
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of -1
Current timestep = 369. State = [[-0.04635616 -0.09338234]]. Action = [[-0.09006765  0.04695773 -0.00241615 -0.889751  ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 369 is [True, False, False, False, True, False]
Current timestep = 370. State = [[-0.05092488 -0.09168909]]. Action = [[-0.18012455  0.01631784  0.11314696  0.9265779 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 370 is [False, True, False, False, True, False]
Scene graph at timestep 370 is [True, False, False, False, True, False]
State prediction error at timestep 370 is tensor(2.8605e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 370 of -1
Current timestep = 371. State = [[-0.06391651 -0.08380498]]. Action = [[-0.16519898  0.10778189  0.23597324  0.5802194 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 371 is [True, False, False, False, True, False]
Current timestep = 372. State = [[-0.07324276 -0.06977297]]. Action = [[ 0.1120674   0.09804934 -0.21363173 -0.42744803]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 372 is [True, False, False, False, True, False]
Scene graph at timestep 372 is [True, False, False, False, True, False]
State prediction error at timestep 372 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 372 of 1
Current timestep = 373. State = [[-0.07738631 -0.07465349]]. Action = [[-0.12723605 -0.23874809 -0.14658752 -0.08925533]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 373 is [True, False, False, False, True, False]
Scene graph at timestep 373 is [True, False, False, False, True, False]
State prediction error at timestep 373 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 373 of -1
Current timestep = 374. State = [[-0.08950666 -0.09792905]]. Action = [[-0.11914115 -0.11033364  0.2135309  -0.8798078 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 374 is [True, False, False, False, True, False]
Scene graph at timestep 374 is [True, False, False, False, True, False]
State prediction error at timestep 374 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 374 of -1
Current timestep = 375. State = [[-0.10733752 -0.09472875]]. Action = [[-0.17696695  0.23847085  0.14173806  0.9423094 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 375 is [True, False, False, False, True, False]
Scene graph at timestep 375 is [True, False, False, False, True, False]
State prediction error at timestep 375 is tensor(2.1123e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of -1
Current timestep = 376. State = [[-0.12130671 -0.0763837 ]]. Action = [[0.02392578 0.03318045 0.02273986 0.4387902 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 376 is [True, False, False, False, True, False]
Current timestep = 377. State = [[-0.12874185 -0.0680824 ]]. Action = [[-0.20386924  0.08123454  0.19030082  0.18912709]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 377 is [True, False, False, False, True, False]
Scene graph at timestep 377 is [True, False, False, False, True, False]
State prediction error at timestep 377 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of -1
Current timestep = 378. State = [[-0.14931457 -0.07426936]]. Action = [[-0.04317933 -0.23265453  0.11965203  0.9627429 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 378 is [True, False, False, False, True, False]
Scene graph at timestep 378 is [True, False, False, False, True, False]
State prediction error at timestep 378 is tensor(8.1471e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 378 of -1
Current timestep = 379. State = [[-0.1615174  -0.10038012]]. Action = [[ 0.01058048 -0.16461363 -0.21387044 -0.66498905]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 379 is [True, False, False, False, True, False]
Scene graph at timestep 379 is [True, False, False, False, True, False]
State prediction error at timestep 379 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 379 of -1
Current timestep = 380. State = [[-0.16708316 -0.10674293]]. Action = [[-0.18149444  0.14737129 -0.12613927 -0.5550721 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 380 is [True, False, False, False, True, False]
Scene graph at timestep 380 is [True, False, False, False, True, False]
State prediction error at timestep 380 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 380 of -1
Current timestep = 381. State = [[-0.1831542  -0.08997351]]. Action = [[ 0.05406588  0.16274774 -0.23808977  0.39138436]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 381 is [True, False, False, False, True, False]
Scene graph at timestep 381 is [True, False, False, False, True, False]
State prediction error at timestep 381 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 381 of -1
Current timestep = 382. State = [[-0.18283635 -0.08660223]]. Action = [[ 0.0206275  -0.20050383 -0.2145977  -0.27217674]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 382 is [True, False, False, False, True, False]
Scene graph at timestep 382 is [True, False, False, False, True, False]
State prediction error at timestep 382 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 382 of -1
Current timestep = 383. State = [[-0.17495087 -0.10799736]]. Action = [[ 0.19543806 -0.16974208  0.07151151 -0.18844986]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 383 is [True, False, False, False, True, False]
Current timestep = 384. State = [[-0.15800853 -0.12922141]]. Action = [[ 0.23780733 -0.12046127 -0.10141332 -0.2968818 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 384 is [True, False, False, False, True, False]
Current timestep = 385. State = [[-0.13815294 -0.14863381]]. Action = [[ 0.0375993  -0.13572979  0.19166836 -0.9613648 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 385 is [True, False, False, True, False, False]
Scene graph at timestep 385 is [True, False, False, True, False, False]
State prediction error at timestep 385 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 385 of -1
Current timestep = 386. State = [[-0.12603942 -0.16106503]]. Action = [[ 0.12705994 -0.01860915  0.00290611  0.94008005]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 386 is [True, False, False, True, False, False]
Scene graph at timestep 386 is [True, False, False, True, False, False]
State prediction error at timestep 386 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.10358758 -0.16747084]]. Action = [[ 0.24110472 -0.08603123  0.14861435 -0.6744599 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 387 is [True, False, False, True, False, False]
Current timestep = 388. State = [[-0.08877952 -0.18722634]]. Action = [[-0.10499899 -0.21824571 -0.16941737 -0.80441016]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 388 is [True, False, False, True, False, False]
Current timestep = 389. State = [[-0.08315355 -0.20189476]]. Action = [[ 0.15757889  0.04267925  0.12501332 -0.06263685]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 389 is [True, False, False, True, False, False]
Scene graph at timestep 389 is [True, False, False, True, False, False]
State prediction error at timestep 389 is tensor(5.0323e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 389 of -1
Current timestep = 390. State = [[-0.07549026 -0.19168673]]. Action = [[-0.22316879  0.21542159 -0.0436797   0.6142895 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 390 is [True, False, False, True, False, False]
Current timestep = 391. State = [[-0.07288685 -0.1665634 ]]. Action = [[ 0.23077899  0.22234005  0.15669265 -0.7024027 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 391 is [True, False, False, True, False, False]
Scene graph at timestep 391 is [True, False, False, True, False, False]
State prediction error at timestep 391 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 391 of 1
Current timestep = 392. State = [[-0.05943743 -0.15405902]]. Action = [[ 0.164164   -0.1811892   0.08702654 -0.53840786]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 392 is [True, False, False, True, False, False]
Current timestep = 393. State = [[-0.03829383 -0.1586261 ]]. Action = [[ 0.20384035  0.05994838  0.03168166 -0.02930552]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 393 is [True, False, False, True, False, False]
Current timestep = 394. State = [[-0.01268533 -0.1572465 ]]. Action = [[ 0.16750193 -0.0380241  -0.23314959  0.13250601]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 394 is [False, True, False, True, False, False]
Current timestep = 395. State = [[ 0.00640431 -0.14773688]]. Action = [[-0.0470843   0.24605763 -0.23636629  0.0816716 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 395 is [False, True, False, True, False, False]
Current timestep = 396. State = [[ 0.01527785 -0.13693896]]. Action = [[ 0.19350284 -0.07531047 -0.06511006 -0.35120142]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 396 is [False, True, False, True, False, False]
Current timestep = 397. State = [[ 0.03651539 -0.1420297 ]]. Action = [[ 0.16288128 -0.09856208 -0.24944931 -0.90118194]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 397 is [False, True, False, True, False, False]
Scene graph at timestep 397 is [False, True, False, True, False, False]
State prediction error at timestep 397 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 397 of -1
Current timestep = 398. State = [[ 0.05735407 -0.15004994]]. Action = [[0.20051026 0.16806087 0.14433327 0.89109397]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 398 is [False, True, False, True, False, False]
Scene graph at timestep 398 is [False, False, True, True, False, False]
State prediction error at timestep 398 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 398 of -1
Current timestep = 399. State = [[ 0.05615017 -0.14205313]]. Action = [[-0.18847595  0.17181605 -0.18054062  0.59021544]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 399 is [False, False, True, True, False, False]
Scene graph at timestep 399 is [False, False, True, True, False, False]
State prediction error at timestep 399 is tensor(5.7287e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 399 of -1
Current timestep = 400. State = [[ 0.0511732 -0.1384352]]. Action = [[-0.08843017 -0.09248953 -0.21742934 -0.0930506 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 400 is [False, False, True, True, False, False]
Scene graph at timestep 400 is [False, False, True, True, False, False]
State prediction error at timestep 400 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 400 of 1
Current timestep = 401. State = [[ 0.04481409 -0.1450406 ]]. Action = [[ 0.09927869  0.01591036  0.01930609 -0.3936401 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 401 is [False, False, True, True, False, False]
Current timestep = 402. State = [[ 0.04536611 -0.14958647]]. Action = [[ 0.02174777 -0.11214823  0.01879656  0.64581656]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 402 is [False, True, False, True, False, False]
Current timestep = 403. State = [[ 0.04550291 -0.15293504]]. Action = [[ 0.20892662 -0.17204086  0.0461551   0.89569676]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 403 is [False, True, False, True, False, False]
Scene graph at timestep 403 is [False, True, False, True, False, False]
State prediction error at timestep 403 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 403 of -1
Current timestep = 404. State = [[ 0.04661115 -0.163738  ]]. Action = [[ 0.08975643 -0.17315853 -0.01316687  0.0590899 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 404 is [False, True, False, True, False, False]
Scene graph at timestep 404 is [False, True, False, True, False, False]
State prediction error at timestep 404 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 404 of -1
Current timestep = 405. State = [[ 0.04826872 -0.16966946]]. Action = [[-0.09784457  0.17277855 -0.23733163  0.32261467]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 405 is [False, True, False, True, False, False]
Scene graph at timestep 405 is [False, True, False, True, False, False]
State prediction error at timestep 405 is tensor(8.2100e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of 1
Current timestep = 406. State = [[ 0.0484562 -0.1613144]]. Action = [[ 0.18599486 -0.03202325  0.15949926 -0.5487522 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 406 is [False, True, False, True, False, False]
Scene graph at timestep 406 is [False, True, False, True, False, False]
State prediction error at timestep 406 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 406 of -1
Current timestep = 407. State = [[ 0.04850093 -0.16777258]]. Action = [[ 0.04896539 -0.14172618  0.08809417 -0.6864439 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 407 is [False, True, False, True, False, False]
Current timestep = 408. State = [[ 0.04815812 -0.16578946]]. Action = [[-0.10348244  0.15677941  0.02359933 -0.41121542]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 408 is [False, True, False, True, False, False]
Scene graph at timestep 408 is [False, True, False, True, False, False]
State prediction error at timestep 408 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 408 of 1
Current timestep = 409. State = [[ 0.04786152 -0.14571804]]. Action = [[-0.04214342  0.23018909 -0.00476716 -0.23178524]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 409 is [False, True, False, True, False, False]
Scene graph at timestep 409 is [False, True, False, True, False, False]
State prediction error at timestep 409 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 409 of 1
Current timestep = 410. State = [[ 0.04442055 -0.13185288]]. Action = [[ 0.05438927 -0.13824981 -0.09249964  0.10842776]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 410 is [False, True, False, True, False, False]
Current timestep = 411. State = [[ 0.04058655 -0.12670873]]. Action = [[-0.16286093  0.20323688 -0.02528147  0.3983574 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 411 is [False, True, False, True, False, False]
Scene graph at timestep 411 is [False, True, False, True, False, False]
State prediction error at timestep 411 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 411 of -1
Current timestep = 412. State = [[-0.21688014 -0.15320991]]. Action = [[-0.06043956  0.2072435   0.07404238  0.72495925]]. Reward = [100.]
Curr episode timestep = 82
Scene graph at timestep 412 is [False, True, False, True, False, False]
Current timestep = 413. State = [[-0.1989431  -0.17504762]]. Action = [[ 0.21330273 -0.08747715  0.21013483 -0.55880195]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 413 is [True, False, False, True, False, False]
Scene graph at timestep 413 is [True, False, False, True, False, False]
State prediction error at timestep 413 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 413 of -1
Current timestep = 414. State = [[-0.17944027 -0.17280641]]. Action = [[0.00172663 0.20535362 0.23534316 0.62448835]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 414 is [True, False, False, True, False, False]
Scene graph at timestep 414 is [True, False, False, True, False, False]
State prediction error at timestep 414 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 414 of 1
Current timestep = 415. State = [[-0.1803705 -0.1711113]]. Action = [[-0.07529673 -0.17652147 -0.05583563  0.67168057]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 415 is [True, False, False, True, False, False]
Current timestep = 416. State = [[-0.18166605 -0.17509604]]. Action = [[0.0572958  0.07974905 0.23523653 0.9507494 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 416 is [True, False, False, True, False, False]
Scene graph at timestep 416 is [True, False, False, True, False, False]
State prediction error at timestep 416 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 416 of -1
Current timestep = 417. State = [[-0.17070952 -0.18458082]]. Action = [[ 0.20873451 -0.2153749   0.01523256  0.19835162]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 417 is [True, False, False, True, False, False]
Scene graph at timestep 417 is [True, False, False, True, False, False]
State prediction error at timestep 417 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 417 of -1
Current timestep = 418. State = [[-0.1417663  -0.19885969]]. Action = [[0.20909959 0.01675567 0.12504762 0.90743685]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 418 is [True, False, False, True, False, False]
Current timestep = 419. State = [[-0.11787401 -0.20433061]]. Action = [[ 0.14679474 -0.05138974  0.21471947 -0.09152937]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 419 is [True, False, False, True, False, False]
Current timestep = 420. State = [[-0.10269214 -0.2080875 ]]. Action = [[-0.02157244  0.02129838 -0.02964441 -0.12811208]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 420 is [True, False, False, True, False, False]
Current timestep = 421. State = [[-0.10529505 -0.19681883]]. Action = [[-0.16268748  0.22984684  0.09544638  0.6177523 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 421 is [True, False, False, True, False, False]
Current timestep = 422. State = [[-0.11607669 -0.16819781]]. Action = [[-0.22072972  0.2244448   0.15464234 -0.30477697]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 422 is [True, False, False, True, False, False]
Scene graph at timestep 422 is [True, False, False, True, False, False]
State prediction error at timestep 422 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 422 of -1
Current timestep = 423. State = [[-0.12671252 -0.15459612]]. Action = [[ 0.24462003 -0.1827931   0.14561635 -0.26721787]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 423 is [True, False, False, True, False, False]
Current timestep = 424. State = [[-0.12868653 -0.15965027]]. Action = [[-0.23215468  0.06329718 -0.12939718 -0.62876236]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 424 is [True, False, False, True, False, False]
Scene graph at timestep 424 is [True, False, False, True, False, False]
State prediction error at timestep 424 is tensor(7.9858e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 424 of -1
Current timestep = 425. State = [[-0.1329041  -0.16657314]]. Action = [[ 0.09611753 -0.1132787   0.06867576 -0.9566428 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 425 is [True, False, False, True, False, False]
Scene graph at timestep 425 is [True, False, False, True, False, False]
State prediction error at timestep 425 is tensor(8.4512e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 425 of -1
Current timestep = 426. State = [[-0.12644333 -0.17965594]]. Action = [[ 0.18737611 -0.14581046 -0.22908323 -0.37681067]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 426 is [True, False, False, True, False, False]
Scene graph at timestep 426 is [True, False, False, True, False, False]
State prediction error at timestep 426 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 426 of 1
Current timestep = 427. State = [[-0.10581537 -0.20440575]]. Action = [[ 0.15935871 -0.22130537  0.03482389 -0.5080194 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 427 is [True, False, False, True, False, False]
Current timestep = 428. State = [[-0.08512133 -0.21511616]]. Action = [[ 0.15790892  0.09008235 -0.22932819  0.643417  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 428 is [True, False, False, True, False, False]
Scene graph at timestep 428 is [True, False, False, True, False, False]
State prediction error at timestep 428 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 428 of 1
Current timestep = 429. State = [[-0.05823063 -0.20100601]]. Action = [[ 0.19299018  0.23448747 -0.21500768 -0.31404006]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 429 is [True, False, False, True, False, False]
Scene graph at timestep 429 is [True, False, False, True, False, False]
State prediction error at timestep 429 is tensor(5.3791e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 429 of 1
Current timestep = 430. State = [[-0.04526963 -0.18666656]]. Action = [[-0.20586543 -0.03360009 -0.05030087 -0.30247414]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 430 is [True, False, False, True, False, False]
Current timestep = 431. State = [[-0.04744238 -0.18499182]]. Action = [[ 0.05476791  0.06826833 -0.13998455 -0.3562932 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 431 is [False, True, False, True, False, False]
Current timestep = 432. State = [[-0.05188688 -0.1700655 ]]. Action = [[-0.1651395   0.19931543 -0.0305007   0.93920183]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 432 is [False, True, False, True, False, False]
Scene graph at timestep 432 is [True, False, False, True, False, False]
State prediction error at timestep 432 is tensor(7.3702e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 432 of 1
Current timestep = 433. State = [[-0.06096833 -0.1480335 ]]. Action = [[ 0.06434432  0.04822254  0.07974574 -0.13164663]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 433 is [True, False, False, True, False, False]
Scene graph at timestep 433 is [True, False, False, True, False, False]
State prediction error at timestep 433 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 433 of -1
Current timestep = 434. State = [[-0.05438488 -0.13938725]]. Action = [[ 0.2055448   0.05652007 -0.24126223 -0.958793  ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 434 is [True, False, False, True, False, False]
Scene graph at timestep 434 is [True, False, False, True, False, False]
State prediction error at timestep 434 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 434 of 1
Current timestep = 435. State = [[-0.04482398 -0.12051665]]. Action = [[0.01540065 0.22220567 0.20161739 0.8430817 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 435 is [True, False, False, True, False, False]
Scene graph at timestep 435 is [False, True, False, False, True, False]
State prediction error at timestep 435 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 435 of 1
Current timestep = 436. State = [[-0.04596482 -0.10081655]]. Action = [[-0.18510295  0.0205276  -0.05828108  0.11350143]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 436 is [False, True, False, False, True, False]
Scene graph at timestep 436 is [False, True, False, False, True, False]
State prediction error at timestep 436 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 436 of 1
Current timestep = 437. State = [[-0.04860993 -0.08651306]]. Action = [[-0.01206608  0.20855188  0.14192283 -0.09041339]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 437 is [False, True, False, False, True, False]
Current timestep = 438. State = [[-0.0505945 -0.079092 ]]. Action = [[ 0.10488579 -0.15386187  0.13522536 -0.9785329 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 438 is [False, True, False, False, True, False]
Current timestep = 439. State = [[-0.05190039 -0.07109523]]. Action = [[-0.06725892  0.24310458  0.02253231 -0.1040619 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 439 is [True, False, False, False, True, False]
Scene graph at timestep 439 is [True, False, False, False, True, False]
State prediction error at timestep 439 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 439 of 1
Current timestep = 440. State = [[-0.06154708 -0.05029403]]. Action = [[-0.23854539  0.05221969  0.19357899 -0.5420015 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 440 is [True, False, False, False, True, False]
Scene graph at timestep 440 is [True, False, False, False, True, False]
State prediction error at timestep 440 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 440 of -1
Current timestep = 441. State = [[-0.0735063 -0.0376962]]. Action = [[ 0.17570245  0.1159527  -0.20216063 -0.9344043 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 441 is [True, False, False, False, True, False]
Current timestep = 442. State = [[-0.07224821 -0.03821073]]. Action = [[-0.03582434 -0.13296941  0.06167069  0.89773357]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 442 is [True, False, False, False, True, False]
Current timestep = 443. State = [[-0.07251268 -0.04306566]]. Action = [[-0.04522711 -0.00943281 -0.09276924 -0.8714697 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 443 is [True, False, False, False, True, False]
Scene graph at timestep 443 is [True, False, False, False, True, False]
State prediction error at timestep 443 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 443 of -1
Current timestep = 444. State = [[-0.06638055 -0.04576634]]. Action = [[ 0.23409647 -0.00139515  0.10954317  0.651119  ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 444 is [True, False, False, False, True, False]
Current timestep = 445. State = [[-0.05876742 -0.04218802]]. Action = [[-0.17699678  0.06245568 -0.06956072  0.87975955]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 445 is [True, False, False, False, True, False]
Current timestep = 446. State = [[-0.06069239 -0.03226624]]. Action = [[0.06972679 0.12338889 0.16390619 0.18059182]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 446 is [True, False, False, False, True, False]
Scene graph at timestep 446 is [True, False, False, False, True, False]
State prediction error at timestep 446 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 446 of 1
Current timestep = 447. State = [[-0.05954895 -0.02134402]]. Action = [[ 0.10883808  0.03754973 -0.20576194 -0.87673527]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 447 is [True, False, False, False, True, False]
Current timestep = 448. State = [[-0.05242633 -0.02293688]]. Action = [[ 0.07349557 -0.08532029  0.19396165 -0.62803537]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 448 is [True, False, False, False, True, False]
Current timestep = 449. State = [[-0.0447202  -0.03528478]]. Action = [[ 0.05610719 -0.17491746 -0.1646481  -0.7168958 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 449 is [True, False, False, False, True, False]
Current timestep = 450. State = [[-0.04095181 -0.03755436]]. Action = [[-0.11190039  0.16882682 -0.20530298 -0.05189294]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 450 is [False, True, False, False, True, False]
Scene graph at timestep 450 is [False, True, False, False, True, False]
State prediction error at timestep 450 is tensor(4.0248e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 450 of 1
Current timestep = 451. State = [[-0.04314377 -0.02725613]]. Action = [[-0.02989148  0.04458559  0.21174565 -0.18752491]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 451 is [False, True, False, False, True, False]
Current timestep = 452. State = [[-0.04914982 -0.01622578]]. Action = [[-0.19957738  0.11206812 -0.04776987 -0.4633484 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 452 is [False, True, False, False, True, False]
Current timestep = 453. State = [[-0.05928042  0.00530685]]. Action = [[ 0.03899065  0.18899727 -0.19632834 -0.828373  ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 453 is [False, True, False, False, True, False]
Current timestep = 454. State = [[-0.06615848  0.01457045]]. Action = [[-0.14093815 -0.076507   -0.08940077  0.817395  ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 454 is [True, False, False, False, True, False]
Current timestep = 455. State = [[-0.07082206  0.01989842]]. Action = [[ 0.16088253  0.12105307  0.12097788 -0.5138812 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 455 is [True, False, False, False, True, False]
Current timestep = 456. State = [[-0.06581553  0.01694259]]. Action = [[ 0.12556851 -0.16459948  0.06207347 -0.00955349]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 456 is [True, False, False, False, True, False]
Current timestep = 457. State = [[-0.0631547  -0.00478722]]. Action = [[-0.16828246 -0.23559405 -0.18315443 -0.54846364]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 457 is [True, False, False, False, True, False]
Current timestep = 458. State = [[-0.06336319 -0.01709088]]. Action = [[ 0.12968668  0.06153896 -0.19903426 -0.8497882 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 458 is [True, False, False, False, True, False]
Scene graph at timestep 458 is [True, False, False, False, True, False]
State prediction error at timestep 458 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 458 of 1
Current timestep = 459. State = [[-0.06248952 -0.01828546]]. Action = [[-0.0776327  -0.03869596  0.0091497  -0.94235367]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 459 is [True, False, False, False, True, False]
Scene graph at timestep 459 is [True, False, False, False, True, False]
State prediction error at timestep 459 is tensor(6.9010e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 459 of -1
Current timestep = 460. State = [[-0.06848552 -0.01245657]]. Action = [[-0.18293424  0.15241149  0.01776367 -0.39747798]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 460 is [True, False, False, False, True, False]
Scene graph at timestep 460 is [True, False, False, False, True, False]
State prediction error at timestep 460 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of 1
Current timestep = 461. State = [[-0.07318827  0.00607821]]. Action = [[ 0.19819081  0.17448169 -0.0984139  -0.49420464]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 461 is [True, False, False, False, True, False]
Current timestep = 462. State = [[-0.06610487  0.03182769]]. Action = [[0.14040315 0.24169397 0.05274254 0.51811457]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 462 is [True, False, False, False, True, False]
Current timestep = 463. State = [[-0.06164446  0.06369454]]. Action = [[-0.14739373  0.19982845  0.09222925 -0.49909157]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 463 is [True, False, False, False, True, False]
Current timestep = 464. State = [[-0.07249649  0.09394384]]. Action = [[-0.11699292  0.18520644  0.07486993 -0.38087362]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 464 is [True, False, False, False, True, False]
Scene graph at timestep 464 is [True, False, False, False, True, False]
State prediction error at timestep 464 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 464 of -1
Current timestep = 465. State = [[-0.08458067  0.12204815]]. Action = [[-0.09289178  0.10642979  0.2278127  -0.44644284]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 465 is [True, False, False, False, True, False]
Scene graph at timestep 465 is [True, False, False, False, True, False]
State prediction error at timestep 465 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of -1
Current timestep = 466. State = [[-0.09839404  0.1405989 ]]. Action = [[-0.21056369  0.11463964 -0.04012796 -0.59049565]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 466 is [True, False, False, False, True, False]
Scene graph at timestep 466 is [True, False, False, False, False, True]
State prediction error at timestep 466 is tensor(2.9310e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 466 of -1
Current timestep = 467. State = [[-0.11010692  0.14861627]]. Action = [[ 0.04634672 -0.08552039  0.02228445 -0.88558656]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 467 is [True, False, False, False, False, True]
Current timestep = 468. State = [[-0.10897855  0.13831238]]. Action = [[-0.0319638  -0.10228345 -0.11900747  0.48306358]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 468 is [True, False, False, False, False, True]
Scene graph at timestep 468 is [True, False, False, False, False, True]
State prediction error at timestep 468 is tensor(5.5778e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 468 of -1
Current timestep = 469. State = [[-0.12185065  0.12940887]]. Action = [[-0.22297436  0.00757372  0.20031577  0.1220665 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 469 is [True, False, False, False, False, True]
Scene graph at timestep 469 is [True, False, False, False, False, True]
State prediction error at timestep 469 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 469 of -1
Current timestep = 470. State = [[-0.14530621  0.12497138]]. Action = [[-0.12297529 -0.04529704 -0.02238856 -0.15875643]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 470 is [True, False, False, False, False, True]
Scene graph at timestep 470 is [True, False, False, False, True, False]
State prediction error at timestep 470 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 470 of -1
Current timestep = 471. State = [[-0.15037863  0.11944065]]. Action = [[ 0.24418047 -0.01546264  0.19741017  0.43273938]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 471 is [True, False, False, False, True, False]
Current timestep = 472. State = [[-0.14860262  0.11104415]]. Action = [[-0.18478505 -0.13381062 -0.19115685  0.717376  ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 472 is [True, False, False, False, True, False]
Current timestep = 473. State = [[-0.16329691  0.09840974]]. Action = [[-0.22982867 -0.05349775  0.13626954 -0.5520177 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 473 is [True, False, False, False, True, False]
Current timestep = 474. State = [[-0.17118333  0.09638077]]. Action = [[0.24781966 0.09385309 0.21955681 0.9529693 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 474 is [True, False, False, False, True, False]
Scene graph at timestep 474 is [True, False, False, False, True, False]
State prediction error at timestep 474 is tensor(2.0035e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 474 of -1
Current timestep = 475. State = [[-0.1708344   0.08851231]]. Action = [[-0.15051077 -0.1978486   0.14115912 -0.21608758]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 475 is [True, False, False, False, True, False]
Scene graph at timestep 475 is [True, False, False, False, True, False]
State prediction error at timestep 475 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 475 of -1
Current timestep = 476. State = [[-0.16684416  0.06276011]]. Action = [[ 0.22913861 -0.21812488  0.09648702  0.33189845]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 476 is [True, False, False, False, True, False]
Current timestep = 477. State = [[-0.14742832  0.05705488]]. Action = [[ 0.222155    0.19259357  0.2087549  -0.02629286]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 477 is [True, False, False, False, True, False]
Current timestep = 478. State = [[-0.13233282  0.05780929]]. Action = [[ 0.00581142 -0.11427251 -0.13328014  0.9042101 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 478 is [True, False, False, False, True, False]
Scene graph at timestep 478 is [True, False, False, False, True, False]
State prediction error at timestep 478 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 478 of 1
Current timestep = 479. State = [[-0.13255739  0.06188765]]. Action = [[-0.17246969  0.15258092  0.04936868 -0.1338082 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 479 is [True, False, False, False, True, False]
Current timestep = 480. State = [[-0.13403933  0.07359254]]. Action = [[ 0.12904516  0.07070082 -0.01430832 -0.673808  ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 480 is [True, False, False, False, True, False]
Scene graph at timestep 480 is [True, False, False, False, True, False]
State prediction error at timestep 480 is tensor(1.7546e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 480 of -1
Current timestep = 481. State = [[-0.13978449  0.08516653]]. Action = [[-0.24444564  0.06317058 -0.15910612 -0.89402246]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 481 is [True, False, False, False, True, False]
Scene graph at timestep 481 is [True, False, False, False, True, False]
State prediction error at timestep 481 is tensor(3.2835e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 481 of -1
Current timestep = 482. State = [[-0.15096305  0.08701924]]. Action = [[-0.05054535 -0.13261108  0.21728063 -0.63960785]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 482 is [True, False, False, False, True, False]
Scene graph at timestep 482 is [True, False, False, False, True, False]
State prediction error at timestep 482 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 482 of -1
Current timestep = 483. State = [[-0.16243742  0.07814778]]. Action = [[-0.18141468  0.00148118 -0.21518537 -0.905781  ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 483 is [True, False, False, False, True, False]
Scene graph at timestep 483 is [True, False, False, False, True, False]
State prediction error at timestep 483 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 483 of -1
Current timestep = 484. State = [[-0.17419024  0.0830237 ]]. Action = [[0.18923387 0.14621612 0.17801028 0.3700546 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 484 is [True, False, False, False, True, False]
Scene graph at timestep 484 is [True, False, False, False, True, False]
State prediction error at timestep 484 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 484 of -1
Current timestep = 485. State = [[-0.17370157  0.1031345 ]]. Action = [[ 0.00364876  0.19657123 -0.04890637  0.45113862]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 485 is [True, False, False, False, True, False]
Scene graph at timestep 485 is [True, False, False, False, True, False]
State prediction error at timestep 485 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 485 of -1
Current timestep = 486. State = [[-0.17449589  0.12229768]]. Action = [[-0.05285068  0.05399203 -0.19327007 -0.61624247]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 486 is [True, False, False, False, True, False]
Current timestep = 487. State = [[-0.17467463  0.11425032]]. Action = [[-0.01999393 -0.2457806  -0.16188246 -0.97342896]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 487 is [True, False, False, False, True, False]
Scene graph at timestep 487 is [True, False, False, False, True, False]
State prediction error at timestep 487 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 487 of -1
Current timestep = 488. State = [[-0.18147722  0.09111138]]. Action = [[-0.22448623 -0.1819229   0.20608729 -0.83048946]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 488 is [True, False, False, False, True, False]
Scene graph at timestep 488 is [True, False, False, False, True, False]
State prediction error at timestep 488 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 488 of -1
Current timestep = 489. State = [[-0.19764279  0.07958666]]. Action = [[ 0.00247458  0.10734212 -0.04543188  0.11250615]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 489 is [True, False, False, False, True, False]
Current timestep = 490. State = [[-0.20898792  0.09128213]]. Action = [[-0.23543131  0.08301285  0.22399431  0.61436224]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 490 is [True, False, False, False, True, False]
Scene graph at timestep 490 is [True, False, False, False, True, False]
State prediction error at timestep 490 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 490 of -1
Current timestep = 491. State = [[-0.23123643  0.09018562]]. Action = [[-0.1271156  -0.20452681 -0.23473035 -0.80866355]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 491 is [True, False, False, False, True, False]
Current timestep = 492. State = [[-0.23854509  0.06633971]]. Action = [[ 0.14083588 -0.1517256   0.14231664  0.6594163 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 492 is [True, False, False, False, True, False]
Scene graph at timestep 492 is [True, False, False, False, True, False]
State prediction error at timestep 492 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 492 of 1
Current timestep = 493. State = [[-0.24457152  0.04613727]]. Action = [[-0.20533392 -0.07643893  0.21507764 -0.73606   ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 493 is [True, False, False, False, True, False]
Scene graph at timestep 493 is [True, False, False, False, True, False]
State prediction error at timestep 493 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 493 of -1
Current timestep = 494. State = [[-0.26006928  0.03535496]]. Action = [[-0.12233876 -0.23549925  0.21313798  0.35161316]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 494 is [True, False, False, False, True, False]
Current timestep = 495. State = [[-0.25850263  0.0236911 ]]. Action = [[ 0.05580166 -0.20307948 -0.2374699   0.95058465]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 495 is [True, False, False, False, True, False]
Scene graph at timestep 495 is [True, False, False, False, True, False]
State prediction error at timestep 495 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 495 of -1
Current timestep = 496. State = [[-0.2562678   0.00851708]]. Action = [[ 0.01126882 -0.00452867  0.17641741  0.1261481 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 496 is [True, False, False, False, True, False]
Scene graph at timestep 496 is [True, False, False, False, True, False]
State prediction error at timestep 496 is tensor(5.2542e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 496 of -1
Current timestep = 497. State = [[-0.25592038  0.00797426]]. Action = [[-0.01749164  0.02590263 -0.01939248  0.9904442 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 497 is [True, False, False, False, True, False]
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(3.7619e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of -1
Current timestep = 498. State = [[-0.25846717  0.00722689]]. Action = [[-0.08885702 -0.02723812 -0.19103974 -0.9308639 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 498 is [True, False, False, False, True, False]
Scene graph at timestep 498 is [True, False, False, False, True, False]
State prediction error at timestep 498 is tensor(8.0571e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 498 of -1
Current timestep = 499. State = [[-0.26823473  0.01652553]]. Action = [[0.08573765 0.24543303 0.16802132 0.89920354]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 499 is [True, False, False, False, True, False]
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(2.2431e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of -1
Current timestep = 500. State = [[-0.26862612  0.03152205]]. Action = [[ 0.01884755 -0.02892472  0.16781792 -0.7364689 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 500 is [True, False, False, False, True, False]
Scene graph at timestep 500 is [True, False, False, False, True, False]
State prediction error at timestep 500 is tensor(3.7467e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 500 of -1
Current timestep = 501. State = [[-0.26597738  0.02725702]]. Action = [[ 0.01881143 -0.08567667 -0.13973086  0.14174569]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 501 is [True, False, False, False, True, False]
Scene graph at timestep 501 is [True, False, False, False, True, False]
State prediction error at timestep 501 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[-0.26064733  0.02290542]]. Action = [[ 0.01147449 -0.03199387 -0.04775554 -0.9519019 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 502 is [True, False, False, False, True, False]
Scene graph at timestep 502 is [True, False, False, False, True, False]
State prediction error at timestep 502 is tensor(9.8156e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 502 of -1
Current timestep = 503. State = [[-0.25766805  0.02197804]]. Action = [[ 0.10201576  0.05521637 -0.17649163  0.05904484]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 503 is [True, False, False, False, True, False]
Current timestep = 504. State = [[-0.25298324  0.01598346]]. Action = [[-0.05559185 -0.1477415  -0.04925448 -0.9814992 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 504 is [True, False, False, False, True, False]
Current timestep = 505. State = [[-0.24649659 -0.00354167]]. Action = [[ 0.07414562 -0.19062275 -0.22233273  0.43825674]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 505 is [True, False, False, False, True, False]
Current timestep = 506. State = [[-0.24143007 -0.0114557 ]]. Action = [[0.03649369 0.16488248 0.08367449 0.35073972]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 506 is [True, False, False, False, True, False]
Scene graph at timestep 506 is [True, False, False, False, True, False]
State prediction error at timestep 506 is tensor(5.7861e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 506 of -1
Current timestep = 507. State = [[-0.24476093 -0.01233174]]. Action = [[-0.1554647  -0.1333751  -0.08972609  0.79235744]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 507 is [True, False, False, False, True, False]
Current timestep = 508. State = [[-0.24128293 -0.032351  ]]. Action = [[ 0.24524587 -0.24187708 -0.18758647 -0.7744842 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 508 is [True, False, False, False, True, False]
Scene graph at timestep 508 is [True, False, False, False, True, False]
State prediction error at timestep 508 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 508 of -1
Current timestep = 509. State = [[-0.22955064 -0.06572549]]. Action = [[ 0.03077248 -0.16332728  0.19447798 -0.42508483]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 509 is [True, False, False, False, True, False]
Current timestep = 510. State = [[-0.22743821 -0.09179486]]. Action = [[-0.04932173 -0.22229911 -0.13299486  0.45964587]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 510 is [True, False, False, False, True, False]
Current timestep = 511. State = [[-0.22643444 -0.09956   ]]. Action = [[ 0.01992118  0.1992656  -0.09523737  0.8313987 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 511 is [True, False, False, False, True, False]
Scene graph at timestep 511 is [True, False, False, False, True, False]
State prediction error at timestep 511 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 511 of -1
Current timestep = 512. State = [[-0.21860607 -0.10170032]]. Action = [[ 0.18673056 -0.20942965 -0.17734048  0.8366585 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 512 is [True, False, False, False, True, False]
Scene graph at timestep 512 is [True, False, False, False, True, False]
State prediction error at timestep 512 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 512 of -1
Current timestep = 513. State = [[-0.20565023 -0.12765755]]. Action = [[ 0.00702104 -0.23807816 -0.18920429  0.40131164]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 513 is [True, False, False, False, True, False]
Current timestep = 514. State = [[-0.19227253 -0.14225473]]. Action = [[ 0.23909283  0.04720974 -0.23350753 -0.75982535]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 514 is [True, False, False, True, False, False]
Current timestep = 515. State = [[-0.16266486 -0.13049394]]. Action = [[ 0.22950315  0.21390757 -0.14433691  0.40832138]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 515 is [True, False, False, True, False, False]
Current timestep = 516. State = [[-0.13466762 -0.11744477]]. Action = [[0.11672342 0.02004406 0.15426701 0.90798604]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 516 is [True, False, False, True, False, False]
Scene graph at timestep 516 is [True, False, False, False, True, False]
State prediction error at timestep 516 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 516 of 1
Current timestep = 517. State = [[-0.11886872 -0.11048842]]. Action = [[ 0.04050362  0.02677071 -0.00805117 -0.90997005]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 517 is [True, False, False, False, True, False]
Scene graph at timestep 517 is [True, False, False, False, True, False]
State prediction error at timestep 517 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 517 of 1
Current timestep = 518. State = [[-0.11071294 -0.11435167]]. Action = [[ 0.09853229 -0.12792614  0.11753553 -0.34135562]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 518 is [True, False, False, False, True, False]
Current timestep = 519. State = [[-0.09345126 -0.11091435]]. Action = [[0.17645174 0.17307901 0.01592186 0.29189706]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 519 is [True, False, False, False, True, False]
Scene graph at timestep 519 is [True, False, False, False, True, False]
State prediction error at timestep 519 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 519 of 1
Current timestep = 520. State = [[-0.07725874 -0.09328588]]. Action = [[-0.22738719  0.13907328  0.11666116 -0.66265464]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 520 is [True, False, False, False, True, False]
Current timestep = 521. State = [[-0.08215567 -0.08064272]]. Action = [[ 0.08381909  0.05514178 -0.07046026 -0.37945247]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 521 is [True, False, False, False, True, False]
Scene graph at timestep 521 is [True, False, False, False, True, False]
State prediction error at timestep 521 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 521 of 1
Current timestep = 522. State = [[-0.07805478 -0.0830688 ]]. Action = [[ 0.14548299 -0.1609356   0.17303705 -0.59339947]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 522 is [True, False, False, False, True, False]
Scene graph at timestep 522 is [True, False, False, False, True, False]
State prediction error at timestep 522 is tensor(6.5805e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 522 of 1
Current timestep = 523. State = [[-0.07388293 -0.09139884]]. Action = [[-0.01101202  0.0050213   0.07100576 -0.152892  ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 523 is [True, False, False, False, True, False]
Current timestep = 524. State = [[-0.0776272  -0.08899004]]. Action = [[-0.19616812  0.06829843  0.05306154 -0.92575645]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 524 is [True, False, False, False, True, False]
Scene graph at timestep 524 is [True, False, False, False, True, False]
State prediction error at timestep 524 is tensor(1.1568e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 524 of -1
Current timestep = 525. State = [[-0.08067235 -0.09408949]]. Action = [[ 0.00491303 -0.1234704  -0.01157598 -0.7694157 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 525 is [True, False, False, False, True, False]
Scene graph at timestep 525 is [True, False, False, False, True, False]
State prediction error at timestep 525 is tensor(1.9409e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 525 of -1
Current timestep = 526. State = [[-0.08452145 -0.09615794]]. Action = [[ 0.04548827  0.11398146 -0.2209711  -0.8708064 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 526 is [True, False, False, False, True, False]
Current timestep = 527. State = [[-0.0919256  -0.10062372]]. Action = [[-0.16599365 -0.17304116 -0.118494   -0.51958287]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 527 is [True, False, False, False, True, False]
Current timestep = 528. State = [[-0.09597115 -0.12122957]]. Action = [[ 0.19431257 -0.23650856  0.06339607 -0.5300795 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 528 is [True, False, False, False, True, False]
Current timestep = 529. State = [[-0.08479349 -0.14910473]]. Action = [[ 0.18034652 -0.18112299 -0.03156519  0.24997663]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 529 is [True, False, False, False, True, False]
Current timestep = 530. State = [[-0.06755182 -0.16336463]]. Action = [[0.04125813 0.0266757  0.1483747  0.41979468]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 530 is [True, False, False, True, False, False]
Scene graph at timestep 530 is [True, False, False, True, False, False]
State prediction error at timestep 530 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 530 of -1
Current timestep = 531. State = [[-0.06174743 -0.15409075]]. Action = [[-0.12361521  0.2020652  -0.23545922  0.02511513]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 531 is [True, False, False, True, False, False]
Scene graph at timestep 531 is [True, False, False, True, False, False]
State prediction error at timestep 531 is tensor(3.5930e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 531 of 1
Current timestep = 532. State = [[-0.06389448 -0.13487963]]. Action = [[-0.06821617  0.13506871 -0.01982503 -0.56189305]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 532 is [True, False, False, True, False, False]
Scene graph at timestep 532 is [True, False, False, True, False, False]
State prediction error at timestep 532 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 532 of 1
Current timestep = 533. State = [[-0.05948201 -0.11160303]]. Action = [[ 0.20401433  0.18896037 -0.06695217  0.9021015 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 533 is [True, False, False, True, False, False]
Current timestep = 534. State = [[-0.05836362 -0.08381704]]. Action = [[-0.07504117  0.23062557 -0.09518752 -0.14830476]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 534 is [True, False, False, False, True, False]
Current timestep = 535. State = [[-0.06660865 -0.0771044 ]]. Action = [[-0.15785383 -0.20865865  0.19296813  0.77092755]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 535 is [True, False, False, False, True, False]
Current timestep = 536. State = [[-0.06750853 -0.08256493]]. Action = [[ 0.15922147  0.02030209 -0.18893677 -0.13527024]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 536 is [True, False, False, False, True, False]
Current timestep = 537. State = [[-0.06944756 -0.06999072]]. Action = [[-0.2015647   0.23743004 -0.2108052  -0.39121902]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 537 is [True, False, False, False, True, False]
Scene graph at timestep 537 is [True, False, False, False, True, False]
State prediction error at timestep 537 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 537 of 1
Current timestep = 538. State = [[-0.08052895 -0.06124672]]. Action = [[-0.06040901 -0.14949606  0.09345117 -0.7886553 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 538 is [True, False, False, False, True, False]
Current timestep = 539. State = [[-0.14285083 -0.01717471]]. Action = [[ 0.03534031  0.08497664  0.22953993 -0.7883199 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 539 is [True, False, False, False, True, False]
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 539 of 0
Current timestep = 540. State = [[-0.13102277 -0.03402278]]. Action = [[-0.23727286 -0.19063666 -0.05999282  0.10150862]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 540 is [True, False, False, False, True, False]
Current timestep = 541. State = [[-0.14579117 -0.06300423]]. Action = [[-0.16756876 -0.22135131  0.21255702  0.22377408]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 541 is [True, False, False, False, True, False]
Scene graph at timestep 541 is [True, False, False, False, True, False]
State prediction error at timestep 541 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 541 of -1
Current timestep = 542. State = [[-0.17005533 -0.09229471]]. Action = [[-0.15342581 -0.13179737 -0.1418076  -0.23628414]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 542 is [True, False, False, False, True, False]
Current timestep = 543. State = [[-0.18457621 -0.10459059]]. Action = [[-0.07632722 -0.03924176 -0.08301181  0.04752421]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 543 is [True, False, False, False, True, False]
Current timestep = 544. State = [[-0.19210213 -0.10620654]]. Action = [[ 0.05173424  0.06586617 -0.05783254 -0.09503782]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 544 is [True, False, False, False, True, False]
Current timestep = 545. State = [[-0.20046146 -0.10999957]]. Action = [[-0.20311317 -0.09620088  0.13328803  0.366876  ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 545 is [True, False, False, False, True, False]
Current timestep = 546. State = [[-0.21100639 -0.12842123]]. Action = [[ 0.09544092 -0.23592366 -0.14886527  0.49683022]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 546 is [True, False, False, False, True, False]
Current timestep = 547. State = [[-0.2171887  -0.14258294]]. Action = [[-0.15818241  0.0362772  -0.04692246  0.930599  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 547 is [True, False, False, True, False, False]
Scene graph at timestep 547 is [True, False, False, True, False, False]
State prediction error at timestep 547 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 547 of -1
Current timestep = 548. State = [[-0.23623373 -0.15949804]]. Action = [[-0.22735837 -0.18205664 -0.22535528 -0.20348144]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 548 is [True, False, False, True, False, False]
Current timestep = 549. State = [[-0.26589674 -0.16023715]]. Action = [[-0.21430236  0.19691223 -0.01618882  0.92980766]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 549 is [True, False, False, True, False, False]
Scene graph at timestep 549 is [True, False, False, True, False, False]
State prediction error at timestep 549 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 549 of -1
Current timestep = 550. State = [[-0.29448858 -0.14938068]]. Action = [[-0.10508877 -0.22232194 -0.05388398 -0.5778391 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 550 is [True, False, False, True, False, False]
Current timestep = 551. State = [[-0.29052114 -0.13553862]]. Action = [[ 0.1307554   0.21249735  0.03082237 -0.9686723 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 551 is [True, False, False, True, False, False]
Scene graph at timestep 551 is [True, False, False, True, False, False]
State prediction error at timestep 551 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 551 of 1
Current timestep = 552. State = [[-0.27807802 -0.1056502 ]]. Action = [[0.16891733 0.16804779 0.15123636 0.365582  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 552 is [True, False, False, True, False, False]
Scene graph at timestep 552 is [True, False, False, False, True, False]
State prediction error at timestep 552 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 552 of 1
Current timestep = 553. State = [[-0.26431617 -0.10064676]]. Action = [[ 0.08048636 -0.19295497  0.22075522 -0.34607494]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 553 is [True, False, False, False, True, False]
Scene graph at timestep 553 is [True, False, False, False, True, False]
State prediction error at timestep 553 is tensor(3.4320e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 553 of -1
Current timestep = 554. State = [[-0.25711682 -0.09949164]]. Action = [[-0.01945211  0.24070156 -0.23568548  0.77734876]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 554 is [True, False, False, False, True, False]
Scene graph at timestep 554 is [True, False, False, False, True, False]
State prediction error at timestep 554 is tensor(6.1922e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 554 of 1
Current timestep = 555. State = [[-0.24766998 -0.08110473]]. Action = [[ 0.23997524  0.06888705  0.11775827 -0.26200068]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 555 is [True, False, False, False, True, False]
Scene graph at timestep 555 is [True, False, False, False, True, False]
State prediction error at timestep 555 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 555 of 1
Current timestep = 556. State = [[-0.2254501  -0.08508534]]. Action = [[ 0.04453728 -0.2263392  -0.11005029 -0.86661655]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 556 is [True, False, False, False, True, False]
Current timestep = 557. State = [[-0.21533252 -0.08989194]]. Action = [[ 0.11748716  0.14231479 -0.13908038 -0.88852626]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 557 is [True, False, False, False, True, False]
Current timestep = 558. State = [[-0.20447323 -0.09215292]]. Action = [[ 0.05807817 -0.12860252  0.04725704 -0.8141278 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 558 is [True, False, False, False, True, False]
Scene graph at timestep 558 is [True, False, False, False, True, False]
State prediction error at timestep 558 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 558 of 1
Current timestep = 559. State = [[-0.194325   -0.08404997]]. Action = [[ 0.02943596  0.23604488 -0.12492406  0.03810823]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 559 is [True, False, False, False, True, False]
Scene graph at timestep 559 is [True, False, False, False, True, False]
State prediction error at timestep 559 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 559 of 1
Current timestep = 560. State = [[-0.18961668 -0.07049375]]. Action = [[ 0.04232132  0.01649764 -0.20877443 -0.24534327]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 560 is [True, False, False, False, True, False]
Current timestep = 561. State = [[-0.18937422 -0.07260319]]. Action = [[-0.11543238 -0.08624309 -0.01292989  0.9032841 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 561 is [True, False, False, False, True, False]
Scene graph at timestep 561 is [True, False, False, False, True, False]
State prediction error at timestep 561 is tensor(3.0409e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 561 of 1
Current timestep = 562. State = [[-0.18934876 -0.0650224 ]]. Action = [[ 0.10644597  0.19862586  0.18361223 -0.3117255 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 562 is [True, False, False, False, True, False]
Scene graph at timestep 562 is [True, False, False, False, True, False]
State prediction error at timestep 562 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 562 of 1
Current timestep = 563. State = [[-0.18877995 -0.05969926]]. Action = [[-0.10168043 -0.18094581 -0.2430225  -0.03375119]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 563 is [True, False, False, False, True, False]
Scene graph at timestep 563 is [True, False, False, False, True, False]
State prediction error at timestep 563 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 563 of -1
Current timestep = 564. State = [[-0.19090508 -0.07551149]]. Action = [[ 0.06573153 -0.07265833 -0.10696119  0.7199167 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 564 is [True, False, False, False, True, False]
Current timestep = 565. State = [[-0.18605381 -0.06891826]]. Action = [[ 0.10271522  0.2047689  -0.19554998  0.8445561 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 565 is [True, False, False, False, True, False]
Current timestep = 566. State = [[-0.17260537 -0.06173204]]. Action = [[ 0.18484163 -0.05733036 -0.05628787 -0.10903329]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 566 is [True, False, False, False, True, False]
Current timestep = 567. State = [[-0.15081622 -0.06806398]]. Action = [[ 0.05669114 -0.10403103 -0.080367   -0.9389642 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 567 is [True, False, False, False, True, False]
Current timestep = 568. State = [[-0.1489416  -0.06106125]]. Action = [[-0.23809709  0.24249983 -0.16765915 -0.1245296 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 568 is [True, False, False, False, True, False]
Scene graph at timestep 568 is [True, False, False, False, True, False]
State prediction error at timestep 568 is tensor(5.4639e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 568 of 1
Current timestep = 569. State = [[-0.15170476 -0.0465338 ]]. Action = [[ 0.1790834  -0.04151076  0.1431359   0.56897974]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 569 is [True, False, False, False, True, False]
Current timestep = 570. State = [[-0.14404055 -0.06172986]]. Action = [[ 0.12840521 -0.24933992  0.10434484 -0.8198499 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 570 is [True, False, False, False, True, False]
Current timestep = 571. State = [[-0.13501853 -0.08949246]]. Action = [[-0.05130032 -0.16540773 -0.12512212 -0.88943344]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 571 is [True, False, False, False, True, False]
Current timestep = 572. State = [[-0.13553374 -0.10559893]]. Action = [[-0.05061372 -0.04197387 -0.19941863 -0.9079725 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 572 is [True, False, False, False, True, False]
Scene graph at timestep 572 is [True, False, False, False, True, False]
State prediction error at timestep 572 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 572 of -1
Current timestep = 573. State = [[-0.12784816 -0.10507508]]. Action = [[ 0.24833122  0.1213491   0.00971097 -0.5191923 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 573 is [True, False, False, False, True, False]
Current timestep = 574. State = [[-0.11274202 -0.09821544]]. Action = [[-0.01597238  0.05218688  0.0115597  -0.5700452 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 574 is [True, False, False, False, True, False]
Scene graph at timestep 574 is [True, False, False, False, True, False]
State prediction error at timestep 574 is tensor(6.7003e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 574 of 1
Current timestep = 575. State = [[-0.10913459 -0.08386072]]. Action = [[ 0.04638591  0.14500022 -0.12029429 -0.49787158]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 575 is [True, False, False, False, True, False]
Current timestep = 576. State = [[-0.09678625 -0.07099465]]. Action = [[ 0.20312664  0.04249895  0.10742855 -0.69372946]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 576 is [True, False, False, False, True, False]
Scene graph at timestep 576 is [True, False, False, False, True, False]
State prediction error at timestep 576 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 576 of 1
Current timestep = 577. State = [[-0.06716891 -0.06612717]]. Action = [[ 0.19111538 -0.02516951  0.16636658  0.9230838 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 577 is [True, False, False, False, True, False]
Current timestep = 578. State = [[-0.05028083 -0.07892732]]. Action = [[ 0.01481208 -0.21762183 -0.12790112 -0.06165165]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 578 is [True, False, False, False, True, False]
Current timestep = 579. State = [[-0.03669114 -0.08973767]]. Action = [[0.20931733 0.0407607  0.19987956 0.7114078 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 579 is [True, False, False, False, True, False]
Scene graph at timestep 579 is [False, True, False, False, True, False]
State prediction error at timestep 579 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 579 of 1
Current timestep = 580. State = [[-0.20302224 -0.02145031]]. Action = [[ 0.09343001  0.10690859 -0.08822641  0.5297849 ]]. Reward = [100.]
Curr episode timestep = 40
Scene graph at timestep 580 is [False, True, False, False, True, False]
Scene graph at timestep 580 is [True, False, False, False, True, False]
State prediction error at timestep 580 is tensor(0.0166, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 580 of 0
Current timestep = 581. State = [[-0.18800911 -0.03927562]]. Action = [[ 0.09524351 -0.23797256 -0.21953875  0.15759432]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 581 is [True, False, False, False, True, False]
Scene graph at timestep 581 is [True, False, False, False, True, False]
State prediction error at timestep 581 is tensor(9.3857e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 581 of -1
Current timestep = 582. State = [[-0.18116678 -0.07451895]]. Action = [[-0.05118443 -0.22859935 -0.03463671  0.35574067]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 582 is [True, False, False, False, True, False]
Current timestep = 583. State = [[-0.1769124  -0.08736731]]. Action = [[ 0.1530385   0.0603368  -0.07149825 -0.267164  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 583 is [True, False, False, False, True, False]
Scene graph at timestep 583 is [True, False, False, False, True, False]
State prediction error at timestep 583 is tensor(1.8376e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 583 of -1
Current timestep = 584. State = [[-0.15613346 -0.09984277]]. Action = [[ 0.13566679 -0.19482626  0.23345548  0.79662704]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 584 is [True, False, False, False, True, False]
Current timestep = 585. State = [[-0.1418917 -0.1065596]]. Action = [[ 0.07978082  0.10469449 -0.16590932 -0.19833326]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 585 is [True, False, False, False, True, False]
Current timestep = 586. State = [[-0.1363438  -0.09300701]]. Action = [[-0.10874748  0.19317454 -0.02316304  0.24240828]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 586 is [True, False, False, False, True, False]
Scene graph at timestep 586 is [True, False, False, False, True, False]
State prediction error at timestep 586 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 586 of 1
Current timestep = 587. State = [[-0.1350858 -0.0753535]]. Action = [[ 0.12269306  0.04453135 -0.05298716 -0.44770008]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 587 is [True, False, False, False, True, False]
Scene graph at timestep 587 is [True, False, False, False, True, False]
State prediction error at timestep 587 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 587 of 1
Current timestep = 588. State = [[-0.13135946 -0.06226106]]. Action = [[-0.06283982  0.15632263 -0.01553541 -0.21448815]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 588 is [True, False, False, False, True, False]
Current timestep = 589. State = [[-0.12727423 -0.06017029]]. Action = [[ 0.15855783 -0.18079972 -0.1870462  -0.11667693]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 589 is [True, False, False, False, True, False]
Scene graph at timestep 589 is [True, False, False, False, True, False]
State prediction error at timestep 589 is tensor(6.2751e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 589 of 1
Current timestep = 590. State = [[-0.11973108 -0.07435244]]. Action = [[-0.1451754  -0.0833793   0.07184091  0.11862457]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 590 is [True, False, False, False, True, False]
Current timestep = 591. State = [[-0.12503466 -0.08999049]]. Action = [[-0.08587971 -0.14764541 -0.15558887 -0.4839927 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 591 is [True, False, False, False, True, False]
Scene graph at timestep 591 is [True, False, False, False, True, False]
State prediction error at timestep 591 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 591 of -1
Current timestep = 592. State = [[-0.13287686 -0.09414149]]. Action = [[-0.03685848  0.18396455 -0.00482145  0.36823237]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 592 is [True, False, False, False, True, False]
Current timestep = 593. State = [[-0.13915619 -0.09367394]]. Action = [[-0.1237064  -0.1647667  -0.10581976  0.22979343]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 593 is [True, False, False, False, True, False]
Scene graph at timestep 593 is [True, False, False, False, True, False]
State prediction error at timestep 593 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 593 of -1
Current timestep = 594. State = [[-0.14780483 -0.10662059]]. Action = [[ 0.10521817 -0.08149709  0.24022186 -0.16666728]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 594 is [True, False, False, False, True, False]
Current timestep = 595. State = [[-0.14808495 -0.10627481]]. Action = [[-0.10076761  0.09679639  0.22139716  0.81095266]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 595 is [True, False, False, False, True, False]
Scene graph at timestep 595 is [True, False, False, False, True, False]
State prediction error at timestep 595 is tensor(1.0835e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 595 of -1
Current timestep = 596. State = [[-0.14494304 -0.09338645]]. Action = [[ 0.17441726  0.16789949 -0.1821135   0.5736804 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 596 is [True, False, False, False, True, False]
Scene graph at timestep 596 is [True, False, False, False, True, False]
State prediction error at timestep 596 is tensor(9.6459e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 596 of 1
Current timestep = 597. State = [[-0.13203634 -0.09022485]]. Action = [[ 0.17008981 -0.22068657 -0.16695105 -0.9517657 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 597 is [True, False, False, False, True, False]
Current timestep = 598. State = [[-0.12163297 -0.0939862 ]]. Action = [[ 0.03049749  0.15305212  0.16552955 -0.24759853]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 598 is [True, False, False, False, True, False]
Current timestep = 599. State = [[-0.11522686 -0.0984225 ]]. Action = [[ 0.03881371 -0.16527146 -0.22168133  0.6519557 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 599 is [True, False, False, False, True, False]
Scene graph at timestep 599 is [True, False, False, False, True, False]
State prediction error at timestep 599 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 599 of 1
Current timestep = 600. State = [[-0.10116754 -0.09401122]]. Action = [[ 0.22492647  0.2266146   0.11098415 -0.9915583 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 600 is [True, False, False, False, True, False]
Scene graph at timestep 600 is [True, False, False, False, True, False]
State prediction error at timestep 600 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 600 of 1
Current timestep = 601. State = [[-0.08395755 -0.06860347]]. Action = [[-0.20670332  0.17750224 -0.19273992 -0.53700256]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 601 is [True, False, False, False, True, False]
Scene graph at timestep 601 is [True, False, False, False, True, False]
State prediction error at timestep 601 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 601 of 1
Current timestep = 602. State = [[-0.08525076 -0.05652327]]. Action = [[ 0.18177795 -0.07030992  0.00241131  0.8782953 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 602 is [True, False, False, False, True, False]
Scene graph at timestep 602 is [True, False, False, False, True, False]
State prediction error at timestep 602 is tensor(4.7756e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 602 of 1
Current timestep = 603. State = [[-0.08469    -0.06317724]]. Action = [[-0.11225504 -0.07050493  0.03025147  0.21712804]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 603 is [True, False, False, False, True, False]
Scene graph at timestep 603 is [True, False, False, False, True, False]
State prediction error at timestep 603 is tensor(3.5250e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 603 of 1
Current timestep = 604. State = [[-0.08171909 -0.07067402]]. Action = [[ 0.18725455 -0.00971749  0.2190769  -0.08924472]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 604 is [True, False, False, False, True, False]
Scene graph at timestep 604 is [True, False, False, False, True, False]
State prediction error at timestep 604 is tensor(5.2240e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 604 of 1
Current timestep = 605. State = [[-0.06494307 -0.07819611]]. Action = [[ 0.1241824  -0.11471745  0.11586997 -0.79476607]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 605 is [True, False, False, False, True, False]
Scene graph at timestep 605 is [True, False, False, False, True, False]
State prediction error at timestep 605 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 605 of 1
Current timestep = 606. State = [[-0.05684439 -0.07647941]]. Action = [[-0.16770825  0.1940487  -0.1010531   0.36947417]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 606 is [True, False, False, False, True, False]
Current timestep = 607. State = [[-0.05377029 -0.05328795]]. Action = [[ 0.2344245   0.20388928 -0.22710536 -0.76520777]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 607 is [True, False, False, False, True, False]
Current timestep = 608. State = [[-0.04590041 -0.04932481]]. Action = [[-0.00703134 -0.197888    0.01504898 -0.01014882]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 608 is [True, False, False, False, True, False]
Scene graph at timestep 608 is [False, True, False, False, True, False]
State prediction error at timestep 608 is tensor(4.5361e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 608 of 1
Current timestep = 609. State = [[-0.0451657  -0.06370374]]. Action = [[-0.08292675 -0.10258463  0.09295678 -0.9517957 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 609 is [False, True, False, False, True, False]
Current timestep = 610. State = [[-0.05251216 -0.08255238]]. Action = [[-0.18248135 -0.16082767  0.01236606  0.14230227]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 610 is [False, True, False, False, True, False]
Scene graph at timestep 610 is [True, False, False, False, True, False]
State prediction error at timestep 610 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 610 of -1
Current timestep = 611. State = [[-0.06699325 -0.09911139]]. Action = [[-0.18885586  0.01423824  0.06214944  0.91617715]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 611 is [True, False, False, False, True, False]
Current timestep = 612. State = [[-0.08541071 -0.11047725]]. Action = [[-0.09987849 -0.18669656 -0.11083615  0.36744714]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 612 is [True, False, False, False, True, False]
Current timestep = 613. State = [[-0.09055129 -0.13358799]]. Action = [[ 0.14722198 -0.19953378 -0.20039481  0.07671428]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 613 is [True, False, False, False, True, False]
Current timestep = 614. State = [[-0.08928215 -0.1455908 ]]. Action = [[0.02083227 0.0600127  0.05070692 0.4723189 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 614 is [True, False, False, True, False, False]
Current timestep = 615. State = [[-0.08074511 -0.14418648]]. Action = [[ 0.19324142  0.00710186 -0.21061482 -0.8704535 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 615 is [True, False, False, True, False, False]
Current timestep = 616. State = [[-0.0607075  -0.13015026]]. Action = [[ 0.2218973   0.22771269 -0.23999955  0.9859452 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 616 is [True, False, False, True, False, False]
Current timestep = 617. State = [[-0.03308055 -0.10761448]]. Action = [[ 0.22099137  0.12007108  0.16539496 -0.8808614 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 617 is [True, False, False, True, False, False]
Scene graph at timestep 617 is [False, True, False, False, True, False]
State prediction error at timestep 617 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 617 of 1
Current timestep = 618. State = [[-0.00684769 -0.1093072 ]]. Action = [[-0.06300914 -0.21821135 -0.21644546  0.53424954]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 618 is [False, True, False, False, True, False]
Current timestep = 619. State = [[-0.0045084  -0.12117543]]. Action = [[ 0.11211127 -0.02660507 -0.1129007  -0.2068876 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 619 is [False, True, False, False, True, False]
Current timestep = 620. State = [[-0.00316537 -0.12422413]]. Action = [[-0.00571159 -0.00677937 -0.03446975  0.25739455]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 620 is [False, True, False, False, True, False]
Current timestep = 621. State = [[-0.00106907 -0.11529544]]. Action = [[ 0.02498186  0.19350463  0.20378527 -0.66779715]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 621 is [False, True, False, False, True, False]
Scene graph at timestep 621 is [False, True, False, False, True, False]
State prediction error at timestep 621 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 621 of 1
Current timestep = 622. State = [[ 0.01125737 -0.10546014]]. Action = [[ 0.18879905 -0.03038114  0.19538498 -0.27184296]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 622 is [False, True, False, False, True, False]
Scene graph at timestep 622 is [False, True, False, False, True, False]
State prediction error at timestep 622 is tensor(8.9321e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of -1
Current timestep = 623. State = [[ 0.03436795 -0.11292258]]. Action = [[ 0.00052223 -0.1289221   0.23291212  0.40613997]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 623 is [False, True, False, False, True, False]
Current timestep = 624. State = [[ 0.03430916 -0.11369423]]. Action = [[-0.01200663  0.12143466 -0.22614412  0.17900419]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 624 is [False, True, False, False, True, False]
Scene graph at timestep 624 is [False, True, False, False, True, False]
State prediction error at timestep 624 is tensor(3.5272e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 624 of -1
Current timestep = 625. State = [[ 0.03061805 -0.12451967]]. Action = [[-0.20775872 -0.23128244  0.08175126  0.07222617]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 625 is [False, True, False, False, True, False]
Current timestep = 626. State = [[ 0.02666444 -0.13055895]]. Action = [[-0.0398469   0.1283108   0.15477264 -0.5402711 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 626 is [False, True, False, False, True, False]
Current timestep = 627. State = [[ 0.02555957 -0.13832219]]. Action = [[ 0.0677821  -0.19304436  0.18735692 -0.49775803]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 627 is [False, True, False, True, False, False]
Current timestep = 628. State = [[ 0.02126676 -0.146843  ]]. Action = [[-0.1652574   0.02134705 -0.19877058  0.01460481]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 628 is [False, True, False, True, False, False]
Scene graph at timestep 628 is [False, True, False, True, False, False]
State prediction error at timestep 628 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 628 of -1
Current timestep = 629. State = [[ 0.00898058 -0.14673337]]. Action = [[-0.13960989  0.05858663 -0.1846711   0.17732239]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 629 is [False, True, False, True, False, False]
Scene graph at timestep 629 is [False, True, False, True, False, False]
State prediction error at timestep 629 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 629 of -1
Current timestep = 630. State = [[-0.01166314 -0.1315026 ]]. Action = [[-0.17306766  0.22610945 -0.20252918  0.9010663 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 630 is [False, True, False, True, False, False]
Current timestep = 631. State = [[-0.02669229 -0.12772219]]. Action = [[ 0.0397087  -0.24282186 -0.14341392 -0.43204844]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 631 is [False, True, False, True, False, False]
Scene graph at timestep 631 is [False, True, False, True, False, False]
State prediction error at timestep 631 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 631 of -1
Current timestep = 632. State = [[-0.02962833 -0.13224559]]. Action = [[0.00856873 0.15524894 0.1236065  0.01778924]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 632 is [False, True, False, True, False, False]
Scene graph at timestep 632 is [False, True, False, True, False, False]
State prediction error at timestep 632 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 632 of 1
Current timestep = 633. State = [[-0.03422441 -0.11329464]]. Action = [[-0.15152806  0.1641708   0.01127151 -0.04144353]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 633 is [False, True, False, True, False, False]
Current timestep = 634. State = [[-0.03862694 -0.10653146]]. Action = [[ 0.1762484  -0.11941382  0.05450517  0.64732385]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 634 is [False, True, False, False, True, False]
Scene graph at timestep 634 is [False, True, False, False, True, False]
State prediction error at timestep 634 is tensor(7.2955e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of 1
Current timestep = 635. State = [[-0.04068681 -0.12490403]]. Action = [[-0.15191495 -0.24170582  0.13887045 -0.02207029]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 635 is [False, True, False, False, True, False]
Scene graph at timestep 635 is [False, True, False, False, True, False]
State prediction error at timestep 635 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 635 of -1
Current timestep = 636. State = [[-0.04469181 -0.139727  ]]. Action = [[0.2028043  0.10577092 0.23453814 0.5822108 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 636 is [False, True, False, False, True, False]
Current timestep = 637. State = [[-0.03825278 -0.14189914]]. Action = [[ 0.08383638 -0.12530744 -0.14546625 -0.98260444]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 637 is [False, True, False, True, False, False]
Current timestep = 638. State = [[-0.02397141 -0.15328692]]. Action = [[ 0.19458419 -0.1023673  -0.2013     -0.7706653 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 638 is [False, True, False, True, False, False]
Current timestep = 639. State = [[-0.00749365 -0.16185068]]. Action = [[-0.00225146 -0.01689345 -0.22964984  0.11555505]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 639 is [False, True, False, True, False, False]
Scene graph at timestep 639 is [False, True, False, True, False, False]
State prediction error at timestep 639 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 639 of 1
Current timestep = 640. State = [[ 0.00266595 -0.16037881]]. Action = [[ 0.24772635  0.06334847 -0.05413482 -0.714616  ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 640 is [False, True, False, True, False, False]
Scene graph at timestep 640 is [False, True, False, True, False, False]
State prediction error at timestep 640 is tensor(1.3344e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 640 of -1
Current timestep = 641. State = [[ 0.02802305 -0.1446409 ]]. Action = [[-0.15088819  0.21581101 -0.04408506 -0.57397074]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 641 is [False, True, False, True, False, False]
Scene graph at timestep 641 is [False, True, False, True, False, False]
State prediction error at timestep 641 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 641 of 1
Current timestep = 642. State = [[ 0.02658412 -0.11843639]]. Action = [[-0.02653813  0.24093667 -0.10294569 -0.7041303 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 642 is [False, True, False, True, False, False]
Scene graph at timestep 642 is [False, True, False, False, True, False]
State prediction error at timestep 642 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 642 of 1
Current timestep = 643. State = [[ 0.01871466 -0.1086908 ]]. Action = [[-0.19001468 -0.20835243  0.02721712 -0.739892  ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 643 is [False, True, False, False, True, False]
Current timestep = 644. State = [[ 0.01434582 -0.11149031]]. Action = [[ 0.24799913  0.14808723  0.19766939 -0.3753147 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 644 is [False, True, False, False, True, False]
Current timestep = 645. State = [[ 0.02318561 -0.11381266]]. Action = [[ 0.18870547 -0.17022343 -0.12278721 -0.385543  ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 645 is [False, True, False, False, True, False]
Current timestep = 646. State = [[ 0.03640111 -0.11057227]]. Action = [[-0.0735652   0.21163172 -0.12562826 -0.6296139 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 646 is [False, True, False, False, True, False]
Current timestep = 647. State = [[ 0.03715639 -0.10637701]]. Action = [[ 0.0642319  -0.12859932  0.06498787  0.4881066 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 647 is [False, True, False, False, True, False]
Scene graph at timestep 647 is [False, True, False, False, True, False]
State prediction error at timestep 647 is tensor(1.4181e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 647 of -1
Current timestep = 648. State = [[-0.15457779 -0.12577711]]. Action = [[-0.01271036  0.16318965  0.14706904 -0.31332392]]. Reward = [100.]
Curr episode timestep = 67
Scene graph at timestep 648 is [False, True, False, False, True, False]
Current timestep = 649. State = [[-0.13621432 -0.13718612]]. Action = [[ 0.05422473  0.08293319  0.08746874 -0.28831708]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 649 is [True, False, False, True, False, False]
Current timestep = 650. State = [[-0.13770935 -0.1211149 ]]. Action = [[-0.16708641  0.23983347  0.14861673 -0.58023006]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 650 is [True, False, False, True, False, False]
Current timestep = 651. State = [[-0.13472736 -0.11099948]]. Action = [[ 0.20744985 -0.11683431 -0.18455407 -0.9680598 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 651 is [True, False, False, False, True, False]
Current timestep = 652. State = [[-0.12389174 -0.10633085]]. Action = [[ 0.04210377  0.13077581 -0.22039473  0.8664316 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 652 is [True, False, False, False, True, False]
Scene graph at timestep 652 is [True, False, False, False, True, False]
State prediction error at timestep 652 is tensor(5.6931e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 652 of 1
Current timestep = 653. State = [[-0.11026445 -0.1120086 ]]. Action = [[ 0.18295932 -0.2372092  -0.17608906  0.14535904]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 653 is [True, False, False, False, True, False]
Scene graph at timestep 653 is [True, False, False, False, True, False]
State prediction error at timestep 653 is tensor(9.0659e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 653 of 1
Current timestep = 654. State = [[-0.08731038 -0.12452471]]. Action = [[ 0.14498979  0.0386177  -0.01014696 -0.46185005]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 654 is [True, False, False, False, True, False]
Scene graph at timestep 654 is [True, False, False, False, True, False]
State prediction error at timestep 654 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 654 of 1
Current timestep = 655. State = [[-0.06415208 -0.11126034]]. Action = [[ 0.1267303   0.23672152  0.17800906 -0.50172865]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 655 is [True, False, False, False, True, False]
Current timestep = 656. State = [[-0.05468414 -0.08687108]]. Action = [[-0.00863035  0.15310103 -0.16105814 -0.91871   ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 656 is [True, False, False, False, True, False]
Scene graph at timestep 656 is [True, False, False, False, True, False]
State prediction error at timestep 656 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 656 of 1
Current timestep = 657. State = [[-0.05769325 -0.06624404]]. Action = [[-0.16821317  0.07548621  0.00158533 -0.86838585]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 657 is [True, False, False, False, True, False]
Current timestep = 658. State = [[-0.06950624 -0.05118421]]. Action = [[-0.22231913  0.1469751   0.24086595 -0.9039343 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 658 is [True, False, False, False, True, False]
Current timestep = 659. State = [[-0.08063132 -0.03159169]]. Action = [[0.10719186 0.12300783 0.16151738 0.47433138]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 659 is [True, False, False, False, True, False]
Scene graph at timestep 659 is [True, False, False, False, True, False]
State prediction error at timestep 659 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 659 of -1
Current timestep = 660. State = [[-0.083508   -0.00716238]]. Action = [[-0.00874984  0.21615896 -0.17838861  0.05611169]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 660 is [True, False, False, False, True, False]
Current timestep = 661. State = [[-0.08384402  0.00015779]]. Action = [[ 0.0638108  -0.14570127 -0.09319636 -0.55269784]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 661 is [True, False, False, False, True, False]
Scene graph at timestep 661 is [True, False, False, False, True, False]
State prediction error at timestep 661 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 661 of 1
Current timestep = 662. State = [[-0.07199811 -0.01374841]]. Action = [[ 0.21661708 -0.13389756 -0.17921013 -0.45062935]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 662 is [True, False, False, False, True, False]
Scene graph at timestep 662 is [True, False, False, False, True, False]
State prediction error at timestep 662 is tensor(9.5450e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 662 of 1
Current timestep = 663. State = [[-0.04537837 -0.02325075]]. Action = [[0.21859556 0.0193668  0.12925774 0.41708994]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 663 is [True, False, False, False, True, False]
Current timestep = 664. State = [[-0.24800715 -0.15050864]]. Action = [[-0.12738965  0.08258259  0.19900358  0.3118893 ]]. Reward = [100.]
Curr episode timestep = 15
Scene graph at timestep 664 is [False, True, False, False, True, False]
Scene graph at timestep 664 is [True, False, False, True, False, False]
State prediction error at timestep 664 is tensor(0.0279, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 664 of -1
Current timestep = 665. State = [[-0.24243826 -0.16259222]]. Action = [[-0.04291838  0.13194865  0.21771348 -0.18031919]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 665 is [True, False, False, True, False, False]
Current timestep = 666. State = [[-0.23702335 -0.14395636]]. Action = [[ 0.15289295  0.24054456 -0.015729    0.4319613 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 666 is [True, False, False, True, False, False]
Scene graph at timestep 666 is [True, False, False, True, False, False]
State prediction error at timestep 666 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 666 of 1
Current timestep = 667. State = [[-0.21812119 -0.1152446 ]]. Action = [[ 0.22420031  0.08502188 -0.07175311 -0.6916353 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 667 is [True, False, False, True, False, False]
Scene graph at timestep 667 is [True, False, False, False, True, False]
State prediction error at timestep 667 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 667 of 1
Current timestep = 668. State = [[-0.19775379 -0.11116853]]. Action = [[ 0.02496535 -0.05365764 -0.04935119 -0.09961951]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 668 is [True, False, False, False, True, False]
Current timestep = 669. State = [[-0.19777592 -0.11942749]]. Action = [[-0.11268494 -0.10544163  0.00505194 -0.63821286]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 669 is [True, False, False, False, True, False]
Current timestep = 670. State = [[-0.19451347 -0.13013831]]. Action = [[ 0.17143017 -0.09376368 -0.07740593 -0.8455455 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 670 is [True, False, False, False, True, False]
Scene graph at timestep 670 is [True, False, False, True, False, False]
State prediction error at timestep 670 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 670 of -1
Current timestep = 671. State = [[-0.1830482  -0.12768131]]. Action = [[ 0.05148795  0.19930997 -0.02720077  0.606766  ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 671 is [True, False, False, True, False, False]
Current timestep = 672. State = [[-0.17242558 -0.11505021]]. Action = [[ 0.16777694  0.05708531 -0.2305767  -0.06422198]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 672 is [True, False, False, True, False, False]
Current timestep = 673. State = [[-0.15428102 -0.10151588]]. Action = [[0.04734382 0.12208486 0.15439066 0.7530494 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 673 is [True, False, False, False, True, False]
Scene graph at timestep 673 is [True, False, False, False, True, False]
State prediction error at timestep 673 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 673 of 1
Current timestep = 674. State = [[-0.1472258  -0.10155325]]. Action = [[ 0.02217832 -0.17723194 -0.22391987  0.30700862]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 674 is [True, False, False, False, True, False]
Current timestep = 675. State = [[-0.14589758 -0.11404333]]. Action = [[ 0.00212485 -0.09133416  0.10707763 -0.13128197]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 675 is [True, False, False, False, True, False]
Scene graph at timestep 675 is [True, False, False, False, True, False]
State prediction error at timestep 675 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 675 of -1
Current timestep = 676. State = [[-0.142682   -0.11002506]]. Action = [[0.06736112 0.22987121 0.09019798 0.90847516]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 676 is [True, False, False, False, True, False]
Current timestep = 677. State = [[-0.13910341 -0.09700292]]. Action = [[-0.04506898  0.03748181  0.12902433  0.7874787 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 677 is [True, False, False, False, True, False]
Scene graph at timestep 677 is [True, False, False, False, True, False]
State prediction error at timestep 677 is tensor(5.4371e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 677 of 1
Current timestep = 678. State = [[-0.12961979 -0.10226824]]. Action = [[ 0.20319015 -0.20270057 -0.13062014 -0.7136342 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 678 is [True, False, False, False, True, False]
Scene graph at timestep 678 is [True, False, False, False, True, False]
State prediction error at timestep 678 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 678 of 1
Current timestep = 679. State = [[-0.09936251 -0.11406855]]. Action = [[ 0.21490014 -0.01060006 -0.047047    0.37058568]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 679 is [True, False, False, False, True, False]
Scene graph at timestep 679 is [True, False, False, False, True, False]
State prediction error at timestep 679 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 679 of 1
Current timestep = 680. State = [[-0.07891491 -0.11624825]]. Action = [[-0.11178164  0.03733239 -0.14274861 -0.7543188 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 680 is [True, False, False, False, True, False]
Current timestep = 681. State = [[-0.07796744 -0.10376021]]. Action = [[0.11982188 0.19607067 0.0791814  0.08144283]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 681 is [True, False, False, False, True, False]
Scene graph at timestep 681 is [True, False, False, False, True, False]
State prediction error at timestep 681 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 681 of 1
Current timestep = 682. State = [[-0.07277425 -0.07372195]]. Action = [[ 0.051458    0.24147904 -0.02300528  0.7829752 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 682 is [True, False, False, False, True, False]
Scene graph at timestep 682 is [True, False, False, False, True, False]
State prediction error at timestep 682 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 682 of 1
Current timestep = 683. State = [[-0.07581428 -0.05210271]]. Action = [[-0.21376301  0.0184328   0.22024131 -0.5773658 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 683 is [True, False, False, False, True, False]
Scene graph at timestep 683 is [True, False, False, False, True, False]
State prediction error at timestep 683 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 683 of -1
Current timestep = 684. State = [[-0.08105447 -0.050651  ]]. Action = [[-0.02426678 -0.02728616  0.0487875  -0.21481478]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 684 is [True, False, False, False, True, False]
Current timestep = 685. State = [[-0.07813319 -0.0421725 ]]. Action = [[ 0.21807778  0.16772029 -0.10928017  0.94108903]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 685 is [True, False, False, False, True, False]
Scene graph at timestep 685 is [True, False, False, False, True, False]
State prediction error at timestep 685 is tensor(7.3799e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 685 of 1
Current timestep = 686. State = [[-0.0625463  -0.03629509]]. Action = [[ 0.17723274 -0.1309889  -0.06874469 -0.92558694]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 686 is [True, False, False, False, True, False]
Current timestep = 687. State = [[-0.05046173 -0.0342522 ]]. Action = [[-0.02805831  0.13232186  0.241985    0.6924857 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 687 is [True, False, False, False, True, False]
Current timestep = 688. State = [[-0.03947372 -0.02969683]]. Action = [[ 0.19977748  0.003883   -0.02249542 -0.17356712]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 688 is [True, False, False, False, True, False]
Current timestep = 689. State = [[-0.20980427 -0.05592059]]. Action = [[-0.04588634  0.16158354 -0.15445457 -0.6407857 ]]. Reward = [100.]
Curr episode timestep = 24
Scene graph at timestep 689 is [False, True, False, False, True, False]
Scene graph at timestep 689 is [True, False, False, False, True, False]
State prediction error at timestep 689 is tensor(0.0144, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 689 of 0
Current timestep = 690. State = [[-0.20057505 -0.05000778]]. Action = [[ 0.07972068  0.24692085 -0.23733683 -0.89396787]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 690 is [True, False, False, False, True, False]
Current timestep = 691. State = [[-0.19814405 -0.04896851]]. Action = [[-0.19647609 -0.19536608 -0.14710262  0.8544102 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 691 is [True, False, False, False, True, False]
Scene graph at timestep 691 is [True, False, False, False, True, False]
State prediction error at timestep 691 is tensor(5.0169e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 691 of -1
Current timestep = 692. State = [[-0.21292222 -0.04980223]]. Action = [[-0.19583614  0.17238942  0.01819476 -0.38308907]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 692 is [True, False, False, False, True, False]
Scene graph at timestep 692 is [True, False, False, False, True, False]
State prediction error at timestep 692 is tensor(5.1945e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 692 of -1
Current timestep = 693. State = [[-0.22398134 -0.04795095]]. Action = [[ 0.17281753 -0.1881226   0.01979417 -0.58262354]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 693 is [True, False, False, False, True, False]
Current timestep = 694. State = [[-0.2095965 -0.0592975]]. Action = [[ 0.20870352 -0.04672745  0.223364   -0.9507894 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 694 is [True, False, False, False, True, False]
Scene graph at timestep 694 is [True, False, False, False, True, False]
State prediction error at timestep 694 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 694 of -1
Current timestep = 695. State = [[-0.18430734 -0.05907966]]. Action = [[ 0.14976716  0.13295954  0.08404845 -0.2057066 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 695 is [True, False, False, False, True, False]
Scene graph at timestep 695 is [True, False, False, False, True, False]
State prediction error at timestep 695 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 695 of 1
Current timestep = 696. State = [[-0.17089455 -0.04278425]]. Action = [[-0.0217047   0.17408526 -0.20076847 -0.2811587 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 696 is [True, False, False, False, True, False]
Scene graph at timestep 696 is [True, False, False, False, True, False]
State prediction error at timestep 696 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 696 of 1
Current timestep = 697. State = [[-0.17221077 -0.03424607]]. Action = [[-0.14624044 -0.1080468  -0.16753092  0.04427516]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 697 is [True, False, False, False, True, False]
Current timestep = 698. State = [[-0.17368571 -0.05258384]]. Action = [[ 0.09799343 -0.2399974   0.10033041  0.50672424]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 698 is [True, False, False, False, True, False]
Current timestep = 699. State = [[-0.17497197 -0.06425215]]. Action = [[-0.08852869  0.08509097  0.04608336 -0.06405884]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 699 is [True, False, False, False, True, False]
Scene graph at timestep 699 is [True, False, False, False, True, False]
State prediction error at timestep 699 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 699 of -1
Current timestep = 700. State = [[-0.17401622 -0.05276919]]. Action = [[0.12660366 0.1823079  0.18510497 0.70895433]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 700 is [True, False, False, False, True, False]
Current timestep = 701. State = [[-0.17567082 -0.02863807]]. Action = [[-0.10380641  0.22827852  0.14977908 -0.95127547]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 701 is [True, False, False, False, True, False]
Current timestep = 702. State = [[-0.17119835 -0.00864622]]. Action = [[ 0.22104743  0.03751042 -0.05193758  0.32318246]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 702 is [True, False, False, False, True, False]
Scene graph at timestep 702 is [True, False, False, False, True, False]
State prediction error at timestep 702 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 702 of 1
Current timestep = 703. State = [[-0.1539334  -0.00180235]]. Action = [[ 0.11854154 -0.01316619  0.11217824 -0.00386161]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 703 is [True, False, False, False, True, False]
Scene graph at timestep 703 is [True, False, False, False, True, False]
State prediction error at timestep 703 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 703 of 1
Current timestep = 704. State = [[-0.12775129  0.01009875]]. Action = [[ 0.24824977  0.18650311  0.20232978 -0.7561151 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 704 is [True, False, False, False, True, False]
Scene graph at timestep 704 is [True, False, False, False, True, False]
State prediction error at timestep 704 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 704 of 1
Current timestep = 705. State = [[-0.10230854  0.03633895]]. Action = [[ 0.04149997  0.20171317 -0.14429696 -0.38872904]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 705 is [True, False, False, False, True, False]
Current timestep = 706. State = [[-0.08958068  0.04068655]]. Action = [[ 0.21443641 -0.16549622  0.0431594  -0.7862197 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 706 is [True, False, False, False, True, False]
Current timestep = 707. State = [[-0.06087773  0.02787754]]. Action = [[ 0.19834995 -0.11841108 -0.21365896  0.6930845 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 707 is [True, False, False, False, True, False]
Current timestep = 708. State = [[-0.14248918 -0.05834222]]. Action = [[ 0.14444792 -0.23630792 -0.13910666  0.11763299]]. Reward = [100.]
Curr episode timestep = 18
Scene graph at timestep 708 is [True, False, False, False, True, False]
Scene graph at timestep 708 is [True, False, False, False, True, False]
State prediction error at timestep 708 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 708 of 0
Current timestep = 709. State = [[-0.12532344 -0.069681  ]]. Action = [[-0.13083452 -0.03806056 -0.04486085 -0.05449009]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 709 is [True, False, False, False, True, False]
Scene graph at timestep 709 is [True, False, False, False, True, False]
State prediction error at timestep 709 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 709 of -1
Current timestep = 710. State = [[-0.13683745 -0.06942742]]. Action = [[-0.23653378  0.09023404 -0.20732157 -0.18696725]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 710 is [True, False, False, False, True, False]
Current timestep = 711. State = [[-0.14748554 -0.06379882]]. Action = [[ 0.21573222  0.03436261 -0.22917484 -0.11713552]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 711 is [True, False, False, False, True, False]
Current timestep = 712. State = [[-0.14669429 -0.05387774]]. Action = [[-0.0982981   0.10433614 -0.03829411 -0.6752622 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 712 is [True, False, False, False, True, False]
Scene graph at timestep 712 is [True, False, False, False, True, False]
State prediction error at timestep 712 is tensor(5.5540e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 712 of -1
Current timestep = 713. State = [[-0.15440892 -0.04414376]]. Action = [[-0.17843147 -0.00736406 -0.1445445  -0.97083944]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 713 is [True, False, False, False, True, False]
Scene graph at timestep 713 is [True, False, False, False, True, False]
State prediction error at timestep 713 is tensor(8.7664e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 713 of -1
Current timestep = 714. State = [[-0.16055775 -0.03411495]]. Action = [[0.19350523 0.16398942 0.16434443 0.09981894]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 714 is [True, False, False, False, True, False]
Scene graph at timestep 714 is [True, False, False, False, True, False]
State prediction error at timestep 714 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 714 of 1
Current timestep = 715. State = [[-0.15714438 -0.00911839]]. Action = [[-0.04889333  0.22232437 -0.06346779 -0.27224553]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 715 is [True, False, False, False, True, False]
Current timestep = 716. State = [[-0.15405144  0.01877891]]. Action = [[ 0.17369962  0.19655508  0.12357503 -0.76608145]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 716 is [True, False, False, False, True, False]
Scene graph at timestep 716 is [True, False, False, False, True, False]
State prediction error at timestep 716 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 716 of 1
Current timestep = 717. State = [[-0.13392815  0.04464564]]. Action = [[0.1990186  0.12695545 0.06675917 0.82241523]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 717 is [True, False, False, False, True, False]
Scene graph at timestep 717 is [True, False, False, False, True, False]
State prediction error at timestep 717 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 717 of 1
Current timestep = 718. State = [[-0.11204074  0.0691081 ]]. Action = [[0.00146681 0.21471357 0.08013833 0.3029561 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 718 is [True, False, False, False, True, False]
Scene graph at timestep 718 is [True, False, False, False, True, False]
State prediction error at timestep 718 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 718 of -1
Current timestep = 719. State = [[-0.11436255  0.10051336]]. Action = [[-0.05316818  0.20098901  0.19709766  0.20506883]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 719 is [True, False, False, False, True, False]
Current timestep = 720. State = [[-0.11246012  0.11340606]]. Action = [[ 0.14143658 -0.02814165  0.00580394 -0.8464099 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 720 is [True, False, False, False, True, False]
Scene graph at timestep 720 is [True, False, False, False, True, False]
State prediction error at timestep 720 is tensor(2.3497e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 720 of -1
Current timestep = 721. State = [[-0.09732491  0.10422087]]. Action = [[-0.03170854 -0.20678456 -0.24144773 -0.7471031 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 721 is [True, False, False, False, True, False]
Current timestep = 722. State = [[-0.10337463  0.08941776]]. Action = [[-0.23544139 -0.05407092 -0.16928756  0.7132542 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 722 is [True, False, False, False, True, False]
Scene graph at timestep 722 is [True, False, False, False, True, False]
State prediction error at timestep 722 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 722 of 1
Current timestep = 723. State = [[-0.11123445  0.0866387 ]]. Action = [[ 0.12064961  0.08781749 -0.08120254  0.13710415]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 723 is [True, False, False, False, True, False]
Current timestep = 724. State = [[-0.10093498  0.08771433]]. Action = [[ 0.24145263 -0.04609112  0.20814899 -0.23920918]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 724 is [True, False, False, False, True, False]
Scene graph at timestep 724 is [True, False, False, False, True, False]
State prediction error at timestep 724 is tensor(2.9801e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 724 of 1
Current timestep = 725. State = [[-0.08996615  0.0869203 ]]. Action = [[-0.21469982 -0.03244358  0.07145771 -0.9204909 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 725 is [True, False, False, False, True, False]
Current timestep = 726. State = [[-0.0971588   0.09080179]]. Action = [[-0.07016669  0.09934568  0.24122623 -0.4300046 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 726 is [True, False, False, False, True, False]
Current timestep = 727. State = [[-0.11006064  0.08408688]]. Action = [[-0.20187493 -0.23163831  0.11361104 -0.34113836]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 727 is [True, False, False, False, True, False]
Scene graph at timestep 727 is [True, False, False, False, True, False]
State prediction error at timestep 727 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 727 of -1
Current timestep = 728. State = [[-0.13050374  0.07182264]]. Action = [[ 0.10701185  0.12639105  0.12528339 -0.83616716]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 728 is [True, False, False, False, True, False]
Scene graph at timestep 728 is [True, False, False, False, True, False]
State prediction error at timestep 728 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of -1
Current timestep = 729. State = [[-0.12225402  0.07176136]]. Action = [[ 0.19785607 -0.11968443 -0.19657286 -0.02452463]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 729 is [True, False, False, False, True, False]
Scene graph at timestep 729 is [True, False, False, False, True, False]
State prediction error at timestep 729 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 729 of 1
Current timestep = 730. State = [[-0.09863576  0.07188777]]. Action = [[0.23654771 0.10346076 0.22747967 0.13557994]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 730 is [True, False, False, False, True, False]
Current timestep = 731. State = [[-0.08364427  0.07235358]]. Action = [[-0.12454979 -0.0987266   0.1372903   0.71160924]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 731 is [True, False, False, False, True, False]
Scene graph at timestep 731 is [True, False, False, False, True, False]
State prediction error at timestep 731 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 731 of 1
Current timestep = 732. State = [[-0.07875676  0.05868769]]. Action = [[ 0.17550153 -0.14194463  0.09378779 -0.29837012]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 732 is [True, False, False, False, True, False]
Scene graph at timestep 732 is [True, False, False, False, True, False]
State prediction error at timestep 732 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 732 of 1
Current timestep = 733. State = [[-0.06538104  0.0478383 ]]. Action = [[ 0.15360695  0.01334897 -0.16063632 -0.0530839 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 733 is [True, False, False, False, True, False]
Scene graph at timestep 733 is [True, False, False, False, True, False]
State prediction error at timestep 733 is tensor(2.7743e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 733 of 1
Current timestep = 734. State = [[-0.04249271  0.03543543]]. Action = [[ 0.16676897 -0.21092236 -0.10595977  0.30527246]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 734 is [True, False, False, False, True, False]
Current timestep = 735. State = [[-0.22203533 -0.1158278 ]]. Action = [[-0.1408585  -0.23035218 -0.17794569  0.19815445]]. Reward = [100.]
Curr episode timestep = 26
Scene graph at timestep 735 is [False, True, False, False, True, False]
Scene graph at timestep 735 is [True, False, False, False, True, False]
State prediction error at timestep 735 is tensor(0.0286, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 735 of 0
Current timestep = 736. State = [[-0.21760206 -0.13677892]]. Action = [[-0.09958597 -0.13640451 -0.07259351 -0.21840668]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 736 is [True, False, False, False, True, False]
Scene graph at timestep 736 is [True, False, False, True, False, False]
State prediction error at timestep 736 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 736 of -1
Current timestep = 737. State = [[-0.21752973 -0.14985909]]. Action = [[ 0.14169204 -0.03080176 -0.19490904  0.02220905]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 737 is [True, False, False, True, False, False]
Current timestep = 738. State = [[-0.20968452 -0.15703894]]. Action = [[-0.00347787 -0.0718575   0.07633126 -0.80492693]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 738 is [True, False, False, True, False, False]
Current timestep = 739. State = [[-0.20523487 -0.1528673 ]]. Action = [[ 0.10089025  0.1842348   0.06671619 -0.8999063 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 739 is [True, False, False, True, False, False]
Current timestep = 740. State = [[-0.19383778 -0.13297133]]. Action = [[ 0.1150431   0.17860514 -0.1325385  -0.70901805]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 740 is [True, False, False, True, False, False]
Scene graph at timestep 740 is [True, False, False, True, False, False]
State prediction error at timestep 740 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 740 of 1
Current timestep = 741. State = [[-0.1709027  -0.10774322]]. Action = [[ 0.19882265  0.16987038  0.10007074 -0.02124566]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 741 is [True, False, False, True, False, False]
Scene graph at timestep 741 is [True, False, False, False, True, False]
State prediction error at timestep 741 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 741 of 1
Current timestep = 742. State = [[-0.1389519  -0.08477587]]. Action = [[ 0.24295071  0.15767115 -0.00553104  0.35926127]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 742 is [True, False, False, False, True, False]
Current timestep = 743. State = [[-0.12125456 -0.07965133]]. Action = [[-0.04974981 -0.10313514 -0.08847295  0.08358049]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 743 is [True, False, False, False, True, False]
Current timestep = 744. State = [[-0.1188771  -0.09320325]]. Action = [[ 0.00967908 -0.18443847  0.15411723  0.8522216 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 744 is [True, False, False, False, True, False]
Scene graph at timestep 744 is [True, False, False, False, True, False]
State prediction error at timestep 744 is tensor(6.1804e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 744 of -1
Current timestep = 745. State = [[-0.12053596 -0.10697912]]. Action = [[-0.03609538 -0.00093792  0.16451928 -0.47555453]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 745 is [True, False, False, False, True, False]
Current timestep = 746. State = [[-0.12123968 -0.1192293 ]]. Action = [[ 0.03649333 -0.18814741 -0.18859899  0.88329196]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 746 is [True, False, False, False, True, False]
Scene graph at timestep 746 is [True, False, False, False, True, False]
State prediction error at timestep 746 is tensor(9.1410e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 746 of -1
Current timestep = 747. State = [[-0.11498417 -0.14149708]]. Action = [[ 0.12203556 -0.15524116  0.12934     0.5848553 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 747 is [True, False, False, False, True, False]
Current timestep = 748. State = [[-0.09540775 -0.14821738]]. Action = [[ 0.14233214  0.1017493   0.1847651  -0.7331337 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 748 is [True, False, False, True, False, False]
Current timestep = 749. State = [[-0.08871831 -0.13947846]]. Action = [[-0.21720889  0.14730054 -0.11962622 -0.48016167]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 749 is [True, False, False, True, False, False]
Scene graph at timestep 749 is [True, False, False, True, False, False]
State prediction error at timestep 749 is tensor(5.3099e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 749 of -1
Current timestep = 750. State = [[-0.08970033 -0.12229265]]. Action = [[ 0.15827554  0.10304376 -0.18095045 -0.5901343 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 750 is [True, False, False, True, False, False]
Current timestep = 751. State = [[-0.08525969 -0.1165414 ]]. Action = [[ 0.07611254 -0.04599905 -0.01020077 -0.85914254]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 751 is [True, False, False, False, True, False]
Scene graph at timestep 751 is [True, False, False, False, True, False]
State prediction error at timestep 751 is tensor(5.1467e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 751 of 1
Current timestep = 752. State = [[-0.07092175 -0.10344272]]. Action = [[0.16306603 0.2397025  0.06390581 0.7883204 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 752 is [True, False, False, False, True, False]
Scene graph at timestep 752 is [True, False, False, False, True, False]
State prediction error at timestep 752 is tensor(8.9118e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 752 of 1
Current timestep = 753. State = [[-0.04455127 -0.08596461]]. Action = [[ 0.24681956 -0.03653605 -0.10694349 -0.60321665]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 753 is [True, False, False, False, True, False]
Current timestep = 754. State = [[-0.15306923 -0.15311882]]. Action = [[-0.03112791 -0.06729579 -0.12921795  0.6196749 ]]. Reward = [100.]
Curr episode timestep = 18
Scene graph at timestep 754 is [False, True, False, False, True, False]
Scene graph at timestep 754 is [True, False, False, True, False, False]
State prediction error at timestep 754 is tensor(0.0083, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 754 of -1
Current timestep = 755. State = [[-0.12726691 -0.16229734]]. Action = [[ 0.23105872  0.16099662 -0.05174299 -0.4366374 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 755 is [True, False, False, True, False, False]
Current timestep = 756. State = [[-0.10290436 -0.15553163]]. Action = [[ 0.10806754 -0.00355011 -0.02466395  0.9247787 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 756 is [True, False, False, True, False, False]
Current timestep = 757. State = [[-0.08635069 -0.14117838]]. Action = [[ 0.08422717  0.21055779 -0.18145418  0.08291364]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 757 is [True, False, False, True, False, False]
Current timestep = 758. State = [[-0.07991307 -0.13724689]]. Action = [[-0.05021763 -0.17062804  0.07059789 -0.92670166]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 758 is [True, False, False, True, False, False]
Current timestep = 759. State = [[-0.08563187 -0.13955387]]. Action = [[-0.21696968  0.08039945  0.21050546 -0.5488876 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 759 is [True, False, False, True, False, False]
Current timestep = 760. State = [[-0.09076335 -0.14620045]]. Action = [[ 0.08430222 -0.13176286 -0.23011974 -0.32451665]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 760 is [True, False, False, True, False, False]
Scene graph at timestep 760 is [True, False, False, True, False, False]
State prediction error at timestep 760 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 760 of 1
Current timestep = 761. State = [[-0.09445938 -0.1427006 ]]. Action = [[-0.11476186  0.19534743 -0.23990215  0.14734352]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 761 is [True, False, False, True, False, False]
Scene graph at timestep 761 is [True, False, False, True, False, False]
State prediction error at timestep 761 is tensor(1.9521e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 761 of 1
Current timestep = 762. State = [[-0.09800745 -0.14140823]]. Action = [[-0.01588488 -0.1734766   0.00543624  0.9699526 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 762 is [True, False, False, True, False, False]
Scene graph at timestep 762 is [True, False, False, True, False, False]
State prediction error at timestep 762 is tensor(3.4041e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 762 of -1
Current timestep = 763. State = [[-0.10006747 -0.14132202]]. Action = [[ 0.11099103  0.2025784   0.12858105 -0.76501137]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 763 is [True, False, False, True, False, False]
Scene graph at timestep 763 is [True, False, False, True, False, False]
State prediction error at timestep 763 is tensor(4.2773e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 763 of -1
Current timestep = 764. State = [[-0.09841666 -0.11661556]]. Action = [[-0.06954601  0.15687847  0.11817977 -0.91707313]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 764 is [True, False, False, True, False, False]
Scene graph at timestep 764 is [True, False, False, False, True, False]
State prediction error at timestep 764 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 764 of 1
Current timestep = 765. State = [[-0.0991321  -0.10168702]]. Action = [[ 0.03162783  0.04012313 -0.16699605  0.42856884]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 765 is [True, False, False, False, True, False]
Current timestep = 766. State = [[-0.09517818 -0.08524404]]. Action = [[ 0.11804312  0.21437737 -0.15493223 -0.34098876]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 766 is [True, False, False, False, True, False]
Scene graph at timestep 766 is [True, False, False, False, True, False]
State prediction error at timestep 766 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 766 of 1
Current timestep = 767. State = [[-0.09119923 -0.0712079 ]]. Action = [[ 0.03799969 -0.10856254  0.03596476 -0.27508736]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 767 is [True, False, False, False, True, False]
Scene graph at timestep 767 is [True, False, False, False, True, False]
State prediction error at timestep 767 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 767 of 1
Current timestep = 768. State = [[-0.0815694  -0.06629349]]. Action = [[0.15088427 0.16438633 0.14119494 0.59758174]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 768 is [True, False, False, False, True, False]
Current timestep = 769. State = [[-0.06045578 -0.04450135]]. Action = [[0.1768431  0.2187975  0.23304084 0.08621705]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 769 is [True, False, False, False, True, False]
Current timestep = 770. State = [[-0.03818417 -0.03131163]]. Action = [[ 0.13352779 -0.04341285 -0.19437265  0.9833088 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 770 is [True, False, False, False, True, False]
Current timestep = 771. State = [[-0.19010185  0.12025598]]. Action = [[ 0.1070613  -0.24583323 -0.1205987   0.54215336]]. Reward = [100.]
Curr episode timestep = 16
Scene graph at timestep 771 is [False, True, False, False, True, False]
Current timestep = 772. State = [[-0.17117135  0.12271568]]. Action = [[ 0.10175452 -0.23484826  0.04383892 -0.19622481]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 772 is [True, False, False, False, True, False]
Scene graph at timestep 772 is [True, False, False, False, True, False]
State prediction error at timestep 772 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 772 of 1
Current timestep = 773. State = [[-0.16448289  0.11663799]]. Action = [[-0.01762494  0.18401033  0.21161163 -0.881287  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 773 is [True, False, False, False, True, False]
Current timestep = 774. State = [[-0.17480654  0.13879552]]. Action = [[-0.20083445  0.21497083 -0.05231921  0.84630966]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 774 is [True, False, False, False, True, False]
Current timestep = 775. State = [[-0.18452473  0.14548779]]. Action = [[-0.06255725 -0.23091228 -0.00459835 -0.9445711 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 775 is [True, False, False, False, False, True]
Current timestep = 776. State = [[-0.17978428  0.12809858]]. Action = [[ 0.1946063  -0.10974649  0.06854743 -0.83461684]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 776 is [True, False, False, False, False, True]
Current timestep = 777. State = [[-0.17627694  0.11545971]]. Action = [[-0.04804593 -0.05249523  0.20003223 -0.2949612 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 777 is [True, False, False, False, False, True]
Scene graph at timestep 777 is [True, False, False, False, True, False]
State prediction error at timestep 777 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 777 of -1
Current timestep = 778. State = [[-0.16893962  0.11117641]]. Action = [[0.19215122 0.01483691 0.10651225 0.5572386 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 778 is [True, False, False, False, True, False]
Scene graph at timestep 778 is [True, False, False, False, True, False]
State prediction error at timestep 778 is tensor(4.3815e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 778 of 1
Current timestep = 779. State = [[-0.14690863  0.12231725]]. Action = [[ 0.17613584  0.21628809 -0.01406199 -0.8429741 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 779 is [True, False, False, False, True, False]
Scene graph at timestep 779 is [True, False, False, False, True, False]
State prediction error at timestep 779 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 779 of 1
Current timestep = 780. State = [[-0.11743696  0.12633392]]. Action = [[ 0.19454801 -0.17995507  0.18184781  0.9792645 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 780 is [True, False, False, False, True, False]
Scene graph at timestep 780 is [True, False, False, False, False, True]
State prediction error at timestep 780 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 780 of 1
Current timestep = 781. State = [[-0.1000034   0.10660819]]. Action = [[ 0.01344231 -0.16544305  0.2049334  -0.95244145]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 781 is [True, False, False, False, False, True]
Current timestep = 782. State = [[-0.09930062  0.09857522]]. Action = [[-0.10302144  0.03715515  0.17512268  0.19952857]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 782 is [True, False, False, False, True, False]
Scene graph at timestep 782 is [True, False, False, False, True, False]
State prediction error at timestep 782 is tensor(2.5510e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 782 of -1
Current timestep = 783. State = [[-0.09620543  0.08343913]]. Action = [[ 0.09431255 -0.23700203  0.1250751  -0.21041942]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 783 is [True, False, False, False, True, False]
Current timestep = 784. State = [[-0.08551566  0.05638551]]. Action = [[ 0.20916456 -0.20850895  0.22938547 -0.245264  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 784 is [True, False, False, False, True, False]
Current timestep = 785. State = [[-0.07268633  0.04998062]]. Action = [[-0.00658801  0.21156216 -0.22650106 -0.61249995]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 785 is [True, False, False, False, True, False]
Scene graph at timestep 785 is [True, False, False, False, True, False]
State prediction error at timestep 785 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 785 of 1
Current timestep = 786. State = [[-0.07744386  0.07153156]]. Action = [[-0.18759237  0.22084105  0.17269042  0.0465548 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 786 is [True, False, False, False, True, False]
Scene graph at timestep 786 is [True, False, False, False, True, False]
State prediction error at timestep 786 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 786 of -1
Current timestep = 787. State = [[-0.08349147  0.10036236]]. Action = [[0.12284052 0.15331787 0.19512445 0.9555677 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 787 is [True, False, False, False, True, False]
Scene graph at timestep 787 is [True, False, False, False, True, False]
State prediction error at timestep 787 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 787 of -1
Current timestep = 788. State = [[-0.07591168  0.0996413 ]]. Action = [[ 0.10820782 -0.22330469 -0.21777134  0.23398077]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 788 is [True, False, False, False, True, False]
Current timestep = 789. State = [[-0.06833013  0.07936165]]. Action = [[-0.07639855 -0.15631145 -0.03929977  0.9378387 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 789 is [True, False, False, False, True, False]
Current timestep = 790. State = [[-0.05869599  0.0536749 ]]. Action = [[ 0.23202384 -0.21500108 -0.13298565  0.22691727]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 790 is [True, False, False, False, True, False]
Scene graph at timestep 790 is [True, False, False, False, True, False]
State prediction error at timestep 790 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 790 of 1
Current timestep = 791. State = [[-0.0407756   0.02803866]]. Action = [[ 0.06338048 -0.07883985 -0.04728454  0.69765675]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 791 is [True, False, False, False, True, False]
Current timestep = 792. State = [[-0.25107402  0.20614089]]. Action = [[-0.02890734  0.03297409 -0.04235753 -0.6459958 ]]. Reward = [100.]
Curr episode timestep = 20
Scene graph at timestep 792 is [False, True, False, False, True, False]
Scene graph at timestep 792 is [True, False, False, False, False, True]
State prediction error at timestep 792 is tensor(0.0374, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 792 of -1
Current timestep = 793. State = [[-0.23760106  0.22138251]]. Action = [[ 0.17272788 -0.1618011  -0.16973434 -0.02896875]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 793 is [True, False, False, False, False, True]
Scene graph at timestep 793 is [True, False, False, False, False, True]
State prediction error at timestep 793 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 793 of 1
Current timestep = 794. State = [[-0.21228139  0.216642  ]]. Action = [[ 0.156959    0.14291269 -0.09832743  0.44467235]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 794 is [True, False, False, False, False, True]
Scene graph at timestep 794 is [True, False, False, False, False, True]
State prediction error at timestep 794 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 794 of 1
Current timestep = 795. State = [[-0.19801258  0.23838408]]. Action = [[-0.02706948  0.1672597   0.1704889   0.524181  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 795 is [True, False, False, False, False, True]
Scene graph at timestep 795 is [True, False, False, False, False, True]
State prediction error at timestep 795 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 795 of -1
Current timestep = 796. State = [[-0.20632564  0.2629625 ]]. Action = [[-0.06940019  0.19366035  0.05535507  0.7497194 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 796 is [True, False, False, False, False, True]
Scene graph at timestep 796 is [True, False, False, False, False, True]
State prediction error at timestep 796 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 796 of -1
Current timestep = 797. State = [[-0.20991771  0.2802043 ]]. Action = [[-0.09942816  0.19793302  0.07285804 -0.5684727 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 797 is [True, False, False, False, False, True]
Current timestep = 798. State = [[-0.20917736  0.27042967]]. Action = [[-0.11412176 -0.24097995 -0.10562673 -0.03745115]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 798 is [True, False, False, False, False, True]
Scene graph at timestep 798 is [True, False, False, False, False, True]
State prediction error at timestep 798 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 798 of 1
Current timestep = 799. State = [[-0.21870056  0.2526018 ]]. Action = [[-0.2409714  -0.11374387  0.2194379  -0.42608285]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 799 is [True, False, False, False, False, True]
Scene graph at timestep 799 is [True, False, False, False, False, True]
State prediction error at timestep 799 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 799 of -1
Current timestep = 800. State = [[-0.23808491  0.23188736]]. Action = [[-0.04800317 -0.23875628  0.12832886  0.3630135 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 800 is [True, False, False, False, False, True]
Current timestep = 801. State = [[-0.23793966  0.20168138]]. Action = [[ 0.07534432 -0.20545071 -0.12191191 -0.6398769 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 801 is [True, False, False, False, False, True]
Scene graph at timestep 801 is [True, False, False, False, False, True]
State prediction error at timestep 801 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 801 of 1
Current timestep = 802. State = [[-0.24065495  0.16699824]]. Action = [[-0.14411952 -0.2496015   0.0226264   0.6225332 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 802 is [True, False, False, False, False, True]
Current timestep = 803. State = [[-0.2408572   0.13510227]]. Action = [[ 0.2387388  -0.20436136  0.1059047   0.07226491]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 803 is [True, False, False, False, False, True]
Scene graph at timestep 803 is [True, False, False, False, False, True]
State prediction error at timestep 803 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 803 of 1
Current timestep = 804. State = [[-0.2247199   0.10314515]]. Action = [[ 0.08098066 -0.14693871  0.12639302  0.43421984]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 804 is [True, False, False, False, False, True]
Scene graph at timestep 804 is [True, False, False, False, True, False]
State prediction error at timestep 804 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 804 of 1
Current timestep = 805. State = [[-0.21497336  0.075604  ]]. Action = [[ 0.06943339 -0.21411593  0.10430121  0.5747553 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 805 is [True, False, False, False, True, False]
Scene graph at timestep 805 is [True, False, False, False, True, False]
State prediction error at timestep 805 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 805 of 1
Current timestep = 806. State = [[-0.20618124  0.06406146]]. Action = [[ 0.11263996  0.16987664 -0.16208443  0.57048917]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 806 is [True, False, False, False, True, False]
Current timestep = 807. State = [[-0.19290721  0.0648674 ]]. Action = [[ 0.20326751 -0.105057   -0.08543579  0.80770147]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 807 is [True, False, False, False, True, False]
Scene graph at timestep 807 is [True, False, False, False, True, False]
State prediction error at timestep 807 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 807 of 1
Current timestep = 808. State = [[-0.17789632  0.05694372]]. Action = [[-0.09277371 -0.08917958  0.1430698  -0.8021117 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 808 is [True, False, False, False, True, False]
Scene graph at timestep 808 is [True, False, False, False, True, False]
State prediction error at timestep 808 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 808 of 1
Current timestep = 809. State = [[-0.16996099  0.04385971]]. Action = [[ 0.22684449 -0.10012528  0.04188147  0.30155122]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 809 is [True, False, False, False, True, False]
Current timestep = 810. State = [[-0.1520866   0.04339692]]. Action = [[ 0.08974606  0.14690948  0.05464289 -0.82346696]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 810 is [True, False, False, False, True, False]
Current timestep = 811. State = [[-0.13718118  0.03599066]]. Action = [[ 0.14955363 -0.23100975  0.2090292  -0.4133116 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 811 is [True, False, False, False, True, False]
Current timestep = 812. State = [[-0.13046996  0.03540711]]. Action = [[-0.19620933  0.21838227  0.19661918 -0.28650928]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 812 is [True, False, False, False, True, False]
Current timestep = 813. State = [[-0.12922959  0.03917421]]. Action = [[ 0.18688071 -0.12615193 -0.12853244 -0.7749862 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 813 is [True, False, False, False, True, False]
Scene graph at timestep 813 is [True, False, False, False, True, False]
State prediction error at timestep 813 is tensor(3.7649e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 813 of 1
Current timestep = 814. State = [[-0.12599906  0.03742345]]. Action = [[-0.04971874  0.07815349  0.07722029 -0.7500778 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 814 is [True, False, False, False, True, False]
Scene graph at timestep 814 is [True, False, False, False, True, False]
State prediction error at timestep 814 is tensor(3.3775e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 814 of 1
Current timestep = 815. State = [[-0.1185108   0.03063822]]. Action = [[ 0.20104215 -0.18896689 -0.03845915  0.01974273]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 815 is [True, False, False, False, True, False]
Scene graph at timestep 815 is [True, False, False, False, True, False]
State prediction error at timestep 815 is tensor(4.9877e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 815 of 1
Current timestep = 816. State = [[-0.10745873  0.02494257]]. Action = [[-0.09166509  0.13249433 -0.04788329  0.31668603]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 816 is [True, False, False, False, True, False]
Scene graph at timestep 816 is [True, False, False, False, True, False]
State prediction error at timestep 816 is tensor(2.8254e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 816 of 1
Current timestep = 817. State = [[-0.1160747   0.04513321]]. Action = [[-0.2099935   0.21413064  0.00136879  0.5820818 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 817 is [True, False, False, False, True, False]
Current timestep = 818. State = [[-0.12124812  0.05550141]]. Action = [[ 0.05294788 -0.1214188  -0.06698681  0.5210253 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 818 is [True, False, False, False, True, False]
Current timestep = 819. State = [[-0.12336384  0.06010345]]. Action = [[0.00069112 0.14607447 0.00929874 0.2335316 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 819 is [True, False, False, False, True, False]
Current timestep = 820. State = [[-0.13271476  0.05581457]]. Action = [[-0.21632443 -0.20498466 -0.11393116  0.29573274]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 820 is [True, False, False, False, True, False]
Current timestep = 821. State = [[-0.14415821  0.05148738]]. Action = [[-0.06851277  0.09199733  0.07027262 -0.2754705 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 821 is [True, False, False, False, True, False]
Scene graph at timestep 821 is [True, False, False, False, True, False]
State prediction error at timestep 821 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 821 of -1
Current timestep = 822. State = [[-0.15715612  0.05290064]]. Action = [[-0.12054646 -0.03478247 -0.1918719   0.74465716]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 822 is [True, False, False, False, True, False]
Current timestep = 823. State = [[-0.16905537  0.05589146]]. Action = [[-0.04580298  0.07361701 -0.23727715 -0.7574885 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 823 is [True, False, False, False, True, False]
Scene graph at timestep 823 is [True, False, False, False, True, False]
State prediction error at timestep 823 is tensor(7.7167e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 823 of -1
Current timestep = 824. State = [[-0.18378519  0.0687124 ]]. Action = [[-0.18165065  0.11719143  0.19090372  0.02429175]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 824 is [True, False, False, False, True, False]
Scene graph at timestep 824 is [True, False, False, False, True, False]
State prediction error at timestep 824 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 824 of -1
Current timestep = 825. State = [[-0.20650746  0.06760649]]. Action = [[-0.1800248  -0.20655274 -0.18464826 -0.6119856 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 825 is [True, False, False, False, True, False]
Scene graph at timestep 825 is [True, False, False, False, True, False]
State prediction error at timestep 825 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 825 of -1
Current timestep = 826. State = [[-0.22405691  0.04215404]]. Action = [[ 0.08880156 -0.12697355  0.1432735   0.20154941]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 826 is [True, False, False, False, True, False]
Current timestep = 827. State = [[-0.2263664   0.03881246]]. Action = [[-0.07823929  0.08337122 -0.18781687  0.30613923]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 827 is [True, False, False, False, True, False]
Scene graph at timestep 827 is [True, False, False, False, True, False]
State prediction error at timestep 827 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 827 of -1
Current timestep = 828. State = [[-0.23103604  0.0395774 ]]. Action = [[-0.05202028 -0.01908104  0.15673667  0.2425586 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 828 is [True, False, False, False, True, False]
Current timestep = 829. State = [[-0.23420133  0.02849595]]. Action = [[ 0.01049358 -0.18108769 -0.14957981  0.8158542 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 829 is [True, False, False, False, True, False]
Current timestep = 830. State = [[-0.23750044  0.00873124]]. Action = [[-0.05059259 -0.13910335 -0.13611741 -0.9132938 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 830 is [True, False, False, False, True, False]
Current timestep = 831. State = [[-0.24272849  0.00954865]]. Action = [[-0.06503209  0.22813231  0.04643524 -0.2685423 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 831 is [True, False, False, False, True, False]
Scene graph at timestep 831 is [True, False, False, False, True, False]
State prediction error at timestep 831 is tensor(9.5184e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 831 of -1
Current timestep = 832. State = [[-0.24806386  0.03557647]]. Action = [[ 0.16135979  0.2420277  -0.22019404 -0.832306  ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 832 is [True, False, False, False, True, False]
Current timestep = 833. State = [[-0.24423645  0.05027275]]. Action = [[ 0.0012852  -0.00688344 -0.01004905  0.5381229 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 833 is [True, False, False, False, True, False]
Current timestep = 834. State = [[-0.24069081  0.04368776]]. Action = [[-0.00040224 -0.19480452  0.03396541 -0.00217843]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 834 is [True, False, False, False, True, False]
Scene graph at timestep 834 is [True, False, False, False, True, False]
State prediction error at timestep 834 is tensor(5.7511e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 834 of -1
Current timestep = 835. State = [[-0.23193096  0.01966008]]. Action = [[ 0.17248541 -0.23858078  0.11780408 -0.14625734]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 835 is [True, False, False, False, True, False]
Scene graph at timestep 835 is [True, False, False, False, True, False]
State prediction error at timestep 835 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 835 of 1
Current timestep = 836. State = [[-0.2192656   0.00940316]]. Action = [[ 0.046951    0.24236766 -0.09821682 -0.8775295 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 836 is [True, False, False, False, True, False]
Scene graph at timestep 836 is [True, False, False, False, True, False]
State prediction error at timestep 836 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 836 of 1
Current timestep = 837. State = [[-0.21509293  0.03402849]]. Action = [[ 0.03835315  0.13588768  0.23586074 -0.902008  ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 837 is [True, False, False, False, True, False]
Current timestep = 838. State = [[-0.2110992   0.03042218]]. Action = [[ 0.01205635 -0.23153816 -0.14064513 -0.2634778 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 838 is [True, False, False, False, True, False]
Current timestep = 839. State = [[-0.2116897   0.00762846]]. Action = [[-0.10948253 -0.20211826  0.09991863  0.9888394 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 839 is [True, False, False, False, True, False]
Scene graph at timestep 839 is [True, False, False, False, True, False]
State prediction error at timestep 839 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 839 of 1
Current timestep = 840. State = [[-0.22415161 -0.02455107]]. Action = [[-0.22083011 -0.21607949 -0.14877117  0.9073546 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 840 is [True, False, False, False, True, False]
Scene graph at timestep 840 is [True, False, False, False, True, False]
State prediction error at timestep 840 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 840 of -1
Current timestep = 841. State = [[-0.2450206  -0.04154651]]. Action = [[-0.12962559  0.04141751  0.07734135 -0.02220416]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 841 is [True, False, False, False, True, False]
Current timestep = 842. State = [[-0.26045507 -0.03516799]]. Action = [[-0.1382488   0.10587031  0.2152234   0.6021466 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 842 is [True, False, False, False, True, False]
Current timestep = 843. State = [[-0.26830348 -0.0303206 ]]. Action = [[ 0.14217278 -0.05065179 -0.07193521  0.00679886]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 843 is [True, False, False, False, True, False]
Current timestep = 844. State = [[-0.2600658  -0.04065036]]. Action = [[ 0.15749463 -0.16240777 -0.05842581 -0.16596967]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 844 is [True, False, False, False, True, False]
Scene graph at timestep 844 is [True, False, False, False, True, False]
State prediction error at timestep 844 is tensor(6.6425e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 844 of -1
Current timestep = 845. State = [[-0.2528901  -0.05511752]]. Action = [[-0.14705595  0.12863588 -0.12721896  0.26788568]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 845 is [True, False, False, False, True, False]
Scene graph at timestep 845 is [True, False, False, False, True, False]
State prediction error at timestep 845 is tensor(1.5657e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 845 of -1
Current timestep = 846. State = [[-0.2535268  -0.05370915]]. Action = [[-0.10006446  0.06235465  0.03445676  0.3186469 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 846 is [True, False, False, False, True, False]
Scene graph at timestep 846 is [True, False, False, False, True, False]
State prediction error at timestep 846 is tensor(6.2460e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 846 of -1
Current timestep = 847. State = [[-0.25011703 -0.05785663]]. Action = [[ 0.16926104 -0.10720663  0.2264204   0.4749744 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 847 is [True, False, False, False, True, False]
Scene graph at timestep 847 is [True, False, False, False, True, False]
State prediction error at timestep 847 is tensor(2.6158e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 847 of 0
Current timestep = 848. State = [[-0.2511035  -0.06930203]]. Action = [[-0.16162851 -0.11182855 -0.20120847  0.34701896]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 848 is [True, False, False, False, True, False]
Current timestep = 849. State = [[-0.2515673  -0.07047193]]. Action = [[ 0.1034146   0.11934903 -0.19258977 -0.66424817]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 849 is [True, False, False, False, True, False]
Scene graph at timestep 849 is [True, False, False, False, True, False]
State prediction error at timestep 849 is tensor(2.4375e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 849 of 0
Current timestep = 850. State = [[-0.24415287 -0.06310701]]. Action = [[ 0.14846298  0.03634045 -0.12019068 -0.3991667 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 850 is [True, False, False, False, True, False]
Current timestep = 851. State = [[-0.23675999 -0.06850636]]. Action = [[-0.0047652  -0.16510752 -0.1641512  -0.6400926 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 851 is [True, False, False, False, True, False]
Scene graph at timestep 851 is [True, False, False, False, True, False]
State prediction error at timestep 851 is tensor(9.6973e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 851 of -1
Current timestep = 852. State = [[-0.23478647 -0.07974658]]. Action = [[-0.08728081  0.0375542  -0.08915082 -0.47069287]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 852 is [True, False, False, False, True, False]
Scene graph at timestep 852 is [True, False, False, False, True, False]
State prediction error at timestep 852 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 852 of -1
Current timestep = 853. State = [[-0.23304987 -0.07123683]]. Action = [[0.11694604 0.1273269  0.18959558 0.30263638]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 853 is [True, False, False, False, True, False]
Current timestep = 854. State = [[-0.22503617 -0.05102021]]. Action = [[ 0.0781512   0.18918008 -0.20727989  0.49779272]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 854 is [True, False, False, False, True, False]
Current timestep = 855. State = [[-0.22374243 -0.04942679]]. Action = [[-0.15964878 -0.20842957  0.15477073 -0.82106775]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 855 is [True, False, False, False, True, False]
Current timestep = 856. State = [[-0.22417346 -0.05045826]]. Action = [[ 0.14478022  0.13768283 -0.16412951  0.9162748 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 856 is [True, False, False, False, True, False]
Current timestep = 857. State = [[-0.2127248  -0.05768999]]. Action = [[ 0.16727883 -0.23220977 -0.03893176  0.08615828]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 857 is [True, False, False, False, True, False]
Current timestep = 858. State = [[-0.19322711 -0.06722573]]. Action = [[ 0.1384409   0.04751676 -0.00804083  0.5616305 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 858 is [True, False, False, False, True, False]
Current timestep = 859. State = [[-0.18129098 -0.07707124]]. Action = [[-0.00164881 -0.185266   -0.12718078 -0.18895167]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 859 is [True, False, False, False, True, False]
Current timestep = 860. State = [[-0.1773844  -0.09719592]]. Action = [[ 0.0504176  -0.13622496  0.03743696 -0.3949616 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 860 is [True, False, False, False, True, False]
Current timestep = 861. State = [[-0.16408505 -0.10712043]]. Action = [[ 0.20313406  0.03781858 -0.23098558  0.65103865]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 861 is [True, False, False, False, True, False]
Current timestep = 862. State = [[-0.1449617  -0.11293583]]. Action = [[ 0.03940001 -0.09903279  0.17189461  0.9541284 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 862 is [True, False, False, False, True, False]
Current timestep = 863. State = [[-0.13469777 -0.10596735]]. Action = [[ 0.01169229  0.24395192 -0.09985545  0.01886225]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 863 is [True, False, False, False, True, False]
Current timestep = 864. State = [[-0.12563986 -0.08796038]]. Action = [[0.15635893 0.08524948 0.07858846 0.86148787]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 864 is [True, False, False, False, True, False]
Scene graph at timestep 864 is [True, False, False, False, True, False]
State prediction error at timestep 864 is tensor(5.8127e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 864 of 1
Current timestep = 865. State = [[-0.11060175 -0.08509978]]. Action = [[-0.06666754 -0.1509073   0.05279461 -0.4869181 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 865 is [True, False, False, False, True, False]
Scene graph at timestep 865 is [True, False, False, False, True, False]
State prediction error at timestep 865 is tensor(4.0635e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 865 of -1
Current timestep = 866. State = [[-0.11211032 -0.09359731]]. Action = [[-0.08950865  0.02820644 -0.07529974  0.5811856 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 866 is [True, False, False, False, True, False]
Scene graph at timestep 866 is [True, False, False, False, True, False]
State prediction error at timestep 866 is tensor(6.1042e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 866 of -1
Current timestep = 867. State = [[-0.11289726 -0.08650469]]. Action = [[ 0.1070196   0.12980688  0.24752992 -0.5684234 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 867 is [True, False, False, False, True, False]
Scene graph at timestep 867 is [True, False, False, False, True, False]
State prediction error at timestep 867 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 867 of 1
Current timestep = 868. State = [[-0.10890689 -0.0687326 ]]. Action = [[ 0.10366496  0.1489113   0.04317942 -0.37095988]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 868 is [True, False, False, False, True, False]
Current timestep = 869. State = [[-0.09341472 -0.04535267]]. Action = [[ 0.23245025  0.19850641 -0.09443025 -0.7581448 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 869 is [True, False, False, False, True, False]
Scene graph at timestep 869 is [True, False, False, False, True, False]
State prediction error at timestep 869 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 869 of 1
Current timestep = 870. State = [[-0.06903048 -0.02665031]]. Action = [[-0.02871576  0.02891326  0.2256217  -0.67288715]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 870 is [True, False, False, False, True, False]
Scene graph at timestep 870 is [True, False, False, False, True, False]
State prediction error at timestep 870 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 870 of 1
Current timestep = 871. State = [[-0.06597473 -0.02457074]]. Action = [[ 0.07607618 -0.05345598  0.1400739  -0.44177055]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 871 is [True, False, False, False, True, False]
Scene graph at timestep 871 is [True, False, False, False, True, False]
State prediction error at timestep 871 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 871 of 1
Current timestep = 872. State = [[-0.06714848 -0.01858773]]. Action = [[-0.18842982  0.11891186  0.23539305  0.6310781 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 872 is [True, False, False, False, True, False]
Current timestep = 873. State = [[-0.0729293  -0.00063374]]. Action = [[-0.0371947   0.19961062 -0.10508752  0.64982533]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 873 is [True, False, False, False, True, False]
Scene graph at timestep 873 is [True, False, False, False, True, False]
State prediction error at timestep 873 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 873 of 1
Current timestep = 874. State = [[-0.07213471  0.01867528]]. Action = [[0.23647171 0.0017308  0.06732431 0.35236895]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 874 is [True, False, False, False, True, False]
Current timestep = 875. State = [[-0.06159399  0.00849354]]. Action = [[ 0.09020683 -0.20211226  0.09903926 -0.30419672]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 875 is [True, False, False, False, True, False]
Scene graph at timestep 875 is [True, False, False, False, True, False]
State prediction error at timestep 875 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 875 of 1
Current timestep = 876. State = [[-0.04428738 -0.01841043]]. Action = [[ 0.16673553 -0.19493726  0.00603992 -0.2854954 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 876 is [True, False, False, False, True, False]
Scene graph at timestep 876 is [False, True, False, False, True, False]
State prediction error at timestep 876 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 876 of 1
Current timestep = 877. State = [[-0.2688872   0.21354386]]. Action = [[ 0.03918448  0.16149533 -0.18802492  0.8449564 ]]. Reward = [100.]
Curr episode timestep = 84
Scene graph at timestep 877 is [False, True, False, False, True, False]
Current timestep = 878. State = [[-0.25387222  0.22767992]]. Action = [[ 0.19366768 -0.17429131 -0.06673685 -0.27366436]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 878 is [True, False, False, False, False, True]
Scene graph at timestep 878 is [True, False, False, False, False, True]
State prediction error at timestep 878 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 878 of 1
Current timestep = 879. State = [[-0.24555959  0.21886767]]. Action = [[-0.19753315  0.00441694 -0.02312246 -0.34511167]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 879 is [True, False, False, False, False, True]
Scene graph at timestep 879 is [True, False, False, False, False, True]
State prediction error at timestep 879 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 879 of 1
Current timestep = 880. State = [[-0.25406176  0.23081787]]. Action = [[ 0.08003175  0.20317581 -0.12194937  0.7016573 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 880 is [True, False, False, False, False, True]
Scene graph at timestep 880 is [True, False, False, False, False, True]
State prediction error at timestep 880 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 880 of -1
Current timestep = 881. State = [[-0.24541768  0.23053606]]. Action = [[ 0.14017007 -0.21722521 -0.2082739  -0.9779292 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 881 is [True, False, False, False, False, True]
Current timestep = 882. State = [[-0.23365264  0.20660116]]. Action = [[ 0.03539956 -0.21211328  0.14706504  0.5571337 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 882 is [True, False, False, False, False, True]
Current timestep = 883. State = [[-0.21947579  0.18050098]]. Action = [[ 0.16223094 -0.164699    0.18374929 -0.3334419 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 883 is [True, False, False, False, False, True]
Scene graph at timestep 883 is [True, False, False, False, False, True]
State prediction error at timestep 883 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 883 of 1
Current timestep = 884. State = [[-0.19965224  0.16114205]]. Action = [[ 0.12962282 -0.04330446  0.2024113   0.61945486]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 884 is [True, False, False, False, False, True]
Scene graph at timestep 884 is [True, False, False, False, False, True]
State prediction error at timestep 884 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 884 of 1
Current timestep = 885. State = [[-0.18189035  0.15569077]]. Action = [[ 0.12618327 -0.0157001   0.16472828 -0.01123953]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 885 is [True, False, False, False, False, True]
Current timestep = 886. State = [[-0.1747132   0.14586599]]. Action = [[-0.18418987 -0.1828208  -0.18885109 -0.515035  ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 886 is [True, False, False, False, False, True]
Current timestep = 887. State = [[-0.17200322  0.1343525 ]]. Action = [[ 0.19749388  0.01073104  0.14962426 -0.68344605]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 887 is [True, False, False, False, False, True]
Current timestep = 888. State = [[-0.16576108  0.11697245]]. Action = [[-0.03780979 -0.23697886  0.1761108   0.1261487 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 888 is [True, False, False, False, False, True]
Scene graph at timestep 888 is [True, False, False, False, True, False]
State prediction error at timestep 888 is tensor(5.9097e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 888 of 1
Current timestep = 889. State = [[-0.15447739  0.11059093]]. Action = [[ 2.2533071e-01  2.2445619e-01  4.9555302e-04 -5.1028234e-01]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 889 is [True, False, False, False, True, False]
Current timestep = 890. State = [[-0.13443679  0.12128216]]. Action = [[ 0.12154734  0.01270598 -0.06867877  0.6961752 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 890 is [True, False, False, False, True, False]
Current timestep = 891. State = [[-0.11250798  0.11999375]]. Action = [[ 0.19157168 -0.08671042 -0.00294112  0.7463107 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 891 is [True, False, False, False, True, False]
Scene graph at timestep 891 is [True, False, False, False, True, False]
State prediction error at timestep 891 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 891 of 1
Current timestep = 892. State = [[-0.09245569  0.10254429]]. Action = [[-0.07359205 -0.24476093 -0.24581957 -0.679046  ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 892 is [True, False, False, False, True, False]
Current timestep = 893. State = [[-0.09270946  0.09002616]]. Action = [[-0.12562668  0.01629302 -0.06329823  0.44129348]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 893 is [True, False, False, False, True, False]
Scene graph at timestep 893 is [True, False, False, False, True, False]
State prediction error at timestep 893 is tensor(2.4536e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 893 of 1
Current timestep = 894. State = [[-0.09641031  0.08153   ]]. Action = [[ 0.05351579 -0.10066114 -0.09925021  0.98450756]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 894 is [True, False, False, False, True, False]
Scene graph at timestep 894 is [True, False, False, False, True, False]
State prediction error at timestep 894 is tensor(4.0364e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 894 of 1
Current timestep = 895. State = [[-0.09462478  0.06016395]]. Action = [[-0.00978152 -0.1995463  -0.1994464   0.2655878 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 895 is [True, False, False, False, True, False]
Current timestep = 896. State = [[-0.09529094  0.05561295]]. Action = [[-0.06354384  0.15713501  0.04661819 -0.9728238 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 896 is [True, False, False, False, True, False]
Scene graph at timestep 896 is [True, False, False, False, True, False]
State prediction error at timestep 896 is tensor(3.5258e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 896 of 1
Current timestep = 897. State = [[-0.09499788  0.07354624]]. Action = [[ 0.22065574  0.20659399 -0.18729314 -0.9589573 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 897 is [True, False, False, False, True, False]
Scene graph at timestep 897 is [True, False, False, False, True, False]
State prediction error at timestep 897 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 897 of 1
Current timestep = 898. State = [[-0.08280629  0.08748442]]. Action = [[ 0.23172662 -0.02061667 -0.11147112  0.69852877]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 898 is [True, False, False, False, True, False]
Scene graph at timestep 898 is [True, False, False, False, True, False]
State prediction error at timestep 898 is tensor(3.3373e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 898 of 1
Current timestep = 899. State = [[-0.05211065  0.09229212]]. Action = [[0.21992439 0.0756225  0.032401   0.27584195]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 899 is [True, False, False, False, True, False]
Scene graph at timestep 899 is [True, False, False, False, True, False]
State prediction error at timestep 899 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 899 of 1
Current timestep = 900. State = [[-0.225128    0.12028523]]. Action = [[-0.06095235 -0.24373835 -0.08211511  0.07409596]]. Reward = [100.]
Curr episode timestep = 22
Scene graph at timestep 900 is [True, False, False, False, True, False]
Current timestep = 901. State = [[-0.21195503  0.14274415]]. Action = [[ 0.10622448  0.13036156 -0.04941872 -0.8203434 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 901 is [True, False, False, False, True, False]
Scene graph at timestep 901 is [True, False, False, False, False, True]
State prediction error at timestep 901 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 901 of -1
Current timestep = 902. State = [[-0.19897772  0.14352208]]. Action = [[-0.00402139 -0.21610275  0.0706563  -0.4857263 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 902 is [True, False, False, False, False, True]
Current timestep = 903. State = [[-0.19349317  0.11922591]]. Action = [[ 0.04227182 -0.20928417  0.0626258  -0.44610953]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 903 is [True, False, False, False, False, True]
Scene graph at timestep 903 is [True, False, False, False, True, False]
State prediction error at timestep 903 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 903 of 1
Current timestep = 904. State = [[-0.18239996  0.10244358]]. Action = [[0.2355614  0.03610393 0.07924798 0.5303285 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 904 is [True, False, False, False, True, False]
Current timestep = 905. State = [[-0.15928513  0.10775061]]. Action = [[ 0.10926476  0.08338889 -0.2238335   0.2737975 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 905 is [True, False, False, False, True, False]
Scene graph at timestep 905 is [True, False, False, False, True, False]
State prediction error at timestep 905 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 905 of 1
Current timestep = 906. State = [[-0.13824919  0.09828693]]. Action = [[ 0.16134158 -0.24794099 -0.15832698 -0.44973284]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 906 is [True, False, False, False, True, False]
Scene graph at timestep 906 is [True, False, False, False, True, False]
State prediction error at timestep 906 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 906 of 1
Current timestep = 907. State = [[-0.11487747  0.07654561]]. Action = [[ 0.15800953 -0.0770061  -0.22650923 -0.673909  ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 907 is [True, False, False, False, True, False]
Current timestep = 908. State = [[-0.09222674  0.0829573 ]]. Action = [[ 0.21992025  0.21173632 -0.17074287 -0.36286694]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 908 is [True, False, False, False, True, False]
Current timestep = 909. State = [[-0.06613841  0.09353299]]. Action = [[ 0.11106172  0.0024336  -0.11110643 -0.74690706]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 909 is [True, False, False, False, True, False]
Current timestep = 910. State = [[-0.05410482  0.08556482]]. Action = [[-0.14619704 -0.21082726  0.05220756 -0.32560027]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 910 is [True, False, False, False, True, False]
Scene graph at timestep 910 is [True, False, False, False, True, False]
State prediction error at timestep 910 is tensor(8.6445e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 910 of 1
Current timestep = 911. State = [[-0.05064942  0.05942688]]. Action = [[ 0.1661956  -0.20694196 -0.19362532  0.68707776]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 911 is [True, False, False, False, True, False]
Current timestep = 912. State = [[-0.04947059  0.03264987]]. Action = [[-0.2172617  -0.20055285 -0.16690175 -0.04254127]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 912 is [True, False, False, False, True, False]
Current timestep = 913. State = [[-0.04989699  0.00403914]]. Action = [[ 0.10585833 -0.213013    0.15979731 -0.57933956]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 913 is [False, True, False, False, True, False]
Current timestep = 914. State = [[-0.04272792 -0.02814449]]. Action = [[ 0.1720038  -0.23045108 -0.09793377 -0.95935255]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 914 is [False, True, False, False, True, False]
Scene graph at timestep 914 is [False, True, False, False, True, False]
State prediction error at timestep 914 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 914 of -1
Current timestep = 915. State = [[-0.03784338 -0.05718476]]. Action = [[-0.16978106 -0.06333008  0.10567868 -0.12728745]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 915 is [False, True, False, False, True, False]
Current timestep = 916. State = [[-0.03726128 -0.06868442]]. Action = [[ 0.10360187 -0.11910902 -0.18522872  0.03889048]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 916 is [False, True, False, False, True, False]
Scene graph at timestep 916 is [False, True, False, False, True, False]
State prediction error at timestep 916 is tensor(4.2692e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 916 of -1
Current timestep = 917. State = [[-0.26098153 -0.00081545]]. Action = [[ 0.19298494  0.01178038  0.06603652 -0.42371196]]. Reward = [100.]
Curr episode timestep = 16
Scene graph at timestep 917 is [False, True, False, False, True, False]
Current timestep = 918. State = [[-0.26016954 -0.00608158]]. Action = [[ 0.03240746 -0.10123006 -0.0689185   0.57135475]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 918 is [True, False, False, False, True, False]
Current timestep = 919. State = [[-0.25766873 -0.00676013]]. Action = [[ 0.08887821  0.13093278 -0.126434   -0.9037752 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 919 is [True, False, False, False, True, False]
Scene graph at timestep 919 is [True, False, False, False, True, False]
State prediction error at timestep 919 is tensor(2.8872e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 919 of 1
Current timestep = 920. State = [[-0.24521358 -0.00331227]]. Action = [[ 0.174658   -0.02532944  0.04939121 -0.3117664 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 920 is [True, False, False, False, True, False]
Current timestep = 921. State = [[-0.23578613 -0.00342009]]. Action = [[-0.12030928 -0.00458045 -0.18516593  0.7223041 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 921 is [True, False, False, False, True, False]
Current timestep = 922. State = [[-0.23267771 -0.00114091]]. Action = [[ 0.16908407  0.0654586  -0.14652878 -0.52864295]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 922 is [True, False, False, False, True, False]
Scene graph at timestep 922 is [True, False, False, False, True, False]
State prediction error at timestep 922 is tensor(5.5490e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 922 of 1
Current timestep = 923. State = [[-0.22174631 -0.00831817]]. Action = [[ 0.0761289  -0.2105299   0.11492378  0.1975255 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 923 is [True, False, False, False, True, False]
Scene graph at timestep 923 is [True, False, False, False, True, False]
State prediction error at timestep 923 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 923 of 1
Current timestep = 924. State = [[-0.21563101 -0.01113657]]. Action = [[-0.11933047  0.16889831 -0.12504622 -0.6765295 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 924 is [True, False, False, False, True, False]
Scene graph at timestep 924 is [True, False, False, False, True, False]
State prediction error at timestep 924 is tensor(4.0085e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 924 of 1
Current timestep = 925. State = [[-0.20997263 -0.01367257]]. Action = [[ 0.22801524 -0.22157769  0.01801571 -0.48488057]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 925 is [True, False, False, False, True, False]
Current timestep = 926. State = [[-0.20047343 -0.03605184]]. Action = [[-0.01286404 -0.1775136  -0.00202279 -0.5982512 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 926 is [True, False, False, False, True, False]
Current timestep = 927. State = [[-0.190448   -0.06269138]]. Action = [[ 0.16680789 -0.21946278 -0.171783    0.9541782 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 927 is [True, False, False, False, True, False]
Scene graph at timestep 927 is [True, False, False, False, True, False]
State prediction error at timestep 927 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 927 of -1
Current timestep = 928. State = [[-0.1814532  -0.07584774]]. Action = [[-0.18903469  0.18627042  0.01988298  0.10223329]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 928 is [True, False, False, False, True, False]
Current timestep = 929. State = [[-0.1848347 -0.0661715]]. Action = [[ 0.0480116   0.01774824 -0.11055174 -0.46655655]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 929 is [True, False, False, False, True, False]
Current timestep = 930. State = [[-0.18931441 -0.05933388]]. Action = [[-0.15198527  0.06890836 -0.03190956 -0.31355762]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 930 is [True, False, False, False, True, False]
Current timestep = 931. State = [[-0.1890042  -0.05035564]]. Action = [[ 0.22931373  0.05791014  0.04787734 -0.9494801 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 931 is [True, False, False, False, True, False]
Scene graph at timestep 931 is [True, False, False, False, True, False]
State prediction error at timestep 931 is tensor(3.9649e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 931 of -1
Current timestep = 932. State = [[-0.17955129 -0.03168164]]. Action = [[ 0.10272157  0.19977328 -0.23471896  0.2776624 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 932 is [True, False, False, False, True, False]
Scene graph at timestep 932 is [True, False, False, False, True, False]
State prediction error at timestep 932 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 932 of 1
Current timestep = 933. State = [[-0.15632617 -0.02433569]]. Action = [[ 0.23403493 -0.1612079  -0.00935625 -0.8372589 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 933 is [True, False, False, False, True, False]
Current timestep = 934. State = [[-0.13538183 -0.02346498]]. Action = [[ 0.07545906  0.150801    0.22075123 -0.7420307 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 934 is [True, False, False, False, True, False]
Scene graph at timestep 934 is [True, False, False, False, True, False]
State prediction error at timestep 934 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 934 of 1
Current timestep = 935. State = [[-0.11158387 -0.02784913]]. Action = [[ 0.23917621 -0.18933763 -0.24220799  0.18833399]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 935 is [True, False, False, False, True, False]
Current timestep = 936. State = [[-0.09436221 -0.02849368]]. Action = [[-0.04986547  0.14699805  0.04230279  0.7263099 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 936 is [True, False, False, False, True, False]
Current timestep = 937. State = [[-0.09877336 -0.02460647]]. Action = [[-0.23062958 -0.01197484 -0.03061289 -0.5459509 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 937 is [True, False, False, False, True, False]
Scene graph at timestep 937 is [True, False, False, False, True, False]
State prediction error at timestep 937 is tensor(4.2273e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 937 of 1
Current timestep = 938. State = [[-0.10482488 -0.01183665]]. Action = [[ 0.03823605  0.17812705 -0.07979052 -0.21721238]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 938 is [True, False, False, False, True, False]
Scene graph at timestep 938 is [True, False, False, False, True, False]
State prediction error at timestep 938 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 938 of 1
Current timestep = 939. State = [[-0.10289585  0.00968854]]. Action = [[ 0.20178789  0.13454038  0.09630153 -0.6611864 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 939 is [True, False, False, False, True, False]
Scene graph at timestep 939 is [True, False, False, False, True, False]
State prediction error at timestep 939 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 939 of 1
Current timestep = 940. State = [[-0.08484221  0.02662849]]. Action = [[ 0.22945154  0.11646873 -0.13826393  0.39801216]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 940 is [True, False, False, False, True, False]
Current timestep = 941. State = [[-0.05471541  0.0318011 ]]. Action = [[ 0.19760662 -0.07277086 -0.17978309  0.16282701]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 941 is [True, False, False, False, True, False]
Current timestep = 942. State = [[-0.27180606  0.07720317]]. Action = [[ 0.20534015 -0.22942704  0.2438755  -0.36834836]]. Reward = [100.]
Curr episode timestep = 24
Scene graph at timestep 942 is [True, False, False, False, True, False]
Current timestep = 943. State = [[-0.26859793  0.08337715]]. Action = [[-7.7098608e-05 -8.5713148e-02  7.3539466e-02  9.2826772e-01]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 943 is [True, False, False, False, True, False]
Scene graph at timestep 943 is [True, False, False, False, True, False]
State prediction error at timestep 943 is tensor(1.8378e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 943 of 1
Current timestep = 944. State = [[-0.2681984   0.08844587]]. Action = [[ 0.04007488  0.17692786  0.04967901 -0.8797103 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 944 is [True, False, False, False, True, False]
Current timestep = 945. State = [[-0.2573034   0.10224459]]. Action = [[ 0.18279898  0.09423926 -0.05663055 -0.83077025]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 945 is [True, False, False, False, True, False]
Scene graph at timestep 945 is [True, False, False, False, True, False]
State prediction error at timestep 945 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 945 of -1
Current timestep = 946. State = [[-0.23660854  0.10668389]]. Action = [[ 0.03318113 -0.14434302 -0.1930511   0.17852521]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 946 is [True, False, False, False, True, False]
Scene graph at timestep 946 is [True, False, False, False, True, False]
State prediction error at timestep 946 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 946 of 1
Current timestep = 947. State = [[-0.23137367  0.11096634]]. Action = [[ 0.08620286  0.21403575 -0.1640263   0.90746856]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 947 is [True, False, False, False, True, False]
Scene graph at timestep 947 is [True, False, False, False, True, False]
State prediction error at timestep 947 is tensor(1.6753e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 947 of -1
Current timestep = 948. State = [[-0.21226706  0.12266333]]. Action = [[ 0.20411676 -0.03605077 -0.00051494 -0.47021806]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 948 is [True, False, False, False, True, False]
Scene graph at timestep 948 is [True, False, False, False, True, False]
State prediction error at timestep 948 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 948 of 1
Current timestep = 949. State = [[-0.18494526  0.1102554 ]]. Action = [[ 0.13791248 -0.229197   -0.14350225 -0.46266997]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 949 is [True, False, False, False, True, False]
Scene graph at timestep 949 is [True, False, False, False, True, False]
State prediction error at timestep 949 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 949 of 1
Current timestep = 950. State = [[-0.17272401  0.07973164]]. Action = [[-0.11727306 -0.22705132 -0.0147326  -0.19626617]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 950 is [True, False, False, False, True, False]
Scene graph at timestep 950 is [True, False, False, False, True, False]
State prediction error at timestep 950 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 950 of 1
Current timestep = 951. State = [[-0.16998902  0.06997266]]. Action = [[ 0.1692161   0.16023737 -0.14881198 -0.2334671 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 951 is [True, False, False, False, True, False]
Scene graph at timestep 951 is [True, False, False, False, True, False]
State prediction error at timestep 951 is tensor(2.7615e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 951 of -1
Current timestep = 952. State = [[-0.15523753  0.08725922]]. Action = [[ 0.2049692   0.15871447 -0.03044184 -0.66030914]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 952 is [True, False, False, False, True, False]
Scene graph at timestep 952 is [True, False, False, False, True, False]
State prediction error at timestep 952 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 952 of -1
Current timestep = 953. State = [[-0.13085565  0.11375862]]. Action = [[0.08098799 0.22212484 0.16941619 0.8576977 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 953 is [True, False, False, False, True, False]
Current timestep = 954. State = [[-0.12662783  0.11783165]]. Action = [[-0.22109933 -0.23233134 -0.09500664  0.93997645]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 954 is [True, False, False, False, True, False]
Current timestep = 955. State = [[-0.12515643  0.11852538]]. Action = [[0.22911274 0.19935256 0.21933347 0.16742802]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 955 is [True, False, False, False, True, False]
Current timestep = 956. State = [[-0.11973788  0.11321652]]. Action = [[-0.02695084 -0.23070426  0.09630856 -0.86627036]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 956 is [True, False, False, False, True, False]
Scene graph at timestep 956 is [True, False, False, False, True, False]
State prediction error at timestep 956 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 956 of -1
Current timestep = 957. State = [[-0.11054555  0.09864215]]. Action = [[ 0.1742149  -0.04706323 -0.01928067  0.80631185]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 957 is [True, False, False, False, True, False]
Scene graph at timestep 957 is [True, False, False, False, True, False]
State prediction error at timestep 957 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 957 of 1
Current timestep = 958. State = [[-0.09598493  0.090318  ]]. Action = [[ 0.13953918 -0.03640157 -0.02103062 -0.347525  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 958 is [True, False, False, False, True, False]
Current timestep = 959. State = [[-0.07406195  0.09889729]]. Action = [[ 0.19539532  0.18405366 -0.15560079  0.7537341 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 959 is [True, False, False, False, True, False]
Scene graph at timestep 959 is [True, False, False, False, True, False]
State prediction error at timestep 959 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 959 of 1
Current timestep = 960. State = [[-0.04826749  0.10130319]]. Action = [[-0.11814338 -0.18133071 -0.03187285  0.7618432 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 960 is [True, False, False, False, True, False]
Scene graph at timestep 960 is [False, True, False, False, True, False]
State prediction error at timestep 960 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 960 of 1
Current timestep = 961. State = [[-0.04709817  0.08643736]]. Action = [[ 0.03322545 -0.08617161  0.12760776 -0.6845205 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 961 is [False, True, False, False, True, False]
Current timestep = 962. State = [[-0.04720794  0.06756266]]. Action = [[-0.08834526 -0.2186686   0.02747253 -0.6577895 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 962 is [False, True, False, False, True, False]
Current timestep = 963. State = [[-0.05077359  0.03804581]]. Action = [[-0.03400832 -0.23268853  0.21071413  0.7872095 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 963 is [False, True, False, False, True, False]
Scene graph at timestep 963 is [True, False, False, False, True, False]
State prediction error at timestep 963 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 963 of 1
Current timestep = 964. State = [[-0.0539212   0.01483829]]. Action = [[-0.04092294 -0.06541707  0.10872096  0.76057374]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 964 is [True, False, False, False, True, False]
Current timestep = 965. State = [[-0.05281731  0.00422766]]. Action = [[ 0.19095045 -0.0231548  -0.0466512   0.44627237]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 965 is [True, False, False, False, True, False]
Scene graph at timestep 965 is [True, False, False, False, True, False]
State prediction error at timestep 965 is tensor(7.5680e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 965 of -1
Current timestep = 966. State = [[-0.04609476  0.00880378]]. Action = [[ 0.09984744  0.1770271  -0.20820144 -0.8550158 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 966 is [True, False, False, False, True, False]
Scene graph at timestep 966 is [False, True, False, False, True, False]
State prediction error at timestep 966 is tensor(3.5693e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 966 of 1
Current timestep = 967. State = [[-0.04339425  0.0203026 ]]. Action = [[-0.09382743 -0.00962652  0.18338251  0.31686044]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 967 is [False, True, False, False, True, False]
Current timestep = 968. State = [[-0.03764637  0.0300346 ]]. Action = [[ 0.23649794  0.1605472   0.10420486 -0.623922  ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 968 is [False, True, False, False, True, False]
Scene graph at timestep 968 is [False, True, False, False, True, False]
State prediction error at timestep 968 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 968 of 1
Current timestep = 969. State = [[-0.26235232 -0.14277652]]. Action = [[-0.00546801 -0.17921925  0.18636292 -0.33655792]]. Reward = [100.]
Curr episode timestep = 26
Scene graph at timestep 969 is [False, True, False, False, True, False]
Current timestep = 970. State = [[-0.25838634 -0.14737229]]. Action = [[ 0.08212438  0.21088171 -0.1696024   0.5165318 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 970 is [True, False, False, True, False, False]
Current timestep = 971. State = [[-0.25446352 -0.13724199]]. Action = [[-0.22890592 -0.07803845 -0.05928457  0.47638655]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 971 is [True, False, False, True, False, False]
Scene graph at timestep 971 is [True, False, False, True, False, False]
State prediction error at timestep 971 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 971 of 1
Current timestep = 972. State = [[-0.24166526 -0.12895583]]. Action = [[ 0.2398206   0.08003649  0.05694985 -0.8876791 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 972 is [True, False, False, True, False, False]
Current timestep = 973. State = [[-0.21539582 -0.11192676]]. Action = [[ 0.17442226  0.18210262  0.05416119 -0.09795702]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 973 is [True, False, False, True, False, False]
Current timestep = 974. State = [[-0.20226137 -0.09077574]]. Action = [[-0.06943217  0.1430237  -0.04643022  0.07892001]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 974 is [True, False, False, False, True, False]
Scene graph at timestep 974 is [True, False, False, False, True, False]
State prediction error at timestep 974 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 974 of 1
Current timestep = 975. State = [[-0.19798192 -0.06492206]]. Action = [[ 0.13376692  0.18251264 -0.02928923  0.29689336]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 975 is [True, False, False, False, True, False]
Scene graph at timestep 975 is [True, False, False, False, True, False]
State prediction error at timestep 975 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 975 of 1
Current timestep = 976. State = [[-0.18060827 -0.03883465]]. Action = [[0.20572662 0.17432716 0.1393581  0.5747435 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 976 is [True, False, False, False, True, False]
Scene graph at timestep 976 is [True, False, False, False, True, False]
State prediction error at timestep 976 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 976 of 1
Current timestep = 977. State = [[-0.1484355  -0.01250598]]. Action = [[ 0.196013    0.17208469 -0.20082445 -0.7025922 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 977 is [True, False, False, False, True, False]
Current timestep = 978. State = [[-0.12764427  0.00039539]]. Action = [[ 0.11054665  0.02159601 -0.08073017  0.04357803]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 978 is [True, False, False, False, True, False]
Scene graph at timestep 978 is [True, False, False, False, True, False]
State prediction error at timestep 978 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 978 of 1
Current timestep = 979. State = [[-0.11548536  0.00143912]]. Action = [[-0.09511176 -0.08044887 -0.18702678 -0.49254978]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 979 is [True, False, False, False, True, False]
Current timestep = 980. State = [[-0.11996026  0.00758246]]. Action = [[-0.10467014  0.1820448  -0.21540403 -0.19433194]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 980 is [True, False, False, False, True, False]
Scene graph at timestep 980 is [True, False, False, False, True, False]
State prediction error at timestep 980 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 980 of -1
Current timestep = 981. State = [[-0.12502725  0.02908324]]. Action = [[ 0.14208919  0.11512184 -0.06526563 -0.00695908]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 981 is [True, False, False, False, True, False]
Current timestep = 982. State = [[-0.11878312  0.02351726]]. Action = [[ 0.05457881 -0.21215583 -0.2054876   0.30335546]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 982 is [True, False, False, False, True, False]
Scene graph at timestep 982 is [True, False, False, False, True, False]
State prediction error at timestep 982 is tensor(1.3371e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 982 of 1
Current timestep = 983. State = [[-0.11411005  0.00672612]]. Action = [[-0.14041223 -0.09109053  0.10531345  0.6793728 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 983 is [True, False, False, False, True, False]
Current timestep = 984. State = [[-0.10841698 -0.00497658]]. Action = [[ 0.23726481 -0.07120243 -0.00483406 -0.32868576]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 984 is [True, False, False, False, True, False]
Scene graph at timestep 984 is [True, False, False, False, True, False]
State prediction error at timestep 984 is tensor(5.9951e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 984 of 1
Current timestep = 985. State = [[-0.09915795 -0.02591344]]. Action = [[-0.10260929 -0.24722841  0.01107299  0.09094906]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 985 is [True, False, False, False, True, False]
Current timestep = 986. State = [[-0.10065287 -0.03988647]]. Action = [[ 0.05085108  0.05963343  0.0199708  -0.58268017]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 986 is [True, False, False, False, True, False]
Current timestep = 987. State = [[-0.10127683 -0.02927474]]. Action = [[ 0.00324035  0.18607453 -0.01558602 -0.18568075]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 987 is [True, False, False, False, True, False]
Current timestep = 988. State = [[-0.10651554 -0.01101412]]. Action = [[-0.19692615  0.11568633  0.00523037 -0.19286579]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 988 is [True, False, False, False, True, False]
Scene graph at timestep 988 is [True, False, False, False, True, False]
State prediction error at timestep 988 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 988 of 1
Current timestep = 989. State = [[-0.10988172 -0.00740825]]. Action = [[ 0.15714622 -0.15637639  0.10173836  0.41436243]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 989 is [True, False, False, False, True, False]
Scene graph at timestep 989 is [True, False, False, False, True, False]
State prediction error at timestep 989 is tensor(1.6847e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 989 of 1
Current timestep = 990. State = [[-0.09920514 -0.00806622]]. Action = [[ 0.21621865  0.16082603 -0.014437   -0.17256409]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 990 is [True, False, False, False, True, False]
Current timestep = 991. State = [[-0.07429823 -0.00366737]]. Action = [[ 0.17513588 -0.05697562 -0.03407967 -0.8937164 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 991 is [True, False, False, False, True, False]
Current timestep = 992. State = [[-0.05219674  0.00775989]]. Action = [[ 0.10491472  0.19959575 -0.14735971  0.25609827]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 992 is [True, False, False, False, True, False]
Scene graph at timestep 992 is [True, False, False, False, True, False]
State prediction error at timestep 992 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 992 of 1
Current timestep = 993. State = [[-0.03665971  0.02728524]]. Action = [[0.08637774 0.09958714 0.11940086 0.0550977 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 993 is [True, False, False, False, True, False]
Scene graph at timestep 993 is [False, True, False, False, True, False]
State prediction error at timestep 993 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 993 of 1
Current timestep = 994. State = [[-0.26778543  0.20129704]]. Action = [[ 0.15454698 -0.07986824  0.20127928 -0.23321009]]. Reward = [100.]
Curr episode timestep = 24
Scene graph at timestep 994 is [False, True, False, False, True, False]
Current timestep = 995. State = [[-0.2574865   0.22755127]]. Action = [[ 0.140369    0.04233643  0.12510836 -0.6192419 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 995 is [True, False, False, False, False, True]
Scene graph at timestep 995 is [True, False, False, False, False, True]
State prediction error at timestep 995 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 995 of -1
Current timestep = 996. State = [[-0.24780022  0.2258044 ]]. Action = [[-0.16495189 -0.17245907 -0.13620396 -0.1309253 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 996 is [True, False, False, False, False, True]
Scene graph at timestep 996 is [True, False, False, False, False, True]
State prediction error at timestep 996 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 996 of 1
Current timestep = 997. State = [[-0.24322918  0.19975515]]. Action = [[ 0.13136074 -0.24026015 -0.08026427 -0.46280408]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 997 is [True, False, False, False, False, True]
Scene graph at timestep 997 is [True, False, False, False, False, True]
State prediction error at timestep 997 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 997 of 1
Current timestep = 998. State = [[-0.23449494  0.17036735]]. Action = [[ 0.00708601 -0.18609166  0.02673262  0.94814205]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 998 is [True, False, False, False, False, True]
Scene graph at timestep 998 is [True, False, False, False, False, True]
State prediction error at timestep 998 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 998 of 1
Current timestep = 999. State = [[-0.22731024  0.14001486]]. Action = [[ 0.0539293  -0.23608169  0.07720917  0.04329085]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 999 is [True, False, False, False, False, True]
Scene graph at timestep 999 is [True, False, False, False, False, True]
State prediction error at timestep 999 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 999 of 1
Current timestep = 1000. State = [[-0.22357993  0.1163914 ]]. Action = [[-0.01576476 -0.0982267  -0.18446244 -0.18540186]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1000 is [True, False, False, False, False, True]
Scene graph at timestep 1000 is [True, False, False, False, True, False]
State prediction error at timestep 1000 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1000 of 1
Current timestep = 1001. State = [[-0.21749632  0.11189426]]. Action = [[0.21673232 0.11774039 0.17512238 0.14244902]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1001 is [True, False, False, False, True, False]
Current timestep = 1002. State = [[-0.20852841  0.11817195]]. Action = [[ 0.07455087  0.03847069  0.0714761  -0.33936012]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1002 is [True, False, False, False, True, False]
Scene graph at timestep 1002 is [True, False, False, False, True, False]
State prediction error at timestep 1002 is tensor(1.1130e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1002 of 1
Current timestep = 1003. State = [[-0.20185468  0.13271016]]. Action = [[-0.02366076  0.18236619  0.09663314 -0.5340206 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1003 is [True, False, False, False, True, False]
Current timestep = 1004. State = [[-0.19473532  0.14159283]]. Action = [[ 0.14631993 -0.04538256 -0.14481363 -0.0343135 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1004 is [True, False, False, False, False, True]
Scene graph at timestep 1004 is [True, False, False, False, False, True]
State prediction error at timestep 1004 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1004 of 1
Current timestep = 1005. State = [[-0.18322013  0.14125027]]. Action = [[-0.02216718 -0.06154263 -0.1077041  -0.80446696]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1005 is [True, False, False, False, False, True]
Current timestep = 1006. State = [[-0.17610574  0.12279543]]. Action = [[ 0.1304965  -0.24447615 -0.03704046  0.18028533]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1006 is [True, False, False, False, False, True]
Scene graph at timestep 1006 is [True, False, False, False, True, False]
State prediction error at timestep 1006 is tensor(3.6634e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1006 of 1
Current timestep = 1007. State = [[-0.155299   0.1001481]]. Action = [[ 0.20755786  0.00915024  0.18767011 -0.6841298 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1007 is [True, False, False, False, True, False]
Scene graph at timestep 1007 is [True, False, False, False, True, False]
State prediction error at timestep 1007 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1007 of 1
Current timestep = 1008. State = [[-0.1297847   0.09593077]]. Action = [[ 0.10886663 -0.08731577  0.11063305 -0.12061417]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1008 is [True, False, False, False, True, False]
Current timestep = 1009. State = [[-0.12048002  0.08475301]]. Action = [[ 0.0947054  -0.05088487  0.03358874  0.0159744 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1009 is [True, False, False, False, True, False]
Scene graph at timestep 1009 is [True, False, False, False, True, False]
State prediction error at timestep 1009 is tensor(5.0082e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1009 of 1
Current timestep = 1010. State = [[-0.10698489  0.06824385]]. Action = [[-0.1581685  -0.23156662 -0.01528326  0.71605635]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1010 is [True, False, False, False, True, False]
Scene graph at timestep 1010 is [True, False, False, False, True, False]
State prediction error at timestep 1010 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1010 of 1
Current timestep = 1011. State = [[-0.10244324  0.03792252]]. Action = [[ 0.22224438 -0.20574747 -0.16741313  0.0003072 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1011 is [True, False, False, False, True, False]
Current timestep = 1012. State = [[-0.08982822  0.01998449]]. Action = [[ 0.11255637  0.00446862  0.1878283  -0.6918857 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1012 is [True, False, False, False, True, False]
Current timestep = 1013. State = [[-0.0843625   0.01366345]]. Action = [[-0.07026914 -0.06556159  0.08473563 -0.07113564]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1013 is [True, False, False, False, True, False]
Current timestep = 1014. State = [[-0.07994071 -0.00384352]]. Action = [[ 0.11756894 -0.21641101 -0.01946372 -0.14880371]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1014 is [True, False, False, False, True, False]
Scene graph at timestep 1014 is [True, False, False, False, True, False]
State prediction error at timestep 1014 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1014 of 1
Current timestep = 1015. State = [[-0.07388114 -0.03201884]]. Action = [[-0.09305605 -0.13256383 -0.21028236  0.54745626]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1015 is [True, False, False, False, True, False]
Current timestep = 1016. State = [[-0.07060033 -0.05542001]]. Action = [[ 0.13095978 -0.24121253 -0.08076268  0.54895556]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1016 is [True, False, False, False, True, False]
Current timestep = 1017. State = [[-0.06689306 -0.06426297]]. Action = [[-0.0073259   0.20007622  0.09260833  0.55758226]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1017 is [True, False, False, False, True, False]
Scene graph at timestep 1017 is [True, False, False, False, True, False]
State prediction error at timestep 1017 is tensor(5.8542e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1017 of -1
Current timestep = 1018. State = [[-0.05868247 -0.06090813]]. Action = [[ 0.15929005 -0.11602858 -0.02578843  0.44039488]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1018 is [True, False, False, False, True, False]
Current timestep = 1019. State = [[-0.04128337 -0.05547447]]. Action = [[ 0.12497571  0.18231422 -0.08493868 -0.84196794]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1019 is [True, False, False, False, True, False]
Scene graph at timestep 1019 is [False, True, False, False, True, False]
State prediction error at timestep 1019 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1019 of 1
Current timestep = 1020. State = [[-0.26433116  0.02891879]]. Action = [[-0.16738874  0.05848056 -0.24073526 -0.6945059 ]]. Reward = [100.]
Curr episode timestep = 25
Scene graph at timestep 1020 is [False, True, False, False, True, False]
Current timestep = 1021. State = [[-0.262891    0.03222805]]. Action = [[-0.1069131   0.04730985  0.14230669 -0.24253142]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1021 is [True, False, False, False, True, False]
Current timestep = 1022. State = [[-0.2509547   0.02388805]]. Action = [[ 0.22962356 -0.17354926  0.22003579  0.8258095 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1022 is [True, False, False, False, True, False]
Current timestep = 1023. State = [[-0.23675528  0.00682186]]. Action = [[-0.032627   -0.15144843  0.00916767 -0.76569355]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1023 is [True, False, False, False, True, False]
Scene graph at timestep 1023 is [True, False, False, False, True, False]
State prediction error at timestep 1023 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1023 of 1
Current timestep = 1024. State = [[-0.23990715 -0.00415635]]. Action = [[-0.12780362  0.06683046 -0.0081041   0.418396  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1024 is [True, False, False, False, True, False]
Current timestep = 1025. State = [[-0.24518678  0.00739421]]. Action = [[-0.04571077  0.16391659 -0.00273703  0.911178  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1025 is [True, False, False, False, True, False]
Current timestep = 1026. State = [[-0.24864596  0.02445887]]. Action = [[ 0.09614274  0.09299687 -0.1784538   0.49461138]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1026 is [True, False, False, False, True, False]
Scene graph at timestep 1026 is [True, False, False, False, True, False]
State prediction error at timestep 1026 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1026 of 1
Current timestep = 1027. State = [[-0.24971692  0.02791948]]. Action = [[-0.092282   -0.1160779   0.0237495   0.43286622]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1027 is [True, False, False, False, True, False]
Scene graph at timestep 1027 is [True, False, False, False, True, False]
State prediction error at timestep 1027 is tensor(8.1096e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1027 of 1
Current timestep = 1028. State = [[-0.2505494  0.006963 ]]. Action = [[-0.0015458  -0.24691862 -0.04211999  0.01968122]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1028 is [True, False, False, False, True, False]
Current timestep = 1029. State = [[-0.25039685 -0.00961024]]. Action = [[ 0.08899534 -0.01270747  0.1581316   0.53429973]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1029 is [True, False, False, False, True, False]
Scene graph at timestep 1029 is [True, False, False, False, True, False]
State prediction error at timestep 1029 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1029 of -1
Current timestep = 1030. State = [[-0.24776405 -0.02462959]]. Action = [[-0.0329506  -0.15073694 -0.23139562 -0.67990446]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1030 is [True, False, False, False, True, False]
Scene graph at timestep 1030 is [True, False, False, False, True, False]
State prediction error at timestep 1030 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1030 of -1
Current timestep = 1031. State = [[-0.24688071 -0.03690323]]. Action = [[ 0.05300677 -0.01751745  0.20071685  0.89543176]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1031 is [True, False, False, False, True, False]
Scene graph at timestep 1031 is [True, False, False, False, True, False]
State prediction error at timestep 1031 is tensor(8.4284e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1031 of -1
Current timestep = 1032. State = [[-0.2491979  -0.03015962]]. Action = [[-0.13049223  0.17340219 -0.19975616 -0.60696435]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1032 is [True, False, False, False, True, False]
Scene graph at timestep 1032 is [True, False, False, False, True, False]
State prediction error at timestep 1032 is tensor(8.6892e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1032 of 1
Current timestep = 1033. State = [[-0.25609025 -0.02040278]]. Action = [[-1.0403964e-01  4.5925379e-05  1.5557945e-01  9.1154933e-02]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1033 is [True, False, False, False, True, False]
Scene graph at timestep 1033 is [True, False, False, False, True, False]
State prediction error at timestep 1033 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1033 of -1
Current timestep = 1034. State = [[-0.2628872  -0.01942835]]. Action = [[-0.20541869  0.17701298 -0.09566779 -0.793618  ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1034 is [True, False, False, False, True, False]
Scene graph at timestep 1034 is [True, False, False, False, True, False]
State prediction error at timestep 1034 is tensor(4.0651e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1034 of -1
Current timestep = 1035. State = [[-0.2649629  -0.02129463]]. Action = [[-0.04800633 -0.04559134 -0.13647586 -0.5891469 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1035 is [True, False, False, False, True, False]
Current timestep = 1036. State = [[-0.2672835  -0.01523057]]. Action = [[0.10112071 0.14740628 0.1847117  0.6422267 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1036 is [True, False, False, False, True, False]
Current timestep = 1037. State = [[-0.25847703 -0.0079211 ]]. Action = [[ 0.18876997  0.01596466 -0.19555095  0.31627762]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1037 is [True, False, False, False, True, False]
Current timestep = 1038. State = [[-0.25021672 -0.0062433 ]]. Action = [[-0.15915668  0.2038551  -0.17917158  0.1763773 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1038 is [True, False, False, False, True, False]
Current timestep = 1039. State = [[-0.24358249 -0.00087159]]. Action = [[0.12471467 0.05816758 0.00322559 0.19566882]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1039 is [True, False, False, False, True, False]
Scene graph at timestep 1039 is [True, False, False, False, True, False]
State prediction error at timestep 1039 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1039 of 1
Current timestep = 1040. State = [[-0.21859     0.01888712]]. Action = [[ 0.24271488  0.21144965 -0.20674151  0.3371657 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1040 is [True, False, False, False, True, False]
Scene graph at timestep 1040 is [True, False, False, False, True, False]
State prediction error at timestep 1040 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1040 of 1
Current timestep = 1041. State = [[-0.1855838   0.02707786]]. Action = [[ 0.15856367 -0.16931847 -0.19813798 -0.48849303]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1041 is [True, False, False, False, True, False]
Scene graph at timestep 1041 is [True, False, False, False, True, False]
State prediction error at timestep 1041 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1041 of 1
Current timestep = 1042. State = [[-0.16681673  0.02391162]]. Action = [[ 0.12443969  0.1265355   0.05669686 -0.18043202]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1042 is [True, False, False, False, True, False]
Scene graph at timestep 1042 is [True, False, False, False, True, False]
State prediction error at timestep 1042 is tensor(4.0252e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1042 of 1
Current timestep = 1043. State = [[-0.15321061  0.01806085]]. Action = [[-0.06032103 -0.2279233  -0.13917226 -0.1686148 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1043 is [True, False, False, False, True, False]
Scene graph at timestep 1043 is [True, False, False, False, True, False]
State prediction error at timestep 1043 is tensor(6.7408e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1043 of 1
Current timestep = 1044. State = [[-0.15454064  0.0078381 ]]. Action = [[-0.03436376  0.08642215 -0.082445    0.7248628 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1044 is [True, False, False, False, True, False]
Current timestep = 1045. State = [[-0.15718661  0.00870575]]. Action = [[-0.08217496 -0.04644045 -0.21770875 -0.15174532]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1045 is [True, False, False, False, True, False]
Current timestep = 1046. State = [[-0.15768988  0.00092166]]. Action = [[ 0.09495091 -0.11195378 -0.0777435  -0.24369138]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1046 is [True, False, False, False, True, False]
Scene graph at timestep 1046 is [True, False, False, False, True, False]
State prediction error at timestep 1046 is tensor(7.9782e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1046 of -1
Current timestep = 1047. State = [[-0.15809171  0.00063699]]. Action = [[ 0.00492716  0.14396316 -0.10590464 -0.9281123 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1047 is [True, False, False, False, True, False]
Current timestep = 1048. State = [[-0.15321897  0.00173486]]. Action = [[ 0.1354121  -0.08608563  0.08858919 -0.5004283 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1048 is [True, False, False, False, True, False]
Current timestep = 1049. State = [[-0.14811148 -0.00199708]]. Action = [[ 0.0032855  -0.0378035  -0.03982136 -0.94934875]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1049 is [True, False, False, False, True, False]
Current timestep = 1050. State = [[-0.142889   -0.01877177]]. Action = [[ 0.00074837 -0.22560285 -0.1470546  -0.6031087 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1050 is [True, False, False, False, True, False]
Current timestep = 1051. State = [[-0.14334236 -0.02514347]]. Action = [[-0.15575403  0.16565534 -0.2314746   0.17293239]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1051 is [True, False, False, False, True, False]
Current timestep = 1052. State = [[-0.1500514  -0.02421546]]. Action = [[-0.11141419 -0.09151392  0.11030501  0.084306  ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1052 is [True, False, False, False, True, False]
Scene graph at timestep 1052 is [True, False, False, False, True, False]
State prediction error at timestep 1052 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1052 of -1
Current timestep = 1053. State = [[-0.15722898 -0.0450975 ]]. Action = [[ 0.09432435 -0.23912519 -0.2153862  -0.36658496]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1053 is [True, False, False, False, True, False]
Current timestep = 1054. State = [[-0.15408827 -0.05543329]]. Action = [[ 0.1106908   0.06283742  0.16212791 -0.23217714]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1054 is [True, False, False, False, True, False]
Current timestep = 1055. State = [[-0.1483053  -0.05907218]]. Action = [[ 0.08819517 -0.06944388  0.20527703  0.3990575 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1055 is [True, False, False, False, True, False]
Scene graph at timestep 1055 is [True, False, False, False, True, False]
State prediction error at timestep 1055 is tensor(2.9893e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1055 of 1
Current timestep = 1056. State = [[-0.13051534 -0.06526273]]. Action = [[ 2.4157506e-01 -3.1907260e-02 -4.3302774e-05 -2.1609622e-01]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1056 is [True, False, False, False, True, False]
Current timestep = 1057. State = [[-0.11017619 -0.06688397]]. Action = [[-0.05929947  0.01401299 -0.02662659  0.5535481 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1057 is [True, False, False, False, True, False]
Scene graph at timestep 1057 is [True, False, False, False, True, False]
State prediction error at timestep 1057 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1057 of 1
Current timestep = 1058. State = [[-0.10951715 -0.072583  ]]. Action = [[-0.06360953 -0.09218025 -0.01417565  0.20920599]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1058 is [True, False, False, False, True, False]
Scene graph at timestep 1058 is [True, False, False, False, True, False]
State prediction error at timestep 1058 is tensor(3.0368e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1058 of 1
Current timestep = 1059. State = [[-0.10252962 -0.06633302]]. Action = [[ 0.23403552  0.22253749  0.18057185 -0.22984236]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1059 is [True, False, False, False, True, False]
Scene graph at timestep 1059 is [True, False, False, False, True, False]
State prediction error at timestep 1059 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1059 of 1
Current timestep = 1060. State = [[-0.08598561 -0.06529468]]. Action = [[-0.06634812 -0.21882716 -0.16137978 -0.2510879 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1060 is [True, False, False, False, True, False]
Scene graph at timestep 1060 is [True, False, False, False, True, False]
State prediction error at timestep 1060 is tensor(6.8104e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1060 of 1
Current timestep = 1061. State = [[-0.08603885 -0.0696312 ]]. Action = [[ 0.04740161  0.16401419  0.03170681 -0.16486323]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1061 is [True, False, False, False, True, False]
Scene graph at timestep 1061 is [True, False, False, False, True, False]
State prediction error at timestep 1061 is tensor(9.1620e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1061 of -1
Current timestep = 1062. State = [[-0.07823513 -0.06212022]]. Action = [[ 0.21727282 -0.02833439 -0.17917442  0.1584872 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1062 is [True, False, False, False, True, False]
Scene graph at timestep 1062 is [True, False, False, False, True, False]
State prediction error at timestep 1062 is tensor(5.1621e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1062 of 1
Current timestep = 1063. State = [[-0.05275239 -0.05119601]]. Action = [[ 0.12413505  0.19549388 -0.11021981 -0.858239  ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1063 is [True, False, False, False, True, False]
Current timestep = 1064. State = [[-0.04105499 -0.04645322]]. Action = [[ 0.02839303 -0.13448821  0.07107499 -0.70345414]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1064 is [True, False, False, False, True, False]
Current timestep = 1065. State = [[-0.1842334   0.13124749]]. Action = [[ 0.13374805  0.16934118 -0.2204092   0.68551326]]. Reward = [100.]
Curr episode timestep = 44
Scene graph at timestep 1065 is [False, True, False, False, True, False]
Scene graph at timestep 1065 is [True, False, False, False, False, True]
State prediction error at timestep 1065 is tensor(0.0270, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1065 of -1
Current timestep = 1066. State = [[-0.15875794  0.14020708]]. Action = [[ 0.23399436 -0.12211254 -0.08248328 -0.33644414]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1066 is [True, False, False, False, False, True]
Current timestep = 1067. State = [[-0.13170362  0.13428058]]. Action = [[ 0.21171719 -0.01342422  0.05441871 -0.72589594]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1067 is [True, False, False, False, False, True]
Current timestep = 1068. State = [[-0.10603703  0.12237477]]. Action = [[ 0.06832254 -0.16687185 -0.10021484  0.90286565]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1068 is [True, False, False, False, False, True]
Current timestep = 1069. State = [[-0.09399787  0.11215438]]. Action = [[ 0.09812412  0.00476754 -0.20747992 -0.30112898]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1069 is [True, False, False, False, True, False]
Current timestep = 1070. State = [[-0.07549737  0.09590355]]. Action = [[ 0.15997103 -0.22556962 -0.20544742  0.04707325]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1070 is [True, False, False, False, True, False]
Current timestep = 1071. State = [[-0.05136838  0.06935322]]. Action = [[ 0.20824313 -0.18190569  0.04822528 -0.04574662]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1071 is [True, False, False, False, True, False]
Current timestep = 1072. State = [[-0.2428698  -0.05194438]]. Action = [[ 0.00751051  0.02900109  0.0435456  -0.15472591]]. Reward = [100.]
Curr episode timestep = 6
Scene graph at timestep 1072 is [True, False, False, False, True, False]
Current timestep = 1073. State = [[-0.23810272 -0.06751727]]. Action = [[-0.01132256 -0.14121677  0.13358086  0.60984254]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1073 is [True, False, False, False, True, False]
Scene graph at timestep 1073 is [True, False, False, False, True, False]
State prediction error at timestep 1073 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1073 of -1
Current timestep = 1074. State = [[-0.23322786 -0.09171005]]. Action = [[ 0.09032124 -0.21258956 -0.01514274  0.10125995]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1074 is [True, False, False, False, True, False]
Current timestep = 1075. State = [[-0.2191189  -0.10744141]]. Action = [[ 0.17232957  0.00873283 -0.07443029  0.6028187 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1075 is [True, False, False, False, True, False]
Scene graph at timestep 1075 is [True, False, False, False, True, False]
State prediction error at timestep 1075 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1075 of -1
Current timestep = 1076. State = [[-0.2036157  -0.11053056]]. Action = [[-0.02959864  0.03110215 -0.04048438  0.29171288]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1076 is [True, False, False, False, True, False]
Current timestep = 1077. State = [[-0.20771949 -0.10171702]]. Action = [[-0.15634374  0.14686015 -0.20142473  0.48674846]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1077 is [True, False, False, False, True, False]
Current timestep = 1078. State = [[-0.20770925 -0.07851003]]. Action = [[ 0.1530301   0.2301048  -0.1115815   0.90221906]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1078 is [True, False, False, False, True, False]
Scene graph at timestep 1078 is [True, False, False, False, True, False]
State prediction error at timestep 1078 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1078 of 1
Current timestep = 1079. State = [[-0.20331188 -0.04722061]]. Action = [[ 0.06738007  0.16047043 -0.21212114 -0.04279578]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1079 is [True, False, False, False, True, False]
Current timestep = 1080. State = [[-0.19375679 -0.03819682]]. Action = [[ 0.13405082 -0.00812559  0.16562176 -0.565223  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1080 is [True, False, False, False, True, False]
Scene graph at timestep 1080 is [True, False, False, False, True, False]
State prediction error at timestep 1080 is tensor(3.1065e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1080 of 1
Current timestep = 1081. State = [[-0.18023595 -0.02950501]]. Action = [[-0.00952226  0.08856469 -0.17863008  0.0134865 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1081 is [True, False, False, False, True, False]
Current timestep = 1082. State = [[-0.17805545 -0.02582186]]. Action = [[ 0.07073876 -0.02627319  0.0907644   0.5078306 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1082 is [True, False, False, False, True, False]
Current timestep = 1083. State = [[-0.16686308 -0.03611918]]. Action = [[ 0.13403481 -0.1973041  -0.07153916  0.45488083]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1083 is [True, False, False, False, True, False]
Scene graph at timestep 1083 is [True, False, False, False, True, False]
State prediction error at timestep 1083 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1083 of 1
Current timestep = 1084. State = [[-0.1480516  -0.05094772]]. Action = [[ 0.05448323 -0.04883642 -0.10152811  0.15926349]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1084 is [True, False, False, False, True, False]
Current timestep = 1085. State = [[-0.13800043 -0.05877709]]. Action = [[ 0.13916978 -0.06699982  0.08296627 -0.68661463]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1085 is [True, False, False, False, True, False]
Scene graph at timestep 1085 is [True, False, False, False, True, False]
State prediction error at timestep 1085 is tensor(5.0390e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1085 of 1
Current timestep = 1086. State = [[-0.11624588 -0.07711609]]. Action = [[ 0.0724847  -0.17578173 -0.24040103 -0.00351202]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1086 is [True, False, False, False, True, False]
Scene graph at timestep 1086 is [True, False, False, False, True, False]
State prediction error at timestep 1086 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1086 of -1
Current timestep = 1087. State = [[-0.11154266 -0.07918101]]. Action = [[ 0.04706946  0.16720173 -0.0900842   0.18470716]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1087 is [True, False, False, False, True, False]
Current timestep = 1088. State = [[-0.11055502 -0.07203323]]. Action = [[-0.03761184  0.02490675  0.15163282 -0.06271648]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1088 is [True, False, False, False, True, False]
Current timestep = 1089. State = [[-0.10074887 -0.055796  ]]. Action = [[ 0.22436428  0.21803057 -0.14461118  0.3887056 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1089 is [True, False, False, False, True, False]
Scene graph at timestep 1089 is [True, False, False, False, True, False]
State prediction error at timestep 1089 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1089 of 1
Current timestep = 1090. State = [[-0.07062881 -0.04229788]]. Action = [[ 0.17077729 -0.08437416 -0.01492049 -0.37860978]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1090 is [True, False, False, False, True, False]
Scene graph at timestep 1090 is [True, False, False, False, True, False]
State prediction error at timestep 1090 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1090 of 1
Current timestep = 1091. State = [[-0.05368251 -0.03372655]]. Action = [[-0.10096958  0.2082037  -0.17404063  0.56970036]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1091 is [True, False, False, False, True, False]
Scene graph at timestep 1091 is [True, False, False, False, True, False]
State prediction error at timestep 1091 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1091 of 1
Current timestep = 1092. State = [[-0.05361869 -0.01218842]]. Action = [[0.15567645 0.11715358 0.11039272 0.8294132 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1092 is [True, False, False, False, True, False]
Current timestep = 1093. State = [[-0.03747185 -0.00850847]]. Action = [[ 0.2024439  -0.07043245 -0.15581733 -0.6049284 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1093 is [True, False, False, False, True, False]
Scene graph at timestep 1093 is [False, True, False, False, True, False]
State prediction error at timestep 1093 is tensor(5.4672e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1093 of 1
Current timestep = 1094. State = [[-0.27372202  0.20389324]]. Action = [[ 0.24283886 -0.15172122 -0.23410513  0.05311251]]. Reward = [100.]
Curr episode timestep = 21
Scene graph at timestep 1094 is [False, True, False, False, True, False]
Current timestep = 1095. State = [[-0.26507014  0.2218797 ]]. Action = [[ 0.10319477 -0.11961529  0.13388026 -0.53135884]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1095 is [True, False, False, False, False, True]
Current timestep = 1096. State = [[-0.24802119  0.2145718 ]]. Action = [[ 0.23445138  0.00287676 -0.20186803 -0.8626402 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1096 is [True, False, False, False, False, True]
Current timestep = 1097. State = [[-0.22977641  0.20416342]]. Action = [[-0.16862117 -0.22975776  0.11833027 -0.32331455]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1097 is [True, False, False, False, False, True]
Scene graph at timestep 1097 is [True, False, False, False, False, True]
State prediction error at timestep 1097 is tensor(2.1862e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1097 of 1
Current timestep = 1098. State = [[-0.22594212  0.188011  ]]. Action = [[ 0.15685236  0.02835423 -0.10443223 -0.90030414]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1098 is [True, False, False, False, False, True]
Scene graph at timestep 1098 is [True, False, False, False, False, True]
State prediction error at timestep 1098 is tensor(8.8689e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1098 of 1
Current timestep = 1099. State = [[-0.22942565  0.1956654 ]]. Action = [[-0.15578955  0.13763374  0.06333297 -0.19447672]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1099 is [True, False, False, False, False, True]
Scene graph at timestep 1099 is [True, False, False, False, False, True]
State prediction error at timestep 1099 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1099 of -1
Current timestep = 1100. State = [[-0.23885117  0.20788884]]. Action = [[-0.07900207  0.00258923  0.0121662  -0.9648609 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1100 is [True, False, False, False, False, True]
Scene graph at timestep 1100 is [True, False, False, False, False, True]
State prediction error at timestep 1100 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1100 of -1
Current timestep = 1101. State = [[-0.24164866  0.21058595]]. Action = [[ 0.06511056  0.02198407 -0.0438453  -0.01050562]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1101 is [True, False, False, False, False, True]
Current timestep = 1102. State = [[-0.24548946  0.2152293 ]]. Action = [[-0.12292355  0.04833788 -0.1811581   0.03998864]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1102 is [True, False, False, False, False, True]
Current timestep = 1103. State = [[-0.2510345   0.21235192]]. Action = [[-0.06897786 -0.15162377  0.05826989 -0.20512736]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1103 is [True, False, False, False, False, True]
Scene graph at timestep 1103 is [True, False, False, False, False, True]
State prediction error at timestep 1103 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1103 of -1
Current timestep = 1104. State = [[-0.24895911  0.19302139]]. Action = [[ 0.17933837 -0.124764    0.22266322 -0.42292988]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1104 is [True, False, False, False, False, True]
Scene graph at timestep 1104 is [True, False, False, False, False, True]
State prediction error at timestep 1104 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1104 of 1
Current timestep = 1105. State = [[-0.24400735  0.18227115]]. Action = [[-0.21993995  0.04202083 -0.07706472  0.7470324 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1105 is [True, False, False, False, False, True]
Current timestep = 1106. State = [[-0.24153578  0.1908442 ]]. Action = [[0.0824222  0.19035107 0.17573512 0.5239742 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1106 is [True, False, False, False, False, True]
Current timestep = 1107. State = [[-0.23722212  0.19433849]]. Action = [[-0.05224779 -0.12209021  0.0419738   0.20447648]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1107 is [True, False, False, False, False, True]
Current timestep = 1108. State = [[-0.23790239  0.18969226]]. Action = [[-0.08452591 -0.06641704 -0.2251355   0.1482786 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1108 is [True, False, False, False, False, True]
Scene graph at timestep 1108 is [True, False, False, False, False, True]
State prediction error at timestep 1108 is tensor(7.6130e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1108 of -1
Current timestep = 1109. State = [[-0.2363325   0.18728134]]. Action = [[ 0.15980458  0.13239741  0.03877816 -0.58450925]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1109 is [True, False, False, False, False, True]
Scene graph at timestep 1109 is [True, False, False, False, False, True]
State prediction error at timestep 1109 is tensor(8.1020e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1109 of -1
Current timestep = 1110. State = [[-0.22804056  0.1827434 ]]. Action = [[ 0.00930262 -0.21960716 -0.03194349  0.5610515 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1110 is [True, False, False, False, False, True]
Current timestep = 1111. State = [[-0.21751088  0.15791366]]. Action = [[ 0.16384554 -0.22487988 -0.20447695 -0.06825137]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1111 is [True, False, False, False, False, True]
Scene graph at timestep 1111 is [True, False, False, False, False, True]
State prediction error at timestep 1111 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1111 of 1
Current timestep = 1112. State = [[-0.20411333  0.13419554]]. Action = [[ 0.07760921 -0.07681197  0.15403542 -0.2670232 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1112 is [True, False, False, False, False, True]
Scene graph at timestep 1112 is [True, False, False, False, False, True]
State prediction error at timestep 1112 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1112 of 1
Current timestep = 1113. State = [[-0.20208056  0.13219   ]]. Action = [[-0.12705301  0.16061336  0.0923124   0.72877383]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1113 is [True, False, False, False, False, True]
Scene graph at timestep 1113 is [True, False, False, False, False, True]
State prediction error at timestep 1113 is tensor(1.4218e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1113 of -1
Current timestep = 1114. State = [[-0.19946653  0.14113577]]. Action = [[ 0.2057941  -0.0222687  -0.16021775 -0.09088671]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1114 is [True, False, False, False, False, True]
Scene graph at timestep 1114 is [True, False, False, False, False, True]
State prediction error at timestep 1114 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1114 of 1
Current timestep = 1115. State = [[-0.18806441  0.14547677]]. Action = [[-0.00929993  0.08361709  0.01131094 -0.50691885]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1115 is [True, False, False, False, False, True]
Scene graph at timestep 1115 is [True, False, False, False, False, True]
State prediction error at timestep 1115 is tensor(5.5776e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1115 of 1
Current timestep = 1116. State = [[-0.18027513  0.15667582]]. Action = [[ 0.16343963  0.12875497 -0.1671486  -0.39261055]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1116 is [True, False, False, False, False, True]
Scene graph at timestep 1116 is [True, False, False, False, False, True]
State prediction error at timestep 1116 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1116 of 1
Current timestep = 1117. State = [[-0.16289207  0.16780801]]. Action = [[ 0.00262916 -0.04511137  0.02111867 -0.8581706 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1117 is [True, False, False, False, False, True]
Current timestep = 1118. State = [[-0.15659471  0.1577426 ]]. Action = [[ 0.13006228 -0.13141519  0.09156847 -0.68885946]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1118 is [True, False, False, False, False, True]
Current timestep = 1119. State = [[-0.1480672   0.14398941]]. Action = [[ 0.04865462 -0.06364161 -0.20113735  0.7470391 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1119 is [True, False, False, False, False, True]
Scene graph at timestep 1119 is [True, False, False, False, False, True]
State prediction error at timestep 1119 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1119 of 1
Current timestep = 1120. State = [[-0.13274734  0.13664107]]. Action = [[ 0.16751286 -0.00174001 -0.1389307  -0.20693254]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1120 is [True, False, False, False, False, True]
Current timestep = 1121. State = [[-0.12138107  0.13896817]]. Action = [[-0.11454284  0.02286819  0.01424977  0.29138803]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1121 is [True, False, False, False, False, True]
Current timestep = 1122. State = [[-0.11275182  0.1350733 ]]. Action = [[ 0.22230127 -0.07386115 -0.17502221  0.1979537 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1122 is [True, False, False, False, False, True]
Current timestep = 1123. State = [[-0.09340378  0.12015224]]. Action = [[ 0.14423215 -0.16953947 -0.10396707  0.4412017 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1123 is [True, False, False, False, False, True]
Current timestep = 1124. State = [[-0.07494926  0.11536938]]. Action = [[0.1121459  0.13416353 0.23774219 0.07467985]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1124 is [True, False, False, False, True, False]
Scene graph at timestep 1124 is [True, False, False, False, True, False]
State prediction error at timestep 1124 is tensor(1.2725e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1124 of 1
Current timestep = 1125. State = [[-0.0568676   0.12062852]]. Action = [[ 0.03037381 -0.04372598  0.23820227 -0.20882916]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1125 is [True, False, False, False, True, False]
Current timestep = 1126. State = [[-0.04940357  0.10992651]]. Action = [[ 0.12993306 -0.14554681 -0.04422495 -0.4951017 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1126 is [True, False, False, False, True, False]
Scene graph at timestep 1126 is [False, True, False, False, True, False]
State prediction error at timestep 1126 is tensor(4.2339e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1126 of 1
Current timestep = 1127. State = [[-0.03752342  0.09857802]]. Action = [[-0.05634432  0.01122651  0.05975294 -0.02049392]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1127 is [False, True, False, False, True, False]
Current timestep = 1128. State = [[-0.2486788  -0.17051707]]. Action = [[ 0.20905423 -0.20801699 -0.09285545  0.31557333]]. Reward = [100.]
Curr episode timestep = 33
Scene graph at timestep 1128 is [False, True, False, False, True, False]
Current timestep = 1129. State = [[-0.2414073  -0.17763972]]. Action = [[ 0.00824785  0.24850455 -0.0357199   0.49397492]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1129 is [True, False, False, True, False, False]
Scene graph at timestep 1129 is [True, False, False, True, False, False]
State prediction error at timestep 1129 is tensor(6.3364e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1129 of 1
Current timestep = 1130. State = [[-0.23280498 -0.16635297]]. Action = [[ 0.1565679  -0.09865028 -0.03540885 -0.4607576 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1130 is [True, False, False, True, False, False]
Scene graph at timestep 1130 is [True, False, False, True, False, False]
State prediction error at timestep 1130 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1130 of 1
Current timestep = 1131. State = [[-0.21321435 -0.15869701]]. Action = [[ 0.20062727  0.19792664 -0.09479566 -0.1091457 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1131 is [True, False, False, True, False, False]
Current timestep = 1132. State = [[-0.2002481  -0.13710438]]. Action = [[-0.10764199  0.19677624 -0.17129885 -0.35492694]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1132 is [True, False, False, True, False, False]
Scene graph at timestep 1132 is [True, False, False, True, False, False]
State prediction error at timestep 1132 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1132 of 1
Current timestep = 1133. State = [[-0.19520546 -0.10417542]]. Action = [[0.16801402 0.23431796 0.21442077 0.50296664]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1133 is [True, False, False, True, False, False]
Scene graph at timestep 1133 is [True, False, False, False, True, False]
State prediction error at timestep 1133 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1133 of 1
Current timestep = 1134. State = [[-0.18106009 -0.07592307]]. Action = [[0.13450164 0.12526667 0.05010858 0.8691362 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1134 is [True, False, False, False, True, False]
Scene graph at timestep 1134 is [True, False, False, False, True, False]
State prediction error at timestep 1134 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1134 of 1
Current timestep = 1135. State = [[-0.17329907 -0.07801686]]. Action = [[-0.19509931 -0.2126948  -0.11274675  0.1809628 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1135 is [True, False, False, False, True, False]
Scene graph at timestep 1135 is [True, False, False, False, True, False]
State prediction error at timestep 1135 is tensor(4.1005e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1135 of -1
Current timestep = 1136. State = [[-0.17713133 -0.07957706]]. Action = [[ 0.13732237  0.23719996  0.2087639  -0.78157526]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1136 is [True, False, False, False, True, False]
Scene graph at timestep 1136 is [True, False, False, False, True, False]
State prediction error at timestep 1136 is tensor(6.9193e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1136 of 1
Current timestep = 1137. State = [[-0.16778795 -0.05520731]]. Action = [[ 0.17524552  0.11390695 -0.07902893  0.7540412 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1137 is [True, False, False, False, True, False]
Current timestep = 1138. State = [[-0.14670683 -0.03830195]]. Action = [[ 0.09833077  0.1366342  -0.01141323 -0.56878763]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1138 is [True, False, False, False, True, False]
Scene graph at timestep 1138 is [True, False, False, False, True, False]
State prediction error at timestep 1138 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1138 of 1
Current timestep = 1139. State = [[-0.13544588 -0.01479407]]. Action = [[ 0.02889225  0.16332468 -0.04847781  0.5479541 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1139 is [True, False, False, False, True, False]
Current timestep = 1140. State = [[-0.13499297 -0.01208901]]. Action = [[-0.10769662 -0.15534125 -0.13672131  0.14896274]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1140 is [True, False, False, False, True, False]
Current timestep = 1141. State = [[-0.13688506 -0.03220469]]. Action = [[-0.04677436 -0.22509055 -0.06733611 -0.4895941 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1141 is [True, False, False, False, True, False]
Current timestep = 1142. State = [[-0.13931611 -0.04226435]]. Action = [[ 0.04201204  0.10467914  0.19665354 -0.36344147]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1142 is [True, False, False, False, True, False]
Current timestep = 1143. State = [[-0.13837108 -0.02770283]]. Action = [[0.04932767 0.20948619 0.20760965 0.15151155]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1143 is [True, False, False, False, True, False]
Current timestep = 1144. State = [[-0.13954818 -0.0088475 ]]. Action = [[-0.10091344  0.06026411 -0.04908347  0.584628  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1144 is [True, False, False, False, True, False]
Scene graph at timestep 1144 is [True, False, False, False, True, False]
State prediction error at timestep 1144 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1144 of 1
Current timestep = 1145. State = [[-0.13610867  0.01009062]]. Action = [[ 0.18529183  0.16323829 -0.09875923 -0.53732926]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1145 is [True, False, False, False, True, False]
Current timestep = 1146. State = [[-0.13246332  0.02741751]]. Action = [[-0.10366362  0.11444059 -0.01150793 -0.03538018]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1146 is [True, False, False, False, True, False]
Scene graph at timestep 1146 is [True, False, False, False, True, False]
State prediction error at timestep 1146 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1146 of 1
Current timestep = 1147. State = [[-0.13734047  0.04208716]]. Action = [[-0.03826666  0.0324409  -0.24628353  0.3586042 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1147 is [True, False, False, False, True, False]
Scene graph at timestep 1147 is [True, False, False, False, True, False]
State prediction error at timestep 1147 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1147 of -1
Current timestep = 1148. State = [[-0.1309513   0.03423436]]. Action = [[ 0.21110141 -0.1990746  -0.05412665  0.0528003 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1148 is [True, False, False, False, True, False]
Current timestep = 1149. State = [[-0.10901802  0.01077769]]. Action = [[ 0.18521672 -0.20395209 -0.17223671  0.32149875]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1149 is [True, False, False, False, True, False]
Current timestep = 1150. State = [[-0.09649135 -0.00443649]]. Action = [[-0.14395626 -0.00837964  0.10500866  0.6943412 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1150 is [True, False, False, False, True, False]
Current timestep = 1151. State = [[-0.09594744 -0.01202688]]. Action = [[ 0.09530941 -0.05174288 -0.11293036 -0.78907573]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1151 is [True, False, False, False, True, False]
Scene graph at timestep 1151 is [True, False, False, False, True, False]
State prediction error at timestep 1151 is tensor(3.3888e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1151 of 1
Current timestep = 1152. State = [[-0.0854217 -0.0204857]]. Action = [[ 0.1789439  -0.05789858 -0.0170778  -0.7217103 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1152 is [True, False, False, False, True, False]
Current timestep = 1153. State = [[-0.07620612 -0.02193195]]. Action = [[-0.10930738  0.04991055 -0.07239673  0.00508046]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1153 is [True, False, False, False, True, False]
Current timestep = 1154. State = [[-0.06986196 -0.02349195]]. Action = [[ 0.19468015 -0.0499842  -0.2242391  -0.08562678]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1154 is [True, False, False, False, True, False]
Current timestep = 1155. State = [[-0.05153969 -0.01700704]]. Action = [[0.17241925 0.14620307 0.17091653 0.68458927]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1155 is [True, False, False, False, True, False]
Scene graph at timestep 1155 is [True, False, False, False, True, False]
State prediction error at timestep 1155 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1155 of 1
Current timestep = 1156. State = [[-0.18174738 -0.1275993 ]]. Action = [[ 0.02787721 -0.08540477  0.17635071 -0.9345385 ]]. Reward = [100.]
Curr episode timestep = 27
Scene graph at timestep 1156 is [True, False, False, False, True, False]
Scene graph at timestep 1156 is [True, False, False, True, False, False]
State prediction error at timestep 1156 is tensor(0.0165, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1156 of 0
Current timestep = 1157. State = [[-0.17493697 -0.1407704 ]]. Action = [[-0.15445098  0.03048402 -0.19440408  0.13580883]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1157 is [True, False, False, True, False, False]
Current timestep = 1158. State = [[-0.17250332 -0.14492512]]. Action = [[ 0.1918056  -0.08774669  0.17150006  0.80013275]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1158 is [True, False, False, True, False, False]
Scene graph at timestep 1158 is [True, False, False, True, False, False]
State prediction error at timestep 1158 is tensor(2.1899e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1158 of -1
Current timestep = 1159. State = [[-0.15818961 -0.15726858]]. Action = [[ 0.1418184  -0.12635699  0.21301594  0.00329661]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1159 is [True, False, False, True, False, False]
Scene graph at timestep 1159 is [True, False, False, True, False, False]
State prediction error at timestep 1159 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1159 of -1
Current timestep = 1160. State = [[-0.14535065 -0.15564176]]. Action = [[-0.00607835  0.23504221  0.01733357 -0.57233495]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1160 is [True, False, False, True, False, False]
Scene graph at timestep 1160 is [True, False, False, True, False, False]
State prediction error at timestep 1160 is tensor(7.9505e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1160 of 1
Current timestep = 1161. State = [[-0.14689428 -0.13249947]]. Action = [[-0.16790898  0.18656182 -0.17880213  0.12241614]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1161 is [True, False, False, True, False, False]
Current timestep = 1162. State = [[-0.1500357  -0.12528776]]. Action = [[ 0.10813439 -0.13458692 -0.00304294  0.91919243]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1162 is [True, False, False, True, False, False]
Scene graph at timestep 1162 is [True, False, False, True, False, False]
State prediction error at timestep 1162 is tensor(3.9315e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1162 of 1
Current timestep = 1163. State = [[-0.14946012 -0.12567033]]. Action = [[ 0.03923726  0.077117   -0.1552511   0.69326115]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1163 is [True, False, False, True, False, False]
Current timestep = 1164. State = [[-0.14385693 -0.11550464]]. Action = [[ 0.0889619   0.12091899 -0.08703291 -0.1781584 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1164 is [True, False, False, True, False, False]
Scene graph at timestep 1164 is [True, False, False, False, True, False]
State prediction error at timestep 1164 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1164 of 1
Current timestep = 1165. State = [[-0.13904949 -0.1176682 ]]. Action = [[-0.13093129 -0.21314599 -0.23656955  0.66708446]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1165 is [True, False, False, False, True, False]
Current timestep = 1166. State = [[-0.13596363 -0.11882377]]. Action = [[0.18991631 0.17837659 0.05470875 0.9031961 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1166 is [True, False, False, False, True, False]
Scene graph at timestep 1166 is [True, False, False, False, True, False]
State prediction error at timestep 1166 is tensor(5.3917e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1166 of -1
Current timestep = 1167. State = [[-0.12938768 -0.10090363]]. Action = [[ 0.00357747  0.14140534  0.01672372 -0.72653055]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1167 is [True, False, False, False, True, False]
Scene graph at timestep 1167 is [True, False, False, False, True, False]
State prediction error at timestep 1167 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1167 of 1
Current timestep = 1168. State = [[-0.12251361 -0.09761684]]. Action = [[ 0.04948938 -0.13800153  0.0156225   0.9154854 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1168 is [True, False, False, False, True, False]
Scene graph at timestep 1168 is [True, False, False, False, True, False]
State prediction error at timestep 1168 is tensor(3.3130e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1168 of 1
Current timestep = 1169. State = [[-0.11467433 -0.1038732 ]]. Action = [[0.06295815 0.01312092 0.16058594 0.91936135]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1169 is [True, False, False, False, True, False]
Scene graph at timestep 1169 is [True, False, False, False, True, False]
State prediction error at timestep 1169 is tensor(1.8831e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1169 of 1
Current timestep = 1170. State = [[-0.11220989 -0.1045686 ]]. Action = [[-0.01922405 -0.02622442  0.21456319  0.01223457]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1170 is [True, False, False, False, True, False]
Current timestep = 1171. State = [[-0.10979294 -0.09852286]]. Action = [[0.08595425 0.13289648 0.00242573 0.18849194]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1171 is [True, False, False, False, True, False]
Scene graph at timestep 1171 is [True, False, False, False, True, False]
State prediction error at timestep 1171 is tensor(8.4106e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1171 of 1
Current timestep = 1172. State = [[-0.09174831 -0.08664085]]. Action = [[ 0.19972312  0.08686134  0.07392013 -0.32730275]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1172 is [True, False, False, False, True, False]
Scene graph at timestep 1172 is [True, False, False, False, True, False]
State prediction error at timestep 1172 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1172 of 1
Current timestep = 1173. State = [[-0.07213816 -0.06835385]]. Action = [[-0.01370277  0.20201969  0.11609221 -0.31354558]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1173 is [True, False, False, False, True, False]
Scene graph at timestep 1173 is [True, False, False, False, True, False]
State prediction error at timestep 1173 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1173 of 1
Current timestep = 1174. State = [[-0.07054094 -0.03967501]]. Action = [[ 0.06872195  0.16169327 -0.12673633  0.42843592]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1174 is [True, False, False, False, True, False]
Current timestep = 1175. State = [[-0.06828436 -0.03565595]]. Action = [[-0.029328   -0.11212099  0.10972053 -0.410374  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1175 is [True, False, False, False, True, False]
Scene graph at timestep 1175 is [True, False, False, False, True, False]
State prediction error at timestep 1175 is tensor(5.0309e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1175 of 1
Current timestep = 1176. State = [[-0.06925522 -0.05204955]]. Action = [[-0.05308314 -0.20377137 -0.13666232  0.76181066]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1176 is [True, False, False, False, True, False]
Current timestep = 1177. State = [[-0.07385333 -0.07789324]]. Action = [[-0.14249039 -0.17986348  0.20174202  0.7750673 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1177 is [True, False, False, False, True, False]
Scene graph at timestep 1177 is [True, False, False, False, True, False]
State prediction error at timestep 1177 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1177 of -1
Current timestep = 1178. State = [[-0.08706816 -0.10618109]]. Action = [[-0.21057868 -0.16729525  0.22774759  0.05741775]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1178 is [True, False, False, False, True, False]
Current timestep = 1179. State = [[-0.09890049 -0.12156328]]. Action = [[-0.03509426 -0.03787695  0.225263    0.84428763]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1179 is [True, False, False, False, True, False]
Scene graph at timestep 1179 is [True, False, False, False, True, False]
State prediction error at timestep 1179 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1179 of -1
Current timestep = 1180. State = [[-0.10025659 -0.12950887]]. Action = [[ 0.20745337 -0.05874403 -0.20323461  0.79119563]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1180 is [True, False, False, False, True, False]
Current timestep = 1181. State = [[-0.09819618 -0.13275076]]. Action = [[-0.11212975 -0.02156775  0.06246206 -0.43436098]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1181 is [True, False, False, True, False, False]
Scene graph at timestep 1181 is [True, False, False, True, False, False]
State prediction error at timestep 1181 is tensor(2.0052e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1181 of 1
Current timestep = 1182. State = [[-0.09796847 -0.12857035]]. Action = [[ 0.07522514  0.14017111 -0.13621634 -0.26999915]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1182 is [True, False, False, True, False, False]
Current timestep = 1183. State = [[-0.08914681 -0.11200098]]. Action = [[ 0.21228242  0.16206783 -0.08045316  0.48725426]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1183 is [True, False, False, True, False, False]
Current timestep = 1184. State = [[-0.06979808 -0.08431084]]. Action = [[ 0.14759436  0.22901312  0.1647951  -0.5271142 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1184 is [True, False, False, False, True, False]
Current timestep = 1185. State = [[-0.04912192 -0.05247021]]. Action = [[ 0.19810975  0.24301326  0.09464717 -0.7020035 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1185 is [True, False, False, False, True, False]
Current timestep = 1186. State = [[-0.23813269  0.14866449]]. Action = [[0.17329067 0.05723512 0.12307191 0.69670177]]. Reward = [100.]
Curr episode timestep = 29
Scene graph at timestep 1186 is [False, True, False, False, True, False]
Current timestep = 1187. State = [[-0.22278251  0.15289044]]. Action = [[ 0.14094359 -0.23394687  0.0743165  -0.33671463]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1187 is [True, False, False, False, False, True]
Current timestep = 1188. State = [[-0.21170643  0.14558381]]. Action = [[ 0.09327853  0.12857252  0.03033891 -0.6368713 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1188 is [True, False, False, False, False, True]
Scene graph at timestep 1188 is [True, False, False, False, False, True]
State prediction error at timestep 1188 is tensor(2.8267e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1188 of 1
Current timestep = 1189. State = [[-0.18524425  0.1605845 ]]. Action = [[ 0.23421067  0.17517674 -0.20763889  0.02051556]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1189 is [True, False, False, False, False, True]
Scene graph at timestep 1189 is [True, False, False, False, False, True]
State prediction error at timestep 1189 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1189 of -1
Current timestep = 1190. State = [[-0.1697974   0.19005835]]. Action = [[-0.15087284  0.23197931  0.05338478 -0.6502547 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1190 is [True, False, False, False, False, True]
Scene graph at timestep 1190 is [True, False, False, False, False, True]
State prediction error at timestep 1190 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1190 of -1
Current timestep = 1191. State = [[-0.18413627  0.20203447]]. Action = [[-0.1703481  -0.21727458 -0.23657523  0.17636716]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1191 is [True, False, False, False, False, True]
Current timestep = 1192. State = [[-0.17934547  0.18188526]]. Action = [[ 0.24697495 -0.12650985 -0.02322529 -0.18145084]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1192 is [True, False, False, False, False, True]
Current timestep = 1193. State = [[-0.17040661  0.15719968]]. Action = [[ 0.02324107 -0.19810675 -0.20127921 -0.86463135]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1193 is [True, False, False, False, False, True]
Scene graph at timestep 1193 is [True, False, False, False, False, True]
State prediction error at timestep 1193 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1193 of 1
Current timestep = 1194. State = [[-0.1605682  0.1294561]]. Action = [[ 0.09299147 -0.16833128  0.09568647 -0.84122705]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1194 is [True, False, False, False, False, True]
Current timestep = 1195. State = [[-0.1627097   0.13100785]]. Action = [[-0.23261578  0.21229652 -0.21098529 -0.82631874]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1195 is [True, False, False, False, False, True]
Current timestep = 1196. State = [[-0.17815208  0.13258025]]. Action = [[-0.19621648 -0.19656995  0.11181659  0.5583756 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1196 is [True, False, False, False, False, True]
Current timestep = 1197. State = [[-0.18410046  0.12066479]]. Action = [[ 0.21173465 -0.01404919 -0.13650234  0.21325052]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1197 is [True, False, False, False, False, True]
Current timestep = 1198. State = [[-0.17162564  0.11213333]]. Action = [[ 0.20068353 -0.0770704  -0.11470222  0.6746731 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1198 is [True, False, False, False, True, False]
Scene graph at timestep 1198 is [True, False, False, False, True, False]
State prediction error at timestep 1198 is tensor(9.1134e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1198 of 1
Current timestep = 1199. State = [[-0.16525105  0.11586899]]. Action = [[-0.21539918  0.1988627   0.23310685 -0.64130175]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1199 is [True, False, False, False, True, False]
Current timestep = 1200. State = [[-0.16921258  0.12311753]]. Action = [[ 0.09565246 -0.10108455 -0.04816519 -0.7916358 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1200 is [True, False, False, False, True, False]
Scene graph at timestep 1200 is [True, False, False, False, True, False]
State prediction error at timestep 1200 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1200 of -1
Current timestep = 1201. State = [[-0.15992144  0.11537056]]. Action = [[ 0.20865339 -0.05990899  0.19979382 -0.44749463]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1201 is [True, False, False, False, True, False]
Current timestep = 1202. State = [[-0.1420722   0.09851635]]. Action = [[ 0.12019518 -0.16672248  0.23050165 -0.8175546 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1202 is [True, False, False, False, True, False]
Current timestep = 1203. State = [[-0.12702356  0.0803289 ]]. Action = [[ 0.04495806 -0.10395433  0.22723114  0.86629486]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1203 is [True, False, False, False, True, False]
Scene graph at timestep 1203 is [True, False, False, False, True, False]
State prediction error at timestep 1203 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1203 of 1
Current timestep = 1204. State = [[-0.12089705  0.05681093]]. Action = [[-0.05298972 -0.20435093  0.11907509 -0.00054228]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1204 is [True, False, False, False, True, False]
Scene graph at timestep 1204 is [True, False, False, False, True, False]
State prediction error at timestep 1204 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1204 of 1
Current timestep = 1205. State = [[-0.12141098  0.0362751 ]]. Action = [[-0.10869309 -0.10298476 -0.10448867 -0.09960961]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1205 is [True, False, False, False, True, False]
Current timestep = 1206. State = [[-0.11929297  0.01653239]]. Action = [[ 0.16022176 -0.18512036 -0.18900046  0.73841095]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1206 is [True, False, False, False, True, False]
Current timestep = 1207. State = [[-0.11503471 -0.01114644]]. Action = [[-0.08757028 -0.18270108  0.12261769 -0.0354836 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1207 is [True, False, False, False, True, False]
Scene graph at timestep 1207 is [True, False, False, False, True, False]
State prediction error at timestep 1207 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1207 of -1
Current timestep = 1208. State = [[-0.11468046 -0.03979378]]. Action = [[-0.02875394 -0.18028745 -0.10333231  0.8262925 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1208 is [True, False, False, False, True, False]
Scene graph at timestep 1208 is [True, False, False, False, True, False]
State prediction error at timestep 1208 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1208 of -1
Current timestep = 1209. State = [[-0.11304533 -0.05670195]]. Action = [[ 0.21213078 -0.02259988  0.01456717  0.6425607 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1209 is [True, False, False, False, True, False]
Scene graph at timestep 1209 is [True, False, False, False, True, False]
State prediction error at timestep 1209 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1209 of -1
Current timestep = 1210. State = [[-0.10160048 -0.06438325]]. Action = [[ 0.12637997 -0.05120106 -0.02258024  0.07966709]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1210 is [True, False, False, False, True, False]
Current timestep = 1211. State = [[-0.09081494 -0.06592201]]. Action = [[0.00315684 0.05004641 0.13228464 0.05909157]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1211 is [True, False, False, False, True, False]
Scene graph at timestep 1211 is [True, False, False, False, True, False]
State prediction error at timestep 1211 is tensor(3.9052e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1211 of 1
Current timestep = 1212. State = [[-0.08479617 -0.05380152]]. Action = [[ 0.08332881  0.19345415  0.20425728 -0.9043255 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1212 is [True, False, False, False, True, False]
Current timestep = 1213. State = [[-0.07607777 -0.04613756]]. Action = [[ 0.05149209 -0.10940711  0.01248235 -0.12624925]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1213 is [True, False, False, False, True, False]
Current timestep = 1214. State = [[-0.06242457 -0.04631509]]. Action = [[ 0.228234    0.03144732  0.17483395 -0.85591155]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1214 is [True, False, False, False, True, False]
Scene graph at timestep 1214 is [True, False, False, False, True, False]
State prediction error at timestep 1214 is tensor(4.2527e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1214 of 1
Current timestep = 1215. State = [[-0.22547412  0.00240431]]. Action = [[ 0.19371569  0.00787652 -0.06946692 -0.5182915 ]]. Reward = [100.]
Curr episode timestep = 28
Scene graph at timestep 1215 is [True, False, False, False, True, False]
Current timestep = 1216. State = [[-0.2087142  -0.00196361]]. Action = [[ 0.16886812 -0.10879292 -0.24295144  0.9252176 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1216 is [True, False, False, False, True, False]
Scene graph at timestep 1216 is [True, False, False, False, True, False]
State prediction error at timestep 1216 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1216 of -1
Current timestep = 1217. State = [[-0.19049504 -0.00998064]]. Action = [[ 0.08657557 -0.04409981  0.07680723  0.5682788 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1217 is [True, False, False, False, True, False]
Current timestep = 1218. State = [[-0.17454393 -0.01968283]]. Action = [[ 0.150388   -0.10295692 -0.19073537  0.44548678]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1218 is [True, False, False, False, True, False]
Scene graph at timestep 1218 is [True, False, False, False, True, False]
State prediction error at timestep 1218 is tensor(9.7022e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1218 of 1
Current timestep = 1219. State = [[-0.15158969 -0.02575368]]. Action = [[ 0.15536886  0.09096444 -0.08024733 -0.10752678]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1219 is [True, False, False, False, True, False]
Current timestep = 1220. State = [[-0.13054079 -0.01552325]]. Action = [[ 0.16488373  0.12097916 -0.24718338  0.27833557]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1220 is [True, False, False, False, True, False]
Scene graph at timestep 1220 is [True, False, False, False, True, False]
State prediction error at timestep 1220 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1220 of 1
Current timestep = 1221. State = [[-0.10252863  0.00680964]]. Action = [[0.18893325 0.17351612 0.05418351 0.77211523]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1221 is [True, False, False, False, True, False]
Scene graph at timestep 1221 is [True, False, False, False, True, False]
State prediction error at timestep 1221 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1221 of 1
Current timestep = 1222. State = [[-0.0771672   0.03427948]]. Action = [[ 0.18935123  0.23790038  0.01253223 -0.8723256 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1222 is [True, False, False, False, True, False]
Scene graph at timestep 1222 is [True, False, False, False, True, False]
State prediction error at timestep 1222 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1222 of 1
Current timestep = 1223. State = [[-0.05745324  0.04691343]]. Action = [[-0.12097627 -0.15637791  0.13672763  0.6091912 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1223 is [True, False, False, False, True, False]
Scene graph at timestep 1223 is [True, False, False, False, True, False]
State prediction error at timestep 1223 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1223 of 1
Current timestep = 1224. State = [[-0.06166741  0.0376738 ]]. Action = [[-0.11973065  0.01952624  0.11169973  0.96058035]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1224 is [True, False, False, False, True, False]
Scene graph at timestep 1224 is [True, False, False, False, True, False]
State prediction error at timestep 1224 is tensor(6.6722e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1224 of 1
Current timestep = 1225. State = [[-0.06416755  0.0324988 ]]. Action = [[ 0.08079153 -0.11858127  0.19492814  0.00231659]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1225 is [True, False, False, False, True, False]
Scene graph at timestep 1225 is [True, False, False, False, True, False]
State prediction error at timestep 1225 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1225 of 1
Current timestep = 1226. State = [[-0.06204746  0.02350569]]. Action = [[ 0.13254416  0.02427676  0.2207408  -0.35936332]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1226 is [True, False, False, False, True, False]
Scene graph at timestep 1226 is [True, False, False, False, True, False]
State prediction error at timestep 1226 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1226 of 1
Current timestep = 1227. State = [[-0.05577616  0.03001164]]. Action = [[0.12562567 0.11618829 0.2212013  0.0121268 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1227 is [True, False, False, False, True, False]
Current timestep = 1228. State = [[-0.04132734  0.03360527]]. Action = [[ 0.04130498 -0.06331694  0.16913956 -0.10627681]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1228 is [True, False, False, False, True, False]
Scene graph at timestep 1228 is [False, True, False, False, True, False]
State prediction error at timestep 1228 is tensor(3.7947e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1228 of 1
Current timestep = 1229. State = [[-0.2471665   0.08958775]]. Action = [[ 0.18516779  0.05460742 -0.24163961 -0.3636533 ]]. Reward = [100.]
Curr episode timestep = 13
Scene graph at timestep 1229 is [False, True, False, False, True, False]
Scene graph at timestep 1229 is [True, False, False, False, True, False]
State prediction error at timestep 1229 is tensor(0.0249, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1229 of -1
Current timestep = 1230. State = [[-0.24026005  0.09107257]]. Action = [[-0.01075283 -0.20278093 -0.18660308  0.01477635]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1230 is [True, False, False, False, True, False]
Scene graph at timestep 1230 is [True, False, False, False, True, False]
State prediction error at timestep 1230 is tensor(2.9255e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1230 of 1
Current timestep = 1231. State = [[-0.23912926  0.07908872]]. Action = [[-0.01470935  0.01259595  0.12072918 -0.4091363 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1231 is [True, False, False, False, True, False]
Scene graph at timestep 1231 is [True, False, False, False, True, False]
State prediction error at timestep 1231 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1231 of 1
Current timestep = 1232. State = [[-0.23719342  0.0736701 ]]. Action = [[ 0.08696097 -0.0892999   0.02168721  0.12379599]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1232 is [True, False, False, False, True, False]
Current timestep = 1233. State = [[-0.23051389  0.07719404]]. Action = [[ 0.1062845   0.18926632 -0.23818089 -0.25637472]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1233 is [True, False, False, False, True, False]
Current timestep = 1234. State = [[-0.21100852  0.08843434]]. Action = [[0.15464386 0.06006095 0.08879444 0.5868182 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1234 is [True, False, False, False, True, False]
Current timestep = 1235. State = [[-0.18731461  0.08173094]]. Action = [[ 0.16153178 -0.24224627  0.07592508 -0.91010535]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1235 is [True, False, False, False, True, False]
Scene graph at timestep 1235 is [True, False, False, False, True, False]
State prediction error at timestep 1235 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1235 of 1
Current timestep = 1236. State = [[-0.16764359  0.06520113]]. Action = [[ 0.11475503 -0.05478635  0.07662827  0.39065218]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1236 is [True, False, False, False, True, False]
Current timestep = 1237. State = [[-0.16000271  0.056622  ]]. Action = [[-0.1446519  -0.09411758 -0.23009731  0.65684295]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1237 is [True, False, False, False, True, False]
Scene graph at timestep 1237 is [True, False, False, False, True, False]
State prediction error at timestep 1237 is tensor(3.9708e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1237 of 1
Current timestep = 1238. State = [[-0.1623394   0.05274308]]. Action = [[ 0.0497542   0.10367224  0.18480426 -0.00340587]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1238 is [True, False, False, False, True, False]
Scene graph at timestep 1238 is [True, False, False, False, True, False]
State prediction error at timestep 1238 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1238 of 1
Current timestep = 1239. State = [[-0.1548262   0.06134467]]. Action = [[ 0.18641037  0.08975306 -0.16373849  0.6047729 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1239 is [True, False, False, False, True, False]
Scene graph at timestep 1239 is [True, False, False, False, True, False]
State prediction error at timestep 1239 is tensor(7.7495e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1239 of 1
Current timestep = 1240. State = [[-0.13104337  0.05899727]]. Action = [[ 0.17806333 -0.17700325  0.17056376  0.8217919 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1240 is [True, False, False, False, True, False]
Current timestep = 1241. State = [[-0.11287796  0.04091786]]. Action = [[ 0.09169206 -0.1743215  -0.21286473  0.07119274]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1241 is [True, False, False, False, True, False]
Scene graph at timestep 1241 is [True, False, False, False, True, False]
State prediction error at timestep 1241 is tensor(6.1805e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1241 of 1
Current timestep = 1242. State = [[-0.09407485  0.01578849]]. Action = [[ 0.17637807 -0.15167049  0.11340386  0.58895063]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1242 is [True, False, False, False, True, False]
Current timestep = 1243. State = [[-0.07144573  0.00294165]]. Action = [[ 0.15490061 -0.02246486 -0.10992092  0.31755197]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1243 is [True, False, False, False, True, False]
Current timestep = 1244. State = [[-0.05960892  0.00503539]]. Action = [[-0.08757234  0.09816799 -0.06700423 -0.20393074]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1244 is [True, False, False, False, True, False]
Current timestep = 1245. State = [[-0.05128848 -0.00217188]]. Action = [[ 0.23151049 -0.18425065  0.11144176  0.04194403]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1245 is [True, False, False, False, True, False]
Current timestep = 1246. State = [[-0.24170929 -0.20788944]]. Action = [[ 0.1376304  -0.13481441 -0.00995094  0.21151435]]. Reward = [100.]
Curr episode timestep = 16
Scene graph at timestep 1246 is [True, False, False, False, True, False]
Current timestep = 1247. State = [[-0.24400909 -0.2204147 ]]. Action = [[-0.14594315  0.22368413 -0.1433375  -0.409131  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1247 is [True, False, False, True, False, False]
Current timestep = 1248. State = [[-0.23986404 -0.20015338]]. Action = [[ 0.18883038  0.17215204 -0.19960608  0.9656445 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1248 is [True, False, False, True, False, False]
Current timestep = 1249. State = [[-0.22554831 -0.17394854]]. Action = [[ 0.18025357  0.17696753 -0.11770764 -0.48674238]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1249 is [True, False, False, True, False, False]
Scene graph at timestep 1249 is [True, False, False, True, False, False]
State prediction error at timestep 1249 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1249 of 1
