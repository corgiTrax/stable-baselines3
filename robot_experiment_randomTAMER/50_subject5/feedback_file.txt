Current timestep = 0. State = [[-0.18739295  0.12144263]]. Action = [[-0.22670414  0.2206726  -0.04357933  0.34638643]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Scene graph at timestep 0 is [True, False, False, False, True, False]
State prediction error at timestep 0 is tensor(0.0243, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 0 of -1
Current timestep = 1. State = [[-0.19872378  0.13851061]]. Action = [[ 0.05896124 -0.10203031  0.03423762 -0.12096667]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
Current timestep = 2. State = [[-0.18767726  0.11797476]]. Action = [[ 0.22274792 -0.21431935 -0.05968112 -0.27503872]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, False, True]
Scene graph at timestep 2 is [True, False, False, False, True, False]
State prediction error at timestep 2 is tensor(0.0134, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2 of 1
Current timestep = 3. State = [[-0.17604515  0.09632465]]. Action = [[-0.15021588 -0.04864652 -0.15154003  0.36481476]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
Current timestep = 4. State = [[-0.18308678  0.10146619]]. Action = [[-0.0994164   0.10461238  0.16573381 -0.9409237 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
Current timestep = 5. State = [[-0.19086407  0.11375241]]. Action = [[-0.03284219  0.1042197   0.08615309 -0.4601401 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
Scene graph at timestep 5 is [True, False, False, False, True, False]
State prediction error at timestep 5 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.20031482  0.13342315]]. Action = [[-0.05525985  0.17851898  0.24572578 -0.56164455]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
Scene graph at timestep 6 is [True, False, False, False, False, True]
State prediction error at timestep 6 is tensor(0.0111, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.20547082  0.13250653]]. Action = [[ 0.00138161 -0.24957196 -0.03976378  0.54817414]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, False, True]
Current timestep = 8. State = [[-0.19846812  0.10587802]]. Action = [[ 0.15584368 -0.19846745  0.20317537 -0.33500135]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, False, True]
Scene graph at timestep 8 is [True, False, False, False, True, False]
State prediction error at timestep 8 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8 of 1
Current timestep = 9. State = [[-0.18257965  0.09979348]]. Action = [[0.2299568  0.24501163 0.1588893  0.70837057]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9 of 1
Current timestep = 10. State = [[-0.1575501   0.11066972]]. Action = [[ 0.1485228  -0.09238668  0.09088033 -0.94602686]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
Current timestep = 11. State = [[-0.13847128  0.10590629]]. Action = [[ 0.16666484 -0.00878309  0.18694285  0.37123024]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
Scene graph at timestep 11 is [True, False, False, False, True, False]
State prediction error at timestep 11 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 11 of 1
Current timestep = 12. State = [[-0.12619278  0.1177441 ]]. Action = [[-0.20338224  0.21128696  0.02079636  0.15221786]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
Current timestep = 13. State = [[-0.13501479  0.13836871]]. Action = [[0.05983931 0.1259569  0.09571451 0.7297672 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
Scene graph at timestep 13 is [True, False, False, False, False, True]
State prediction error at timestep 13 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.13321178  0.13471209]]. Action = [[ 0.09241363 -0.23907615 -0.12550317 -0.5040639 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, False, True]
Current timestep = 15. State = [[-0.11931105  0.12027383]]. Action = [[ 0.17458522 -0.0079969   0.2096979  -0.49516976]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, False, True]
Scene graph at timestep 15 is [True, False, False, False, True, False]
State prediction error at timestep 15 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 15 of 1
Current timestep = 16. State = [[-0.09339473  0.11602931]]. Action = [[ 0.18961781 -0.01954544  0.19817352 -0.78241915]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
Current timestep = 17. State = [[-0.08149622  0.12820892]]. Action = [[-0.08573231  0.22748375  0.09838611  0.6694164 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
Current timestep = 18. State = [[-0.084236    0.14939003]]. Action = [[ 0.0426122   0.1452229  -0.21244028  0.36607862]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, False, True]
Current timestep = 19. State = [[-0.07114906  0.15731591]]. Action = [[ 0.21657696 -0.05276695  0.0561032  -0.29042494]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, False, True]
Current timestep = 20. State = [[-0.05942866  0.16664526]]. Action = [[-0.16333944  0.10664037  0.1647642  -0.74012506]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, False, True]
Current timestep = 21. State = [[-0.05493124  0.16222402]]. Action = [[ 0.21741563 -0.17804858 -0.05486128  0.9059502 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, False, True]
Scene graph at timestep 21 is [True, False, False, False, False, True]
State prediction error at timestep 21 is tensor(0.0121, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 21 of 1
Current timestep = 22. State = [[-0.04021195  0.14011478]]. Action = [[-0.21373573 -0.21992233  0.1501947   0.64152014]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, False, True]
Current timestep = 23. State = [[-0.0502468   0.13901548]]. Action = [[-0.2393119   0.15654093  0.16077533 -0.02364939]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [False, True, False, False, False, True]
Current timestep = 24. State = [[-0.06852493  0.14928725]]. Action = [[0.0150167  0.03592306 0.12057137 0.7673166 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, False, True]
Scene graph at timestep 24 is [True, False, False, False, False, True]
State prediction error at timestep 24 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 24 of -1
Current timestep = 25. State = [[-0.08263135  0.1682848 ]]. Action = [[-0.21988927  0.23911127 -0.04452036  0.6202166 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, False, True]
Scene graph at timestep 25 is [True, False, False, False, False, True]
State prediction error at timestep 25 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 25 of -1
Current timestep = 26. State = [[-0.09943374  0.19728962]]. Action = [[ 0.16366681  0.1459102  -0.07269052  0.8282114 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, False, True]
Current timestep = 27. State = [[-0.09226231  0.21521774]]. Action = [[0.22772425 0.2454403  0.06903982 0.5737425 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, False, True]
Scene graph at timestep 27 is [True, False, False, False, False, True]
State prediction error at timestep 27 is tensor(0.0088, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 27 of -1
Current timestep = 28. State = [[-0.06876558  0.24723236]]. Action = [[ 0.13945743  0.18624449  0.10594413 -0.47586238]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, False, True]
Current timestep = 29. State = [[-0.05183312  0.2506038 ]]. Action = [[ 0.06549302 -0.17758375 -0.11730419  0.92580736]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, False, True]
Current timestep = 30. State = [[-0.03776692  0.2390844 ]]. Action = [[ 0.13948542 -0.0863829  -0.17605376 -0.8867914 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, False, True]
Current timestep = 31. State = [[-0.02253564  0.22965698]]. Action = [[ 0.11919707 -0.01902613  0.20578194  0.20629573]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [False, True, False, False, False, True]
Current timestep = 32. State = [[-0.00181695  0.21734065]]. Action = [[ 0.23769036 -0.19099998 -0.17431444  0.7293601 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [False, True, False, False, False, True]
Scene graph at timestep 32 is [False, True, False, False, False, True]
State prediction error at timestep 32 is tensor(0.0117, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 32 of 1
Current timestep = 33. State = [[0.02871269 0.19992524]]. Action = [[-0.14632145 -0.04925388  0.07879946  0.39539695]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [False, True, False, False, False, True]
Current timestep = 34. State = [[0.02879762 0.19455709]]. Action = [[ 0.02922848 -0.04559566  0.23371512 -0.34668434]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [False, True, False, False, False, True]
Scene graph at timestep 34 is [False, True, False, False, False, True]
State prediction error at timestep 34 is tensor(0.0109, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of 1
Current timestep = 35. State = [[0.03109814 0.18809664]]. Action = [[ 0.10469127 -0.0354725  -0.12180504 -0.89653945]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [False, True, False, False, False, True]
Current timestep = 36. State = [[0.03115745 0.18137859]]. Action = [[-0.23760174 -0.10317034 -0.12337852  0.34807372]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [False, True, False, False, False, True]
Current timestep = 37. State = [[0.02601751 0.1664062 ]]. Action = [[-0.09345391 -0.15514295  0.07680851 -0.3126371 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [False, True, False, False, False, True]
Scene graph at timestep 37 is [False, True, False, False, False, True]
State prediction error at timestep 37 is tensor(0.0068, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 37 of 1
Current timestep = 38. State = [[0.01170005 0.1454418 ]]. Action = [[-0.15342374 -0.1347317   0.04457647  0.01096165]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [False, True, False, False, False, True]
Scene graph at timestep 38 is [False, True, False, False, False, True]
State prediction error at timestep 38 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.00419292  0.13305597]]. Action = [[ 0.03544796  0.0206272  -0.0608297   0.241364  ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [False, True, False, False, False, True]
Scene graph at timestep 39 is [False, True, False, False, False, True]
State prediction error at timestep 39 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 39 of 1
Current timestep = 40. State = [[-0.00565449  0.12642606]]. Action = [[-0.09612495 -0.13244009 -0.18512951  0.39936447]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [False, True, False, False, False, True]
Current timestep = 41. State = [[-0.22262163 -0.09806805]]. Action = [[-0.05054238 -0.2181994  -0.07005809  0.47612727]]. Reward = [100.]
Curr episode timestep = 41
Scene graph at timestep 41 is [False, True, False, False, False, True]
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0371, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.20820907 -0.12034518]]. Action = [[ 0.16685253 -0.19770575 -0.22084002 -0.8605607 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 42 is [True, False, False, False, True, False]
Current timestep = 43. State = [[-0.19848855 -0.13255638]]. Action = [[-0.14890888  0.04964757 -0.06336679  0.53675437]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 43 is [True, False, False, False, True, False]
Scene graph at timestep 43 is [True, False, False, True, False, False]
State prediction error at timestep 43 is tensor(0.0441, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.19526342 -0.13440582]]. Action = [[0.21078342 0.00483212 0.11755896 0.01198351]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 44 is [True, False, False, True, False, False]
Current timestep = 45. State = [[-0.17492299 -0.1286794 ]]. Action = [[0.21014857 0.09146225 0.23228532 0.4131317 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 45 is [True, False, False, True, False, False]
Scene graph at timestep 45 is [True, False, False, True, False, False]
State prediction error at timestep 45 is tensor(0.0361, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 45 of 1
Current timestep = 46. State = [[-0.1574064  -0.11206412]]. Action = [[-0.20067827  0.15695348 -0.10793751  0.61966825]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 46 is [True, False, False, True, False, False]
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0342, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of 0
Current timestep = 47. State = [[-0.16084532 -0.09542566]]. Action = [[0.07993257 0.09741223 0.1923813  0.04163647]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 47 is [True, False, False, False, True, False]
Current timestep = 48. State = [[-0.16537236 -0.08687104]]. Action = [[-0.18060988  0.02150714 -0.02159125  0.2664554 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 48 is [True, False, False, False, True, False]
Current timestep = 49. State = [[-0.18384439 -0.07353209]]. Action = [[-0.21233077  0.16467696  0.14444304  0.37610793]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 49 is [True, False, False, False, True, False]
Current timestep = 50. State = [[-0.19983892 -0.07647102]]. Action = [[ 0.01893464 -0.23869312  0.08346221  0.47428656]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 50 is [True, False, False, False, True, False]
Current timestep = 51. State = [[-0.21369669 -0.08343447]]. Action = [[-0.223313    0.09327504  0.01857704 -0.782409  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 51 is [True, False, False, False, True, False]
Current timestep = 52. State = [[-0.22771496 -0.07412365]]. Action = [[ 0.0954873   0.11314529  0.15890339 -0.9721975 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 52 is [True, False, False, False, True, False]
Current timestep = 53. State = [[-0.2199018  -0.07246985]]. Action = [[ 0.2008878  -0.12470512  0.02772406 -0.6467714 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 53 is [True, False, False, False, True, False]
Current timestep = 54. State = [[-0.21731701 -0.06705908]]. Action = [[-0.15048136  0.16268569 -0.23954184  0.10181379]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 54 is [True, False, False, False, True, False]
Current timestep = 55. State = [[-0.22295165 -0.05779441]]. Action = [[-0.08931877  0.0317542  -0.13586476 -0.8836849 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 55 is [True, False, False, False, True, False]
Scene graph at timestep 55 is [True, False, False, False, True, False]
State prediction error at timestep 55 is tensor(0.0192, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 55 of -1
Current timestep = 56. State = [[-0.22590712 -0.06041092]]. Action = [[ 0.07466367 -0.13081197  0.06481999  0.7990842 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 56 is [True, False, False, False, True, False]
Current timestep = 57. State = [[-0.22577605 -0.06027083]]. Action = [[ 0.02956969  0.11885995 -0.02579021 -0.00398266]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 57 is [True, False, False, False, True, False]
Current timestep = 58. State = [[-0.22702861 -0.04756441]]. Action = [[-0.0667448   0.14508519 -0.1614067  -0.43572772]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 58 is [True, False, False, False, True, False]
Current timestep = 59. State = [[-0.2336177  -0.04446348]]. Action = [[-0.15023233 -0.13118337 -0.17822708 -0.5949991 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 59 is [True, False, False, False, True, False]
Current timestep = 60. State = [[-0.23638634 -0.06241038]]. Action = [[ 0.11176735 -0.20353802 -0.18596643  0.8869046 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 60 is [True, False, False, False, True, False]
Current timestep = 61. State = [[-0.22920081 -0.08903281]]. Action = [[ 0.18098462 -0.24161391 -0.23221949  0.01185882]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 61 is [True, False, False, False, True, False]
Scene graph at timestep 61 is [True, False, False, False, True, False]
State prediction error at timestep 61 is tensor(0.0173, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 61 of 1
Current timestep = 62. State = [[-0.22131889 -0.10768249]]. Action = [[-0.06532601  0.12259829  0.21150565 -0.55585164]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 62 is [True, False, False, False, True, False]
Current timestep = 63. State = [[-0.21938366 -0.09123885]]. Action = [[ 0.09308013  0.1859355   0.15044421 -0.33565938]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 63 is [True, False, False, False, True, False]
Current timestep = 64. State = [[-0.2152932  -0.08770451]]. Action = [[ 0.02493793 -0.18018019  0.23548767 -0.31050777]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 64 is [True, False, False, False, True, False]
Scene graph at timestep 64 is [True, False, False, False, True, False]
State prediction error at timestep 64 is tensor(0.0148, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 64 of -1
Current timestep = 65. State = [[-0.20729372 -0.08765099]]. Action = [[ 0.1260317   0.13887018 -0.1634612  -0.46743703]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 65 is [True, False, False, False, True, False]
Scene graph at timestep 65 is [True, False, False, False, True, False]
State prediction error at timestep 65 is tensor(0.0149, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 65 of 1
Current timestep = 66. State = [[-0.19125205 -0.07855537]]. Action = [[ 0.09897813 -0.02830775  0.07768583  0.67438674]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 66 is [True, False, False, False, True, False]
Current timestep = 67. State = [[-0.17637949 -0.08860689]]. Action = [[ 0.1448429  -0.15671049 -0.09198973  0.2401613 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 67 is [True, False, False, False, True, False]
Current timestep = 68. State = [[-0.16655761 -0.09445522]]. Action = [[-0.14851291  0.08386537  0.05455583 -0.25707084]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 68 is [True, False, False, False, True, False]
Scene graph at timestep 68 is [True, False, False, False, True, False]
State prediction error at timestep 68 is tensor(0.0111, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 68 of 1
Current timestep = 69. State = [[-0.17259578 -0.10543415]]. Action = [[-0.11998376 -0.18942492 -0.14501187  0.4842255 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 69 is [True, False, False, False, True, False]
Current timestep = 70. State = [[-0.17694254 -0.1155026 ]]. Action = [[ 0.10287407 -0.00201902  0.18029451  0.0559994 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 70 is [True, False, False, False, True, False]
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0142, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of -1
Current timestep = 71. State = [[-0.1769951  -0.11434878]]. Action = [[-0.0491707   0.06169209 -0.05767791  0.614321  ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 71 is [True, False, False, False, True, False]
Scene graph at timestep 71 is [True, False, False, False, True, False]
State prediction error at timestep 71 is tensor(0.0137, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 71 of 0
Current timestep = 72. State = [[-0.17529155 -0.11289049]]. Action = [[ 0.11576891 -0.00436127  0.20191866  0.5611141 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 72 is [True, False, False, False, True, False]
Scene graph at timestep 72 is [True, False, False, False, True, False]
State prediction error at timestep 72 is tensor(0.0121, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 72 of 0
Current timestep = 73. State = [[-0.16906683 -0.10223462]]. Action = [[ 0.07700431  0.14872748 -0.12466341 -0.79357976]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 73 is [True, False, False, False, True, False]
Current timestep = 74. State = [[-0.1690479  -0.09666809]]. Action = [[-0.16428967 -0.04457162 -0.22729844  0.284024  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 74 is [True, False, False, False, True, False]
Current timestep = 75. State = [[-0.18222772 -0.1115619 ]]. Action = [[-0.24008776 -0.21153405 -0.0711695  -0.44280374]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 75 is [True, False, False, False, True, False]
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(0.0113, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.19546218 -0.13648215]]. Action = [[ 0.24128199 -0.1551482  -0.22457199  0.46421874]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 76 is [True, False, False, False, True, False]
Current timestep = 77. State = [[-0.17744794 -0.1593369 ]]. Action = [[ 0.20208728 -0.24352373 -0.10637665  0.02721918]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 77 is [True, False, False, True, False, False]
Scene graph at timestep 77 is [True, False, False, True, False, False]
State prediction error at timestep 77 is tensor(0.0172, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 77 of -1
Current timestep = 78. State = [[-0.14990163 -0.171105  ]]. Action = [[ 0.20430231  0.19783193 -0.16073114  0.21591723]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 78 is [True, False, False, True, False, False]
Current timestep = 79. State = [[-0.12653252 -0.14584437]]. Action = [[ 0.13927135  0.24113613 -0.03784733  0.58340836]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 79 is [True, False, False, True, False, False]
Scene graph at timestep 79 is [True, False, False, True, False, False]
State prediction error at timestep 79 is tensor(0.0148, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of 1
Current timestep = 80. State = [[-0.1124084  -0.13541529]]. Action = [[-0.02423808 -0.23213616  0.24418181 -0.5454389 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 80 is [True, False, False, True, False, False]
Current timestep = 81. State = [[-0.11057571 -0.14856917]]. Action = [[ 0.06601107 -0.00597097  0.10099024  0.59038305]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 81 is [True, False, False, True, False, False]
Current timestep = 82. State = [[-0.10186091 -0.15549646]]. Action = [[ 0.06187782 -0.06200269  0.16826448 -0.34033877]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 82 is [True, False, False, True, False, False]
Current timestep = 83. State = [[-0.09084105 -0.16440697]]. Action = [[ 0.0676496  -0.06313491 -0.0436866  -0.05515492]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 83 is [True, False, False, True, False, False]
Current timestep = 84. State = [[-0.07473347 -0.18187791]]. Action = [[ 0.22702897 -0.21922158  0.20841789 -0.18368113]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 84 is [True, False, False, True, False, False]
Scene graph at timestep 84 is [True, False, False, True, False, False]
State prediction error at timestep 84 is tensor(0.0186, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of -1
Current timestep = 85. State = [[-0.04267674 -0.18946296]]. Action = [[ 0.17055976  0.23251414 -0.01369388 -0.78398967]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 85 is [True, False, False, True, False, False]
Scene graph at timestep 85 is [False, True, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0243, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.02780661 -0.17454052]]. Action = [[-0.23695533  0.04751    -0.1924108  -0.77795637]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 86 is [False, True, False, True, False, False]
Current timestep = 87. State = [[-0.04131956 -0.16299693]]. Action = [[-0.21491243  0.17534876 -0.07921389  0.30679083]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 87 is [False, True, False, True, False, False]
Scene graph at timestep 87 is [False, True, False, True, False, False]
State prediction error at timestep 87 is tensor(0.0185, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 87 of 1
Current timestep = 88. State = [[-0.06448429 -0.1615297 ]]. Action = [[-0.07745665 -0.241512   -0.11495638  0.11852443]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 88 is [False, True, False, True, False, False]
Scene graph at timestep 88 is [True, False, False, True, False, False]
State prediction error at timestep 88 is tensor(0.0148, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of -1
Current timestep = 89. State = [[-0.07377546 -0.18838009]]. Action = [[ 0.1393722  -0.19117466 -0.06700838  0.12177205]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 89 is [True, False, False, True, False, False]
Scene graph at timestep 89 is [True, False, False, True, False, False]
State prediction error at timestep 89 is tensor(0.0178, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 89 of -1
Current timestep = 90. State = [[-0.07566492 -0.1979286 ]]. Action = [[-0.17769065  0.11313683 -0.11937664 -0.1097194 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 90 is [True, False, False, True, False, False]
Current timestep = 91. State = [[-0.08646884 -0.19697699]]. Action = [[-0.16968484 -0.03207034  0.11731404  0.3519895 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 91 is [True, False, False, True, False, False]
Scene graph at timestep 91 is [True, False, False, True, False, False]
State prediction error at timestep 91 is tensor(0.0173, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 91 of -1
Current timestep = 92. State = [[-0.1009097 -0.1928448]]. Action = [[ 0.18343383  0.0744414  -0.05366704 -0.8718455 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 92 is [True, False, False, True, False, False]
Scene graph at timestep 92 is [True, False, False, True, False, False]
State prediction error at timestep 92 is tensor(0.0145, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 92 of 1
Current timestep = 93. State = [[-0.08636133 -0.1734814 ]]. Action = [[ 0.2199502   0.17750144 -0.12870611 -0.16331983]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 93 is [True, False, False, True, False, False]
Current timestep = 94. State = [[-0.07370531 -0.14773539]]. Action = [[-0.14376129  0.23915786  0.23587722  0.43498504]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 94 is [True, False, False, True, False, False]
Scene graph at timestep 94 is [True, False, False, True, False, False]
State prediction error at timestep 94 is tensor(0.0088, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.07605945 -0.11527235]]. Action = [[-0.08215316  0.15896231  0.22965693  0.7153406 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 95 is [True, False, False, True, False, False]
Current timestep = 96. State = [[-0.08891912 -0.1153032 ]]. Action = [[-0.19508202 -0.17072879  0.13146922  0.63917696]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 96 is [True, False, False, False, True, False]
Current timestep = 97. State = [[-0.10814288 -0.12223825]]. Action = [[-0.10548261  0.01798677 -0.15852602  0.9206755 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 97 is [True, False, False, False, True, False]
Scene graph at timestep 97 is [True, False, False, False, True, False]
State prediction error at timestep 97 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 97 of -1
Current timestep = 98. State = [[-0.11451402 -0.12725043]]. Action = [[ 0.22729021 -0.07969208  0.13726783 -0.8922797 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 98 is [True, False, False, False, True, False]
Scene graph at timestep 98 is [True, False, False, True, False, False]
State prediction error at timestep 98 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 98 of 1
Current timestep = 99. State = [[-0.09906969 -0.11564209]]. Action = [[0.23679787 0.24510205 0.12824726 0.6057379 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 99 is [True, False, False, True, False, False]
Current timestep = 100. State = [[-0.08851358 -0.08927653]]. Action = [[-0.2332695   0.219731    0.2128399   0.81762457]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 100 is [True, False, False, False, True, False]
Current timestep = 101. State = [[-0.08964375 -0.07591053]]. Action = [[ 0.13759091 -0.11489974 -0.20722558 -0.06463754]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 101 is [True, False, False, False, True, False]
Current timestep = 102. State = [[-0.09006587 -0.07145386]]. Action = [[-0.01839699  0.1329619  -0.15403365  0.344954  ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 102 is [True, False, False, False, True, False]
Current timestep = 103. State = [[-0.09323601 -0.07823704]]. Action = [[-0.15137935 -0.22908732 -0.16478986 -0.2510624 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 103 is [True, False, False, False, True, False]
Current timestep = 104. State = [[-0.09612081 -0.09124343]]. Action = [[ 0.03506124 -0.0300042  -0.04087546  0.8338158 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 104 is [True, False, False, False, True, False]
Current timestep = 105. State = [[-0.0985451 -0.0969815]]. Action = [[-0.0492063  -0.01428044 -0.11574626  0.69022346]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 105 is [True, False, False, False, True, False]
Current timestep = 106. State = [[-0.10164413 -0.09577379]]. Action = [[ 0.00053686  0.07529435 -0.17554192  0.18564057]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 106 is [True, False, False, False, True, False]
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 106 of -1
Current timestep = 107. State = [[-0.10061615 -0.09585357]]. Action = [[ 0.10685137 -0.07188496 -0.15160008  0.2897761 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 107 is [True, False, False, False, True, False]
Current timestep = 108. State = [[-0.09813709 -0.11155164]]. Action = [[-0.00117597 -0.24167249 -0.22866945 -0.12860537]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 108 is [True, False, False, False, True, False]
Scene graph at timestep 108 is [True, False, False, False, True, False]
State prediction error at timestep 108 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of -1
Current timestep = 109. State = [[-0.09698979 -0.12312807]]. Action = [[ 0.00557163  0.18034476 -0.16052197  0.23241222]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 109 is [True, False, False, False, True, False]
Scene graph at timestep 109 is [True, False, False, False, True, False]
State prediction error at timestep 109 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 109 of 1
Current timestep = 110. State = [[-0.09632912 -0.12141771]]. Action = [[-0.03008974 -0.16878308 -0.14794473  0.25235987]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 110 is [True, False, False, False, True, False]
Current timestep = 111. State = [[-0.09571903 -0.14038724]]. Action = [[ 0.07889342 -0.17771944  0.02539074  0.8747668 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 111 is [True, False, False, False, True, False]
Current timestep = 112. State = [[-0.09136664 -0.14930736]]. Action = [[0.11517337 0.07776085 0.08984202 0.6470456 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 112 is [True, False, False, True, False, False]
Current timestep = 113. State = [[-0.07728079 -0.14402814]]. Action = [[0.12870365 0.06584275 0.12802857 0.48071325]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 113 is [True, False, False, True, False, False]
Scene graph at timestep 113 is [True, False, False, True, False, False]
State prediction error at timestep 113 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.06751163 -0.14063528]]. Action = [[-0.20644948  0.01153874  0.09611169 -0.5960777 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 114 is [True, False, False, True, False, False]
Scene graph at timestep 114 is [True, False, False, True, False, False]
State prediction error at timestep 114 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 114 of -1
Current timestep = 115. State = [[-0.08001494 -0.14590244]]. Action = [[-0.1764443  -0.08275715  0.02602902  0.8441334 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 115 is [True, False, False, True, False, False]
Current timestep = 116. State = [[-0.0850915  -0.14709121]]. Action = [[ 0.22098285  0.05472839 -0.18714306 -0.6133457 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 116 is [True, False, False, True, False, False]
Scene graph at timestep 116 is [True, False, False, True, False, False]
State prediction error at timestep 116 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 116 of -1
Current timestep = 117. State = [[-0.07452033 -0.13714722]]. Action = [[ 0.16111791  0.08828661 -0.18773453  0.50690913]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 117 is [True, False, False, True, False, False]
Current timestep = 118. State = [[-0.06158189 -0.14032689]]. Action = [[ 0.11709529 -0.18343629  0.16205385 -0.16113281]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 118 is [True, False, False, True, False, False]
Current timestep = 119. State = [[-0.05489138 -0.15316482]]. Action = [[-0.21278341 -0.03883725 -0.11347243  0.27112758]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 119 is [True, False, False, True, False, False]
Scene graph at timestep 119 is [True, False, False, True, False, False]
State prediction error at timestep 119 is tensor(0.0069, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of -1
Current timestep = 120. State = [[-0.0655231  -0.16011135]]. Action = [[-0.20176685  0.02267843  0.14276606 -0.71202224]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 120 is [True, False, False, True, False, False]
Current timestep = 121. State = [[-0.07845926 -0.15217191]]. Action = [[ 0.02564171  0.13873151 -0.09542072 -0.22218287]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 121 is [True, False, False, True, False, False]
Scene graph at timestep 121 is [True, False, False, True, False, False]
State prediction error at timestep 121 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 121 of -1
Current timestep = 122. State = [[-0.08943566 -0.13098218]]. Action = [[-0.23961094  0.1758936   0.21167076  0.96815133]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 122 is [True, False, False, True, False, False]
Current timestep = 123. State = [[-0.1091425  -0.12694386]]. Action = [[-0.05300122 -0.14734076 -0.17768213  0.87311935]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 123 is [True, False, False, True, False, False]
Scene graph at timestep 123 is [True, False, False, True, False, False]
State prediction error at timestep 123 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of -1
Current timestep = 124. State = [[-0.12460249 -0.14137486]]. Action = [[-0.20820507 -0.08214056  0.11595044  0.3783934 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 124 is [True, False, False, True, False, False]
Current timestep = 125. State = [[-0.14566661 -0.15807623]]. Action = [[-0.03666866 -0.18238159 -0.22628108  0.7259662 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 125 is [True, False, False, True, False, False]
Current timestep = 126. State = [[-0.14742368 -0.17550254]]. Action = [[ 0.2392968  -0.11225548  0.23168713  0.7772534 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 126 is [True, False, False, True, False, False]
Scene graph at timestep 126 is [True, False, False, True, False, False]
State prediction error at timestep 126 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of -1
Current timestep = 127. State = [[-0.1494748 -0.1874185]]. Action = [[-0.23150687  0.00691777  0.2160241  -0.50342077]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 127 is [True, False, False, True, False, False]
Scene graph at timestep 127 is [True, False, False, True, False, False]
State prediction error at timestep 127 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 127 of -1
Current timestep = 128. State = [[-0.15402257 -0.1902285 ]]. Action = [[ 0.12531874  0.00774103 -0.02993366 -0.84664786]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 128 is [True, False, False, True, False, False]
Current timestep = 129. State = [[-0.15355538 -0.18920363]]. Action = [[-0.04472962  0.0251067  -0.1609424  -0.83745104]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 129 is [True, False, False, True, False, False]
Current timestep = 130. State = [[-0.1588477  -0.19808936]]. Action = [[-0.1465633  -0.16099182  0.10230634  0.13717127]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 130 is [True, False, False, True, False, False]
Scene graph at timestep 130 is [True, False, False, True, False, False]
State prediction error at timestep 130 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 130 of -1
Current timestep = 131. State = [[-0.15954146 -0.19718392]]. Action = [[0.184941   0.21624047 0.14829075 0.10848296]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 131 is [True, False, False, True, False, False]
Scene graph at timestep 131 is [True, False, False, True, False, False]
State prediction error at timestep 131 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.15415002 -0.16993901]]. Action = [[-0.13490936  0.16730031 -0.06208012 -0.45298833]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 132 is [True, False, False, True, False, False]
Current timestep = 133. State = [[-0.16504328 -0.17118163]]. Action = [[-0.18498845 -0.17451087 -0.17696731 -0.5994229 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 133 is [True, False, False, True, False, False]
Scene graph at timestep 133 is [True, False, False, True, False, False]
State prediction error at timestep 133 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 133 of -1
Current timestep = 134. State = [[-0.1832093 -0.1739132]]. Action = [[ 0.04105997  0.12778741 -0.24610475  0.60747814]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 134 is [True, False, False, True, False, False]
Scene graph at timestep 134 is [True, False, False, True, False, False]
State prediction error at timestep 134 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.17742041 -0.15598965]]. Action = [[0.16473967 0.12735161 0.19894058 0.7267016 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 135 is [True, False, False, True, False, False]
Scene graph at timestep 135 is [True, False, False, True, False, False]
State prediction error at timestep 135 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 135 of 1
Current timestep = 136. State = [[-0.16465192 -0.14429294]]. Action = [[ 0.16836655 -0.04753906  0.11606222 -0.7126546 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 136 is [True, False, False, True, False, False]
Scene graph at timestep 136 is [True, False, False, True, False, False]
State prediction error at timestep 136 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.145589  -0.1327133]]. Action = [[ 0.16589403  0.20885092 -0.01235732 -0.6293172 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 137 is [True, False, False, True, False, False]
Current timestep = 138. State = [[-0.12110371 -0.10990052]]. Action = [[ 0.23313963  0.16955769 -0.09868392  0.47835827]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 138 is [True, False, False, True, False, False]
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 138 of 1
Current timestep = 139. State = [[-0.09569295 -0.10284866]]. Action = [[ 0.09552395 -0.20902377 -0.10603414 -0.30497396]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 139 is [True, False, False, False, True, False]
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 139 of 1
Current timestep = 140. State = [[-0.08063217 -0.12525468]]. Action = [[ 0.16064104 -0.13568698 -0.08815171  0.45580578]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 140 is [True, False, False, False, True, False]
Current timestep = 141. State = [[-0.06974148 -0.1280825 ]]. Action = [[-0.11821079  0.13533372  0.12314296 -0.4915241 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 141 is [True, False, False, True, False, False]
Scene graph at timestep 141 is [True, False, False, True, False, False]
State prediction error at timestep 141 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.06092527 -0.11075506]]. Action = [[0.23534262 0.20699817 0.20361039 0.22633588]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 142 is [True, False, False, True, False, False]
Current timestep = 143. State = [[-0.0381529  -0.10666554]]. Action = [[ 0.1992808  -0.20490481  0.21343619 -0.1870482 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 143 is [True, False, False, False, True, False]
Current timestep = 144. State = [[-0.02425536 -0.11156347]]. Action = [[-0.12256005  0.07256317 -0.15724939  0.1111927 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 144 is [False, True, False, False, True, False]
Scene graph at timestep 144 is [False, True, False, False, True, False]
State prediction error at timestep 144 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 144 of 1
Current timestep = 145. State = [[-0.0294221  -0.11883261]]. Action = [[-0.17991541 -0.11603752  0.10384411 -0.68437743]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 145 is [False, True, False, False, True, False]
Scene graph at timestep 145 is [False, True, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.0383259  -0.11529063]]. Action = [[-0.07398494  0.21542335  0.08315721  0.02161968]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 146 is [False, True, False, False, True, False]
Current timestep = 147. State = [[-0.03742231 -0.08800349]]. Action = [[ 0.22270218  0.21098548  0.11474025 -0.88871306]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 147 is [False, True, False, False, True, False]
Scene graph at timestep 147 is [False, True, False, False, True, False]
State prediction error at timestep 147 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 147 of 1
Current timestep = 148. State = [[-0.26692846 -0.21352236]]. Action = [[ 0.18928108 -0.18452528  0.14754137  0.94080997]]. Reward = [100.]
Curr episode timestep = 106
Scene graph at timestep 148 is [False, True, False, False, True, False]
Scene graph at timestep 148 is [True, False, False, True, False, False]
State prediction error at timestep 148 is tensor(0.0203, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.25447795 -0.22869676]]. Action = [[ 0.2333929   0.16732126 -0.21726944  0.06930673]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 149 is [True, False, False, True, False, False]
Scene graph at timestep 149 is [True, False, False, True, False, False]
State prediction error at timestep 149 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 149 of 1
Current timestep = 150. State = [[-0.22671692 -0.21162283]]. Action = [[ 0.15756977  0.09405291 -0.19988492  0.06530392]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 150 is [True, False, False, True, False, False]
Current timestep = 151. State = [[-0.21266235 -0.2161969 ]]. Action = [[ 0.09111634 -0.19912717 -0.02360854 -0.15244555]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 151 is [True, False, False, True, False, False]
Scene graph at timestep 151 is [True, False, False, True, False, False]
State prediction error at timestep 151 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 151 of 1
Current timestep = 152. State = [[-0.19019608 -0.21717669]]. Action = [[ 0.18296617  0.18168569  0.03403676 -0.20460272]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 152 is [True, False, False, True, False, False]
Scene graph at timestep 152 is [True, False, False, True, False, False]
State prediction error at timestep 152 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 152 of 1
Current timestep = 153. State = [[-0.17005691 -0.21373945]]. Action = [[ 0.09115404 -0.13678677  0.2457124  -0.81431097]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 153 is [True, False, False, True, False, False]
Scene graph at timestep 153 is [True, False, False, True, False, False]
State prediction error at timestep 153 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 153 of 1
Current timestep = 154. State = [[-0.16182645 -0.22732039]]. Action = [[-0.13353406 -0.03725116  0.22876638 -0.3950343 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 154 is [True, False, False, True, False, False]
Current timestep = 155. State = [[-0.16897906 -0.24376462]]. Action = [[-0.0138718  -0.20309354 -0.02627839  0.09279096]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 155 is [True, False, False, True, False, False]
Scene graph at timestep 155 is [True, False, False, True, False, False]
State prediction error at timestep 155 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of -1
Current timestep = 156. State = [[-0.17266768 -0.25724477]]. Action = [[0.03247565 0.03726628 0.1437679  0.2137891 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 156 is [True, False, False, True, False, False]
Current timestep = 157. State = [[-0.17200091 -0.2561412 ]]. Action = [[ 0.02495539 -0.00139405  0.07759762 -0.13346636]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 157 is [True, False, False, True, False, False]
Scene graph at timestep 157 is [True, False, False, True, False, False]
State prediction error at timestep 157 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 157 of -1
Current timestep = 158. State = [[-0.17613623 -0.26849678]]. Action = [[-0.07267278 -0.2344932   0.23194867 -0.36612546]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 158 is [True, False, False, True, False, False]
Scene graph at timestep 158 is [True, False, False, True, False, False]
State prediction error at timestep 158 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of -1
Current timestep = 159. State = [[-0.18054537 -0.2855885 ]]. Action = [[-0.00738293 -0.16355765  0.16898036  0.7376728 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 159 is [True, False, False, True, False, False]
Current timestep = 160. State = [[-0.18581524 -0.27978432]]. Action = [[-0.20668422  0.19411632  0.05684596  0.61785245]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 160 is [True, False, False, True, False, False]
Current timestep = 161. State = [[-0.19645268 -0.2647928 ]]. Action = [[-0.08536808  0.15203565 -0.09658042  0.41301894]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 161 is [True, False, False, True, False, False]
Scene graph at timestep 161 is [True, False, False, True, False, False]
State prediction error at timestep 161 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.21156932 -0.25626397]]. Action = [[-0.14756359 -0.09073603 -0.01693775  0.16977966]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 162 is [True, False, False, True, False, False]
Scene graph at timestep 162 is [True, False, False, True, False, False]
State prediction error at timestep 162 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.2350052  -0.27534637]]. Action = [[-0.12742166 -0.22958913 -0.11144486 -0.7187784 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 163 is [True, False, False, True, False, False]
Scene graph at timestep 163 is [True, False, False, True, False, False]
State prediction error at timestep 163 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 163 of -1
Current timestep = 164. State = [[-0.25194445 -0.2807602 ]]. Action = [[-0.09693286  0.23680586 -0.22946267 -0.91758484]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 164 is [True, False, False, True, False, False]
Current timestep = 165. State = [[-0.25688943 -0.267986  ]]. Action = [[-0.1950341  -0.00825235 -0.09928668  0.49415696]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 165 is [True, False, False, True, False, False]
Scene graph at timestep 165 is [True, False, False, True, False, False]
State prediction error at timestep 165 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.25680476 -0.2664562 ]]. Action = [[ 0.08066407 -0.24614277  0.10564104  0.6853564 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 166 is [True, False, False, True, False, False]
Current timestep = 167. State = [[-0.25106952 -0.25578493]]. Action = [[ 0.1743649   0.13842702 -0.16615924 -0.37152857]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 167 is [True, False, False, True, False, False]
Current timestep = 168. State = [[-0.23561046 -0.24672931]]. Action = [[ 0.22468188 -0.09635296 -0.11654615  0.40077674]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 168 is [True, False, False, True, False, False]
Scene graph at timestep 168 is [True, False, False, True, False, False]
State prediction error at timestep 168 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of 1
Current timestep = 169. State = [[-0.2154733  -0.24094377]]. Action = [[ 0.07477975  0.09910715 -0.16606943  0.96228445]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 169 is [True, False, False, True, False, False]
Scene graph at timestep 169 is [True, False, False, True, False, False]
State prediction error at timestep 169 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of 1
Current timestep = 170. State = [[-0.19897717 -0.22348781]]. Action = [[ 0.1931361   0.18437967 -0.0108175   0.8287051 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 170 is [True, False, False, True, False, False]
Current timestep = 171. State = [[-0.17924872 -0.22220153]]. Action = [[ 0.15356219 -0.22450548 -0.16491413  0.9421158 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 171 is [True, False, False, True, False, False]
Current timestep = 172. State = [[-0.16168252 -0.23764285]]. Action = [[ 0.01411304 -0.08791134 -0.16984293 -0.752057  ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 172 is [True, False, False, True, False, False]
Current timestep = 173. State = [[-0.15306436 -0.25876582]]. Action = [[ 0.10886782 -0.23373175 -0.08930793  0.23681259]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 173 is [True, False, False, True, False, False]
Scene graph at timestep 173 is [True, False, False, True, False, False]
State prediction error at timestep 173 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 173 of -1
Current timestep = 174. State = [[-0.14667238 -0.2778761 ]]. Action = [[-0.22441243  0.11853698 -0.00814652  0.17665446]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 174 is [True, False, False, True, False, False]
Scene graph at timestep 174 is [True, False, False, True, False, False]
State prediction error at timestep 174 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 174 of -1
Current timestep = 175. State = [[-0.15442558 -0.27659526]]. Action = [[ 0.1079486  -0.07435161 -0.2041017   0.91693497]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 175 is [True, False, False, True, False, False]
Scene graph at timestep 175 is [True, False, False, True, False, False]
State prediction error at timestep 175 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of -1
Current timestep = 176. State = [[-0.15326747 -0.2774969 ]]. Action = [[ 0.02798581  0.00221342 -0.11643377 -0.11533272]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 176 is [True, False, False, True, False, False]
Current timestep = 177. State = [[-0.1405215  -0.26391286]]. Action = [[0.23149407 0.23333138 0.21389648 0.7098036 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 177 is [True, False, False, True, False, False]
Scene graph at timestep 177 is [True, False, False, True, False, False]
State prediction error at timestep 177 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 177 of 1
Current timestep = 178. State = [[-0.11269143 -0.24002536]]. Action = [[ 0.1909037   0.06079486  0.2474072  -0.10738671]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 178 is [True, False, False, True, False, False]
Scene graph at timestep 178 is [True, False, False, True, False, False]
State prediction error at timestep 178 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 178 of 1
Current timestep = 179. State = [[-0.10123476 -0.23830666]]. Action = [[-0.24856083  0.03060859 -0.16720968 -0.55502224]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 179 is [True, False, False, True, False, False]
Scene graph at timestep 179 is [True, False, False, True, False, False]
State prediction error at timestep 179 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 179 of 1
Current timestep = 180. State = [[-0.10535603 -0.23020193]]. Action = [[ 0.06842566  0.13860649 -0.11434034 -0.02730149]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 180 is [True, False, False, True, False, False]
Scene graph at timestep 180 is [True, False, False, True, False, False]
State prediction error at timestep 180 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of 1
Current timestep = 181. State = [[-0.10793717 -0.22719309]]. Action = [[-0.03632283 -0.17099766  0.0210695   0.8165355 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 181 is [True, False, False, True, False, False]
Scene graph at timestep 181 is [True, False, False, True, False, False]
State prediction error at timestep 181 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.11237063 -0.24472542]]. Action = [[ 0.05278155 -0.14358605  0.18163794  0.13545299]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 182 is [True, False, False, True, False, False]
Current timestep = 183. State = [[-0.10309839 -0.25173116]]. Action = [[ 0.22025418 -0.00871058 -0.14514989 -0.58290464]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 183 is [True, False, False, True, False, False]
Scene graph at timestep 183 is [True, False, False, True, False, False]
State prediction error at timestep 183 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of 1
Current timestep = 184. State = [[-0.07637723 -0.26344615]]. Action = [[ 0.21400768 -0.1502679   0.02880919 -0.20076215]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 184 is [True, False, False, True, False, False]
Scene graph at timestep 184 is [True, False, False, True, False, False]
State prediction error at timestep 184 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 184 of 1
Current timestep = 185. State = [[-0.05238689 -0.2752964 ]]. Action = [[ 0.14639717 -0.23576769  0.03021297 -0.6572966 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 185 is [True, False, False, True, False, False]
Current timestep = 186. State = [[-0.05238689 -0.2752964 ]]. Action = [[-0.10079211 -0.2127296  -0.10150069  0.66388464]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 186 is [True, False, False, True, False, False]
Current timestep = 187. State = [[-0.05254789 -0.27582157]]. Action = [[-0.01242524 -0.00588259 -0.06465803  0.09915543]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 187 is [True, False, False, True, False, False]
Current timestep = 188. State = [[-0.05349358 -0.28495082]]. Action = [[ 0.03633368 -0.14628142 -0.14677808  0.02893126]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 188 is [True, False, False, True, False, False]
Scene graph at timestep 188 is [True, False, False, True, False, False]
State prediction error at timestep 188 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of -1
Current timestep = 189. State = [[-0.05291617 -0.29582062]]. Action = [[-0.00217329 -0.19444154 -0.0402611  -0.76115423]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 189 is [True, False, False, True, False, False]
Current timestep = 190. State = [[-0.05506918 -0.2907604 ]]. Action = [[-0.20784362  0.17714861  0.01169568  0.13660383]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 190 is [True, False, False, True, False, False]
Current timestep = 191. State = [[-0.05639188 -0.2874012 ]]. Action = [[ 0.07903022 -0.23594508 -0.21140836 -0.12025928]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 191 is [True, False, False, True, False, False]
Scene graph at timestep 191 is [True, False, False, True, False, False]
State prediction error at timestep 191 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of 1
Current timestep = 192. State = [[-0.05570928 -0.2801703 ]]. Action = [[ 0.10252699  0.06091332 -0.21947183  0.77388346]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 192 is [True, False, False, True, False, False]
Current timestep = 193. State = [[-0.05307612 -0.26374426]]. Action = [[-0.05251998  0.21489054 -0.18735693 -0.8090247 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 193 is [True, False, False, True, False, False]
Current timestep = 194. State = [[-0.05668148 -0.24878848]]. Action = [[-0.12162822 -0.00460686  0.11100531  0.909601  ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 194 is [True, False, False, True, False, False]
Current timestep = 195. State = [[-0.0557516  -0.23892507]]. Action = [[0.22732604 0.09004331 0.03600672 0.23698127]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 195 is [True, False, False, True, False, False]
Current timestep = 196. State = [[-0.04602874 -0.22315763]]. Action = [[ 0.16852844  0.0848712  -0.2246754  -0.5964089 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 196 is [True, False, False, True, False, False]
Scene graph at timestep 196 is [False, True, False, True, False, False]
State prediction error at timestep 196 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 196 of 1
Current timestep = 197. State = [[-0.03757314 -0.20550577]]. Action = [[-0.20399804  0.17442176  0.09949493  0.43284035]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 197 is [False, True, False, True, False, False]
Scene graph at timestep 197 is [False, True, False, True, False, False]
State prediction error at timestep 197 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.03528587 -0.18264471]]. Action = [[0.22897053 0.12583995 0.12113395 0.05750775]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 198 is [False, True, False, True, False, False]
Scene graph at timestep 198 is [False, True, False, True, False, False]
State prediction error at timestep 198 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 198 of 1
Current timestep = 199. State = [[-0.03474969 -0.17103179]]. Action = [[-0.19984895  0.0185692   0.20147729  0.45895183]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 199 is [False, True, False, True, False, False]
Scene graph at timestep 199 is [False, True, False, True, False, False]
State prediction error at timestep 199 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of 1
Current timestep = 200. State = [[-0.03521827 -0.15651941]]. Action = [[ 0.12781161  0.23035526 -0.16705444 -0.86643296]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 200 is [False, True, False, True, False, False]
Current timestep = 201. State = [[-0.04038106 -0.13792557]]. Action = [[-0.24457711  0.01650223 -0.18087257 -0.57409847]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 201 is [False, True, False, True, False, False]
Scene graph at timestep 201 is [False, True, False, True, False, False]
State prediction error at timestep 201 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 201 of 1
Current timestep = 202. State = [[-0.04952454 -0.13402897]]. Action = [[ 0.0106039  -0.02037618  0.02537444  0.3371458 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 202 is [False, True, False, True, False, False]
Current timestep = 203. State = [[-0.04471629 -0.13573228]]. Action = [[ 0.23872423 -0.05155136  0.13310343  0.9449049 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 203 is [False, True, False, True, False, False]
Scene graph at timestep 203 is [False, True, False, True, False, False]
State prediction error at timestep 203 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of 1
Current timestep = 204. State = [[-0.04331226 -0.13291979]]. Action = [[-0.23307718  0.10025179 -0.0576632  -0.38935965]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 204 is [False, True, False, True, False, False]
Current timestep = 205. State = [[-0.05702129 -0.14476414]]. Action = [[-0.17849539 -0.2474723  -0.24140331  0.8577595 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 205 is [False, True, False, True, False, False]
Current timestep = 206. State = [[-0.0646155  -0.14806676]]. Action = [[ 0.0244644   0.21550268 -0.13691662 -0.16073322]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 206 is [True, False, False, True, False, False]
Current timestep = 207. State = [[-0.06505294 -0.1489833 ]]. Action = [[ 0.07550573 -0.23038723 -0.04604425 -0.64339775]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 207 is [True, False, False, True, False, False]
Current timestep = 208. State = [[-0.06699392 -0.15592833]]. Action = [[-0.08688554  0.07737836  0.15165079  0.6524334 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 208 is [True, False, False, True, False, False]
Current timestep = 209. State = [[-0.0755625  -0.15593581]]. Action = [[-0.10197929  0.00550193 -0.22798951 -0.1934973 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 209 is [True, False, False, True, False, False]
Current timestep = 210. State = [[-0.07890476 -0.1615513 ]]. Action = [[ 0.11817276 -0.130461   -0.1548972  -0.23088038]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 210 is [True, False, False, True, False, False]
Scene graph at timestep 210 is [True, False, False, True, False, False]
State prediction error at timestep 210 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[-0.07729257 -0.1548502 ]]. Action = [[ 0.01603365  0.23540711 -0.19240019  0.91299725]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 211 is [True, False, False, True, False, False]
Current timestep = 212. State = [[-0.06869303 -0.1529701 ]]. Action = [[ 0.23418748 -0.22920163  0.15583813  0.13231719]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 212 is [True, False, False, True, False, False]
Scene graph at timestep 212 is [True, False, False, True, False, False]
State prediction error at timestep 212 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 212 of 1
Current timestep = 213. State = [[-0.0473376  -0.16164598]]. Action = [[ 0.21893501  0.05690852 -0.01882397  0.81818724]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 213 is [True, False, False, True, False, False]
Scene graph at timestep 213 is [False, True, False, True, False, False]
State prediction error at timestep 213 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 213 of 1
Current timestep = 214. State = [[-0.03070231 -0.16651186]]. Action = [[-0.14718947 -0.09696296  0.08389539  0.6147655 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 214 is [False, True, False, True, False, False]
Current timestep = 215. State = [[-0.03824661 -0.17570053]]. Action = [[-0.173416   -0.060229   -0.16802229 -0.8541592 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 215 is [False, True, False, True, False, False]
Current timestep = 216. State = [[-0.03968771 -0.18132927]]. Action = [[0.24492866 0.00958151 0.12728533 0.53034484]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 216 is [False, True, False, True, False, False]
Scene graph at timestep 216 is [False, True, False, True, False, False]
State prediction error at timestep 216 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 216 of -1
Current timestep = 217. State = [[-0.03846462 -0.19017696]]. Action = [[-0.05936237 -0.15998316 -0.08319366  0.5534873 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 217 is [False, True, False, True, False, False]
Current timestep = 218. State = [[-0.03775145 -0.19780515]]. Action = [[ 0.08543444  0.02787349  0.12435374 -0.16061586]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 218 is [False, True, False, True, False, False]
Current timestep = 219. State = [[-0.03474141 -0.1916441 ]]. Action = [[ 0.03093383  0.11180598  0.10386336 -0.16088998]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 219 is [False, True, False, True, False, False]
Scene graph at timestep 219 is [False, True, False, True, False, False]
State prediction error at timestep 219 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of -1
Current timestep = 220. State = [[-0.031571   -0.17229995]]. Action = [[-0.15419306  0.22572994  0.24124122  0.05427718]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 220 is [False, True, False, True, False, False]
Scene graph at timestep 220 is [False, True, False, True, False, False]
State prediction error at timestep 220 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 220 of 1
Current timestep = 221. State = [[-0.03612036 -0.16811946]]. Action = [[-0.07181244 -0.20707467 -0.15402232  0.05653346]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 221 is [False, True, False, True, False, False]
Current timestep = 222. State = [[-0.03879941 -0.16829322]]. Action = [[-0.02046955  0.18991226  0.12641543  0.4475174 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 222 is [False, True, False, True, False, False]
Scene graph at timestep 222 is [False, True, False, True, False, False]
State prediction error at timestep 222 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 222 of -1
Current timestep = 223. State = [[-0.0410386 -0.147497 ]]. Action = [[-0.07979411  0.18929195 -0.00103047 -0.89268816]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 223 is [False, True, False, True, False, False]
Current timestep = 224. State = [[-0.0484512  -0.14037862]]. Action = [[-0.05498216 -0.11564359  0.15175402 -0.7640148 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 224 is [False, True, False, True, False, False]
Scene graph at timestep 224 is [False, True, False, True, False, False]
State prediction error at timestep 224 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of -1
Current timestep = 225. State = [[-0.052777   -0.13877034]]. Action = [[ 0.16566283  0.11970019 -0.07903546 -0.5536024 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 225 is [False, True, False, True, False, False]
Scene graph at timestep 225 is [True, False, False, True, False, False]
State prediction error at timestep 225 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 225 of 1
Current timestep = 226. State = [[-0.04264845 -0.13688411]]. Action = [[ 0.21990052 -0.14601964  0.23013258 -0.13808852]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 226 is [True, False, False, True, False, False]
Current timestep = 227. State = [[-0.02248592 -0.15196273]]. Action = [[ 0.16761726 -0.16281907  0.01594564  0.7239481 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 227 is [False, True, False, True, False, False]
Current timestep = 228. State = [[-0.00063187 -0.15341495]]. Action = [[0.132168   0.19071758 0.17968822 0.93639374]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 228 is [False, True, False, True, False, False]
Scene graph at timestep 228 is [False, True, False, True, False, False]
State prediction error at timestep 228 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 228 of 1
Current timestep = 229. State = [[ 0.02666236 -0.14898854]]. Action = [[ 0.21225351 -0.10088861 -0.01663701  0.42274058]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 229 is [False, True, False, True, False, False]
Current timestep = 230. State = [[ 0.04716974 -0.14964515]]. Action = [[0.0590108  0.0597983  0.00838417 0.8957639 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 230 is [False, True, False, True, False, False]
Current timestep = 231. State = [[ 0.05745283 -0.14646316]]. Action = [[ 0.2360208   0.0272482   0.05090803 -0.89360476]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 231 is [False, True, False, True, False, False]
Current timestep = 232. State = [[ 0.06044334 -0.13772905]]. Action = [[-0.00582071  0.15491343  0.12250718  0.83300734]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 232 is [False, False, True, True, False, False]
Current timestep = 233. State = [[ 0.0623044  -0.13340096]]. Action = [[-0.02803148 -0.0605807  -0.22238451 -0.87948924]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 233 is [False, False, True, True, False, False]
Scene graph at timestep 233 is [False, False, True, True, False, False]
State prediction error at timestep 233 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 233 of -1
Current timestep = 234. State = [[ 0.06230943 -0.13363855]]. Action = [[ 0.06970254 -0.07623781  0.24561033  0.33460236]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 234 is [False, False, True, True, False, False]
Current timestep = 235. State = [[ 0.06230943 -0.13363855]]. Action = [[0.1654242  0.03424928 0.08766773 0.03324068]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 235 is [False, False, True, True, False, False]
Current timestep = 236. State = [[ 0.06227101 -0.1260814 ]]. Action = [[0.01763818 0.13771698 0.2053566  0.92775273]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 236 is [False, False, True, True, False, False]
Scene graph at timestep 236 is [False, False, True, True, False, False]
State prediction error at timestep 236 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 236 of 1
Current timestep = 237. State = [[ 0.06206653 -0.11831089]]. Action = [[ 0.1143074   0.14202583 -0.2198871  -0.37678295]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 237 is [False, False, True, True, False, False]
Scene graph at timestep 237 is [False, False, True, False, True, False]
State prediction error at timestep 237 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 237 of 1
Current timestep = 238. State = [[ 0.06206653 -0.11831089]]. Action = [[ 0.04891312  0.07790047  0.23774254 -0.16788983]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 238 is [False, False, True, False, True, False]
Scene graph at timestep 238 is [False, False, True, False, True, False]
State prediction error at timestep 238 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 238 of 1
Current timestep = 239. State = [[ 0.06206653 -0.11831089]]. Action = [[ 0.18074784  0.24031761 -0.08904472  0.19561219]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 239 is [False, False, True, False, True, False]
Current timestep = 240. State = [[ 0.06206653 -0.11831089]]. Action = [[ 0.13191932 -0.18914601  0.04309401  0.5235529 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 240 is [False, False, True, False, True, False]
Scene graph at timestep 240 is [False, False, True, False, True, False]
State prediction error at timestep 240 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 240 of 1
Current timestep = 241. State = [[ 0.06206653 -0.11831089]]. Action = [[ 0.05761808 -0.13505252  0.20149568 -0.33851302]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 241 is [False, False, True, False, True, False]
Scene graph at timestep 241 is [False, False, True, False, True, False]
State prediction error at timestep 241 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 241 of 1
Current timestep = 242. State = [[ 0.06206653 -0.11831089]]. Action = [[ 0.14198369 -0.21485193 -0.13915321  0.4236045 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 242 is [False, False, True, False, True, False]
Current timestep = 243. State = [[ 0.06206653 -0.11831089]]. Action = [[0.23789525 0.02980298 0.2003836  0.7562802 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 243 is [False, False, True, False, True, False]
Scene graph at timestep 243 is [False, False, True, False, True, False]
State prediction error at timestep 243 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 243 of 1
Current timestep = 244. State = [[ 0.06206653 -0.11831089]]. Action = [[ 0.07469898 -0.03801881 -0.22015648 -0.04758775]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 244 is [False, False, True, False, True, False]
Scene graph at timestep 244 is [False, False, True, False, True, False]
State prediction error at timestep 244 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of 1
Current timestep = 245. State = [[ 0.06206653 -0.11831089]]. Action = [[0.14853889 0.20949274 0.15089583 0.05229723]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 245 is [False, False, True, False, True, False]
Scene graph at timestep 245 is [False, False, True, False, True, False]
State prediction error at timestep 245 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 245 of 1
Current timestep = 246. State = [[ 0.06206653 -0.11831089]]. Action = [[ 0.24722093  0.05580914 -0.16513515  0.28142047]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 246 is [False, False, True, False, True, False]
Scene graph at timestep 246 is [False, False, True, False, True, False]
State prediction error at timestep 246 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 246 of 1
Current timestep = 247. State = [[ 0.06206005 -0.11815305]]. Action = [[ 0.15996665 -0.08482565  0.20145905  0.68621945]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 247 is [False, False, True, False, True, False]
Scene graph at timestep 247 is [False, False, True, False, True, False]
State prediction error at timestep 247 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 247 of 1
Current timestep = 248. State = [[ 0.0606511  -0.11090855]]. Action = [[-0.19484325  0.11557859 -0.1796375   0.90220857]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 248 is [False, False, True, False, True, False]
Current timestep = 249. State = [[ 0.05854676 -0.09482624]]. Action = [[-0.01184686  0.11403924  0.08581266 -0.96707666]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 249 is [False, False, True, False, True, False]
Current timestep = 250. State = [[ 0.04997901 -0.085149  ]]. Action = [[-0.21343276  0.01267993 -0.1409149  -0.3228693 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 250 is [False, False, True, False, True, False]
Scene graph at timestep 250 is [False, True, False, False, True, False]
State prediction error at timestep 250 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 250 of 1
Current timestep = 251. State = [[-0.20443918 -0.19138424]]. Action = [[-0.1334816  -0.15215078  0.14256024  0.5226805 ]]. Reward = [100.]
Curr episode timestep = 102
Scene graph at timestep 251 is [False, True, False, False, True, False]
Current timestep = 252. State = [[-0.19339114 -0.20603727]]. Action = [[0.07942253 0.12862116 0.22957754 0.8278699 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 252 is [True, False, False, True, False, False]
Scene graph at timestep 252 is [True, False, False, True, False, False]
State prediction error at timestep 252 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of 1
Current timestep = 253. State = [[-0.18479557 -0.18774454]]. Action = [[ 0.02767476  0.20331329 -0.19203436 -0.3948748 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 253 is [True, False, False, True, False, False]
Scene graph at timestep 253 is [True, False, False, True, False, False]
State prediction error at timestep 253 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 253 of 1
Current timestep = 254. State = [[-0.17013437 -0.1725975 ]]. Action = [[ 0.19913784 -0.03932047  0.16244829  0.60123444]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 254 is [True, False, False, True, False, False]
Scene graph at timestep 254 is [True, False, False, True, False, False]
State prediction error at timestep 254 is tensor(8.3855e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 254 of 1
Current timestep = 255. State = [[-0.15127006 -0.16877237]]. Action = [[0.04014176 0.12216786 0.15769821 0.7514348 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 255 is [True, False, False, True, False, False]
Current timestep = 256. State = [[-0.14503983 -0.14639634]]. Action = [[ 0.08146575  0.24418205 -0.19468264 -0.7174606 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 256 is [True, False, False, True, False, False]
Current timestep = 257. State = [[-0.14298864 -0.13583207]]. Action = [[-0.16062514 -0.12175927  0.20685756 -0.8268686 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 257 is [True, False, False, True, False, False]
Scene graph at timestep 257 is [True, False, False, True, False, False]
State prediction error at timestep 257 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of 1
Current timestep = 258. State = [[-0.14056309 -0.13966952]]. Action = [[ 0.21257207  0.01984867 -0.07137567  0.6737853 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 258 is [True, False, False, True, False, False]
Scene graph at timestep 258 is [True, False, False, True, False, False]
State prediction error at timestep 258 is tensor(5.4481e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 258 of 1
Current timestep = 259. State = [[-0.13084017 -0.14203781]]. Action = [[ 0.01377374 -0.10355505 -0.09840374 -0.8020788 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 259 is [True, False, False, True, False, False]
Scene graph at timestep 259 is [True, False, False, True, False, False]
State prediction error at timestep 259 is tensor(6.7179e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 259 of 1
Current timestep = 260. State = [[-0.12135185 -0.13811742]]. Action = [[ 0.10524553  0.18074748 -0.01608768  0.45281482]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 260 is [True, False, False, True, False, False]
Scene graph at timestep 260 is [True, False, False, True, False, False]
State prediction error at timestep 260 is tensor(3.2979e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 260 of 1
Current timestep = 261. State = [[-0.10322405 -0.13707082]]. Action = [[ 0.18146646 -0.18490884  0.20352629  0.07255292]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 261 is [True, False, False, True, False, False]
Current timestep = 262. State = [[-0.07655744 -0.13481312]]. Action = [[ 0.21249583  0.20773715  0.07487416 -0.0527007 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 262 is [True, False, False, True, False, False]
Current timestep = 263. State = [[-0.05568364 -0.1147184 ]]. Action = [[ 0.03670403  0.18106353  0.17997742 -0.96426827]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 263 is [True, False, False, True, False, False]
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 263 of 1
Current timestep = 264. State = [[-0.05286503 -0.09167835]]. Action = [[-0.24165207  0.11358744  0.22366047 -0.45166647]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 264 is [True, False, False, False, True, False]
Scene graph at timestep 264 is [True, False, False, False, True, False]
State prediction error at timestep 264 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 264 of -1
Current timestep = 265. State = [[-0.05874964 -0.06955183]]. Action = [[ 0.13012248  0.2031653  -0.01335317  0.7829006 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 265 is [True, False, False, False, True, False]
Scene graph at timestep 265 is [True, False, False, False, True, False]
State prediction error at timestep 265 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 265 of 1
Current timestep = 266. State = [[-0.0560834  -0.04410826]]. Action = [[ 0.12252766  0.12396914  0.12460876 -0.07729733]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 266 is [True, False, False, False, True, False]
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 266 of 1
Current timestep = 267. State = [[-0.04628128 -0.01994613]]. Action = [[0.04161665 0.20815265 0.24241501 0.8534703 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 267 is [True, False, False, False, True, False]
Current timestep = 268. State = [[-0.25761327 -0.00303771]]. Action = [[ 0.17068207 -0.12382472 -0.00761539 -0.677978  ]]. Reward = [100.]
Curr episode timestep = 16
Scene graph at timestep 268 is [False, True, False, False, True, False]
Current timestep = 269. State = [[-0.25220108  0.00853417]]. Action = [[ 0.11795112  0.23957142 -0.11057626 -0.93113875]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 269 is [True, False, False, False, True, False]
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of 1
Current timestep = 270. State = [[-0.2338624   0.03410789]]. Action = [[ 0.22619385  0.17206675 -0.20402294 -0.8907401 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 270 is [True, False, False, False, True, False]
Current timestep = 271. State = [[-0.20646536  0.05766138]]. Action = [[ 0.1997981   0.1900931  -0.04964592  0.7365999 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 271 is [True, False, False, False, True, False]
Current timestep = 272. State = [[-0.19669338  0.08032203]]. Action = [[-0.16587815  0.10170662  0.21637204  0.825161  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 272 is [True, False, False, False, True, False]
Scene graph at timestep 272 is [True, False, False, False, True, False]
State prediction error at timestep 272 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 272 of 1
Current timestep = 273. State = [[-0.20079915  0.08510523]]. Action = [[ 0.02145869 -0.14413144 -0.18153168  0.9054792 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 273 is [True, False, False, False, True, False]
Scene graph at timestep 273 is [True, False, False, False, True, False]
State prediction error at timestep 273 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 273 of 1
Current timestep = 274. State = [[-0.2016161   0.07359722]]. Action = [[-0.1090076  -0.05695926  0.08853695  0.87157464]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 274 is [True, False, False, False, True, False]
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 274 of 1
Current timestep = 275. State = [[-0.20241266  0.05593674]]. Action = [[ 0.01984388 -0.20166108  0.19609097  0.38513827]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 275 is [True, False, False, False, True, False]
Current timestep = 276. State = [[-0.20252706  0.04257249]]. Action = [[ 0.02285013 -0.00988795  0.06904241 -0.4018854 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 276 is [True, False, False, False, True, False]
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(2.2581e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of 1
Current timestep = 277. State = [[-0.21124417  0.04437037]]. Action = [[-0.20075533  0.08719456  0.20702243 -0.73340905]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 277 is [True, False, False, False, True, False]
Current timestep = 278. State = [[-0.22082298  0.05326571]]. Action = [[ 0.02407402  0.07761139 -0.09621821 -0.9900566 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 278 is [True, False, False, False, True, False]
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 278 of -1
Current timestep = 279. State = [[-0.22614002  0.04629079]]. Action = [[-0.1030893  -0.22180684 -0.02288187 -0.5335425 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 279 is [True, False, False, False, True, False]
Scene graph at timestep 279 is [True, False, False, False, True, False]
State prediction error at timestep 279 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 279 of -1
Current timestep = 280. State = [[-0.23136559  0.01868358]]. Action = [[-0.05670486 -0.19608104 -0.03424983  0.21693361]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 280 is [True, False, False, False, True, False]
Current timestep = 281. State = [[-0.23167986  0.01067136]]. Action = [[0.20012233 0.11485663 0.20271027 0.20936894]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 281 is [True, False, False, False, True, False]
Current timestep = 282. State = [[-0.23035677  0.00633693]]. Action = [[-0.1527657  -0.14124677  0.01134813  0.29291248]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 282 is [True, False, False, False, True, False]
Scene graph at timestep 282 is [True, False, False, False, True, False]
State prediction error at timestep 282 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 282 of -1
Current timestep = 283. State = [[-0.24440198 -0.00747493]]. Action = [[-0.18687001 -0.08716747  0.0655905   0.17461538]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 283 is [True, False, False, False, True, False]
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 283 of -1
Current timestep = 284. State = [[-0.25474355 -0.02967315]]. Action = [[ 0.17669082 -0.23881951  0.02982503  0.30369985]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 284 is [True, False, False, False, True, False]
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 284 of -1
Current timestep = 285. State = [[-0.24545743 -0.06188298]]. Action = [[ 0.03725281 -0.20828605  0.16423887  0.5527239 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 285 is [True, False, False, False, True, False]
Current timestep = 286. State = [[-0.24283521 -0.07762071]]. Action = [[-0.1911075  -0.21833293 -0.15857972  0.72498107]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 286 is [True, False, False, False, True, False]
Current timestep = 287. State = [[-0.23483379 -0.09320921]]. Action = [[ 0.16569889 -0.20768362 -0.16293329 -0.6721822 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 287 is [True, False, False, False, True, False]
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 287 of -1
Current timestep = 288. State = [[-0.22844668 -0.12315594]]. Action = [[-0.12354648 -0.1710866  -0.15436691  0.01434255]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 288 is [True, False, False, False, True, False]
Current timestep = 289. State = [[-0.22744986 -0.1278661 ]]. Action = [[ 0.13540184  0.14554876  0.22718593 -0.9073399 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 289 is [True, False, False, False, True, False]
Current timestep = 290. State = [[-0.22699472 -0.12090886]]. Action = [[-0.13341455  0.04025206 -0.19653197  0.57098675]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 290 is [True, False, False, True, False, False]
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 290 of -1
Current timestep = 291. State = [[-0.22621898 -0.1151361 ]]. Action = [[ 0.11356401  0.04618511  0.17802802 -0.11212349]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 291 is [True, False, False, False, True, False]
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(9.3298e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 291 of 1
Current timestep = 292. State = [[-0.22954923 -0.12007283]]. Action = [[-0.22401214 -0.19305149 -0.14721423 -0.57312554]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 292 is [True, False, False, False, True, False]
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 292 of -1
Current timestep = 293. State = [[-0.24918893 -0.1444709 ]]. Action = [[-0.13220762 -0.13572726 -0.12984185 -0.7304067 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 293 is [True, False, False, False, True, False]
Current timestep = 294. State = [[-0.25923586 -0.15459564]]. Action = [[-0.15944244  0.1523068   0.1417914  -0.20169246]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 294 is [True, False, False, True, False, False]
Current timestep = 295. State = [[-0.252897   -0.14497514]]. Action = [[ 0.23728642  0.1887838  -0.06402829  0.8588016 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 295 is [True, False, False, True, False, False]
Current timestep = 296. State = [[-0.24567293 -0.12589331]]. Action = [[-0.0893181   0.13833603 -0.13223225  0.10880744]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 296 is [True, False, False, True, False, False]
Current timestep = 297. State = [[-0.23643154 -0.12644087]]. Action = [[ 0.22373205 -0.22234985  0.07386568  0.3648939 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 297 is [True, False, False, True, False, False]
Current timestep = 298. State = [[-0.2170236  -0.14742696]]. Action = [[ 0.1940198  -0.18340892 -0.08863673 -0.6623037 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 298 is [True, False, False, True, False, False]
Current timestep = 299. State = [[-0.19188705 -0.17395374]]. Action = [[ 0.15926921 -0.2288737  -0.06010337 -0.6217169 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 299 is [True, False, False, True, False, False]
Current timestep = 300. State = [[-0.1782979  -0.19169146]]. Action = [[-0.01343229  0.02413881 -0.21026622  0.9043865 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 300 is [True, False, False, True, False, False]
Scene graph at timestep 300 is [True, False, False, True, False, False]
State prediction error at timestep 300 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 300 of 1
Current timestep = 301. State = [[-0.1731426  -0.19709133]]. Action = [[ 0.10378903 -0.07949692  0.19239908 -0.9095028 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 301 is [True, False, False, True, False, False]
Current timestep = 302. State = [[-0.16397919 -0.19235197]]. Action = [[ 0.00466838  0.18793476 -0.12800758  0.28877354]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 302 is [True, False, False, True, False, False]
Scene graph at timestep 302 is [True, False, False, True, False, False]
State prediction error at timestep 302 is tensor(1.1153e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 302 of 1
Current timestep = 303. State = [[-0.16252573 -0.1933437 ]]. Action = [[-0.04647465 -0.18132186  0.14769876  0.55173683]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 303 is [True, False, False, True, False, False]
Scene graph at timestep 303 is [True, False, False, True, False, False]
State prediction error at timestep 303 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 303 of -1
Current timestep = 304. State = [[-0.15356323 -0.20515913]]. Action = [[ 0.23731357 -0.03756934  0.17124423  0.70352674]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 304 is [True, False, False, True, False, False]
Current timestep = 305. State = [[-0.13353989 -0.22105503]]. Action = [[ 0.08109018 -0.21568511  0.2451165   0.99592996]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 305 is [True, False, False, True, False, False]
Scene graph at timestep 305 is [True, False, False, True, False, False]
State prediction error at timestep 305 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of 1
Current timestep = 306. State = [[-0.12445407 -0.23792268]]. Action = [[-0.18758215  0.09864691  0.17888999 -0.37782216]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 306 is [True, False, False, True, False, False]
Scene graph at timestep 306 is [True, False, False, True, False, False]
State prediction error at timestep 306 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 306 of -1
Current timestep = 307. State = [[-0.14291619 -0.25101492]]. Action = [[-0.23291108 -0.22673577 -0.16306952 -0.41822863]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 307 is [True, False, False, True, False, False]
Scene graph at timestep 307 is [True, False, False, True, False, False]
State prediction error at timestep 307 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 307 of -1
Current timestep = 308. State = [[-0.15767685 -0.27864185]]. Action = [[ 0.14589024 -0.2054674   0.1568278  -0.5153738 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 308 is [True, False, False, True, False, False]
Current timestep = 309. State = [[-0.15229999 -0.29537195]]. Action = [[ 0.13861823 -0.09991772 -0.12533891 -0.7348056 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 309 is [True, False, False, True, False, False]
Scene graph at timestep 309 is [True, False, False, True, False, False]
State prediction error at timestep 309 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 309 of -1
Current timestep = 310. State = [[-0.1312601  -0.29621398]]. Action = [[0.17193931 0.14463624 0.12491089 0.34050167]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 310 is [True, False, False, True, False, False]
Scene graph at timestep 310 is [True, False, False, True, False, False]
State prediction error at timestep 310 is tensor(2.6799e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 310 of 1
Current timestep = 311. State = [[-0.10626389 -0.27402842]]. Action = [[ 0.20941168  0.17651275 -0.18257101 -0.7523769 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 311 is [True, False, False, True, False, False]
Current timestep = 312. State = [[-0.07921308 -0.2523216 ]]. Action = [[ 0.20986754  0.20040804 -0.23846436 -0.9584358 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 312 is [True, False, False, True, False, False]
Scene graph at timestep 312 is [True, False, False, True, False, False]
State prediction error at timestep 312 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 312 of 1
Current timestep = 313. State = [[-0.05907182 -0.22733966]]. Action = [[-0.17934665  0.16249526 -0.22237793 -0.737129  ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 313 is [True, False, False, True, False, False]
Current timestep = 314. State = [[-0.05965151 -0.20495939]]. Action = [[ 0.04573539  0.2190144  -0.01070507  0.4444151 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 314 is [True, False, False, True, False, False]
Current timestep = 315. State = [[-0.05998427 -0.18360041]]. Action = [[-0.09466454  0.05116257 -0.14166816  0.49158096]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 315 is [True, False, False, True, False, False]
Current timestep = 316. State = [[-0.05985292 -0.16650517]]. Action = [[ 0.14630246  0.16227484 -0.096643   -0.74025923]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 316 is [True, False, False, True, False, False]
Current timestep = 317. State = [[-0.05724355 -0.14873756]]. Action = [[ 0.06555471  0.06243703  0.23637569 -0.82830596]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 317 is [True, False, False, True, False, False]
Current timestep = 318. State = [[-0.05610177 -0.13182278]]. Action = [[-0.12633127  0.16258341 -0.18983018  0.66616285]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 318 is [True, False, False, True, False, False]
Current timestep = 319. State = [[-0.05232205 -0.10512744]]. Action = [[0.19500554 0.21689576 0.22692013 0.33204436]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 319 is [True, False, False, True, False, False]
Scene graph at timestep 319 is [True, False, False, False, True, False]
State prediction error at timestep 319 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 319 of 1
Current timestep = 320. State = [[-0.05051392 -0.07439793]]. Action = [[-0.22435085  0.1801182  -0.01818177  0.06349218]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 320 is [True, False, False, False, True, False]
Current timestep = 321. State = [[-0.0629753 -0.0680173]]. Action = [[-0.1924755  -0.1379281  -0.05067977  0.13109922]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 321 is [True, False, False, False, True, False]
Current timestep = 322. State = [[-0.07155606 -0.0721588 ]]. Action = [[0.2170192  0.04267523 0.10429859 0.87546754]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 322 is [True, False, False, False, True, False]
Current timestep = 323. State = [[-0.06426594 -0.08075752]]. Action = [[ 0.15100086 -0.20560534 -0.23057908 -0.0613535 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 323 is [True, False, False, False, True, False]
Scene graph at timestep 323 is [True, False, False, False, True, False]
State prediction error at timestep 323 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 323 of 1
Current timestep = 324. State = [[-0.05381318 -0.09623233]]. Action = [[-0.08736962 -0.02836505  0.18317086 -0.58724886]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 324 is [True, False, False, False, True, False]
Current timestep = 325. State = [[-0.05066315 -0.09125517]]. Action = [[ 0.15131545  0.12383702 -0.02233382 -0.51124936]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 325 is [True, False, False, False, True, False]
Current timestep = 326. State = [[-0.05022479 -0.07832504]]. Action = [[-0.16130184  0.12536013 -0.11127624 -0.75509083]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 326 is [True, False, False, False, True, False]
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of 1
Current timestep = 327. State = [[-0.04783961 -0.06060527]]. Action = [[0.21787232 0.11830053 0.24478853 0.916131  ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 327 is [True, False, False, False, True, False]
Scene graph at timestep 327 is [False, True, False, False, True, False]
State prediction error at timestep 327 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 327 of 1
Current timestep = 328. State = [[-0.04193108 -0.04528959]]. Action = [[-0.197566    0.08258381 -0.16344675 -0.9585999 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 328 is [False, True, False, False, True, False]
Current timestep = 329. State = [[-0.0408296 -0.0518675]]. Action = [[ 0.2082454  -0.23242052 -0.21750958  0.3577919 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 329 is [False, True, False, False, True, False]
Current timestep = 330. State = [[-0.17982695 -0.1930649 ]]. Action = [[ 0.17079449 -0.04822265 -0.13868728 -0.43028086]]. Reward = [100.]
Curr episode timestep = 61
Scene graph at timestep 330 is [False, True, False, False, True, False]
Scene graph at timestep 330 is [True, False, False, True, False, False]
State prediction error at timestep 330 is tensor(0.0178, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 330 of 1
Current timestep = 331. State = [[-0.17158018 -0.20709166]]. Action = [[-0.23129357  0.2202093  -0.11531463 -0.14725107]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 331 is [True, False, False, True, False, False]
Scene graph at timestep 331 is [True, False, False, True, False, False]
State prediction error at timestep 331 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 331 of -1
Current timestep = 332. State = [[-0.18281075 -0.20451154]]. Action = [[-0.0012648  -0.15410045 -0.09695399  0.7730663 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 332 is [True, False, False, True, False, False]
Scene graph at timestep 332 is [True, False, False, True, False, False]
State prediction error at timestep 332 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 332 of 1
Current timestep = 333. State = [[-0.19120854 -0.2049329 ]]. Action = [[-0.14454196  0.16042435 -0.07352382  0.9508611 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 333 is [True, False, False, True, False, False]
Current timestep = 334. State = [[-0.19111487 -0.19396146]]. Action = [[ 0.24389619  0.03053194 -0.06433144 -0.74088734]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 334 is [True, False, False, True, False, False]
Current timestep = 335. State = [[-0.18625985 -0.19390911]]. Action = [[-0.00569619 -0.10928369 -0.24423143 -0.5132868 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 335 is [True, False, False, True, False, False]
Scene graph at timestep 335 is [True, False, False, True, False, False]
State prediction error at timestep 335 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 335 of 1
Current timestep = 336. State = [[-0.1830217 -0.1860856]]. Action = [[0.02596503 0.2024594  0.10669887 0.3199948 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 336 is [True, False, False, True, False, False]
Scene graph at timestep 336 is [True, False, False, True, False, False]
State prediction error at timestep 336 is tensor(5.7158e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of 1
Current timestep = 337. State = [[-0.18649322 -0.16141292]]. Action = [[-0.19833474  0.23703316  0.22086307  0.15859139]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 337 is [True, False, False, True, False, False]
Scene graph at timestep 337 is [True, False, False, True, False, False]
State prediction error at timestep 337 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 337 of 1
Current timestep = 338. State = [[-0.1956062  -0.15026866]]. Action = [[-0.10069206 -0.12921566 -0.21743526 -0.57262874]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 338 is [True, False, False, True, False, False]
Scene graph at timestep 338 is [True, False, False, True, False, False]
State prediction error at timestep 338 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 338 of -1
Current timestep = 339. State = [[-0.2007245  -0.14849669]]. Action = [[0.12856376 0.20784855 0.12115031 0.15686917]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 339 is [True, False, False, True, False, False]
Scene graph at timestep 339 is [True, False, False, True, False, False]
State prediction error at timestep 339 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 339 of 1
Current timestep = 340. State = [[-0.20156994 -0.11814694]]. Action = [[-0.16410682  0.24046162  0.15818638 -0.5933425 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 340 is [True, False, False, True, False, False]
Scene graph at timestep 340 is [True, False, False, False, True, False]
State prediction error at timestep 340 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 340 of -1
Current timestep = 341. State = [[-0.2208975 -0.0820237]]. Action = [[-0.19991583  0.18398586  0.14369336 -0.95775205]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 341 is [True, False, False, False, True, False]
Current timestep = 342. State = [[-0.22952192 -0.06271823]]. Action = [[0.19349644 0.09417051 0.18887609 0.4551748 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 342 is [True, False, False, False, True, False]
Current timestep = 343. State = [[-0.22176053 -0.04184104]]. Action = [[0.12428159 0.19326365 0.03336421 0.56021035]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 343 is [True, False, False, False, True, False]
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 343 of 1
Current timestep = 344. State = [[-0.2143937  -0.03533365]]. Action = [[-0.07647178 -0.19632168  0.23057133  0.64651036]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 344 is [True, False, False, False, True, False]
Current timestep = 345. State = [[-0.22416112 -0.04265533]]. Action = [[-0.2110424   0.04692537  0.23902655 -0.34706682]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 345 is [True, False, False, False, True, False]
Scene graph at timestep 345 is [True, False, False, False, True, False]
State prediction error at timestep 345 is tensor(7.1822e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 345 of -1
Current timestep = 346. State = [[-0.23736048 -0.03843679]]. Action = [[-0.08647493  0.07756972 -0.13356547  0.4256636 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 346 is [True, False, False, False, True, False]
Current timestep = 347. State = [[-0.24965723 -0.02603341]]. Action = [[-0.08315754  0.10531771 -0.02655381  0.808023  ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 347 is [True, False, False, False, True, False]
Current timestep = 348. State = [[-0.25274256 -0.00743788]]. Action = [[0.20421278 0.1761584  0.19949025 0.83693373]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 348 is [True, False, False, False, True, False]
Current timestep = 349. State = [[-0.24936232  0.00321667]]. Action = [[-0.21822834 -0.1275339   0.0457812   0.59060884]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 349 is [True, False, False, False, True, False]
Current timestep = 350. State = [[-0.2520822   0.02025944]]. Action = [[-0.08621389  0.222161    0.08145446 -0.12696451]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 350 is [True, False, False, False, True, False]
Scene graph at timestep 350 is [True, False, False, False, True, False]
State prediction error at timestep 350 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 350 of -1
Current timestep = 351. State = [[-0.25628826  0.02808026]]. Action = [[ 0.00909847 -0.23073761 -0.12294622 -0.7704972 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 351 is [True, False, False, False, True, False]
Current timestep = 352. State = [[-0.25741962  0.01425494]]. Action = [[-0.07736343 -0.02898499  0.07198298  0.6491451 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 352 is [True, False, False, False, True, False]
Current timestep = 353. State = [[-0.259295    0.00908803]]. Action = [[-0.167875   -0.20755357 -0.19793391  0.62409925]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 353 is [True, False, False, False, True, False]
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of -1
Current timestep = 354. State = [[-0.25952223  0.00808078]]. Action = [[-0.15845019  0.16421819  0.15008569 -0.07557034]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 354 is [True, False, False, False, True, False]
Scene graph at timestep 354 is [True, False, False, False, True, False]
State prediction error at timestep 354 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 354 of -1
Current timestep = 355. State = [[-0.25287133  0.01912791]]. Action = [[ 0.20737734  0.2086052  -0.14657041  0.6652911 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 355 is [True, False, False, False, True, False]
Current timestep = 356. State = [[-0.2333444   0.02416592]]. Action = [[ 0.21918672 -0.10457012  0.13450837 -0.26156795]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 356 is [True, False, False, False, True, False]
Current timestep = 357. State = [[-0.22076969  0.01029238]]. Action = [[-0.19441229 -0.22892924  0.0624218  -0.738943  ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 357 is [True, False, False, False, True, False]
Current timestep = 358. State = [[-0.21707654  0.00087407]]. Action = [[0.22498405 0.09995824 0.09179705 0.14094424]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 358 is [True, False, False, False, True, False]
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(4.5011e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 358 of 1
Current timestep = 359. State = [[-0.20753202 -0.00696712]]. Action = [[ 0.03918344 -0.1737983   0.12465408 -0.4473399 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 359 is [True, False, False, False, True, False]
Current timestep = 360. State = [[-0.19448881 -0.0105455 ]]. Action = [[ 0.20191538  0.1256249  -0.16993473 -0.50653964]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 360 is [True, False, False, False, True, False]
Scene graph at timestep 360 is [True, False, False, False, True, False]
State prediction error at timestep 360 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 360 of 1
Current timestep = 361. State = [[-0.17209831 -0.0069925 ]]. Action = [[ 0.07013267 -0.04754972 -0.05367701  0.8829267 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 361 is [True, False, False, False, True, False]
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 361 of 1
Current timestep = 362. State = [[-0.16029677 -0.01159451]]. Action = [[ 0.13472718 -0.05463351 -0.08939466  0.05321085]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 362 is [True, False, False, False, True, False]
Scene graph at timestep 362 is [True, False, False, False, True, False]
State prediction error at timestep 362 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 362 of 1
Current timestep = 363. State = [[-0.13566417 -0.02322455]]. Action = [[ 0.231134   -0.12833674 -0.054537    0.09777951]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 363 is [True, False, False, False, True, False]
Current timestep = 364. State = [[-0.12149379 -0.03001695]]. Action = [[-0.20542972  0.04680538  0.02377364 -0.28917384]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 364 is [True, False, False, False, True, False]
Scene graph at timestep 364 is [True, False, False, False, True, False]
State prediction error at timestep 364 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 364 of 1
Current timestep = 365. State = [[-0.12058281 -0.04154874]]. Action = [[ 0.1546492  -0.20301089  0.07054383  0.28016376]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 365 is [True, False, False, False, True, False]
Scene graph at timestep 365 is [True, False, False, False, True, False]
State prediction error at timestep 365 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 365 of 1
Current timestep = 366. State = [[-0.10747135 -0.06545081]]. Action = [[ 0.18943205 -0.13981968 -0.24718367  0.8924844 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 366 is [True, False, False, False, True, False]
Current timestep = 367. State = [[-0.09222659 -0.07570569]]. Action = [[ 0.0036642  -0.004923    0.18755752  0.17370117]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 367 is [True, False, False, False, True, False]
Current timestep = 368. State = [[-0.08406271 -0.08587123]]. Action = [[ 0.12310362 -0.12420556  0.12512732  0.238487  ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 368 is [True, False, False, False, True, False]
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of 1
Current timestep = 369. State = [[-0.07440869 -0.10567457]]. Action = [[-0.086741   -0.13665226  0.08012813 -0.08029163]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 369 is [True, False, False, False, True, False]
Current timestep = 370. State = [[-0.06880613 -0.12030049]]. Action = [[ 0.1921488  -0.07769242  0.16041273  0.11701775]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 370 is [True, False, False, False, True, False]
Scene graph at timestep 370 is [True, False, False, False, True, False]
State prediction error at timestep 370 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 370 of 1
Current timestep = 371. State = [[-0.05680343 -0.141266  ]]. Action = [[ 0.02223882 -0.2302915   0.12915254  0.34359193]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 371 is [True, False, False, False, True, False]
Current timestep = 372. State = [[-0.05854508 -0.16421643]]. Action = [[-0.20871964 -0.06007604  0.21402538  0.984797  ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 372 is [True, False, False, True, False, False]
Current timestep = 373. State = [[-0.06532569 -0.17147262]]. Action = [[-0.10428891  0.0248456  -0.1428651  -0.9512281 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 373 is [True, False, False, True, False, False]
Scene graph at timestep 373 is [True, False, False, True, False, False]
State prediction error at timestep 373 is tensor(5.3735e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 373 of -1
Current timestep = 374. State = [[-0.0779286 -0.1691248]]. Action = [[-0.1123679   0.0962171  -0.036763    0.14042723]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 374 is [True, False, False, True, False, False]
Scene graph at timestep 374 is [True, False, False, True, False, False]
State prediction error at timestep 374 is tensor(5.4265e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 374 of -1
Current timestep = 375. State = [[-0.0880899  -0.17398572]]. Action = [[-0.09809032 -0.1654258   0.100761    0.70368993]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 375 is [True, False, False, True, False, False]
Scene graph at timestep 375 is [True, False, False, True, False, False]
State prediction error at timestep 375 is tensor(5.9599e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of -1
Current timestep = 376. State = [[-0.098321   -0.17172724]]. Action = [[ 0.06686437  0.21635139 -0.19037911 -0.02580237]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 376 is [True, False, False, True, False, False]
Scene graph at timestep 376 is [True, False, False, True, False, False]
State prediction error at timestep 376 is tensor(1.5802e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 376 of 1
Current timestep = 377. State = [[-0.09697989 -0.1553898 ]]. Action = [[ 0.07615766 -0.01794766 -0.21199296  0.99190784]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 377 is [True, False, False, True, False, False]
Scene graph at timestep 377 is [True, False, False, True, False, False]
State prediction error at timestep 377 is tensor(5.1181e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of 1
Current timestep = 378. State = [[-0.08666152 -0.14080349]]. Action = [[ 0.20859966  0.2448492  -0.10281222 -0.80550194]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 378 is [True, False, False, True, False, False]
Scene graph at timestep 378 is [True, False, False, True, False, False]
State prediction error at timestep 378 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 378 of 1
Current timestep = 379. State = [[-0.06624796 -0.13151509]]. Action = [[ 0.20010293 -0.2410581   0.11783057  0.7600608 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 379 is [True, False, False, True, False, False]
Scene graph at timestep 379 is [True, False, False, True, False, False]
State prediction error at timestep 379 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 379 of 1
Current timestep = 380. State = [[-0.04475616 -0.16144939]]. Action = [[ 0.11111659 -0.21369258  0.2143125  -0.2628981 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 380 is [True, False, False, True, False, False]
Scene graph at timestep 380 is [False, True, False, True, False, False]
State prediction error at timestep 380 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 380 of -1
Current timestep = 381. State = [[-0.02778465 -0.1694817 ]]. Action = [[0.09369865 0.16505992 0.1896433  0.28803766]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 381 is [False, True, False, True, False, False]
Current timestep = 382. State = [[-0.01533027 -0.17147526]]. Action = [[ 0.1630635  -0.17964225  0.22439003  0.90215945]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 382 is [False, True, False, True, False, False]
Scene graph at timestep 382 is [False, True, False, True, False, False]
State prediction error at timestep 382 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 382 of -1
Current timestep = 383. State = [[ 0.00224137 -0.19129026]]. Action = [[-0.19559926 -0.10846955 -0.07997082  0.6421678 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 383 is [False, True, False, True, False, False]
Current timestep = 384. State = [[-0.00629269 -0.20916101]]. Action = [[-0.0961718  -0.13837795  0.05975419 -0.69528174]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 384 is [False, True, False, True, False, False]
Scene graph at timestep 384 is [False, True, False, True, False, False]
State prediction error at timestep 384 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 384 of -1
Current timestep = 385. State = [[-0.01032461 -0.22939828]]. Action = [[ 0.16115373 -0.13408038  0.22888184  0.7326306 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 385 is [False, True, False, True, False, False]
Current timestep = 386. State = [[-0.01055973 -0.23445527]]. Action = [[-0.05481578  0.06319979  0.21693462  0.9466491 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 386 is [False, True, False, True, False, False]
Scene graph at timestep 386 is [False, True, False, True, False, False]
State prediction error at timestep 386 is tensor(1.9695e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.00389834 -0.24124414]]. Action = [[ 0.23559058 -0.1583602   0.13364297  0.9013587 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 387 is [False, True, False, True, False, False]
Current timestep = 388. State = [[ 0.0167137  -0.24872856]]. Action = [[ 0.10170937  0.01205137 -0.05315052  0.09194338]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 388 is [False, True, False, True, False, False]
Current timestep = 389. State = [[ 0.02900875 -0.23783128]]. Action = [[-0.03166145  0.23181051  0.09086576  0.7709186 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 389 is [False, True, False, True, False, False]
Current timestep = 390. State = [[ 0.03745222 -0.21205257]]. Action = [[ 1.2824941e-01  2.2989497e-01 -7.7323616e-04  9.2344904e-01]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 390 is [False, True, False, True, False, False]
Scene graph at timestep 390 is [False, True, False, True, False, False]
State prediction error at timestep 390 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 390 of 1
Current timestep = 391. State = [[ 0.04597456 -0.18135493]]. Action = [[-0.17577322  0.1462931  -0.22619188 -0.63485605]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 391 is [False, True, False, True, False, False]
Current timestep = 392. State = [[ 0.04486624 -0.17175329]]. Action = [[ 0.12503228  0.16434973 -0.21531576 -0.42716956]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 392 is [False, True, False, True, False, False]
Current timestep = 393. State = [[ 0.03856788 -0.1613798 ]]. Action = [[-0.19746728  0.1528101  -0.16610953  0.20946252]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 393 is [False, True, False, True, False, False]
Scene graph at timestep 393 is [False, True, False, True, False, False]
State prediction error at timestep 393 is tensor(2.1881e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 393 of 1
Current timestep = 394. State = [[ 0.02720224 -0.14937788]]. Action = [[ 0.05741149 -0.0543294   0.02809966  0.8925822 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 394 is [False, True, False, True, False, False]
Current timestep = 395. State = [[ 0.03103823 -0.15779158]]. Action = [[ 0.20308167 -0.12988997 -0.10768926  0.6298677 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 395 is [False, True, False, True, False, False]
Current timestep = 396. State = [[ 0.0328568  -0.16096742]]. Action = [[-0.01994348  0.09263632  0.18822306  0.03022444]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 396 is [False, True, False, True, False, False]
Scene graph at timestep 396 is [False, True, False, True, False, False]
State prediction error at timestep 396 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 396 of 0
Current timestep = 397. State = [[ 0.03295593 -0.15856318]]. Action = [[ 0.21814597 -0.21630993  0.15695125  0.75927997]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 397 is [False, True, False, True, False, False]
Scene graph at timestep 397 is [False, True, False, True, False, False]
State prediction error at timestep 397 is tensor(7.4540e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 397 of 0
Current timestep = 398. State = [[ 0.0367568  -0.14464404]]. Action = [[0.05972931 0.23279071 0.157832   0.78027225]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 398 is [False, True, False, True, False, False]
Scene graph at timestep 398 is [False, True, False, True, False, False]
State prediction error at timestep 398 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 398 of 1
Current timestep = 399. State = [[ 0.03785613 -0.1358381 ]]. Action = [[-0.07173386 -0.18085003 -0.2037328  -0.50653636]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 399 is [False, True, False, True, False, False]
Current timestep = 400. State = [[ 0.03759506 -0.13162096]]. Action = [[-0.03312267  0.23362476 -0.16425823  0.850014  ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 400 is [False, True, False, True, False, False]
Current timestep = 401. State = [[ 0.04096953 -0.10698115]]. Action = [[ 0.1425103   0.21116576 -0.18012226  0.01968086]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 401 is [False, True, False, True, False, False]
Current timestep = 402. State = [[ 0.04193584 -0.08980555]]. Action = [[ 0.1487608  -0.00419509 -0.05731255 -0.32768834]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 402 is [False, True, False, False, True, False]
Current timestep = 403. State = [[ 0.04201913 -0.08738966]]. Action = [[ 0.24429783 -0.14187641 -0.23059079 -0.38927174]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 403 is [False, True, False, False, True, False]
Scene graph at timestep 403 is [False, True, False, False, True, False]
State prediction error at timestep 403 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 403 of 1
Current timestep = 404. State = [[ 0.04228754 -0.08969141]]. Action = [[-0.02313723 -0.07697231  0.18620333  0.29889703]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 404 is [False, True, False, False, True, False]
Scene graph at timestep 404 is [False, True, False, False, True, False]
State prediction error at timestep 404 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 404 of 1
Current timestep = 405. State = [[ 0.04149635 -0.08086891]]. Action = [[-0.08950439  0.20387411  0.01851583 -0.78245264]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 405 is [False, True, False, False, True, False]
Scene graph at timestep 405 is [False, True, False, False, True, False]
State prediction error at timestep 405 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of 1
Current timestep = 406. State = [[ 0.04007754 -0.06743938]]. Action = [[ 0.20904917  0.09237236 -0.13704586 -0.23799974]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 406 is [False, True, False, False, True, False]
Scene graph at timestep 406 is [False, True, False, False, True, False]
State prediction error at timestep 406 is tensor(9.8625e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 406 of 1
Current timestep = 407. State = [[ 0.03942538 -0.06016111]]. Action = [[ 0.04187873  0.10606754 -0.14706327 -0.6698841 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 407 is [False, True, False, False, True, False]
Current timestep = 408. State = [[ 0.03971548 -0.04296104]]. Action = [[ 0.07567304  0.16131276 -0.1741283   0.01264894]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 408 is [False, True, False, False, True, False]
Current timestep = 409. State = [[-0.18167077  0.1096542 ]]. Action = [[-0.14522909  0.12052017 -0.07500967  0.7904942 ]]. Reward = [100.]
Curr episode timestep = 78
Scene graph at timestep 409 is [False, True, False, False, True, False]
Current timestep = 410. State = [[-0.16108263  0.12209494]]. Action = [[ 0.11595565 -0.04587011  0.22350097  0.12205875]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 410 is [True, False, False, False, True, False]
Scene graph at timestep 410 is [True, False, False, False, True, False]
State prediction error at timestep 410 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 410 of 1
Current timestep = 411. State = [[-0.15704902  0.12924497]]. Action = [[-0.20289743  0.12309104  0.0677475  -0.7819404 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 411 is [True, False, False, False, True, False]
Current timestep = 412. State = [[-0.17058004  0.128301  ]]. Action = [[-0.17048931 -0.17926021  0.12716138  0.7308209 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 412 is [True, False, False, False, False, True]
Scene graph at timestep 412 is [True, False, False, False, False, True]
State prediction error at timestep 412 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 412 of -1
Current timestep = 413. State = [[-0.1848184   0.12581919]]. Action = [[ 0.05580211  0.16725582 -0.01295272  0.82075214]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 413 is [True, False, False, False, False, True]
Scene graph at timestep 413 is [True, False, False, False, False, True]
State prediction error at timestep 413 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 413 of -1
Current timestep = 414. State = [[-0.18775709  0.12628752]]. Action = [[-0.08050065 -0.20615657  0.23321915 -0.05021995]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 414 is [True, False, False, False, False, True]
Current timestep = 415. State = [[-0.18478167  0.11899432]]. Action = [[ 0.14782608  0.09392679 -0.04517971 -0.9755456 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 415 is [True, False, False, False, False, True]
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of 1
Current timestep = 416. State = [[-0.17388187  0.12054936]]. Action = [[ 0.19358957 -0.0161811  -0.00450836 -0.40545166]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 416 is [True, False, False, False, True, False]
Scene graph at timestep 416 is [True, False, False, False, True, False]
State prediction error at timestep 416 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 416 of 1
Current timestep = 417. State = [[-0.1613838   0.11827261]]. Action = [[-0.21397363 -0.05894338 -0.10262218  0.21147728]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 417 is [True, False, False, False, True, False]
Current timestep = 418. State = [[-0.17018737  0.10279746]]. Action = [[-1.2912315e-01 -2.3112035e-01  2.9492378e-04  4.7592437e-01]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 418 is [True, False, False, False, True, False]
Scene graph at timestep 418 is [True, False, False, False, True, False]
State prediction error at timestep 418 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 418 of -1
Current timestep = 419. State = [[-0.17699216  0.07034454]]. Action = [[ 0.23451239 -0.17545699  0.10030225  0.64945567]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 419 is [True, False, False, False, True, False]
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 419 of 1
Current timestep = 420. State = [[-0.15853903  0.04086631]]. Action = [[ 0.23377556 -0.22532049 -0.02839571  0.50200105]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 420 is [True, False, False, False, True, False]
Current timestep = 421. State = [[-0.14666785  0.03108378]]. Action = [[-0.13236894  0.11554518 -0.12218881  0.6796496 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 421 is [True, False, False, False, True, False]
Current timestep = 422. State = [[-0.14514156  0.03443747]]. Action = [[ 0.12810427  0.01511529 -0.17709751 -0.23743773]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 422 is [True, False, False, False, True, False]
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(6.2506e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 422 of 1
Current timestep = 423. State = [[-0.13127805  0.04037996]]. Action = [[ 0.2022118   0.08624601 -0.07812089  0.7990539 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 423 is [True, False, False, False, True, False]
Scene graph at timestep 423 is [True, False, False, False, True, False]
State prediction error at timestep 423 is tensor(7.2940e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 423 of 1
Current timestep = 424. State = [[-0.11263136  0.04614959]]. Action = [[ 0.03408009 -0.0077222  -0.01053394  0.47683263]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 424 is [True, False, False, False, True, False]
Scene graph at timestep 424 is [True, False, False, False, True, False]
State prediction error at timestep 424 is tensor(5.2087e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 424 of 1
Current timestep = 425. State = [[-0.1073977   0.04838446]]. Action = [[0.06577194 0.04869571 0.10822141 0.33705592]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 425 is [True, False, False, False, True, False]
Scene graph at timestep 425 is [True, False, False, False, True, False]
State prediction error at timestep 425 is tensor(1.2384e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 425 of 1
Current timestep = 426. State = [[-0.09921847  0.06118911]]. Action = [[-0.12288252  0.10279334 -0.1328039  -0.00262678]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 426 is [True, False, False, False, True, False]
Scene graph at timestep 426 is [True, False, False, False, True, False]
State prediction error at timestep 426 is tensor(2.8122e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 426 of -1
Current timestep = 427. State = [[-0.10671949  0.08416077]]. Action = [[-0.07209241  0.23683184 -0.1523603   0.74144244]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 427 is [True, False, False, False, True, False]
Current timestep = 428. State = [[-0.11482852  0.11486586]]. Action = [[ 0.05855837  0.23528725 -0.19627474  0.3897525 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 428 is [True, False, False, False, True, False]
Current timestep = 429. State = [[-0.11054388  0.12753335]]. Action = [[ 0.12345335 -0.07846554 -0.14073081  0.90637255]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 429 is [True, False, False, False, True, False]
Scene graph at timestep 429 is [True, False, False, False, False, True]
State prediction error at timestep 429 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 429 of 1
Current timestep = 430. State = [[-0.09875097  0.1327822 ]]. Action = [[ 0.08784497  0.10230631 -0.14044178 -0.07945317]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 430 is [True, False, False, False, False, True]
Current timestep = 431. State = [[-0.08643471  0.141717  ]]. Action = [[0.03459579 0.05317277 0.20408595 0.94227004]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 431 is [True, False, False, False, False, True]
Scene graph at timestep 431 is [True, False, False, False, False, True]
State prediction error at timestep 431 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 431 of -1
Current timestep = 432. State = [[-0.07344848  0.13561073]]. Action = [[ 0.16728777 -0.20170267  0.17287898  0.90346384]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 432 is [True, False, False, False, False, True]
Scene graph at timestep 432 is [True, False, False, False, False, True]
State prediction error at timestep 432 is tensor(8.4876e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 432 of 1
Current timestep = 433. State = [[-0.06166535  0.12920955]]. Action = [[-0.07466373  0.10487598 -0.21484913 -0.5680245 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 433 is [True, False, False, False, False, True]
Current timestep = 434. State = [[-0.06870638  0.14556174]]. Action = [[-0.13605072  0.18234593  0.21676213  0.45945334]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 434 is [True, False, False, False, False, True]
Scene graph at timestep 434 is [True, False, False, False, False, True]
State prediction error at timestep 434 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 434 of -1
Current timestep = 435. State = [[-0.07310831  0.16186619]]. Action = [[ 0.17030981 -0.01511319  0.04140696 -0.59449613]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 435 is [True, False, False, False, False, True]
Current timestep = 436. State = [[-0.07427326  0.17189051]]. Action = [[-0.2424969   0.13387448  0.0361467  -0.4870752 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 436 is [True, False, False, False, False, True]
Current timestep = 437. State = [[-0.07358406  0.16668037]]. Action = [[ 0.24008387 -0.22472145 -0.18615325 -0.02422607]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 437 is [True, False, False, False, False, True]
Scene graph at timestep 437 is [True, False, False, False, False, True]
State prediction error at timestep 437 is tensor(4.3495e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 437 of 0
Current timestep = 438. State = [[-0.06280689  0.13848436]]. Action = [[-0.03044802 -0.20232414 -0.1643162  -0.01676297]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 438 is [True, False, False, False, False, True]
Current timestep = 439. State = [[-0.05535147  0.1353146 ]]. Action = [[ 0.19884798  0.18744871 -0.14781547 -0.95797634]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 439 is [True, False, False, False, False, True]
Scene graph at timestep 439 is [True, False, False, False, False, True]
State prediction error at timestep 439 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 439 of 1
Current timestep = 440. State = [[-0.03335045  0.15773559]]. Action = [[0.12740678 0.2274741  0.14166912 0.75416493]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 440 is [True, False, False, False, False, True]
Current timestep = 441. State = [[-0.01063066  0.18085094]]. Action = [[ 0.23757845  0.1365292   0.21393913 -0.5799898 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 441 is [False, True, False, False, False, True]
Current timestep = 442. State = [[0.01522177 0.1881819 ]]. Action = [[ 0.04210424 -0.11487147 -0.05148789  0.09479988]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 442 is [False, True, False, False, False, True]
Scene graph at timestep 442 is [False, True, False, False, False, True]
State prediction error at timestep 442 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 442 of -1
Current timestep = 443. State = [[0.02435817 0.18857446]]. Action = [[-0.0263031   0.04465479  0.01091263  0.48213553]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 443 is [False, True, False, False, False, True]
Current timestep = 444. State = [[0.03080885 0.17879736]]. Action = [[ 0.19143724 -0.17772146  0.18458733  0.808866  ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 444 is [False, True, False, False, False, True]
Current timestep = 445. State = [[0.0537649  0.15413041]]. Action = [[ 0.16367543 -0.23657455 -0.14532392  0.79861593]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 445 is [False, True, False, False, False, True]
Current timestep = 446. State = [[0.07156342 0.13679971]]. Action = [[ 0.1993143  -0.20911556  0.17258373 -0.4082744 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 446 is [False, False, True, False, False, True]
Scene graph at timestep 446 is [False, False, True, False, False, True]
State prediction error at timestep 446 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 446 of -1
Current timestep = 447. State = [[0.07798517 0.12053895]]. Action = [[-0.07287633 -0.22216077 -0.22262701  0.88600636]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 447 is [False, False, True, False, False, True]
Current timestep = 448. State = [[0.07669402 0.11092016]]. Action = [[-0.20983873  0.03516823  0.09110126  0.60221815]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 448 is [False, False, True, False, True, False]
Current timestep = 449. State = [[0.07436296 0.10995357]]. Action = [[ 0.0314225   0.17012334  0.12288254 -0.01627588]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 449 is [False, False, True, False, True, False]
Current timestep = 450. State = [[0.07386007 0.11025488]]. Action = [[0.17922288 0.16712335 0.1050477  0.57165504]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 450 is [False, False, True, False, True, False]
Current timestep = 451. State = [[0.06689278 0.10593389]]. Action = [[-0.20886596 -0.08055344  0.1695877   0.5565789 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 451 is [False, False, True, False, True, False]
Scene graph at timestep 451 is [False, False, True, False, True, False]
State prediction error at timestep 451 is tensor(3.3769e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 451 of 1
Current timestep = 452. State = [[0.04633918 0.0950953 ]]. Action = [[ 0.11878064 -0.06370898 -0.24142069  0.1202687 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 452 is [False, False, True, False, True, False]
Scene graph at timestep 452 is [False, True, False, False, True, False]
State prediction error at timestep 452 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 452 of -1
Current timestep = 453. State = [[0.04516341 0.101735  ]]. Action = [[-0.11231604  0.16721559  0.15518409 -0.15731293]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 453 is [False, True, False, False, True, False]
Scene graph at timestep 453 is [False, True, False, False, True, False]
State prediction error at timestep 453 is tensor(6.1637e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 453 of -1
Current timestep = 454. State = [[0.04199889 0.10963787]]. Action = [[ 0.02053574 -0.07564132  0.08092436  0.24764931]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 454 is [False, True, False, False, True, False]
Current timestep = 455. State = [[0.04211691 0.10755578]]. Action = [[ 0.23604429 -0.1704517  -0.17929837 -0.19254231]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 455 is [False, True, False, False, True, False]
Current timestep = 456. State = [[0.04473319 0.10269693]]. Action = [[ 0.13906491 -0.03931874 -0.04894951  0.84182715]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 456 is [False, True, False, False, True, False]
Current timestep = 457. State = [[0.04618066 0.09844767]]. Action = [[ 0.19449788 -0.15643668  0.22079474 -0.089068  ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 457 is [False, True, False, False, True, False]
Scene graph at timestep 457 is [False, True, False, False, True, False]
State prediction error at timestep 457 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 457 of 1
Current timestep = 458. State = [[0.04646354 0.09767077]]. Action = [[0.1893807  0.179807   0.20395875 0.04606426]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 458 is [False, True, False, False, True, False]
Current timestep = 459. State = [[0.04651241 0.091011  ]]. Action = [[-0.12818256 -0.1175656  -0.06077281  0.8120966 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 459 is [False, True, False, False, True, False]
Current timestep = 460. State = [[0.04060094 0.07010074]]. Action = [[-0.16728537 -0.23435594 -0.22697133 -0.22474682]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 460 is [False, True, False, False, True, False]
Scene graph at timestep 460 is [False, True, False, False, True, False]
State prediction error at timestep 460 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of 1
Current timestep = 461. State = [[-0.22081794 -0.18948792]]. Action = [[ 0.08362177 -0.0087826  -0.00346816 -0.7786542 ]]. Reward = [100.]
Curr episode timestep = 51
Scene graph at timestep 461 is [False, True, False, False, True, False]
Current timestep = 462. State = [[-0.2017229  -0.19762014]]. Action = [[ 0.2464686   0.2034254  -0.11041348 -0.48102242]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 462 is [True, False, False, True, False, False]
Current timestep = 463. State = [[-0.17562218 -0.19260032]]. Action = [[ 0.19149101 -0.06364977  0.06058273 -0.29883206]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 463 is [True, False, False, True, False, False]
Scene graph at timestep 463 is [True, False, False, True, False, False]
State prediction error at timestep 463 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 463 of 1
Current timestep = 464. State = [[-0.15346698 -0.19479193]]. Action = [[ 0.04022878 -0.00505808 -0.11502308 -0.81101036]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 464 is [True, False, False, True, False, False]
Current timestep = 465. State = [[-0.15117885 -0.1817867 ]]. Action = [[-0.09976038  0.23276976  0.03022337 -0.552135  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 465 is [True, False, False, True, False, False]
Scene graph at timestep 465 is [True, False, False, True, False, False]
State prediction error at timestep 465 is tensor(4.6795e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of 1
Current timestep = 466. State = [[-0.15658233 -0.17615956]]. Action = [[-0.1448763  -0.17037572 -0.04221117  0.7822881 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 466 is [True, False, False, True, False, False]
Scene graph at timestep 466 is [True, False, False, True, False, False]
State prediction error at timestep 466 is tensor(8.1788e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 466 of -1
Current timestep = 467. State = [[-0.16258873 -0.18772094]]. Action = [[ 0.14878786 -0.03633733  0.09583396  0.8862202 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 467 is [True, False, False, True, False, False]
Current timestep = 468. State = [[-0.16432592 -0.18998368]]. Action = [[-0.16915086 -0.00639293 -0.09375605  0.53940296]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 468 is [True, False, False, True, False, False]
Current timestep = 469. State = [[-0.16271266 -0.20048028]]. Action = [[ 0.19197655 -0.156999    0.16893393  0.02787542]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 469 is [True, False, False, True, False, False]
Current timestep = 470. State = [[-0.148276   -0.21126437]]. Action = [[ 0.18470067 -0.04073583  0.20106542 -0.7684339 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 470 is [True, False, False, True, False, False]
Current timestep = 471. State = [[-0.13161498 -0.21278687]]. Action = [[ 0.00054768  0.09138185  0.11717474 -0.05203921]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 471 is [True, False, False, True, False, False]
Scene graph at timestep 471 is [True, False, False, True, False, False]
State prediction error at timestep 471 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 471 of 1
Current timestep = 472. State = [[-0.1289985  -0.21616584]]. Action = [[ 0.00241977 -0.15891188 -0.06689563  0.5369842 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 472 is [True, False, False, True, False, False]
Scene graph at timestep 472 is [True, False, False, True, False, False]
State prediction error at timestep 472 is tensor(5.3367e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 472 of -1
Current timestep = 473. State = [[-0.11538386 -0.23182346]]. Action = [[ 0.24427307 -0.10342765 -0.16644321  0.6595763 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 473 is [True, False, False, True, False, False]
Scene graph at timestep 473 is [True, False, False, True, False, False]
State prediction error at timestep 473 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 473 of -1
Current timestep = 474. State = [[-0.07995646 -0.2528689 ]]. Action = [[ 0.22826356 -0.18651621 -0.24110135  0.76769423]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 474 is [True, False, False, True, False, False]
Scene graph at timestep 474 is [True, False, False, True, False, False]
State prediction error at timestep 474 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 474 of -1
Current timestep = 475. State = [[-0.04840011 -0.2579319 ]]. Action = [[ 0.21899539  0.1900146   0.14848334 -0.22863537]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 475 is [True, False, False, True, False, False]
Current timestep = 476. State = [[-0.02103616 -0.24457161]]. Action = [[ 0.22825745  0.06865308 -0.12920642  0.31621146]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 476 is [False, True, False, True, False, False]
Current timestep = 477. State = [[ 0.00522293 -0.23539315]]. Action = [[ 0.04806358  0.0630331   0.07657504 -0.92590594]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 477 is [False, True, False, True, False, False]
Current timestep = 478. State = [[ 0.02312907 -0.21858627]]. Action = [[0.20588222 0.18375346 0.0573197  0.99574745]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 478 is [False, True, False, True, False, False]
Current timestep = 479. State = [[ 0.04122572 -0.21873954]]. Action = [[ 0.00263685 -0.2291245  -0.04754016  0.6804464 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 479 is [False, True, False, True, False, False]
Scene graph at timestep 479 is [False, True, False, True, False, False]
State prediction error at timestep 479 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 479 of -1
Current timestep = 480. State = [[ 0.05017215 -0.22107755]]. Action = [[-0.06087936  0.18141043 -0.22128454 -0.51814663]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 480 is [False, True, False, True, False, False]
Scene graph at timestep 480 is [False, False, True, True, False, False]
State prediction error at timestep 480 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 480 of 1
Current timestep = 481. State = [[ 0.05052933 -0.21230999]]. Action = [[ 0.23543209 -0.03254601 -0.13059714 -0.8253157 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 481 is [False, False, True, True, False, False]
Scene graph at timestep 481 is [False, False, True, True, False, False]
State prediction error at timestep 481 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 481 of 1
Current timestep = 482. State = [[ 0.05026899 -0.20874232]]. Action = [[-0.10016327  0.0672816   0.2109626  -0.6413816 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 482 is [False, False, True, True, False, False]
Current timestep = 483. State = [[ 0.04321517 -0.1937238 ]]. Action = [[-0.21351218  0.21612105  0.1023314  -0.70807505]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 483 is [False, False, True, True, False, False]
Scene graph at timestep 483 is [False, True, False, True, False, False]
State prediction error at timestep 483 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 483 of 1
Current timestep = 484. State = [[ 0.02880408 -0.17846355]]. Action = [[ 0.21227854 -0.14540981  0.03398162 -0.24543476]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 484 is [False, True, False, True, False, False]
Current timestep = 485. State = [[ 0.02998148 -0.17809047]]. Action = [[-0.10363308  0.11774108 -0.12894508 -0.28361464]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 485 is [False, True, False, True, False, False]
Scene graph at timestep 485 is [False, True, False, True, False, False]
State prediction error at timestep 485 is tensor(6.2474e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 485 of 0
Current timestep = 486. State = [[ 0.03022088 -0.1752447 ]]. Action = [[0.23048025 0.12735575 0.03644967 0.8674607 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 486 is [False, True, False, True, False, False]
Current timestep = 487. State = [[ 0.03411246 -0.16031587]]. Action = [[ 0.15110493  0.22564727 -0.19343194 -0.45892978]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 487 is [False, True, False, True, False, False]
Scene graph at timestep 487 is [False, True, False, True, False, False]
State prediction error at timestep 487 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 487 of 1
Current timestep = 488. State = [[ 0.03709664 -0.1405119 ]]. Action = [[ 0.22389007 -0.0408401  -0.09672299  0.31974268]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 488 is [False, True, False, True, False, False]
Scene graph at timestep 488 is [False, True, False, True, False, False]
State prediction error at timestep 488 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 488 of 1
Current timestep = 489. State = [[ 0.03984048 -0.1507315 ]]. Action = [[ 0.08677879 -0.19730367  0.13686928 -0.9843849 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 489 is [False, True, False, True, False, False]
Scene graph at timestep 489 is [False, True, False, True, False, False]
State prediction error at timestep 489 is tensor(2.1427e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 489 of -1
Current timestep = 490. State = [[ 0.04750972 -0.16958213]]. Action = [[ 0.03842962 -0.11295623  0.14741784  0.42648983]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 490 is [False, True, False, True, False, False]
Current timestep = 491. State = [[ 0.04821942 -0.17026396]]. Action = [[-0.08740601  0.13632548 -0.11182119 -0.08781469]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 491 is [False, True, False, True, False, False]
Current timestep = 492. State = [[ 0.0483719  -0.16655205]]. Action = [[ 0.19598937  0.01415122  0.06393811 -0.6614749 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 492 is [False, True, False, True, False, False]
Scene graph at timestep 492 is [False, True, False, True, False, False]
State prediction error at timestep 492 is tensor(4.1514e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 492 of -1
Current timestep = 493. State = [[ 0.04838544 -0.1659297 ]]. Action = [[ 0.19900984 -0.05451107  0.22978982 -0.32039994]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 493 is [False, True, False, True, False, False]
Current timestep = 494. State = [[ 0.04565073 -0.17968285]]. Action = [[-0.10013732 -0.21852405  0.0681617   0.16541064]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 494 is [False, True, False, True, False, False]
Scene graph at timestep 494 is [False, True, False, True, False, False]
State prediction error at timestep 494 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 494 of -1
Current timestep = 495. State = [[ 0.04251065 -0.19316982]]. Action = [[ 0.19888425 -0.17953986  0.18744564  0.04534566]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 495 is [False, True, False, True, False, False]
Scene graph at timestep 495 is [False, True, False, True, False, False]
State prediction error at timestep 495 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 495 of -1
Current timestep = 496. State = [[ 0.03860035 -0.18934721]]. Action = [[-0.15539534  0.10548931 -0.23619662  0.4750309 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 496 is [False, True, False, True, False, False]
Scene graph at timestep 496 is [False, True, False, True, False, False]
State prediction error at timestep 496 is tensor(1.5344e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 496 of -1
Current timestep = 497. State = [[ 0.02468826 -0.17355892]]. Action = [[-0.20228608  0.22691864 -0.04007623  0.07422423]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 497 is [False, True, False, True, False, False]
Scene graph at timestep 497 is [False, True, False, True, False, False]
State prediction error at timestep 497 is tensor(7.6883e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of 1
Current timestep = 498. State = [[ 0.00396194 -0.15575968]]. Action = [[ 0.0817346  -0.09800324  0.18606317  0.50008345]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 498 is [False, True, False, True, False, False]
Current timestep = 499. State = [[ 0.00915955 -0.16804038]]. Action = [[ 0.23438668 -0.17406504  0.10215411 -0.41452014]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 499 is [False, True, False, True, False, False]
Current timestep = 500. State = [[ 0.01499414 -0.16933447]]. Action = [[-0.20851003  0.22206277 -0.17346783 -0.7009575 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 500 is [False, True, False, True, False, False]
Scene graph at timestep 500 is [False, True, False, True, False, False]
State prediction error at timestep 500 is tensor(6.5224e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 500 of 1
Current timestep = 501. State = [[ 0.00637009 -0.16807872]]. Action = [[-0.18136215 -0.15184246  0.18074757  0.5996058 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 501 is [False, True, False, True, False, False]
Scene graph at timestep 501 is [False, True, False, True, False, False]
State prediction error at timestep 501 is tensor(4.4422e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[-0.0166664  -0.18347618]]. Action = [[-0.2071258  -0.05126762  0.04885033 -0.57448626]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 502 is [False, True, False, True, False, False]
Scene graph at timestep 502 is [False, True, False, True, False, False]
State prediction error at timestep 502 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 502 of -1
Current timestep = 503. State = [[-0.03698877 -0.17553124]]. Action = [[0.01877901 0.2051729  0.02059111 0.91010904]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 503 is [False, True, False, True, False, False]
Current timestep = 504. State = [[-0.03549046 -0.14728117]]. Action = [[ 0.03800499  0.23663592 -0.09286766  0.26084673]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 504 is [False, True, False, True, False, False]
Current timestep = 505. State = [[-0.04067389 -0.14029083]]. Action = [[-0.13880104 -0.21741456  0.18867719 -0.9311992 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 505 is [False, True, False, True, False, False]
Current timestep = 506. State = [[-0.04427464 -0.13733323]]. Action = [[ 0.13903683  0.23735815 -0.14342679 -0.2923174 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 506 is [False, True, False, True, False, False]
Current timestep = 507. State = [[-0.04269649 -0.13238266]]. Action = [[-0.05079027 -0.18548214  0.0519211  -0.24428749]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 507 is [False, True, False, True, False, False]
Scene graph at timestep 507 is [False, True, False, True, False, False]
State prediction error at timestep 507 is tensor(9.2414e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 507 of 1
Current timestep = 508. State = [[-0.04274586 -0.1351466 ]]. Action = [[ 0.12432212  0.09652102 -0.12368076  0.70508385]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 508 is [False, True, False, True, False, False]
Scene graph at timestep 508 is [False, True, False, True, False, False]
State prediction error at timestep 508 is tensor(2.2437e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 508 of 1
Current timestep = 509. State = [[-0.03130661 -0.12178126]]. Action = [[ 0.23730633  0.12578958  0.07010984 -0.44675446]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 509 is [False, True, False, True, False, False]
Scene graph at timestep 509 is [False, True, False, False, True, False]
State prediction error at timestep 509 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 509 of 1
Current timestep = 510. State = [[-0.00974816 -0.11969738]]. Action = [[ 0.20999816 -0.17079052  0.24530864 -0.58096147]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 510 is [False, True, False, False, True, False]
Current timestep = 511. State = [[ 0.01540718 -0.1328207 ]]. Action = [[ 0.1665051  -0.06481604  0.00675887  0.69128275]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 511 is [False, True, False, False, True, False]
Current timestep = 512. State = [[ 0.03773815 -0.14102083]]. Action = [[ 0.13593423 -0.04509428  0.01753664  0.90599453]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 512 is [False, True, False, True, False, False]
Scene graph at timestep 512 is [False, True, False, True, False, False]
State prediction error at timestep 512 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 512 of -1
Current timestep = 513. State = [[ 0.05454092 -0.1614199 ]]. Action = [[-0.1143479  -0.23570766  0.18546909  0.32113135]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 513 is [False, True, False, True, False, False]
Scene graph at timestep 513 is [False, False, True, True, False, False]
State prediction error at timestep 513 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 513 of -1
Current timestep = 514. State = [[ 0.0491939 -0.1960619]]. Action = [[-0.08457926 -0.21395633  0.17320198  0.78108644]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 514 is [False, False, True, True, False, False]
Current timestep = 515. State = [[ 0.0474808  -0.20143378]]. Action = [[-0.0564678   0.1897338   0.16261494  0.43198943]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 515 is [False, True, False, True, False, False]
Scene graph at timestep 515 is [False, True, False, True, False, False]
State prediction error at timestep 515 is tensor(2.8796e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 515 of -1
Current timestep = 516. State = [[ 0.04703666 -0.19492486]]. Action = [[ 0.24307033 -0.08612859 -0.03387982 -0.15062326]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 516 is [False, True, False, True, False, False]
Current timestep = 517. State = [[ 0.04836031 -0.18284088]]. Action = [[0.1012736  0.18361801 0.05244979 0.6518309 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 517 is [False, True, False, True, False, False]
Current timestep = 518. State = [[ 0.04896655 -0.1636566 ]]. Action = [[ 0.04536152  0.05899426  0.17954993 -0.8981173 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 518 is [False, True, False, True, False, False]
Current timestep = 519. State = [[ 0.04908721 -0.15789677]]. Action = [[ 0.1402353  -0.10344306  0.15968674  0.38510752]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 519 is [False, True, False, True, False, False]
Scene graph at timestep 519 is [False, True, False, True, False, False]
State prediction error at timestep 519 is tensor(2.0641e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 519 of 1
Current timestep = 520. State = [[ 0.04906559 -0.15682293]]. Action = [[ 0.216735    0.22025052 -0.16513455 -0.45044822]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 520 is [False, True, False, True, False, False]
Current timestep = 521. State = [[ 0.04879683 -0.16823405]]. Action = [[-0.02389544 -0.21032669 -0.15998416 -0.4995563 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 521 is [False, True, False, True, False, False]
Current timestep = 522. State = [[ 0.04847218 -0.17839403]]. Action = [[ 0.22384953  0.11523086  0.18247798 -0.5320924 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 522 is [False, True, False, True, False, False]
Current timestep = 523. State = [[ 0.0484727  -0.17737979]]. Action = [[-0.04476252  0.07142898 -0.20891881 -0.2246629 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 523 is [False, True, False, True, False, False]
Current timestep = 524. State = [[ 0.04852943 -0.1768383 ]]. Action = [[ 0.15569049 -0.19365218  0.23715189 -0.7395215 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 524 is [False, True, False, True, False, False]
Current timestep = 525. State = [[ 0.04246847 -0.19087952]]. Action = [[-0.16320139 -0.22705585  0.09356186 -0.46942592]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 525 is [False, True, False, True, False, False]
Scene graph at timestep 525 is [False, True, False, True, False, False]
State prediction error at timestep 525 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 525 of -1
Current timestep = 526. State = [[ 0.03488417 -0.21346824]]. Action = [[ 0.15930855 -0.12518685 -0.1964754   0.68542457]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 526 is [False, True, False, True, False, False]
Current timestep = 527. State = [[ 0.03492564 -0.22007282]]. Action = [[ 0.20786202 -0.19371718 -0.13234936 -0.3877747 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 527 is [False, True, False, True, False, False]
Scene graph at timestep 527 is [False, True, False, True, False, False]
State prediction error at timestep 527 is tensor(8.1808e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 527 of -1
Current timestep = 528. State = [[ 0.03932305 -0.22401436]]. Action = [[ 0.13136926 -0.06484127  0.15861991  0.4116478 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 528 is [False, True, False, True, False, False]
Current timestep = 529. State = [[ 0.04912715 -0.22906293]]. Action = [[ 0.17182529 -0.06411973 -0.16817604 -0.17146182]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 529 is [False, True, False, True, False, False]
Current timestep = 530. State = [[ 0.04914216 -0.22928935]]. Action = [[ 0.17301023 -0.01403368 -0.15320073 -0.5819149 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 530 is [False, True, False, True, False, False]
Scene graph at timestep 530 is [False, True, False, True, False, False]
State prediction error at timestep 530 is tensor(7.7620e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 530 of -1
Current timestep = 531. State = [[ 0.05399509 -0.22008295]]. Action = [[0.07194039 0.19008756 0.04509267 0.9758458 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 531 is [False, True, False, True, False, False]
Current timestep = 532. State = [[ 0.05818343 -0.21305476]]. Action = [[-0.20674641 -0.00290452  0.06285399 -0.6191516 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 532 is [False, False, True, True, False, False]
Current timestep = 533. State = [[ 0.05673722 -0.2143719 ]]. Action = [[ 0.01933783 -0.04182944  0.19593528 -0.55677325]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 533 is [False, False, True, True, False, False]
Scene graph at timestep 533 is [False, False, True, True, False, False]
State prediction error at timestep 533 is tensor(1.3219e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 533 of -1
Current timestep = 534. State = [[ 0.05616926 -0.21569769]]. Action = [[ 0.14234555  0.04034072  0.21803147 -0.7052253 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 534 is [False, False, True, True, False, False]
Current timestep = 535. State = [[ 0.05232555 -0.20924664]]. Action = [[-0.13868138  0.12493014  0.24155292  0.10465825]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 535 is [False, False, True, True, False, False]
Scene graph at timestep 535 is [False, False, True, True, False, False]
State prediction error at timestep 535 is tensor(6.6669e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 535 of 1
Current timestep = 536. State = [[ 0.04768694 -0.19062163]]. Action = [[ 0.06437248  0.17564821 -0.12082478 -0.7888642 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 536 is [False, False, True, True, False, False]
Scene graph at timestep 536 is [False, True, False, True, False, False]
State prediction error at timestep 536 is tensor(9.1189e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 536 of 1
Current timestep = 537. State = [[ 0.04802703 -0.17820527]]. Action = [[ 0.03385818 -0.08844128 -0.22510023 -0.51530755]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 537 is [False, True, False, True, False, False]
Current timestep = 538. State = [[ 0.04804372 -0.18841396]]. Action = [[ 0.06624377 -0.13548335 -0.1009362   0.41543078]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 538 is [False, True, False, True, False, False]
Scene graph at timestep 538 is [False, True, False, True, False, False]
State prediction error at timestep 538 is tensor(8.9085e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 538 of -1
Current timestep = 539. State = [[ 0.04637204 -0.20614858]]. Action = [[-0.08169654 -0.11080465  0.01939976 -0.18289089]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 539 is [False, True, False, True, False, False]
Current timestep = 540. State = [[ 0.04431272 -0.22595829]]. Action = [[ 0.04043606 -0.18430482  0.09925181 -0.59607524]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 540 is [False, True, False, True, False, False]
Current timestep = 541. State = [[ 0.04317039 -0.23574921]]. Action = [[-0.03567776  0.10226715 -0.02523878  0.750895  ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 541 is [False, True, False, True, False, False]
Scene graph at timestep 541 is [False, True, False, True, False, False]
State prediction error at timestep 541 is tensor(2.7312e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 541 of -1
Current timestep = 542. State = [[ 0.04434467 -0.22371899]]. Action = [[-0.06585933  0.20917886 -0.07159045  0.78194046]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 542 is [False, True, False, True, False, False]
Current timestep = 543. State = [[ 0.03983655 -0.20924717]]. Action = [[-0.1372831   0.00982574 -0.13377841  0.9154544 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 543 is [False, True, False, True, False, False]
Current timestep = 544. State = [[ 0.03297565 -0.2012801 ]]. Action = [[ 0.03791067  0.05783668  0.17978907 -0.75183237]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 544 is [False, True, False, True, False, False]
Current timestep = 545. State = [[ 0.02727149 -0.20986487]]. Action = [[-0.11219403 -0.20228861  0.01245594  0.3158301 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 545 is [False, True, False, True, False, False]
Scene graph at timestep 545 is [False, True, False, True, False, False]
State prediction error at timestep 545 is tensor(8.8819e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 545 of 1
Current timestep = 546. State = [[ 0.02203417 -0.22209148]]. Action = [[ 0.2450468  -0.00917596 -0.0423395  -0.7961259 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 546 is [False, True, False, True, False, False]
Current timestep = 547. State = [[ 0.02816016 -0.21138397]]. Action = [[ 0.03234196  0.16544494  0.13376296 -0.8826451 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 547 is [False, True, False, True, False, False]
Current timestep = 548. State = [[ 0.03724091 -0.19015674]]. Action = [[ 0.11540312  0.17413872  0.07190704 -0.94060636]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 548 is [False, True, False, True, False, False]
Scene graph at timestep 548 is [False, True, False, True, False, False]
State prediction error at timestep 548 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 548 of 1
Current timestep = 549. State = [[ 0.04541521 -0.17421521]]. Action = [[ 0.16488954 -0.09322116  0.15176469 -0.8504503 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 549 is [False, True, False, True, False, False]
Scene graph at timestep 549 is [False, True, False, True, False, False]
State prediction error at timestep 549 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 549 of 1
Current timestep = 550. State = [[ 0.04541771 -0.174138  ]]. Action = [[0.21445864 0.17735466 0.17212456 0.20014393]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 550 is [False, True, False, True, False, False]
Scene graph at timestep 550 is [False, True, False, True, False, False]
State prediction error at timestep 550 is tensor(3.1360e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 550 of 1
Current timestep = 551. State = [[ 0.04555398 -0.17409858]]. Action = [[ 0.19634601 -0.15394385 -0.05346228 -0.2710868 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 551 is [False, True, False, True, False, False]
Scene graph at timestep 551 is [False, True, False, True, False, False]
State prediction error at timestep 551 is tensor(5.8193e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 551 of 1
Current timestep = 552. State = [[ 0.04439911 -0.18526976]]. Action = [[-0.11182201 -0.20132399 -0.09465408  0.21473742]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 552 is [False, True, False, True, False, False]
Current timestep = 553. State = [[ 0.04381273 -0.19585451]]. Action = [[ 0.20548052 -0.21641907  0.11044976 -0.9861496 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 553 is [False, True, False, True, False, False]
Current timestep = 554. State = [[ 0.04374076 -0.19701384]]. Action = [[ 0.19000906  0.00126097 -0.08442211  0.75193024]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 554 is [False, True, False, True, False, False]
Scene graph at timestep 554 is [False, True, False, True, False, False]
State prediction error at timestep 554 is tensor(3.1677e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 554 of -1
Current timestep = 555. State = [[ 0.0350261  -0.20319253]]. Action = [[-0.21228756 -0.05285054  0.07833904 -0.3031354 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 555 is [False, True, False, True, False, False]
Current timestep = 556. State = [[ 0.02679787 -0.20892099]]. Action = [[ 0.24137995 -0.15699151  0.22628063  0.61712825]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 556 is [False, True, False, True, False, False]
Current timestep = 557. State = [[ 0.02027691 -0.22259812]]. Action = [[-0.04086137 -0.20760034  0.12683806  0.9861481 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 557 is [False, True, False, True, False, False]
Scene graph at timestep 557 is [False, True, False, True, False, False]
State prediction error at timestep 557 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 557 of -1
Current timestep = 558. State = [[ 0.00881536 -0.24108796]]. Action = [[-0.18152437  0.00216764  0.21739781 -0.93489707]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 558 is [False, True, False, True, False, False]
Current timestep = 559. State = [[-0.01277076 -0.24388646]]. Action = [[-0.23340179  0.02806678  0.24863088 -0.4753877 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 559 is [False, True, False, True, False, False]
Current timestep = 560. State = [[-0.02418211 -0.23038511]]. Action = [[0.24550101 0.22708726 0.06690025 0.5511893 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 560 is [False, True, False, True, False, False]
Current timestep = 561. State = [[-0.02464926 -0.20770521]]. Action = [[-0.18471406  0.06057259  0.04910615 -0.88516605]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 561 is [False, True, False, True, False, False]
Current timestep = 562. State = [[-0.0300916  -0.18980034]]. Action = [[-0.09969532  0.21173507 -0.03884721  0.26750135]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 562 is [False, True, False, True, False, False]
Current timestep = 563. State = [[-0.05455548 -0.1778302 ]]. Action = [[-0.22426733 -0.08272359  0.1512712  -0.4219035 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 563 is [False, True, False, True, False, False]
Scene graph at timestep 563 is [True, False, False, True, False, False]
State prediction error at timestep 563 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 563 of -1
Current timestep = 564. State = [[-0.07941918 -0.18039007]]. Action = [[ 0.07468301 -0.04602629 -0.20446205 -0.9758374 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 564 is [True, False, False, True, False, False]
Current timestep = 565. State = [[-0.08102699 -0.18985261]]. Action = [[-0.06963155 -0.13301015  0.14078754  0.6047114 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 565 is [True, False, False, True, False, False]
Current timestep = 566. State = [[-0.0811331  -0.20455272]]. Action = [[ 0.13038167 -0.10986957 -0.05407509 -0.5312934 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 566 is [True, False, False, True, False, False]
Current timestep = 567. State = [[-0.07721359 -0.20319502]]. Action = [[0.05301774 0.18674707 0.23850736 0.5956274 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 567 is [True, False, False, True, False, False]
Scene graph at timestep 567 is [True, False, False, True, False, False]
State prediction error at timestep 567 is tensor(2.5483e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 567 of -1
Current timestep = 568. State = [[-0.07243314 -0.18137014]]. Action = [[-0.02887556  0.23186576 -0.09469783 -0.8268253 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 568 is [True, False, False, True, False, False]
Current timestep = 569. State = [[-0.06551458 -0.1503632 ]]. Action = [[ 0.17303449  0.19065654 -0.10063478 -0.138412  ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 569 is [True, False, False, True, False, False]
Scene graph at timestep 569 is [True, False, False, True, False, False]
State prediction error at timestep 569 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 569 of 1
Current timestep = 570. State = [[-0.05696454 -0.13623478]]. Action = [[ 0.09778768 -0.13284782  0.17137468  0.3457712 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 570 is [True, False, False, True, False, False]
Scene graph at timestep 570 is [True, False, False, True, False, False]
State prediction error at timestep 570 is tensor(9.8889e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 570 of 1
Current timestep = 571. State = [[-0.05472123 -0.13895418]]. Action = [[-0.18256463  0.08485943  0.1956445  -0.09088391]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 571 is [True, False, False, True, False, False]
Scene graph at timestep 571 is [True, False, False, True, False, False]
State prediction error at timestep 571 is tensor(2.5850e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 571 of -1
Current timestep = 572. State = [[-0.06567117 -0.14277755]]. Action = [[-0.19920789 -0.06830692  0.0341087  -0.7212448 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 572 is [True, False, False, True, False, False]
Scene graph at timestep 572 is [True, False, False, True, False, False]
State prediction error at timestep 572 is tensor(7.3125e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 572 of -1
Current timestep = 573. State = [[-0.07582669 -0.15093128]]. Action = [[-0.04117374 -0.06731921  0.14482152 -0.03453231]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 573 is [True, False, False, True, False, False]
Scene graph at timestep 573 is [True, False, False, True, False, False]
State prediction error at timestep 573 is tensor(5.5352e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 573 of -1
Current timestep = 574. State = [[-0.09156251 -0.15682673]]. Action = [[-0.17631143  0.00369594  0.01672024 -0.7770121 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 574 is [True, False, False, True, False, False]
Scene graph at timestep 574 is [True, False, False, True, False, False]
State prediction error at timestep 574 is tensor(9.3221e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 574 of -1
Current timestep = 575. State = [[-0.11432458 -0.14855045]]. Action = [[-0.0920983   0.18875495  0.16070783 -0.9560229 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 575 is [True, False, False, True, False, False]
Scene graph at timestep 575 is [True, False, False, True, False, False]
State prediction error at timestep 575 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 575 of -1
Current timestep = 576. State = [[-0.12556203 -0.12029402]]. Action = [[-0.17355087  0.1750189   0.09188169  0.46802688]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 576 is [True, False, False, True, False, False]
Current timestep = 577. State = [[-0.1406689  -0.10193714]]. Action = [[0.12472466 0.07490626 0.1466558  0.85484004]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 577 is [True, False, False, False, True, False]
Current timestep = 578. State = [[-0.13554877 -0.08646175]]. Action = [[ 0.12765393  0.12425578 -0.08545427 -0.9532265 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 578 is [True, False, False, False, True, False]
Scene graph at timestep 578 is [True, False, False, False, True, False]
State prediction error at timestep 578 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 578 of 1
Current timestep = 579. State = [[-0.13535605 -0.06720299]]. Action = [[-0.15176432  0.12726226  0.03823632  0.08864379]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 579 is [True, False, False, False, True, False]
Current timestep = 580. State = [[-0.13353617 -0.06081194]]. Action = [[ 0.19938707 -0.09263048  0.19334507  0.37831676]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 580 is [True, False, False, False, True, False]
Scene graph at timestep 580 is [True, False, False, False, True, False]
State prediction error at timestep 580 is tensor(1.8213e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 580 of 1
Current timestep = 581. State = [[-0.13382952 -0.07060952]]. Action = [[-0.18119456 -0.1197674   0.10603485  0.3997141 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 581 is [True, False, False, False, True, False]
Scene graph at timestep 581 is [True, False, False, False, True, False]
State prediction error at timestep 581 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 581 of -1
Current timestep = 582. State = [[-0.14431609 -0.09206207]]. Action = [[-0.10264914 -0.1831149  -0.2117211  -0.41597903]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 582 is [True, False, False, False, True, False]
Current timestep = 583. State = [[-0.14592265 -0.09117155]]. Action = [[ 0.21623978  0.22922733 -0.11708874 -0.0565092 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 583 is [True, False, False, False, True, False]
Scene graph at timestep 583 is [True, False, False, False, True, False]
State prediction error at timestep 583 is tensor(3.6174e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 583 of 1
Current timestep = 584. State = [[-0.13785078 -0.07853666]]. Action = [[0.02395278 0.02555463 0.13619363 0.69161665]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 584 is [True, False, False, False, True, False]
Scene graph at timestep 584 is [True, False, False, False, True, False]
State prediction error at timestep 584 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 584 of 1
Current timestep = 585. State = [[-0.13718286 -0.08557338]]. Action = [[-0.12278003 -0.17998901 -0.20349872 -0.17839712]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 585 is [True, False, False, False, True, False]
Current timestep = 586. State = [[-0.1409221  -0.09076303]]. Action = [[-0.02273598  0.07311371 -0.02395833 -0.22444886]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 586 is [True, False, False, False, True, False]
Current timestep = 587. State = [[-0.14525467 -0.1019259 ]]. Action = [[-0.03589122 -0.21022412 -0.07031566 -0.8968459 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 587 is [True, False, False, False, True, False]
Scene graph at timestep 587 is [True, False, False, False, True, False]
State prediction error at timestep 587 is tensor(7.5801e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 587 of -1
Current timestep = 588. State = [[-0.13932148  0.10666048]]. Action = [[-0.09092891 -0.02374265  0.06443363 -0.91724014]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 588 is [True, False, False, False, True, False]
Scene graph at timestep 588 is [True, False, False, False, True, False]
State prediction error at timestep 588 is tensor(0.0238, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 588 of 1
Current timestep = 589. State = [[-0.1125384   0.11381926]]. Action = [[ 0.17140988 -0.11601415  0.01488173 -0.08857858]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 589 is [True, False, False, False, True, False]
Current timestep = 590. State = [[-0.09188031  0.10284992]]. Action = [[ 0.133062   -0.09925351 -0.19249639  0.8900759 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 590 is [True, False, False, False, True, False]
Current timestep = 591. State = [[-0.07860059  0.10820398]]. Action = [[ 0.03658921  0.23488796  0.15470445 -0.95050615]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 591 is [True, False, False, False, True, False]
Scene graph at timestep 591 is [True, False, False, False, True, False]
State prediction error at timestep 591 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 591 of 1
Current timestep = 592. State = [[-0.06943118  0.11255238]]. Action = [[-0.13254888 -0.21948105 -0.14199121 -0.08329237]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 592 is [True, False, False, False, True, False]
Scene graph at timestep 592 is [True, False, False, False, True, False]
State prediction error at timestep 592 is tensor(1.9489e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 592 of 1
Current timestep = 593. State = [[-0.06670435  0.08679964]]. Action = [[ 0.08468199 -0.1946382   0.20175421  0.72184813]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 593 is [True, False, False, False, True, False]
Scene graph at timestep 593 is [True, False, False, False, True, False]
State prediction error at timestep 593 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 593 of 1
Current timestep = 594. State = [[-0.0627021   0.06342807]]. Action = [[ 0.09347415 -0.12054162 -0.21805196  0.77177167]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 594 is [True, False, False, False, True, False]
Current timestep = 595. State = [[-0.05807547  0.04189951]]. Action = [[-0.02476746 -0.20872162 -0.13949011  0.7339008 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 595 is [True, False, False, False, True, False]
Scene graph at timestep 595 is [True, False, False, False, True, False]
State prediction error at timestep 595 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 595 of 1
Current timestep = 596. State = [[-0.05189002  0.02226746]]. Action = [[ 0.14042452 -0.02381507  0.21844566 -0.05183572]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 596 is [True, False, False, False, True, False]
Scene graph at timestep 596 is [True, False, False, False, True, False]
State prediction error at timestep 596 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 596 of 1
Current timestep = 597. State = [[-0.04918091  0.01830835]]. Action = [[-0.15817896 -0.01283582  0.08516419 -0.19730163]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 597 is [True, False, False, False, True, False]
Scene graph at timestep 597 is [False, True, False, False, True, False]
State prediction error at timestep 597 is tensor(3.7699e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 597 of 1
Current timestep = 598. State = [[-0.05195021  0.02592711]]. Action = [[ 0.02732384  0.16593924 -0.20488532  0.6071141 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 598 is [False, True, False, False, True, False]
Scene graph at timestep 598 is [True, False, False, False, True, False]
State prediction error at timestep 598 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 598 of -1
Current timestep = 599. State = [[-0.05473238  0.03729747]]. Action = [[-0.06633437  0.01988611  0.11267832  0.24422288]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 599 is [True, False, False, False, True, False]
Scene graph at timestep 599 is [True, False, False, False, True, False]
State prediction error at timestep 599 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 599 of -1
Current timestep = 600. State = [[-0.06121267  0.03810879]]. Action = [[-0.2098242  -0.05672279  0.14532375  0.70318246]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 600 is [True, False, False, False, True, False]
Scene graph at timestep 600 is [True, False, False, False, True, False]
State prediction error at timestep 600 is tensor(3.7199e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 600 of -1
Current timestep = 601. State = [[-0.07691008  0.02621722]]. Action = [[-0.00063185 -0.15263925  0.15674543 -0.29362595]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 601 is [True, False, False, False, True, False]
Scene graph at timestep 601 is [True, False, False, False, True, False]
State prediction error at timestep 601 is tensor(9.6522e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 601 of -1
Current timestep = 602. State = [[-0.08136362  0.00904917]]. Action = [[-0.09217624 -0.08391613 -0.22586791 -0.1780284 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 602 is [True, False, False, False, True, False]
Scene graph at timestep 602 is [True, False, False, False, True, False]
State prediction error at timestep 602 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 602 of -1
Current timestep = 603. State = [[-0.08154888 -0.00037788]]. Action = [[ 0.18366557 -0.03633827  0.20918909 -0.5166638 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 603 is [True, False, False, False, True, False]
Scene graph at timestep 603 is [True, False, False, False, True, False]
State prediction error at timestep 603 is tensor(5.0273e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 603 of 1
Current timestep = 604. State = [[-0.07950351 -0.00328055]]. Action = [[-0.08157651 -0.03356734  0.06192711  0.7580786 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 604 is [True, False, False, False, True, False]
Scene graph at timestep 604 is [True, False, False, False, True, False]
State prediction error at timestep 604 is tensor(5.4511e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 604 of -1
Current timestep = 605. State = [[-0.07750883 -0.01205101]]. Action = [[ 0.1363802  -0.06613638 -0.09034547 -0.85186666]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 605 is [True, False, False, False, True, False]
Scene graph at timestep 605 is [True, False, False, False, True, False]
State prediction error at timestep 605 is tensor(9.9994e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 605 of 1
Current timestep = 606. State = [[-0.07166241 -0.02249481]]. Action = [[ 0.08871788 -0.07900891 -0.22851746  0.53599477]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 606 is [True, False, False, False, True, False]
Scene graph at timestep 606 is [True, False, False, False, True, False]
State prediction error at timestep 606 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 606 of 1
Current timestep = 607. State = [[-0.07071497 -0.03931745]]. Action = [[-0.1663971  -0.15921037 -0.14021316  0.9528594 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 607 is [True, False, False, False, True, False]
Scene graph at timestep 607 is [True, False, False, False, True, False]
State prediction error at timestep 607 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 607 of -1
Current timestep = 608. State = [[-0.07566506 -0.05113603]]. Action = [[-0.10936132  0.04969162  0.13692081  0.58018863]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 608 is [True, False, False, False, True, False]
Scene graph at timestep 608 is [True, False, False, False, True, False]
State prediction error at timestep 608 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 608 of -1
Current timestep = 609. State = [[-0.07838031 -0.05274666]]. Action = [[ 0.20922548 -0.05452409  0.04862073 -0.37293255]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 609 is [True, False, False, False, True, False]
Scene graph at timestep 609 is [True, False, False, False, True, False]
State prediction error at timestep 609 is tensor(1.2900e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 609 of 1
Current timestep = 610. State = [[-0.07452239 -0.05097657]]. Action = [[-0.05275336  0.08099562  0.16449448 -0.6356169 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 610 is [True, False, False, False, True, False]
Current timestep = 611. State = [[-0.06860721 -0.03913987]]. Action = [[ 0.1882919   0.14706364 -0.03551789  0.7407062 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 611 is [True, False, False, False, True, False]
Scene graph at timestep 611 is [True, False, False, False, True, False]
State prediction error at timestep 611 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 611 of 1
Current timestep = 612. State = [[-0.06007288 -0.02917137]]. Action = [[ 0.00982508 -0.0705035  -0.16031907  0.5137062 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 612 is [True, False, False, False, True, False]
Scene graph at timestep 612 is [True, False, False, False, True, False]
State prediction error at timestep 612 is tensor(7.1776e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 612 of 1
Current timestep = 613. State = [[-0.05316573 -0.04060894]]. Action = [[ 0.11978444 -0.12277889  0.14984065  0.929966  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 613 is [True, False, False, False, True, False]
Scene graph at timestep 613 is [True, False, False, False, True, False]
State prediction error at timestep 613 is tensor(4.9831e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 613 of 1
Current timestep = 614. State = [[-0.2637907  -0.20393384]]. Action = [[ 0.14418751 -0.10390219  0.10372758 -0.32957947]]. Reward = [100.]
Curr episode timestep = 25
Scene graph at timestep 614 is [True, False, False, False, True, False]
Scene graph at timestep 614 is [True, False, False, True, False, False]
State prediction error at timestep 614 is tensor(0.0336, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 614 of -1
Current timestep = 615. State = [[-0.26165712 -0.2265164 ]]. Action = [[-0.13689329  0.07000449  0.01161274  0.7987907 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 615 is [True, False, False, True, False, False]
Current timestep = 616. State = [[-0.25379094 -0.22799104]]. Action = [[ 0.17114401  0.01099247 -0.15884958 -0.48984516]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 616 is [True, False, False, True, False, False]
Scene graph at timestep 616 is [True, False, False, True, False, False]
State prediction error at timestep 616 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 616 of 1
Current timestep = 617. State = [[-0.23589511 -0.22212206]]. Action = [[ 0.10622439  0.0953027   0.23643911 -0.38999075]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 617 is [True, False, False, True, False, False]
Scene graph at timestep 617 is [True, False, False, True, False, False]
State prediction error at timestep 617 is tensor(5.9191e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 617 of 1
Current timestep = 618. State = [[-0.2156221  -0.21759596]]. Action = [[ 0.23301929 -0.03832692 -0.19206786 -0.41669506]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 618 is [True, False, False, True, False, False]
Scene graph at timestep 618 is [True, False, False, True, False, False]
State prediction error at timestep 618 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 618 of 1
Current timestep = 619. State = [[-0.18893763 -0.21897356]]. Action = [[0.0718686  0.01462293 0.16364908 0.02651918]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 619 is [True, False, False, True, False, False]
Scene graph at timestep 619 is [True, False, False, True, False, False]
State prediction error at timestep 619 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 619 of 1
Current timestep = 620. State = [[-0.1868808  -0.20827635]]. Action = [[-0.21075061  0.21912074 -0.11216216 -0.7423333 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 620 is [True, False, False, True, False, False]
Current timestep = 621. State = [[-0.18428773 -0.19881989]]. Action = [[ 0.2256341  -0.0395737  -0.00587797 -0.61260545]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 621 is [True, False, False, True, False, False]
Scene graph at timestep 621 is [True, False, False, True, False, False]
State prediction error at timestep 621 is tensor(1.8605e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 621 of 1
Current timestep = 622. State = [[-0.16756625 -0.18124716]]. Action = [[ 0.23549911  0.21583527  0.04946893 -0.38855386]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 622 is [True, False, False, True, False, False]
Scene graph at timestep 622 is [True, False, False, True, False, False]
State prediction error at timestep 622 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of 1
Current timestep = 623. State = [[-0.13649552 -0.15246113]]. Action = [[ 0.19935426  0.17988709 -0.10074148 -0.3707415 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 623 is [True, False, False, True, False, False]
Scene graph at timestep 623 is [True, False, False, True, False, False]
State prediction error at timestep 623 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 623 of 1
Current timestep = 624. State = [[-0.10486433 -0.1395269 ]]. Action = [[ 0.24902037 -0.02445012 -0.05228947 -0.38281113]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 624 is [True, False, False, True, False, False]
Current timestep = 625. State = [[-0.07365754 -0.1410417 ]]. Action = [[0.22718531 0.00820339 0.2018275  0.8585701 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 625 is [True, False, False, True, False, False]
Scene graph at timestep 625 is [True, False, False, True, False, False]
State prediction error at timestep 625 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 625 of 1
Current timestep = 626. State = [[-0.04502779 -0.14728163]]. Action = [[-0.01084322 -0.12164262 -0.0416431   0.6861142 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 626 is [True, False, False, True, False, False]
Current timestep = 627. State = [[-0.04553143 -0.15098211]]. Action = [[ 0.01222554  0.02142277 -0.1928705  -0.827111  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 627 is [False, True, False, True, False, False]
Scene graph at timestep 627 is [False, True, False, True, False, False]
State prediction error at timestep 627 is tensor(2.6784e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 627 of -1
Current timestep = 628. State = [[-0.03665129 -0.14348416]]. Action = [[ 0.20397013  0.12347865 -0.08649018 -0.16149586]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 628 is [False, True, False, True, False, False]
Current timestep = 629. State = [[-0.0185664  -0.15000357]]. Action = [[ 0.00285694 -0.20392309  0.14538649 -0.51864165]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 629 is [False, True, False, True, False, False]
Scene graph at timestep 629 is [False, True, False, True, False, False]
State prediction error at timestep 629 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 629 of -1
Current timestep = 630. State = [[-0.01486966 -0.15167768]]. Action = [[-0.14580524  0.21918753  0.11106455 -0.37133908]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 630 is [False, True, False, True, False, False]
Scene graph at timestep 630 is [False, True, False, True, False, False]
State prediction error at timestep 630 is tensor(5.4176e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 630 of 1
Current timestep = 631. State = [[-0.01797553 -0.1321812 ]]. Action = [[0.10882705 0.07558277 0.03875369 0.30526483]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 631 is [False, True, False, True, False, False]
Scene graph at timestep 631 is [False, True, False, True, False, False]
State prediction error at timestep 631 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 631 of 1
Current timestep = 632. State = [[-0.01940485 -0.13222909]]. Action = [[-0.13492149 -0.10965738  0.04510477  0.4130721 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 632 is [False, True, False, True, False, False]
Current timestep = 633. State = [[-0.02338006 -0.15210879]]. Action = [[-0.02747029 -0.2369026   0.10077032 -0.58697623]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 633 is [False, True, False, True, False, False]
Current timestep = 634. State = [[-0.02415955 -0.15345748]]. Action = [[ 0.17067945  0.2426632  -0.10047275 -0.5823014 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 634 is [False, True, False, True, False, False]
Scene graph at timestep 634 is [False, True, False, True, False, False]
State prediction error at timestep 634 is tensor(6.1286e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of 0
Current timestep = 635. State = [[-0.02051213 -0.1468215 ]]. Action = [[-0.02238886 -0.16373084 -0.0166799   0.2940451 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 635 is [False, True, False, True, False, False]
Scene graph at timestep 635 is [False, True, False, True, False, False]
State prediction error at timestep 635 is tensor(1.5700e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 635 of -1
Current timestep = 636. State = [[-0.0150171  -0.14453676]]. Action = [[ 0.1516585   0.2180134  -0.01727498 -0.10730779]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 636 is [False, True, False, True, False, False]
Current timestep = 637. State = [[-0.00218632 -0.12064402]]. Action = [[-0.00192526  0.21998459 -0.07861781  0.4185524 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 637 is [False, True, False, True, False, False]
Current timestep = 638. State = [[ 0.00380588 -0.11130909]]. Action = [[ 0.10661274 -0.16598774 -0.08080405 -0.723475  ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 638 is [False, True, False, False, True, False]
Current timestep = 639. State = [[ 0.01015532 -0.12587309]]. Action = [[-0.19425796 -0.11849374 -0.15772276  0.03212941]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 639 is [False, True, False, False, True, False]
Scene graph at timestep 639 is [False, True, False, True, False, False]
State prediction error at timestep 639 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 639 of 1
Current timestep = 640. State = [[ 0.01036311 -0.15164556]]. Action = [[ 0.23100194 -0.23429105 -0.13085811  0.24415743]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 640 is [False, True, False, True, False, False]
Scene graph at timestep 640 is [False, True, False, True, False, False]
State prediction error at timestep 640 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 640 of -1
Current timestep = 641. State = [[ 0.02524487 -0.16754171]]. Action = [[ 0.242457   -0.15089552  0.24766308 -0.4235155 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 641 is [False, True, False, True, False, False]
Scene graph at timestep 641 is [False, True, False, True, False, False]
State prediction error at timestep 641 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 641 of -1
Current timestep = 642. State = [[ 0.0274102  -0.15566428]]. Action = [[0.05327818 0.22763985 0.10501564 0.20104206]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 642 is [False, True, False, True, False, False]
Scene graph at timestep 642 is [False, True, False, True, False, False]
State prediction error at timestep 642 is tensor(3.9453e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 642 of 1
Current timestep = 643. State = [[ 0.03284071 -0.13360144]]. Action = [[0.00281551 0.14177266 0.0888257  0.5172429 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 643 is [False, True, False, True, False, False]
Scene graph at timestep 643 is [False, True, False, True, False, False]
State prediction error at timestep 643 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 643 of 1
Current timestep = 644. State = [[ 0.0299287  -0.13283502]]. Action = [[-0.21290182 -0.17322125 -0.14626403 -0.06927514]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 644 is [False, True, False, True, False, False]
Scene graph at timestep 644 is [False, True, False, True, False, False]
State prediction error at timestep 644 is tensor(2.1904e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 644 of -1
Current timestep = 645. State = [[ 0.02291115 -0.15283376]]. Action = [[-0.06265196 -0.12385812 -0.15833905  0.65082145]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 645 is [False, True, False, True, False, False]
Current timestep = 646. State = [[ 0.02076997 -0.15589498]]. Action = [[ 0.13823289  0.0866825   0.18641275 -0.3090225 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 646 is [False, True, False, True, False, False]
Current timestep = 647. State = [[ 0.02872774 -0.14639643]]. Action = [[ 0.22405362  0.08989772 -0.09663759 -0.16016632]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 647 is [False, True, False, True, False, False]
Scene graph at timestep 647 is [False, True, False, True, False, False]
State prediction error at timestep 647 is tensor(5.7834e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 647 of -1
Current timestep = 648. State = [[ 0.05057234 -0.13993512]]. Action = [[0.097426   0.00220358 0.1438615  0.6307583 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 648 is [False, True, False, True, False, False]
Current timestep = 649. State = [[ 0.05051558 -0.14129066]]. Action = [[ 0.01282698 -0.0484632   0.12018558 -0.08515543]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 649 is [False, False, True, True, False, False]
Scene graph at timestep 649 is [False, False, True, True, False, False]
State prediction error at timestep 649 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 649 of -1
Current timestep = 650. State = [[ 0.05078419 -0.15174465]]. Action = [[ 0.02443016 -0.16177647  0.15909874  0.13640773]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 650 is [False, False, True, True, False, False]
Current timestep = 651. State = [[ 0.05039476 -0.16378403]]. Action = [[-0.14324379 -0.01816554 -0.06193517  0.2849362 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 651 is [False, False, True, True, False, False]
Current timestep = 652. State = [[ 0.04888524 -0.15774806]]. Action = [[-0.02916244  0.19265747  0.0163447   0.74222565]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 652 is [False, False, True, True, False, False]
Current timestep = 653. State = [[ 0.04846832 -0.14766784]]. Action = [[0.1639117  0.04703194 0.07639709 0.6405444 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 653 is [False, True, False, True, False, False]
Scene graph at timestep 653 is [False, True, False, True, False, False]
State prediction error at timestep 653 is tensor(9.5958e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 653 of 1
Current timestep = 654. State = [[ 0.04841821 -0.13721818]]. Action = [[ 0.03196555  0.14320457 -0.19738872  0.597584  ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 654 is [False, True, False, True, False, False]
Scene graph at timestep 654 is [False, True, False, True, False, False]
State prediction error at timestep 654 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 654 of 1
Current timestep = 655. State = [[ 0.04550225 -0.12634781]]. Action = [[-0.14798975 -0.01185483 -0.12837723  0.1993494 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 655 is [False, True, False, True, False, False]
Current timestep = 656. State = [[ 0.0439625  -0.11582087]]. Action = [[0.10692954 0.18591923 0.0290634  0.31642795]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 656 is [False, True, False, True, False, False]
Current timestep = 657. State = [[ 0.03994622 -0.11042795]]. Action = [[-0.19703767 -0.13500872  0.1067135   0.49791002]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 657 is [False, True, False, False, True, False]
Scene graph at timestep 657 is [False, True, False, False, True, False]
State prediction error at timestep 657 is tensor(3.5827e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 657 of 1
Current timestep = 658. State = [[ 0.02911636 -0.10917198]]. Action = [[-0.07048538  0.13959408 -0.19715333 -0.80770946]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 658 is [False, True, False, False, True, False]
Scene graph at timestep 658 is [False, True, False, False, True, False]
State prediction error at timestep 658 is tensor(3.0360e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 658 of 1
Current timestep = 659. State = [[-0.21207497 -0.11682575]]. Action = [[ 0.20552933 -0.05615826 -0.03051645 -0.7010123 ]]. Reward = [100.]
Curr episode timestep = 44
Scene graph at timestep 659 is [False, True, False, False, True, False]
Current timestep = 660. State = [[-0.19986948 -0.12913142]]. Action = [[ 0.09706357 -0.00079988 -0.01341689 -0.46316308]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 660 is [True, False, False, False, True, False]
Scene graph at timestep 660 is [True, False, False, True, False, False]
State prediction error at timestep 660 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 660 of 1
Current timestep = 661. State = [[-0.18806548 -0.13222905]]. Action = [[ 0.05931064 -0.03346545 -0.21446967  0.8051834 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 661 is [True, False, False, True, False, False]
Scene graph at timestep 661 is [True, False, False, True, False, False]
State prediction error at timestep 661 is tensor(6.1607e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 661 of 1
Current timestep = 662. State = [[-0.17333926 -0.1398801 ]]. Action = [[ 0.17408949 -0.09230246  0.22074324 -0.5291234 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 662 is [True, False, False, True, False, False]
Scene graph at timestep 662 is [True, False, False, True, False, False]
State prediction error at timestep 662 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 662 of 1
Current timestep = 663. State = [[-0.1628125 -0.1487676]]. Action = [[-0.2279725   0.03924721 -0.16064717  0.64341784]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 663 is [True, False, False, True, False, False]
Current timestep = 664. State = [[-0.16832991 -0.14474835]]. Action = [[-0.04168023  0.09530368  0.11409491  0.94304657]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 664 is [True, False, False, True, False, False]
Current timestep = 665. State = [[-0.16436455 -0.1416525 ]]. Action = [[ 0.24384522 -0.06151778  0.12765265  0.24100077]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 665 is [True, False, False, True, False, False]
Scene graph at timestep 665 is [True, False, False, True, False, False]
State prediction error at timestep 665 is tensor(1.1762e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 665 of 1
Current timestep = 666. State = [[-0.16298224 -0.13178585]]. Action = [[-0.15747614  0.19027412 -0.1710173  -0.61783385]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 666 is [True, False, False, True, False, False]
Current timestep = 667. State = [[-0.16080481 -0.11042049]]. Action = [[ 0.17216656  0.1632112   0.04886761 -0.4877429 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 667 is [True, False, False, True, False, False]
Current timestep = 668. State = [[-0.16105923 -0.10950088]]. Action = [[-0.13762105 -0.21943948 -0.15549797  0.8080565 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 668 is [True, False, False, False, True, False]
Current timestep = 669. State = [[-0.16306397 -0.10740881]]. Action = [[ 0.03523812  0.21036601 -0.11870192 -0.94163626]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 669 is [True, False, False, False, True, False]
Current timestep = 670. State = [[-0.15670665 -0.08250021]]. Action = [[ 0.19776237  0.24018493 -0.00143233 -0.9532294 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 670 is [True, False, False, False, True, False]
Current timestep = 671. State = [[-0.13875178 -0.05144534]]. Action = [[ 0.21828187  0.21080309  0.1934571  -0.96010137]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 671 is [True, False, False, False, True, False]
Scene graph at timestep 671 is [True, False, False, False, True, False]
State prediction error at timestep 671 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 671 of 1
Current timestep = 672. State = [[-0.106738   -0.02914063]]. Action = [[ 0.15478218  0.02210712 -0.1547274  -0.45798695]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 672 is [True, False, False, False, True, False]
Scene graph at timestep 672 is [True, False, False, False, True, False]
State prediction error at timestep 672 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 672 of 1
Current timestep = 673. State = [[-0.08407757 -0.01712458]]. Action = [[ 0.1430201   0.15163505  0.18025917 -0.76959157]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 673 is [True, False, False, False, True, False]
Scene graph at timestep 673 is [True, False, False, False, True, False]
State prediction error at timestep 673 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 673 of 1
Current timestep = 674. State = [[-0.07163379  0.01154917]]. Action = [[-0.1981176   0.24579453 -0.18845548  0.16321373]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 674 is [True, False, False, False, True, False]
Current timestep = 675. State = [[-0.07487505  0.04241883]]. Action = [[0.19218177 0.18635744 0.02474543 0.4629717 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 675 is [True, False, False, False, True, False]
Scene graph at timestep 675 is [True, False, False, False, True, False]
State prediction error at timestep 675 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 675 of 1
Current timestep = 676. State = [[-0.06955589  0.05835959]]. Action = [[ 0.01658744 -0.04169139  0.18505326  0.71999025]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 676 is [True, False, False, False, True, False]
Scene graph at timestep 676 is [True, False, False, False, True, False]
State prediction error at timestep 676 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 676 of 1
Current timestep = 677. State = [[-0.06350459  0.06569239]]. Action = [[ 0.13079742  0.1603468  -0.17017563  0.7071924 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 677 is [True, False, False, False, True, False]
Scene graph at timestep 677 is [True, False, False, False, True, False]
State prediction error at timestep 677 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 677 of 1
Current timestep = 678. State = [[-0.04183126  0.06448746]]. Action = [[ 0.1401959  -0.21485233  0.02480951 -0.29510838]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 678 is [True, False, False, False, True, False]
Current timestep = 679. State = [[-0.20507762  0.01482216]]. Action = [[ 0.20010337 -0.21059771 -0.23384836 -0.18276364]]. Reward = [100.]
Curr episode timestep = 19
Scene graph at timestep 679 is [False, True, False, False, True, False]
Current timestep = 680. State = [[-0.18662876  0.03144808]]. Action = [[ 0.20034593  0.22622067 -0.00727451  0.39944935]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 680 is [True, False, False, False, True, False]
Current timestep = 681. State = [[-0.1676404   0.04518851]]. Action = [[ 0.03662434 -0.04686165  0.23941424  0.0952605 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 681 is [True, False, False, False, True, False]
Scene graph at timestep 681 is [True, False, False, False, True, False]
State prediction error at timestep 681 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 681 of 1
Current timestep = 682. State = [[-0.16551726  0.056073  ]]. Action = [[-0.15191425  0.15705019 -0.06178367  0.22895563]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 682 is [True, False, False, False, True, False]
Scene graph at timestep 682 is [True, False, False, False, True, False]
State prediction error at timestep 682 is tensor(3.0183e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 682 of -1
Current timestep = 683. State = [[-0.17104837  0.07047021]]. Action = [[ 0.08221033  0.04390323 -0.04325821  0.98249257]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 683 is [True, False, False, False, True, False]
Scene graph at timestep 683 is [True, False, False, False, True, False]
State prediction error at timestep 683 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 683 of -1
Current timestep = 684. State = [[-0.1765018   0.07536638]]. Action = [[-0.23046553 -0.01040094 -0.13396144 -0.72332215]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 684 is [True, False, False, False, True, False]
Current timestep = 685. State = [[-0.1791428   0.06230232]]. Action = [[ 0.14552504 -0.24034804  0.17214435 -0.51772743]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 685 is [True, False, False, False, True, False]
Scene graph at timestep 685 is [True, False, False, False, True, False]
State prediction error at timestep 685 is tensor(7.9215e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 685 of 1
Current timestep = 686. State = [[-0.17072158  0.03255891]]. Action = [[ 0.16260874 -0.16923995 -0.01853983 -0.18700367]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 686 is [True, False, False, False, True, False]
Scene graph at timestep 686 is [True, False, False, False, True, False]
State prediction error at timestep 686 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 686 of 1
Current timestep = 687. State = [[-0.1476765   0.01155108]]. Action = [[ 0.23969412 -0.12435147  0.08146214 -0.61409104]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 687 is [True, False, False, False, True, False]
Current timestep = 688. State = [[-0.13180155  0.00516375]]. Action = [[-0.11294484  0.02639043  0.00819233 -0.4475903 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 688 is [True, False, False, False, True, False]
Current timestep = 689. State = [[-0.13031358  0.00772156]]. Action = [[ 0.11625925  0.07397678  0.04891407 -0.930414  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 689 is [True, False, False, False, True, False]
Scene graph at timestep 689 is [True, False, False, False, True, False]
State prediction error at timestep 689 is tensor(6.9166e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 689 of 1
Current timestep = 690. State = [[-0.12227581  0.01118381]]. Action = [[ 0.04378563  0.00085557 -0.10457522 -0.14362466]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 690 is [True, False, False, False, True, False]
Scene graph at timestep 690 is [True, False, False, False, True, False]
State prediction error at timestep 690 is tensor(5.2336e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 690 of 1
Current timestep = 691. State = [[-0.12219975  0.02589983]]. Action = [[-0.15694131  0.2176686  -0.09398597  0.11135292]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 691 is [True, False, False, False, True, False]
Current timestep = 692. State = [[-0.1265049   0.03319554]]. Action = [[-0.0528613  -0.13413423 -0.11373247  0.95671165]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 692 is [True, False, False, False, True, False]
Current timestep = 693. State = [[-0.12255134  0.01307557]]. Action = [[ 0.20021403 -0.22484434  0.17089361 -0.7012306 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 693 is [True, False, False, False, True, False]
Current timestep = 694. State = [[-0.11453626  0.00499724]]. Action = [[ 0.09320053  0.11168271  0.02141657 -0.9412325 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 694 is [True, False, False, False, True, False]
Current timestep = 695. State = [[-0.10202489  0.01938415]]. Action = [[ 0.14913672  0.19759202 -0.10186408  0.8035234 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 695 is [True, False, False, False, True, False]
Scene graph at timestep 695 is [True, False, False, False, True, False]
State prediction error at timestep 695 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 695 of 1
Current timestep = 696. State = [[-0.08208717  0.02216956]]. Action = [[-0.05742869 -0.23438814 -0.1696752   0.28582335]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 696 is [True, False, False, False, True, False]
Current timestep = 697. State = [[-0.0810481  -0.00010313]]. Action = [[ 0.02887648 -0.15238108  0.0748966   0.87852716]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 697 is [True, False, False, False, True, False]
Current timestep = 698. State = [[-0.08080187 -0.00417954]]. Action = [[-0.08207163  0.15100941 -0.15788423  0.831285  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 698 is [True, False, False, False, True, False]
Scene graph at timestep 698 is [True, False, False, False, True, False]
State prediction error at timestep 698 is tensor(1.1268e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 698 of 1
Current timestep = 699. State = [[-0.07802635 -0.01059431]]. Action = [[ 0.19952872 -0.22365083  0.16415921 -0.66395104]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 699 is [True, False, False, False, True, False]
Scene graph at timestep 699 is [True, False, False, False, True, False]
State prediction error at timestep 699 is tensor(6.3240e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 699 of 1
Current timestep = 700. State = [[-0.0621712  -0.02772453]]. Action = [[ 0.23229313 -0.01774855 -0.08072099 -0.13376218]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 700 is [True, False, False, False, True, False]
Scene graph at timestep 700 is [True, False, False, False, True, False]
State prediction error at timestep 700 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 700 of 1
Current timestep = 701. State = [[-0.18152496 -0.11524168]]. Action = [[-0.03859812 -0.20687944  0.20283267 -0.5832384 ]]. Reward = [100.]
Curr episode timestep = 21
Scene graph at timestep 701 is [True, False, False, False, True, False]
Scene graph at timestep 701 is [True, False, False, False, True, False]
State prediction error at timestep 701 is tensor(0.0106, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 701 of 1
Current timestep = 702. State = [[-0.17479026 -0.13045815]]. Action = [[-0.14639142 -0.037126   -0.16348822 -0.44154888]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 702 is [True, False, False, False, True, False]
Current timestep = 703. State = [[-0.17848563 -0.12443327]]. Action = [[-0.00757003  0.20752701 -0.20928529  0.40328884]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 703 is [True, False, False, True, False, False]
Scene graph at timestep 703 is [True, False, False, False, True, False]
State prediction error at timestep 703 is tensor(8.6814e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 703 of 1
Current timestep = 704. State = [[-0.1816408  -0.10136795]]. Action = [[-0.05602665  0.17102355 -0.23482208  0.9706855 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 704 is [True, False, False, False, True, False]
Current timestep = 705. State = [[-0.18514451 -0.09157062]]. Action = [[-0.03693144 -0.04919896  0.14130342 -0.16637266]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 705 is [True, False, False, False, True, False]
Scene graph at timestep 705 is [True, False, False, False, True, False]
State prediction error at timestep 705 is tensor(8.4397e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 705 of -1
Current timestep = 706. State = [[-0.18731964 -0.10092244]]. Action = [[ 0.02422929 -0.15606491  0.2182889   0.20364702]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 706 is [True, False, False, False, True, False]
Scene graph at timestep 706 is [True, False, False, False, True, False]
State prediction error at timestep 706 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 706 of -1
Current timestep = 707. State = [[-0.19299987 -0.123414  ]]. Action = [[-0.08350167 -0.18887351  0.05585724 -0.20814085]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 707 is [True, False, False, False, True, False]
Current timestep = 708. State = [[-0.19905344 -0.13092458]]. Action = [[-0.07148015  0.11076403 -0.1716267  -0.1927008 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 708 is [True, False, False, False, True, False]
Scene graph at timestep 708 is [True, False, False, True, False, False]
State prediction error at timestep 708 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 708 of -1
Current timestep = 709. State = [[-0.21439745 -0.11714194]]. Action = [[-0.17647164  0.19150862  0.03654802  0.49644208]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 709 is [True, False, False, True, False, False]
Scene graph at timestep 709 is [True, False, False, False, True, False]
State prediction error at timestep 709 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 709 of -1
Current timestep = 710. State = [[-0.22099392 -0.08722983]]. Action = [[ 0.21360347  0.22121209 -0.17776711 -0.15663856]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 710 is [True, False, False, False, True, False]
Scene graph at timestep 710 is [True, False, False, False, True, False]
State prediction error at timestep 710 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 710 of 1
Current timestep = 711. State = [[-0.20755516 -0.05624079]]. Action = [[ 0.17154181  0.18910697 -0.23857637  0.5506258 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 711 is [True, False, False, False, True, False]
Current timestep = 712. State = [[-0.18744262 -0.03859209]]. Action = [[ 0.15121585  0.08415335 -0.12593709 -0.49204355]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 712 is [True, False, False, False, True, False]
Current timestep = 713. State = [[-0.17949697 -0.04077348]]. Action = [[-0.13329498 -0.18122867 -0.17209612  0.5319624 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 713 is [True, False, False, False, True, False]
Scene graph at timestep 713 is [True, False, False, False, True, False]
State prediction error at timestep 713 is tensor(6.8236e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 713 of 1
Current timestep = 714. State = [[-0.18103142 -0.04979498]]. Action = [[ 0.04991841 -0.02323642  0.11145839 -0.3422197 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 714 is [True, False, False, False, True, False]
Scene graph at timestep 714 is [True, False, False, False, True, False]
State prediction error at timestep 714 is tensor(5.9074e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 714 of 1
Current timestep = 715. State = [[-0.17547274 -0.04062452]]. Action = [[ 0.1422554   0.20902103  0.04173553 -0.5430628 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 715 is [True, False, False, False, True, False]
Current timestep = 716. State = [[-0.15512533 -0.04006647]]. Action = [[ 0.20354992 -0.21108729  0.18739197 -0.7789293 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 716 is [True, False, False, False, True, False]
Scene graph at timestep 716 is [True, False, False, False, True, False]
State prediction error at timestep 716 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 716 of 1
Current timestep = 717. State = [[-0.1249196  -0.04159531]]. Action = [[0.24038732 0.16791546 0.09595767 0.54314363]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 717 is [True, False, False, False, True, False]
Current timestep = 718. State = [[-0.10982283 -0.03168043]]. Action = [[-0.16912253  0.03758475 -0.24238989 -0.8679892 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 718 is [True, False, False, False, True, False]
Scene graph at timestep 718 is [True, False, False, False, True, False]
State prediction error at timestep 718 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 718 of 1
Current timestep = 719. State = [[-0.10868389 -0.02792023]]. Action = [[ 0.12712061 -0.04253328 -0.17848071  0.24462521]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 719 is [True, False, False, False, True, False]
Current timestep = 720. State = [[-0.0956642  -0.02308701]]. Action = [[ 0.21961576  0.12479874  0.22556233 -0.9549549 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 720 is [True, False, False, False, True, False]
Current timestep = 721. State = [[-0.08012112 -0.00548284]]. Action = [[-0.07695755  0.18410861 -0.00757325  0.893564  ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 721 is [True, False, False, False, True, False]
Current timestep = 722. State = [[-0.07640587  0.02297324]]. Action = [[ 0.15930057  0.23111075 -0.23980355  0.02886784]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 722 is [True, False, False, False, True, False]
Current timestep = 723. State = [[-0.05536543  0.03937265]]. Action = [[ 0.18353307 -0.01315051 -0.1731882  -0.8270571 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 723 is [True, False, False, False, True, False]
Scene graph at timestep 723 is [True, False, False, False, True, False]
State prediction error at timestep 723 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 723 of 1
Current timestep = 724. State = [[-0.21782398 -0.10464777]]. Action = [[ 0.14157283 -0.07105803  0.0107564  -0.6648121 ]]. Reward = [100.]
Curr episode timestep = 22
Scene graph at timestep 724 is [True, False, False, False, True, False]
Scene graph at timestep 724 is [True, False, False, False, True, False]
State prediction error at timestep 724 is tensor(0.0220, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 724 of 1
Current timestep = 725. State = [[-0.20885609 -0.11928751]]. Action = [[-0.05279619 -0.05788659 -0.07600789 -0.50785756]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 725 is [True, False, False, False, True, False]
Scene graph at timestep 725 is [True, False, False, False, True, False]
State prediction error at timestep 725 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 725 of -1
Current timestep = 726. State = [[-0.20255637 -0.12617934]]. Action = [[ 0.1951707  -0.00520749  0.12860096  0.656286  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 726 is [True, False, False, False, True, False]
Current timestep = 727. State = [[-0.1802618 -0.1163706]]. Action = [[ 0.24105066  0.20326462  0.12187949 -0.6653904 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 727 is [True, False, False, True, False, False]
Scene graph at timestep 727 is [True, False, False, False, True, False]
State prediction error at timestep 727 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 727 of 1
Current timestep = 728. State = [[-0.15454449 -0.10421471]]. Action = [[-0.00448531 -0.05779055  0.09779364 -0.7517659 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 728 is [True, False, False, False, True, False]
Scene graph at timestep 728 is [True, False, False, False, True, False]
State prediction error at timestep 728 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of 1
Current timestep = 729. State = [[-0.14733937 -0.09269027]]. Action = [[ 0.15981424  0.23215735  0.22861779 -0.85292053]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 729 is [True, False, False, False, True, False]
Current timestep = 730. State = [[-0.13744484 -0.06558207]]. Action = [[-0.08176717  0.24172217 -0.02826263 -0.80643564]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 730 is [True, False, False, False, True, False]
Scene graph at timestep 730 is [True, False, False, False, True, False]
State prediction error at timestep 730 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 730 of 1
Current timestep = 731. State = [[-0.13993253 -0.03763768]]. Action = [[ 0.02969617  0.08462489  0.01999307 -0.09607136]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 731 is [True, False, False, False, True, False]
Current timestep = 732. State = [[-0.14049283 -0.0336715 ]]. Action = [[-0.10903615 -0.02860351  0.08814085  0.57443464]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 732 is [True, False, False, False, True, False]
Scene graph at timestep 732 is [True, False, False, False, True, False]
State prediction error at timestep 732 is tensor(8.7194e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 732 of 0
Current timestep = 733. State = [[-0.13623704 -0.02933042]]. Action = [[0.20172346 0.08101031 0.03297567 0.7456312 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 733 is [True, False, False, False, True, False]
Scene graph at timestep 733 is [True, False, False, False, True, False]
State prediction error at timestep 733 is tensor(3.0575e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 733 of 1
Current timestep = 734. State = [[-0.11494181 -0.01539244]]. Action = [[0.24898684 0.13685787 0.09970587 0.7709303 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 734 is [True, False, False, False, True, False]
Scene graph at timestep 734 is [True, False, False, False, True, False]
State prediction error at timestep 734 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 734 of 1
Current timestep = 735. State = [[-0.08852164 -0.01611303]]. Action = [[-0.08421752 -0.21596593  0.03307408  0.54812026]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 735 is [True, False, False, False, True, False]
Current timestep = 736. State = [[-0.082637  -0.0431007]]. Action = [[ 0.21233469 -0.23585393 -0.18831937  0.8933219 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 736 is [True, False, False, False, True, False]
Current timestep = 737. State = [[-0.06670655 -0.04817135]]. Action = [[ 0.10394168  0.18896109 -0.07570708  0.33293533]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 737 is [True, False, False, False, True, False]
Current timestep = 738. State = [[-0.05393445 -0.05530708]]. Action = [[ 0.04462859 -0.22263937  0.19923389 -0.2542069 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 738 is [True, False, False, False, True, False]
Current timestep = 739. State = [[-0.0542487  -0.08132347]]. Action = [[-0.21704528 -0.24323714  0.14444917 -0.7166928 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 739 is [True, False, False, False, True, False]
Current timestep = 740. State = [[-0.05340922 -0.08521211]]. Action = [[ 0.24168202  0.24334443 -0.20420744  0.4117992 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 740 is [True, False, False, False, True, False]
Scene graph at timestep 740 is [True, False, False, False, True, False]
State prediction error at timestep 740 is tensor(1.0004e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 740 of 1
Current timestep = 741. State = [[-0.04341988 -0.07441752]]. Action = [[-0.01075952 -0.03017338  0.19714573 -0.31014383]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 741 is [True, False, False, False, True, False]
Current timestep = 742. State = [[-0.04096879 -0.06947149]]. Action = [[ 0.09597054  0.09356824 -0.03299877 -0.9698994 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 742 is [False, True, False, False, True, False]
Scene graph at timestep 742 is [False, True, False, False, True, False]
State prediction error at timestep 742 is tensor(5.7062e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 742 of 1
Current timestep = 743. State = [[-0.23957427  0.00340292]]. Action = [[ 0.14617881  0.2055403  -0.0095253  -0.4470353 ]]. Reward = [100.]
Curr episode timestep = 18
Scene graph at timestep 743 is [False, True, False, False, True, False]
Scene graph at timestep 743 is [True, False, False, False, True, False]
State prediction error at timestep 743 is tensor(0.0205, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 743 of 1
Current timestep = 744. State = [[-0.22993764 -0.00432792]]. Action = [[ 0.09418711 -0.16039863 -0.24847692  0.14466918]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 744 is [True, False, False, False, True, False]
Current timestep = 745. State = [[-0.21460761 -0.00869634]]. Action = [[ 0.1896714   0.06125987  0.1261034  -0.90085983]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 745 is [True, False, False, False, True, False]
Current timestep = 746. State = [[-0.19418587  0.00472456]]. Action = [[0.08109331 0.22222024 0.05172098 0.508971  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 746 is [True, False, False, False, True, False]
Current timestep = 747. State = [[-0.17643994  0.00640928]]. Action = [[ 0.13640946 -0.20858157 -0.17780723 -0.8326193 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 747 is [True, False, False, False, True, False]
Scene graph at timestep 747 is [True, False, False, False, True, False]
State prediction error at timestep 747 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 747 of 1
Current timestep = 748. State = [[-0.15257026  0.00491194]]. Action = [[ 0.22263116  0.15781772 -0.03284472 -0.27479726]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 748 is [True, False, False, False, True, False]
Current timestep = 749. State = [[-0.13624956  0.01315712]]. Action = [[-0.07361199  0.01410219  0.08044916 -0.38153607]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 749 is [True, False, False, False, True, False]
Scene graph at timestep 749 is [True, False, False, False, True, False]
State prediction error at timestep 749 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 749 of 1
Current timestep = 750. State = [[-0.12687138  0.01525697]]. Action = [[ 0.23455763 -0.02772917  0.20032775 -0.08841932]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 750 is [True, False, False, False, True, False]
Current timestep = 751. State = [[-0.11389377  0.00875382]]. Action = [[-0.18964285 -0.11592852  0.1083363   0.69239235]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 751 is [True, False, False, False, True, False]
Scene graph at timestep 751 is [True, False, False, False, True, False]
State prediction error at timestep 751 is tensor(6.2220e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 751 of 1
Current timestep = 752. State = [[-0.12140384 -0.00146631]]. Action = [[-0.13526033 -0.02429654  0.11951828  0.80990267]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 752 is [True, False, False, False, True, False]
Scene graph at timestep 752 is [True, False, False, False, True, False]
State prediction error at timestep 752 is tensor(3.5028e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 752 of -1
Current timestep = 753. State = [[-0.12718688 -0.00426578]]. Action = [[ 0.09057122 -0.00600672  0.11046273  0.53178   ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 753 is [True, False, False, False, True, False]
Scene graph at timestep 753 is [True, False, False, False, True, False]
State prediction error at timestep 753 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 753 of -1
Current timestep = 754. State = [[-0.12466288 -0.00850928]]. Action = [[ 0.09768939 -0.0731481   0.01733369  0.54794025]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 754 is [True, False, False, False, True, False]
Scene graph at timestep 754 is [True, False, False, False, True, False]
State prediction error at timestep 754 is tensor(9.1648e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 754 of 1
Current timestep = 755. State = [[-0.11781806 -0.00533037]]. Action = [[ 0.11651182  0.17741168  0.01720029 -0.8226693 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 755 is [True, False, False, False, True, False]
Current timestep = 756. State = [[-0.11179265 -0.00425509]]. Action = [[-0.16978608 -0.15007061  0.12354249  0.5500748 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 756 is [True, False, False, False, True, False]
Current timestep = 757. State = [[-0.10916578 -0.00496615]]. Action = [[ 0.21999633  0.09661031 -0.11470565  0.6460738 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 757 is [True, False, False, False, True, False]
Scene graph at timestep 757 is [True, False, False, False, True, False]
State prediction error at timestep 757 is tensor(4.1809e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 757 of 1
Current timestep = 758. State = [[-0.10071433 -0.0009883 ]]. Action = [[-0.02811337  0.00746563  0.01036626  0.11583853]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 758 is [True, False, False, False, True, False]
Current timestep = 759. State = [[-0.10680246  0.01172134]]. Action = [[-0.21964918  0.19090337  0.01461044 -0.03458077]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 759 is [True, False, False, False, True, False]
Current timestep = 760. State = [[-0.12241745  0.03297559]]. Action = [[-0.19130658  0.06247556  0.18905538  0.09988081]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 760 is [True, False, False, False, True, False]
Scene graph at timestep 760 is [True, False, False, False, True, False]
State prediction error at timestep 760 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 760 of -1
Current timestep = 761. State = [[-0.14502676  0.02937249]]. Action = [[-0.17287426 -0.19939062  0.23925254  0.5459199 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 761 is [True, False, False, False, True, False]
Scene graph at timestep 761 is [True, False, False, False, True, False]
State prediction error at timestep 761 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 761 of -1
Current timestep = 762. State = [[-0.15707073  0.02522147]]. Action = [[0.19218165 0.224293   0.2175805  0.9234139 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 762 is [True, False, False, False, True, False]
Current timestep = 763. State = [[-0.14843734  0.03385722]]. Action = [[ 0.16319057 -0.07211687  0.14094931  0.49140596]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 763 is [True, False, False, False, True, False]
Scene graph at timestep 763 is [True, False, False, False, True, False]
State prediction error at timestep 763 is tensor(2.2851e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 763 of 1
Current timestep = 764. State = [[-0.12429319  0.02591413]]. Action = [[ 0.2160261  -0.14059235  0.01400366 -0.63527626]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 764 is [True, False, False, False, True, False]
Scene graph at timestep 764 is [True, False, False, False, True, False]
State prediction error at timestep 764 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 764 of 1
Current timestep = 765. State = [[-0.10384036  0.01873741]]. Action = [[ 0.08532119  0.08069333 -0.21415415 -0.7758762 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 765 is [True, False, False, False, True, False]
Scene graph at timestep 765 is [True, False, False, False, True, False]
State prediction error at timestep 765 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 765 of 1
Current timestep = 766. State = [[-0.09283003  0.01474846]]. Action = [[-0.08577861 -0.15389995  0.02583164 -0.30866635]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 766 is [True, False, False, False, True, False]
Scene graph at timestep 766 is [True, False, False, False, True, False]
State prediction error at timestep 766 is tensor(8.6338e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 766 of 1
Current timestep = 767. State = [[-0.09067927 -0.00175996]]. Action = [[ 0.12763757 -0.11780256  0.15412015  0.12567782]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 767 is [True, False, False, False, True, False]
Scene graph at timestep 767 is [True, False, False, False, True, False]
State prediction error at timestep 767 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 767 of 1
Current timestep = 768. State = [[-0.08644781 -0.02459264]]. Action = [[ 0.01211023 -0.20025924  0.09247157 -0.4620288 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 768 is [True, False, False, False, True, False]
Current timestep = 769. State = [[-0.07634617 -0.0400732 ]]. Action = [[ 0.19513202 -0.01601869 -0.17680664 -0.13464916]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 769 is [True, False, False, False, True, False]
Current timestep = 770. State = [[-0.0663624  -0.05390873]]. Action = [[-0.21421053 -0.16190451  0.04107204 -0.9349596 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 770 is [True, False, False, False, True, False]
Current timestep = 771. State = [[-0.06845832 -0.06806931]]. Action = [[ 0.04481524 -0.04774076  0.18241543 -0.8287609 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 771 is [True, False, False, False, True, False]
Current timestep = 772. State = [[-0.06552294 -0.07855733]]. Action = [[ 0.13569683 -0.08784652  0.02733052  0.7315278 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 772 is [True, False, False, False, True, False]
Current timestep = 773. State = [[-0.05254311 -0.09400575]]. Action = [[ 0.19520777 -0.10860334  0.03321052  0.06169164]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 773 is [True, False, False, False, True, False]
Current timestep = 774. State = [[-0.04332007 -0.11523664]]. Action = [[-0.17979868 -0.19760269  0.15089938  0.98003674]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 774 is [True, False, False, False, True, False]
Current timestep = 775. State = [[-0.04898678 -0.143185  ]]. Action = [[-0.10366395 -0.1778743   0.13425645 -0.64560884]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 775 is [False, True, False, False, True, False]
Scene graph at timestep 775 is [False, True, False, True, False, False]
State prediction error at timestep 775 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 775 of -1
Current timestep = 776. State = [[-0.0457205  -0.15238883]]. Action = [[ 0.24708754  0.12871057  0.08474672 -0.7748662 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 776 is [False, True, False, True, False, False]
Current timestep = 777. State = [[-0.02962764 -0.13955148]]. Action = [[0.24323207 0.08661002 0.23341066 0.36688876]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 777 is [False, True, False, True, False, False]
Current timestep = 778. State = [[-0.00338828 -0.13192578]]. Action = [[ 0.09826201  0.0415476  -0.23517084  0.9365668 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 778 is [False, True, False, True, False, False]
Current timestep = 779. State = [[ 0.01192314 -0.11829294]]. Action = [[-0.00229734  0.17171466 -0.09225127 -0.7959892 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 779 is [False, True, False, True, False, False]
Scene graph at timestep 779 is [False, True, False, False, True, False]
State prediction error at timestep 779 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 779 of 1
Current timestep = 780. State = [[ 0.02195949 -0.1161088 ]]. Action = [[ 0.21007752 -0.23253497 -0.18426087 -0.30141807]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 780 is [False, True, False, False, True, False]
Current timestep = 781. State = [[ 0.04856891 -0.12837236]]. Action = [[ 0.23754281 -0.013309   -0.14944644 -0.9349598 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 781 is [False, True, False, False, True, False]
Scene graph at timestep 781 is [False, True, False, True, False, False]
State prediction error at timestep 781 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 781 of -1
Current timestep = 782. State = [[ 0.07993177 -0.13525452]]. Action = [[-0.09255083 -0.02958846  0.01860562  0.6878699 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 782 is [False, True, False, True, False, False]
Current timestep = 783. State = [[ 0.0792021 -0.1382927]]. Action = [[ 0.12643969  0.00709489  0.22991881 -0.21954817]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 783 is [False, False, True, True, False, False]
Current timestep = 784. State = [[ 0.07883074 -0.13975587]]. Action = [[ 0.24188602 -0.24478865 -0.12093699 -0.77638096]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 784 is [False, False, True, True, False, False]
Scene graph at timestep 784 is [False, False, True, True, False, False]
State prediction error at timestep 784 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 784 of -1
Current timestep = 785. State = [[ 0.07872457 -0.14021511]]. Action = [[ 0.02735141 -0.24022923  0.13934499 -0.32338786]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 785 is [False, False, True, True, False, False]
Current timestep = 786. State = [[ 0.07850944 -0.14067926]]. Action = [[ 0.00161108  0.11499459  0.170342   -0.7548272 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 786 is [False, False, True, True, False, False]
Scene graph at timestep 786 is [False, False, True, True, False, False]
State prediction error at timestep 786 is tensor(2.2222e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 786 of -1
Current timestep = 787. State = [[ 0.0775526  -0.13245568]]. Action = [[-0.1131838   0.15167809  0.0031943   0.33191967]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 787 is [False, False, True, True, False, False]
Scene graph at timestep 787 is [False, False, True, True, False, False]
State prediction error at timestep 787 is tensor(1.6977e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 787 of 1
Current timestep = 788. State = [[ 0.07646779 -0.12318081]]. Action = [[ 0.19597453 -0.05563901  0.04738349 -0.3493421 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 788 is [False, False, True, True, False, False]
Current timestep = 789. State = [[ 0.07149015 -0.12727559]]. Action = [[-0.16508938 -0.0603698   0.05009368 -0.39618254]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 789 is [False, False, True, False, True, False]
Scene graph at timestep 789 is [False, False, True, True, False, False]
State prediction error at timestep 789 is tensor(3.3265e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 789 of -1
Current timestep = 790. State = [[ 0.0657209  -0.13307631]]. Action = [[ 0.03372338 -0.00740246  0.14055997  0.710896  ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 790 is [False, False, True, True, False, False]
Current timestep = 791. State = [[ 0.06562997 -0.12084346]]. Action = [[-0.01342919  0.2199825  -0.23665456  0.24651265]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 791 is [False, False, True, True, False, False]
Current timestep = 792. State = [[ 0.06304357 -0.10193886]]. Action = [[ 0.20012867 -0.10876977 -0.16921875 -0.4829446 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 792 is [False, False, True, False, True, False]
Scene graph at timestep 792 is [False, False, True, False, True, False]
State prediction error at timestep 792 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 792 of 1
Current timestep = 793. State = [[ 0.05469002 -0.10872531]]. Action = [[-0.18476288 -0.15615594  0.23608708 -0.946145  ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 793 is [False, False, True, False, True, False]
Current timestep = 794. State = [[ 0.03883433 -0.11632872]]. Action = [[ 0.16134918 -0.19612929  0.08378011 -0.7239296 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 794 is [False, False, True, False, True, False]
Scene graph at timestep 794 is [False, True, False, False, True, False]
State prediction error at timestep 794 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 794 of 1
Current timestep = 795. State = [[ 0.03689959 -0.11867509]]. Action = [[ 0.17972296  0.1865437  -0.11625876 -0.94335556]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 795 is [False, True, False, False, True, False]
Scene graph at timestep 795 is [False, True, False, False, True, False]
State prediction error at timestep 795 is tensor(1.3059e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 795 of 1
Current timestep = 796. State = [[ 0.02804783 -0.13209641]]. Action = [[-0.17626359 -0.19229355  0.06805933  0.42044544]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 796 is [False, True, False, False, True, False]
Scene graph at timestep 796 is [False, True, False, True, False, False]
State prediction error at timestep 796 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 796 of -1
Current timestep = 797. State = [[ 0.00811628 -0.15405002]]. Action = [[-0.08188954 -0.10846308 -0.0428585   0.13111377]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 797 is [False, True, False, True, False, False]
Current timestep = 798. State = [[ 0.00183671 -0.16032322]]. Action = [[-0.0057057   0.0398168  -0.20718801 -0.5109205 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 798 is [False, True, False, True, False, False]
Current timestep = 799. State = [[ 0.00190041 -0.16404082]]. Action = [[ 0.0847441  -0.09201244 -0.21954377 -0.55718887]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 799 is [False, True, False, True, False, False]
Scene graph at timestep 799 is [False, True, False, True, False, False]
State prediction error at timestep 799 is tensor(1.4834e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 799 of -1
Current timestep = 800. State = [[ 0.00086668 -0.1815404 ]]. Action = [[-0.02308348 -0.20045604 -0.21751288 -0.90833807]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 800 is [False, True, False, True, False, False]
Scene graph at timestep 800 is [False, True, False, True, False, False]
State prediction error at timestep 800 is tensor(8.9507e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 800 of -1
Current timestep = 801. State = [[ 0.0022285  -0.18743542]]. Action = [[ 0.23584348  0.2077766   0.09933814 -0.79908496]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 801 is [False, True, False, True, False, False]
Scene graph at timestep 801 is [False, True, False, True, False, False]
State prediction error at timestep 801 is tensor(7.2259e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 801 of 1
Current timestep = 802. State = [[ 0.00519667 -0.18063535]]. Action = [[-0.03634508 -0.13254426 -0.23148264 -0.2875799 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 802 is [False, True, False, True, False, False]
Scene graph at timestep 802 is [False, True, False, True, False, False]
State prediction error at timestep 802 is tensor(3.3354e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 802 of -1
Current timestep = 803. State = [[ 0.00597976 -0.19433166]]. Action = [[ 0.06210241 -0.14435549  0.22990209  0.9280808 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 803 is [False, True, False, True, False, False]
Scene graph at timestep 803 is [False, True, False, True, False, False]
State prediction error at timestep 803 is tensor(5.0457e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 803 of -1
Current timestep = 804. State = [[ 0.01691579 -0.20305254]]. Action = [[ 0.21537077  0.05775371  0.22025698 -0.63705194]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 804 is [False, True, False, True, False, False]
Scene graph at timestep 804 is [False, True, False, True, False, False]
State prediction error at timestep 804 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 804 of -1
Current timestep = 805. State = [[ 0.03850976 -0.21222584]]. Action = [[-0.05038384 -0.19443429 -0.10079762 -0.84309286]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 805 is [False, True, False, True, False, False]
Current timestep = 806. State = [[ 0.03653602 -0.21329449]]. Action = [[-0.1814363   0.24643928  0.11827213 -0.5435549 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 806 is [False, True, False, True, False, False]
Current timestep = 807. State = [[ 0.03514986 -0.20372775]]. Action = [[-0.02997053  0.01067135 -0.18459813 -0.27311957]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 807 is [False, True, False, True, False, False]
Current timestep = 808. State = [[ 0.03497431 -0.2016928 ]]. Action = [[ 0.22054622  0.21775186 -0.18706475 -0.21339512]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 808 is [False, True, False, True, False, False]
Current timestep = 809. State = [[ 0.0352746  -0.20184657]]. Action = [[ 0.16863614 -0.05926456 -0.16331059  0.95977545]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 809 is [False, True, False, True, False, False]
Current timestep = 810. State = [[ 0.03528798 -0.20183794]]. Action = [[ 0.233957   -0.01643072  0.16979241 -0.19076705]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 810 is [False, True, False, True, False, False]
Scene graph at timestep 810 is [False, True, False, True, False, False]
State prediction error at timestep 810 is tensor(4.6618e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 810 of -1
Current timestep = 811. State = [[ 0.03533057 -0.20122635]]. Action = [[-0.04861924  0.04318854  0.06033045 -0.3843695 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 811 is [False, True, False, True, False, False]
Scene graph at timestep 811 is [False, True, False, True, False, False]
State prediction error at timestep 811 is tensor(8.6860e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 811 of -1
Current timestep = 812. State = [[ 0.03533584 -0.20115018]]. Action = [[ 0.23662508  0.1361554   0.0920653  -0.07822603]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 812 is [False, True, False, True, False, False]
Scene graph at timestep 812 is [False, True, False, True, False, False]
State prediction error at timestep 812 is tensor(6.0314e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 812 of -1
Current timestep = 813. State = [[ 0.03533584 -0.20115018]]. Action = [[0.21189272 0.16685098 0.02696082 0.5498278 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 813 is [False, True, False, True, False, False]
Scene graph at timestep 813 is [False, True, False, True, False, False]
State prediction error at timestep 813 is tensor(3.2867e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 813 of -1
Current timestep = 814. State = [[ 0.03508271 -0.20134725]]. Action = [[-0.11679602  0.00470605  0.23583752 -0.00039893]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 814 is [False, True, False, True, False, False]
Current timestep = 815. State = [[ 0.03476412 -0.20270969]]. Action = [[ 0.0232555  -0.04003145 -0.23256075 -0.6140462 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 815 is [False, True, False, True, False, False]
Current timestep = 816. State = [[ 0.03464835 -0.2031722 ]]. Action = [[0.23409909 0.09957951 0.22853601 0.9068049 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 816 is [False, True, False, True, False, False]
Scene graph at timestep 816 is [False, True, False, True, False, False]
State prediction error at timestep 816 is tensor(1.2942e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 816 of -1
Current timestep = 817. State = [[ 0.03468677 -0.20330988]]. Action = [[ 0.143543    0.00664452 -0.15586741  0.14489067]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 817 is [False, True, False, True, False, False]
Current timestep = 818. State = [[ 0.04112812 -0.19155578]]. Action = [[ 0.15262926  0.17602575  0.04753351 -0.7940599 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 818 is [False, True, False, True, False, False]
Scene graph at timestep 818 is [False, True, False, True, False, False]
State prediction error at timestep 818 is tensor(6.1237e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 818 of 1
Current timestep = 819. State = [[ 0.04175582 -0.18985943]]. Action = [[-0.1212713  -0.22657609  0.01180938 -0.8714207 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 819 is [False, True, False, True, False, False]
Scene graph at timestep 819 is [False, True, False, True, False, False]
State prediction error at timestep 819 is tensor(1.0014e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 819 of -1
Current timestep = 820. State = [[ 0.04095592 -0.21598752]]. Action = [[ 0.08826122 -0.23136356  0.12447029  0.4252876 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 820 is [False, True, False, True, False, False]
Current timestep = 821. State = [[ 0.03942163 -0.23333871]]. Action = [[ 0.1524604  -0.02784601 -0.15601926 -0.8906965 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 821 is [False, True, False, True, False, False]
Current timestep = 822. State = [[ 0.0457124  -0.22682495]]. Action = [[ 0.11559698  0.15604079  0.2127139  -0.8351934 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 822 is [False, True, False, True, False, False]
Scene graph at timestep 822 is [False, True, False, True, False, False]
State prediction error at timestep 822 is tensor(5.1595e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 822 of -1
Current timestep = 823. State = [[ 0.05912887 -0.22129886]]. Action = [[ 0.0487287   0.24283177 -0.21126626 -0.8909717 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 823 is [False, True, False, True, False, False]
Scene graph at timestep 823 is [False, False, True, True, False, False]
State prediction error at timestep 823 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 823 of -1
Current timestep = 824. State = [[ 0.06201494 -0.20742814]]. Action = [[-0.00130299  0.24488086  0.1978462  -0.55445176]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 824 is [False, False, True, True, False, False]
Current timestep = 825. State = [[ 0.06312267 -0.1894978 ]]. Action = [[ 0.21369678 -0.10903913  0.1368939   0.8289814 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 825 is [False, False, True, True, False, False]
Scene graph at timestep 825 is [False, False, True, True, False, False]
State prediction error at timestep 825 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 825 of 1
Current timestep = 826. State = [[ 0.06329585 -0.1864632 ]]. Action = [[ 0.15923887 -0.10434031 -0.02774386 -0.7426413 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 826 is [False, False, True, True, False, False]
Scene graph at timestep 826 is [False, False, True, True, False, False]
State prediction error at timestep 826 is tensor(1.5164e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 826 of 1
Current timestep = 827. State = [[ 0.06329585 -0.1864632 ]]. Action = [[0.24003077 0.10312468 0.2351386  0.3591287 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 827 is [False, False, True, True, False, False]
Current timestep = 828. State = [[ 0.06329585 -0.1864632 ]]. Action = [[ 0.08904222 -0.18724841 -0.18004629  0.46148705]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 828 is [False, False, True, True, False, False]
Current timestep = 829. State = [[ 0.06329585 -0.1864632 ]]. Action = [[0.1722303  0.02542278 0.07656693 0.1950401 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 829 is [False, False, True, True, False, False]
Scene graph at timestep 829 is [False, False, True, True, False, False]
State prediction error at timestep 829 is tensor(7.0491e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 829 of 1
Current timestep = 830. State = [[ 0.06329585 -0.1864632 ]]. Action = [[0.05369955 0.08935815 0.18006009 0.4912789 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 830 is [False, False, True, True, False, False]
Scene graph at timestep 830 is [False, False, True, True, False, False]
State prediction error at timestep 830 is tensor(3.6802e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 830 of 1
Current timestep = 831. State = [[ 0.06058108 -0.18120904]]. Action = [[-0.23904379  0.10710049 -0.14912242 -0.2696061 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 831 is [False, False, True, True, False, False]
Scene graph at timestep 831 is [False, False, True, True, False, False]
State prediction error at timestep 831 is tensor(5.7315e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 831 of 1
Current timestep = 832. State = [[ 0.05671583 -0.1597998 ]]. Action = [[ 0.04025811  0.2140804  -0.21323287  0.1352179 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 832 is [False, False, True, True, False, False]
Current timestep = 833. State = [[ 0.05442103 -0.14389671]]. Action = [[-0.10097741 -0.0088395  -0.11602663 -0.5311882 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 833 is [False, False, True, True, False, False]
Scene graph at timestep 833 is [False, False, True, True, False, False]
State prediction error at timestep 833 is tensor(8.0179e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 833 of 1
Current timestep = 834. State = [[ 0.04465907 -0.13793167]]. Action = [[-0.17305432  0.07449228 -0.08263776  0.73517156]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 834 is [False, False, True, True, False, False]
Current timestep = 835. State = [[ 0.03074918 -0.13079827]]. Action = [[ 0.09717277  0.00215611 -0.15725312  0.96651924]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 835 is [False, True, False, True, False, False]
Current timestep = 836. State = [[ 0.02489571 -0.14048235]]. Action = [[-0.13995652 -0.1750185   0.15378284  0.54819727]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 836 is [False, True, False, True, False, False]
Current timestep = 837. State = [[ 0.00946266 -0.13888092]]. Action = [[-0.23116235  0.21696007 -0.1582391   0.31549776]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 837 is [False, True, False, True, False, False]
Scene graph at timestep 837 is [False, True, False, True, False, False]
State prediction error at timestep 837 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 837 of 1
Current timestep = 838. State = [[-0.01099627 -0.12743825]]. Action = [[ 0.20431691 -0.10201007 -0.0706749   0.8036089 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 838 is [False, True, False, True, False, False]
Scene graph at timestep 838 is [False, True, False, True, False, False]
State prediction error at timestep 838 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 838 of 1
Current timestep = 839. State = [[ 0.0002199  -0.12006851]]. Action = [[ 0.21794188  0.18590215  0.20761257 -0.47842717]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 839 is [False, True, False, True, False, False]
Scene graph at timestep 839 is [False, True, False, False, True, False]
State prediction error at timestep 839 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 839 of 1
Current timestep = 840. State = [[-0.25855875  0.00150306]]. Action = [[ 0.11012208  0.19960415  0.05250242 -0.4856481 ]]. Reward = [100.]
Curr episode timestep = 96
Scene graph at timestep 840 is [False, True, False, False, True, False]
Scene graph at timestep 840 is [True, False, False, False, True, False]
State prediction error at timestep 840 is tensor(0.0384, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 840 of 0
Current timestep = 841. State = [[-0.24790302  0.01338628]]. Action = [[ 0.24084294  0.22714001 -0.0471157  -0.62824714]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 841 is [True, False, False, False, True, False]
Scene graph at timestep 841 is [True, False, False, False, True, False]
State prediction error at timestep 841 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 841 of 1
Current timestep = 842. State = [[-0.22124322  0.03032432]]. Action = [[0.2376805  0.037157   0.19398713 0.4470582 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 842 is [True, False, False, False, True, False]
Scene graph at timestep 842 is [True, False, False, False, True, False]
State prediction error at timestep 842 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 842 of 1
Current timestep = 843. State = [[-0.18572745  0.02184023]]. Action = [[ 0.20340687 -0.22789161 -0.01652318  0.04252946]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 843 is [True, False, False, False, True, False]
Current timestep = 844. State = [[-0.17428584  0.00817024]]. Action = [[-0.12595555 -0.05671227  0.04080108  0.13728333]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 844 is [True, False, False, False, True, False]
Scene graph at timestep 844 is [True, False, False, False, True, False]
State prediction error at timestep 844 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 844 of 1
Current timestep = 845. State = [[-0.1754399  -0.00112714]]. Action = [[ 0.06443971 -0.01291186 -0.09093329 -0.85678756]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 845 is [True, False, False, False, True, False]
Scene graph at timestep 845 is [True, False, False, False, True, False]
State prediction error at timestep 845 is tensor(7.3670e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 845 of 1
Current timestep = 846. State = [[-0.17942491 -0.01404751]]. Action = [[-0.21601036 -0.18187463  0.17299962  0.64403534]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 846 is [True, False, False, False, True, False]
Scene graph at timestep 846 is [True, False, False, False, True, False]
State prediction error at timestep 846 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 846 of -1
Current timestep = 847. State = [[-0.18295515 -0.01954888]]. Action = [[ 0.21568936  0.19041455  0.14719722 -0.28585124]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 847 is [True, False, False, False, True, False]
Current timestep = 848. State = [[-0.16532226 -0.00558122]]. Action = [[ 0.22785419  0.04594314  0.02525055 -0.76143456]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 848 is [True, False, False, False, True, False]
Scene graph at timestep 848 is [True, False, False, False, True, False]
State prediction error at timestep 848 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 848 of 1
Current timestep = 849. State = [[-0.13463315  0.01219764]]. Action = [[ 0.14719328  0.20393747  0.09270909 -0.54532707]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 849 is [True, False, False, False, True, False]
Scene graph at timestep 849 is [True, False, False, False, True, False]
State prediction error at timestep 849 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 849 of 1
Current timestep = 850. State = [[-0.11261629  0.02869773]]. Action = [[ 0.14638993 -0.03322107 -0.20916818  0.8597255 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 850 is [True, False, False, False, True, False]
Scene graph at timestep 850 is [True, False, False, False, True, False]
State prediction error at timestep 850 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 850 of 1
Current timestep = 851. State = [[-0.09538571  0.01610571]]. Action = [[ 0.03915438 -0.20867166 -0.04131508  0.9179659 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 851 is [True, False, False, False, True, False]
Scene graph at timestep 851 is [True, False, False, False, True, False]
State prediction error at timestep 851 is tensor(8.8198e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 851 of 1
Current timestep = 852. State = [[-0.08392399  0.00151758]]. Action = [[ 0.2025984   0.00491396 -0.10051098  0.34902334]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 852 is [True, False, False, False, True, False]
Current timestep = 853. State = [[-0.06406702 -0.01314605]]. Action = [[ 0.04550764 -0.22493726 -0.11492324 -0.11379158]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 853 is [True, False, False, False, True, False]
Current timestep = 854. State = [[-0.0486062 -0.0254695]]. Action = [[ 0.22095871  0.03649548  0.16805542 -0.95000553]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 854 is [True, False, False, False, True, False]
Current timestep = 855. State = [[-0.1554971  -0.07680537]]. Action = [[ 0.23802632 -0.207783   -0.15630767 -0.8739231 ]]. Reward = [100.]
Curr episode timestep = 14
Scene graph at timestep 855 is [False, True, False, False, True, False]
Current timestep = 856. State = [[-0.1319238  -0.07438213]]. Action = [[0.16696551 0.20543486 0.18646336 0.09945393]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 856 is [True, False, False, False, True, False]
Scene graph at timestep 856 is [True, False, False, False, True, False]
State prediction error at timestep 856 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 856 of 1
Current timestep = 857. State = [[-0.10415876 -0.06458313]]. Action = [[ 0.24708337 -0.06977451  0.23902178 -0.09654295]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 857 is [True, False, False, False, True, False]
Scene graph at timestep 857 is [True, False, False, False, True, False]
State prediction error at timestep 857 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 857 of 1
Current timestep = 858. State = [[-0.08098568 -0.07271685]]. Action = [[-0.19570667 -0.05845995 -0.22871321  0.0093075 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 858 is [True, False, False, False, True, False]
Current timestep = 859. State = [[-0.08134058 -0.07939359]]. Action = [[ 0.18193495 -0.03841776  0.02557725 -0.01323313]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 859 is [True, False, False, False, True, False]
Current timestep = 860. State = [[-0.07217021 -0.07976341]]. Action = [[ 0.19036502  0.02208236  0.15973353 -0.5547093 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 860 is [True, False, False, False, True, False]
Current timestep = 861. State = [[-0.05719243 -0.07665616]]. Action = [[-0.01507609  0.07745406  0.19303328 -0.76956165]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 861 is [True, False, False, False, True, False]
Current timestep = 862. State = [[-0.04715869 -0.06078951]]. Action = [[ 0.201637    0.19330776 -0.18320632 -0.42341268]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 862 is [True, False, False, False, True, False]
Current timestep = 863. State = [[-0.22925565 -0.16812903]]. Action = [[ 0.23387939  0.24088535  0.16273847 -0.5665778 ]]. Reward = [100.]
Curr episode timestep = 7
Scene graph at timestep 863 is [False, True, False, False, True, False]
Scene graph at timestep 863 is [True, False, False, True, False, False]
State prediction error at timestep 863 is tensor(0.0221, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 863 of -1
Current timestep = 864. State = [[-0.22193797 -0.17940564]]. Action = [[ 0.06567314  0.12887245 -0.15117629  0.19782043]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 864 is [True, False, False, True, False, False]
Scene graph at timestep 864 is [True, False, False, True, False, False]
State prediction error at timestep 864 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 864 of 1
Current timestep = 865. State = [[-0.20425346 -0.15763526]]. Action = [[ 0.23409194  0.22891307 -0.11438403 -0.37899423]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 865 is [True, False, False, True, False, False]
Scene graph at timestep 865 is [True, False, False, True, False, False]
State prediction error at timestep 865 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 865 of 1
Current timestep = 866. State = [[-0.17380852 -0.14707148]]. Action = [[ 0.21592808 -0.12839185 -0.15280703 -0.7786293 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 866 is [True, False, False, True, False, False]
Current timestep = 867. State = [[-0.1519391  -0.16056906]]. Action = [[ 0.03016663 -0.11275958 -0.19051385  0.09734535]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 867 is [True, False, False, True, False, False]
Current timestep = 868. State = [[-0.1394772  -0.15466462]]. Action = [[0.1668433  0.23281395 0.11585623 0.5953393 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 868 is [True, False, False, True, False, False]
Current timestep = 869. State = [[-0.12448733 -0.14059092]]. Action = [[ 0.01937348  0.07300475 -0.01429665 -0.35292828]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 869 is [True, False, False, True, False, False]
Scene graph at timestep 869 is [True, False, False, True, False, False]
State prediction error at timestep 869 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 869 of 1
Current timestep = 870. State = [[-0.11051202 -0.12783022]]. Action = [[ 0.17379913  0.05573931  0.1098963  -0.8022568 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 870 is [True, False, False, True, False, False]
Current timestep = 871. State = [[-0.10129695 -0.11207388]]. Action = [[-0.15149727  0.23301545  0.07737941 -0.5361453 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 871 is [True, False, False, True, False, False]
Current timestep = 872. State = [[-0.10209049 -0.08015919]]. Action = [[0.08394045 0.23295528 0.23259616 0.36795485]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 872 is [True, False, False, False, True, False]
Current timestep = 873. State = [[-0.09793895 -0.05300493]]. Action = [[ 0.11954921  0.11511946  0.04676548 -0.23613751]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 873 is [True, False, False, False, True, False]
Current timestep = 874. State = [[-0.08756719 -0.03438998]]. Action = [[ 0.00773853  0.10969305 -0.07188976  0.47885418]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 874 is [True, False, False, False, True, False]
Scene graph at timestep 874 is [True, False, False, False, True, False]
State prediction error at timestep 874 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 874 of 1
Current timestep = 875. State = [[-0.07750274 -0.03769244]]. Action = [[ 0.15993127 -0.24688402  0.14569932 -0.6766979 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 875 is [True, False, False, False, True, False]
Current timestep = 876. State = [[-0.06811289 -0.04304092]]. Action = [[-0.11164196  0.12897205  0.22985825 -0.7902369 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 876 is [True, False, False, False, True, False]
Scene graph at timestep 876 is [True, False, False, False, True, False]
State prediction error at timestep 876 is tensor(2.9272e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 876 of 1
Current timestep = 877. State = [[-0.06944865 -0.05174416]]. Action = [[ 0.01484346 -0.21071447 -0.22871609  0.7819109 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 877 is [True, False, False, False, True, False]
Scene graph at timestep 877 is [True, False, False, False, True, False]
State prediction error at timestep 877 is tensor(6.2942e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 877 of -1
Current timestep = 878. State = [[-0.0728054  -0.06481896]]. Action = [[-0.13884306  0.02454042  0.03633919 -0.9393586 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 878 is [True, False, False, False, True, False]
Current timestep = 879. State = [[-0.07395952 -0.07900912]]. Action = [[ 0.10846388 -0.23989609 -0.11537504 -0.80512226]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 879 is [True, False, False, False, True, False]
Current timestep = 880. State = [[-0.06626733 -0.07904002]]. Action = [[0.20126423 0.23696339 0.12030953 0.6385994 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 880 is [True, False, False, False, True, False]
Current timestep = 881. State = [[-0.05112952 -0.06999007]]. Action = [[ 0.0242748   0.00038543 -0.21361443 -0.14572376]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 881 is [True, False, False, False, True, False]
Scene graph at timestep 881 is [True, False, False, False, True, False]
State prediction error at timestep 881 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 881 of 1
Current timestep = 882. State = [[-0.05005626 -0.08196311]]. Action = [[-0.23871297 -0.23598415  0.02920505 -0.615188  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 882 is [True, False, False, False, True, False]
Scene graph at timestep 882 is [True, False, False, False, True, False]
State prediction error at timestep 882 is tensor(3.0916e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 882 of -1
Current timestep = 883. State = [[-0.05547188 -0.09502631]]. Action = [[ 0.07930163  0.08682317 -0.11285174  0.5446241 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 883 is [True, False, False, False, True, False]
Scene graph at timestep 883 is [True, False, False, False, True, False]
State prediction error at timestep 883 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 883 of -1
Current timestep = 884. State = [[-0.0536455 -0.0802995]]. Action = [[ 0.08961731  0.149647   -0.21109541 -0.7694864 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 884 is [True, False, False, False, True, False]
Current timestep = 885. State = [[-0.05288244 -0.08086994]]. Action = [[-0.00731947 -0.18101162 -0.2310866   0.03206456]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 885 is [True, False, False, False, True, False]
Scene graph at timestep 885 is [True, False, False, False, True, False]
State prediction error at timestep 885 is tensor(3.3143e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 885 of -1
Current timestep = 886. State = [[-0.05185463 -0.09752069]]. Action = [[ 0.02184793 -0.16271242  0.14979795 -0.02083385]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 886 is [True, False, False, False, True, False]
Current timestep = 887. State = [[-0.04798662 -0.11688936]]. Action = [[ 0.07298899 -0.11711276 -0.06458019 -0.24934918]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 887 is [True, False, False, False, True, False]
Scene graph at timestep 887 is [False, True, False, False, True, False]
State prediction error at timestep 887 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 887 of 1
Current timestep = 888. State = [[-0.03663089 -0.11692964]]. Action = [[ 0.18289456  0.18904597 -0.11924836 -0.36170435]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 888 is [False, True, False, False, True, False]
Scene graph at timestep 888 is [False, True, False, False, True, False]
State prediction error at timestep 888 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 888 of 1
Current timestep = 889. State = [[-0.01105137 -0.11965928]]. Action = [[ 0.13605928 -0.22760431 -0.09098938  0.03034604]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 889 is [False, True, False, False, True, False]
Current timestep = 890. State = [[ 0.0023871  -0.12471054]]. Action = [[ 0.00676638  0.12276575  0.21383178 -0.0864628 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 890 is [False, True, False, False, True, False]
Scene graph at timestep 890 is [False, True, False, False, True, False]
State prediction error at timestep 890 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 890 of 1
Current timestep = 891. State = [[ 0.00422717 -0.1093785 ]]. Action = [[-0.18396574  0.23460943  0.14480633 -0.68020815]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 891 is [False, True, False, False, True, False]
Current timestep = 892. State = [[-0.15886489  0.02746712]]. Action = [[-0.10508168 -0.12875786 -0.12865165 -0.6257499 ]]. Reward = [100.]
Curr episode timestep = 28
Scene graph at timestep 892 is [False, True, False, False, True, False]
Current timestep = 893. State = [[-0.13485502  0.03642555]]. Action = [[ 0.21478641  0.00643659 -0.22126363  0.75836337]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 893 is [True, False, False, False, True, False]
Current timestep = 894. State = [[-0.11271752  0.02689108]]. Action = [[ 0.01923737 -0.18444999 -0.11336946 -0.57553864]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 894 is [True, False, False, False, True, False]
Current timestep = 895. State = [[-0.09855887  0.01063717]]. Action = [[ 0.20824859 -0.09004742 -0.06682298 -0.9668085 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 895 is [True, False, False, False, True, False]
Scene graph at timestep 895 is [True, False, False, False, True, False]
State prediction error at timestep 895 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 895 of 1
Current timestep = 896. State = [[-0.06712504  0.00923393]]. Action = [[ 0.23603529  0.14958751 -0.17923595  0.04789257]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 896 is [True, False, False, False, True, False]
Current timestep = 897. State = [[-0.03909856  0.00788948]]. Action = [[ 0.16041285 -0.14710027  0.18062866  0.21336532]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 897 is [True, False, False, False, True, False]
Current timestep = 898. State = [[-0.19507082  0.09267706]]. Action = [[-0.09675822  0.12599608  0.04513761 -0.39165485]]. Reward = [100.]
Curr episode timestep = 5
Scene graph at timestep 898 is [False, True, False, False, True, False]
Scene graph at timestep 898 is [True, False, False, False, True, False]
State prediction error at timestep 898 is tensor(0.0153, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 898 of 1
Current timestep = 899. State = [[-0.17273521  0.1149107 ]]. Action = [[ 0.20601737  0.1832729  -0.02122548 -0.9445326 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 899 is [True, False, False, False, True, False]
Scene graph at timestep 899 is [True, False, False, False, True, False]
State prediction error at timestep 899 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 899 of -1
Current timestep = 900. State = [[-0.14770095  0.11761943]]. Action = [[ 0.075221   -0.24471296 -0.10153586 -0.7389038 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 900 is [True, False, False, False, True, False]
Current timestep = 901. State = [[-0.13783102  0.09757802]]. Action = [[ 0.10232389 -0.10561025  0.19136554 -0.37673068]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 901 is [True, False, False, False, True, False]
Current timestep = 902. State = [[-0.12028529  0.07756945]]. Action = [[ 0.17496103 -0.17548488  0.2012822   0.45970583]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 902 is [True, False, False, False, True, False]
Scene graph at timestep 902 is [True, False, False, False, True, False]
State prediction error at timestep 902 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 902 of 1
Current timestep = 903. State = [[-0.09232216  0.05510356]]. Action = [[ 0.19426697 -0.12142569 -0.13512692  0.88179576]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 903 is [True, False, False, False, True, False]
Current timestep = 904. State = [[-0.07319869  0.0569219 ]]. Action = [[ 0.08740175  0.1874826  -0.0370996  -0.04026735]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 904 is [True, False, False, False, True, False]
Scene graph at timestep 904 is [True, False, False, False, True, False]
State prediction error at timestep 904 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 904 of 1
Current timestep = 905. State = [[-0.06026619  0.07630694]]. Action = [[-0.01917508  0.16837686 -0.15960339 -0.05507296]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 905 is [True, False, False, False, True, False]
Current timestep = 906. State = [[-0.06199213  0.08620397]]. Action = [[ 0.0158278  -0.01860653 -0.20484774 -0.33099228]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 906 is [True, False, False, False, True, False]
Current timestep = 907. State = [[-0.05107573  0.07895112]]. Action = [[ 0.24437028 -0.15678136 -0.2265714  -0.5081828 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 907 is [True, False, False, False, True, False]
Current timestep = 908. State = [[-0.19403672 -0.04229466]]. Action = [[-0.11144266 -0.11379926 -0.12461233  0.33294797]]. Reward = [100.]
Curr episode timestep = 9
Scene graph at timestep 908 is [True, False, False, False, True, False]
Current timestep = 909. State = [[-0.1783048  -0.04421059]]. Action = [[ 0.10853103  0.0937435  -0.21717055 -0.8643818 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 909 is [True, False, False, False, True, False]
Scene graph at timestep 909 is [True, False, False, False, True, False]
State prediction error at timestep 909 is tensor(9.7091e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 909 of 1
Current timestep = 910. State = [[-0.1687398  -0.04415724]]. Action = [[-0.1762574  -0.06021628  0.16820711 -0.24317145]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 910 is [True, False, False, False, True, False]
Current timestep = 911. State = [[-0.17633    -0.03628938]]. Action = [[-0.10861385  0.17939252 -0.1503061  -0.8144229 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 911 is [True, False, False, False, True, False]
Scene graph at timestep 911 is [True, False, False, False, True, False]
State prediction error at timestep 911 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 911 of -1
Current timestep = 912. State = [[-0.18003951 -0.01650131]]. Action = [[0.24479848 0.10951385 0.02340823 0.6579999 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 912 is [True, False, False, False, True, False]
Scene graph at timestep 912 is [True, False, False, False, True, False]
State prediction error at timestep 912 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 912 of 1
Current timestep = 913. State = [[-0.16787682 -0.00792635]]. Action = [[0.10950005 0.04275525 0.15558872 0.75485444]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 913 is [True, False, False, False, True, False]
Current timestep = 914. State = [[-0.15979044 -0.01568992]]. Action = [[-0.08178879 -0.23639211 -0.23057097  0.6137564 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 914 is [True, False, False, False, True, False]
Scene graph at timestep 914 is [True, False, False, False, True, False]
State prediction error at timestep 914 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 914 of 1
Current timestep = 915. State = [[-0.15519018 -0.01701174]]. Action = [[0.17616105 0.22714466 0.1254611  0.03485072]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 915 is [True, False, False, False, True, False]
Scene graph at timestep 915 is [True, False, False, False, True, False]
State prediction error at timestep 915 is tensor(2.4867e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 915 of 1
Current timestep = 916. State = [[-0.13380753 -0.01188635]]. Action = [[ 0.21129036 -0.17134017  0.00538981 -0.8491296 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 916 is [True, False, False, False, True, False]
Current timestep = 917. State = [[-0.11043373 -0.01868048]]. Action = [[ 0.10466143  0.0169332  -0.01161116 -0.8346694 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 917 is [True, False, False, False, True, False]
Current timestep = 918. State = [[-0.09626058 -0.00740861]]. Action = [[ 0.03653005  0.21646863 -0.22787237 -0.7844321 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 918 is [True, False, False, False, True, False]
Scene graph at timestep 918 is [True, False, False, False, True, False]
State prediction error at timestep 918 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 918 of 1
Current timestep = 919. State = [[-0.08685187  0.00146372]]. Action = [[ 0.07837918 -0.09084813 -0.04466841 -0.14601773]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 919 is [True, False, False, False, True, False]
Scene graph at timestep 919 is [True, False, False, False, True, False]
State prediction error at timestep 919 is tensor(8.2508e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 919 of 1
Current timestep = 920. State = [[-0.07519951 -0.01425163]]. Action = [[ 0.07559618 -0.18407598 -0.20791735 -0.25686216]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 920 is [True, False, False, False, True, False]
Scene graph at timestep 920 is [True, False, False, False, True, False]
State prediction error at timestep 920 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 920 of 1
Current timestep = 921. State = [[-0.0715212  -0.02196458]]. Action = [[-0.20189542  0.10204735 -0.10256921 -0.5285864 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 921 is [True, False, False, False, True, False]
Scene graph at timestep 921 is [True, False, False, False, True, False]
State prediction error at timestep 921 is tensor(6.3421e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 921 of -1
Current timestep = 922. State = [[-0.0707102  -0.02260963]]. Action = [[ 0.2004202  -0.09970765 -0.15221569  0.02707458]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 922 is [True, False, False, False, True, False]
Scene graph at timestep 922 is [True, False, False, False, True, False]
State prediction error at timestep 922 is tensor(4.3816e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 922 of 1
Current timestep = 923. State = [[-0.06194837 -0.02018052]]. Action = [[ 0.16300404  0.14709103  0.00817186 -0.89501256]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 923 is [True, False, False, False, True, False]
Current timestep = 924. State = [[-0.04833701 -0.01170511]]. Action = [[-0.0267365   0.03840202 -0.18149655 -0.26055294]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 924 is [True, False, False, False, True, False]
Scene graph at timestep 924 is [False, True, False, False, True, False]
State prediction error at timestep 924 is tensor(7.5289e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 924 of 1
Current timestep = 925. State = [[-0.04855735  0.00387763]]. Action = [[5.7515502e-04 1.6464579e-01 1.6987467e-01 7.6906526e-01]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 925 is [False, True, False, False, True, False]
Scene graph at timestep 925 is [False, True, False, False, True, False]
State prediction error at timestep 925 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 925 of 1
Current timestep = 926. State = [[-0.0477357  0.0131556]]. Action = [[-0.05796219 -0.08170366  0.15948522  0.15591717]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 926 is [False, True, False, False, True, False]
Scene graph at timestep 926 is [False, True, False, False, True, False]
State prediction error at timestep 926 is tensor(1.8702e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 926 of 1
Current timestep = 927. State = [[-0.0404873   0.00873003]]. Action = [[ 0.24016476 -0.01578495 -0.20524633  0.29329538]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 927 is [False, True, False, False, True, False]
Scene graph at timestep 927 is [False, True, False, False, True, False]
State prediction error at timestep 927 is tensor(1.0732e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 927 of 1
Current timestep = 928. State = [[-0.15308881  0.20109282]]. Action = [[ 0.23226312  0.01486596  0.12391466 -0.7467974 ]]. Reward = [100.]
Curr episode timestep = 19
Scene graph at timestep 928 is [False, True, False, False, True, False]
Current timestep = 929. State = [[-0.1317749   0.22375451]]. Action = [[ 0.06225544 -0.03588471 -0.22131091 -0.51739466]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 929 is [True, False, False, False, False, True]
Current timestep = 930. State = [[-0.12219036  0.21558385]]. Action = [[ 0.00726202 -0.14669254  0.21091169  0.403448  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 930 is [True, False, False, False, False, True]
Current timestep = 931. State = [[-0.11889597  0.21039987]]. Action = [[ 0.0719685   0.09611741 -0.17635773 -0.2034586 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 931 is [True, False, False, False, False, True]
Current timestep = 932. State = [[-0.10294084  0.20061073]]. Action = [[ 0.19276014 -0.21752483 -0.06829518 -0.45214176]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 932 is [True, False, False, False, False, True]
Current timestep = 933. State = [[-0.08092742  0.19446743]]. Action = [[ 0.13271433  0.09024251 -0.0633508  -0.08300704]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 933 is [True, False, False, False, False, True]
Scene graph at timestep 933 is [True, False, False, False, False, True]
State prediction error at timestep 933 is tensor(8.1071e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 933 of 1
Current timestep = 934. State = [[-0.04957162  0.18476208]]. Action = [[ 0.20567924 -0.23178227  0.21411723 -0.8297163 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 934 is [True, False, False, False, False, True]
Scene graph at timestep 934 is [False, True, False, False, False, True]
State prediction error at timestep 934 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 934 of 1
Current timestep = 935. State = [[-0.02023963  0.15394212]]. Action = [[ 0.22770116 -0.23146501 -0.04632381 -0.73817897]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 935 is [False, True, False, False, False, True]
Current timestep = 936. State = [[0.00769412 0.12366117]]. Action = [[ 0.18265116 -0.23759605 -0.09804231 -0.9487089 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 936 is [False, True, False, False, False, True]
Scene graph at timestep 936 is [False, True, False, False, True, False]
State prediction error at timestep 936 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 936 of 1
Current timestep = 937. State = [[-0.2023573  -0.04612894]]. Action = [[-0.15865164 -0.17637846 -0.0469157  -0.00714821]]. Reward = [100.]
Curr episode timestep = 8
Scene graph at timestep 937 is [False, True, False, False, True, False]
Scene graph at timestep 937 is [True, False, False, False, True, False]
State prediction error at timestep 937 is tensor(0.0376, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 937 of 1
Current timestep = 938. State = [[-0.19546829 -0.06532934]]. Action = [[-0.10192817 -0.2137036  -0.01983923  0.05958796]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 938 is [True, False, False, False, True, False]
Scene graph at timestep 938 is [True, False, False, False, True, False]
State prediction error at timestep 938 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 938 of -1
Current timestep = 939. State = [[-0.18855843 -0.08432917]]. Action = [[ 0.24481064 -0.02664141 -0.04650563  0.7910352 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 939 is [True, False, False, False, True, False]
Scene graph at timestep 939 is [True, False, False, False, True, False]
State prediction error at timestep 939 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 939 of 1
Current timestep = 940. State = [[-0.16620654 -0.09587996]]. Action = [[ 0.11331755 -0.14627676 -0.10695362 -0.29685068]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 940 is [True, False, False, False, True, False]
Current timestep = 941. State = [[-0.14407285 -0.09782045]]. Action = [[0.24710536 0.16113704 0.22605246 0.9572885 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 941 is [True, False, False, False, True, False]
Scene graph at timestep 941 is [True, False, False, False, True, False]
State prediction error at timestep 941 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 941 of 1
Current timestep = 942. State = [[-0.10812378 -0.08966887]]. Action = [[ 0.20452523  0.02178237 -0.11556333  0.47657216]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 942 is [True, False, False, False, True, False]
Scene graph at timestep 942 is [True, False, False, False, True, False]
State prediction error at timestep 942 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 942 of 1
Current timestep = 943. State = [[-0.08292824 -0.08258032]]. Action = [[ 0.10047784  0.08558798 -0.23226117  0.51335526]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 943 is [True, False, False, False, True, False]
Current timestep = 944. State = [[-0.07356356 -0.07811549]]. Action = [[ 0.01550373  0.02004853 -0.14350705 -0.8698575 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 944 is [True, False, False, False, True, False]
Scene graph at timestep 944 is [True, False, False, False, True, False]
State prediction error at timestep 944 is tensor(6.1588e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 944 of 1
Current timestep = 945. State = [[-0.07282925 -0.06296317]]. Action = [[-0.15525484  0.20360583  0.05398104 -0.39162982]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 945 is [True, False, False, False, True, False]
Current timestep = 946. State = [[-0.07089007 -0.04492576]]. Action = [[ 0.20777589  0.0382393  -0.170453    0.79544926]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 946 is [True, False, False, False, True, False]
Scene graph at timestep 946 is [True, False, False, False, True, False]
State prediction error at timestep 946 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 946 of 1
Current timestep = 947. State = [[-0.06247929 -0.045371  ]]. Action = [[ 0.00358909 -0.12693751  0.09588915  0.41119754]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 947 is [True, False, False, False, True, False]
Current timestep = 948. State = [[-0.06304095 -0.0526164 ]]. Action = [[-0.09267503 -0.02478909  0.18862408 -0.32851553]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 948 is [True, False, False, False, True, False]
Current timestep = 949. State = [[-0.06553023 -0.04293283]]. Action = [[-0.04039021  0.22243369  0.02202991  0.43827927]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 949 is [True, False, False, False, True, False]
Current timestep = 950. State = [[-0.07242811 -0.02544908]]. Action = [[-0.13617963  0.0648545  -0.1734078  -0.9130284 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 950 is [True, False, False, False, True, False]
Scene graph at timestep 950 is [True, False, False, False, True, False]
State prediction error at timestep 950 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 950 of -1
Current timestep = 951. State = [[-0.07543593 -0.0254523 ]]. Action = [[ 0.10549143 -0.14412665 -0.24400036 -0.06985456]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 951 is [True, False, False, False, True, False]
Scene graph at timestep 951 is [True, False, False, False, True, False]
State prediction error at timestep 951 is tensor(2.4981e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 951 of -1
Current timestep = 952. State = [[-0.07125483 -0.03937552]]. Action = [[ 0.13383335 -0.07082826  0.03191182 -0.47752792]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 952 is [True, False, False, False, True, False]
Current timestep = 953. State = [[-0.0683037  -0.04587461]]. Action = [[-0.07864586 -0.03811619  0.19254628  0.05636764]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 953 is [True, False, False, False, True, False]
Scene graph at timestep 953 is [True, False, False, False, True, False]
State prediction error at timestep 953 is tensor(4.6781e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 953 of 1
Current timestep = 954. State = [[-0.06674705 -0.05934624]]. Action = [[ 0.11464009 -0.15543018  0.01243997  0.5938319 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 954 is [True, False, False, False, True, False]
Scene graph at timestep 954 is [True, False, False, False, True, False]
State prediction error at timestep 954 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 954 of -1
Current timestep = 955. State = [[-0.0541184  -0.06183893]]. Action = [[ 0.21948439  0.19249207  0.23723769 -0.83973664]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 955 is [True, False, False, False, True, False]
Current timestep = 956. State = [[-0.03814914 -0.05371818]]. Action = [[-0.12846541 -0.02207509 -0.05351667  0.63800204]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 956 is [True, False, False, False, True, False]
Current timestep = 957. State = [[-0.0377314  -0.06264155]]. Action = [[ 0.05557162 -0.168934    0.05715299 -0.29585993]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 957 is [False, True, False, False, True, False]
Scene graph at timestep 957 is [False, True, False, False, True, False]
State prediction error at timestep 957 is tensor(4.2715e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 957 of 1
Current timestep = 958. State = [[-0.17357701  0.13285406]]. Action = [[ 0.22131059 -0.01585884 -0.05020601  0.5901282 ]]. Reward = [100.]
Curr episode timestep = 20
Scene graph at timestep 958 is [False, True, False, False, True, False]
Current timestep = 959. State = [[-0.14532426  0.13803686]]. Action = [[ 0.24896768 -0.18822004 -0.09393358 -0.20958376]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 959 is [True, False, False, False, False, True]
Current timestep = 960. State = [[-0.12175503  0.11305831]]. Action = [[ 0.07625276 -0.24090008  0.2117846  -0.1395626 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 960 is [True, False, False, False, False, True]
Scene graph at timestep 960 is [True, False, False, False, True, False]
State prediction error at timestep 960 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 960 of 1
Current timestep = 961. State = [[-0.10015716  0.08767087]]. Action = [[ 0.20804411 -0.07070158  0.17614815 -0.94349164]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 961 is [True, False, False, False, True, False]
Scene graph at timestep 961 is [True, False, False, False, True, False]
State prediction error at timestep 961 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 961 of 1
Current timestep = 962. State = [[-0.07106818  0.09483334]]. Action = [[ 0.24666113  0.21843147 -0.08597142 -0.9356124 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 962 is [True, False, False, False, True, False]
Scene graph at timestep 962 is [True, False, False, False, True, False]
State prediction error at timestep 962 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 962 of 1
Current timestep = 963. State = [[-0.04224173  0.10614646]]. Action = [[-0.13300319 -0.0801461   0.18825984 -0.6334839 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 963 is [True, False, False, False, True, False]
Current timestep = 964. State = [[-0.04901241  0.08957601]]. Action = [[-0.20192602 -0.24074829  0.1829297  -0.55832523]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 964 is [False, True, False, False, True, False]
Scene graph at timestep 964 is [False, True, False, False, True, False]
State prediction error at timestep 964 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 964 of 1
Current timestep = 965. State = [[-0.06383989  0.07964617]]. Action = [[-0.12214988  0.14632672 -0.04192296 -0.826341  ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 965 is [False, True, False, False, True, False]
Scene graph at timestep 965 is [True, False, False, False, True, False]
State prediction error at timestep 965 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 965 of -1
Current timestep = 966. State = [[-0.07520139  0.08862568]]. Action = [[ 0.23877311 -0.03109971  0.0887692  -0.00462937]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 966 is [True, False, False, False, True, False]
Scene graph at timestep 966 is [True, False, False, False, True, False]
State prediction error at timestep 966 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 966 of 1
Current timestep = 967. State = [[-0.06582811  0.07924369]]. Action = [[ 0.12476379 -0.10727966 -0.02100107 -0.9288497 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 967 is [True, False, False, False, True, False]
Scene graph at timestep 967 is [True, False, False, False, True, False]
State prediction error at timestep 967 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 967 of 1
Current timestep = 968. State = [[-0.05454151  0.07258326]]. Action = [[-0.0111641   0.04932219  0.03362277 -0.9321022 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 968 is [True, False, False, False, True, False]
Scene graph at timestep 968 is [True, False, False, False, True, False]
State prediction error at timestep 968 is tensor(5.0897e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 968 of 1
Current timestep = 969. State = [[-0.0491587   0.06632822]]. Action = [[ 0.14164501 -0.15009443  0.19245869 -0.66271013]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 969 is [True, False, False, False, True, False]
Current timestep = 970. State = [[-0.04591458  0.04831016]]. Action = [[-0.23205255 -0.14844145  0.1390982   0.03168511]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 970 is [False, True, False, False, True, False]
Scene graph at timestep 970 is [False, True, False, False, True, False]
State prediction error at timestep 970 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 970 of 1
Current timestep = 971. State = [[-0.05428429  0.03017191]]. Action = [[-0.1755931  -0.09893371  0.06101295  0.72018003]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 971 is [False, True, False, False, True, False]
Scene graph at timestep 971 is [True, False, False, False, True, False]
State prediction error at timestep 971 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 971 of -1
Current timestep = 972. State = [[-0.0725598   0.00640651]]. Action = [[-0.09426282 -0.23017384 -0.04314172 -0.44123548]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 972 is [True, False, False, False, True, False]
Current timestep = 973. State = [[-0.07850242 -0.00708743]]. Action = [[0.08179516 0.04219043 0.14645931 0.6847651 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 973 is [True, False, False, False, True, False]
Current timestep = 974. State = [[-0.071673   -0.01756371]]. Action = [[ 0.20897135 -0.17134182  0.193858   -0.31058562]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 974 is [True, False, False, False, True, False]
Current timestep = 975. State = [[-0.06071366 -0.02725142]]. Action = [[ 0.12678424  0.07574427 -0.14112583  0.9045757 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 975 is [True, False, False, False, True, False]
Scene graph at timestep 975 is [True, False, False, False, True, False]
State prediction error at timestep 975 is tensor(7.2781e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 975 of 1
Current timestep = 976. State = [[-0.04985844 -0.02840757]]. Action = [[-0.01301663 -0.06657174 -0.01655747 -0.52018046]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 976 is [True, False, False, False, True, False]
Scene graph at timestep 976 is [False, True, False, False, True, False]
State prediction error at timestep 976 is tensor(6.5817e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 976 of 1
Current timestep = 977. State = [[-0.04948469 -0.03244281]]. Action = [[-0.10557875 -0.00725603  0.16338027 -0.19911736]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 977 is [False, True, False, False, True, False]
Current timestep = 978. State = [[-0.05195645 -0.04909284]]. Action = [[-0.02731502 -0.24242978 -0.0032492  -0.38464332]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 978 is [False, True, False, False, True, False]
Scene graph at timestep 978 is [True, False, False, False, True, False]
State prediction error at timestep 978 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 978 of -1
Current timestep = 979. State = [[-0.04862842 -0.08319367]]. Action = [[ 0.16885936 -0.24287966  0.23967534 -0.99227536]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 979 is [True, False, False, False, True, False]
Current timestep = 980. State = [[-0.03565901 -0.11371575]]. Action = [[ 0.21661842 -0.17709105  0.15595442 -0.6794525 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 980 is [False, True, False, False, True, False]
Scene graph at timestep 980 is [False, True, False, False, True, False]
State prediction error at timestep 980 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 980 of 1
Current timestep = 981. State = [[-0.00564576 -0.12132461]]. Action = [[ 0.21295899  0.18157366  0.2348755  -0.0541172 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 981 is [False, True, False, False, True, False]
Scene graph at timestep 981 is [False, True, False, False, True, False]
State prediction error at timestep 981 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 981 of 1
Current timestep = 982. State = [[ 0.01800666 -0.11314286]]. Action = [[-0.02250198 -0.03421433  0.03363445  0.24630618]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 982 is [False, True, False, False, True, False]
Scene graph at timestep 982 is [False, True, False, False, True, False]
State prediction error at timestep 982 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 982 of 1
Current timestep = 983. State = [[ 0.02412653 -0.11209999]]. Action = [[0.17152342 0.03911045 0.19986802 0.26512456]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 983 is [False, True, False, False, True, False]
Current timestep = 984. State = [[ 0.03222271 -0.10661527]]. Action = [[-0.24576998  0.06366691  0.14111501  0.71544445]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 984 is [False, True, False, False, True, False]
Current timestep = 985. State = [[-0.24555232  0.10503928]]. Action = [[ 0.20780271  0.07784057  0.19369674 -0.7971402 ]]. Reward = [100.]
Curr episode timestep = 26
Scene graph at timestep 985 is [False, True, False, False, True, False]
Current timestep = 986. State = [[-0.24661934  0.12950718]]. Action = [[-0.11543939  0.17825407  0.06601131  0.37123537]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 986 is [True, False, False, False, True, False]
Scene graph at timestep 986 is [True, False, False, False, False, True]
State prediction error at timestep 986 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 986 of -1
Current timestep = 987. State = [[-0.25568867  0.14461583]]. Action = [[-0.19424921 -0.17575249 -0.1099602  -0.431902  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 987 is [True, False, False, False, False, True]
Current timestep = 988. State = [[-0.24834807  0.1465291 ]]. Action = [[ 0.15950343  0.00549868 -0.16851418  0.04348779]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 988 is [True, False, False, False, False, True]
Scene graph at timestep 988 is [True, False, False, False, False, True]
State prediction error at timestep 988 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 988 of 1
Current timestep = 989. State = [[-0.23830186  0.15948999]]. Action = [[-0.02746011  0.19025886  0.09670609  0.00641239]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 989 is [True, False, False, False, False, True]
Current timestep = 990. State = [[-0.22949262  0.1803612 ]]. Action = [[ 0.22271517  0.18413842 -0.20416792 -0.47680092]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 990 is [True, False, False, False, False, True]
Current timestep = 991. State = [[-0.1998895   0.19016212]]. Action = [[ 0.19357973 -0.11281002 -0.07965887 -0.72806   ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 991 is [True, False, False, False, False, True]
Scene graph at timestep 991 is [True, False, False, False, False, True]
State prediction error at timestep 991 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 991 of 1
Current timestep = 992. State = [[-0.17368104  0.18610464]]. Action = [[ 0.10943568 -0.02634165 -0.09029442 -0.66545296]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 992 is [True, False, False, False, False, True]
Current timestep = 993. State = [[-0.16722621  0.18952739]]. Action = [[-0.06073602  0.08672455 -0.19515166 -0.01387405]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 993 is [True, False, False, False, False, True]
Current timestep = 994. State = [[-0.16474064  0.18037422]]. Action = [[ 0.04745609 -0.22208934 -0.04840089 -0.77602226]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 994 is [True, False, False, False, False, True]
Current timestep = 995. State = [[-0.15749604  0.16141477]]. Action = [[ 0.10557574 -0.10362744  0.16252998  0.7429509 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 995 is [True, False, False, False, False, True]
Scene graph at timestep 995 is [True, False, False, False, False, True]
State prediction error at timestep 995 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 995 of 1
Current timestep = 996. State = [[-0.151544    0.14388114]]. Action = [[-0.21905155 -0.14192784 -0.18572652 -0.51003027]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 996 is [True, False, False, False, False, True]
Current timestep = 997. State = [[-0.1479316   0.12114188]]. Action = [[ 0.20564729 -0.23167828 -0.09764439  0.55396533]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 997 is [True, False, False, False, False, True]
Current timestep = 998. State = [[-0.14122483  0.09220025]]. Action = [[-0.04365189 -0.1975067   0.01885635 -0.7336567 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 998 is [True, False, False, False, True, False]
Scene graph at timestep 998 is [True, False, False, False, True, False]
State prediction error at timestep 998 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 998 of 1
Current timestep = 999. State = [[-0.13182424  0.07180974]]. Action = [[ 0.22517556 -0.00518218 -0.04543705 -0.85358673]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 999 is [True, False, False, False, True, False]
Scene graph at timestep 999 is [True, False, False, False, True, False]
State prediction error at timestep 999 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 999 of 1
Current timestep = 1000. State = [[-0.12543695  0.07622305]]. Action = [[-0.02919413  0.13917032 -0.21214576 -0.45501828]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1000 is [True, False, False, False, True, False]
Scene graph at timestep 1000 is [True, False, False, False, True, False]
State prediction error at timestep 1000 is tensor(3.6394e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1000 of -1
Current timestep = 1001. State = [[-0.117032    0.07529049]]. Action = [[ 0.19337785 -0.13661782 -0.1748343  -0.3756597 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1001 is [True, False, False, False, True, False]
Scene graph at timestep 1001 is [True, False, False, False, True, False]
State prediction error at timestep 1001 is tensor(9.1986e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1001 of 1
Current timestep = 1002. State = [[-0.09666869  0.06466026]]. Action = [[ 0.07711843 -0.06541699 -0.18734848 -0.25269783]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1002 is [True, False, False, False, True, False]
Current timestep = 1003. State = [[-0.08795207  0.05183255]]. Action = [[ 0.04755789 -0.13771644  0.05811843 -0.43876982]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1003 is [True, False, False, False, True, False]
Scene graph at timestep 1003 is [True, False, False, False, True, False]
State prediction error at timestep 1003 is tensor(9.5719e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1003 of 1
Current timestep = 1004. State = [[-0.07851519  0.02859977]]. Action = [[ 0.06836182 -0.18031462 -0.22187209  0.04057765]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1004 is [True, False, False, False, True, False]
Current timestep = 1005. State = [[-0.07598306  0.02859615]]. Action = [[-0.03389177  0.23641911  0.07393211 -0.52079856]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1005 is [True, False, False, False, True, False]
Scene graph at timestep 1005 is [True, False, False, False, True, False]
State prediction error at timestep 1005 is tensor(1.4794e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1005 of 1
Current timestep = 1006. State = [[-0.07550897  0.02832917]]. Action = [[-0.06034642 -0.22349632 -0.21668725  0.5232632 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1006 is [True, False, False, False, True, False]
Scene graph at timestep 1006 is [True, False, False, False, True, False]
State prediction error at timestep 1006 is tensor(1.8857e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1006 of 1
Current timestep = 1007. State = [[-0.07255702  0.0149014 ]]. Action = [[ 0.13494763 -0.01275611 -0.13084921 -0.43235624]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1007 is [True, False, False, False, True, False]
Current timestep = 1008. State = [[-0.06371733  0.01325272]]. Action = [[ 0.06485054 -0.00895822 -0.06114489 -0.89122266]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1008 is [True, False, False, False, True, False]
Current timestep = 1009. State = [[-0.0589667   0.02166887]]. Action = [[-0.00367564  0.18408275  0.18529063  0.17326558]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1009 is [True, False, False, False, True, False]
Current timestep = 1010. State = [[-0.04808434  0.02228766]]. Action = [[ 0.14868945 -0.13911504  0.05417567  0.3175621 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1010 is [True, False, False, False, True, False]
Current timestep = 1011. State = [[-0.27432373  0.1345407 ]]. Action = [[ 0.13357344  0.01083148  0.07679611 -0.27037692]]. Reward = [100.]
Curr episode timestep = 25
Scene graph at timestep 1011 is [False, True, False, False, True, False]
Current timestep = 1012. State = [[-0.26122677  0.15262133]]. Action = [[ 0.22468477  0.03175533  0.06007123 -0.9212671 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1012 is [True, False, False, False, False, True]
Current timestep = 1013. State = [[-0.24239606  0.15755542]]. Action = [[-0.13248225 -0.12111051  0.12653041  0.9315146 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1013 is [True, False, False, False, False, True]
Scene graph at timestep 1013 is [True, False, False, False, False, True]
State prediction error at timestep 1013 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1013 of 1
Current timestep = 1014. State = [[-0.24315469  0.1552273 ]]. Action = [[-0.16744286 -0.08126047 -0.16439028 -0.5397321 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1014 is [True, False, False, False, False, True]
Current timestep = 1015. State = [[-0.24486859  0.1459892 ]]. Action = [[ 0.01368997 -0.11293429  0.20851594 -0.7493678 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1015 is [True, False, False, False, False, True]
Scene graph at timestep 1015 is [True, False, False, False, False, True]
State prediction error at timestep 1015 is tensor(4.9047e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1015 of -1
Current timestep = 1016. State = [[-0.24841958  0.12763488]]. Action = [[-0.08768988 -0.12322655  0.22077867  0.89079523]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1016 is [True, False, False, False, False, True]
Scene graph at timestep 1016 is [True, False, False, False, False, True]
State prediction error at timestep 1016 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1016 of -1
Current timestep = 1017. State = [[-0.2465695   0.10532153]]. Action = [[ 0.21522105 -0.19361553 -0.20794147 -0.90256226]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1017 is [True, False, False, False, False, True]
Current timestep = 1018. State = [[-0.23011082  0.09323903]]. Action = [[0.24080831 0.02667469 0.00504953 0.6486418 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1018 is [True, False, False, False, True, False]
Scene graph at timestep 1018 is [True, False, False, False, True, False]
State prediction error at timestep 1018 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1018 of 1
Current timestep = 1019. State = [[-0.19942893  0.0986243 ]]. Action = [[ 0.21868005  0.14870918 -0.12545763 -0.49398315]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1019 is [True, False, False, False, True, False]
Current timestep = 1020. State = [[-0.17035776  0.11830282]]. Action = [[ 0.19072905  0.18334761 -0.19338924  0.7516885 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1020 is [True, False, False, False, True, False]
Scene graph at timestep 1020 is [True, False, False, False, True, False]
State prediction error at timestep 1020 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1020 of 1
Current timestep = 1021. State = [[-0.135056    0.13149802]]. Action = [[ 0.24642459 -0.05342889 -0.14848535 -0.24112314]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1021 is [True, False, False, False, True, False]
Scene graph at timestep 1021 is [True, False, False, False, False, True]
State prediction error at timestep 1021 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1021 of 1
Current timestep = 1022. State = [[-0.11213937  0.1274082 ]]. Action = [[ 0.00198814 -0.08351955 -0.1487581   0.38995433]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1022 is [True, False, False, False, False, True]
Scene graph at timestep 1022 is [True, False, False, False, False, True]
State prediction error at timestep 1022 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1022 of 1
Current timestep = 1023. State = [[-0.10715575  0.13095401]]. Action = [[ 0.09810036  0.17854121 -0.01345682 -0.95064616]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1023 is [True, False, False, False, False, True]
Current timestep = 1024. State = [[-0.09269505  0.1261231 ]]. Action = [[ 0.09572446 -0.23819073  0.15446338 -0.26959765]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1024 is [True, False, False, False, False, True]
Current timestep = 1025. State = [[-0.08367167  0.12341405]]. Action = [[ 0.04937464  0.16670018 -0.13742736 -0.27181518]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1025 is [True, False, False, False, False, True]
Current timestep = 1026. State = [[-0.07254776  0.13782027]]. Action = [[ 0.13253665  0.1543265  -0.04943043 -0.7248986 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1026 is [True, False, False, False, True, False]
Current timestep = 1027. State = [[-0.04868606  0.13777862]]. Action = [[ 0.15203458 -0.2346076   0.20215821  0.09823239]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1027 is [True, False, False, False, False, True]
Current timestep = 1028. State = [[-0.17629568  0.16954444]]. Action = [[ 0.21803874 -0.17131527 -0.15798162  0.00549245]]. Reward = [100.]
Curr episode timestep = 16
Scene graph at timestep 1028 is [False, True, False, False, False, True]
Scene graph at timestep 1028 is [True, False, False, False, False, True]
State prediction error at timestep 1028 is tensor(0.0097, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1028 of 1
Current timestep = 1029. State = [[-0.15622844  0.18525156]]. Action = [[-0.08428234 -0.1677173  -0.15293105  0.5138433 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1029 is [True, False, False, False, False, True]
Current timestep = 1030. State = [[-0.15105815  0.16716342]]. Action = [[ 0.1574645  -0.12794212  0.1849584  -0.34621483]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1030 is [True, False, False, False, False, True]
Scene graph at timestep 1030 is [True, False, False, False, False, True]
State prediction error at timestep 1030 is tensor(8.9795e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1030 of 1
Current timestep = 1031. State = [[-0.14442466  0.14742708]]. Action = [[ 0.0527992  -0.1067836   0.04057962  0.60733604]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1031 is [True, False, False, False, False, True]
Scene graph at timestep 1031 is [True, False, False, False, False, True]
State prediction error at timestep 1031 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1031 of 1
Current timestep = 1032. State = [[-0.14712824  0.14858033]]. Action = [[-0.19483961  0.14330888 -0.17368671 -0.556752  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1032 is [True, False, False, False, False, True]
Current timestep = 1033. State = [[-0.15174106  0.16463213]]. Action = [[ 0.13875848  0.14275968 -0.14002433 -0.92406344]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1033 is [True, False, False, False, False, True]
Current timestep = 1034. State = [[-0.1437354   0.17021939]]. Action = [[ 0.10034487 -0.05462989  0.12438509  0.7485783 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1034 is [True, False, False, False, False, True]
Scene graph at timestep 1034 is [True, False, False, False, False, True]
State prediction error at timestep 1034 is tensor(2.8564e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1034 of 1
Current timestep = 1035. State = [[-0.13089482  0.17105374]]. Action = [[ 0.10909161  0.00803021 -0.13280807  0.12647223]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1035 is [True, False, False, False, False, True]
Current timestep = 1036. State = [[-0.11365667  0.1750056 ]]. Action = [[ 0.1691032   0.05025518  0.0704543  -0.62809974]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1036 is [True, False, False, False, False, True]
Scene graph at timestep 1036 is [True, False, False, False, False, True]
State prediction error at timestep 1036 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1036 of 1
Current timestep = 1037. State = [[-0.08643328  0.17087524]]. Action = [[ 0.15856937 -0.13341437  0.17134732 -0.67999965]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1037 is [True, False, False, False, False, True]
Current timestep = 1038. State = [[-0.06667274  0.1604195 ]]. Action = [[ 0.12136731 -0.04350907 -0.05478694 -0.3468951 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1038 is [True, False, False, False, False, True]
Scene graph at timestep 1038 is [True, False, False, False, False, True]
State prediction error at timestep 1038 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1038 of 1
Current timestep = 1039. State = [[-0.04217899  0.14381477]]. Action = [[ 0.23934501 -0.17808254 -0.08478102  0.49187708]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1039 is [True, False, False, False, False, True]
Scene graph at timestep 1039 is [False, True, False, False, False, True]
State prediction error at timestep 1039 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1039 of 1
Current timestep = 1040. State = [[-0.2453483  -0.21074575]]. Action = [[ 0.11702421 -0.20252     0.01277143  0.04718661]]. Reward = [100.]
Curr episode timestep = 11
Scene graph at timestep 1040 is [False, True, False, False, False, True]
Current timestep = 1041. State = [[-0.23767418 -0.23402998]]. Action = [[ 0.11225063 -0.01969345 -0.05277228  0.9014797 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1041 is [True, False, False, True, False, False]
Current timestep = 1042. State = [[-0.22518457 -0.23572628]]. Action = [[ 0.07095215  0.05164886 -0.1297578   0.38352537]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1042 is [True, False, False, True, False, False]
Current timestep = 1043. State = [[-0.20671694 -0.2239249 ]]. Action = [[ 0.21112525  0.1749537  -0.14084148 -0.4620778 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1043 is [True, False, False, True, False, False]
Current timestep = 1044. State = [[-0.18832706 -0.19999483]]. Action = [[0.05528092 0.20699942 0.02651778 0.86803484]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1044 is [True, False, False, True, False, False]
Scene graph at timestep 1044 is [True, False, False, True, False, False]
State prediction error at timestep 1044 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1044 of 1
Current timestep = 1045. State = [[-0.16927886 -0.19025296]]. Action = [[ 0.19514912 -0.18667667 -0.04093057 -0.9678972 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1045 is [True, False, False, True, False, False]
Current timestep = 1046. State = [[-0.1427129  -0.20687643]]. Action = [[ 0.221493   -0.1341073   0.07297632 -0.00281096]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1046 is [True, False, False, True, False, False]
Scene graph at timestep 1046 is [True, False, False, True, False, False]
State prediction error at timestep 1046 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1046 of 1
Current timestep = 1047. State = [[-0.11148167 -0.21283871]]. Action = [[ 0.10722706  0.15029943  0.17180848 -0.2562831 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1047 is [True, False, False, True, False, False]
Scene graph at timestep 1047 is [True, False, False, True, False, False]
State prediction error at timestep 1047 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1047 of 1
Current timestep = 1048. State = [[-0.1051557 -0.1906586]]. Action = [[-0.00924243  0.24105924  0.19148782 -0.41833246]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1048 is [True, False, False, True, False, False]
Current timestep = 1049. State = [[-0.09569922 -0.16586985]]. Action = [[0.18849975 0.12395808 0.13083565 0.970505  ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1049 is [True, False, False, True, False, False]
Current timestep = 1050. State = [[-0.07854654 -0.14302655]]. Action = [[ 0.02812135  0.17669845 -0.02641504 -0.84381264]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1050 is [True, False, False, True, False, False]
Scene graph at timestep 1050 is [True, False, False, True, False, False]
State prediction error at timestep 1050 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1050 of 1
Current timestep = 1051. State = [[-0.07158767 -0.12966606]]. Action = [[ 0.03007153 -0.07626513  0.03747293 -0.20478344]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1051 is [True, False, False, True, False, False]
Current timestep = 1052. State = [[-0.07093968 -0.13381943]]. Action = [[-0.02191035 -0.03360245 -0.05143997 -0.45445216]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1052 is [True, False, False, True, False, False]
Scene graph at timestep 1052 is [True, False, False, True, False, False]
State prediction error at timestep 1052 is tensor(5.1991e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1052 of -1
Current timestep = 1053. State = [[-0.06192696 -0.13730076]]. Action = [[ 0.225043    0.02723628  0.04842582 -0.02216494]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1053 is [True, False, False, True, False, False]
Current timestep = 1054. State = [[-0.0403086  -0.12992705]]. Action = [[0.04264796 0.12709153 0.08160242 0.8649032 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1054 is [True, False, False, True, False, False]
Scene graph at timestep 1054 is [False, True, False, True, False, False]
State prediction error at timestep 1054 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1054 of 1
Current timestep = 1055. State = [[-0.02236977 -0.11748385]]. Action = [[0.22794965 0.03538829 0.21295938 0.69534326]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1055 is [False, True, False, True, False, False]
Scene graph at timestep 1055 is [False, True, False, False, True, False]
State prediction error at timestep 1055 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1055 of 1
Current timestep = 1056. State = [[ 0.00132833 -0.12602496]]. Action = [[-0.1656055  -0.16040301  0.13761073 -0.51131576]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1056 is [False, True, False, False, True, False]
Current timestep = 1057. State = [[-0.00133688 -0.12843192]]. Action = [[ 0.15093094  0.11212769 -0.01065518  0.1026113 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1057 is [False, True, False, True, False, False]
Current timestep = 1058. State = [[ 0.00245343 -0.12676787]]. Action = [[ 0.11009195 -0.07639527 -0.12508889  0.0497576 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1058 is [False, True, False, True, False, False]
Scene graph at timestep 1058 is [False, True, False, True, False, False]
State prediction error at timestep 1058 is tensor(4.4438e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1058 of -1
Current timestep = 1059. State = [[ 0.01478809 -0.1386064 ]]. Action = [[-0.10682714 -0.12516688  0.16356081 -0.47660196]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1059 is [False, True, False, True, False, False]
Current timestep = 1060. State = [[ 0.0199621 -0.1349251]]. Action = [[ 0.23858464  0.19011486 -0.13608299 -0.04971939]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1060 is [False, True, False, True, False, False]
Current timestep = 1061. State = [[ 0.04419815 -0.12015904]]. Action = [[ 0.23947847  0.06899101 -0.16072342 -0.9463915 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1061 is [False, True, False, True, False, False]
Scene graph at timestep 1061 is [False, True, False, False, True, False]
State prediction error at timestep 1061 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1061 of -1
Current timestep = 1062. State = [[ 0.07758295 -0.11417858]]. Action = [[ 0.17898253 -0.19757886  0.14269847 -0.9706033 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1062 is [False, True, False, False, True, False]
Scene graph at timestep 1062 is [False, False, True, False, True, False]
State prediction error at timestep 1062 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1062 of -1
Current timestep = 1063. State = [[ 0.07758295 -0.11417858]]. Action = [[ 0.19035995 -0.2186069   0.04586542  0.3359903 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1063 is [False, False, True, False, True, False]
Current timestep = 1064. State = [[ 0.07758295 -0.11417858]]. Action = [[0.2006526  0.03246993 0.07047361 0.55675554]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1064 is [False, False, True, False, True, False]
Scene graph at timestep 1064 is [False, False, True, False, True, False]
State prediction error at timestep 1064 is tensor(9.6200e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1064 of -1
Current timestep = 1065. State = [[ 0.07758295 -0.11417858]]. Action = [[ 0.06088874 -0.1179153  -0.18718202 -0.17923546]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1065 is [False, False, True, False, True, False]
Scene graph at timestep 1065 is [False, False, True, False, True, False]
State prediction error at timestep 1065 is tensor(6.3046e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1065 of -1
Current timestep = 1066. State = [[ 0.07756405 -0.11417662]]. Action = [[ 0.03647327 -0.09491456 -0.05557624  0.21645677]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1066 is [False, False, True, False, True, False]
Current timestep = 1067. State = [[ 0.07756405 -0.11417662]]. Action = [[ 0.07122481  0.02827522  0.15846616 -0.54462916]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1067 is [False, False, True, False, True, False]
Current timestep = 1068. State = [[ 0.07756405 -0.11417662]]. Action = [[ 0.12884527  0.22312337  0.18575868 -0.31470954]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1068 is [False, False, True, False, True, False]
Current timestep = 1069. State = [[ 0.07756405 -0.11417662]]. Action = [[ 0.15429115 -0.19410872 -0.02716736 -0.262434  ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1069 is [False, False, True, False, True, False]
Scene graph at timestep 1069 is [False, False, True, False, True, False]
State prediction error at timestep 1069 is tensor(8.4591e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1069 of -1
Current timestep = 1070. State = [[ 0.07756405 -0.11417662]]. Action = [[0.1699825  0.03116837 0.23104781 0.29540968]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1070 is [False, False, True, False, True, False]
Scene graph at timestep 1070 is [False, False, True, False, True, False]
State prediction error at timestep 1070 is tensor(1.5385e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1070 of -1
Current timestep = 1071. State = [[ 0.0759796 -0.1252468]]. Action = [[-0.10297251 -0.18273798 -0.07342418  0.12161291]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1071 is [False, False, True, False, True, False]
Scene graph at timestep 1071 is [False, False, True, True, False, False]
State prediction error at timestep 1071 is tensor(2.8152e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1071 of -1
Current timestep = 1072. State = [[ 0.07148252 -0.13944975]]. Action = [[-0.14197555  0.00892666 -0.12283605 -0.10101503]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1072 is [False, False, True, True, False, False]
Current timestep = 1073. State = [[ 0.0691596 -0.1406225]]. Action = [[ 0.06351677 -0.07744917  0.18462491  0.6264589 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1073 is [False, False, True, True, False, False]
Scene graph at timestep 1073 is [False, False, True, True, False, False]
State prediction error at timestep 1073 is tensor(2.9156e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1073 of -1
Current timestep = 1074. State = [[ 0.06892226 -0.14055404]]. Action = [[0.09976715 0.01141909 0.14428902 0.9693475 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1074 is [False, False, True, True, False, False]
Scene graph at timestep 1074 is [False, False, True, True, False, False]
State prediction error at timestep 1074 is tensor(7.0063e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1074 of -1
Current timestep = 1075. State = [[ 0.06763865 -0.1388838 ]]. Action = [[-0.05030133  0.03563493  0.08070767 -0.9121967 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1075 is [False, False, True, True, False, False]
Scene graph at timestep 1075 is [False, False, True, True, False, False]
State prediction error at timestep 1075 is tensor(5.4238e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1075 of 1
Current timestep = 1076. State = [[ 0.05532569 -0.15002331]]. Action = [[-0.23700878 -0.20936279  0.1222969  -0.29683584]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1076 is [False, False, True, True, False, False]
Scene graph at timestep 1076 is [False, False, True, True, False, False]
State prediction error at timestep 1076 is tensor(4.2573e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1076 of -1
Current timestep = 1077. State = [[ 0.02780364 -0.17969412]]. Action = [[-0.14008325 -0.20247348  0.18960416 -0.48535073]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1077 is [False, False, True, True, False, False]
Scene graph at timestep 1077 is [False, True, False, True, False, False]
State prediction error at timestep 1077 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1077 of -1
Current timestep = 1078. State = [[ 0.0100144  -0.20173985]]. Action = [[-0.09884113 -0.08886257  0.09093386 -0.26298845]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1078 is [False, True, False, True, False, False]
Current timestep = 1079. State = [[ 0.00543053 -0.21860756]]. Action = [[ 0.06074733 -0.1714549   0.14227295 -0.72306615]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1079 is [False, True, False, True, False, False]
Current timestep = 1080. State = [[ 0.00648398 -0.21702377]]. Action = [[0.09594923 0.21827722 0.23542976 0.41393614]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1080 is [False, True, False, True, False, False]
Current timestep = 1081. State = [[ 0.00457328 -0.20241798]]. Action = [[-0.19860546  0.075975    0.16880885 -0.15842533]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1081 is [False, True, False, True, False, False]
Current timestep = 1082. State = [[-0.00732702 -0.20725617]]. Action = [[-0.04275751 -0.18915534  0.11109373  0.6913624 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1082 is [False, True, False, True, False, False]
Current timestep = 1083. State = [[-0.00784312 -0.22491801]]. Action = [[ 0.18349218 -0.17806569 -0.0729399  -0.39181018]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1083 is [False, True, False, True, False, False]
Current timestep = 1084. State = [[-0.00733465 -0.23792145]]. Action = [[ 0.04118362 -0.01562262  0.21014455 -0.77027994]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1084 is [False, True, False, True, False, False]
Current timestep = 1085. State = [[-0.0066354  -0.24598628]]. Action = [[-0.04414579 -0.07399428 -0.23590387 -0.65169483]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1085 is [False, True, False, True, False, False]
Scene graph at timestep 1085 is [False, True, False, True, False, False]
State prediction error at timestep 1085 is tensor(8.5989e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1085 of -1
Current timestep = 1086. State = [[-0.00756938 -0.25158393]]. Action = [[-0.07039016  0.06323346  0.17160475 -0.9017485 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1086 is [False, True, False, True, False, False]
Current timestep = 1087. State = [[-0.00541308 -0.24078266]]. Action = [[0.10955697 0.15616328 0.13572946 0.6253035 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1087 is [False, True, False, True, False, False]
Scene graph at timestep 1087 is [False, True, False, True, False, False]
State prediction error at timestep 1087 is tensor(3.6662e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1087 of 1
Current timestep = 1088. State = [[ 0.00708343 -0.21345937]]. Action = [[0.24000853 0.23221254 0.20715165 0.41160822]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1088 is [False, True, False, True, False, False]
Current timestep = 1089. State = [[ 0.01427053 -0.19017324]]. Action = [[-0.21816497  0.09058753  0.15132952 -0.20639783]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1089 is [False, True, False, True, False, False]
Scene graph at timestep 1089 is [False, True, False, True, False, False]
State prediction error at timestep 1089 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1089 of 1
Current timestep = 1090. State = [[ 0.01633747 -0.1757654 ]]. Action = [[ 0.20190412  0.05711415  0.1899404  -0.8263017 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1090 is [False, True, False, True, False, False]
Scene graph at timestep 1090 is [False, True, False, True, False, False]
State prediction error at timestep 1090 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1090 of 1
Current timestep = 1091. State = [[ 0.02146528 -0.1671934 ]]. Action = [[ 0.10935879  0.03166339 -0.1403613  -0.52309537]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1091 is [False, True, False, True, False, False]
Current timestep = 1092. State = [[ 0.03923808 -0.16093846]]. Action = [[ 0.18781501  0.05902731  0.18764663 -0.36534947]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1092 is [False, True, False, True, False, False]
Scene graph at timestep 1092 is [False, True, False, True, False, False]
State prediction error at timestep 1092 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1092 of -1
Current timestep = 1093. State = [[ 0.05989264 -0.1556567 ]]. Action = [[ 0.17377353 -0.17007075 -0.22265507 -0.60394543]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1093 is [False, True, False, True, False, False]
Scene graph at timestep 1093 is [False, False, True, True, False, False]
State prediction error at timestep 1093 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1093 of -1
Current timestep = 1094. State = [[ 0.05796558 -0.14921202]]. Action = [[-0.24241821  0.12273729  0.04183504 -0.49093437]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1094 is [False, False, True, True, False, False]
Current timestep = 1095. State = [[ 0.05376081 -0.1423409 ]]. Action = [[-0.078225   -0.0174624  -0.08694695 -0.8195538 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1095 is [False, False, True, True, False, False]
Current timestep = 1096. State = [[ 0.0478756  -0.14242071]]. Action = [[ 0.08811134  0.06972712 -0.12191111  0.76588047]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1096 is [False, False, True, True, False, False]
Scene graph at timestep 1096 is [False, True, False, True, False, False]
State prediction error at timestep 1096 is tensor(1.7686e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1096 of 1
Current timestep = 1097. State = [[ 0.04737468 -0.1362103 ]]. Action = [[ 0.10246563  0.09090644 -0.0389798   0.42043686]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1097 is [False, True, False, True, False, False]
Scene graph at timestep 1097 is [False, True, False, True, False, False]
State prediction error at timestep 1097 is tensor(4.3323e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1097 of 1
Current timestep = 1098. State = [[ 0.04717905 -0.12788893]]. Action = [[ 0.17155194  0.20557123 -0.11493261 -0.57462615]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1098 is [False, True, False, True, False, False]
Scene graph at timestep 1098 is [False, True, False, True, False, False]
State prediction error at timestep 1098 is tensor(5.6095e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1098 of 1
Current timestep = 1099. State = [[ 0.04772812 -0.1346119 ]]. Action = [[ 0.043331   -0.13177937  0.17344764  0.43251097]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1099 is [False, True, False, True, False, False]
Scene graph at timestep 1099 is [False, True, False, True, False, False]
State prediction error at timestep 1099 is tensor(2.1690e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1099 of -1
Current timestep = 1100. State = [[ 0.04753455 -0.13936253]]. Action = [[ 0.12905449 -0.22101197  0.12963188  0.73889685]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1100 is [False, True, False, True, False, False]
Current timestep = 1101. State = [[ 0.04753455 -0.13936253]]. Action = [[ 0.20995647  0.17386943  0.20525718 -0.83335394]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1101 is [False, True, False, True, False, False]
Scene graph at timestep 1101 is [False, True, False, True, False, False]
State prediction error at timestep 1101 is tensor(3.4720e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1101 of -1
Current timestep = 1102. State = [[ 0.04672549 -0.13733424]]. Action = [[-0.14647134  0.07837433  0.10725525 -0.11452556]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1102 is [False, True, False, True, False, False]
Scene graph at timestep 1102 is [False, True, False, True, False, False]
State prediction error at timestep 1102 is tensor(5.7476e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1102 of 1
Current timestep = 1103. State = [[ 0.04059571 -0.12976928]]. Action = [[-0.09670177  0.09721747 -0.21631256  0.14268637]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1103 is [False, True, False, True, False, False]
Scene graph at timestep 1103 is [False, True, False, True, False, False]
State prediction error at timestep 1103 is tensor(6.1293e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1103 of 1
Current timestep = 1104. State = [[ 0.03247084 -0.10948177]]. Action = [[-0.03868268  0.20500672  0.18751693  0.0393517 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1104 is [False, True, False, True, False, False]
Current timestep = 1105. State = [[-0.2578099   0.07811534]]. Action = [[-0.11962047  0.19094166  0.22229269  0.17450368]]. Reward = [100.]
Curr episode timestep = 64
Scene graph at timestep 1105 is [False, True, False, False, True, False]
Scene graph at timestep 1105 is [True, False, False, False, True, False]
State prediction error at timestep 1105 is tensor(0.0597, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1105 of -1
Current timestep = 1106. State = [[-0.24842978  0.09476981]]. Action = [[ 0.18962538  0.15583795 -0.18254459 -0.5368628 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1106 is [True, False, False, False, True, False]
Current timestep = 1107. State = [[-0.23618875  0.10885863]]. Action = [[-0.12587251 -0.00025409  0.11699936 -0.13171059]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1107 is [True, False, False, False, True, False]
Current timestep = 1108. State = [[-0.24435362  0.12279016]]. Action = [[-0.10197787  0.14391094 -0.18125051 -0.00743026]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1108 is [True, False, False, False, True, False]
Scene graph at timestep 1108 is [True, False, False, False, True, False]
State prediction error at timestep 1108 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1108 of -1
Current timestep = 1109. State = [[-0.24391164  0.12444788]]. Action = [[ 0.19919577 -0.19976085 -0.20922214  0.4702052 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1109 is [True, False, False, False, True, False]
Scene graph at timestep 1109 is [True, False, False, False, True, False]
State prediction error at timestep 1109 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1109 of 1
Current timestep = 1110. State = [[-0.22790535  0.11827929]]. Action = [[ 0.11722496  0.19143683 -0.05250014  0.20025778]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1110 is [True, False, False, False, True, False]
Current timestep = 1111. State = [[-0.21703486  0.13301331]]. Action = [[-0.01925392  0.06472033 -0.05803952  0.4291954 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1111 is [True, False, False, False, True, False]
Current timestep = 1112. State = [[-0.21927562  0.14248265]]. Action = [[ 0.0059129   0.07147998  0.18171245 -0.427132  ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1112 is [True, False, False, False, False, True]
Scene graph at timestep 1112 is [True, False, False, False, False, True]
State prediction error at timestep 1112 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1112 of 1
Current timestep = 1113. State = [[-0.21478654  0.15322903]]. Action = [[ 0.03369743  0.05932733 -0.08572072 -0.7476155 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1113 is [True, False, False, False, False, True]
Scene graph at timestep 1113 is [True, False, False, False, False, True]
State prediction error at timestep 1113 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1113 of 1
Current timestep = 1114. State = [[-0.20239267  0.15125541]]. Action = [[ 0.17796418 -0.14100811  0.19404417 -0.5769288 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1114 is [True, False, False, False, False, True]
Current timestep = 1115. State = [[-0.1871449   0.15683086]]. Action = [[ 0.03091651  0.18000597  0.2190516  -0.5577593 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1115 is [True, False, False, False, False, True]
Current timestep = 1116. State = [[-0.1716529   0.16387165]]. Action = [[ 0.18288821 -0.02853756  0.22392309  0.6701112 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1116 is [True, False, False, False, False, True]
Scene graph at timestep 1116 is [True, False, False, False, False, True]
State prediction error at timestep 1116 is tensor(5.3058e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1116 of 1
Current timestep = 1117. State = [[-0.14533709  0.15869164]]. Action = [[ 0.19776148 -0.11808032 -0.03366715  0.5174408 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1117 is [True, False, False, False, False, True]
Scene graph at timestep 1117 is [True, False, False, False, False, True]
State prediction error at timestep 1117 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1117 of 1
Current timestep = 1118. State = [[-0.12102181  0.15439859]]. Action = [[ 0.12590384  0.09031749  0.01062417 -0.33291936]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1118 is [True, False, False, False, False, True]
Scene graph at timestep 1118 is [True, False, False, False, False, True]
State prediction error at timestep 1118 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1118 of 1
Current timestep = 1119. State = [[-0.10837292  0.16181652]]. Action = [[-0.04286155  0.02905315  0.01315072 -0.61026037]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1119 is [True, False, False, False, False, True]
Scene graph at timestep 1119 is [True, False, False, False, False, True]
State prediction error at timestep 1119 is tensor(4.3843e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1119 of -1
Current timestep = 1120. State = [[-0.1046019   0.16699511]]. Action = [[ 0.14595252  0.05892581 -0.0473391  -0.04026181]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1120 is [True, False, False, False, False, True]
Current timestep = 1121. State = [[-0.09903439  0.17344914]]. Action = [[-0.19220653 -0.00672069 -0.1994612  -0.43879038]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1121 is [True, False, False, False, False, True]
Current timestep = 1122. State = [[-0.0978527   0.16928992]]. Action = [[ 0.17247868 -0.09404685 -0.08656935 -0.12034756]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1122 is [True, False, False, False, False, True]
Scene graph at timestep 1122 is [True, False, False, False, False, True]
State prediction error at timestep 1122 is tensor(4.2797e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1122 of 1
Current timestep = 1123. State = [[-0.09595184  0.16916303]]. Action = [[ 0.02105123  0.1487177   0.12883395 -0.22009838]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1123 is [True, False, False, False, False, True]
Scene graph at timestep 1123 is [True, False, False, False, False, True]
State prediction error at timestep 1123 is tensor(3.4528e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1123 of -1
Current timestep = 1124. State = [[-0.08957545  0.1681802 ]]. Action = [[-0.14432675 -0.23399118 -0.00819796 -0.8289275 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1124 is [True, False, False, False, False, True]
Current timestep = 1125. State = [[-0.09023128  0.15892285]]. Action = [[-0.01887614  0.03396371 -0.07314631 -0.43062562]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1125 is [True, False, False, False, False, True]
Current timestep = 1126. State = [[-0.09582391  0.16301823]]. Action = [[-0.10178348  0.05809614  0.13112196 -0.8504986 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1126 is [True, False, False, False, False, True]
Scene graph at timestep 1126 is [True, False, False, False, False, True]
State prediction error at timestep 1126 is tensor(7.6077e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1126 of -1
Current timestep = 1127. State = [[-0.10299978  0.17626007]]. Action = [[ 0.1398136   0.17976522  0.19555593 -0.78048515]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1127 is [True, False, False, False, False, True]
Scene graph at timestep 1127 is [True, False, False, False, False, True]
State prediction error at timestep 1127 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1127 of -1
Current timestep = 1128. State = [[-0.09729222  0.17371109]]. Action = [[ 0.17380542 -0.1933961   0.18839908  0.04207599]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1128 is [True, False, False, False, False, True]
Scene graph at timestep 1128 is [True, False, False, False, False, True]
State prediction error at timestep 1128 is tensor(4.5235e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1128 of 1
Current timestep = 1129. State = [[-0.07895187  0.1678541 ]]. Action = [[ 0.20683587  0.13252914 -0.05919848  0.6961988 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1129 is [True, False, False, False, False, True]
Current timestep = 1130. State = [[-0.05356611  0.18319319]]. Action = [[ 0.12024099  0.13126415 -0.09805176 -0.7516278 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1130 is [True, False, False, False, False, True]
Scene graph at timestep 1130 is [True, False, False, False, False, True]
State prediction error at timestep 1130 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1130 of -1
Current timestep = 1131. State = [[-0.02561633  0.18624504]]. Action = [[ 0.05150339 -0.21261562 -0.03372644 -0.15759212]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1131 is [True, False, False, False, False, True]
Current timestep = 1132. State = [[-0.02105138  0.16819207]]. Action = [[-0.0816609  -0.13269596 -0.07187638 -0.9849006 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1132 is [False, True, False, False, False, True]
Current timestep = 1133. State = [[-0.02506753  0.15105928]]. Action = [[-0.23153281 -0.14286217  0.21331754  0.13712072]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1133 is [False, True, False, False, False, True]
Current timestep = 1134. State = [[-0.02758698  0.12548122]]. Action = [[ 0.16269979 -0.22905986 -0.20987752 -0.770425  ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1134 is [False, True, False, False, False, True]
Scene graph at timestep 1134 is [False, True, False, False, False, True]
State prediction error at timestep 1134 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1134 of 1
Current timestep = 1135. State = [[-0.15672208  0.17542797]]. Action = [[-0.14957114 -0.15372257  0.1788572   0.59696305]]. Reward = [100.]
Curr episode timestep = 29
Scene graph at timestep 1135 is [False, True, False, False, False, True]
Scene graph at timestep 1135 is [True, False, False, False, False, True]
State prediction error at timestep 1135 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1135 of -1
Current timestep = 1136. State = [[-0.13589504  0.19994266]]. Action = [[ 0.05591875  0.0715954  -0.01902162 -0.6316195 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1136 is [True, False, False, False, False, True]
Scene graph at timestep 1136 is [True, False, False, False, False, True]
State prediction error at timestep 1136 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1136 of -1
Current timestep = 1137. State = [[-0.13748497  0.2134478 ]]. Action = [[-0.19876124  0.07110766  0.00414369  0.19744289]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1137 is [True, False, False, False, False, True]
Current timestep = 1138. State = [[-0.14965953  0.23078342]]. Action = [[ 0.03202441  0.15721953 -0.15793559 -0.08333331]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1138 is [True, False, False, False, False, True]
Current timestep = 1139. State = [[-0.14711678  0.2434962 ]]. Action = [[ 0.14259303  0.09776565 -0.03250948 -0.04784501]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1139 is [True, False, False, False, False, True]
Scene graph at timestep 1139 is [True, False, False, False, False, True]
State prediction error at timestep 1139 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1139 of -1
Current timestep = 1140. State = [[-0.11962677  0.24834011]]. Action = [[ 0.23353562 -0.12459072 -0.22804521 -0.67645067]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1140 is [True, False, False, False, False, True]
Current timestep = 1141. State = [[-0.09109099  0.23342425]]. Action = [[ 0.20106474 -0.15526685 -0.13668902 -0.24649179]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1141 is [True, False, False, False, False, True]
Current timestep = 1142. State = [[-0.06891897  0.21219277]]. Action = [[ 0.0072827  -0.19476463 -0.13829511 -0.58459973]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1142 is [True, False, False, False, False, True]
Current timestep = 1143. State = [[-0.05705005  0.19577709]]. Action = [[ 0.20647949 -0.02183335  0.15014029 -0.9609357 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1143 is [True, False, False, False, False, True]
Scene graph at timestep 1143 is [True, False, False, False, False, True]
State prediction error at timestep 1143 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1143 of 1
Current timestep = 1144. State = [[-0.03558541  0.19459856]]. Action = [[-0.00942709  0.07096189  0.21684217  0.5518031 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1144 is [True, False, False, False, False, True]
Scene graph at timestep 1144 is [False, True, False, False, False, True]
State prediction error at timestep 1144 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1144 of 1
Current timestep = 1145. State = [[-0.02967526  0.18794692]]. Action = [[ 0.15283692 -0.14934522 -0.1090005  -0.81805384]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1145 is [False, True, False, False, False, True]
Current timestep = 1146. State = [[-0.00757097  0.16779798]]. Action = [[ 0.1804181  -0.20957658 -0.12387589 -0.19895214]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1146 is [False, True, False, False, False, True]
Scene graph at timestep 1146 is [False, True, False, False, False, True]
State prediction error at timestep 1146 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1146 of 1
Current timestep = 1147. State = [[0.02043782 0.1440979 ]]. Action = [[ 0.16844016 -0.09385046  0.16335791 -0.8226036 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1147 is [False, True, False, False, False, True]
Current timestep = 1148. State = [[0.03060712 0.146296  ]]. Action = [[-0.1242862   0.14865667 -0.15233509  0.669579  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1148 is [False, True, False, False, False, True]
Current timestep = 1149. State = [[0.02747987 0.15219305]]. Action = [[ 0.21976244 -0.01901238 -0.14301024 -0.85868824]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1149 is [False, True, False, False, False, True]
Scene graph at timestep 1149 is [False, True, False, False, False, True]
State prediction error at timestep 1149 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1149 of 1
Current timestep = 1150. State = [[0.03353269 0.14009587]]. Action = [[ 0.18630046 -0.2259442  -0.20702985  0.70433843]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1150 is [False, True, False, False, False, True]
Current timestep = 1151. State = [[0.04942735 0.11341897]]. Action = [[ 0.06419989 -0.2163334   0.1741423  -0.32484078]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1151 is [False, True, False, False, False, True]
Current timestep = 1152. State = [[0.05878083 0.09003711]]. Action = [[-0.01282781 -0.1041494   0.12639439 -0.36093712]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1152 is [False, True, False, False, True, False]
Current timestep = 1153. State = [[0.06321067 0.07202032]]. Action = [[ 0.00142887 -0.13355115  0.19573036 -0.05480796]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1153 is [False, False, True, False, True, False]
Current timestep = 1154. State = [[0.06517976 0.06280572]]. Action = [[ 0.24453649 -0.17363684 -0.04187891 -0.8064649 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1154 is [False, False, True, False, True, False]
Scene graph at timestep 1154 is [False, False, True, False, True, False]
State prediction error at timestep 1154 is tensor(3.7107e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1154 of 1
Current timestep = 1155. State = [[0.06390445 0.0570685 ]]. Action = [[-0.22538455 -0.07213806  0.17335853  0.46994925]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1155 is [False, False, True, False, True, False]
Scene graph at timestep 1155 is [False, False, True, False, True, False]
State prediction error at timestep 1155 is tensor(4.0722e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1155 of 1
Current timestep = 1156. State = [[0.05936285 0.0347472 ]]. Action = [[-0.01713727 -0.23624551 -0.24384402  0.50970507]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1156 is [False, False, True, False, True, False]
Scene graph at timestep 1156 is [False, False, True, False, True, False]
State prediction error at timestep 1156 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1156 of 1
Current timestep = 1157. State = [[0.05674395 0.01775808]]. Action = [[ 0.08943179  0.18388712 -0.23284264  0.47297466]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1157 is [False, False, True, False, True, False]
Scene graph at timestep 1157 is [False, False, True, False, True, False]
State prediction error at timestep 1157 is tensor(2.7955e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1157 of 1
Current timestep = 1158. State = [[0.05470878 0.01342631]]. Action = [[-0.06874558 -0.06276879  0.2038705  -0.73342735]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1158 is [False, False, True, False, True, False]
Scene graph at timestep 1158 is [False, False, True, False, True, False]
State prediction error at timestep 1158 is tensor(1.2984e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1158 of 1
Current timestep = 1159. State = [[0.05122403 0.00585337]]. Action = [[0.21982786 0.11807212 0.22926086 0.07604825]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1159 is [False, False, True, False, True, False]
Current timestep = 1160. State = [[0.04680571 0.00773004]]. Action = [[-0.12204947  0.02997947  0.0634717   0.42437828]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1160 is [False, False, True, False, True, False]
Scene graph at timestep 1160 is [False, True, False, False, True, False]
State prediction error at timestep 1160 is tensor(6.4852e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1160 of 1
Current timestep = 1161. State = [[-0.23409063  0.02549532]]. Action = [[ 0.10463613 -0.13790289  0.24002528 -0.00673771]]. Reward = [100.]
Curr episode timestep = 25
Scene graph at timestep 1161 is [False, True, False, False, True, False]
Current timestep = 1162. State = [[-0.22640577  0.02961151]]. Action = [[ 0.07209641 -0.01038568 -0.12719107  0.7931931 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1162 is [True, False, False, False, True, False]
Current timestep = 1163. State = [[-0.222608    0.03993468]]. Action = [[-0.04355878  0.15659109 -0.20813017 -0.886843  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1163 is [True, False, False, False, True, False]
Scene graph at timestep 1163 is [True, False, False, False, True, False]
State prediction error at timestep 1163 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1163 of 1
Current timestep = 1164. State = [[-0.22245881  0.05199813]]. Action = [[ 4.7415853e-02  1.2320280e-04 -1.6693081e-01  8.2262444e-01]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1164 is [True, False, False, False, True, False]
Current timestep = 1165. State = [[-0.21636237  0.06513487]]. Action = [[ 0.08837351  0.20458567 -0.1793746   0.4102763 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1165 is [True, False, False, False, True, False]
Scene graph at timestep 1165 is [True, False, False, False, True, False]
State prediction error at timestep 1165 is tensor(9.6138e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1165 of 1
Current timestep = 1166. State = [[-0.21140727  0.09109286]]. Action = [[-0.20435394  0.14590293  0.19074798 -0.36298436]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1166 is [True, False, False, False, True, False]
Scene graph at timestep 1166 is [True, False, False, False, True, False]
State prediction error at timestep 1166 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1166 of -1
Current timestep = 1167. State = [[-0.2165053   0.10198744]]. Action = [[ 0.18902862 -0.09161517 -0.19354753 -0.9117469 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1167 is [True, False, False, False, True, False]
Scene graph at timestep 1167 is [True, False, False, False, True, False]
State prediction error at timestep 1167 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1167 of 1
Current timestep = 1168. State = [[-0.20058972  0.09819612]]. Action = [[0.19855887 0.04473016 0.21410608 0.5371094 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1168 is [True, False, False, False, True, False]
Scene graph at timestep 1168 is [True, False, False, False, True, False]
State prediction error at timestep 1168 is tensor(7.9147e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1168 of 1
Current timestep = 1169. State = [[-0.16869733  0.09943984]]. Action = [[ 0.16250676 -0.04263829 -0.01038113 -0.9598262 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1169 is [True, False, False, False, True, False]
Scene graph at timestep 1169 is [True, False, False, False, True, False]
State prediction error at timestep 1169 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1169 of 1
Current timestep = 1170. State = [[-0.14379685  0.08506253]]. Action = [[ 0.19738597 -0.19147906 -0.1953032  -0.1653071 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1170 is [True, False, False, False, True, False]
Scene graph at timestep 1170 is [True, False, False, False, True, False]
State prediction error at timestep 1170 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1170 of 1
Current timestep = 1171. State = [[-0.1171561   0.07199378]]. Action = [[ 0.21009088  0.02345946  0.1116035  -0.49070698]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1171 is [True, False, False, False, True, False]
Current timestep = 1172. State = [[-0.0996692   0.06079269]]. Action = [[-0.0477725  -0.20424674  0.11402613 -0.6947485 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1172 is [True, False, False, False, True, False]
Current timestep = 1173. State = [[-0.0886313   0.05032676]]. Action = [[ 0.24638247  0.03739136 -0.07245514 -0.52873397]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1173 is [True, False, False, False, True, False]
Scene graph at timestep 1173 is [True, False, False, False, True, False]
State prediction error at timestep 1173 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1173 of 1
Current timestep = 1174. State = [[-0.05755304  0.03598409]]. Action = [[ 0.1858939  -0.23857218  0.08804595  0.29915977]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1174 is [True, False, False, False, True, False]
Scene graph at timestep 1174 is [True, False, False, False, True, False]
State prediction error at timestep 1174 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1174 of 1
Current timestep = 1175. State = [[-0.15540175  0.15868556]]. Action = [[ 0.225469   -0.16542745 -0.16698481 -0.1516704 ]]. Reward = [100.]
Curr episode timestep = 13
Scene graph at timestep 1175 is [True, False, False, False, True, False]
Scene graph at timestep 1175 is [True, False, False, False, False, True]
State prediction error at timestep 1175 is tensor(0.0133, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1175 of 1
Current timestep = 1176. State = [[-0.12897144  0.17950575]]. Action = [[ 0.19052362  0.0543589  -0.02340747 -0.9201595 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1176 is [True, False, False, False, False, True]
Scene graph at timestep 1176 is [True, False, False, False, False, True]
State prediction error at timestep 1176 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1176 of 1
Current timestep = 1177. State = [[-0.1072632  0.172202 ]]. Action = [[ 0.02269617 -0.23815595  0.16080615 -0.34662747]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1177 is [True, False, False, False, False, True]
Scene graph at timestep 1177 is [True, False, False, False, False, True]
State prediction error at timestep 1177 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1177 of 1
Current timestep = 1178. State = [[-0.10636682  0.14970082]]. Action = [[-0.17162125 -0.1491107  -0.09239984 -0.8022    ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1178 is [True, False, False, False, False, True]
Scene graph at timestep 1178 is [True, False, False, False, False, True]
State prediction error at timestep 1178 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1178 of 1
Current timestep = 1179. State = [[-0.10374845  0.14046602]]. Action = [[ 0.22753328  0.05234128 -0.2346573   0.6501328 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1179 is [True, False, False, False, False, True]
Current timestep = 1180. State = [[-0.09849416  0.13111131]]. Action = [[ 0.01022872 -0.16196495 -0.04911225  0.6598289 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1180 is [True, False, False, False, False, True]
Scene graph at timestep 1180 is [True, False, False, False, False, True]
State prediction error at timestep 1180 is tensor(2.5475e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1180 of 1
Current timestep = 1181. State = [[-0.09111891  0.12384192]]. Action = [[ 0.12315613  0.0397813  -0.05942367  0.3514936 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1181 is [True, False, False, False, False, True]
Scene graph at timestep 1181 is [True, False, False, False, True, False]
State prediction error at timestep 1181 is tensor(1.1580e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1181 of 1
Current timestep = 1182. State = [[-0.07963342  0.11815029]]. Action = [[-0.16832878 -0.1611456   0.05920696  0.11231148]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1182 is [True, False, False, False, True, False]
Scene graph at timestep 1182 is [True, False, False, False, True, False]
State prediction error at timestep 1182 is tensor(1.6704e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1182 of 1
Current timestep = 1183. State = [[-0.08628433  0.09747029]]. Action = [[-0.18439235 -0.13829117  0.21361363  0.7880664 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1183 is [True, False, False, False, True, False]
Scene graph at timestep 1183 is [True, False, False, False, True, False]
State prediction error at timestep 1183 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1183 of -1
Current timestep = 1184. State = [[-0.09501357  0.07823174]]. Action = [[ 0.21681577 -0.11094895 -0.02906066 -0.952776  ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1184 is [True, False, False, False, True, False]
Current timestep = 1185. State = [[-0.09191599  0.06155605]]. Action = [[-0.11718719 -0.14319676  0.09186691 -0.6565721 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1185 is [True, False, False, False, True, False]
Scene graph at timestep 1185 is [True, False, False, False, True, False]
State prediction error at timestep 1185 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1185 of 1
Current timestep = 1186. State = [[-0.09543535  0.0575459 ]]. Action = [[-0.09759513  0.12281793  0.02430898  0.13993335]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1186 is [True, False, False, False, True, False]
Current timestep = 1187. State = [[-0.09631719  0.06076621]]. Action = [[ 0.12697726 -0.04728937  0.12696204 -0.9354311 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1187 is [True, False, False, False, True, False]
Scene graph at timestep 1187 is [True, False, False, False, True, False]
State prediction error at timestep 1187 is tensor(9.2182e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1187 of -1
Current timestep = 1188. State = [[-0.08848104  0.06896932]]. Action = [[ 0.21048719  0.17769378  0.08996272 -0.6128738 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1188 is [True, False, False, False, True, False]
Current timestep = 1189. State = [[-0.0764318   0.07192384]]. Action = [[ 0.0852938  -0.0856213   0.15719202 -0.4162581 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1189 is [True, False, False, False, True, False]
Current timestep = 1190. State = [[-0.06820512  0.06577614]]. Action = [[ 0.0068574  -0.07860756  0.09377444  0.15848374]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1190 is [True, False, False, False, True, False]
Scene graph at timestep 1190 is [True, False, False, False, True, False]
State prediction error at timestep 1190 is tensor(3.1334e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1190 of 1
Current timestep = 1191. State = [[-0.05944399  0.05241555]]. Action = [[ 0.13482916 -0.11860573  0.19263673  0.7758856 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1191 is [True, False, False, False, True, False]
Current timestep = 1192. State = [[-0.04817526  0.05060368]]. Action = [[ 0.05906773  0.13534844  0.12252095 -0.40277684]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1192 is [True, False, False, False, True, False]
Scene graph at timestep 1192 is [False, True, False, False, True, False]
State prediction error at timestep 1192 is tensor(4.2968e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1192 of 1
Current timestep = 1193. State = [[-0.03788287  0.04827078]]. Action = [[-0.10114583 -0.16086876  0.2198902  -0.43045443]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1193 is [False, True, False, False, True, False]
Current timestep = 1194. State = [[-0.0377919   0.04454956]]. Action = [[ 0.04173788  0.06065667  0.07750535 -0.89130753]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1194 is [False, True, False, False, True, False]
Current timestep = 1195. State = [[-0.04412437  0.03234987]]. Action = [[-0.24526845 -0.21445814 -0.19688977  0.399966  ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1195 is [False, True, False, False, True, False]
Current timestep = 1196. State = [[-0.04445304  0.02440184]]. Action = [[ 0.24474198  0.09858593  0.07306144 -0.7743625 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1196 is [False, True, False, False, True, False]
Scene graph at timestep 1196 is [False, True, False, False, True, False]
State prediction error at timestep 1196 is tensor(4.0950e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1196 of 1
Current timestep = 1197. State = [[-0.04334278  0.0375987 ]]. Action = [[-0.00231148  0.20909747 -0.15306033  0.6899003 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1197 is [False, True, False, False, True, False]
Scene graph at timestep 1197 is [False, True, False, False, True, False]
State prediction error at timestep 1197 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1197 of -1
Current timestep = 1198. State = [[-0.03754345  0.0526067 ]]. Action = [[ 0.16332066 -0.04618476  0.18199146 -0.85157514]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1198 is [False, True, False, False, True, False]
Current timestep = 1199. State = [[-0.18596582 -0.22915809]]. Action = [[-0.11652042 -0.15697409 -0.08378746 -0.41311377]]. Reward = [100.]
Curr episode timestep = 23
Scene graph at timestep 1199 is [False, True, False, False, True, False]
Current timestep = 1200. State = [[-0.16741747 -0.24581103]]. Action = [[ 0.12020397  0.16569373 -0.06582101  0.8571813 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1200 is [True, False, False, True, False, False]
Scene graph at timestep 1200 is [True, False, False, True, False, False]
State prediction error at timestep 1200 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1200 of 1
Current timestep = 1201. State = [[-0.14742225 -0.2215226 ]]. Action = [[ 0.22367042  0.22160584 -0.04984683 -0.6304809 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1201 is [True, False, False, True, False, False]
Current timestep = 1202. State = [[-0.1332743  -0.19824599]]. Action = [[-0.14662392  0.23093146 -0.08726874  0.36307573]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1202 is [True, False, False, True, False, False]
Scene graph at timestep 1202 is [True, False, False, True, False, False]
State prediction error at timestep 1202 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1202 of 1
Current timestep = 1203. State = [[-0.12934329 -0.18463211]]. Action = [[ 0.15001741 -0.17246881  0.04469281  0.05926955]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1203 is [True, False, False, True, False, False]
Current timestep = 1204. State = [[-0.11513573 -0.18991153]]. Action = [[0.24072465 0.0083524  0.19832605 0.9010438 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1204 is [True, False, False, True, False, False]
Current timestep = 1205. State = [[-0.08495174 -0.18113983]]. Action = [[0.15689182 0.20962977 0.13132104 0.08935201]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1205 is [True, False, False, True, False, False]
Scene graph at timestep 1205 is [True, False, False, True, False, False]
State prediction error at timestep 1205 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1205 of 1
Current timestep = 1206. State = [[-0.06188558 -0.16253886]]. Action = [[ 0.13511169  0.07807562 -0.15539797 -0.5255056 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1206 is [True, False, False, True, False, False]
Current timestep = 1207. State = [[-0.042039   -0.14938627]]. Action = [[0.16420072 0.11986715 0.09365767 0.5680406 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1207 is [True, False, False, True, False, False]
Current timestep = 1208. State = [[-0.02070607 -0.13869902]]. Action = [[ 0.1300788   0.02670479  0.22108674 -0.8014695 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1208 is [False, True, False, True, False, False]
Scene graph at timestep 1208 is [False, True, False, True, False, False]
State prediction error at timestep 1208 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1208 of 1
Current timestep = 1209. State = [[ 0.00387605 -0.13008729]]. Action = [[ 0.18138143  0.06093335 -0.03052127 -0.944612  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1209 is [False, True, False, True, False, False]
Current timestep = 1210. State = [[ 0.02020259 -0.12274633]]. Action = [[-0.0410461   0.05969074 -0.02999428 -0.09115887]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1210 is [False, True, False, True, False, False]
Scene graph at timestep 1210 is [False, True, False, False, True, False]
State prediction error at timestep 1210 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1210 of 1
Current timestep = 1211. State = [[ 0.02361701 -0.11332625]]. Action = [[0.10809791 0.05860397 0.14021304 0.04963934]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1211 is [False, True, False, False, True, False]
Scene graph at timestep 1211 is [False, True, False, False, True, False]
State prediction error at timestep 1211 is tensor(3.7026e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1211 of 1
Current timestep = 1212. State = [[ 0.03700736 -0.10932936]]. Action = [[ 0.21150959  0.2282359  -0.17676803 -0.63141346]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1212 is [False, True, False, False, True, False]
Current timestep = 1213. State = [[ 0.03640212 -0.10487872]]. Action = [[-0.06810714  0.06216413 -0.04380447  0.42559242]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1213 is [False, True, False, False, True, False]
Scene graph at timestep 1213 is [False, True, False, False, True, False]
State prediction error at timestep 1213 is tensor(1.3353e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1213 of 1
Current timestep = 1214. State = [[-0.16784877  0.15729126]]. Action = [[ 0.12036699  0.17294249 -0.11008757  0.94044685]]. Reward = [100.]
Curr episode timestep = 14
Scene graph at timestep 1214 is [False, True, False, False, True, False]
Current timestep = 1215. State = [[-0.14798966  0.17249098]]. Action = [[-0.08353344 -0.11726491  0.09681043 -0.25396705]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1215 is [True, False, False, False, False, True]
Current timestep = 1216. State = [[-0.14415337  0.16716863]]. Action = [[ 0.17636237  0.00084558 -0.18256551 -0.3428192 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1216 is [True, False, False, False, False, True]
Scene graph at timestep 1216 is [True, False, False, False, False, True]
State prediction error at timestep 1216 is tensor(1.4850e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1216 of 1
Current timestep = 1217. State = [[-0.14332412  0.1646069 ]]. Action = [[-0.14812042 -0.00983372 -0.22207536 -0.79401755]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1217 is [True, False, False, False, False, True]
Current timestep = 1218. State = [[-0.14085467  0.1585448 ]]. Action = [[ 0.15465415 -0.08253145  0.22711647 -0.66660404]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1218 is [True, False, False, False, False, True]
Current timestep = 1219. State = [[-0.132981    0.14506763]]. Action = [[ 1.2276387e-01 -1.0819709e-01  5.6120753e-04  6.7429018e-01]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1219 is [True, False, False, False, False, True]
Scene graph at timestep 1219 is [True, False, False, False, False, True]
State prediction error at timestep 1219 is tensor(3.9919e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1219 of 1
Current timestep = 1220. State = [[-0.10627552  0.13229646]]. Action = [[ 0.24717566 -0.04219876  0.1592421  -0.57470447]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1220 is [True, False, False, False, False, True]
Current timestep = 1221. State = [[-0.08456577  0.1283726 ]]. Action = [[ 0.03962761 -0.04932296 -0.16407844 -0.657304  ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1221 is [True, False, False, False, False, True]
Current timestep = 1222. State = [[-0.07081629  0.12012297]]. Action = [[ 0.16892463 -0.06326154 -0.01351276 -0.4956479 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1222 is [True, False, False, False, False, True]
Scene graph at timestep 1222 is [True, False, False, False, True, False]
State prediction error at timestep 1222 is tensor(6.9254e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1222 of 1
Current timestep = 1223. State = [[-0.04160819  0.101153  ]]. Action = [[ 0.24485084 -0.21704634  0.09399879  0.07349586]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1223 is [True, False, False, False, True, False]
Scene graph at timestep 1223 is [False, True, False, False, True, False]
State prediction error at timestep 1223 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1223 of 1
Current timestep = 1224. State = [[-0.15152226 -0.09167358]]. Action = [[-0.22111438  0.10589355  0.17987838 -0.56188077]]. Reward = [100.]
Curr episode timestep = 9
Scene graph at timestep 1224 is [False, True, False, False, True, False]
Current timestep = 1225. State = [[-0.12436663 -0.10846264]]. Action = [[ 0.22619528 -0.10476132  0.16099033  0.04879093]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1225 is [True, False, False, False, True, False]
Current timestep = 1226. State = [[-0.10770752 -0.10565333]]. Action = [[-0.03633791  0.18752655 -0.07255158  0.63089323]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1226 is [True, False, False, False, True, False]
Scene graph at timestep 1226 is [True, False, False, False, True, False]
State prediction error at timestep 1226 is tensor(5.7914e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1226 of 1
Current timestep = 1227. State = [[-0.10406897 -0.09983809]]. Action = [[ 0.02138585 -0.07344511  0.09750867  0.7947738 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1227 is [True, False, False, False, True, False]
Scene graph at timestep 1227 is [True, False, False, False, True, False]
State prediction error at timestep 1227 is tensor(1.8345e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1227 of 1
Current timestep = 1228. State = [[-0.10466852 -0.09140541]]. Action = [[-0.08304164  0.20601588  0.00639743  0.7965783 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1228 is [True, False, False, False, True, False]
Current timestep = 1229. State = [[-0.10243327 -0.08142547]]. Action = [[ 0.15185809 -0.00836068  0.19718379  0.01744187]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1229 is [True, False, False, False, True, False]
Current timestep = 1230. State = [[-0.08694007 -0.08624515]]. Action = [[ 0.21410716 -0.13197921 -0.05672197  0.39181828]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1230 is [True, False, False, False, True, False]
Scene graph at timestep 1230 is [True, False, False, False, True, False]
State prediction error at timestep 1230 is tensor(4.4575e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1230 of 1
Current timestep = 1231. State = [[-0.065683   -0.08032715]]. Action = [[-0.14751351  0.2364828   0.07929683  0.11411393]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1231 is [True, False, False, False, True, False]
Current timestep = 1232. State = [[-0.06729938 -0.07537429]]. Action = [[ 0.07838482 -0.15607102  0.17783639  0.59477496]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1232 is [True, False, False, False, True, False]
Current timestep = 1233. State = [[-0.06265681 -0.07439534]]. Action = [[ 0.15196308  0.11624333 -0.10610956  0.80970097]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1233 is [True, False, False, False, True, False]
Current timestep = 1234. State = [[-0.04761278 -0.06951314]]. Action = [[ 0.1840741   0.00397938  0.1033909  -0.6754461 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1234 is [True, False, False, False, True, False]
Current timestep = 1235. State = [[-0.20999537 -0.10603306]]. Action = [[-0.18491855  0.21912089 -0.06902675  0.50912166]]. Reward = [100.]
Curr episode timestep = 10
Scene graph at timestep 1235 is [False, True, False, False, True, False]
Current timestep = 1236. State = [[-0.19070621 -0.11957512]]. Action = [[ 0.19144285 -0.03579839  0.18997085  0.7179897 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1236 is [True, False, False, False, True, False]
Current timestep = 1237. State = [[-0.1723725  -0.12046817]]. Action = [[ 0.067148    0.07349551 -0.13877282  0.84080434]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1237 is [True, False, False, False, True, False]
Scene graph at timestep 1237 is [True, False, False, False, True, False]
State prediction error at timestep 1237 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1237 of 1
Current timestep = 1238. State = [[-0.16754176 -0.10959949]]. Action = [[-0.1755334   0.15707552  0.16404164 -0.63058484]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1238 is [True, False, False, False, True, False]
Scene graph at timestep 1238 is [True, False, False, False, True, False]
State prediction error at timestep 1238 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1238 of 1
Current timestep = 1239. State = [[-0.16446187 -0.0890803 ]]. Action = [[ 0.22508597  0.15473413 -0.23547108 -0.9637872 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1239 is [True, False, False, False, True, False]
Current timestep = 1240. State = [[-0.15192203 -0.08264431]]. Action = [[ 0.11489168 -0.08890155 -0.11586294  0.6012274 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1240 is [True, False, False, False, True, False]
Current timestep = 1241. State = [[-0.13649051 -0.08597704]]. Action = [[ 0.08228946 -0.0476571   0.15899283  0.43590188]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1241 is [True, False, False, False, True, False]
Scene graph at timestep 1241 is [True, False, False, False, True, False]
State prediction error at timestep 1241 is tensor(2.1494e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1241 of 1
Current timestep = 1242. State = [[-0.1186311 -0.0998928]]. Action = [[ 0.12333149 -0.17679003 -0.01545116  0.5447538 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1242 is [True, False, False, False, True, False]
Scene graph at timestep 1242 is [True, False, False, False, True, False]
State prediction error at timestep 1242 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1242 of 1
Current timestep = 1243. State = [[-0.09795271 -0.1077373 ]]. Action = [[ 0.23238021  0.11431879 -0.2085812  -0.38837397]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1243 is [True, False, False, False, True, False]
Current timestep = 1244. State = [[-0.07398383 -0.1039725 ]]. Action = [[ 0.07811639  0.02707031  0.12760496 -0.23120409]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1244 is [True, False, False, False, True, False]
Scene graph at timestep 1244 is [True, False, False, False, True, False]
State prediction error at timestep 1244 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1244 of 1
Current timestep = 1245. State = [[-0.06401024 -0.09247419]]. Action = [[-0.07609397  0.12121058 -0.16452028  0.7940903 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1245 is [True, False, False, False, True, False]
Current timestep = 1246. State = [[-0.05733543 -0.07514614]]. Action = [[ 0.2137484   0.13372976 -0.03316614  0.6081315 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1246 is [True, False, False, False, True, False]
Current timestep = 1247. State = [[-0.05087883 -0.05924316]]. Action = [[-0.1581624   0.10806376 -0.22779205 -0.16292971]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1247 is [True, False, False, False, True, False]
Scene graph at timestep 1247 is [True, False, False, False, True, False]
State prediction error at timestep 1247 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1247 of 1
Current timestep = 1248. State = [[-0.05681748 -0.05976893]]. Action = [[-0.13584056 -0.18907043 -0.20320992  0.30552328]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1248 is [True, False, False, False, True, False]
Scene graph at timestep 1248 is [True, False, False, False, True, False]
State prediction error at timestep 1248 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1248 of -1
Current timestep = 1249. State = [[-0.06193846 -0.06251992]]. Action = [[-0.02419673  0.20796022  0.17842582 -0.06841183]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1249 is [True, False, False, False, True, False]
