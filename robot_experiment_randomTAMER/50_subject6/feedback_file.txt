Current timestep = 0. State = [[-0.2070065   0.22539605]]. Action = [[ 0.06296539 -0.07997203  0.10207781  0.6931701 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Current timestep = 1. State = [[-0.21319936  0.23324563]]. Action = [[-0.2329208   0.16457558 -0.17289045 -0.9050901 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, False, True]
Scene graph at timestep 1 is [True, False, False, False, False, True]
State prediction error at timestep 1 is tensor(0.0592, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.22130823  0.23672917]]. Action = [[ 0.11263984 -0.17558606  0.23866346 -0.82140326]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, False, True]
Scene graph at timestep 2 is [True, False, False, False, False, True]
State prediction error at timestep 2 is tensor(0.0388, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2 of 1
Current timestep = 3. State = [[-0.21677664  0.21236733]]. Action = [[-0.15695119 -0.21784724 -0.08752307 -0.94052887]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, False, True]
Scene graph at timestep 3 is [True, False, False, False, False, True]
State prediction error at timestep 3 is tensor(0.0284, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3 of 0
Current timestep = 4. State = [[-0.22998343  0.19381714]]. Action = [[-0.22492737 -0.08030173 -0.01225802 -0.96606785]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, False, True]
Current timestep = 5. State = [[-0.25498384  0.20315158]]. Action = [[-0.15814684  0.22563273 -0.17808409 -0.81452495]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, False, True]
Current timestep = 6. State = [[-0.27079102  0.21770167]]. Action = [[-0.22981378 -0.2362774  -0.01322864 -0.94047016]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, False, True]
Current timestep = 7. State = [[-0.26441926  0.20801882]]. Action = [[ 0.24866313 -0.1609566   0.10780233 -0.4378234 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, False, True]
Scene graph at timestep 7 is [True, False, False, False, False, True]
State prediction error at timestep 7 is tensor(0.0181, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.25546157  0.19374695]]. Action = [[-0.12276541 -0.00734946  0.23063004  0.69208956]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, False, True]
Scene graph at timestep 8 is [True, False, False, False, False, True]
State prediction error at timestep 8 is tensor(0.0220, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.25085086  0.20415437]]. Action = [[0.11637729 0.22306252 0.18127215 0.40301907]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, False, True]
Scene graph at timestep 9 is [True, False, False, False, False, True]
State prediction error at timestep 9 is tensor(0.0215, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9 of 0
Current timestep = 10. State = [[-0.24410819  0.2151431 ]]. Action = [[-0.11525455 -0.11691992 -0.12379982 -0.86257607]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, False, True]
Scene graph at timestep 10 is [True, False, False, False, False, True]
State prediction error at timestep 10 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 10 of 0
Current timestep = 11. State = [[-0.24402665  0.2098684 ]]. Action = [[ 0.09658808  0.01149267 -0.1868575  -0.25880003]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, False, True]
Current timestep = 12. State = [[-0.23698202  0.20525326]]. Action = [[ 0.13121617 -0.05254132  0.24409223 -0.14347428]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, False, True]
Scene graph at timestep 12 is [True, False, False, False, False, True]
State prediction error at timestep 12 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 12 of 1
Current timestep = 13. State = [[-0.2305521   0.19862482]]. Action = [[-0.16003674 -0.0779655  -0.1097914  -0.09721869]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, False, True]
Scene graph at timestep 13 is [True, False, False, False, False, True]
State prediction error at timestep 13 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of 0
Current timestep = 14. State = [[-0.24131691  0.1905976 ]]. Action = [[-0.1945559  -0.09042171  0.20059022 -0.14428467]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, False, True]
Current timestep = 15. State = [[-0.25120896  0.1851177 ]]. Action = [[-0.23034163 -0.1515714  -0.22879045 -0.08439565]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, False, True]
Current timestep = 16. State = [[-0.2535341  0.1851843]]. Action = [[ 0.03014824  0.05111599 -0.15845153 -0.69236785]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, False, True]
Scene graph at timestep 16 is [True, False, False, False, False, True]
State prediction error at timestep 16 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of -1
Current timestep = 17. State = [[-0.24975511  0.1934869 ]]. Action = [[ 0.17591998  0.15729177  0.24495468 -0.9520505 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, False, True]
Current timestep = 18. State = [[-0.24022615  0.1971085 ]]. Action = [[ 0.09622017 -0.07847343  0.19025648  0.5764332 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, False, True]
Current timestep = 19. State = [[-0.22580822  0.19261043]]. Action = [[ 0.19599915 -0.04569393 -0.2136372   0.8004278 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, False, True]
Scene graph at timestep 19 is [True, False, False, False, False, True]
State prediction error at timestep 19 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 19 of 1
Current timestep = 20. State = [[-0.19784275  0.18712743]]. Action = [[ 0.19718063 -0.02226007 -0.20160282  0.00088894]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, False, True]
Current timestep = 21. State = [[-0.17346029  0.18937959]]. Action = [[0.17310551 0.07044715 0.24141169 0.7106304 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, False, True]
Current timestep = 22. State = [[-0.158447   0.1829009]]. Action = [[-0.10497263 -0.20968659  0.10423407  0.8543453 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, False, True]
Scene graph at timestep 22 is [True, False, False, False, False, True]
State prediction error at timestep 22 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 22 of 1
Current timestep = 23. State = [[-0.15618932  0.17025591]]. Action = [[ 0.02450684  0.04938984 -0.02144384 -0.91049415]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, False, True]
Scene graph at timestep 23 is [True, False, False, False, False, True]
State prediction error at timestep 23 is tensor(0.0113, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 23 of 1
Current timestep = 24. State = [[-0.15797415  0.1657552 ]]. Action = [[-0.14858645 -0.14592852 -0.00798635  0.39899445]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, False, True]
Current timestep = 25. State = [[-0.16509187  0.14905924]]. Action = [[-0.11440492 -0.18565574  0.16957945  0.68166804]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, False, True]
Scene graph at timestep 25 is [True, False, False, False, False, True]
State prediction error at timestep 25 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 25 of 0
Current timestep = 26. State = [[-0.18268     0.11998948]]. Action = [[-0.18386963 -0.18117927 -0.02608892 -0.29589152]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, False, True]
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 26 of -1
Current timestep = 27. State = [[-0.2076171   0.09367854]]. Action = [[-0.11238658 -0.14261472 -0.17184994  0.6985986 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 27 of -1
Current timestep = 28. State = [[-0.22282967  0.08665926]]. Action = [[-0.19235066  0.07877108  0.1097419   0.96857953]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
Current timestep = 29. State = [[-0.24010621  0.08691075]]. Action = [[-0.0171628  -0.07641622 -0.19936946 -0.19946778]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
Current timestep = 30. State = [[-0.2533345  0.0839747]]. Action = [[-0.13250984  0.01914507  0.20873773  0.45379233]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
Current timestep = 31. State = [[-0.26108545  0.0722673 ]]. Action = [[ 0.00593963 -0.17209797 -0.10134143 -0.971218  ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
Current timestep = 32. State = [[-0.2624919   0.07096782]]. Action = [[0.14682013 0.20156425 0.20921266 0.67021036]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
Current timestep = 33. State = [[-0.25974795  0.07033981]]. Action = [[-0.06088802 -0.17749935 -0.06952    -0.5318437 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
Current timestep = 34. State = [[-0.26298374  0.0504866 ]]. Action = [[-0.0498092  -0.1932338   0.02131304 -0.19147748]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0115, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of 0
Current timestep = 35. State = [[-0.26605344  0.04284362]]. Action = [[0.07185012 0.2044146  0.18966702 0.6344067 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
Current timestep = 36. State = [[-0.256212    0.04214107]]. Action = [[ 0.1848545  -0.17279783  0.03327945 -0.80162585]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
Current timestep = 37. State = [[-0.23786634  0.04378923]]. Action = [[ 0.23066658  0.16356573 -0.0973597  -0.45239192]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
Current timestep = 38. State = [[-0.21928868  0.03951754]]. Action = [[-0.03817412 -0.21916518 -0.11259632 -0.70631796]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
Current timestep = 39. State = [[-0.2116423   0.02552066]]. Action = [[ 0.14026642 -0.02234007 -0.09323947  0.2870201 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
Current timestep = 40. State = [[-0.20066729  0.01003932]]. Action = [[ 0.09458452 -0.17693424 -0.11842015  0.47023308]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
Current timestep = 41. State = [[-0.19038786 -0.01219369]]. Action = [[-0.03889233 -0.15023994  0.24616629 -0.25199795]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0093, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.19010898 -0.02832114]]. Action = [[-0.1226453  -0.02185303 -0.09269243 -0.6701872 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0109, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of -1
Current timestep = 43. State = [[-0.19399896 -0.02366667]]. Action = [[ 0.105452    0.14483863 -0.15046763 -0.7925789 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
Scene graph at timestep 43 is [True, False, False, False, True, False]
State prediction error at timestep 43 is tensor(0.0109, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of 0
Current timestep = 44. State = [[-0.19286473 -0.00398185]]. Action = [[ 0.00657159  0.17633462  0.05833474 -0.86734504]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0092, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 44 of 0
Current timestep = 45. State = [[-0.19514298  0.01324919]]. Action = [[-0.13972084  0.03786308 -0.24226463 -0.50322425]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 45 of -1
Current timestep = 46. State = [[-0.20486182  0.02672979]]. Action = [[-0.14686611  0.12825418  0.15532845  0.22656846]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of -1
Current timestep = 47. State = [[-0.22143455  0.04059695]]. Action = [[-0.12261811  0.02179933 -0.06460012  0.6818187 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
Scene graph at timestep 47 is [True, False, False, False, True, False]
State prediction error at timestep 47 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 47 of -1
Current timestep = 48. State = [[-0.22753614  0.03417884]]. Action = [[ 0.15958607 -0.1451438  -0.19475266  0.5908226 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
Current timestep = 49. State = [[-0.23057394  0.02171622]]. Action = [[-0.19340351 -0.05556324 -0.23924327 -0.72637576]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 49 of 0
Current timestep = 50. State = [[-0.23996931  0.01080094]]. Action = [[-0.02957533 -0.02834108  0.17561814 -0.6205905 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 50 of 0
Current timestep = 51. State = [[-0.24091993  0.00873059]]. Action = [[-0.23048379 -0.19658537  0.09885374  0.49938774]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 51 of 0
Current timestep = 52. State = [[-0.23528834 -0.0041413 ]]. Action = [[ 0.16877288 -0.21459082 -0.22050647 -0.3054155 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
Scene graph at timestep 52 is [True, False, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.223747   -0.01397625]]. Action = [[0.1643171  0.13404429 0.1953989  0.791657  ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
Current timestep = 54. State = [[-0.21263137 -0.0193794 ]]. Action = [[ 0.08194846 -0.21640104  0.1520366  -0.03808534]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
Current timestep = 55. State = [[-0.2061666  -0.01846829]]. Action = [[-0.04907987  0.23557335  0.02142668  0.01354778]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
Current timestep = 56. State = [[-0.195354    0.00187637]]. Action = [[0.23295373 0.14011425 0.22870263 0.5799091 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of 1
Current timestep = 57. State = [[-0.1758385   0.00867429]]. Action = [[-0.03508088 -0.13115866  0.09607217 -0.9071684 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
Current timestep = 58. State = [[-0.17097731 -0.00740635]]. Action = [[ 0.13203618 -0.19067115 -0.16155893  0.91851306]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0091, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 58 of 1
Current timestep = 59. State = [[-0.16491832 -0.02796075]]. Action = [[ 0.03899792 -0.03918311 -0.01233369  0.9101974 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
Scene graph at timestep 59 is [True, False, False, False, True, False]
State prediction error at timestep 59 is tensor(0.0106, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 59 of 1
Current timestep = 60. State = [[-0.15335348 -0.02811716]]. Action = [[0.21739441 0.06121123 0.0233756  0.29276013]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
Current timestep = 61. State = [[-0.13793631 -0.03137042]]. Action = [[-0.15369198 -0.12971061  0.09918293 -0.08265322]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
Current timestep = 62. State = [[-0.13230978 -0.04694711]]. Action = [[ 0.23273742 -0.15059818  0.00042757  0.40673554]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
Current timestep = 63. State = [[-0.12199663 -0.064373  ]]. Action = [[-0.10372166 -0.06341517 -0.19601329  0.8738208 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
Current timestep = 64. State = [[-0.12181206 -0.07463259]]. Action = [[-0.02060127 -0.06688702  0.21018913 -0.53228873]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
Current timestep = 65. State = [[-0.11788322 -0.08028293]]. Action = [[ 1.5989500e-01  5.7080388e-04 -2.3621036e-01 -6.1308151e-01]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
Current timestep = 66. State = [[-0.11555379 -0.08747438]]. Action = [[-0.15084477 -0.08775261 -0.23181474 -0.80481756]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(0.0164, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 66 of 1
Current timestep = 67. State = [[-0.12434716 -0.09821841]]. Action = [[-0.196137   -0.01264472 -0.21701685 -0.3509462 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0122, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.14290181 -0.08861946]]. Action = [[-0.10678373  0.23345453  0.12626928  0.8529552 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
Scene graph at timestep 68 is [True, False, False, False, True, False]
State prediction error at timestep 68 is tensor(0.0155, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 68 of -1
Current timestep = 69. State = [[-0.14561877 -0.07014445]]. Action = [[ 0.20434809 -0.04584689  0.18302464  0.18791485]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
Scene graph at timestep 69 is [True, False, False, False, True, False]
State prediction error at timestep 69 is tensor(0.0102, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of 1
Current timestep = 70. State = [[-0.13978763 -0.06634586]]. Action = [[ 0.06180322  0.09402007 -0.12288161 -0.18102229]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0068, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of 1
Current timestep = 71. State = [[-0.13949727 -0.04997219]]. Action = [[-0.21907225  0.18791181 -0.23714858  0.49858928]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
Current timestep = 72. State = [[-0.14138047 -0.03821181]]. Action = [[ 0.20395634 -0.04029754 -0.11956258  0.63344765]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
Current timestep = 73. State = [[-0.14086102 -0.04177152]]. Action = [[-0.14604418 -0.07315183 -0.05303222  0.09409094]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
Scene graph at timestep 73 is [True, False, False, False, True, False]
State prediction error at timestep 73 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-0.13932784 -0.04827557]]. Action = [[ 0.17112511 -0.04640919 -0.12387195  0.21586001]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
Current timestep = 75. State = [[-0.13048989 -0.05139735]]. Action = [[1.3436407e-01 6.6804886e-04 4.9185395e-02 9.8778832e-01]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
Scene graph at timestep 75 is [True, False, False, False, True, False]
State prediction error at timestep 75 is tensor(0.0124, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of 1
Current timestep = 76. State = [[-0.12079264 -0.06618605]]. Action = [[-0.10141873 -0.23924144 -0.03949031  0.57960296]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
Current timestep = 77. State = [[-0.11629523 -0.0827554 ]]. Action = [[ 0.14742187  0.01398754 -0.09058765 -0.01259428]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
Scene graph at timestep 77 is [True, False, False, False, True, False]
State prediction error at timestep 77 is tensor(0.0088, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 77 of 1
Current timestep = 78. State = [[-0.10836864 -0.09059483]]. Action = [[ 0.13399187 -0.08236465 -0.08326685  0.35354972]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
Scene graph at timestep 78 is [True, False, False, False, True, False]
State prediction error at timestep 78 is tensor(0.0119, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 78 of 1
Current timestep = 79. State = [[-0.09488928 -0.09263729]]. Action = [[-0.18377604  0.096544    0.22685999 -0.33177555]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0099, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.09512702 -0.10073828]]. Action = [[ 0.13370946 -0.22139642  0.23338884 -0.80096585]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
Current timestep = 81. State = [[-0.09069435 -0.11472164]]. Action = [[ 0.09399968 -0.02459645  0.16536775 -0.4963771 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
Scene graph at timestep 81 is [True, False, False, False, True, False]
State prediction error at timestep 81 is tensor(0.0140, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 81 of 1
Current timestep = 82. State = [[-0.0803782  -0.11451574]]. Action = [[0.1064634  0.0729304  0.23593819 0.050372  ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
Current timestep = 83. State = [[-0.07229689 -0.11100733]]. Action = [[-0.07366313  0.00211585  0.24350625 -0.64187884]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
Current timestep = 84. State = [[-0.07780014 -0.10276011]]. Action = [[-0.20582406  0.14459431 -0.04226008 -0.6525522 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
Scene graph at timestep 84 is [True, False, False, False, True, False]
State prediction error at timestep 84 is tensor(0.0130, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of 0
Current timestep = 85. State = [[-0.08154537 -0.09687252]]. Action = [[ 0.08928096 -0.08313291  0.16928214 -0.5182212 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(0.0112, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 85 of 0
Current timestep = 86. State = [[-0.08204781 -0.09089608]]. Action = [[-0.00963101  0.15853584 -0.16835837  0.19343781]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
Current timestep = 87. State = [[-0.08541439 -0.07327011]]. Action = [[-0.14777926  0.16934443 -0.22150427  0.22215271]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
Current timestep = 88. State = [[-0.08958736 -0.06128282]]. Action = [[ 0.09462819 -0.03353849  0.21138322 -0.34500873]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of -1
Current timestep = 89. State = [[-0.08972319 -0.05390891]]. Action = [[ 0.03351307  0.11891904  0.09459001 -0.6751278 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 89 of -1
Current timestep = 90. State = [[-0.0827283  -0.05019163]]. Action = [[ 0.19489858 -0.10582918 -0.03995156 -0.93598676]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of 1
Current timestep = 91. State = [[-0.07612824 -0.06271429]]. Action = [[-0.17459516 -0.14302787  0.09206951 -0.20159483]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
Current timestep = 92. State = [[-0.0783684  -0.07366847]]. Action = [[ 0.03209689 -0.03246126  0.12624237  0.8940971 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
Current timestep = 93. State = [[-0.07541656 -0.08723618]]. Action = [[ 0.12438542 -0.16228715  0.22305965  0.21415329]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
Scene graph at timestep 93 is [True, False, False, False, True, False]
State prediction error at timestep 93 is tensor(0.0098, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 93 of 0
Current timestep = 94. State = [[-0.06678856 -0.09669714]]. Action = [[0.13231218 0.10999945 0.21572185 0.25640798]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0097, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.05620702 -0.0961068 ]]. Action = [[-0.14354506 -0.09789421 -0.11621827  0.83263683]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
Scene graph at timestep 95 is [True, False, False, False, True, False]
State prediction error at timestep 95 is tensor(0.0147, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 95 of -1
Current timestep = 96. State = [[-0.06748497 -0.1156987 ]]. Action = [[-0.1957359  -0.20766336 -0.07281955 -0.7145654 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
Current timestep = 97. State = [[-0.07757451 -0.12671465]]. Action = [[ 0.12507376  0.06318021 -0.1706634  -0.6403046 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
Current timestep = 98. State = [[-0.08124133 -0.11499638]]. Action = [[-0.18854074  0.20893645 -0.21771906 -0.1484139 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, True, False, False]
Current timestep = 99. State = [[-0.08824933 -0.09294179]]. Action = [[-0.03910987  0.12644076  0.16904688 -0.50168455]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
Current timestep = 100. State = [[-0.09858079 -0.08996287]]. Action = [[-0.14052545 -0.1303293  -0.02525692 -0.4487151 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 100 of -1
Current timestep = 101. State = [[-0.10649324 -0.10817171]]. Action = [[ 0.24075997 -0.18375483 -0.16959125  0.6342175 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
Current timestep = 102. State = [[-0.10239254 -0.11846637]]. Action = [[-0.06302403 -0.00870046  0.14449584 -0.83463186]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
Current timestep = 103. State = [[-0.10291786 -0.13020265]]. Action = [[-0.02070291 -0.14746122 -0.19361205  0.06346583]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
Current timestep = 104. State = [[-0.09506463 -0.14036688]]. Action = [[ 0.24187833  0.00802645 -0.20590617 -0.7801414 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, True, False, False]
Scene graph at timestep 104 is [True, False, False, True, False, False]
State prediction error at timestep 104 is tensor(0.0129, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 104 of 1
Current timestep = 105. State = [[-0.07374189 -0.13888173]]. Action = [[ 0.14781696  0.0500882  -0.22074555 -0.7979356 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, True, False, False]
Current timestep = 106. State = [[-0.06529707 -0.1255679 ]]. Action = [[-0.12115274  0.20810872 -0.05349609  0.01365519]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, True, False, False]
Scene graph at timestep 106 is [True, False, False, True, False, False]
State prediction error at timestep 106 is tensor(0.0085, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 106 of 1
Current timestep = 107. State = [[-0.06455361 -0.12252716]]. Action = [[ 0.08399919 -0.21436931 -0.22973326  0.27278435]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, True, False, False]
Scene graph at timestep 107 is [True, False, False, False, True, False]
State prediction error at timestep 107 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 107 of -1
Current timestep = 108. State = [[-0.06183689 -0.13443834]]. Action = [[0.04807013 0.02717069 0.19604218 0.9647527 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
Scene graph at timestep 108 is [True, False, False, True, False, False]
State prediction error at timestep 108 is tensor(0.0174, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of 0
Current timestep = 109. State = [[-0.05455598 -0.13867362]]. Action = [[ 0.10694104 -0.10167804  0.11358064 -0.8162586 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, True, False, False]
Current timestep = 110. State = [[-0.04944895 -0.15734094]]. Action = [[-0.10524309 -0.19995114 -0.19633597 -0.0296737 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, True, False, False]
Current timestep = 111. State = [[-0.04292504 -0.17513016]]. Action = [[ 0.20819628 -0.04200949  0.16953605 -0.9201532 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [False, True, False, True, False, False]
Scene graph at timestep 111 is [False, True, False, True, False, False]
State prediction error at timestep 111 is tensor(0.0211, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 111 of -1
Current timestep = 112. State = [[-0.02831565 -0.18714982]]. Action = [[-0.09795704 -0.07626119  0.05874157  0.1783253 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [False, True, False, True, False, False]
Current timestep = 113. State = [[-0.02249653 -0.19879232]]. Action = [[ 0.2453304  -0.1087634  -0.21613918  0.5658665 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [False, True, False, True, False, False]
Scene graph at timestep 113 is [False, True, False, True, False, False]
State prediction error at timestep 113 is tensor(0.0271, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of -1
Current timestep = 114. State = [[ 0.00193968 -0.21391097]]. Action = [[ 0.1640073  -0.09816612 -0.11999142  0.57032704]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [False, True, False, True, False, False]
Current timestep = 115. State = [[ 0.01538967 -0.22926164]]. Action = [[-0.03426839 -0.12813638 -0.21742669  0.9354522 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [False, True, False, True, False, False]
Current timestep = 116. State = [[ 0.02415187 -0.23474649]]. Action = [[ 0.19519764  0.05760667 -0.24196447 -0.04824454]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [False, True, False, True, False, False]
Scene graph at timestep 116 is [False, True, False, True, False, False]
State prediction error at timestep 116 is tensor(0.0310, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 116 of -1
Current timestep = 117. State = [[ 0.04600301 -0.22779825]]. Action = [[-0.08887544  0.1267606   0.04108241  0.21053565]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [False, True, False, True, False, False]
Current timestep = 118. State = [[ 0.04643259 -0.22102925]]. Action = [[ 0.02606279  0.03911406  0.158171   -0.09315187]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [False, True, False, True, False, False]
Current timestep = 119. State = [[ 0.04665868 -0.21817556]]. Action = [[ 0.23823172 -0.16725965 -0.12142554  0.04337275]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [False, True, False, True, False, False]
Scene graph at timestep 119 is [False, True, False, True, False, False]
State prediction error at timestep 119 is tensor(0.0310, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[ 0.04985468 -0.20773494]]. Action = [[0.07310244 0.1733214  0.11543202 0.8630159 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [False, True, False, True, False, False]
Current timestep = 121. State = [[ 0.0508244  -0.19341128]]. Action = [[ 0.20675653 -0.22448307  0.16720355  0.96337533]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [False, True, False, True, False, False]
Current timestep = 122. State = [[ 0.05093795 -0.19141094]]. Action = [[ 0.11830446 -0.09135096 -0.21874633  0.88309026]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [False, False, True, True, False, False]
Current timestep = 123. State = [[ 0.05039966 -0.19468634]]. Action = [[-0.09042227 -0.07258183  0.11972433 -0.48682964]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [False, False, True, True, False, False]
Current timestep = 124. State = [[ 0.04926509 -0.19617294]]. Action = [[ 0.15668663  0.20674953 -0.15298198  0.57298076]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [False, False, True, True, False, False]
Scene graph at timestep 124 is [False, True, False, True, False, False]
State prediction error at timestep 124 is tensor(0.0257, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[ 0.04871158 -0.19699639]]. Action = [[ 0.21158987  0.21086997 -0.1313114  -0.9268948 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [False, True, False, True, False, False]
Scene graph at timestep 125 is [False, True, False, True, False, False]
State prediction error at timestep 125 is tensor(0.0295, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 125 of 0
Current timestep = 126. State = [[-0.2503307   0.01972084]]. Action = [[-0.23723973 -0.0384967   0.09182686  0.26886857]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [False, True, False, True, False, False]
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(0.0128, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.23908554  0.02896969]]. Action = [[ 0.1910783   0.12136412  0.22412151 -0.74415445]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 127 is [True, False, False, False, True, False]
Current timestep = 128. State = [[-0.21964106  0.04343784]]. Action = [[0.10228011 0.09543937 0.12542218 0.30484605]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 128 is [True, False, False, False, True, False]
Current timestep = 129. State = [[-0.21037634  0.06277378]]. Action = [[-0.00545016  0.16767132  0.01781729  0.6662357 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 129 is [True, False, False, False, True, False]
Current timestep = 130. State = [[-0.19807675  0.06640398]]. Action = [[ 0.20922726 -0.16982353 -0.14021865 -0.03761023]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 130 is [True, False, False, False, True, False]
Scene graph at timestep 130 is [True, False, False, False, True, False]
State prediction error at timestep 130 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 130 of 1
Current timestep = 131. State = [[-0.1813436   0.05017769]]. Action = [[-0.14084564 -0.13975963 -0.02623613 -0.79514986]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 131 is [True, False, False, False, True, False]
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 0
Current timestep = 132. State = [[-0.18329093  0.03924682]]. Action = [[ 0.00434741  0.01239076  0.23468971 -0.8380752 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 132 is [True, False, False, False, True, False]
Current timestep = 133. State = [[-0.18087323  0.0296018 ]]. Action = [[ 0.0974918  -0.15831009  0.20069486 -0.9351997 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 133 is [True, False, False, False, True, False]
Current timestep = 134. State = [[-0.17334776  0.00884093]]. Action = [[ 0.15259084 -0.19729432  0.05602089 -0.66794074]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 134 is [True, False, False, False, True, False]
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.16187786 -0.00901866]]. Action = [[ 0.00721169  0.02526301  0.00928417 -0.5753417 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 135 is [True, False, False, False, True, False]
Scene graph at timestep 135 is [True, False, False, False, True, False]
State prediction error at timestep 135 is tensor(9.5433e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 135 of 1
Current timestep = 136. State = [[-1.5527984e-01 -1.0763066e-04]]. Action = [[0.14630848 0.1553908  0.16657451 0.21177518]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 136 is [True, False, False, False, True, False]
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.13343102  0.00864564]]. Action = [[ 0.06221789 -0.06002741 -0.06161749 -0.5387406 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 137 is [True, False, False, False, True, False]
Current timestep = 138. State = [[-0.1297546   0.00013793]]. Action = [[ 0.03354311 -0.1082904  -0.1123144   0.78540087]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 138 is [True, False, False, False, True, False]
Current timestep = 139. State = [[-0.13088013  0.00382568]]. Action = [[-0.12583491  0.1884509  -0.18887143 -0.7445496 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 139 is [True, False, False, False, True, False]
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 139 of 0
Current timestep = 140. State = [[-0.13761887  0.02739157]]. Action = [[-0.09383881  0.20654929 -0.11863106 -0.5826354 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 140 is [True, False, False, False, True, False]
Scene graph at timestep 140 is [True, False, False, False, True, False]
State prediction error at timestep 140 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 140 of -1
Current timestep = 141. State = [[-0.14173801  0.04226374]]. Action = [[ 0.13320458 -0.05612025  0.12971053 -0.50513345]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 141 is [True, False, False, False, True, False]
Current timestep = 142. State = [[-0.13957132  0.03405417]]. Action = [[-0.0213713  -0.10724013 -0.13438672 -0.45866656]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 142 is [True, False, False, False, True, False]
Scene graph at timestep 142 is [True, False, False, False, True, False]
State prediction error at timestep 142 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 142 of 1
Current timestep = 143. State = [[-0.14309816  0.01829391]]. Action = [[-0.22731015 -0.12527448  0.03498104  0.27395856]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 143 is [True, False, False, False, True, False]
Current timestep = 144. State = [[-0.14750643  0.00030663]]. Action = [[ 0.00459653 -0.1355729  -0.01212943  0.38641787]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 144 is [True, False, False, False, True, False]
Scene graph at timestep 144 is [True, False, False, False, True, False]
State prediction error at timestep 144 is tensor(6.9544e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 144 of 0
Current timestep = 145. State = [[-0.14632517 -0.00351809]]. Action = [[0.18656886 0.18286508 0.20491302 0.56677127]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 145 is [True, False, False, False, True, False]
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of 1
Current timestep = 146. State = [[-0.14597902 -0.00263578]]. Action = [[-0.16355461 -0.22023469  0.18278044  0.5916443 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 146 is [True, False, False, False, True, False]
Current timestep = 147. State = [[-0.14870375 -0.02103017]]. Action = [[-0.03179422 -0.09696141 -0.16556196  0.17464364]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 147 is [True, False, False, False, True, False]
Current timestep = 148. State = [[-0.15677516 -0.039238  ]]. Action = [[-0.12016425 -0.14457697  0.11588389  0.7915778 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 148 is [True, False, False, False, True, False]
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.16620584 -0.06217273]]. Action = [[ 0.01697469 -0.14532231 -0.07675952  0.6414666 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 149 is [True, False, False, False, True, False]
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 149 of -1
Current timestep = 150. State = [[-0.1672874 -0.0621039]]. Action = [[0.11631766 0.22646749 0.18948573 0.49490523]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 150 is [True, False, False, False, True, False]
Current timestep = 151. State = [[-0.16950646 -0.03566569]]. Action = [[-0.21394269  0.23317885  0.15686205  0.77217793]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 151 is [True, False, False, False, True, False]
Current timestep = 152. State = [[-0.18110035 -0.00351079]]. Action = [[-0.10863854  0.1843825   0.00203875 -0.7883349 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 152 is [True, False, False, False, True, False]
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 152 of 0
Current timestep = 153. State = [[-0.18892832  0.02437288]]. Action = [[0.15141991 0.13315016 0.01414293 0.8630916 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 153 is [True, False, False, False, True, False]
Current timestep = 154. State = [[-0.19010574  0.02393849]]. Action = [[-0.14891279 -0.14956874 -0.20470533  0.78251743]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 154 is [True, False, False, False, True, False]
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of 0
Current timestep = 155. State = [[-0.18994187  0.0070491 ]]. Action = [[ 0.08213982 -0.15080398  0.05257285  0.7970488 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 155 is [True, False, False, False, True, False]
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of 1
Current timestep = 156. State = [[-0.17995568  0.0065892 ]]. Action = [[ 0.24580282  0.20141506  0.23200673 -0.9964391 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 156 is [True, False, False, False, True, False]
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of 1
Current timestep = 157. State = [[-0.17083766  0.01593306]]. Action = [[-0.14757395 -0.07124187  0.10275799  0.6371424 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 157 is [True, False, False, False, True, False]
Current timestep = 158. State = [[-0.1699371   0.02081132]]. Action = [[ 0.14206794  0.14306808  0.05705523 -0.84195167]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 158 is [True, False, False, False, True, False]
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of 1
Current timestep = 159. State = [[-0.15857941  0.03383523]]. Action = [[ 0.23011047  0.05186102  0.00382048 -0.42339444]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 159 is [True, False, False, False, True, False]
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 159 of 1
Current timestep = 160. State = [[-0.13593066  0.0409746 ]]. Action = [[ 0.01490581  0.05161366 -0.17256705 -0.7644722 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 160 is [True, False, False, False, True, False]
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of 1
Current timestep = 161. State = [[-0.13493471  0.05695408]]. Action = [[ 0.0086641   0.20186013  0.13042581 -0.7471486 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 161 is [True, False, False, False, True, False]
Current timestep = 162. State = [[-0.13902351  0.06620011]]. Action = [[-0.18863024 -0.0787926  -0.24318519  0.6490201 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 162 is [True, False, False, False, True, False]
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.1412004   0.07404776]]. Action = [[ 0.14071304  0.14570811 -0.16885571  0.12280703]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 163 is [True, False, False, False, True, False]
Current timestep = 164. State = [[-0.14429303  0.08599334]]. Action = [[-0.0495813   0.08374864  0.05954686 -0.6431855 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 164 is [True, False, False, False, True, False]
Current timestep = 165. State = [[-0.14358808  0.10535596]]. Action = [[ 0.11377794  0.21520603 -0.20970789 -0.46906817]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 165 is [True, False, False, False, True, False]
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.1355443   0.12218345]]. Action = [[-0.16152793 -0.097848   -0.19333129 -0.38895333]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 166 is [True, False, False, False, True, False]
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of -1
Current timestep = 167. State = [[-0.13622077  0.1182616 ]]. Action = [[ 0.17001086  0.03178844  0.05515265 -0.02078545]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 167 is [True, False, False, False, True, False]
Current timestep = 168. State = [[-0.13559638  0.11856171]]. Action = [[-0.07701266 -0.01276517 -0.00369456  0.6543577 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 168 is [True, False, False, False, True, False]
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of 0
Current timestep = 169. State = [[-0.13535234  0.12957661]]. Action = [[ 0.07883996  0.21676493 -0.15137288 -0.8221475 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 169 is [True, False, False, False, True, False]
Scene graph at timestep 169 is [True, False, False, False, False, True]
State prediction error at timestep 169 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of 0
Current timestep = 170. State = [[-0.11953492  0.13896763]]. Action = [[ 0.23100081 -0.11636791 -0.04068786  0.91421735]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 170 is [True, False, False, False, False, True]
Current timestep = 171. State = [[-0.10310841  0.14346385]]. Action = [[-0.00978795  0.146718    0.0685485   0.9013375 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 171 is [True, False, False, False, False, True]
Current timestep = 172. State = [[-0.09609524  0.15734425]]. Action = [[ 0.07367432  0.11833426  0.13323516 -0.9454304 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 172 is [True, False, False, False, False, True]
Scene graph at timestep 172 is [True, False, False, False, False, True]
State prediction error at timestep 172 is tensor(0.0082, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 172 of 1
Current timestep = 173. State = [[-0.08117092  0.15642719]]. Action = [[ 0.14958248 -0.20719086 -0.11008029  0.6876476 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 173 is [True, False, False, False, False, True]
Current timestep = 174. State = [[-0.06681621  0.13492146]]. Action = [[ 0.08709118 -0.15332985  0.1649211  -0.9543224 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 174 is [True, False, False, False, False, True]
Current timestep = 175. State = [[-0.06168358  0.11768511]]. Action = [[-0.21843775 -0.11433515 -0.11617503 -0.9077884 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 175 is [True, False, False, False, False, True]
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0102, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.06330357  0.11367764]]. Action = [[ 0.1569891  0.1413536  0.244932  -0.6623086]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 176 is [True, False, False, False, True, False]
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of 0
Current timestep = 177. State = [[-0.05541854  0.11567337]]. Action = [[ 0.19818223 -0.05702189  0.19153559 -0.12748396]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 177 is [True, False, False, False, True, False]
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 177 of 1
Current timestep = 178. State = [[-0.2094629  -0.08466088]]. Action = [[-0.05000114 -0.17379117  0.01253393 -0.6305331 ]]. Reward = [100.]
Curr episode timestep = 51
Scene graph at timestep 178 is [True, False, False, False, True, False]
Scene graph at timestep 178 is [True, False, False, False, True, False]
State prediction error at timestep 178 is tensor(0.0099, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 178 of 1
Current timestep = 179. State = [[-0.20113538 -0.09735428]]. Action = [[-0.09441994 -0.03764948 -0.21589509  0.6275594 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 179 is [True, False, False, False, True, False]
Scene graph at timestep 179 is [True, False, False, False, True, False]
State prediction error at timestep 179 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 179 of 0
Current timestep = 180. State = [[-0.20978396 -0.09581511]]. Action = [[-0.1907318   0.12272036  0.01157293  0.07936835]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 180 is [True, False, False, False, True, False]
Current timestep = 181. State = [[-0.21909297 -0.10045501]]. Action = [[ 0.01135474 -0.17189269  0.03913391  0.14065194]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 181 is [True, False, False, False, True, False]
Current timestep = 182. State = [[-0.2228639 -0.100992 ]]. Action = [[-0.0110421   0.14710599 -0.08356041  0.13720667]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 182 is [True, False, False, False, True, False]
Current timestep = 183. State = [[-0.23114419 -0.09994737]]. Action = [[-0.14528117 -0.09110445 -0.0047386  -0.64534646]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 183 is [True, False, False, False, True, False]
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.23996934 -0.10574861]]. Action = [[-0.21847959 -0.01863061  0.20347589  0.77566886]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 184 is [True, False, False, False, True, False]
Scene graph at timestep 184 is [True, False, False, False, True, False]
State prediction error at timestep 184 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 184 of -1
Current timestep = 185. State = [[-0.23816451 -0.09281589]]. Action = [[ 0.12129188  0.2290701  -0.09395388  0.842543  ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 185 is [True, False, False, False, True, False]
Scene graph at timestep 185 is [True, False, False, False, True, False]
State prediction error at timestep 185 is tensor(0.0086, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 185 of 0
Current timestep = 186. State = [[-0.2348225  -0.08138704]]. Action = [[ 0.06539899 -0.14666773  0.04790145  0.42668462]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 186 is [True, False, False, False, True, False]
Current timestep = 187. State = [[-0.22366509 -0.09543041]]. Action = [[ 0.16549075 -0.14883937  0.15833673  0.99098134]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 187 is [True, False, False, False, True, False]
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 187 of 1
Current timestep = 188. State = [[-0.20052321 -0.1191533 ]]. Action = [[ 0.17138141 -0.15163115 -0.10615474 -0.31953955]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 188 is [True, False, False, False, True, False]
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of 1
Current timestep = 189. State = [[-0.17803766 -0.12093042]]. Action = [[ 0.17754158  0.19498876 -0.22954597  0.14665544]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 189 is [True, False, False, False, True, False]
Scene graph at timestep 189 is [True, False, False, False, True, False]
State prediction error at timestep 189 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 189 of 1
Current timestep = 190. State = [[-0.15612668 -0.1159846 ]]. Action = [[ 0.05294144 -0.13838908  0.0365462   0.8990929 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 190 is [True, False, False, False, True, False]
Scene graph at timestep 190 is [True, False, False, False, True, False]
State prediction error at timestep 190 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of 0
Current timestep = 191. State = [[-0.14189127 -0.12769346]]. Action = [[ 0.24555838 -0.08223026  0.05979028  0.60184216]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 191 is [True, False, False, False, True, False]
Scene graph at timestep 191 is [True, False, False, True, False, False]
State prediction error at timestep 191 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of 1
Current timestep = 192. State = [[-0.10884351 -0.13378257]]. Action = [[ 0.14149657  0.04651403  0.01496923 -0.95410925]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 192 is [True, False, False, True, False, False]
Scene graph at timestep 192 is [True, False, False, True, False, False]
State prediction error at timestep 192 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 192 of 1
Current timestep = 193. State = [[-0.09007138 -0.13643059]]. Action = [[ 0.10968167 -0.07120553  0.05457598  0.66822386]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 193 is [True, False, False, True, False, False]
Current timestep = 194. State = [[-0.07249028 -0.12985136]]. Action = [[ 0.1964637   0.16781455  0.19832486 -0.74625754]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 194 is [True, False, False, True, False, False]
Scene graph at timestep 194 is [True, False, False, True, False, False]
State prediction error at timestep 194 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 194 of 1
Current timestep = 195. State = [[-0.05309742 -0.12733246]]. Action = [[-0.05821879 -0.11923042 -0.19236013 -0.927137  ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 195 is [True, False, False, True, False, False]
Current timestep = 196. State = [[-0.05668214 -0.14129046]]. Action = [[-0.1062156  -0.11548898  0.19016123  0.676929  ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 196 is [True, False, False, True, False, False]
Current timestep = 197. State = [[-0.05410654 -0.15961245]]. Action = [[ 0.23232365 -0.16238277 -0.14199163 -0.09904319]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 197 is [True, False, False, True, False, False]
Scene graph at timestep 197 is [True, False, False, True, False, False]
State prediction error at timestep 197 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of -1
Current timestep = 198. State = [[-0.03994269 -0.16137771]]. Action = [[ 0.01599953  0.21457356 -0.08678314 -0.69729894]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 198 is [True, False, False, True, False, False]
Current timestep = 199. State = [[-0.04376392 -0.1481964 ]]. Action = [[-0.23824246  0.05903864 -0.21743731 -0.38877952]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 199 is [False, True, False, True, False, False]
Current timestep = 200. State = [[-0.04643663 -0.15196347]]. Action = [[ 0.1502974  -0.1851554   0.08110031  0.38447702]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 200 is [False, True, False, True, False, False]
Current timestep = 201. State = [[-0.04527818 -0.17128314]]. Action = [[ 0.0566881  -0.20372425  0.19722623 -0.53085285]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 201 is [False, True, False, True, False, False]
Current timestep = 202. State = [[-0.04668117 -0.19876523]]. Action = [[-0.05282916 -0.19630049 -0.18236965  0.3013488 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 202 is [False, True, False, True, False, False]
Current timestep = 203. State = [[-0.04650039 -0.20534872]]. Action = [[0.00889996 0.19261241 0.05042285 0.26566732]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 203 is [False, True, False, True, False, False]
Scene graph at timestep 203 is [False, True, False, True, False, False]
State prediction error at timestep 203 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of -1
Current timestep = 204. State = [[-0.03652196 -0.18366642]]. Action = [[ 0.20950365  0.20878318 -0.19911475  0.85740423]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 204 is [False, True, False, True, False, False]
Scene graph at timestep 204 is [False, True, False, True, False, False]
State prediction error at timestep 204 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 204 of 1
Current timestep = 205. State = [[-0.01995658 -0.16831034]]. Action = [[ 0.00110191 -0.04238366  0.01394904  0.04184186]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 205 is [False, True, False, True, False, False]
Scene graph at timestep 205 is [False, True, False, True, False, False]
State prediction error at timestep 205 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of 0
Current timestep = 206. State = [[-0.02004431 -0.16936219]]. Action = [[ 0.01111805 -0.01388663  0.00218585 -0.8063352 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 206 is [False, True, False, True, False, False]
Scene graph at timestep 206 is [False, True, False, True, False, False]
State prediction error at timestep 206 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of 0
Current timestep = 207. State = [[-0.0136625 -0.166464 ]]. Action = [[ 0.15730882  0.06337589 -0.21527056  0.7198888 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 207 is [False, True, False, True, False, False]
Scene graph at timestep 207 is [False, True, False, True, False, False]
State prediction error at timestep 207 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 207 of 1
Current timestep = 208. State = [[ 0.01036994 -0.15575223]]. Action = [[ 0.09083414  0.14788705 -0.11471917 -0.9533157 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 208 is [False, True, False, True, False, False]
Scene graph at timestep 208 is [False, True, False, True, False, False]
State prediction error at timestep 208 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 208 of 1
Current timestep = 209. State = [[ 0.01559296 -0.14813384]]. Action = [[-0.18563116 -0.0526281  -0.19070283 -0.9740808 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 209 is [False, True, False, True, False, False]
Current timestep = 210. State = [[ 0.01117892 -0.15864001]]. Action = [[-0.07323343 -0.09315145  0.00295669 -0.4183209 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 210 is [False, True, False, True, False, False]
Scene graph at timestep 210 is [False, True, False, True, False, False]
State prediction error at timestep 210 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[ 0.00535598 -0.17362805]]. Action = [[ 0.1271292  -0.13097848 -0.06239429  0.4614135 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 211 is [False, True, False, True, False, False]
Current timestep = 212. State = [[ 0.00995896 -0.19167215]]. Action = [[ 0.16215539 -0.23296362 -0.16966006 -0.45358747]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 212 is [False, True, False, True, False, False]
Scene graph at timestep 212 is [False, True, False, True, False, False]
State prediction error at timestep 212 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[ 0.03315976 -0.21539387]]. Action = [[ 0.20606616 -0.06339911  0.1809972  -0.08384693]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 213 is [False, True, False, True, False, False]
Scene graph at timestep 213 is [False, True, False, True, False, False]
State prediction error at timestep 213 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 213 of -1
Current timestep = 214. State = [[ 0.06205476 -0.22200045]]. Action = [[ 0.22064292  0.01094386 -0.02014165 -0.43621242]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 214 is [False, True, False, True, False, False]
Current timestep = 215. State = [[ 0.06182395 -0.21235287]]. Action = [[-0.12523572  0.22099423  0.19729662 -0.83857656]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 215 is [False, False, True, True, False, False]
Current timestep = 216. State = [[ 0.06169665 -0.1940059 ]]. Action = [[-0.07509491  0.12906772  0.11642554  0.00376534]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 216 is [False, False, True, True, False, False]
Current timestep = 217. State = [[ 0.05354115 -0.19761463]]. Action = [[-0.15825376 -0.20452848 -0.10748523  0.02753043]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 217 is [False, False, True, True, False, False]
Current timestep = 218. State = [[ 0.04445399 -0.19885498]]. Action = [[-0.02726263  0.17173064  0.12963468 -0.27820343]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 218 is [False, False, True, True, False, False]
Scene graph at timestep 218 is [False, True, False, True, False, False]
State prediction error at timestep 218 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of 0
Current timestep = 219. State = [[ 0.03700863 -0.18573795]]. Action = [[-0.00373028  0.01825744 -0.1018952  -0.8532309 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 219 is [False, True, False, True, False, False]
Scene graph at timestep 219 is [False, True, False, True, False, False]
State prediction error at timestep 219 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of 0
Current timestep = 220. State = [[ 0.03728577 -0.19666263]]. Action = [[ 0.10006365 -0.2318463   0.0950475   0.11952281]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 220 is [False, True, False, True, False, False]
Current timestep = 221. State = [[ 0.03167073 -0.21731442]]. Action = [[-0.15965335 -0.1001375   0.09183693 -0.77788424]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 221 is [False, True, False, True, False, False]
Current timestep = 222. State = [[ 0.02259485 -0.24189177]]. Action = [[-0.05723692 -0.23498046 -0.08976521  0.7858428 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 222 is [False, True, False, True, False, False]
Current timestep = 223. State = [[ 0.01265219 -0.24834529]]. Action = [[-0.10362303  0.20767224 -0.00327738 -0.5952361 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 223 is [False, True, False, True, False, False]
Current timestep = 224. State = [[ 0.0086572  -0.23386867]]. Action = [[ 0.01126322  0.09831697 -0.14704561 -0.9002657 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 224 is [False, True, False, True, False, False]
Scene graph at timestep 224 is [False, True, False, True, False, False]
State prediction error at timestep 224 is tensor(1.0116e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of -1
Current timestep = 225. State = [[ 0.00857642 -0.2196008 ]]. Action = [[-0.04010147  0.09061241  0.21737984  0.03447831]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 225 is [False, True, False, True, False, False]
Current timestep = 226. State = [[ 0.00149439 -0.22233129]]. Action = [[-0.0854153  -0.14145163 -0.02700213 -0.04779959]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 226 is [False, True, False, True, False, False]
Current timestep = 227. State = [[-0.00433307 -0.24140868]]. Action = [[ 0.13977367 -0.21995977  0.18285808 -0.33216608]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 227 is [False, True, False, True, False, False]
Current timestep = 228. State = [[-0.00463361 -0.26078263]]. Action = [[ 0.04069054 -0.09154382 -0.07423942 -0.8965489 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 228 is [False, True, False, True, False, False]
Scene graph at timestep 228 is [False, True, False, True, False, False]
State prediction error at timestep 228 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 228 of -1
Current timestep = 229. State = [[-0.00846087 -0.26917684]]. Action = [[-0.17025584  0.05562776 -0.23494054  0.14374506]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 229 is [False, True, False, True, False, False]
Current timestep = 230. State = [[-0.00868538 -0.2788002 ]]. Action = [[ 0.20175561 -0.20569871 -0.07816395 -0.690193  ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 230 is [False, True, False, True, False, False]
Scene graph at timestep 230 is [False, True, False, True, False, False]
State prediction error at timestep 230 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 230 of -1
Current timestep = 231. State = [[-0.00834877 -0.2886534 ]]. Action = [[ 0.05786502 -0.00652681 -0.00028947 -0.14945453]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 231 is [False, True, False, True, False, False]
Scene graph at timestep 231 is [False, True, False, True, False, False]
State prediction error at timestep 231 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of -1
Current timestep = 232. State = [[-0.00935002 -0.28571385]]. Action = [[-0.21365985  0.1264016   0.12935156 -0.22914302]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 232 is [False, True, False, True, False, False]
Current timestep = 233. State = [[-0.00889082 -0.2724891 ]]. Action = [[-0.02152708  0.20004553  0.22004968  0.5720514 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 233 is [False, True, False, True, False, False]
Current timestep = 234. State = [[-0.00841201 -0.2645047 ]]. Action = [[ 0.09145775 -0.14589703  0.10348484 -0.27068424]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 234 is [False, True, False, True, False, False]
Scene graph at timestep 234 is [False, True, False, True, False, False]
State prediction error at timestep 234 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 234 of 0
Current timestep = 235. State = [[-0.00862359 -0.2763157 ]]. Action = [[ 0.08336246 -0.16699769 -0.09430134 -0.8092076 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 235 is [False, True, False, True, False, False]
Scene graph at timestep 235 is [False, True, False, True, False, False]
State prediction error at timestep 235 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 235 of -1
Current timestep = 236. State = [[-0.0097672  -0.28843707]]. Action = [[-0.2204279  -0.21959104 -0.00145562 -0.27243245]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 236 is [False, True, False, True, False, False]
Current timestep = 237. State = [[-0.00983507 -0.2831121 ]]. Action = [[-0.15072319  0.15322787  0.05546588 -0.38162506]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 237 is [False, True, False, True, False, False]
Scene graph at timestep 237 is [False, True, False, True, False, False]
State prediction error at timestep 237 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 237 of 1
Current timestep = 238. State = [[-0.00369211 -0.2652667 ]]. Action = [[ 0.19175848  0.22032782 -0.19320562 -0.37371957]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 238 is [False, True, False, True, False, False]
Current timestep = 239. State = [[ 0.00178472 -0.24084632]]. Action = [[ 0.0994823   0.04283041 -0.01716661 -0.46885735]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 239 is [False, True, False, True, False, False]
Current timestep = 240. State = [[ 0.01090034 -0.22633274]]. Action = [[ 0.11625278  0.12305719 -0.14318949  0.9741111 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 240 is [False, True, False, True, False, False]
Scene graph at timestep 240 is [False, True, False, True, False, False]
State prediction error at timestep 240 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 240 of 1
Current timestep = 241. State = [[ 0.02621723 -0.21489319]]. Action = [[ 0.0705415  -0.01323742 -0.02141285  0.20130062]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 241 is [False, True, False, True, False, False]
Current timestep = 242. State = [[ 0.02808647 -0.21453336]]. Action = [[ 0.24133062  0.11606741 -0.13918735 -0.11891347]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 242 is [False, True, False, True, False, False]
Current timestep = 243. State = [[ 0.03141669 -0.20750691]]. Action = [[ 0.02152917  0.117865   -0.08367273 -0.54665524]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 243 is [False, True, False, True, False, False]
Current timestep = 244. State = [[ 0.0399879  -0.19455533]]. Action = [[ 0.17303899  0.07257798  0.13916978 -0.6607215 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 244 is [False, True, False, True, False, False]
Scene graph at timestep 244 is [False, True, False, True, False, False]
State prediction error at timestep 244 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 244 of 0
Current timestep = 245. State = [[ 0.06035371 -0.17423643]]. Action = [[-0.1402762   0.22722375 -0.19144207 -0.93202823]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 245 is [False, True, False, True, False, False]
Current timestep = 246. State = [[ 0.05620885 -0.16162302]]. Action = [[-0.16191824 -0.01473351 -0.0320271  -0.3066259 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 246 is [False, False, True, True, False, False]
Scene graph at timestep 246 is [False, False, True, True, False, False]
State prediction error at timestep 246 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 246 of 1
Current timestep = 247. State = [[ 0.04398069 -0.15126538]]. Action = [[-0.09437263  0.1326552  -0.04359093  0.28035378]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 247 is [False, False, True, True, False, False]
Scene graph at timestep 247 is [False, True, False, True, False, False]
State prediction error at timestep 247 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 247 of 1
Current timestep = 248. State = [[ 0.03495529 -0.1506534 ]]. Action = [[ 0.06646726 -0.21928424  0.2089811  -0.97189707]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 248 is [False, True, False, True, False, False]
Current timestep = 249. State = [[ 0.03302664 -0.15234324]]. Action = [[-0.12657908  0.21202251 -0.13076365 -0.44128394]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 249 is [False, True, False, True, False, False]
Scene graph at timestep 249 is [False, True, False, True, False, False]
State prediction error at timestep 249 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 249 of 0
Current timestep = 250. State = [[ 0.02492032 -0.14911912]]. Action = [[ 0.14569598 -0.18822882 -0.06535763 -0.44456887]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 250 is [False, True, False, True, False, False]
Scene graph at timestep 250 is [False, True, False, True, False, False]
State prediction error at timestep 250 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 250 of -1
Current timestep = 251. State = [[ 0.01853346 -0.1704174 ]]. Action = [[-0.23552176 -0.11744034  0.217778    0.9031284 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 251 is [False, True, False, True, False, False]
Current timestep = 252. State = [[ 0.00190483 -0.1909961 ]]. Action = [[-0.16365387 -0.150834    0.15575755 -0.50856566]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 252 is [False, True, False, True, False, False]
Scene graph at timestep 252 is [False, True, False, True, False, False]
State prediction error at timestep 252 is tensor(4.3418e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.02312466 -0.22113374]]. Action = [[-0.17373943 -0.24165711  0.11850101 -0.23892224]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 253 is [False, True, False, True, False, False]
Scene graph at timestep 253 is [False, True, False, True, False, False]
State prediction error at timestep 253 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 253 of -1
Current timestep = 254. State = [[-0.05198525 -0.24945737]]. Action = [[-0.19588889 -0.07398808  0.21610367  0.6993675 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 254 is [False, True, False, True, False, False]
Current timestep = 255. State = [[-0.05962567 -0.24783397]]. Action = [[ 0.24067992  0.11052772 -0.13468085  0.640756  ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 255 is [True, False, False, True, False, False]
Current timestep = 256. State = [[-0.06435706 -0.2522647 ]]. Action = [[-0.21768874 -0.20211993  0.0796105  -0.53665274]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 256 is [True, False, False, True, False, False]
Scene graph at timestep 256 is [True, False, False, True, False, False]
State prediction error at timestep 256 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 256 of 0
Current timestep = 257. State = [[-0.0778868  -0.25664607]]. Action = [[-0.11787975  0.1990292   0.19146067 -0.6707162 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 257 is [True, False, False, True, False, False]
Scene graph at timestep 257 is [True, False, False, True, False, False]
State prediction error at timestep 257 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of 0
Current timestep = 258. State = [[-0.08001719 -0.24940908]]. Action = [[ 0.20020449 -0.1512321  -0.07020545 -0.7611547 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 258 is [True, False, False, True, False, False]
Scene graph at timestep 258 is [True, False, False, True, False, False]
State prediction error at timestep 258 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 258 of 0
Current timestep = 259. State = [[-0.07489035 -0.2527285 ]]. Action = [[ 0.08409432  0.02969036 -0.16200265  0.4431497 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 259 is [True, False, False, True, False, False]
Current timestep = 260. State = [[-0.06772093 -0.2396141 ]]. Action = [[0.04649466 0.21973109 0.07545465 0.32825303]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 260 is [True, False, False, True, False, False]
Current timestep = 261. State = [[-0.06123561 -0.22220871]]. Action = [[0.05001599 0.06147489 0.09513882 0.09890056]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 261 is [True, False, False, True, False, False]
Current timestep = 262. State = [[-0.04898124 -0.22359572]]. Action = [[ 0.20942283 -0.17616329  0.06088865  0.7111621 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 262 is [True, False, False, True, False, False]
Scene graph at timestep 262 is [False, True, False, True, False, False]
State prediction error at timestep 262 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 262 of 1
Current timestep = 263. State = [[-0.03112074 -0.23134919]]. Action = [[-0.17950734  0.08446157 -0.15504429  0.02192998]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 263 is [False, True, False, True, False, False]
Scene graph at timestep 263 is [False, True, False, True, False, False]
State prediction error at timestep 263 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 263 of 0
Current timestep = 264. State = [[-0.04470389 -0.23642798]]. Action = [[-0.24180686 -0.08556119  0.1759159  -0.83336794]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 264 is [False, True, False, True, False, False]
Current timestep = 265. State = [[-0.062879   -0.24717224]]. Action = [[ 0.01862976 -0.1146926   0.16554469  0.95467913]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 265 is [False, True, False, True, False, False]
Current timestep = 266. State = [[-0.0713661  -0.24522601]]. Action = [[-0.14021897  0.1638233   0.12412524  0.7545965 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 266 is [True, False, False, True, False, False]
Current timestep = 267. State = [[-0.07183049 -0.22538123]]. Action = [[ 0.18059322  0.2085776  -0.2002909  -0.83958596]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 267 is [True, False, False, True, False, False]
Current timestep = 268. State = [[-0.06277612 -0.19973335]]. Action = [[0.17020586 0.11022672 0.09490323 0.21045327]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 268 is [True, False, False, True, False, False]
Current timestep = 269. State = [[-0.05874911 -0.19038723]]. Action = [[-0.14851002 -0.01614866 -0.1677568  -0.5244451 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 269 is [True, False, False, True, False, False]
Scene graph at timestep 269 is [True, False, False, True, False, False]
State prediction error at timestep 269 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of 1
Current timestep = 270. State = [[-0.05463798 -0.19542362]]. Action = [[ 0.21357682 -0.13519844  0.16249144 -0.2376371 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 270 is [True, False, False, True, False, False]
Current timestep = 271. State = [[-0.04971892 -0.19982666]]. Action = [[-0.0268966   0.01155254  0.10640427 -0.00829887]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 271 is [True, False, False, True, False, False]
Scene graph at timestep 271 is [False, True, False, True, False, False]
State prediction error at timestep 271 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 271 of 0
Current timestep = 272. State = [[-0.04982546 -0.20828901]]. Action = [[-0.02443878 -0.12798999 -0.020871   -0.9676587 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 272 is [False, True, False, True, False, False]
Current timestep = 273. State = [[-0.05034187 -0.22270602]]. Action = [[ 0.03240645 -0.09689486  0.18073738  0.85548675]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 273 is [False, True, False, True, False, False]
Current timestep = 274. State = [[-0.0501648  -0.24311683]]. Action = [[-0.00891197 -0.21010204  0.2332766  -0.5272303 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 274 is [True, False, False, True, False, False]
Scene graph at timestep 274 is [True, False, False, True, False, False]
State prediction error at timestep 274 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 274 of -1
Current timestep = 275. State = [[-0.04751977 -0.25619105]]. Action = [[ 0.04416791  0.14153835  0.14658365 -0.65807515]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 275 is [True, False, False, True, False, False]
Scene graph at timestep 275 is [False, True, False, True, False, False]
State prediction error at timestep 275 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 275 of 1
Current timestep = 276. State = [[-0.04580267 -0.23952703]]. Action = [[-0.18297075  0.19893086  0.06133345 -0.73405445]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 276 is [False, True, False, True, False, False]
Scene graph at timestep 276 is [False, True, False, True, False, False]
State prediction error at timestep 276 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of 1
Current timestep = 277. State = [[-0.04498337 -0.21613337]]. Action = [[ 0.16594815  0.10337141  0.12533602 -0.80791086]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 277 is [False, True, False, True, False, False]
Current timestep = 278. State = [[-0.03160567 -0.19747105]]. Action = [[ 0.2263116   0.14353642 -0.21824639  0.39439917]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 278 is [False, True, False, True, False, False]
Scene graph at timestep 278 is [False, True, False, True, False, False]
State prediction error at timestep 278 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 278 of 1
Current timestep = 279. State = [[-0.01777471 -0.19082445]]. Action = [[-0.05074699 -0.10165885 -0.13706017 -0.97120684]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 279 is [False, True, False, True, False, False]
Current timestep = 280. State = [[-0.01264538 -0.19487883]]. Action = [[ 0.15588969 -0.04881251 -0.24693419 -0.31099308]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 280 is [False, True, False, True, False, False]
Current timestep = 281. State = [[-0.00299918 -0.19169484]]. Action = [[-0.06300122  0.13581508 -0.20303185  0.39724064]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 281 is [False, True, False, True, False, False]
Scene graph at timestep 281 is [False, True, False, True, False, False]
State prediction error at timestep 281 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 281 of 1
Current timestep = 282. State = [[ 0.00234777 -0.19404486]]. Action = [[ 0.17207766 -0.1633985   0.0062755   0.10011411]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 282 is [False, True, False, True, False, False]
Current timestep = 283. State = [[ 0.01711816 -0.19884112]]. Action = [[ 0.07208446  0.02801663 -0.09711769 -0.2923609 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 283 is [False, True, False, True, False, False]
Scene graph at timestep 283 is [False, True, False, True, False, False]
State prediction error at timestep 283 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 283 of 0
Current timestep = 284. State = [[ 0.01947885 -0.21210347]]. Action = [[-0.16014518 -0.23144121  0.12759733  0.70790005]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 284 is [False, True, False, True, False, False]
Scene graph at timestep 284 is [False, True, False, True, False, False]
State prediction error at timestep 284 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 284 of -1
Current timestep = 285. State = [[ 0.01334008 -0.22419941]]. Action = [[-0.12875871  0.199103   -0.13580857  0.7708423 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 285 is [False, True, False, True, False, False]
Current timestep = 286. State = [[ 0.0152111  -0.20773113]]. Action = [[ 0.16998982  0.09944415 -0.08496654 -0.0045647 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 286 is [False, True, False, True, False, False]
Current timestep = 287. State = [[ 0.0192447  -0.18476106]]. Action = [[ 0.01795879  0.22675622 -0.23117557  0.8840151 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 287 is [False, True, False, True, False, False]
Scene graph at timestep 287 is [False, True, False, True, False, False]
State prediction error at timestep 287 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 287 of 1
Current timestep = 288. State = [[ 0.02055071 -0.15669571]]. Action = [[ 0.01950052  0.10623792  0.23874307 -0.8069634 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 288 is [False, True, False, True, False, False]
Scene graph at timestep 288 is [False, True, False, True, False, False]
State prediction error at timestep 288 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 288 of 1
Current timestep = 289. State = [[ 0.02335585 -0.15571809]]. Action = [[ 0.10495019 -0.16795896  0.01583108  0.9319674 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 289 is [False, True, False, True, False, False]
Scene graph at timestep 289 is [False, True, False, True, False, False]
State prediction error at timestep 289 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 289 of 0
Current timestep = 290. State = [[ 0.02889892 -0.1761142 ]]. Action = [[-0.02391364 -0.18080933 -0.05781434  0.35990608]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 290 is [False, True, False, True, False, False]
Scene graph at timestep 290 is [False, True, False, True, False, False]
State prediction error at timestep 290 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 290 of -1
Current timestep = 291. State = [[ 0.03558243 -0.17795844]]. Action = [[ 0.08363879  0.24737981  0.09526318 -0.2611894 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 291 is [False, True, False, True, False, False]
Current timestep = 292. State = [[ 0.04820796 -0.14964288]]. Action = [[ 0.06281379  0.22464198 -0.1818536   0.7074976 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 292 is [False, True, False, True, False, False]
Current timestep = 293. State = [[ 0.05358278 -0.13278581]]. Action = [[ 0.22481501  0.10411891 -0.23310901  0.05972564]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 293 is [False, True, False, True, False, False]
Scene graph at timestep 293 is [False, False, True, True, False, False]
State prediction error at timestep 293 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 293 of 1
Current timestep = 294. State = [[ 0.05369272 -0.12919015]]. Action = [[ 0.10253263  0.06838095  0.02322966 -0.08935148]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 294 is [False, False, True, True, False, False]
Current timestep = 295. State = [[ 0.05054409 -0.11738165]]. Action = [[-0.22136289  0.21083274 -0.20369454 -0.47886753]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 295 is [False, False, True, True, False, False]
Current timestep = 296. State = [[ 0.03417893 -0.1040985 ]]. Action = [[-0.18528908 -0.03455541 -0.14951439  0.0301435 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 296 is [False, False, True, False, True, False]
Scene graph at timestep 296 is [False, True, False, False, True, False]
State prediction error at timestep 296 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 296 of 1
Current timestep = 297. State = [[-0.14830408 -0.2068616 ]]. Action = [[ 0.1825313   0.03209263 -0.19896613  0.3885839 ]]. Reward = [100.]
Curr episode timestep = 118
Scene graph at timestep 297 is [False, True, False, False, True, False]
Scene graph at timestep 297 is [True, False, False, True, False, False]
State prediction error at timestep 297 is tensor(0.0130, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 297 of 1
Current timestep = 298. State = [[-0.13079402 -0.22478212]]. Action = [[-0.02844632  0.14638615 -0.14455807 -0.7600727 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 298 is [True, False, False, True, False, False]
Scene graph at timestep 298 is [True, False, False, True, False, False]
State prediction error at timestep 298 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 298 of 1
Current timestep = 299. State = [[-0.12964444 -0.23065783]]. Action = [[ 0.05050617 -0.22726996 -0.18650442 -0.1587305 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 299 is [True, False, False, True, False, False]
Current timestep = 300. State = [[-0.12831677 -0.23534922]]. Action = [[-0.09768276  0.15958071  0.02071333 -0.77982235]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 300 is [True, False, False, True, False, False]
Current timestep = 301. State = [[-0.12293829 -0.24106973]]. Action = [[ 0.22920233 -0.21345672  0.22386187 -0.8722669 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 301 is [True, False, False, True, False, False]
Current timestep = 302. State = [[-0.10232402 -0.2538627 ]]. Action = [[ 0.17459366 -0.08523798 -0.22659715  0.38530576]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 302 is [True, False, False, True, False, False]
Current timestep = 303. State = [[-0.08927688 -0.25987828]]. Action = [[-0.10110462  0.04923379  0.21306941 -0.83588773]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 303 is [True, False, False, True, False, False]
Current timestep = 304. State = [[-0.09596434 -0.2552057 ]]. Action = [[-0.2102265   0.13967896 -0.02627411  0.62052345]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 304 is [True, False, False, True, False, False]
Scene graph at timestep 304 is [True, False, False, True, False, False]
State prediction error at timestep 304 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 304 of 0
Current timestep = 305. State = [[-0.10502908 -0.25638595]]. Action = [[ 0.09026924 -0.15122272 -0.17550299 -0.5253457 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 305 is [True, False, False, True, False, False]
Scene graph at timestep 305 is [True, False, False, True, False, False]
State prediction error at timestep 305 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of -1
Current timestep = 306. State = [[-0.09643839 -0.24905019]]. Action = [[ 0.24571362  0.23804551 -0.20961247  0.60162485]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 306 is [True, False, False, True, False, False]
Scene graph at timestep 306 is [True, False, False, True, False, False]
State prediction error at timestep 306 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 306 of 1
Current timestep = 307. State = [[-0.07317983 -0.23100017]]. Action = [[ 0.18222642 -0.00189799  0.03527227  0.4117211 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 307 is [True, False, False, True, False, False]
Current timestep = 308. State = [[-0.0597881  -0.22430329]]. Action = [[-0.0270988   0.11738959 -0.09183633  0.3366058 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 308 is [True, False, False, True, False, False]
Current timestep = 309. State = [[-0.05074366 -0.21531475]]. Action = [[ 0.16789654 -0.00665043  0.1970464  -0.6881392 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 309 is [True, False, False, True, False, False]
Current timestep = 310. State = [[-0.03995264 -0.20475157]]. Action = [[-0.14100045  0.20234436  0.23187667  0.8874991 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 310 is [True, False, False, True, False, False]
Current timestep = 311. State = [[-0.0416845  -0.19569762]]. Action = [[-0.04286213 -0.07498538  0.22345865 -0.29944938]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 311 is [False, True, False, True, False, False]
Scene graph at timestep 311 is [False, True, False, True, False, False]
State prediction error at timestep 311 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 311 of 1
Current timestep = 312. State = [[-0.03740003 -0.18932436]]. Action = [[ 0.22424561  0.11542726 -0.05715793 -0.857463  ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 312 is [False, True, False, True, False, False]
Scene graph at timestep 312 is [False, True, False, True, False, False]
State prediction error at timestep 312 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 312 of 1
Current timestep = 313. State = [[-0.03139365 -0.18101948]]. Action = [[-0.19168948  0.02348566 -0.24734986 -0.562838  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 313 is [False, True, False, True, False, False]
Scene graph at timestep 313 is [False, True, False, True, False, False]
State prediction error at timestep 313 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 313 of 1
Current timestep = 314. State = [[-0.04048412 -0.17096984]]. Action = [[-0.17069538  0.1710779  -0.10006531  0.726454  ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 314 is [False, True, False, True, False, False]
Current timestep = 315. State = [[-0.04696107 -0.1476423 ]]. Action = [[ 0.2209003   0.15449426 -0.12233737  0.06062615]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 315 is [False, True, False, True, False, False]
Current timestep = 316. State = [[-0.03764156 -0.12810835]]. Action = [[ 0.15950823  0.0599494  -0.24274127  0.37345958]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 316 is [False, True, False, True, False, False]
Scene graph at timestep 316 is [False, True, False, True, False, False]
State prediction error at timestep 316 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 316 of 1
Current timestep = 317. State = [[-0.02530281 -0.11730568]]. Action = [[ 0.01265901  0.05908364 -0.07313752  0.6018745 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 317 is [False, True, False, True, False, False]
Current timestep = 318. State = [[-0.02558762 -0.10875411]]. Action = [[-0.11762816  0.07857257 -0.18940194  0.43951583]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 318 is [False, True, False, False, True, False]
Scene graph at timestep 318 is [False, True, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 318 of 1
Current timestep = 319. State = [[-0.1683553   0.01080051]]. Action = [[ 3.1459332e-04  9.0232432e-02 -6.3072950e-02 -4.9935913e-01]]. Reward = [100.]
Curr episode timestep = 21
Scene graph at timestep 319 is [False, True, False, False, True, False]
Current timestep = 320. State = [[-0.15511261  0.01687314]]. Action = [[-0.12673399  0.02279687 -0.06230977  0.33303058]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 320 is [True, False, False, False, True, False]
Current timestep = 321. State = [[-0.1518707   0.03150441]]. Action = [[ 0.20222777  0.22258121 -0.1370936  -0.96799284]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 321 is [True, False, False, False, True, False]
Current timestep = 322. State = [[-0.13361809  0.04009599]]. Action = [[ 0.21932316 -0.10559818  0.10302514  0.8319951 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 322 is [True, False, False, False, True, False]
Current timestep = 323. State = [[-0.11921997  0.03330872]]. Action = [[-0.1388747  -0.09324712  0.0380168  -0.78804064]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 323 is [True, False, False, False, True, False]
Current timestep = 324. State = [[-0.11253572  0.02246896]]. Action = [[ 0.20954502 -0.06815246 -0.09732533  0.55767345]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 324 is [True, False, False, False, True, False]
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 324 of 1
Current timestep = 325. State = [[-0.09801697  0.02679664]]. Action = [[ 0.06058586  0.19208497 -0.17210452  0.4590639 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 325 is [True, False, False, False, True, False]
Current timestep = 326. State = [[-0.09751964  0.02382989]]. Action = [[-0.22862329 -0.21794917 -0.18089458 -0.5112066 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 326 is [True, False, False, False, True, False]
Scene graph at timestep 326 is [True, False, False, False, True, False]
State prediction error at timestep 326 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of 0
Current timestep = 327. State = [[-0.10397156  0.01532581]]. Action = [[-0.08180436  0.05720371  0.16409588  0.9639827 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 327 is [True, False, False, False, True, False]
Current timestep = 328. State = [[-0.10426778  0.00926748]]. Action = [[ 0.14769775 -0.1632205  -0.22570342  0.6433581 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 328 is [True, False, False, False, True, False]
Current timestep = 329. State = [[-0.10036143  0.00373089]]. Action = [[ 0.11122835  0.07091576 -0.03494355 -0.9340453 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 329 is [True, False, False, False, True, False]
Current timestep = 330. State = [[-0.10120857 -0.0096554 ]]. Action = [[-0.19303933 -0.2403224   0.06341082 -0.8473785 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 330 is [True, False, False, False, True, False]
Current timestep = 331. State = [[-0.09922481 -0.03184196]]. Action = [[ 0.19389802 -0.13825099 -0.2307884  -0.693646  ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 331 is [True, False, False, False, True, False]
Scene graph at timestep 331 is [True, False, False, False, True, False]
State prediction error at timestep 331 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 331 of 0
Current timestep = 332. State = [[-0.0948563  -0.05774812]]. Action = [[-0.0245239  -0.19163115 -0.14447425 -0.34414017]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 332 is [True, False, False, False, True, False]
Current timestep = 333. State = [[-0.08982887 -0.07386552]]. Action = [[ 0.12923545 -0.02883354  0.0167115  -0.1769973 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 333 is [True, False, False, False, True, False]
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 333 of 1
Current timestep = 334. State = [[-0.0818813  -0.08619014]]. Action = [[-0.0159128  -0.11162618 -0.19219193  0.85763144]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 334 is [True, False, False, False, True, False]
Current timestep = 335. State = [[-0.07547811 -0.10495301]]. Action = [[ 0.1268608  -0.18214248  0.1812726   0.7831563 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 335 is [True, False, False, False, True, False]
Current timestep = 336. State = [[-0.05698654 -0.12323899]]. Action = [[ 0.22164023 -0.05312082 -0.13743137  0.5416317 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 336 is [True, False, False, False, True, False]
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 336 of 1
Current timestep = 337. State = [[-0.03455973 -0.11864641]]. Action = [[-0.05874836  0.20205122  0.19554031 -0.18138695]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 337 is [True, False, False, False, True, False]
Current timestep = 338. State = [[-0.04112638 -0.12274893]]. Action = [[-0.2262208  -0.18814762  0.0837819   0.4276005 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 338 is [False, True, False, False, True, False]
Scene graph at timestep 338 is [False, True, False, False, True, False]
State prediction error at timestep 338 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 338 of -1
Current timestep = 339. State = [[-0.04811358 -0.13048114]]. Action = [[-0.03039053  0.06814796 -0.12642609 -0.9574298 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 339 is [False, True, False, False, True, False]
Scene graph at timestep 339 is [False, True, False, True, False, False]
State prediction error at timestep 339 is tensor(1.0663e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 339 of -1
Current timestep = 340. State = [[-0.05753763 -0.11505899]]. Action = [[-0.12057519  0.19207877 -0.05532578 -0.43661857]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 340 is [False, True, False, True, False, False]
Current timestep = 341. State = [[-0.06136596 -0.10163847]]. Action = [[ 0.14576975 -0.0207089  -0.11842613  0.8685889 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 341 is [True, False, False, False, True, False]
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(3.3486e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 341 of -1
Current timestep = 342. State = [[-0.05555547 -0.10055252]]. Action = [[ 0.14710006 -0.02116911  0.0047937  -0.48276842]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 342 is [True, False, False, False, True, False]
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 342 of 1
Current timestep = 343. State = [[-0.05389009 -0.10014565]]. Action = [[-0.0569499   0.02212897  0.18267825 -0.83491206]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 343 is [True, False, False, False, True, False]
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 343 of 1
Current timestep = 344. State = [[-0.05438551 -0.08665866]]. Action = [[-0.06281218  0.22377795 -0.15508407 -0.20702285]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 344 is [True, False, False, False, True, False]
Scene graph at timestep 344 is [True, False, False, False, True, False]
State prediction error at timestep 344 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 344 of 1
Current timestep = 345. State = [[-0.05026324 -0.06130378]]. Action = [[ 0.22378224  0.08628964 -0.1701096   0.2474544 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 345 is [True, False, False, False, True, False]
Current timestep = 346. State = [[-0.04092694 -0.0432575 ]]. Action = [[-0.01601703  0.18199217 -0.05244619 -0.5722005 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 346 is [True, False, False, False, True, False]
Current timestep = 347. State = [[-0.04147421 -0.03007511]]. Action = [[-0.02437741  0.01795042 -0.1128132   0.20743227]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 347 is [False, True, False, False, True, False]
Scene graph at timestep 347 is [False, True, False, False, True, False]
State prediction error at timestep 347 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 347 of 1
Current timestep = 348. State = [[-0.18342753 -0.09053458]]. Action = [[ 0.1401358   0.09002036 -0.05201548 -0.9064639 ]]. Reward = [100.]
Curr episode timestep = 28
Scene graph at timestep 348 is [False, True, False, False, True, False]
Current timestep = 349. State = [[-0.16223066 -0.09681037]]. Action = [[ 0.23694474  0.08862531 -0.23105916  0.42251623]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 349 is [True, False, False, False, True, False]
Scene graph at timestep 349 is [True, False, False, False, True, False]
State prediction error at timestep 349 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 349 of 1
Current timestep = 350. State = [[-0.1304881  -0.08180136]]. Action = [[ 0.19446534  0.18297574 -0.01102982  0.8899243 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 350 is [True, False, False, False, True, False]
Current timestep = 351. State = [[-0.1052589  -0.05600942]]. Action = [[ 0.15719903  0.24632812  0.01422697 -0.42115438]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 351 is [True, False, False, False, True, False]
Current timestep = 352. State = [[-0.0872144  -0.02937989]]. Action = [[ 0.05095831  0.1173901  -0.23940776  0.13388109]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 352 is [True, False, False, False, True, False]
Current timestep = 353. State = [[-0.08234097 -0.017002  ]]. Action = [[-0.0476843   0.01312065  0.07420954  0.9322964 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 353 is [True, False, False, False, True, False]
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 353 of 1
Current timestep = 354. State = [[-0.08680777 -0.00697132]]. Action = [[-0.17594026  0.08385143  0.13450935 -0.19582117]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 354 is [True, False, False, False, True, False]
Current timestep = 355. State = [[-0.09931354 -0.00814438]]. Action = [[-0.2149682  -0.11660399  0.02217215  0.95339704]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 355 is [True, False, False, False, True, False]
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(7.2191e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 355 of -1
Current timestep = 356. State = [[-0.11223182 -0.00312581]]. Action = [[0.22676757 0.22209114 0.02626371 0.02862108]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 356 is [True, False, False, False, True, False]
Current timestep = 357. State = [[-0.0989348   0.01410228]]. Action = [[0.20461929 0.07013977 0.1750691  0.8491062 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 357 is [True, False, False, False, True, False]
Scene graph at timestep 357 is [True, False, False, False, True, False]
State prediction error at timestep 357 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 357 of 1
Current timestep = 358. State = [[-0.07680092  0.01730579]]. Action = [[ 0.02429929 -0.11260165 -0.02198815 -0.3250543 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 358 is [True, False, False, False, True, False]
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 358 of 1
Current timestep = 359. State = [[-0.07562345  0.01958791]]. Action = [[ 0.03081638  0.14075625  0.1474598  -0.17313403]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 359 is [True, False, False, False, True, False]
Scene graph at timestep 359 is [True, False, False, False, True, False]
State prediction error at timestep 359 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 359 of 1
Current timestep = 360. State = [[-0.07781065  0.033322  ]]. Action = [[-0.13682759  0.0855453  -0.13140002  0.29432666]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 360 is [True, False, False, False, True, False]
Current timestep = 361. State = [[-0.07859898  0.03272183]]. Action = [[ 0.05155051 -0.13855866  0.04297104 -0.9172688 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 361 is [True, False, False, False, True, False]
Scene graph at timestep 361 is [True, False, False, False, True, False]
State prediction error at timestep 361 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 361 of 0
Current timestep = 362. State = [[-0.07641681  0.01051482]]. Action = [[ 0.0463551  -0.23665518 -0.07672504 -0.46167588]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 362 is [True, False, False, False, True, False]
Current timestep = 363. State = [[-0.07316392 -0.00905243]]. Action = [[-0.0019687  -0.06677276 -0.06176439  0.51312923]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 363 is [True, False, False, False, True, False]
Current timestep = 364. State = [[-0.07124861 -0.01698242]]. Action = [[ 0.05712357 -0.02012177 -0.10100833  0.7818872 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 364 is [True, False, False, False, True, False]
Scene graph at timestep 364 is [True, False, False, False, True, False]
State prediction error at timestep 364 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 364 of 1
Current timestep = 365. State = [[-0.06936891 -0.0170324 ]]. Action = [[ 0.05997294  0.07247144  0.11292702 -0.50398564]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 365 is [True, False, False, False, True, False]
Current timestep = 366. State = [[-0.06524764 -0.01035473]]. Action = [[ 0.01053479  0.07853195 -0.03456484  0.7427175 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 366 is [True, False, False, False, True, False]
Scene graph at timestep 366 is [True, False, False, False, True, False]
State prediction error at timestep 366 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 366 of 1
Current timestep = 367. State = [[-0.06632835  0.00851376]]. Action = [[-0.2180897   0.23305026 -0.18110904  0.29773498]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 367 is [True, False, False, False, True, False]
Current timestep = 368. State = [[-0.06733392  0.01857367]]. Action = [[ 0.22270602 -0.14843392  0.1469264  -0.28836155]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 368 is [True, False, False, False, True, False]
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of 0
Current timestep = 369. State = [[-0.06144643  0.01896375]]. Action = [[-0.12807788  0.1032103   0.04833886  0.8492048 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 369 is [True, False, False, False, True, False]
Current timestep = 370. State = [[-0.06284048  0.0304766 ]]. Action = [[ 0.10601044  0.11818442 -0.18618995  0.01763153]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 370 is [True, False, False, False, True, False]
Current timestep = 371. State = [[-0.05511167  0.04735006]]. Action = [[ 0.24465549  0.15666938  0.0815953  -0.7962876 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 371 is [True, False, False, False, True, False]
Current timestep = 372. State = [[-0.23508301 -0.05836497]]. Action = [[ 0.18220294  0.20250845 -0.11862034  0.14999664]]. Reward = [100.]
Curr episode timestep = 23
Scene graph at timestep 372 is [True, False, False, False, True, False]
Current timestep = 373. State = [[-0.23121881 -0.07936645]]. Action = [[-0.04651932 -0.18758674  0.24614826 -0.2878251 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 373 is [True, False, False, False, True, False]
Current timestep = 374. State = [[-0.23758562 -0.09446713]]. Action = [[-0.18094991 -0.0335281  -0.2048695  -0.8199048 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 374 is [True, False, False, False, True, False]
Scene graph at timestep 374 is [True, False, False, False, True, False]
State prediction error at timestep 374 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 374 of -1
Current timestep = 375. State = [[-0.2459925  -0.08980184]]. Action = [[ 0.09690022  0.21415836 -0.06833935  0.01057518]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 375 is [True, False, False, False, True, False]
Scene graph at timestep 375 is [True, False, False, False, True, False]
State prediction error at timestep 375 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of 0
Current timestep = 376. State = [[-0.24343306 -0.082137  ]]. Action = [[ 0.04552302 -0.13646917  0.04580486  0.46589243]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 376 is [True, False, False, False, True, False]
Current timestep = 377. State = [[-0.24154894 -0.08010498]]. Action = [[-0.03155166  0.14402282 -0.09371129 -0.6979015 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 377 is [True, False, False, False, True, False]
Scene graph at timestep 377 is [True, False, False, False, True, False]
State prediction error at timestep 377 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of 0
Current timestep = 378. State = [[-0.24368617 -0.08756538]]. Action = [[-0.09041741 -0.2273142  -0.18234937 -0.75792706]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 378 is [True, False, False, False, True, False]
Scene graph at timestep 378 is [True, False, False, False, True, False]
State prediction error at timestep 378 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 378 of -1
Current timestep = 379. State = [[-0.24889638 -0.11403655]]. Action = [[ 0.00706002 -0.1960744   0.06691411  0.7001047 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 379 is [True, False, False, False, True, False]
Current timestep = 380. State = [[-0.24161023 -0.11868685]]. Action = [[0.19995975 0.15553623 0.19900292 0.4796368 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 380 is [True, False, False, False, True, False]
Current timestep = 381. State = [[-0.2227317  -0.10120952]]. Action = [[0.22378051 0.20392945 0.24050629 0.5975976 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 381 is [True, False, False, False, True, False]
Scene graph at timestep 381 is [True, False, False, False, True, False]
State prediction error at timestep 381 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 381 of 1
Current timestep = 382. State = [[-0.20322797 -0.08612665]]. Action = [[-0.14939214 -0.05869007  0.08980006  0.73186755]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 382 is [True, False, False, False, True, False]
Scene graph at timestep 382 is [True, False, False, False, True, False]
State prediction error at timestep 382 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 382 of 0
Current timestep = 383. State = [[-0.20465566 -0.08697561]]. Action = [[ 0.11985883  0.06978428  0.19725502 -0.42147028]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 383 is [True, False, False, False, True, False]
Scene graph at timestep 383 is [True, False, False, False, True, False]
State prediction error at timestep 383 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 383 of 0
Current timestep = 384. State = [[-0.20375185 -0.08009396]]. Action = [[-0.0842846   0.03740126 -0.15738201 -0.09340096]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 384 is [True, False, False, False, True, False]
Scene graph at timestep 384 is [True, False, False, False, True, False]
State prediction error at timestep 384 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 384 of 0
Current timestep = 385. State = [[-0.21141079 -0.08159113]]. Action = [[-0.19184384 -0.06408072 -0.03761424  0.8778119 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 385 is [True, False, False, False, True, False]
Scene graph at timestep 385 is [True, False, False, False, True, False]
State prediction error at timestep 385 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 385 of -1
Current timestep = 386. State = [[-0.21657619 -0.07477389]]. Action = [[ 0.21806532  0.17001843 -0.1745824   0.06069934]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 386 is [True, False, False, False, True, False]
Scene graph at timestep 386 is [True, False, False, False, True, False]
State prediction error at timestep 386 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of 1
Current timestep = 387. State = [[-0.20953953 -0.06290346]]. Action = [[-0.05416581 -0.01498941  0.22838044 -0.8715289 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 387 is [True, False, False, False, True, False]
Scene graph at timestep 387 is [True, False, False, False, True, False]
State prediction error at timestep 387 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 387 of 1
Current timestep = 388. State = [[-0.21199134 -0.06152361]]. Action = [[-0.10885674  0.02819809  0.20988604  0.9436927 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 388 is [True, False, False, False, True, False]
Current timestep = 389. State = [[-0.20916468 -0.05393928]]. Action = [[ 0.22021821  0.10209799 -0.02291656  0.01794016]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 389 is [True, False, False, False, True, False]
Current timestep = 390. State = [[-0.20497625 -0.04807087]]. Action = [[ 0.02129972 -0.01893824 -0.1783916  -0.7429949 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 390 is [True, False, False, False, True, False]
Current timestep = 391. State = [[-0.19774015 -0.04139934]]. Action = [[ 0.11871767  0.0934495  -0.24349663 -0.85091877]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 391 is [True, False, False, False, True, False]
Current timestep = 392. State = [[-0.18781391 -0.04176185]]. Action = [[-0.08460073 -0.11692992  0.10474694  0.37028384]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 392 is [True, False, False, False, True, False]
Scene graph at timestep 392 is [True, False, False, False, True, False]
State prediction error at timestep 392 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 392 of 1
Current timestep = 393. State = [[-0.18565598 -0.04996185]]. Action = [[ 0.10135192 -0.07824466  0.02368391 -0.9266666 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 393 is [True, False, False, False, True, False]
Current timestep = 394. State = [[-0.18598796 -0.05842347]]. Action = [[-0.15363763 -0.05533358  0.18550318 -0.87167406]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 394 is [True, False, False, False, True, False]
Scene graph at timestep 394 is [True, False, False, False, True, False]
State prediction error at timestep 394 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 394 of 1
Current timestep = 395. State = [[-0.19082926 -0.06510476]]. Action = [[-0.05847973  0.01981482 -0.136193   -0.92400306]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 395 is [True, False, False, False, True, False]
Scene graph at timestep 395 is [True, False, False, False, True, False]
State prediction error at timestep 395 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 395 of -1
Current timestep = 396. State = [[-0.19304086 -0.06680933]]. Action = [[-0.02510622 -0.0392319  -0.06949422 -0.8629202 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 396 is [True, False, False, False, True, False]
Scene graph at timestep 396 is [True, False, False, False, True, False]
State prediction error at timestep 396 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 396 of -1
Current timestep = 397. State = [[-0.20429246 -0.08403259]]. Action = [[-0.1716805  -0.20802681 -0.21922365  0.00668752]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 397 is [True, False, False, False, True, False]
Current timestep = 398. State = [[-0.2132316  -0.08537877]]. Action = [[ 0.1428993   0.19589168 -0.17126289  0.54016423]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 398 is [True, False, False, False, True, False]
Current timestep = 399. State = [[-0.20980127 -0.0835836 ]]. Action = [[ 0.06896734 -0.13815823 -0.1419111   0.11997902]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 399 is [True, False, False, False, True, False]
Scene graph at timestep 399 is [True, False, False, False, True, False]
State prediction error at timestep 399 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 399 of -1
Current timestep = 400. State = [[-0.21137026 -0.084297  ]]. Action = [[-0.19762641  0.07877204  0.0754256  -0.40548474]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 400 is [True, False, False, False, True, False]
Scene graph at timestep 400 is [True, False, False, False, True, False]
State prediction error at timestep 400 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 400 of -1
Current timestep = 401. State = [[-0.21186085 -0.09023171]]. Action = [[ 0.19399181 -0.15133898  0.17469329 -0.7339956 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 401 is [True, False, False, False, True, False]
Scene graph at timestep 401 is [True, False, False, False, True, False]
State prediction error at timestep 401 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 401 of 1
Current timestep = 402. State = [[-0.20210207 -0.0847748 ]]. Action = [[ 0.11961156  0.24425995  0.17521858 -0.30921632]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 402 is [True, False, False, False, True, False]
Scene graph at timestep 402 is [True, False, False, False, True, False]
State prediction error at timestep 402 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 402 of 1
Current timestep = 403. State = [[-0.18651566 -0.05520017]]. Action = [[ 0.11012647  0.22989151  0.13013917 -0.49840307]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 403 is [True, False, False, False, True, False]
Current timestep = 404. State = [[-0.18179932 -0.03826701]]. Action = [[-0.13123153  0.02319086 -0.20685594  0.97805   ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 404 is [True, False, False, False, True, False]
Current timestep = 405. State = [[-0.18037269 -0.02547485]]. Action = [[ 0.1567064   0.12859154  0.23815003 -0.7153032 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 405 is [True, False, False, False, True, False]
Scene graph at timestep 405 is [True, False, False, False, True, False]
State prediction error at timestep 405 is tensor(5.6308e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of 1
Current timestep = 406. State = [[-0.17581485 -0.01357983]]. Action = [[-0.04256803  0.01083684  0.2002151   0.4014759 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 406 is [True, False, False, False, True, False]
Scene graph at timestep 406 is [True, False, False, False, True, False]
State prediction error at timestep 406 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 406 of 1
Current timestep = 407. State = [[-0.17826112 -0.01253329]]. Action = [[-0.12084039 -0.0063456  -0.18020625 -0.31319845]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 407 is [True, False, False, False, True, False]
Current timestep = 408. State = [[-0.19021274 -0.00205426]]. Action = [[-0.20747143  0.16213912 -0.08155975  0.8729094 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 408 is [True, False, False, False, True, False]
Current timestep = 409. State = [[-0.21337722  0.01757362]]. Action = [[-0.2042779   0.09929064 -0.1412654   0.30170226]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 409 is [True, False, False, False, True, False]
Scene graph at timestep 409 is [True, False, False, False, True, False]
State prediction error at timestep 409 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 409 of -1
Current timestep = 410. State = [[-0.23479253  0.03764232]]. Action = [[ 0.11239615  0.16465902 -0.10539104  0.07145619]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 410 is [True, False, False, False, True, False]
Current timestep = 411. State = [[-0.22374773  0.05071327]]. Action = [[ 0.2040276   0.06957605 -0.12202631  0.3134389 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 411 is [True, False, False, False, True, False]
Current timestep = 412. State = [[-0.19974887  0.05472895]]. Action = [[ 0.24611476 -0.04118074  0.12718475 -0.5467671 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 412 is [True, False, False, False, True, False]
Current timestep = 413. State = [[-0.16833428  0.04607681]]. Action = [[ 0.22564104 -0.18544658  0.05639547 -0.8247789 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 413 is [True, False, False, False, True, False]
Scene graph at timestep 413 is [True, False, False, False, True, False]
State prediction error at timestep 413 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 413 of 1
Current timestep = 414. State = [[-0.14699477  0.03215748]]. Action = [[-0.2080801  -0.01798755 -0.18441711 -0.07994735]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 414 is [True, False, False, False, True, False]
Scene graph at timestep 414 is [True, False, False, False, True, False]
State prediction error at timestep 414 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 414 of -1
Current timestep = 415. State = [[-0.15754858  0.0393291 ]]. Action = [[-0.12445024  0.19461751 -0.24161007 -0.6646894 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 415 is [True, False, False, False, True, False]
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of -1
Current timestep = 416. State = [[-0.16326313  0.04326976]]. Action = [[ 0.17769018 -0.22096847 -0.085751   -0.83960956]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 416 is [True, False, False, False, True, False]
Scene graph at timestep 416 is [True, False, False, False, True, False]
State prediction error at timestep 416 is tensor(1.9829e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 416 of 1
Current timestep = 417. State = [[-0.15108675  0.03528853]]. Action = [[ 0.19311476  0.15202093  0.0025523  -0.9313313 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 417 is [True, False, False, False, True, False]
Scene graph at timestep 417 is [True, False, False, False, True, False]
State prediction error at timestep 417 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 417 of 1
Current timestep = 418. State = [[-0.13348725  0.03130566]]. Action = [[-0.05079442 -0.22460979 -0.15235843 -0.8041753 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 418 is [True, False, False, False, True, False]
Current timestep = 419. State = [[-0.12619336  0.02725489]]. Action = [[ 0.2122913   0.14732361 -0.03630586 -0.4721468 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 419 is [True, False, False, False, True, False]
Scene graph at timestep 419 is [True, False, False, False, True, False]
State prediction error at timestep 419 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 419 of 1
Current timestep = 420. State = [[-0.11491879  0.029472  ]]. Action = [[-0.04826389 -0.06601839 -0.06164239 -0.5303356 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 420 is [True, False, False, False, True, False]
Scene graph at timestep 420 is [True, False, False, False, True, False]
State prediction error at timestep 420 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 420 of 1
Current timestep = 421. State = [[-0.11285229  0.01377682]]. Action = [[ 0.02031913 -0.22755487  0.12634134 -0.7477569 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 421 is [True, False, False, False, True, False]
Current timestep = 422. State = [[-0.10171445 -0.00464843]]. Action = [[ 0.21836245 -0.03768566  0.04516855  0.6697521 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 422 is [True, False, False, False, True, False]
Scene graph at timestep 422 is [True, False, False, False, True, False]
State prediction error at timestep 422 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 422 of 1
Current timestep = 423. State = [[-0.08068009 -0.02201665]]. Action = [[ 0.07828385 -0.19726747 -0.19722982 -0.27656448]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 423 is [True, False, False, False, True, False]
Current timestep = 424. State = [[-0.0664925  -0.03205488]]. Action = [[ 0.2338081   0.07909542  0.01047879 -0.5183656 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 424 is [True, False, False, False, True, False]
Scene graph at timestep 424 is [True, False, False, False, True, False]
State prediction error at timestep 424 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 424 of 1
Current timestep = 425. State = [[-0.04023132 -0.02818352]]. Action = [[-0.13148473  0.00744316 -0.06522682 -0.85432005]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 425 is [True, False, False, False, True, False]
Scene graph at timestep 425 is [False, True, False, False, True, False]
State prediction error at timestep 425 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 425 of 1
Current timestep = 426. State = [[-0.04512829 -0.01724931]]. Action = [[-0.1377807   0.18679273 -0.20009637  0.83617985]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 426 is [False, True, False, False, True, False]
Current timestep = 427. State = [[-0.05944576 -0.00518425]]. Action = [[-0.22532116 -0.00616366  0.15437758  0.04813719]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 427 is [False, True, False, False, True, False]
Scene graph at timestep 427 is [True, False, False, False, True, False]
State prediction error at timestep 427 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 427 of -1
Current timestep = 428. State = [[-0.07979117 -0.00246663]]. Action = [[0.12859869 0.02937856 0.05639106 0.3919822 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 428 is [True, False, False, False, True, False]
Current timestep = 429. State = [[-0.0693424   0.00534837]]. Action = [[ 0.2439124   0.10968158 -0.13755338  0.06979597]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 429 is [True, False, False, False, True, False]
Current timestep = 430. State = [[-0.05026233  0.01035115]]. Action = [[ 0.14390457 -0.04587565  0.12948853  0.21152449]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 430 is [True, False, False, False, True, False]
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 430 of 1
Current timestep = 431. State = [[-0.03819356  0.00493762]]. Action = [[-0.08556417 -0.11220078  0.00326508 -0.72788566]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 431 is [True, False, False, False, True, False]
Scene graph at timestep 431 is [False, True, False, False, True, False]
State prediction error at timestep 431 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 431 of 0
Current timestep = 432. State = [[-0.2582588  -0.09907544]]. Action = [[ 0.10044715  0.21231118  0.21502745 -0.00826818]]. Reward = [100.]
Curr episode timestep = 59
Scene graph at timestep 432 is [False, True, False, False, True, False]
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(0.0231, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 432 of 0
Current timestep = 433. State = [[-0.24656552 -0.09801241]]. Action = [[ 0.2223782   0.2318263  -0.15488102 -0.24237007]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 433 is [True, False, False, False, True, False]
Current timestep = 434. State = [[-0.22364539 -0.07645918]]. Action = [[ 0.15384746  0.16834486 -0.0393654  -0.17200571]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 434 is [True, False, False, False, True, False]
Current timestep = 435. State = [[-0.21473458 -0.06437709]]. Action = [[-0.18041869 -0.02588528 -0.1907851   0.7672825 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 435 is [True, False, False, False, True, False]
Scene graph at timestep 435 is [True, False, False, False, True, False]
State prediction error at timestep 435 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 435 of 1
Current timestep = 436. State = [[-0.21260272 -0.05695105]]. Action = [[0.19732463 0.09095085 0.17855889 0.310547  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 436 is [True, False, False, False, True, False]
Scene graph at timestep 436 is [True, False, False, False, True, False]
State prediction error at timestep 436 is tensor(5.0361e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 436 of 1
Current timestep = 437. State = [[-0.19425982 -0.04058559]]. Action = [[ 0.22845042  0.14900622 -0.2300462   0.17514122]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 437 is [True, False, False, False, True, False]
Current timestep = 438. State = [[-0.17962341 -0.01592727]]. Action = [[-0.0959484   0.23428255 -0.03135702 -0.92681193]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 438 is [True, False, False, False, True, False]
Scene graph at timestep 438 is [True, False, False, False, True, False]
State prediction error at timestep 438 is tensor(2.6148e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 438 of 1
Current timestep = 439. State = [[-0.1902579  0.0179166]]. Action = [[-0.18421008  0.19455767 -0.04272714 -0.01081163]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 439 is [True, False, False, False, True, False]
Current timestep = 440. State = [[-0.20392764  0.04020562]]. Action = [[-0.13120237  0.07951111 -0.07590586  0.3150829 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 440 is [True, False, False, False, True, False]
Current timestep = 441. State = [[-0.21059868  0.06175271]]. Action = [[ 0.12470084  0.22088695  0.19038954 -0.36279637]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 441 is [True, False, False, False, True, False]
Current timestep = 442. State = [[-0.2168814   0.08348865]]. Action = [[-0.11770517  0.10532776 -0.10799822 -0.06582397]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 442 is [True, False, False, False, True, False]
Current timestep = 443. State = [[-0.22162029  0.09274112]]. Action = [[ 0.01500449 -0.03024426 -0.01776484  0.38574886]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 443 is [True, False, False, False, True, False]
Current timestep = 444. State = [[-0.21830027  0.09402097]]. Action = [[ 0.11887956  0.03905046 -0.1000355  -0.37855273]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 444 is [True, False, False, False, True, False]
Current timestep = 445. State = [[-0.22015165  0.10868838]]. Action = [[-0.14505798  0.20343271  0.10891664  0.9057685 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 445 is [True, False, False, False, True, False]
Current timestep = 446. State = [[-0.2224208   0.11617172]]. Action = [[ 0.10609549 -0.11804157 -0.03822754 -0.5284572 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 446 is [True, False, False, False, True, False]
Current timestep = 447. State = [[-0.21236819  0.11845946]]. Action = [[0.11056179 0.10906148 0.14636922 0.6744263 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 447 is [True, False, False, False, True, False]
Current timestep = 448. State = [[-0.19501103  0.13543622]]. Action = [[ 0.1760838   0.19461891  0.07846969 -0.22106999]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 448 is [True, False, False, False, True, False]
Current timestep = 449. State = [[-0.16829044  0.15253694]]. Action = [[0.20823288 0.0711391  0.0664182  0.66489613]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 449 is [True, False, False, False, False, True]
Current timestep = 450. State = [[-0.14801179  0.15788555]]. Action = [[-0.01258966 -0.07596338 -0.20627384 -0.942415  ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 450 is [True, False, False, False, False, True]
Current timestep = 451. State = [[-0.13854507  0.15194057]]. Action = [[ 0.1286377  -0.06782362 -0.07390019 -0.35511732]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 451 is [True, False, False, False, False, True]
Current timestep = 452. State = [[-0.12275201  0.15092368]]. Action = [[ 0.18324846  0.06652707 -0.19363905 -0.33919454]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 452 is [True, False, False, False, False, True]
Scene graph at timestep 452 is [True, False, False, False, False, True]
State prediction error at timestep 452 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 452 of 1
Current timestep = 453. State = [[-0.1099468   0.16749851]]. Action = [[-0.20603047  0.19051829 -0.0398626   0.3331257 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 453 is [True, False, False, False, False, True]
Scene graph at timestep 453 is [True, False, False, False, False, True]
State prediction error at timestep 453 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 453 of -1
Current timestep = 454. State = [[-0.11678702  0.17055447]]. Action = [[ 0.02077055 -0.22306351 -0.1556357   0.5086503 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 454 is [True, False, False, False, False, True]
Scene graph at timestep 454 is [True, False, False, False, False, True]
State prediction error at timestep 454 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 454 of 1
Current timestep = 455. State = [[-0.11151905  0.15709937]]. Action = [[ 0.1785276   0.04689294 -0.08663414  0.51363516]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 455 is [True, False, False, False, False, True]
Current timestep = 456. State = [[-0.10667212  0.16077343]]. Action = [[ 0.03209224  0.06024382  0.06097153 -0.3198862 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 456 is [True, False, False, False, False, True]
Current timestep = 457. State = [[-0.09332797  0.1709635 ]]. Action = [[ 0.1418499   0.12385321 -0.18226472  0.82908463]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 457 is [True, False, False, False, False, True]
Current timestep = 458. State = [[-0.07522245  0.18079966]]. Action = [[ 0.06257778  0.03241378 -0.13186833  0.7151828 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 458 is [True, False, False, False, False, True]
Current timestep = 459. State = [[-0.05896198  0.19677481]]. Action = [[ 0.2150799   0.20555666 -0.15432924 -0.20734978]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 459 is [True, False, False, False, False, True]
Scene graph at timestep 459 is [True, False, False, False, False, True]
State prediction error at timestep 459 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 459 of -1
Current timestep = 460. State = [[-0.03743365  0.20512505]]. Action = [[-0.08183706 -0.16119811  0.03080213  0.394485  ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 460 is [True, False, False, False, False, True]
Scene graph at timestep 460 is [False, True, False, False, False, True]
State prediction error at timestep 460 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of -1
Current timestep = 461. State = [[-0.0390676   0.19624498]]. Action = [[-0.19882606 -0.08271652  0.01407939 -0.6958887 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 461 is [False, True, False, False, False, True]
Scene graph at timestep 461 is [False, True, False, False, False, True]
State prediction error at timestep 461 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 461 of 1
Current timestep = 462. State = [[-0.04692188  0.17960888]]. Action = [[-0.0878389  -0.23041248 -0.09926638  0.5446609 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 462 is [False, True, False, False, False, True]
Scene graph at timestep 462 is [False, True, False, False, False, True]
State prediction error at timestep 462 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 462 of 1
Current timestep = 463. State = [[-0.05229488  0.16887362]]. Action = [[-0.03413315  0.09476709  0.18910295  0.23903012]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 463 is [False, True, False, False, False, True]
Scene graph at timestep 463 is [True, False, False, False, False, True]
State prediction error at timestep 463 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 463 of -1
Current timestep = 464. State = [[-0.05359583  0.17887464]]. Action = [[ 0.20961988  0.10192865 -0.21136674 -0.46347237]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 464 is [True, False, False, False, False, True]
Current timestep = 465. State = [[-0.04374582  0.1790334 ]]. Action = [[ 0.24066347 -0.03074634  0.18901643 -0.37691623]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 465 is [True, False, False, False, False, True]
Scene graph at timestep 465 is [False, True, False, False, False, True]
State prediction error at timestep 465 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of 0
Current timestep = 466. State = [[-0.01591197  0.17444676]]. Action = [[ 0.24641323 -0.06863645 -0.08424391  0.8323531 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 466 is [False, True, False, False, False, True]
Scene graph at timestep 466 is [False, True, False, False, False, True]
State prediction error at timestep 466 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 466 of 0
Current timestep = 467. State = [[0.00964028 0.17431487]]. Action = [[-0.00971952  0.09614158 -0.04295948 -0.17352188]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 467 is [False, True, False, False, False, True]
Current timestep = 468. State = [[0.01233585 0.16845092]]. Action = [[ 0.05303711 -0.15134697  0.03544623 -0.9014449 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 468 is [False, True, False, False, False, True]
Current timestep = 469. State = [[0.02077733 0.16812111]]. Action = [[ 0.17471212  0.10877591  0.21118903 -0.08545172]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 469 is [False, True, False, False, False, True]
Scene graph at timestep 469 is [False, True, False, False, False, True]
State prediction error at timestep 469 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 469 of 0
Current timestep = 470. State = [[0.04496072 0.17974433]]. Action = [[ 0.20578885  0.11164549 -0.02131368 -0.46784532]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 470 is [False, True, False, False, False, True]
Scene graph at timestep 470 is [False, True, False, False, False, True]
State prediction error at timestep 470 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 470 of 0
Current timestep = 471. State = [[0.04872794 0.17034365]]. Action = [[ 0.07820481 -0.14370617 -0.12443569 -0.41947007]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 471 is [False, True, False, False, False, True]
Current timestep = 472. State = [[0.052988   0.15953723]]. Action = [[ 0.19553065  0.02358508 -0.17804986  0.93926823]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 472 is [False, True, False, False, False, True]
Scene graph at timestep 472 is [False, False, True, False, False, True]
State prediction error at timestep 472 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 472 of 1
Current timestep = 473. State = [[0.05350783 0.15727171]]. Action = [[-0.15174668 -0.01576746 -0.06685981 -0.7765134 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 473 is [False, False, True, False, False, True]
Scene graph at timestep 473 is [False, False, True, False, False, True]
State prediction error at timestep 473 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 473 of 1
Current timestep = 474. State = [[0.05306376 0.15707424]]. Action = [[ 0.21253788 -0.01917958 -0.06360099  0.49166858]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 474 is [False, False, True, False, False, True]
Scene graph at timestep 474 is [False, False, True, False, False, True]
State prediction error at timestep 474 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 474 of 0
Current timestep = 475. State = [[0.05306376 0.15707424]]. Action = [[ 0.23163077  0.22054696 -0.01361959  0.92647827]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 475 is [False, False, True, False, False, True]
Current timestep = 476. State = [[0.05569715 0.14608604]]. Action = [[ 0.0215309  -0.19117615 -0.22734706 -0.4212265 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 476 is [False, False, True, False, False, True]
Current timestep = 477. State = [[0.05903868 0.13320817]]. Action = [[ 0.04983133 -0.03274158 -0.08179291 -0.6408817 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 477 is [False, False, True, False, False, True]
Current timestep = 478. State = [[0.06069611 0.12873343]]. Action = [[ 0.10218504 -0.05550456 -0.14461805 -0.30193377]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 478 is [False, False, True, False, False, True]
Current timestep = 479. State = [[0.06089665 0.12817965]]. Action = [[0.05951947 0.02242339 0.10148624 0.18228793]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 479 is [False, False, True, False, False, True]
Current timestep = 480. State = [[0.06089665 0.12817965]]. Action = [[ 0.06107762  0.12292165 -0.11504862  0.51168275]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 480 is [False, False, True, False, False, True]
Scene graph at timestep 480 is [False, False, True, False, False, True]
State prediction error at timestep 480 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 480 of 1
Current timestep = 481. State = [[0.05824383 0.13353908]]. Action = [[-0.11944349  0.09445971 -0.08091795  0.94890404]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 481 is [False, False, True, False, False, True]
Scene graph at timestep 481 is [False, False, True, False, False, True]
State prediction error at timestep 481 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 481 of -1
Current timestep = 482. State = [[0.05631477 0.13831832]]. Action = [[ 0.21767166  0.11216554 -0.06826454 -0.6340908 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 482 is [False, False, True, False, False, True]
Scene graph at timestep 482 is [False, False, True, False, False, True]
State prediction error at timestep 482 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 482 of -1
Current timestep = 483. State = [[0.05631477 0.13831832]]. Action = [[ 0.23842669  0.22704741 -0.22840886 -0.9110841 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 483 is [False, False, True, False, False, True]
Current timestep = 484. State = [[0.05387262 0.13547967]]. Action = [[-0.1338473  -0.06759343 -0.19907944  0.10042441]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 484 is [False, False, True, False, False, True]
Current timestep = 485. State = [[0.05158128 0.13308086]]. Action = [[ 0.01440772  0.01124737 -0.15000863  0.9625734 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 485 is [False, False, True, False, False, True]
Current timestep = 486. State = [[0.0506814  0.13371809]]. Action = [[ 0.13856626 -0.03686041  0.1088669   0.44866037]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 486 is [False, False, True, False, False, True]
Scene graph at timestep 486 is [False, False, True, False, False, True]
State prediction error at timestep 486 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 486 of 1
Current timestep = 487. State = [[0.0434562  0.12585229]]. Action = [[-0.19424254 -0.14591356  0.00506684  0.9647591 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 487 is [False, False, True, False, False, True]
Current timestep = 488. State = [[-0.2691138  -0.06975134]]. Action = [[ 0.22597945 -0.10064605  0.02193317  0.68668413]]. Reward = [100.]
Curr episode timestep = 55
Scene graph at timestep 488 is [False, True, False, False, False, True]
Current timestep = 489. State = [[-0.26547506 -0.08269417]]. Action = [[ 0.08287281 -0.08884117  0.11440423  0.5895746 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 489 is [True, False, False, False, True, False]
Current timestep = 490. State = [[-0.2556901  -0.08004241]]. Action = [[ 0.11573622  0.17428601 -0.14059947 -0.7028893 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 490 is [True, False, False, False, True, False]
Scene graph at timestep 490 is [True, False, False, False, True, False]
State prediction error at timestep 490 is tensor(4.1574e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 490 of 1
Current timestep = 491. State = [[-0.23603827 -0.06021157]]. Action = [[ 0.20907015  0.17484486 -0.07556495  0.8793974 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 491 is [True, False, False, False, True, False]
Scene graph at timestep 491 is [True, False, False, False, True, False]
State prediction error at timestep 491 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 491 of 1
Current timestep = 492. State = [[-0.22008094 -0.05738259]]. Action = [[-0.12290443 -0.18803912 -0.22783728  0.5008569 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 492 is [True, False, False, False, True, False]
Scene graph at timestep 492 is [True, False, False, False, True, False]
State prediction error at timestep 492 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 492 of -1
Current timestep = 493. State = [[-0.23136055 -0.07617451]]. Action = [[-0.16885462 -0.09916401  0.06777948  0.5433049 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 493 is [True, False, False, False, True, False]
Scene graph at timestep 493 is [True, False, False, False, True, False]
State prediction error at timestep 493 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 493 of -1
Current timestep = 494. State = [[-0.24048685 -0.08797272]]. Action = [[ 0.10293639 -0.06780693  0.05321062  0.19691432]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 494 is [True, False, False, False, True, False]
Scene graph at timestep 494 is [True, False, False, False, True, False]
State prediction error at timestep 494 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 494 of -1
Current timestep = 495. State = [[-0.23222135 -0.08344439]]. Action = [[ 0.16784066  0.17701858  0.11713189 -0.9706111 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 495 is [True, False, False, False, True, False]
Current timestep = 496. State = [[-0.22573112 -0.07549346]]. Action = [[-0.09196006 -0.01266095 -0.1571101   0.00482929]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 496 is [True, False, False, False, True, False]
Scene graph at timestep 496 is [True, False, False, False, True, False]
State prediction error at timestep 496 is tensor(1.1461e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 496 of 1
Current timestep = 497. State = [[-0.23077984 -0.08264481]]. Action = [[-0.13814452 -0.14000554  0.20406392 -0.6771551 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 497 is [True, False, False, False, True, False]
Scene graph at timestep 497 is [True, False, False, False, True, False]
State prediction error at timestep 497 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of -1
Current timestep = 498. State = [[-0.23526156 -0.08096544]]. Action = [[0.15667722 0.18319476 0.00912002 0.24823415]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 498 is [True, False, False, False, True, False]
Scene graph at timestep 498 is [True, False, False, False, True, False]
State prediction error at timestep 498 is tensor(1.9596e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 498 of 1
Current timestep = 499. State = [[-0.22902045 -0.05499328]]. Action = [[ 0.026647    0.21470934 -0.1697982   0.6431587 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 499 is [True, False, False, False, True, False]
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of 1
Current timestep = 500. State = [[-0.2333146  -0.04661522]]. Action = [[-0.21163751 -0.14212595 -0.0287381  -0.09881508]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 500 is [True, False, False, False, True, False]
Current timestep = 501. State = [[-0.24974824 -0.04429418]]. Action = [[-0.20492369  0.15401441  0.2439186   0.22407734]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 501 is [True, False, False, False, True, False]
Scene graph at timestep 501 is [True, False, False, False, True, False]
State prediction error at timestep 501 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of -1
Current timestep = 502. State = [[-0.26764372 -0.04069479]]. Action = [[-0.02076596 -0.12015353 -0.02191477  0.96553135]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 502 is [True, False, False, False, True, False]
Scene graph at timestep 502 is [True, False, False, False, True, False]
State prediction error at timestep 502 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 502 of -1
Current timestep = 503. State = [[-0.2709229  -0.04496804]]. Action = [[ 0.08068976  0.09819269 -0.0894212  -0.14860487]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 503 is [True, False, False, False, True, False]
Current timestep = 504. State = [[-0.26119682 -0.03967615]]. Action = [[ 0.20979655  0.00396511 -0.06473088  0.8979566 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 504 is [True, False, False, False, True, False]
Scene graph at timestep 504 is [True, False, False, False, True, False]
State prediction error at timestep 504 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 504 of 1
Current timestep = 505. State = [[-0.24733244 -0.02754746]]. Action = [[0.02304012 0.15377545 0.18908542 0.12764037]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 505 is [True, False, False, False, True, False]
Current timestep = 506. State = [[-0.24975862 -0.00679405]]. Action = [[-0.1514008   0.18343681  0.21749237  0.8214214 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 506 is [True, False, False, False, True, False]
Scene graph at timestep 506 is [True, False, False, False, True, False]
State prediction error at timestep 506 is tensor(2.3609e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 506 of 0
Current timestep = 507. State = [[-0.24920593  0.02262295]]. Action = [[ 0.22271723  0.19649404 -0.1804696  -0.07837903]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 507 is [True, False, False, False, True, False]
Scene graph at timestep 507 is [True, False, False, False, True, False]
State prediction error at timestep 507 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 507 of 1
Current timestep = 508. State = [[-0.22227186  0.04236089]]. Action = [[ 0.23989064  0.07301736 -0.09593119  0.72155607]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 508 is [True, False, False, False, True, False]
Scene graph at timestep 508 is [True, False, False, False, True, False]
State prediction error at timestep 508 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 508 of 1
Current timestep = 509. State = [[-0.19213112  0.05074925]]. Action = [[ 0.19278443  0.01563919  0.10941809 -0.05458641]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 509 is [True, False, False, False, True, False]
Scene graph at timestep 509 is [True, False, False, False, True, False]
State prediction error at timestep 509 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 509 of 1
Current timestep = 510. State = [[-0.16353983  0.06523215]]. Action = [[0.20150888 0.18714729 0.18899143 0.11679399]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 510 is [True, False, False, False, True, False]
Scene graph at timestep 510 is [True, False, False, False, True, False]
State prediction error at timestep 510 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 510 of 1
Current timestep = 511. State = [[-0.13026746  0.06712949]]. Action = [[ 0.24717826 -0.21484792 -0.13811055  0.10388148]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 511 is [True, False, False, False, True, False]
Scene graph at timestep 511 is [True, False, False, False, True, False]
State prediction error at timestep 511 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 511 of 1
Current timestep = 512. State = [[-0.11109391  0.0436707 ]]. Action = [[-0.1944202  -0.18670247 -0.1978428  -0.43407047]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 512 is [True, False, False, False, True, False]
Current timestep = 513. State = [[-0.10968406  0.02506805]]. Action = [[ 0.19627762 -0.08112907  0.08704057 -0.18385273]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 513 is [True, False, False, False, True, False]
Scene graph at timestep 513 is [True, False, False, False, True, False]
State prediction error at timestep 513 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 513 of 1
Current timestep = 514. State = [[-0.10386474  0.02264468]]. Action = [[ 0.07839912  0.12322968  0.1004481  -0.11392528]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 514 is [True, False, False, False, True, False]
Current timestep = 515. State = [[-0.10235906  0.01987343]]. Action = [[-0.22305726 -0.14446333 -0.16609149 -0.38252324]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 515 is [True, False, False, False, True, False]
Current timestep = 516. State = [[-0.11198355  0.0171177 ]]. Action = [[-0.11725405  0.05881992  0.23880541  0.03162265]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 516 is [True, False, False, False, True, False]
Scene graph at timestep 516 is [True, False, False, False, True, False]
State prediction error at timestep 516 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 516 of 0
Current timestep = 517. State = [[-0.12718244  0.00482689]]. Action = [[-0.17952412 -0.23878527 -0.11927608  0.65899324]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 517 is [True, False, False, False, True, False]
Scene graph at timestep 517 is [True, False, False, False, True, False]
State prediction error at timestep 517 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 517 of -1
Current timestep = 518. State = [[-0.14947037 -0.02598193]]. Action = [[-0.12221995 -0.19705173  0.01849037 -0.01949567]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 518 is [True, False, False, False, True, False]
Scene graph at timestep 518 is [True, False, False, False, True, False]
State prediction error at timestep 518 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 518 of -1
Current timestep = 519. State = [[-0.15636751 -0.03431598]]. Action = [[ 0.22954077  0.14937338 -0.19937254 -0.21775705]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 519 is [True, False, False, False, True, False]
Current timestep = 520. State = [[-0.13982633 -0.02019214]]. Action = [[ 0.22141808  0.10950643  0.01934364 -0.36898756]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 520 is [True, False, False, False, True, False]
Current timestep = 521. State = [[-0.11403542 -0.00707693]]. Action = [[ 0.17829138  0.0586637  -0.00328359  0.2009418 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 521 is [True, False, False, False, True, False]
Scene graph at timestep 521 is [True, False, False, False, True, False]
State prediction error at timestep 521 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 521 of 1
Current timestep = 522. State = [[-0.09784721 -0.00524701]]. Action = [[-0.10538211 -0.12226129  0.10350305  0.5529537 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 522 is [True, False, False, False, True, False]
Current timestep = 523. State = [[-0.09520426 -0.02098009]]. Action = [[ 0.1052663  -0.15598474 -0.04566741 -0.5304306 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 523 is [True, False, False, False, True, False]
Current timestep = 524. State = [[-0.08486298 -0.02568846]]. Action = [[0.19652665 0.139523   0.04243091 0.90006113]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 524 is [True, False, False, False, True, False]
Current timestep = 525. State = [[-0.0731812  -0.03351502]]. Action = [[-0.09915423 -0.22453983  0.20276368  0.27532017]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 525 is [True, False, False, False, True, False]
Current timestep = 526. State = [[-0.0644466  -0.05257727]]. Action = [[ 0.22290307 -0.11343226  0.20859751 -0.5607939 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 526 is [True, False, False, False, True, False]
Current timestep = 527. State = [[-0.04117608 -0.05269595]]. Action = [[ 0.23672992  0.17481795  0.14768171 -0.16229606]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 527 is [True, False, False, False, True, False]
Scene graph at timestep 527 is [False, True, False, False, True, False]
State prediction error at timestep 527 is tensor(6.4614e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 527 of 1
Current timestep = 528. State = [[-0.26229107  0.17107032]]. Action = [[ 0.21367818 -0.04573259 -0.24616565 -0.94810677]]. Reward = [100.]
Curr episode timestep = 39
Scene graph at timestep 528 is [False, True, False, False, True, False]
Scene graph at timestep 528 is [True, False, False, False, False, True]
State prediction error at timestep 528 is tensor(0.0507, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 528 of -1
Current timestep = 529. State = [[-0.25439167  0.18110865]]. Action = [[ 0.06580502 -0.18671139  0.17611107  0.99123335]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 529 is [True, False, False, False, False, True]
Scene graph at timestep 529 is [True, False, False, False, False, True]
State prediction error at timestep 529 is tensor(8.4165e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 529 of 1
Current timestep = 530. State = [[-0.25284114  0.16946806]]. Action = [[-0.12843902 -0.01358944  0.09702229  0.5087404 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 530 is [True, False, False, False, False, True]
Scene graph at timestep 530 is [True, False, False, False, False, True]
State prediction error at timestep 530 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 530 of 0
Current timestep = 531. State = [[-0.25673357  0.16535763]]. Action = [[-0.07196005 -0.06863306  0.03761685  0.83763814]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 531 is [True, False, False, False, False, True]
Current timestep = 532. State = [[-0.2591671   0.14903377]]. Action = [[-0.00856976 -0.21129669  0.05574647  0.69677854]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 532 is [True, False, False, False, False, True]
Current timestep = 533. State = [[-0.26218277  0.13810253]]. Action = [[ 0.01510382  0.08142322 -0.05978659 -0.8092301 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 533 is [True, False, False, False, False, True]
Scene graph at timestep 533 is [True, False, False, False, False, True]
State prediction error at timestep 533 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 533 of -1
Current timestep = 534. State = [[-0.25464535  0.12541176]]. Action = [[ 0.21369463 -0.23015264 -0.23325883  0.72443736]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 534 is [True, False, False, False, False, True]
Current timestep = 535. State = [[-0.23972523  0.10339841]]. Action = [[ 0.15268129 -0.10352628  0.24664837 -0.83295536]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 535 is [True, False, False, False, False, True]
Current timestep = 536. State = [[-0.22155023  0.08592416]]. Action = [[ 0.08916491 -0.1254194  -0.18446822 -0.3632686 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 536 is [True, False, False, False, True, False]
Scene graph at timestep 536 is [True, False, False, False, True, False]
State prediction error at timestep 536 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 536 of 1
Current timestep = 537. State = [[-0.1999122   0.05998843]]. Action = [[ 0.22017461 -0.22614492 -0.11865686 -0.11315036]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 537 is [True, False, False, False, True, False]
Current timestep = 538. State = [[-0.16956301  0.03226202]]. Action = [[ 0.2443999  -0.17605473  0.13693309 -0.7897772 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 538 is [True, False, False, False, True, False]
Current timestep = 539. State = [[-0.13690709  0.02887543]]. Action = [[ 0.21737424  0.20149922  0.10583913 -0.6661595 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 539 is [True, False, False, False, True, False]
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 539 of 1
Current timestep = 540. State = [[-0.09991836  0.05220853]]. Action = [[ 0.22938472  0.24070376  0.01151973 -0.63827664]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 540 is [True, False, False, False, True, False]
Current timestep = 541. State = [[-0.08850077  0.07293928]]. Action = [[-0.21115467  0.03681359  0.00726447 -0.502166  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 541 is [True, False, False, False, True, False]
Current timestep = 542. State = [[-0.09187166  0.08114388]]. Action = [[0.01973027 0.02116323 0.08582711 0.8910489 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 542 is [True, False, False, False, True, False]
Scene graph at timestep 542 is [True, False, False, False, True, False]
State prediction error at timestep 542 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 542 of 0
Current timestep = 543. State = [[-0.10167516  0.09000408]]. Action = [[-0.23734649  0.07444483  0.17799252 -0.03466833]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 543 is [True, False, False, False, True, False]
Scene graph at timestep 543 is [True, False, False, False, True, False]
State prediction error at timestep 543 is tensor(5.5598e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 543 of -1
Current timestep = 544. State = [[-0.11412227  0.09576924]]. Action = [[ 0.13766277 -0.03479174 -0.02374879  0.6901262 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 544 is [True, False, False, False, True, False]
Scene graph at timestep 544 is [True, False, False, False, True, False]
State prediction error at timestep 544 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 544 of -1
Current timestep = 545. State = [[-0.10960571  0.08018654]]. Action = [[ 0.02997115 -0.21724002 -0.08481161  0.8566979 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 545 is [True, False, False, False, True, False]
Current timestep = 546. State = [[-0.11332791  0.06063428]]. Action = [[-0.2243948  -0.09641545  0.09320939  0.4082011 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 546 is [True, False, False, False, True, False]
Current timestep = 547. State = [[-0.1140708   0.03970222]]. Action = [[ 0.19517434 -0.19701071 -0.22772585 -0.5421738 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 547 is [True, False, False, False, True, False]
Scene graph at timestep 547 is [True, False, False, False, True, False]
State prediction error at timestep 547 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 547 of 1
Current timestep = 548. State = [[-0.10306908  0.00992314]]. Action = [[ 0.12632453 -0.17098677  0.15144828 -0.83357215]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 548 is [True, False, False, False, True, False]
Current timestep = 549. State = [[-0.09842749 -0.00487123]]. Action = [[-0.0026509  -0.00661416  0.03867018  0.5218785 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 549 is [True, False, False, False, True, False]
Scene graph at timestep 549 is [True, False, False, False, True, False]
State prediction error at timestep 549 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 549 of 1
Current timestep = 550. State = [[-0.0980278  -0.00757462]]. Action = [[-0.12284614 -0.00348297 -0.14336213 -0.63784975]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 550 is [True, False, False, False, True, False]
Current timestep = 551. State = [[-0.09907275 -0.02332381]]. Action = [[ 0.02197266 -0.23520537  0.13865912  0.4570756 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 551 is [True, False, False, False, True, False]
Current timestep = 552. State = [[-0.10168633 -0.04154153]]. Action = [[-0.07291102 -0.04172146  0.17992234  0.7343757 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 552 is [True, False, False, False, True, False]
Scene graph at timestep 552 is [True, False, False, False, True, False]
State prediction error at timestep 552 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 552 of -1
Current timestep = 553. State = [[-0.11500493 -0.04337732]]. Action = [[-0.20297241  0.10485533 -0.23321699  0.44531822]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 553 is [True, False, False, False, True, False]
Current timestep = 554. State = [[-0.12138302 -0.05083537]]. Action = [[ 0.15632176 -0.21250616  0.05051935  0.30344558]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 554 is [True, False, False, False, True, False]
Current timestep = 555. State = [[-0.1159227  -0.05077982]]. Action = [[ 0.10018402  0.22502804 -0.04439086 -0.23239356]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 555 is [True, False, False, False, True, False]
Current timestep = 556. State = [[-0.11555655 -0.03438283]]. Action = [[-0.16279548  0.1048556   0.21705243 -0.85285324]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 556 is [True, False, False, False, True, False]
Scene graph at timestep 556 is [True, False, False, False, True, False]
State prediction error at timestep 556 is tensor(8.4194e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 556 of 0
Current timestep = 557. State = [[-0.11727726 -0.02895704]]. Action = [[ 0.09822798 -0.1176843  -0.04149726  0.54639626]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 557 is [True, False, False, False, True, False]
Current timestep = 558. State = [[-0.11657928 -0.0337516 ]]. Action = [[ 0.00580001 -0.01476032 -0.15107612  0.51374733]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 558 is [True, False, False, False, True, False]
Current timestep = 559. State = [[-0.10960572 -0.0395981 ]]. Action = [[ 0.16474867 -0.05426261  0.16420937 -0.86811996]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 559 is [True, False, False, False, True, False]
Scene graph at timestep 559 is [True, False, False, False, True, False]
State prediction error at timestep 559 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 559 of 1
Current timestep = 560. State = [[-0.09060302 -0.04965296]]. Action = [[ 0.21340048 -0.08452386 -0.1390935   0.81271195]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 560 is [True, False, False, False, True, False]
Scene graph at timestep 560 is [True, False, False, False, True, False]
State prediction error at timestep 560 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 560 of 1
Current timestep = 561. State = [[-0.07418337 -0.05289681]]. Action = [[-0.0631901   0.08781573 -0.15779857  0.12679052]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 561 is [True, False, False, False, True, False]
Scene graph at timestep 561 is [True, False, False, False, True, False]
State prediction error at timestep 561 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 561 of 0
Current timestep = 562. State = [[-0.06725619 -0.05298749]]. Action = [[ 0.19521755 -0.06665087 -0.00551905  0.17007256]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 562 is [True, False, False, False, True, False]
Current timestep = 563. State = [[-0.05049063 -0.05128597]]. Action = [[ 0.12216341  0.05040559  0.1230633  -0.00251323]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 563 is [True, False, False, False, True, False]
Scene graph at timestep 563 is [True, False, False, False, True, False]
State prediction error at timestep 563 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 563 of 1
Current timestep = 564. State = [[-0.2399764   0.10139631]]. Action = [[ 0.18747365  0.05900422 -0.1577837  -0.8454081 ]]. Reward = [100.]
Curr episode timestep = 35
Scene graph at timestep 564 is [True, False, False, False, True, False]
Current timestep = 565. State = [[-0.22544211  0.09979229]]. Action = [[ 0.1609644  -0.23202837 -0.04616231 -0.42032617]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 565 is [True, False, False, False, True, False]
Current timestep = 566. State = [[-0.20308599  0.09673046]]. Action = [[ 0.24362126  0.18496937  0.02993351 -0.15178072]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 566 is [True, False, False, False, True, False]
Scene graph at timestep 566 is [True, False, False, False, True, False]
State prediction error at timestep 566 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 566 of 1
Current timestep = 567. State = [[-0.16866435  0.12043475]]. Action = [[0.22130698 0.24760234 0.07481009 0.886606  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 567 is [True, False, False, False, True, False]
Current timestep = 568. State = [[-0.13638498  0.1404572 ]]. Action = [[ 0.22724861  0.0382272  -0.22740616 -0.63959575]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 568 is [True, False, False, False, True, False]
Current timestep = 569. State = [[-0.1216813   0.16049528]]. Action = [[-0.12289765  0.22013229  0.16144896  0.257226  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 569 is [True, False, False, False, False, True]
Current timestep = 570. State = [[-0.13327673  0.16862546]]. Action = [[-0.23653077 -0.17947288  0.01806861  0.95338726]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 570 is [True, False, False, False, False, True]
Current timestep = 571. State = [[-0.13463117  0.15393333]]. Action = [[ 0.11333877 -0.153514   -0.0782814  -0.8032738 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 571 is [True, False, False, False, False, True]
Scene graph at timestep 571 is [True, False, False, False, False, True]
State prediction error at timestep 571 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 571 of 0
Current timestep = 572. State = [[-0.12605554  0.14520507]]. Action = [[ 0.20995092  0.11608714 -0.21409753 -0.9475883 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 572 is [True, False, False, False, False, True]
Current timestep = 573. State = [[-0.12174286  0.14487147]]. Action = [[-0.24451332 -0.1301804  -0.14965147 -0.55229807]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 573 is [True, False, False, False, False, True]
Current timestep = 574. State = [[-0.12785074  0.1468396 ]]. Action = [[-0.0226423   0.10393983  0.02124113 -0.8328295 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 574 is [True, False, False, False, False, True]
Scene graph at timestep 574 is [True, False, False, False, False, True]
State prediction error at timestep 574 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 574 of 0
Current timestep = 575. State = [[-0.1340955   0.16309488]]. Action = [[ 0.09887761  0.19934374  0.04239437 -0.18061095]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 575 is [True, False, False, False, False, True]
Current timestep = 576. State = [[-0.13373165  0.17875603]]. Action = [[0.08765343 0.10143888 0.19508529 0.8354969 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 576 is [True, False, False, False, False, True]
Current timestep = 577. State = [[-0.12764801  0.19090807]]. Action = [[ 0.00717741  0.06314132 -0.0702782   0.2314694 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 577 is [True, False, False, False, False, True]
Scene graph at timestep 577 is [True, False, False, False, False, True]
State prediction error at timestep 577 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 577 of -1
Current timestep = 578. State = [[-0.12964968  0.19386803]]. Action = [[-0.19900273 -0.11243112 -0.19571976  0.53937817]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 578 is [True, False, False, False, False, True]
Scene graph at timestep 578 is [True, False, False, False, False, True]
State prediction error at timestep 578 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 578 of -1
Current timestep = 579. State = [[-0.13721298  0.19855644]]. Action = [[-0.01523983  0.12019584  0.1301463  -0.68586844]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 579 is [True, False, False, False, False, True]
Current timestep = 580. State = [[-0.13480848  0.19246362]]. Action = [[ 0.18747303 -0.1897623   0.2064186  -0.01605916]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 580 is [True, False, False, False, False, True]
Scene graph at timestep 580 is [True, False, False, False, False, True]
State prediction error at timestep 580 is tensor(8.6044e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 580 of 0
Current timestep = 581. State = [[-0.12818338  0.18818396]]. Action = [[ 0.04937932  0.18510479 -0.12598266 -0.98261917]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 581 is [True, False, False, False, False, True]
Current timestep = 582. State = [[-0.12991995  0.20737255]]. Action = [[-0.1663353   0.13615006  0.04342616 -0.6443105 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 582 is [True, False, False, False, False, True]
Current timestep = 583. State = [[-0.1333251   0.22530034]]. Action = [[ 0.1414257   0.10947597  0.2171635  -0.03281426]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 583 is [True, False, False, False, False, True]
Scene graph at timestep 583 is [True, False, False, False, False, True]
State prediction error at timestep 583 is tensor(7.3932e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 583 of -1
Current timestep = 584. State = [[-0.11925244  0.2334313 ]]. Action = [[ 0.1234279  -0.04234976  0.12031129  0.7272278 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 584 is [True, False, False, False, False, True]
Current timestep = 585. State = [[-0.1180559   0.24008311]]. Action = [[-0.20098166  0.07686904 -0.1837136   0.78191173]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 585 is [True, False, False, False, False, True]
Scene graph at timestep 585 is [True, False, False, False, False, True]
State prediction error at timestep 585 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 585 of -1
Current timestep = 586. State = [[-0.11607653  0.23295504]]. Action = [[ 0.20384294 -0.2146943   0.18487    -0.8817157 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 586 is [True, False, False, False, False, True]
Scene graph at timestep 586 is [True, False, False, False, False, True]
State prediction error at timestep 586 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 586 of 1
Current timestep = 587. State = [[-0.09784917  0.20527762]]. Action = [[ 0.19422662 -0.1787076  -0.22693373 -0.4064814 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 587 is [True, False, False, False, False, True]
Current timestep = 588. State = [[-0.07700495  0.17913908]]. Action = [[ 0.07475072 -0.22696888  0.13965338  0.8201475 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 588 is [True, False, False, False, False, True]
Scene graph at timestep 588 is [True, False, False, False, False, True]
State prediction error at timestep 588 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 588 of 1
Current timestep = 589. State = [[-0.06831986  0.17154646]]. Action = [[ 0.03565884  0.21262944  0.04975504 -0.439601  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 589 is [True, False, False, False, False, True]
Scene graph at timestep 589 is [True, False, False, False, False, True]
State prediction error at timestep 589 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 589 of 1
Current timestep = 590. State = [[-0.0653513  0.1942183]]. Action = [[-0.16561887  0.15895319 -0.2148723  -0.79616743]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 590 is [True, False, False, False, False, True]
Current timestep = 591. State = [[-0.06969359  0.20367618]]. Action = [[ 0.15388304 -0.0460588   0.15671897 -0.8389961 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 591 is [True, False, False, False, False, True]
Scene graph at timestep 591 is [True, False, False, False, False, True]
State prediction error at timestep 591 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 591 of -1
Current timestep = 592. State = [[-0.06080131  0.18898287]]. Action = [[ 0.16129225 -0.22535811  0.13377267 -0.8920349 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 592 is [True, False, False, False, False, True]
Scene graph at timestep 592 is [True, False, False, False, False, True]
State prediction error at timestep 592 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 592 of 1
Current timestep = 593. State = [[-0.03722542  0.16878027]]. Action = [[ 0.0171499  -0.07491735 -0.12736931 -0.00252438]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 593 is [True, False, False, False, False, True]
Current timestep = 594. State = [[-0.03038846  0.17033432]]. Action = [[0.17330593 0.14772505 0.01076105 0.9065702 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 594 is [False, True, False, False, False, True]
Scene graph at timestep 594 is [False, True, False, False, False, True]
State prediction error at timestep 594 is tensor(6.0741e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 594 of 0
Current timestep = 595. State = [[-0.0075301   0.17096685]]. Action = [[-0.01518877 -0.2242757  -0.14418022 -0.80128133]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 595 is [False, True, False, False, False, True]
Current timestep = 596. State = [[-0.0056788   0.15710263]]. Action = [[-0.12034124 -0.03700195 -0.18104802  0.81673896]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 596 is [False, True, False, False, False, True]
Current timestep = 597. State = [[-0.00854627  0.13860802]]. Action = [[-0.1177561  -0.22285831 -0.24162461 -0.25463337]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 597 is [False, True, False, False, False, True]
Scene graph at timestep 597 is [False, True, False, False, False, True]
State prediction error at timestep 597 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 597 of 1
Current timestep = 598. State = [[-0.02261208  0.13145064]]. Action = [[-0.17995039  0.21249086 -0.13438603 -0.3580206 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 598 is [False, True, False, False, False, True]
Scene graph at timestep 598 is [False, True, False, False, False, True]
State prediction error at timestep 598 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 598 of -1
Current timestep = 599. State = [[-0.03970042  0.14974315]]. Action = [[ 0.07865798  0.00519025  0.06162724 -0.5911115 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 599 is [False, True, False, False, False, True]
Current timestep = 600. State = [[-0.03446897  0.14097692]]. Action = [[ 0.14317596 -0.1414777  -0.13887683 -0.9223712 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 600 is [False, True, False, False, False, True]
Scene graph at timestep 600 is [False, True, False, False, False, True]
State prediction error at timestep 600 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 600 of 1
Current timestep = 601. State = [[-0.03118816  0.13336399]]. Action = [[-0.18635543  0.04909143 -0.03571466 -0.79337245]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 601 is [False, True, False, False, False, True]
Current timestep = 602. State = [[-0.04348358  0.12118457]]. Action = [[-2.4174929e-01 -2.4127269e-01  5.2720308e-04 -7.7086240e-01]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 602 is [False, True, False, False, False, True]
Scene graph at timestep 602 is [False, True, False, False, True, False]
State prediction error at timestep 602 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 602 of -1
Current timestep = 603. State = [[-0.06801689  0.10903936]]. Action = [[ 0.17517892  0.18215054 -0.05164243  0.15779138]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 603 is [False, True, False, False, True, False]
Current timestep = 604. State = [[-0.06067488  0.12481466]]. Action = [[ 0.20242155  0.14531404  0.18067366 -0.32575333]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 604 is [True, False, False, False, True, False]
Current timestep = 605. State = [[-0.04064273  0.13164383]]. Action = [[ 0.14567953 -0.09272432 -0.08346695  0.79393685]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 605 is [True, False, False, False, True, False]
Current timestep = 606. State = [[-0.02817775  0.13912629]]. Action = [[ 0.05567575  0.16551635 -0.02708767  0.41905558]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 606 is [False, True, False, False, False, True]
Scene graph at timestep 606 is [False, True, False, False, False, True]
State prediction error at timestep 606 is tensor(7.7871e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 606 of -1
Current timestep = 607. State = [[-0.01008136  0.1377558 ]]. Action = [[ 0.17770255 -0.23456839  0.175057   -0.08469582]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 607 is [False, True, False, False, False, True]
Current timestep = 608. State = [[-0.17181876 -0.09597155]]. Action = [[ 0.1545867  -0.17731573  0.08079964 -0.3257339 ]]. Reward = [100.]
Curr episode timestep = 43
Scene graph at timestep 608 is [False, True, False, False, False, True]
Scene graph at timestep 608 is [True, False, False, False, True, False]
State prediction error at timestep 608 is tensor(0.0376, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 608 of 1
Current timestep = 609. State = [[-0.15620323 -0.09718729]]. Action = [[0.01773608 0.19335765 0.12801188 0.9453182 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 609 is [True, False, False, False, True, False]
Scene graph at timestep 609 is [True, False, False, False, True, False]
State prediction error at timestep 609 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 609 of 1
Current timestep = 610. State = [[-0.15546004 -0.08368552]]. Action = [[ 1.7967820e-04  4.6480626e-02 -7.2822675e-02 -9.0102762e-01]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 610 is [True, False, False, False, True, False]
Current timestep = 611. State = [[-0.15577142 -0.07389652]]. Action = [[-0.0081163   0.12460771  0.11805677 -0.5848826 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 611 is [True, False, False, False, True, False]
Scene graph at timestep 611 is [True, False, False, False, True, False]
State prediction error at timestep 611 is tensor(6.7371e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 611 of 0
Current timestep = 612. State = [[-0.14995354 -0.06508604]]. Action = [[ 0.15725273 -0.08538634  0.10615286  0.8880298 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 612 is [True, False, False, False, True, False]
Current timestep = 613. State = [[-0.12873372 -0.05608106]]. Action = [[0.20383188 0.20576876 0.12224382 0.08079243]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 613 is [True, False, False, False, True, False]
Scene graph at timestep 613 is [True, False, False, False, True, False]
State prediction error at timestep 613 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 613 of 1
Current timestep = 614. State = [[-0.09368187 -0.04874734]]. Action = [[ 0.24841666 -0.10367161  0.05890477 -0.91306293]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 614 is [True, False, False, False, True, False]
Current timestep = 615. State = [[-0.07175556 -0.06577428]]. Action = [[ 0.00980794 -0.2040886   0.17532447 -0.85025615]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 615 is [True, False, False, False, True, False]
Scene graph at timestep 615 is [True, False, False, False, True, False]
State prediction error at timestep 615 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 615 of 1
Current timestep = 616. State = [[-0.05782354 -0.09421711]]. Action = [[ 0.21243405 -0.2144849   0.0976305   0.81795025]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 616 is [True, False, False, False, True, False]
Current timestep = 617. State = [[-0.25333083 -0.1001967 ]]. Action = [[0.18417674 0.21494472 0.14861363 0.41097367]]. Reward = [100.]
Curr episode timestep = 8
Scene graph at timestep 617 is [True, False, False, False, True, False]
Scene graph at timestep 617 is [True, False, False, False, True, False]
State prediction error at timestep 617 is tensor(0.0204, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 617 of 1
Current timestep = 618. State = [[-0.24580854 -0.12425274]]. Action = [[ 0.09456861 -0.1988686   0.00544429  0.22264111]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 618 is [True, False, False, False, True, False]
Current timestep = 619. State = [[-0.2411963  -0.14484446]]. Action = [[-0.10864505 -0.07101175 -0.12712318  0.93111145]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 619 is [True, False, False, False, True, False]
Scene graph at timestep 619 is [True, False, False, True, False, False]
State prediction error at timestep 619 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 619 of -1
Current timestep = 620. State = [[-0.24537396 -0.15337828]]. Action = [[ 0.04902554  0.04071686 -0.23895665  0.3726591 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 620 is [True, False, False, True, False, False]
Scene graph at timestep 620 is [True, False, False, True, False, False]
State prediction error at timestep 620 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 620 of 0
Current timestep = 621. State = [[-0.24330652 -0.1564127 ]]. Action = [[ 0.02982277 -0.10198918 -0.13640192 -0.59990805]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 621 is [True, False, False, True, False, False]
Scene graph at timestep 621 is [True, False, False, True, False, False]
State prediction error at timestep 621 is tensor(5.6317e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 621 of 0
Current timestep = 622. State = [[-0.2388389  -0.17036521]]. Action = [[ 0.03340349 -0.1563093   0.22083288  0.13903463]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 622 is [True, False, False, True, False, False]
Scene graph at timestep 622 is [True, False, False, True, False, False]
State prediction error at timestep 622 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of -1
Current timestep = 623. State = [[-0.23746929 -0.18327703]]. Action = [[-0.03706367  0.04217067  0.19125974  0.08094192]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 623 is [True, False, False, True, False, False]
Scene graph at timestep 623 is [True, False, False, True, False, False]
State prediction error at timestep 623 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 623 of -1
Current timestep = 624. State = [[-0.22761784 -0.19306087]]. Action = [[ 0.2420134  -0.19772312 -0.10404983  0.31662226]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 624 is [True, False, False, True, False, False]
Current timestep = 625. State = [[-0.2023212  -0.19324914]]. Action = [[0.14676908 0.2217578  0.04473305 0.7770686 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 625 is [True, False, False, True, False, False]
Current timestep = 626. State = [[-0.18444155 -0.17673513]]. Action = [[ 0.11395624  0.1129449   0.15019965 -0.1759671 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 626 is [True, False, False, True, False, False]
Scene graph at timestep 626 is [True, False, False, True, False, False]
State prediction error at timestep 626 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 626 of 1
Current timestep = 627. State = [[-0.17738529 -0.16285428]]. Action = [[-0.18851157  0.06057763  0.00791147  0.8477907 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 627 is [True, False, False, True, False, False]
Scene graph at timestep 627 is [True, False, False, True, False, False]
State prediction error at timestep 627 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 627 of -1
Current timestep = 628. State = [[-0.18831559 -0.16508125]]. Action = [[-0.17235187 -0.06633367 -0.11132026  0.30176377]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 628 is [True, False, False, True, False, False]
Scene graph at timestep 628 is [True, False, False, True, False, False]
State prediction error at timestep 628 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 628 of -1
Current timestep = 629. State = [[-0.2008992  -0.15836325]]. Action = [[ 0.10770851  0.17581362 -0.01516917 -0.2600578 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 629 is [True, False, False, True, False, False]
Current timestep = 630. State = [[-0.20190261 -0.15514928]]. Action = [[-0.01064438 -0.18707107 -0.21016896  0.97543144]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 630 is [True, False, False, True, False, False]
Current timestep = 631. State = [[-0.20409173 -0.16267315]]. Action = [[-1.9740433e-02  4.6479702e-04  1.5595758e-01  8.8199079e-01]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 631 is [True, False, False, True, False, False]
Current timestep = 632. State = [[-0.20885639 -0.16203085]]. Action = [[-0.15612975  0.07471639 -0.11806464  0.4584651 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 632 is [True, False, False, True, False, False]
Scene graph at timestep 632 is [True, False, False, True, False, False]
State prediction error at timestep 632 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 632 of -1
Current timestep = 633. State = [[-0.21397778 -0.15880184]]. Action = [[ 0.05193704  0.02474105 -0.10329227  0.4569577 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 633 is [True, False, False, True, False, False]
Scene graph at timestep 633 is [True, False, False, True, False, False]
State prediction error at timestep 633 is tensor(9.7839e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 633 of -1
Current timestep = 634. State = [[-0.21537556 -0.15098114]]. Action = [[-0.08353101  0.09142125  0.10728589 -0.11743098]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 634 is [True, False, False, True, False, False]
Scene graph at timestep 634 is [True, False, False, True, False, False]
State prediction error at timestep 634 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of -1
Current timestep = 635. State = [[-0.21262652 -0.13047308]]. Action = [[ 0.23276758  0.21412027  0.11257613 -0.81434023]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 635 is [True, False, False, True, False, False]
Scene graph at timestep 635 is [True, False, False, True, False, False]
State prediction error at timestep 635 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 635 of 1
Current timestep = 636. State = [[-0.19526388 -0.11576755]]. Action = [[ 0.16224393 -0.12357506 -0.22962014 -0.83108175]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 636 is [True, False, False, True, False, False]
Scene graph at timestep 636 is [True, False, False, False, True, False]
State prediction error at timestep 636 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 636 of 1
Current timestep = 637. State = [[-0.18243863 -0.11696663]]. Action = [[-0.0889563   0.09500253  0.00303406  0.8283802 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 637 is [True, False, False, False, True, False]
Scene graph at timestep 637 is [True, False, False, False, True, False]
State prediction error at timestep 637 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 637 of 1
Current timestep = 638. State = [[-0.17774661 -0.10379375]]. Action = [[0.1938808  0.17369318 0.03685677 0.75711465]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 638 is [True, False, False, False, True, False]
Current timestep = 639. State = [[-0.16507933 -0.07776336]]. Action = [[ 0.15626627  0.21492702  0.09429473 -0.93023765]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 639 is [True, False, False, False, True, False]
Scene graph at timestep 639 is [True, False, False, False, True, False]
State prediction error at timestep 639 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 639 of 1
Current timestep = 640. State = [[-0.14816582 -0.06817668]]. Action = [[-0.14218436 -0.18560588  0.187967    0.54130363]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 640 is [True, False, False, False, True, False]
Current timestep = 641. State = [[-0.15779135 -0.0697315 ]]. Action = [[-0.15152083  0.15412566 -0.18322484 -0.12339014]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 641 is [True, False, False, False, True, False]
Current timestep = 642. State = [[-0.16047354 -0.04908204]]. Action = [[ 0.23511672  0.23075834  0.2378034  -0.9078111 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 642 is [True, False, False, False, True, False]
Current timestep = 643. State = [[-0.1455114  -0.03799143]]. Action = [[ 0.213216   -0.12270629 -0.10016555  0.8917842 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 643 is [True, False, False, False, True, False]
Scene graph at timestep 643 is [True, False, False, False, True, False]
State prediction error at timestep 643 is tensor(5.3085e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 643 of 1
Current timestep = 644. State = [[-0.11315368 -0.03936386]]. Action = [[ 0.17821714  0.00173509 -0.08883169  0.96119666]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 644 is [True, False, False, False, True, False]
Current timestep = 645. State = [[-0.10127167 -0.04541923]]. Action = [[-0.07316586 -0.09411308 -0.06537527  0.9057307 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 645 is [True, False, False, False, True, False]
Scene graph at timestep 645 is [True, False, False, False, True, False]
State prediction error at timestep 645 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 645 of 1
Current timestep = 646. State = [[-0.10796465 -0.05161466]]. Action = [[-0.21052852  0.05090594  0.18913406  0.9007821 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 646 is [True, False, False, False, True, False]
Current timestep = 647. State = [[-0.11398113 -0.04726673]]. Action = [[-0.02656712  0.05216163 -0.1092464  -0.28345817]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 647 is [True, False, False, False, True, False]
Current timestep = 648. State = [[-0.12882183 -0.0350448 ]]. Action = [[-0.2371764   0.12014943  0.18719211  0.14940143]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 648 is [True, False, False, False, True, False]
Scene graph at timestep 648 is [True, False, False, False, True, False]
State prediction error at timestep 648 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 648 of -1
Current timestep = 649. State = [[-0.15154971 -0.01072007]]. Action = [[ 0.05410868  0.22912937  0.18014383 -0.92246485]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 649 is [True, False, False, False, True, False]
Scene graph at timestep 649 is [True, False, False, False, True, False]
State prediction error at timestep 649 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 649 of 0
Current timestep = 650. State = [[-0.14881721 -0.00156952]]. Action = [[ 0.16022387 -0.18147251  0.08573046 -0.25875986]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 650 is [True, False, False, False, True, False]
Scene graph at timestep 650 is [True, False, False, False, True, False]
State prediction error at timestep 650 is tensor(1.4143e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 650 of 1
Current timestep = 651. State = [[-0.1344391  -0.00827208]]. Action = [[ 0.17136025  0.10280573  0.22911358 -0.8698681 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 651 is [True, False, False, False, True, False]
Scene graph at timestep 651 is [True, False, False, False, True, False]
State prediction error at timestep 651 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 651 of 1
Current timestep = 652. State = [[-1.24792516e-01  5.10834070e-05]]. Action = [[-0.2097286   0.04971316  0.06453863  0.8592725 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 652 is [True, False, False, False, True, False]
Current timestep = 653. State = [[-0.12315135 -0.00898903]]. Action = [[ 0.217484  -0.2422798  0.0770224  0.9811121]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 653 is [True, False, False, False, True, False]
Scene graph at timestep 653 is [True, False, False, False, True, False]
State prediction error at timestep 653 is tensor(7.8616e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 653 of 0
Current timestep = 654. State = [[-0.11304237 -0.01772415]]. Action = [[ 0.12505287  0.14744627 -0.17410126  0.66902304]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 654 is [True, False, False, False, True, False]
Current timestep = 655. State = [[-0.09269101 -0.00184284]]. Action = [[0.19384831 0.15671435 0.10859883 0.12817073]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 655 is [True, False, False, False, True, False]
Scene graph at timestep 655 is [True, False, False, False, True, False]
State prediction error at timestep 655 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 655 of 1
Current timestep = 656. State = [[-0.0718347   0.00301317]]. Action = [[-0.12970346 -0.17001845 -0.14795491  0.07401669]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 656 is [True, False, False, False, True, False]
Scene graph at timestep 656 is [True, False, False, False, True, False]
State prediction error at timestep 656 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 656 of -1
Current timestep = 657. State = [[-0.07077404 -0.01500291]]. Action = [[ 0.1304706  -0.14695275 -0.1471515  -0.03269362]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 657 is [True, False, False, False, True, False]
Current timestep = 658. State = [[-0.06147722 -0.0187231 ]]. Action = [[ 0.16302049  0.11909866  0.1648767  -0.40234786]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 658 is [True, False, False, False, True, False]
Scene graph at timestep 658 is [True, False, False, False, True, False]
State prediction error at timestep 658 is tensor(1.6895e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 658 of 1
Current timestep = 659. State = [[-0.04818939 -0.02243796]]. Action = [[-0.17494285 -0.16101584 -0.04083642 -0.26822257]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 659 is [True, False, False, False, True, False]
Scene graph at timestep 659 is [False, True, False, False, True, False]
State prediction error at timestep 659 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 659 of 0
Current timestep = 660. State = [[-0.04735927 -0.04352709]]. Action = [[ 0.18528986 -0.16813484  0.2144993   0.43701768]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 660 is [False, True, False, False, True, False]
Current timestep = 661. State = [[-0.04615198 -0.04597647]]. Action = [[-0.1283365   0.17216188 -0.15090746  0.53804016]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 661 is [False, True, False, False, True, False]
Scene graph at timestep 661 is [False, True, False, False, True, False]
State prediction error at timestep 661 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 661 of 1
Current timestep = 662. State = [[-0.04152436 -0.04536493]]. Action = [[ 0.24388525 -0.13227457  0.04140547  0.6527984 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 662 is [False, True, False, False, True, False]
Current timestep = 663. State = [[-0.22893858 -0.14802152]]. Action = [[ 0.07671982  0.12513793 -0.05787665  0.79859424]]. Reward = [100.]
Curr episode timestep = 45
Scene graph at timestep 663 is [False, True, False, False, True, False]
Scene graph at timestep 663 is [True, False, False, True, False, False]
State prediction error at timestep 663 is tensor(0.0267, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 663 of -1
Current timestep = 664. State = [[-0.22094446 -0.16453595]]. Action = [[ 0.09720576  0.01062182  0.15551281 -0.8579818 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 664 is [True, False, False, True, False, False]
Current timestep = 665. State = [[-0.20254225 -0.15987137]]. Action = [[ 0.2398498   0.10087782 -0.16421644  0.9358257 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 665 is [True, False, False, True, False, False]
Current timestep = 666. State = [[-0.18577515 -0.1652555 ]]. Action = [[-0.10795668 -0.1613364  -0.11102812  0.2420063 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 666 is [True, False, False, True, False, False]
Current timestep = 667. State = [[-0.17848592 -0.16367511]]. Action = [[0.19176042 0.14210999 0.0717797  0.373554  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 667 is [True, False, False, True, False, False]
Scene graph at timestep 667 is [True, False, False, True, False, False]
State prediction error at timestep 667 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 667 of 1
Current timestep = 668. State = [[-0.15380917 -0.16582257]]. Action = [[ 0.20988181 -0.13267832  0.07740992 -0.2980218 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 668 is [True, False, False, True, False, False]
Current timestep = 669. State = [[-0.12607667 -0.1687824 ]]. Action = [[ 0.2326703   0.06328696 -0.02632123 -0.9121143 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 669 is [True, False, False, True, False, False]
Scene graph at timestep 669 is [True, False, False, True, False, False]
State prediction error at timestep 669 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 669 of 1
Current timestep = 670. State = [[-0.0905189  -0.16092107]]. Action = [[0.17577314 0.10413963 0.01449534 0.17469192]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 670 is [True, False, False, True, False, False]
Scene graph at timestep 670 is [True, False, False, True, False, False]
State prediction error at timestep 670 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 670 of 1
Current timestep = 671. State = [[-0.0657482  -0.14046212]]. Action = [[ 0.13188577  0.23394388 -0.22812065  0.38589215]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 671 is [True, False, False, True, False, False]
Current timestep = 672. State = [[-0.04933306 -0.1290263 ]]. Action = [[ 0.19083998 -0.10420895  0.15803331 -0.06125599]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 672 is [True, False, False, True, False, False]
Current timestep = 673. State = [[-0.03120227 -0.14025667]]. Action = [[-0.0211423  -0.1280344   0.12607402 -0.931487  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 673 is [False, True, False, True, False, False]
Scene graph at timestep 673 is [False, True, False, True, False, False]
State prediction error at timestep 673 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 673 of 1
Current timestep = 674. State = [[-0.02130975 -0.14923851]]. Action = [[ 0.1651662  -0.03369814  0.15917051 -0.5630131 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 674 is [False, True, False, True, False, False]
Current timestep = 675. State = [[-0.00024557 -0.1414679 ]]. Action = [[ 0.15462226  0.19548374 -0.05799463  0.42289186]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 675 is [False, True, False, True, False, False]
Current timestep = 676. State = [[ 0.02444186 -0.13669819]]. Action = [[ 0.19333243 -0.09499279 -0.22625238  0.4699602 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 676 is [False, True, False, True, False, False]
Current timestep = 677. State = [[ 0.04513292 -0.14036794]]. Action = [[ 0.23986748  0.04519126 -0.1421926  -0.19801664]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 677 is [False, True, False, True, False, False]
Current timestep = 678. State = [[ 0.05150766 -0.14128284]]. Action = [[ 0.19642174 -0.05456658 -0.17132127  0.26226592]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 678 is [False, True, False, True, False, False]
Scene graph at timestep 678 is [False, False, True, True, False, False]
State prediction error at timestep 678 is tensor(4.4508e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 678 of -1
Current timestep = 679. State = [[ 0.05334985 -0.1401527 ]]. Action = [[ 0.07385409  0.02974916 -0.1013938  -0.91250956]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 679 is [False, False, True, True, False, False]
Scene graph at timestep 679 is [False, False, True, True, False, False]
State prediction error at timestep 679 is tensor(3.6129e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 679 of -1
Current timestep = 680. State = [[ 0.06122936 -0.1399386 ]]. Action = [[ 0.07682836  0.21549422 -0.22452044  0.15035737]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 680 is [False, False, True, True, False, False]
Current timestep = 681. State = [[ 0.06134558 -0.13992418]]. Action = [[ 0.03956416 -0.18492661 -0.17171526 -0.91203576]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 681 is [False, False, True, True, False, False]
Scene graph at timestep 681 is [False, False, True, True, False, False]
State prediction error at timestep 681 is tensor(2.1325e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 681 of -1
Current timestep = 682. State = [[ 0.06124339 -0.14243755]]. Action = [[ 0.00345269 -0.06537698  0.1408509  -0.97126305]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 682 is [False, False, True, True, False, False]
Scene graph at timestep 682 is [False, False, True, True, False, False]
State prediction error at timestep 682 is tensor(4.3404e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 682 of -1
Current timestep = 683. State = [[ 0.06120107 -0.14414434]]. Action = [[ 0.1244368  -0.12252024 -0.05104113  0.5623518 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 683 is [False, False, True, True, False, False]
Scene graph at timestep 683 is [False, False, True, True, False, False]
State prediction error at timestep 683 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 683 of -1
Current timestep = 684. State = [[ 0.06136301 -0.1441554 ]]. Action = [[ 0.22305155  0.20881158 -0.09440745  0.53329897]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 684 is [False, False, True, True, False, False]
Scene graph at timestep 684 is [False, False, True, True, False, False]
State prediction error at timestep 684 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 684 of -1
Current timestep = 685. State = [[ 0.06169573 -0.1379581 ]]. Action = [[ 0.01350787  0.12566563 -0.22750384 -0.4171207 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 685 is [False, False, True, True, False, False]
Scene graph at timestep 685 is [False, False, True, True, False, False]
State prediction error at timestep 685 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 685 of -1
Current timestep = 686. State = [[ 0.06227736 -0.12306988]]. Action = [[-0.1440886   0.1493628  -0.16439049 -0.50405115]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 686 is [False, False, True, True, False, False]
Current timestep = 687. State = [[ 0.06001782 -0.10051019]]. Action = [[-0.06202404  0.18059582 -0.22228932 -0.75860476]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 687 is [False, False, True, False, True, False]
Scene graph at timestep 687 is [False, False, True, False, True, False]
State prediction error at timestep 687 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 687 of 1
Current timestep = 688. State = [[ 0.05548643 -0.07005043]]. Action = [[0.03567678 0.23105389 0.20471755 0.12952685]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 688 is [False, False, True, False, True, False]
Scene graph at timestep 688 is [False, False, True, False, True, False]
State prediction error at timestep 688 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 688 of 1
Current timestep = 689. State = [[ 0.05243668 -0.05160959]]. Action = [[-0.0800944  -0.04243696  0.20304817 -0.96167094]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 689 is [False, False, True, False, True, False]
Current timestep = 690. State = [[ 0.05218767 -0.05347945]]. Action = [[ 0.1424391  -0.21013789  0.05037987  0.4934095 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 690 is [False, False, True, False, True, False]
Current timestep = 691. State = [[ 0.05212191 -0.05386831]]. Action = [[ 0.15441728  0.0982132  -0.07019304  0.19952655]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 691 is [False, False, True, False, True, False]
Scene graph at timestep 691 is [False, False, True, False, True, False]
State prediction error at timestep 691 is tensor(4.5093e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 691 of 0
Current timestep = 692. State = [[ 0.05212191 -0.05386831]]. Action = [[ 0.12072462  0.051568    0.09982944 -0.6767276 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 692 is [False, False, True, False, True, False]
Current timestep = 693. State = [[ 0.04601663 -0.06644569]]. Action = [[-0.18642914 -0.18466404  0.19250959 -0.8502267 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 693 is [False, False, True, False, True, False]
Current timestep = 694. State = [[-0.2412601  -0.06788894]]. Action = [[-0.13246535  0.05740151 -0.1629802   0.47876072]]. Reward = [100.]
Curr episode timestep = 30
Scene graph at timestep 694 is [False, True, False, False, True, False]
Current timestep = 695. State = [[-0.22689484 -0.06657854]]. Action = [[ 0.21336609  0.18875504 -0.21645196  0.2133956 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 695 is [True, False, False, False, True, False]
Current timestep = 696. State = [[-0.21794161 -0.04674197]]. Action = [[-0.21285437  0.19153345  0.02636328 -0.38764364]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 696 is [True, False, False, False, True, False]
Current timestep = 697. State = [[-0.21918297 -0.02264431]]. Action = [[0.17232582 0.14324802 0.16487539 0.30816925]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 697 is [True, False, False, False, True, False]
Current timestep = 698. State = [[-0.21867116 -0.02403842]]. Action = [[-0.20022373 -0.23152532  0.21487856 -0.06670725]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 698 is [True, False, False, False, True, False]
Scene graph at timestep 698 is [True, False, False, False, True, False]
State prediction error at timestep 698 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 698 of 1
Current timestep = 699. State = [[-0.22034445 -0.03614784]]. Action = [[ 0.21808118  0.03464872 -0.14186004  0.4643216 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 699 is [True, False, False, False, True, False]
Current timestep = 700. State = [[-0.20446697 -0.03801128]]. Action = [[ 0.13768792 -0.07331753  0.09053499  0.13072991]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 700 is [True, False, False, False, True, False]
Current timestep = 701. State = [[-0.1859089  -0.05441741]]. Action = [[ 0.10330698 -0.23295473 -0.16415247  0.33648348]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 701 is [True, False, False, False, True, False]
Current timestep = 702. State = [[-0.16665211 -0.05906431]]. Action = [[ 0.2425521   0.22331095 -0.02686808 -0.17836845]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 702 is [True, False, False, False, True, False]
Current timestep = 703. State = [[-0.13340229 -0.05044224]]. Action = [[ 0.23574668  0.00750715  0.15965617 -0.7761645 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 703 is [True, False, False, False, True, False]
Scene graph at timestep 703 is [True, False, False, False, True, False]
State prediction error at timestep 703 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 703 of 1
Current timestep = 704. State = [[-0.10855042 -0.03560137]]. Action = [[-0.20303157  0.18109724  0.16133273 -0.12368757]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 704 is [True, False, False, False, True, False]
Scene graph at timestep 704 is [True, False, False, False, True, False]
State prediction error at timestep 704 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 704 of -1
Current timestep = 705. State = [[-0.11332603 -0.01975256]]. Action = [[ 0.08554167  0.01672754  0.22692925 -0.40759915]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 705 is [True, False, False, False, True, False]
Scene graph at timestep 705 is [True, False, False, False, True, False]
State prediction error at timestep 705 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 705 of -1
Current timestep = 706. State = [[-0.10697034 -0.01277466]]. Action = [[0.18501055 0.09940714 0.08860809 0.5550535 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 706 is [True, False, False, False, True, False]
Current timestep = 707. State = [[-0.09064081 -0.00477055]]. Action = [[ 0.10649127  0.03086242  0.15640852 -0.37070775]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 707 is [True, False, False, False, True, False]
Current timestep = 708. State = [[-0.0700597  -0.00639761]]. Action = [[ 0.21983802 -0.10735765 -0.05984266  0.9271078 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 708 is [True, False, False, False, True, False]
Scene graph at timestep 708 is [True, False, False, False, True, False]
State prediction error at timestep 708 is tensor(6.8995e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 708 of 1
Current timestep = 709. State = [[-0.05238276 -0.00092307]]. Action = [[-0.16906774  0.14652017  0.06135502 -0.27242452]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 709 is [True, False, False, False, True, False]
Scene graph at timestep 709 is [True, False, False, False, True, False]
State prediction error at timestep 709 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 709 of 1
Current timestep = 710. State = [[-0.05078375  0.0092059 ]]. Action = [[ 0.2138797   0.03882241 -0.10060115 -0.6213081 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 710 is [True, False, False, False, True, False]
Current timestep = 711. State = [[-0.24534363  0.09347413]]. Action = [[ 0.18267524  0.20472431 -0.18434379  0.9761586 ]]. Reward = [100.]
Curr episode timestep = 16
Scene graph at timestep 711 is [True, False, False, False, True, False]
Current timestep = 712. State = [[-0.22962868  0.10645127]]. Action = [[ 0.23688811  0.05340567  0.09223959 -0.69187546]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 712 is [True, False, False, False, True, False]
Current timestep = 713. State = [[-0.1993005   0.11271868]]. Action = [[ 0.18754447 -0.00991967 -0.11563292 -0.31255913]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 713 is [True, False, False, False, True, False]
Current timestep = 714. State = [[-0.1725785   0.10461747]]. Action = [[ 0.14852756 -0.16877809 -0.18566976  0.34252214]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 714 is [True, False, False, False, True, False]
Scene graph at timestep 714 is [True, False, False, False, True, False]
State prediction error at timestep 714 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 714 of 1
Current timestep = 715. State = [[-0.14686476  0.08086155]]. Action = [[ 0.18049502 -0.20725536 -0.11892262  0.04498458]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 715 is [True, False, False, False, True, False]
Scene graph at timestep 715 is [True, False, False, False, True, False]
State prediction error at timestep 715 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 715 of 1
Current timestep = 716. State = [[-0.11886539  0.06484965]]. Action = [[ 0.22832537 -0.00110582  0.0330382  -0.10302144]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 716 is [True, False, False, False, True, False]
Scene graph at timestep 716 is [True, False, False, False, True, False]
State prediction error at timestep 716 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 716 of 1
Current timestep = 717. State = [[-0.09706809  0.06541513]]. Action = [[ 0.00980636  0.04620159  0.01488453 -0.0624724 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 717 is [True, False, False, False, True, False]
Scene graph at timestep 717 is [True, False, False, False, True, False]
State prediction error at timestep 717 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 717 of 1
Current timestep = 718. State = [[-0.08578349  0.05358587]]. Action = [[ 0.17971653 -0.22108355 -0.02298541 -0.31907338]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 718 is [True, False, False, False, True, False]
Scene graph at timestep 718 is [True, False, False, False, True, False]
State prediction error at timestep 718 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 718 of 1
Current timestep = 719. State = [[-0.06625071  0.02808915]]. Action = [[ 0.00728172 -0.19016668 -0.1847987  -0.5127478 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 719 is [True, False, False, False, True, False]
Scene graph at timestep 719 is [True, False, False, False, True, False]
State prediction error at timestep 719 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 719 of 1
Current timestep = 720. State = [[-0.06199151 -0.00112767]]. Action = [[ 0.04888597 -0.22660266 -0.02256984 -0.8880848 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 720 is [True, False, False, False, True, False]
Scene graph at timestep 720 is [True, False, False, False, True, False]
State prediction error at timestep 720 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 720 of 1
Current timestep = 721. State = [[-0.05010226 -0.0105048 ]]. Action = [[ 0.20261514  0.17481133 -0.12793383  0.72314847]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 721 is [True, False, False, False, True, False]
Current timestep = 722. State = [[-0.25248614  0.20367754]]. Action = [[ 0.02782589 -0.14547177 -0.03793773 -0.10697919]]. Reward = [100.]
Curr episode timestep = 10
Scene graph at timestep 722 is [True, False, False, False, True, False]
Current timestep = 723. State = [[-0.23714313  0.23050089]]. Action = [[ 0.18763477  0.05044615  0.07286564 -0.17504382]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 723 is [True, False, False, False, False, True]
Current timestep = 724. State = [[-0.22260238  0.22752668]]. Action = [[-0.0842869  -0.18170454 -0.0791339   0.47786093]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 724 is [True, False, False, False, False, True]
Current timestep = 725. State = [[-0.21787441  0.20612799]]. Action = [[ 0.02067134 -0.2045379   0.08272102 -0.905608  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 725 is [True, False, False, False, False, True]
Current timestep = 726. State = [[-0.207853   0.1836289]]. Action = [[ 0.2120888  -0.09696418 -0.20995854  0.6604942 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 726 is [True, False, False, False, False, True]
Current timestep = 727. State = [[-0.18987882  0.16576242]]. Action = [[ 0.20686835 -0.11117178 -0.09208998 -0.6686513 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 727 is [True, False, False, False, False, True]
Scene graph at timestep 727 is [True, False, False, False, False, True]
State prediction error at timestep 727 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 727 of 1
Current timestep = 728. State = [[-0.1659235   0.15778084]]. Action = [[0.08505023 0.01988566 0.2308189  0.70780087]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 728 is [True, False, False, False, False, True]
Scene graph at timestep 728 is [True, False, False, False, False, True]
State prediction error at timestep 728 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of 1
Current timestep = 729. State = [[-0.15775113  0.1707876 ]]. Action = [[-0.02360311  0.21881562  0.12285331  0.84727216]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 729 is [True, False, False, False, False, True]
Current timestep = 730. State = [[-0.15608205  0.1729298 ]]. Action = [[ 0.08855984 -0.14147834  0.15125698  0.6761856 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 730 is [True, False, False, False, False, True]
Scene graph at timestep 730 is [True, False, False, False, False, True]
State prediction error at timestep 730 is tensor(5.9095e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 730 of 0
Current timestep = 731. State = [[-0.14694251  0.17618342]]. Action = [[ 0.08978325  0.16426963  0.15846926 -0.5016181 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 731 is [True, False, False, False, False, True]
Current timestep = 732. State = [[-0.12465573  0.19283445]]. Action = [[ 0.22180003  0.14606231 -0.01414688 -0.9021485 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 732 is [True, False, False, False, False, True]
Current timestep = 733. State = [[-0.10016259  0.19241367]]. Action = [[ 0.0263491  -0.24561329 -0.2100749   0.7955226 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 733 is [True, False, False, False, False, True]
Scene graph at timestep 733 is [True, False, False, False, False, True]
State prediction error at timestep 733 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 733 of 1
Current timestep = 734. State = [[-0.092576    0.18159941]]. Action = [[ 0.09763572  0.0498696  -0.14251989 -0.9439621 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 734 is [True, False, False, False, False, True]
Current timestep = 735. State = [[-0.07457595  0.17771663]]. Action = [[ 0.20226136 -0.11064728 -0.19012001 -0.23951888]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 735 is [True, False, False, False, False, True]
Current timestep = 736. State = [[-0.04763224  0.16018297]]. Action = [[ 0.16840237 -0.18598704 -0.09148443 -0.72600853]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 736 is [True, False, False, False, False, True]
Scene graph at timestep 736 is [False, True, False, False, False, True]
State prediction error at timestep 736 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 736 of 1
Current timestep = 737. State = [[-0.01495031  0.13693233]]. Action = [[ 0.22658032 -0.13536642 -0.15781625  0.81079495]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 737 is [False, True, False, False, False, True]
Scene graph at timestep 737 is [False, True, False, False, False, True]
State prediction error at timestep 737 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 737 of 1
Current timestep = 738. State = [[0.00829486 0.12717195]]. Action = [[-0.00306743  0.02926058 -0.0583369  -0.115798  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 738 is [False, True, False, False, False, True]
Scene graph at timestep 738 is [False, True, False, False, False, True]
State prediction error at timestep 738 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 738 of 1
Current timestep = 739. State = [[0.01370743 0.13677604]]. Action = [[ 0.1683498   0.18548822  0.16319036 -0.8981067 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 739 is [False, True, False, False, False, True]
Scene graph at timestep 739 is [False, True, False, False, False, True]
State prediction error at timestep 739 is tensor(7.1896e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 739 of -1
Current timestep = 740. State = [[0.03895175 0.1655158 ]]. Action = [[ 0.06367433  0.23309126  0.18320361 -0.9205027 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 740 is [False, True, False, False, False, True]
Current timestep = 741. State = [[0.04956307 0.18613894]]. Action = [[0.13120538 0.08923998 0.02702829 0.30877066]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 741 is [False, True, False, False, False, True]
Current timestep = 742. State = [[0.06451381 0.18700399]]. Action = [[-0.03321382 -0.1928111   0.04246384  0.04196   ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 742 is [False, True, False, False, False, True]
Scene graph at timestep 742 is [False, False, True, False, False, True]
State prediction error at timestep 742 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 742 of -1
Current timestep = 743. State = [[0.0669726  0.17803732]]. Action = [[0.04018086 0.24369884 0.19074109 0.06725109]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 743 is [False, False, True, False, False, True]
Current timestep = 744. State = [[0.06705168 0.17762348]]. Action = [[ 0.03371787 -0.22357993  0.12076169  0.98043084]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 744 is [False, False, True, False, False, True]
Scene graph at timestep 744 is [False, False, True, False, False, True]
State prediction error at timestep 744 is tensor(6.6201e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 744 of -1
Current timestep = 745. State = [[0.06709718 0.17733344]]. Action = [[ 0.16428584  0.1687032   0.12235731 -0.7401061 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 745 is [False, False, True, False, False, True]
Current timestep = 746. State = [[0.06712674 0.17717816]]. Action = [[ 0.20342395  0.03294781  0.20302051 -0.89044726]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 746 is [False, False, True, False, False, True]
Current timestep = 747. State = [[0.06709871 0.17713112]]. Action = [[ 0.23207808 -0.11757421  0.12510157 -0.2878813 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 747 is [False, False, True, False, False, True]
Current timestep = 748. State = [[0.06918085 0.16960414]]. Action = [[-0.04993229 -0.14448875  0.06628671 -0.16656786]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 748 is [False, False, True, False, False, True]
Scene graph at timestep 748 is [False, False, True, False, False, True]
State prediction error at timestep 748 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 748 of 0
Current timestep = 749. State = [[0.06511485 0.17157613]]. Action = [[-0.2348234   0.17193499 -0.09016582 -0.01732868]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 749 is [False, False, True, False, False, True]
Scene graph at timestep 749 is [False, False, True, False, False, True]
State prediction error at timestep 749 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 749 of -1
Current timestep = 750. State = [[0.05637586 0.18749301]]. Action = [[ 0.22292596  0.20882696  0.04538763 -0.9645822 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 750 is [False, False, True, False, False, True]
Scene graph at timestep 750 is [False, False, True, False, False, True]
State prediction error at timestep 750 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 750 of -1
Current timestep = 751. State = [[0.05634403 0.18747142]]. Action = [[ 0.19048265 -0.2294691  -0.11646385  0.39575315]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 751 is [False, False, True, False, False, True]
Current timestep = 752. State = [[0.05157774 0.19198845]]. Action = [[-0.15895456  0.03498197 -0.00680475 -0.839492  ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 752 is [False, False, True, False, False, True]
Scene graph at timestep 752 is [False, False, True, False, False, True]
State prediction error at timestep 752 is tensor(2.1914e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 752 of -1
Current timestep = 753. State = [[0.03812819 0.1953422 ]]. Action = [[ 0.19474357 -0.16893703 -0.06124122 -0.59167033]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 753 is [False, False, True, False, False, True]
Current timestep = 754. State = [[0.03796325 0.19509925]]. Action = [[ 0.21052134  0.20120096  0.11522734 -0.34716463]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 754 is [False, True, False, False, False, True]
Current timestep = 755. State = [[0.03331203 0.20340346]]. Action = [[-0.10844687  0.12659186 -0.04923025 -0.7586981 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 755 is [False, True, False, False, False, True]
Current timestep = 756. State = [[0.02142294 0.22343807]]. Action = [[0.01895708 0.16049385 0.02723166 0.10597992]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 756 is [False, True, False, False, False, True]
Current timestep = 757. State = [[0.01692029 0.23640831]]. Action = [[ 0.13933808  0.09635821 -0.01511955  0.37109423]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 757 is [False, True, False, False, False, True]
Scene graph at timestep 757 is [False, True, False, False, False, True]
State prediction error at timestep 757 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 757 of -1
Current timestep = 758. State = [[0.01947104 0.24167392]]. Action = [[ 0.16984499  0.03001925 -0.08129123  0.43158782]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 758 is [False, True, False, False, False, True]
Scene graph at timestep 758 is [False, True, False, False, False, True]
State prediction error at timestep 758 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 758 of -1
Current timestep = 759. State = [[0.03307884 0.25199026]]. Action = [[ 0.11732107  0.11351833 -0.17081662  0.37924087]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 759 is [False, True, False, False, False, True]
Scene graph at timestep 759 is [False, True, False, False, False, True]
State prediction error at timestep 759 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 759 of -1
Current timestep = 760. State = [[0.04460977 0.26450247]]. Action = [[ 0.15501264 -0.0074442  -0.20472032 -0.54340965]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 760 is [False, True, False, False, False, True]
Current timestep = 761. State = [[0.04459222 0.26449504]]. Action = [[0.15754932 0.17443165 0.13720015 0.68511367]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 761 is [False, True, False, False, False, True]
Current timestep = 762. State = [[0.04459222 0.26449504]]. Action = [[ 0.23006123 -0.05133976  0.09022436 -0.9732759 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 762 is [False, True, False, False, False, True]
Current timestep = 763. State = [[0.04459222 0.26449504]]. Action = [[ 0.17476553 -0.17049745  0.19801241 -0.9611694 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 763 is [False, True, False, False, False, True]
Scene graph at timestep 763 is [False, True, False, False, False, True]
State prediction error at timestep 763 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 763 of -1
Current timestep = 764. State = [[0.04459222 0.26449504]]. Action = [[ 0.24721175 -0.23378395  0.2366195  -0.8313305 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 764 is [False, True, False, False, False, True]
Current timestep = 765. State = [[0.04459222 0.26449504]]. Action = [[ 0.21735793  0.24059886 -0.17943282 -0.4819157 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 765 is [False, True, False, False, False, True]
Current timestep = 766. State = [[0.04459222 0.26449504]]. Action = [[ 0.1341624   0.01517829 -0.0467457  -0.24396253]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 766 is [False, True, False, False, False, True]
Scene graph at timestep 766 is [False, True, False, False, False, True]
State prediction error at timestep 766 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 766 of -1
Current timestep = 767. State = [[0.04977005 0.2527487 ]]. Action = [[ 0.03366196 -0.23415731 -0.16090715  0.17068696]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 767 is [False, True, False, False, False, True]
Current timestep = 768. State = [[0.05725222 0.23723419]]. Action = [[ 0.13935393 -0.17089868 -0.02960601 -0.8482788 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 768 is [False, True, False, False, False, True]
Scene graph at timestep 768 is [False, False, True, False, False, True]
State prediction error at timestep 768 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 768 of 0
Current timestep = 769. State = [[0.05675217 0.23757003]]. Action = [[-0.14298947  0.03558666  0.00124511  0.7868018 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 769 is [False, False, True, False, False, True]
Current timestep = 770. State = [[0.05554279 0.23925313]]. Action = [[ 0.19570655  0.17750737 -0.07846436  0.5526247 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 770 is [False, False, True, False, False, True]
Current timestep = 771. State = [[0.0555029  0.23933138]]. Action = [[ 0.23596364 -0.06097806  0.21799618 -0.47496605]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 771 is [False, False, True, False, False, True]
Scene graph at timestep 771 is [False, False, True, False, False, True]
State prediction error at timestep 771 is tensor(2.9229e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 771 of -1
Current timestep = 772. State = [[0.0555029  0.23933138]]. Action = [[ 0.07080534 -0.10430911  0.03945619 -0.33411407]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 772 is [False, False, True, False, False, True]
Scene graph at timestep 772 is [False, False, True, False, False, True]
State prediction error at timestep 772 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 772 of -1
Current timestep = 773. State = [[0.05546261 0.23941037]]. Action = [[ 0.12354231  0.18910262 -0.21549658 -0.79960555]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 773 is [False, False, True, False, False, True]
Current timestep = 774. State = [[0.04850438 0.2525242 ]]. Action = [[-0.17929244  0.24383834  0.00994    -0.730664  ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 774 is [False, False, True, False, False, True]
Current timestep = 775. State = [[0.0295392 0.2860043]]. Action = [[-0.01019868  0.23509169 -0.14360206 -0.40784758]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 775 is [False, True, False, False, False, True]
Scene graph at timestep 775 is [False, True, False, False, False, True]
State prediction error at timestep 775 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 775 of -1
Current timestep = 776. State = [[0.02146359 0.2977458 ]]. Action = [[-0.00777215 -0.18695235  0.1169984  -0.36000735]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 776 is [False, True, False, False, False, True]
Current timestep = 777. State = [[0.02471057 0.2904259 ]]. Action = [[ 0.03600916 -0.02231237  0.05950117 -0.0913707 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 777 is [False, True, False, False, False, True]
Current timestep = 778. State = [[0.03162212 0.27764174]]. Action = [[ 0.20430508 -0.15590097  0.22104737 -0.10215801]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 778 is [False, True, False, False, False, True]
Scene graph at timestep 778 is [False, True, False, False, False, True]
State prediction error at timestep 778 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 778 of 1
Current timestep = 779. State = [[0.04375063 0.257367  ]]. Action = [[ 0.1988138   0.06710517 -0.07996768  0.9033601 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 779 is [False, True, False, False, False, True]
Scene graph at timestep 779 is [False, True, False, False, False, True]
State prediction error at timestep 779 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 779 of 1
Current timestep = 780. State = [[0.04379277 0.25728923]]. Action = [[ 0.17620575 -0.1834041  -0.06899783 -0.5167361 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 780 is [False, True, False, False, False, True]
Current timestep = 781. State = [[0.04573809 0.25318727]]. Action = [[-0.04217362 -0.09323055 -0.13901807 -0.6866837 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 781 is [False, True, False, False, False, True]
Scene graph at timestep 781 is [False, True, False, False, False, True]
State prediction error at timestep 781 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 781 of 1
Current timestep = 782. State = [[0.04822327 0.24276137]]. Action = [[-0.12237111 -0.09541389  0.21407712 -0.07827508]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 782 is [False, True, False, False, False, True]
Current timestep = 783. State = [[0.04849759 0.23823614]]. Action = [[ 0.14262965 -0.23793335 -0.24041006  0.94506073]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 783 is [False, True, False, False, False, True]
Current timestep = 784. State = [[0.04728041 0.22321308]]. Action = [[-0.04952934 -0.24814673 -0.08415702  0.07329476]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 784 is [False, True, False, False, False, True]
Scene graph at timestep 784 is [False, True, False, False, False, True]
State prediction error at timestep 784 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 784 of 1
Current timestep = 785. State = [[0.03958777 0.18783258]]. Action = [[-0.08905739 -0.23587824 -0.12411961 -0.6453701 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 785 is [False, True, False, False, False, True]
Current timestep = 786. State = [[0.0376785  0.15967616]]. Action = [[ 0.14052773 -0.13736084  0.0645819  -0.26642013]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 786 is [False, True, False, False, False, True]
Current timestep = 787. State = [[0.04219045 0.14789997]]. Action = [[0.12687576 0.02742776 0.0760079  0.8040054 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 787 is [False, True, False, False, False, True]
Scene graph at timestep 787 is [False, True, False, False, False, True]
State prediction error at timestep 787 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 787 of 1
Current timestep = 788. State = [[0.04771985 0.14334977]]. Action = [[ 0.11852187  0.01051775 -0.12924278  0.63269913]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 788 is [False, True, False, False, False, True]
Current timestep = 789. State = [[0.05093004 0.13929683]]. Action = [[-0.05691305 -0.05277507  0.144901    0.91745377]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 789 is [False, True, False, False, False, True]
Scene graph at timestep 789 is [False, False, True, False, False, True]
State prediction error at timestep 789 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 789 of 1
Current timestep = 790. State = [[0.0518191  0.13695325]]. Action = [[0.13050228 0.00305662 0.24255964 0.8777714 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 790 is [False, False, True, False, False, True]
Current timestep = 791. State = [[0.04704555 0.14930294]]. Action = [[-0.1960409   0.22229254  0.1859402   0.7928381 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 791 is [False, False, True, False, False, True]
Current timestep = 792. State = [[0.04144968 0.16248935]]. Action = [[0.09168231 0.01948377 0.176534   0.08313179]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 792 is [False, True, False, False, False, True]
Current timestep = 793. State = [[0.04093971 0.16817085]]. Action = [[ 0.06392562  0.0888845  -0.06841449  0.53405976]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 793 is [False, True, False, False, False, True]
Current timestep = 794. State = [[0.04569388 0.18530855]]. Action = [[ 0.13115227  0.21996102 -0.15325211 -0.7106417 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 794 is [False, True, False, False, False, True]
Scene graph at timestep 794 is [False, True, False, False, False, True]
State prediction error at timestep 794 is tensor(8.4245e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 794 of -1
Current timestep = 795. State = [[0.05615228 0.20711774]]. Action = [[ 0.21199936 -0.20624295 -0.1811916  -0.95431745]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 795 is [False, True, False, False, False, True]
Current timestep = 796. State = [[0.05615605 0.20707658]]. Action = [[ 0.16976413  0.13330576 -0.15102887 -0.6598347 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 796 is [False, False, True, False, False, True]
Scene graph at timestep 796 is [False, False, True, False, False, True]
State prediction error at timestep 796 is tensor(2.8887e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 796 of -1
Current timestep = 797. State = [[0.05836881 0.20216398]]. Action = [[ 0.00631565 -0.10587615 -0.08055416 -0.6736167 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 797 is [False, False, True, False, False, True]
Current timestep = 798. State = [[0.06243489 0.19030644]]. Action = [[ 0.00344676 -0.15725607  0.22223371  0.4545852 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 798 is [False, False, True, False, False, True]
Current timestep = 799. State = [[0.06628046 0.1806173 ]]. Action = [[ 0.22605664 -0.22734907 -0.22870572 -0.9140468 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 799 is [False, False, True, False, False, True]
Current timestep = 800. State = [[0.06528644 0.18244535]]. Action = [[-0.01695567  0.10186064 -0.05126449  0.19233716]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 800 is [False, False, True, False, False, True]
Scene graph at timestep 800 is [False, False, True, False, False, True]
State prediction error at timestep 800 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 800 of 1
Current timestep = 801. State = [[0.06453238 0.18419403]]. Action = [[ 0.15646803  0.08548597 -0.04522756  0.36570048]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 801 is [False, False, True, False, False, True]
Scene graph at timestep 801 is [False, False, True, False, False, True]
State prediction error at timestep 801 is tensor(6.8590e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 801 of 0
Current timestep = 802. State = [[0.06453238 0.18419403]]. Action = [[ 0.10920608 -0.08552748 -0.21135677 -0.02841139]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 802 is [False, False, True, False, False, True]
Current timestep = 803. State = [[0.0619967  0.18941998]]. Action = [[-0.13911311  0.03860712 -0.09857732  0.14324462]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 803 is [False, False, True, False, False, True]
Current timestep = 804. State = [[0.05770949 0.19820245]]. Action = [[-0.04545605  0.04118294 -0.22132133  0.8323438 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 804 is [False, False, True, False, False, True]
Scene graph at timestep 804 is [False, False, True, False, False, True]
State prediction error at timestep 804 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 804 of -1
Current timestep = 805. State = [[0.05131582 0.19740063]]. Action = [[-0.16579977 -0.10387138 -0.08306447  0.77946854]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 805 is [False, False, True, False, False, True]
Scene graph at timestep 805 is [False, False, True, False, False, True]
State prediction error at timestep 805 is tensor(9.3864e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 805 of 1
Current timestep = 806. State = [[0.03868405 0.17713328]]. Action = [[ 0.1035929  -0.24316613  0.14232886  0.02825832]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 806 is [False, False, True, False, False, True]
Current timestep = 807. State = [[0.04065426 0.16215457]]. Action = [[ 0.2346856   0.12532097 -0.21989128 -0.5913914 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 807 is [False, True, False, False, False, True]
Scene graph at timestep 807 is [False, True, False, False, False, True]
State prediction error at timestep 807 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 807 of 1
Current timestep = 808. State = [[0.04138965 0.15976119]]. Action = [[ 0.16763455  0.0931375   0.08273891 -0.06890339]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 808 is [False, True, False, False, False, True]
Current timestep = 809. State = [[0.03529701 0.16089076]]. Action = [[-0.23682454  0.020574   -0.13376527  0.06331182]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 809 is [False, True, False, False, False, True]
Current timestep = 810. State = [[0.02259146 0.15966296]]. Action = [[ 0.23150718 -0.01961185  0.18813449  0.95630646]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 810 is [False, True, False, False, False, True]
Scene graph at timestep 810 is [False, True, False, False, False, True]
State prediction error at timestep 810 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 810 of 1
Current timestep = 811. State = [[0.01884606 0.145238  ]]. Action = [[-0.10368392 -0.23773287 -0.16316438 -0.566337  ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 811 is [False, True, False, False, False, True]
Scene graph at timestep 811 is [False, True, False, False, False, True]
State prediction error at timestep 811 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 811 of 1
Current timestep = 812. State = [[0.00270647 0.1227449 ]]. Action = [[ 0.00194171  0.03529343 -0.06775197  0.4609275 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 812 is [False, True, False, False, False, True]
Scene graph at timestep 812 is [False, True, False, False, True, False]
State prediction error at timestep 812 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 812 of 1
Current timestep = 813. State = [[0.00380107 0.12582117]]. Action = [[ 0.1621623   0.05896741  0.08250493 -0.64284885]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 813 is [False, True, False, False, True, False]
Current timestep = 814. State = [[0.00381726 0.1408519 ]]. Action = [[-0.04617068  0.2423454  -0.23594251 -0.49715245]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 814 is [False, True, False, False, False, True]
Current timestep = 815. State = [[0.00513287 0.14651443]]. Action = [[ 2.36681938e-01 -2.22273216e-01  1.08003616e-04  1.82694554e-01]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 815 is [False, True, False, False, False, True]
Scene graph at timestep 815 is [False, True, False, False, False, True]
State prediction error at timestep 815 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 815 of 0
Current timestep = 816. State = [[0.02385335 0.14762093]]. Action = [[ 0.18097565  0.24682832  0.13334495 -0.18646675]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 816 is [False, True, False, False, False, True]
Current timestep = 817. State = [[0.03975764 0.17265473]]. Action = [[ 0.0399408   0.13202584 -0.0695041  -0.9732128 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 817 is [False, True, False, False, False, True]
Scene graph at timestep 817 is [False, True, False, False, False, True]
State prediction error at timestep 817 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 817 of -1
Current timestep = 818. State = [[0.04615837 0.18746853]]. Action = [[ 0.18075877  0.21652335 -0.1037695  -0.40626615]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 818 is [False, True, False, False, False, True]
Current timestep = 819. State = [[0.04898452 0.17485705]]. Action = [[ 0.00997645 -0.24825975  0.04094833  0.59208655]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 819 is [False, True, False, False, False, True]
Current timestep = 820. State = [[0.0552845  0.15830462]]. Action = [[ 0.2098094  -0.02359337  0.08903271  0.8706219 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 820 is [False, True, False, False, False, True]
Current timestep = 821. State = [[0.05347397 0.16644391]]. Action = [[-0.05520616  0.19979283 -0.18203875 -0.87314266]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 821 is [False, False, True, False, False, True]
Scene graph at timestep 821 is [False, False, True, False, False, True]
State prediction error at timestep 821 is tensor(1.2380e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 821 of -1
Current timestep = 822. State = [[0.05261264 0.16774175]]. Action = [[-0.00214532 -0.18432371  0.02268034 -0.8891283 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 822 is [False, False, True, False, False, True]
Current timestep = 823. State = [[0.05492214 0.15461308]]. Action = [[-0.06745288 -0.10793254 -0.1262476  -0.2974739 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 823 is [False, False, True, False, False, True]
Current timestep = 824. State = [[0.05042791 0.13609853]]. Action = [[-0.08001827 -0.16636026  0.22409499 -0.06322688]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 824 is [False, False, True, False, False, True]
Current timestep = 825. State = [[0.04311873 0.13629344]]. Action = [[-0.10642394  0.22797     0.08043435 -0.1137737 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 825 is [False, False, True, False, False, True]
Scene graph at timestep 825 is [False, True, False, False, False, True]
State prediction error at timestep 825 is tensor(2.6765e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 825 of -1
Current timestep = 826. State = [[0.03416091 0.14922461]]. Action = [[ 0.23158646 -0.04528578 -0.17247064  0.6253606 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 826 is [False, True, False, False, False, True]
Scene graph at timestep 826 is [False, True, False, False, False, True]
State prediction error at timestep 826 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 826 of -1
Current timestep = 827. State = [[0.03416091 0.14922461]]. Action = [[ 0.22992754 -0.22651409 -0.14984575  0.11340475]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 827 is [False, True, False, False, False, True]
Current timestep = 828. State = [[0.03766764 0.14151943]]. Action = [[ 0.15332991 -0.1424397  -0.0093164   0.4083439 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 828 is [False, True, False, False, False, True]
Current timestep = 829. State = [[0.04063472 0.13395   ]]. Action = [[ 0.17179099  0.08587098 -0.12010121 -0.9957808 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 829 is [False, True, False, False, False, True]
Current timestep = 830. State = [[0.04167191 0.13125822]]. Action = [[ 0.03943843 -0.01920579  0.20563775 -0.12380666]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 830 is [False, True, False, False, False, True]
Current timestep = 831. State = [[0.04381661 0.1278681 ]]. Action = [[ 0.07858872 -0.00147659  0.13104433  0.15297818]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 831 is [False, True, False, False, False, True]
Scene graph at timestep 831 is [False, True, False, False, False, True]
State prediction error at timestep 831 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 831 of 1
Current timestep = 832. State = [[0.04273095 0.11273287]]. Action = [[-0.15919028 -0.22513124  0.21887001 -0.651156  ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 832 is [False, True, False, False, False, True]
Current timestep = 833. State = [[0.04039191 0.0987587 ]]. Action = [[ 0.14143768 -0.11893843 -0.07753095  0.8895371 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 833 is [False, True, False, False, True, False]
Scene graph at timestep 833 is [False, True, False, False, True, False]
State prediction error at timestep 833 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 833 of 1
Current timestep = 834. State = [[-0.24408957 -0.09222524]]. Action = [[-0.18283738 -0.041788   -0.12069464  0.43280232]]. Reward = [100.]
Curr episode timestep = 111
Scene graph at timestep 834 is [False, True, False, False, True, False]
Current timestep = 835. State = [[-0.23633783 -0.10449491]]. Action = [[0.07720783 0.01565158 0.20413446 0.12459564]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 835 is [True, False, False, False, True, False]
Scene graph at timestep 835 is [True, False, False, False, True, False]
State prediction error at timestep 835 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 835 of 1
Current timestep = 836. State = [[-0.2241444  -0.10225448]]. Action = [[0.16035146 0.06044114 0.20873925 0.8679848 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 836 is [True, False, False, False, True, False]
Current timestep = 837. State = [[-0.21245523 -0.09965463]]. Action = [[-0.09800823 -0.00272483 -0.22443843 -0.9496879 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 837 is [True, False, False, False, True, False]
Scene graph at timestep 837 is [True, False, False, False, True, False]
State prediction error at timestep 837 is tensor(3.2199e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 837 of 1
Current timestep = 838. State = [[-0.20756656 -0.09052705]]. Action = [[ 0.1535542   0.15017688  0.24181598 -0.37178254]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 838 is [True, False, False, False, True, False]
Scene graph at timestep 838 is [True, False, False, False, True, False]
State prediction error at timestep 838 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 838 of 1
Current timestep = 839. State = [[-0.18868448 -0.0657407 ]]. Action = [[ 0.23841476  0.1995571   0.20617533 -0.6318521 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 839 is [True, False, False, False, True, False]
Current timestep = 840. State = [[-0.1595777  -0.04222727]]. Action = [[ 0.19422412  0.16677564  0.07554296 -0.07267684]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 840 is [True, False, False, False, True, False]
Scene graph at timestep 840 is [True, False, False, False, True, False]
State prediction error at timestep 840 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 840 of 1
Current timestep = 841. State = [[-0.13905768 -0.01683859]]. Action = [[-0.12994471  0.16849521  0.01809871 -0.40187562]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 841 is [True, False, False, False, True, False]
Current timestep = 842. State = [[-0.14577891  0.00677339]]. Action = [[-0.05845477  0.14460948 -0.03827755  0.14651883]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 842 is [True, False, False, False, True, False]
Current timestep = 843. State = [[-0.15014946  0.02127154]]. Action = [[0.07616496 0.04258159 0.12216759 0.51756656]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 843 is [True, False, False, False, True, False]
Current timestep = 844. State = [[-0.14226376  0.01686549]]. Action = [[ 0.17929178 -0.1530116  -0.21996559  0.22832441]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 844 is [True, False, False, False, True, False]
Current timestep = 845. State = [[-0.12983544  0.0106166 ]]. Action = [[-0.03864037  0.00277588 -0.11717752 -0.6063944 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 845 is [True, False, False, False, True, False]
Current timestep = 846. State = [[-0.13027656  0.00393491]]. Action = [[-0.06232187 -0.08776131  0.04039845  0.8109001 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 846 is [True, False, False, False, True, False]
Scene graph at timestep 846 is [True, False, False, False, True, False]
State prediction error at timestep 846 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 846 of 0
Current timestep = 847. State = [[-0.13359328  0.00708587]]. Action = [[0.00157392 0.21952742 0.12802374 0.6268699 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 847 is [True, False, False, False, True, False]
Scene graph at timestep 847 is [True, False, False, False, True, False]
State prediction error at timestep 847 is tensor(4.5394e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 847 of -1
Current timestep = 848. State = [[-0.13292058  0.01024798]]. Action = [[ 0.09679115 -0.21837695 -0.07761206 -0.49737465]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 848 is [True, False, False, False, True, False]
Current timestep = 849. State = [[-0.1212322  -0.00808003]]. Action = [[ 0.16239536 -0.14319849 -0.02220128 -0.9004204 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 849 is [True, False, False, False, True, False]
Current timestep = 850. State = [[-0.10820709 -0.03203397]]. Action = [[-0.06854913 -0.19549432 -0.16107416 -0.48423743]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 850 is [True, False, False, False, True, False]
Current timestep = 851. State = [[-0.10151171 -0.0356931 ]]. Action = [[0.20231551 0.20480207 0.24013147 0.8694694 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 851 is [True, False, False, False, True, False]
Scene graph at timestep 851 is [True, False, False, False, True, False]
State prediction error at timestep 851 is tensor(2.0828e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 851 of 1
Current timestep = 852. State = [[-0.08485695 -0.01589994]]. Action = [[-0.08114614  0.19160694  0.17251885 -0.6208256 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 852 is [True, False, False, False, True, False]
Scene graph at timestep 852 is [True, False, False, False, True, False]
State prediction error at timestep 852 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 852 of 1
Current timestep = 853. State = [[-0.0884037  -0.00561459]]. Action = [[-0.07288232 -0.12261161 -0.16548142  0.07249606]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 853 is [True, False, False, False, True, False]
Current timestep = 854. State = [[-0.08723576 -0.01177021]]. Action = [[ 0.14280388 -0.02451617 -0.19515486  0.66004395]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 854 is [True, False, False, False, True, False]
Current timestep = 855. State = [[-0.08002643 -0.00613616]]. Action = [[ 0.11619592  0.15502971 -0.11643372  0.8291069 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 855 is [True, False, False, False, True, False]
Current timestep = 856. State = [[-6.6757776e-02 -8.4433326e-05]]. Action = [[ 5.0422281e-02 -3.5673380e-05 -1.3085516e-01 -8.8763511e-01]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 856 is [True, False, False, False, True, False]
Current timestep = 857. State = [[-0.053866    0.00741654]]. Action = [[ 0.193376    0.08978018 -0.22246642 -0.35103703]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 857 is [True, False, False, False, True, False]
Scene graph at timestep 857 is [True, False, False, False, True, False]
State prediction error at timestep 857 is tensor(9.3770e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 857 of 1
Current timestep = 858. State = [[-0.18665542 -0.1699323 ]]. Action = [[ 0.00638133 -0.10703193 -0.23677474 -0.37527895]]. Reward = [100.]
Curr episode timestep = 23
Scene graph at timestep 858 is [True, False, False, False, True, False]
Scene graph at timestep 858 is [True, False, False, True, False, False]
State prediction error at timestep 858 is tensor(0.0221, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 858 of -1
Current timestep = 859. State = [[-0.16933946 -0.18919912]]. Action = [[ 0.18304229 -0.04867649  0.03182331  0.83235836]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 859 is [True, False, False, True, False, False]
Current timestep = 860. State = [[-0.1430072  -0.20439833]]. Action = [[ 0.21016768 -0.18333577 -0.05142392 -0.14177185]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 860 is [True, False, False, True, False, False]
Current timestep = 861. State = [[-0.11430459 -0.21285856]]. Action = [[ 0.15600574  0.13592649 -0.06349719  0.13369823]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 861 is [True, False, False, True, False, False]
Scene graph at timestep 861 is [True, False, False, True, False, False]
State prediction error at timestep 861 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 861 of 1
Current timestep = 862. State = [[-0.09977863 -0.19935761]]. Action = [[-0.15631485  0.23440343 -0.13723637 -0.66716594]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 862 is [True, False, False, True, False, False]
Current timestep = 863. State = [[-0.09609064 -0.19342084]]. Action = [[ 0.2102611  -0.21603817 -0.17611533 -0.10127842]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 863 is [True, False, False, True, False, False]
Current timestep = 864. State = [[-0.089302   -0.19317737]]. Action = [[ 0.00686583  0.14239359  0.22716063 -0.20660746]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 864 is [True, False, False, True, False, False]
Scene graph at timestep 864 is [True, False, False, True, False, False]
State prediction error at timestep 864 is tensor(7.9439e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 864 of 1
Current timestep = 865. State = [[-0.08691119 -0.18404296]]. Action = [[-0.04618968  0.07188147  0.03942883  0.00928056]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 865 is [True, False, False, True, False, False]
Current timestep = 866. State = [[-0.08667704 -0.17713955]]. Action = [[-0.0167692   0.06435379  0.08500203  0.22537625]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 866 is [True, False, False, True, False, False]
Scene graph at timestep 866 is [True, False, False, True, False, False]
State prediction error at timestep 866 is tensor(1.7202e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 866 of 1
Current timestep = 867. State = [[-0.09120948 -0.17964633]]. Action = [[-0.13939269 -0.13194169  0.02822176  0.5940677 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 867 is [True, False, False, True, False, False]
Scene graph at timestep 867 is [True, False, False, True, False, False]
State prediction error at timestep 867 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 867 of -1
Current timestep = 868. State = [[-0.09669077 -0.18577442]]. Action = [[ 0.03874737  0.05367339 -0.2217615  -0.10997105]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 868 is [True, False, False, True, False, False]
Scene graph at timestep 868 is [True, False, False, True, False, False]
State prediction error at timestep 868 is tensor(3.5424e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 868 of 0
Current timestep = 869. State = [[-0.08892218 -0.16853671]]. Action = [[ 0.21516794  0.23220754 -0.1551996  -0.6824058 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 869 is [True, False, False, True, False, False]
Current timestep = 870. State = [[-0.07064264 -0.13858467]]. Action = [[0.22981802 0.18788484 0.22125548 0.9884372 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 870 is [True, False, False, True, False, False]
Current timestep = 871. State = [[-0.04965364 -0.10999148]]. Action = [[ 0.02864432  0.2025938  -0.081543    0.31241918]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 871 is [True, False, False, True, False, False]
Current timestep = 872. State = [[-0.03896526 -0.08421578]]. Action = [[ 0.13624907  0.12515596 -0.18291324  0.67451286]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 872 is [False, True, False, False, True, False]
Scene graph at timestep 872 is [False, True, False, False, True, False]
State prediction error at timestep 872 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 872 of 1
Current timestep = 873. State = [[-0.21499658  0.16058066]]. Action = [[ 0.07040116 -0.13204451  0.18128747 -0.83998054]]. Reward = [100.]
Curr episode timestep = 14
Scene graph at timestep 873 is [False, True, False, False, True, False]
Scene graph at timestep 873 is [True, False, False, False, False, True]
State prediction error at timestep 873 is tensor(0.0439, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 873 of -1
Current timestep = 874. State = [[-0.20495878  0.19210063]]. Action = [[ 0.05871463  0.23914069 -0.17220706  0.591529  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 874 is [True, False, False, False, False, True]
Current timestep = 875. State = [[-0.18787025  0.19942431]]. Action = [[ 0.17647013 -0.1841119   0.11037743 -0.91419816]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 875 is [True, False, False, False, False, True]
Current timestep = 876. State = [[-0.17298964  0.18045902]]. Action = [[-0.04933903 -0.20942691 -0.22668786 -0.9080073 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 876 is [True, False, False, False, False, True]
Scene graph at timestep 876 is [True, False, False, False, False, True]
State prediction error at timestep 876 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 876 of 1
Current timestep = 877. State = [[-0.16004999  0.14959808]]. Action = [[ 0.22942662 -0.20531584  0.21495861  0.52823067]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 877 is [True, False, False, False, False, True]
Current timestep = 878. State = [[-0.14589775  0.12171203]]. Action = [[ 0.00579309 -0.24163525  0.1121749   0.53807783]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 878 is [True, False, False, False, False, True]
Scene graph at timestep 878 is [True, False, False, False, True, False]
State prediction error at timestep 878 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 878 of 1
Current timestep = 879. State = [[-0.14528862  0.09786882]]. Action = [[-0.20280886 -0.06129166  0.22281498 -0.8118217 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 879 is [True, False, False, False, True, False]
Scene graph at timestep 879 is [True, False, False, False, True, False]
State prediction error at timestep 879 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 879 of 0
Current timestep = 880. State = [[-0.1512883   0.08632226]]. Action = [[ 0.04075304 -0.08391404 -0.14863221  0.97267866]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 880 is [True, False, False, False, True, False]
Scene graph at timestep 880 is [True, False, False, False, True, False]
State prediction error at timestep 880 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 880 of 0
Current timestep = 881. State = [[-0.14740746  0.07759778]]. Action = [[ 0.15666798 -0.03776586  0.04789102 -0.696542  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 881 is [True, False, False, False, True, False]
Scene graph at timestep 881 is [True, False, False, False, True, False]
State prediction error at timestep 881 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 881 of 1
Current timestep = 882. State = [[-0.14164716  0.07760455]]. Action = [[ 0.04007202  0.10858133 -0.06024827  0.18536305]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 882 is [True, False, False, False, True, False]
Scene graph at timestep 882 is [True, False, False, False, True, False]
State prediction error at timestep 882 is tensor(3.3179e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 882 of 0
Current timestep = 883. State = [[-0.13118598  0.09665135]]. Action = [[ 0.19509149  0.22268036 -0.21204714  0.569196  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 883 is [True, False, False, False, True, False]
Current timestep = 884. State = [[-0.11200614  0.11708007]]. Action = [[ 0.0396862   0.10364857  0.2395494  -0.82130295]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 884 is [True, False, False, False, True, False]
Current timestep = 885. State = [[-0.09962105  0.13261825]]. Action = [[ 0.12505722  0.10171807 -0.03970332  0.48912072]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 885 is [True, False, False, False, True, False]
Current timestep = 886. State = [[-0.08008625  0.12967762]]. Action = [[ 0.16213804 -0.20217067 -0.17242442  0.95521593]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 886 is [True, False, False, False, False, True]
Scene graph at timestep 886 is [True, False, False, False, False, True]
State prediction error at timestep 886 is tensor(9.8423e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 886 of 1
Current timestep = 887. State = [[-0.05827941  0.1212539 ]]. Action = [[ 0.1693106   0.01993909 -0.15866846  0.45964396]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 887 is [True, False, False, False, False, True]
Scene graph at timestep 887 is [True, False, False, False, True, False]
State prediction error at timestep 887 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 887 of 1
Current timestep = 888. State = [[-0.25135702 -0.10858042]]. Action = [[ 0.09026942 -0.177386   -0.20049109  0.83400023]]. Reward = [100.]
Curr episode timestep = 14
Scene graph at timestep 888 is [True, False, False, False, True, False]
Scene graph at timestep 888 is [True, False, False, False, True, False]
State prediction error at timestep 888 is tensor(0.0481, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 888 of -1
Current timestep = 889. State = [[-0.23567761 -0.131738  ]]. Action = [[ 0.2446424  -0.20010242  0.19841033  0.4505267 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 889 is [True, False, False, False, True, False]
Current timestep = 890. State = [[-0.21381408 -0.14092611]]. Action = [[ 0.05493623  0.12734413  0.09093446 -0.3539338 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 890 is [True, False, False, True, False, False]
Current timestep = 891. State = [[-0.20996508 -0.12873207]]. Action = [[-0.13568498  0.19554138  0.21667644 -0.8211393 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 891 is [True, False, False, True, False, False]
Scene graph at timestep 891 is [True, False, False, True, False, False]
State prediction error at timestep 891 is tensor(7.7574e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 891 of 1
Current timestep = 892. State = [[-0.21638301 -0.11450323]]. Action = [[-0.11433625 -0.02797805 -0.18089452  0.43437123]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 892 is [True, False, False, True, False, False]
Scene graph at timestep 892 is [True, False, False, False, True, False]
State prediction error at timestep 892 is tensor(2.9847e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 892 of 0
Current timestep = 893. State = [[-0.22389486 -0.11968409]]. Action = [[ 0.09221193 -0.11022995  0.16356659 -0.15643829]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 893 is [True, False, False, False, True, False]
Current timestep = 894. State = [[-0.21503642 -0.11491665]]. Action = [[ 0.18468288  0.15889207 -0.07101592 -0.8341012 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 894 is [True, False, False, False, True, False]
Scene graph at timestep 894 is [True, False, False, False, True, False]
State prediction error at timestep 894 is tensor(9.2210e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 894 of 1
Current timestep = 895. State = [[-0.19123447 -0.11189383]]. Action = [[ 0.238311   -0.11702105 -0.22522686  0.60254705]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 895 is [True, False, False, False, True, False]
Scene graph at timestep 895 is [True, False, False, False, True, False]
State prediction error at timestep 895 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 895 of 1
Current timestep = 896. State = [[-0.17273259 -0.11926072]]. Action = [[-0.1418553  -0.01597911  0.05725169  0.36225033]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 896 is [True, False, False, False, True, False]
Scene graph at timestep 896 is [True, False, False, False, True, False]
State prediction error at timestep 896 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 896 of 0
Current timestep = 897. State = [[-0.17733875 -0.11980692]]. Action = [[-0.04326022  0.08682427  0.12664697  0.40596366]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 897 is [True, False, False, False, True, False]
Scene graph at timestep 897 is [True, False, False, False, True, False]
State prediction error at timestep 897 is tensor(9.5129e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 897 of 0
Current timestep = 898. State = [[-0.18584833 -0.10562794]]. Action = [[-0.12837213  0.16879854  0.17797759  0.50201106]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 898 is [True, False, False, False, True, False]
Current timestep = 899. State = [[-0.20165114 -0.07996295]]. Action = [[-0.21519391  0.22527528  0.02052733 -0.30864978]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 899 is [True, False, False, False, True, False]
Current timestep = 900. State = [[-0.21840255 -0.04726316]]. Action = [[ 0.10453212  0.19727603 -0.1926951  -0.9158737 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 900 is [True, False, False, False, True, False]
Current timestep = 901. State = [[-0.221332   -0.03637546]]. Action = [[-0.09598643 -0.08373637  0.17271492  0.22930741]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 901 is [True, False, False, False, True, False]
Scene graph at timestep 901 is [True, False, False, False, True, False]
State prediction error at timestep 901 is tensor(2.7258e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 901 of 0
Current timestep = 902. State = [[-0.22691976 -0.0484876 ]]. Action = [[-0.05726899 -0.13723081 -0.16131228  0.88391113]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 902 is [True, False, False, False, True, False]
Current timestep = 903. State = [[-0.22294278 -0.0438888 ]]. Action = [[ 0.23405468  0.20827815 -0.06778857  0.14778674]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 903 is [True, False, False, False, True, False]
Current timestep = 904. State = [[-0.20660748 -0.03196023]]. Action = [[ 0.21053556  0.00140369  0.13080281 -0.8126169 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 904 is [True, False, False, False, True, False]
Scene graph at timestep 904 is [True, False, False, False, True, False]
State prediction error at timestep 904 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 904 of 1
Current timestep = 905. State = [[-0.18751435 -0.04012104]]. Action = [[ 0.01929796 -0.18724996  0.12914178 -0.5198847 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 905 is [True, False, False, False, True, False]
Current timestep = 906. State = [[-0.18220104 -0.06143111]]. Action = [[ 0.07228443 -0.21408673 -0.10922918 -0.78386945]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 906 is [True, False, False, False, True, False]
Current timestep = 907. State = [[-0.18029515 -0.06925496]]. Action = [[-0.11407748  0.17238474 -0.21358982 -0.33568358]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 907 is [True, False, False, False, True, False]
Current timestep = 908. State = [[-0.1813144  -0.05012137]]. Action = [[0.06046072 0.22858053 0.1306712  0.00166285]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 908 is [True, False, False, False, True, False]
Scene graph at timestep 908 is [True, False, False, False, True, False]
State prediction error at timestep 908 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 908 of 1
Current timestep = 909. State = [[-0.1822202  -0.04210789]]. Action = [[-0.00982682 -0.20958278  0.17035049 -0.856227  ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 909 is [True, False, False, False, True, False]
Current timestep = 910. State = [[-0.17251453 -0.06088828]]. Action = [[ 0.20613393 -0.16024305  0.11619648  0.76071966]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 910 is [True, False, False, False, True, False]
Scene graph at timestep 910 is [True, False, False, False, True, False]
State prediction error at timestep 910 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 910 of 1
Current timestep = 911. State = [[-0.15626378 -0.07988188]]. Action = [[-0.0348313  -0.05896194 -0.23098184 -0.1554923 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 911 is [True, False, False, False, True, False]
Current timestep = 912. State = [[-0.16170818 -0.07256806]]. Action = [[-0.19671756  0.22279626  0.0873194  -0.81399083]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 912 is [True, False, False, False, True, False]
Scene graph at timestep 912 is [True, False, False, False, True, False]
State prediction error at timestep 912 is tensor(9.4781e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 912 of -1
Current timestep = 913. State = [[-0.16466752 -0.06556746]]. Action = [[ 0.19472101 -0.12702002 -0.21811637 -0.35555875]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 913 is [True, False, False, False, True, False]
Current timestep = 914. State = [[-0.1517316  -0.06068571]]. Action = [[0.18951124 0.17851949 0.21078956 0.71418476]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 914 is [True, False, False, False, True, False]
Current timestep = 915. State = [[-0.14122102 -0.04787046]]. Action = [[-0.15238753  0.09583434 -0.23490645 -0.9653627 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 915 is [True, False, False, False, True, False]
Scene graph at timestep 915 is [True, False, False, False, True, False]
State prediction error at timestep 915 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 915 of 1
Current timestep = 916. State = [[-0.14626409 -0.02704827]]. Action = [[-0.04998454  0.18459815 -0.12835236  0.06631827]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 916 is [True, False, False, False, True, False]
Current timestep = 917. State = [[-0.14170246 -0.00191958]]. Action = [[ 0.23814017  0.19518241 -0.17796063  0.7489369 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 917 is [True, False, False, False, True, False]
Current timestep = 918. State = [[-0.11885997  0.01835945]]. Action = [[ 0.24308938  0.09481287 -0.07532686  0.9103966 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 918 is [True, False, False, False, True, False]
Current timestep = 919. State = [[-0.09470477  0.0231764 ]]. Action = [[ 0.00951859 -0.1011117  -0.07168098  0.86257243]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 919 is [True, False, False, False, True, False]
Current timestep = 920. State = [[-0.09038992  0.00827772]]. Action = [[ 0.05988002 -0.2134918   0.04449737  0.63152957]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 920 is [True, False, False, False, True, False]
Current timestep = 921. State = [[-0.08638281 -0.00075232]]. Action = [[-0.09287336  0.07589668 -0.1656807  -0.8974183 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 921 is [True, False, False, False, True, False]
Current timestep = 922. State = [[-0.08268335 -0.01010258]]. Action = [[ 0.1365729  -0.16894187 -0.24100845 -0.97014916]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 922 is [True, False, False, False, True, False]
Current timestep = 923. State = [[-0.08197602 -0.01664426]]. Action = [[-0.19826622  0.06352127  0.14212531 -0.24142545]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 923 is [True, False, False, False, True, False]
Current timestep = 924. State = [[-0.08925183 -0.01251758]]. Action = [[-0.1385786   0.06277823  0.01347676 -0.62088764]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 924 is [True, False, False, False, True, False]
Current timestep = 925. State = [[-0.10238089  0.00244938]]. Action = [[-0.10931069  0.15590358 -0.23863077  0.8703356 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 925 is [True, False, False, False, True, False]
Scene graph at timestep 925 is [True, False, False, False, True, False]
State prediction error at timestep 925 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 925 of 1
Current timestep = 926. State = [[-0.10808291  0.00442945]]. Action = [[ 0.21574569 -0.19888896 -0.08111471 -0.8095867 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 926 is [True, False, False, False, True, False]
Scene graph at timestep 926 is [True, False, False, False, True, False]
State prediction error at timestep 926 is tensor(6.5585e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 926 of 1
Current timestep = 927. State = [[-0.10520944 -0.00396418]]. Action = [[-0.16866954  0.07493556  0.1923036   0.47821832]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 927 is [True, False, False, False, True, False]
Current timestep = 928. State = [[-0.10698146 -0.01156544]]. Action = [[ 0.00439927 -0.16156857 -0.15279992 -0.0043028 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 928 is [True, False, False, False, True, False]
Current timestep = 929. State = [[-0.1079516  -0.02118103]]. Action = [[ 0.063218   -0.01959279 -0.0975334   0.878868  ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 929 is [True, False, False, False, True, False]
Scene graph at timestep 929 is [True, False, False, False, True, False]
State prediction error at timestep 929 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 929 of -1
Current timestep = 930. State = [[-0.10554343 -0.03054794]]. Action = [[ 0.09036085 -0.11910798  0.16945392 -0.7880014 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 930 is [True, False, False, False, True, False]
Scene graph at timestep 930 is [True, False, False, False, True, False]
State prediction error at timestep 930 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 930 of 0
Current timestep = 931. State = [[-0.09775865 -0.0413553 ]]. Action = [[ 0.159318   -0.01747496 -0.1244626   0.2317394 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 931 is [True, False, False, False, True, False]
Current timestep = 932. State = [[-0.08214927 -0.04085623]]. Action = [[ 0.16331887  0.04199108 -0.09755874 -0.57136405]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 932 is [True, False, False, False, True, False]
Scene graph at timestep 932 is [True, False, False, False, True, False]
State prediction error at timestep 932 is tensor(5.5772e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 932 of 1
Current timestep = 933. State = [[-0.06454597 -0.02574155]]. Action = [[-0.00593035  0.22989616  0.13515353  0.21560204]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 933 is [True, False, False, False, True, False]
Scene graph at timestep 933 is [True, False, False, False, True, False]
State prediction error at timestep 933 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 933 of 0
Current timestep = 934. State = [[-0.06513157 -0.00184359]]. Action = [[-0.01592745  0.17222059 -0.22308215 -0.1564753 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 934 is [True, False, False, False, True, False]
Current timestep = 935. State = [[-0.06264811  0.00192912]]. Action = [[ 0.05507213 -0.16508639 -0.14359373  0.52990234]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 935 is [True, False, False, False, True, False]
Current timestep = 936. State = [[-0.0517935  -0.00821231]]. Action = [[ 0.23011106 -0.05956006  0.1881051  -0.27040774]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 936 is [True, False, False, False, True, False]
Current timestep = 937. State = [[-0.15557036  0.02701099]]. Action = [[ 0.21285433 -0.02384263  0.17315447  0.13004506]]. Reward = [100.]
Curr episode timestep = 48
Scene graph at timestep 937 is [True, False, False, False, True, False]
Current timestep = 938. State = [[-0.1272664   0.02985674]]. Action = [[ 0.2394076  -0.06635013  0.0617944  -0.05814064]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 938 is [True, False, False, False, True, False]
Current timestep = 939. State = [[-0.10022859  0.03478293]]. Action = [[ 0.17405498  0.1149615  -0.18771102  0.20390356]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 939 is [True, False, False, False, True, False]
Scene graph at timestep 939 is [True, False, False, False, True, False]
State prediction error at timestep 939 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 939 of 1
Current timestep = 940. State = [[-0.07092608  0.03298996]]. Action = [[ 0.18555248 -0.16155992  0.07642213 -0.31447542]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 940 is [True, False, False, False, True, False]
Current timestep = 941. State = [[-0.04713021  0.01417839]]. Action = [[ 0.16714853 -0.17100692 -0.05229951  0.7613822 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 941 is [True, False, False, False, True, False]
Scene graph at timestep 941 is [False, True, False, False, True, False]
State prediction error at timestep 941 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 941 of 1
Current timestep = 942. State = [[-0.17468692  0.04470043]]. Action = [[ 0.01518923 -0.06638718 -0.22411358 -0.7366175 ]]. Reward = [100.]
Curr episode timestep = 4
Scene graph at timestep 942 is [False, True, False, False, True, False]
Current timestep = 943. State = [[-0.16256893  0.0515001 ]]. Action = [[-0.21130733 -0.03754476 -0.17163181 -0.46764076]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 943 is [True, False, False, False, True, False]
Current timestep = 944. State = [[-0.16913374  0.05992989]]. Action = [[0.03428322 0.1298641  0.09621722 0.40975857]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 944 is [True, False, False, False, True, False]
Scene graph at timestep 944 is [True, False, False, False, True, False]
State prediction error at timestep 944 is tensor(9.7057e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 944 of -1
Current timestep = 945. State = [[-0.1720201   0.06964339]]. Action = [[ 0.06636587  0.03764552 -0.09898926 -0.03553706]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 945 is [True, False, False, False, True, False]
Scene graph at timestep 945 is [True, False, False, False, True, False]
State prediction error at timestep 945 is tensor(8.4721e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 945 of 0
Current timestep = 946. State = [[-0.17060675  0.06227608]]. Action = [[-0.04740298 -0.18913916  0.09185049  0.8686137 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 946 is [True, False, False, False, True, False]
Current timestep = 947. State = [[-0.1785294  0.0454709]]. Action = [[-0.22814608 -0.10962272  0.2142429  -0.47502637]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 947 is [True, False, False, False, True, False]
Scene graph at timestep 947 is [True, False, False, False, True, False]
State prediction error at timestep 947 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 947 of -1
Current timestep = 948. State = [[-0.198623    0.02347911]]. Action = [[-0.12886846 -0.16359513  0.09259835 -0.10205346]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 948 is [True, False, False, False, True, False]
Current timestep = 949. State = [[-0.20252684  0.00276326]]. Action = [[ 0.1905967  -0.1621045   0.23557034 -0.851641  ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 949 is [True, False, False, False, True, False]
Current timestep = 950. State = [[-0.19835909 -0.0203127 ]]. Action = [[ 0.01616517 -0.18734449 -0.17909831 -0.12398416]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 950 is [True, False, False, False, True, False]
Current timestep = 951. State = [[-0.20135668 -0.03226476]]. Action = [[-0.15673277  0.06116304 -0.06422186 -0.08318686]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 951 is [True, False, False, False, True, False]
Current timestep = 952. State = [[-0.21000858 -0.03460336]]. Action = [[-0.10637289 -0.03432998  0.23126099  0.6449816 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 952 is [True, False, False, False, True, False]
Current timestep = 953. State = [[-0.2220185  -0.04379952]]. Action = [[-0.12473002 -0.09436563 -0.14442585  0.38711977]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 953 is [True, False, False, False, True, False]
Current timestep = 954. State = [[-0.23150772 -0.06041154]]. Action = [[ 0.10650003 -0.15702203 -0.13071714  0.58316016]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 954 is [True, False, False, False, True, False]
Scene graph at timestep 954 is [True, False, False, False, True, False]
State prediction error at timestep 954 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 954 of -1
Current timestep = 955. State = [[-0.2248596  -0.07557348]]. Action = [[ 0.16249791 -0.05391763 -0.08265227  0.3617314 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 955 is [True, False, False, False, True, False]
Current timestep = 956. State = [[-0.22435391 -0.07655734]]. Action = [[-0.18749976  0.08452308  0.11498761  0.85971797]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 956 is [True, False, False, False, True, False]
Scene graph at timestep 956 is [True, False, False, False, True, False]
State prediction error at timestep 956 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 956 of -1
Current timestep = 957. State = [[-0.23555306 -0.06320163]]. Action = [[-0.12255993  0.21269149  0.02813834  0.9518337 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 957 is [True, False, False, False, True, False]
Current timestep = 958. State = [[-0.2362534 -0.051365 ]]. Action = [[ 0.19387734 -0.09267828 -0.01255739 -0.3154471 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 958 is [True, False, False, False, True, False]
Scene graph at timestep 958 is [True, False, False, False, True, False]
State prediction error at timestep 958 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 958 of 0
Current timestep = 959. State = [[-0.22331363 -0.05893649]]. Action = [[ 0.19966269 -0.12242627 -0.02201483  0.60537696]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 959 is [True, False, False, False, True, False]
Current timestep = 960. State = [[-0.20429076 -0.07936008]]. Action = [[ 0.12099507 -0.20932665  0.05694389 -0.3940971 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 960 is [True, False, False, False, True, False]
Current timestep = 961. State = [[-0.19386737 -0.08511104]]. Action = [[-0.02725159  0.20717189 -0.12962751  0.05312419]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 961 is [True, False, False, False, True, False]
Current timestep = 962. State = [[-0.1858626  -0.06678596]]. Action = [[ 0.1614222   0.16935343 -0.14762187 -0.6255714 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 962 is [True, False, False, False, True, False]
Current timestep = 963. State = [[-0.16335414 -0.05517374]]. Action = [[ 0.22679788 -0.02968436  0.20979738 -0.92502433]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 963 is [True, False, False, False, True, False]
Current timestep = 964. State = [[-0.14949413 -0.066422  ]]. Action = [[-0.16140041 -0.20079783 -0.08126697  0.4954449 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 964 is [True, False, False, False, True, False]
Scene graph at timestep 964 is [True, False, False, False, True, False]
State prediction error at timestep 964 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 964 of 1
Current timestep = 965. State = [[-0.14848506 -0.07397191]]. Action = [[ 0.16350001  0.09956533 -0.19933212  0.64221764]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 965 is [True, False, False, False, True, False]
Scene graph at timestep 965 is [True, False, False, False, True, False]
State prediction error at timestep 965 is tensor(7.5043e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 965 of 1
Current timestep = 966. State = [[-0.13856107 -0.05477697]]. Action = [[ 0.10892251  0.24032289  0.00798512 -0.38366425]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 966 is [True, False, False, False, True, False]
Scene graph at timestep 966 is [True, False, False, False, True, False]
State prediction error at timestep 966 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 966 of 1
Current timestep = 967. State = [[-0.11855142 -0.04266068]]. Action = [[ 0.13949883 -0.14695087 -0.19936703  0.09213316]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 967 is [True, False, False, False, True, False]
Current timestep = 968. State = [[-0.0977314  -0.04652132]]. Action = [[ 0.18545836  0.03262612 -0.21494359  0.5256643 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 968 is [True, False, False, False, True, False]
Current timestep = 969. State = [[-0.0777384  -0.05645026]]. Action = [[ 0.05253994 -0.16664402 -0.03484637 -0.768643  ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 969 is [True, False, False, False, True, False]
Current timestep = 970. State = [[-0.06648886 -0.0687146 ]]. Action = [[ 0.0941647  -0.03265542  0.08759311 -0.14230144]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 970 is [True, False, False, False, True, False]
Scene graph at timestep 970 is [True, False, False, False, True, False]
State prediction error at timestep 970 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 970 of 1
Current timestep = 971. State = [[-0.04666147 -0.06531355]]. Action = [[ 0.17745572  0.15704495  0.16300112 -0.3255086 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 971 is [True, False, False, False, True, False]
Scene graph at timestep 971 is [False, True, False, False, True, False]
State prediction error at timestep 971 is tensor(2.3203e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 971 of 1
Current timestep = 972. State = [[-0.2264695  0.1677215]]. Action = [[ 0.121795    0.07323954 -0.23218282  0.18549705]]. Reward = [100.]
Curr episode timestep = 29
Scene graph at timestep 972 is [False, True, False, False, True, False]
Current timestep = 973. State = [[-0.21169546  0.1737391 ]]. Action = [[ 0.1196169  -0.21756802  0.05060634 -0.4873507 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 973 is [True, False, False, False, False, True]
Current timestep = 974. State = [[-0.20608081  0.17095566]]. Action = [[ 0.00386441  0.16385621 -0.17474507  0.94125056]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 974 is [True, False, False, False, False, True]
Scene graph at timestep 974 is [True, False, False, False, False, True]
State prediction error at timestep 974 is tensor(7.8208e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 974 of -1
Current timestep = 975. State = [[-0.2057664   0.19040816]]. Action = [[-0.22430818  0.1530394   0.07804555  0.71591043]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 975 is [True, False, False, False, False, True]
Scene graph at timestep 975 is [True, False, False, False, False, True]
State prediction error at timestep 975 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 975 of -1
Current timestep = 976. State = [[-0.2252086   0.20063385]]. Action = [[-0.17151475 -0.14222343 -0.1830672   0.2149167 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 976 is [True, False, False, False, False, True]
Scene graph at timestep 976 is [True, False, False, False, False, True]
State prediction error at timestep 976 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 976 of -1
Current timestep = 977. State = [[-0.2370532   0.20065032]]. Action = [[ 0.10907692  0.2181614  -0.10153979 -0.8603746 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 977 is [True, False, False, False, False, True]
Current timestep = 978. State = [[-0.22685197  0.21254514]]. Action = [[ 0.21278697  0.00297111 -0.10978843 -0.5959084 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 978 is [True, False, False, False, False, True]
Scene graph at timestep 978 is [True, False, False, False, False, True]
State prediction error at timestep 978 is tensor(8.3033e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 978 of -1
Current timestep = 979. State = [[-0.20510998  0.20590147]]. Action = [[-0.0027678  -0.2301038  -0.10967267 -0.2689104 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 979 is [True, False, False, False, False, True]
Scene graph at timestep 979 is [True, False, False, False, False, True]
State prediction error at timestep 979 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 979 of 1
Current timestep = 980. State = [[-0.19386142  0.17503875]]. Action = [[ 0.14717218 -0.23203547  0.23539853  0.9194051 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 980 is [True, False, False, False, False, True]
Scene graph at timestep 980 is [True, False, False, False, False, True]
State prediction error at timestep 980 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 980 of 1
Current timestep = 981. State = [[-0.18923873  0.16074836]]. Action = [[-0.11266154  0.07231665 -0.2396858  -0.78190106]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 981 is [True, False, False, False, False, True]
Current timestep = 982. State = [[-0.19646883  0.16995636]]. Action = [[-0.12017605  0.0971216  -0.22330244 -0.7496872 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 982 is [True, False, False, False, False, True]
Scene graph at timestep 982 is [True, False, False, False, False, True]
State prediction error at timestep 982 is tensor(2.3535e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 982 of -1
Current timestep = 983. State = [[-0.19950917  0.16652665]]. Action = [[ 0.07376397 -0.2121769   0.0398182  -0.83927625]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 983 is [True, False, False, False, False, True]
Scene graph at timestep 983 is [True, False, False, False, False, True]
State prediction error at timestep 983 is tensor(3.1269e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 983 of 0
Current timestep = 984. State = [[-0.19217479  0.13734885]]. Action = [[ 0.06682083 -0.23885308 -0.18184073  0.61504126]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 984 is [True, False, False, False, False, True]
Current timestep = 985. State = [[-0.18216856  0.12022699]]. Action = [[ 2.1631753e-01  6.8694353e-04 -2.7198941e-02 -7.5989926e-01]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 985 is [True, False, False, False, False, True]
Current timestep = 986. State = [[-0.16827516  0.12518916]]. Action = [[ 0.09831065  0.20184708  0.24382752 -0.46714413]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 986 is [True, False, False, False, True, False]
Current timestep = 987. State = [[-0.15275216  0.14630459]]. Action = [[ 0.12952521  0.20187011  0.17960757 -0.12432319]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 987 is [True, False, False, False, False, True]
Scene graph at timestep 987 is [True, False, False, False, False, True]
State prediction error at timestep 987 is tensor(9.9212e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 987 of 0
Current timestep = 988. State = [[-0.12878062  0.15116318]]. Action = [[ 0.22360596 -0.226187   -0.02129765  0.09844422]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 988 is [True, False, False, False, False, True]
Current timestep = 989. State = [[-0.11085107  0.13475004]]. Action = [[-0.02470052 -0.10055438 -0.16333598  0.2216022 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 989 is [True, False, False, False, False, True]
Scene graph at timestep 989 is [True, False, False, False, False, True]
State prediction error at timestep 989 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 989 of 1
Current timestep = 990. State = [[-0.10773212  0.12618725]]. Action = [[-0.05551174  0.00231746  0.11149281 -0.57186174]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 990 is [True, False, False, False, False, True]
Scene graph at timestep 990 is [True, False, False, False, False, True]
State prediction error at timestep 990 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 990 of 1
Current timestep = 991. State = [[-0.10831838  0.12092185]]. Action = [[-0.08994353 -0.10112116 -0.0938091   0.60370684]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 991 is [True, False, False, False, False, True]
Scene graph at timestep 991 is [True, False, False, False, True, False]
State prediction error at timestep 991 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 991 of 1
Current timestep = 992. State = [[-0.11800604  0.12560184]]. Action = [[-0.20102969  0.19788069  0.0212974   0.55430675]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 992 is [True, False, False, False, True, False]
Scene graph at timestep 992 is [True, False, False, False, False, True]
State prediction error at timestep 992 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 992 of -1
Current timestep = 993. State = [[-0.12719862  0.13271256]]. Action = [[ 0.21577275 -0.11614008  0.11332572  0.73135114]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 993 is [True, False, False, False, False, True]
Scene graph at timestep 993 is [True, False, False, False, False, True]
State prediction error at timestep 993 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 993 of 0
Current timestep = 994. State = [[-0.11648293  0.12857153]]. Action = [[0.13442814 0.12591568 0.10251749 0.42644322]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 994 is [True, False, False, False, False, True]
Current timestep = 995. State = [[-0.10958891  0.14840743]]. Action = [[ 0.02518603  0.24270797  0.23063338 -0.8710493 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 995 is [True, False, False, False, False, True]
Scene graph at timestep 995 is [True, False, False, False, False, True]
State prediction error at timestep 995 is tensor(1.2911e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 995 of 0
Current timestep = 996. State = [[-0.10986091  0.17711811]]. Action = [[-0.1412137   0.14570493  0.01563102 -0.27752787]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 996 is [True, False, False, False, False, True]
Scene graph at timestep 996 is [True, False, False, False, False, True]
State prediction error at timestep 996 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 996 of -1
Current timestep = 997. State = [[-0.11421675  0.19267875]]. Action = [[ 0.13639766  0.05149603  0.05515021 -0.71682197]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 997 is [True, False, False, False, False, True]
Scene graph at timestep 997 is [True, False, False, False, False, True]
State prediction error at timestep 997 is tensor(1.7567e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 997 of -1
Current timestep = 998. State = [[-0.09455413  0.18476993]]. Action = [[ 0.2402995  -0.23987107  0.00299072  0.48246086]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 998 is [True, False, False, False, False, True]
Current timestep = 999. State = [[-0.06683663  0.16282468]]. Action = [[ 0.18010685 -0.1290949  -0.15422404 -0.515055  ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 999 is [True, False, False, False, False, True]
Current timestep = 1000. State = [[-0.04122659  0.15740074]]. Action = [[ 0.18784547  0.09448698 -0.03965273 -0.57249004]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1000 is [True, False, False, False, False, True]
Scene graph at timestep 1000 is [False, True, False, False, False, True]
State prediction error at timestep 1000 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1000 of 1
Current timestep = 1001. State = [[-0.00663216  0.15172671]]. Action = [[ 0.22577977 -0.20427808  0.02504891  0.20448577]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1001 is [False, True, False, False, False, True]
Current timestep = 1002. State = [[0.01670826 0.14832294]]. Action = [[0.08427054 0.16667831 0.20068514 0.295619  ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1002 is [False, True, False, False, False, True]
Current timestep = 1003. State = [[0.03282366 0.16408905]]. Action = [[ 0.11603644  0.12166771 -0.13836391  0.11461675]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1003 is [False, True, False, False, False, True]
Current timestep = 1004. State = [[0.04588016 0.17614813]]. Action = [[ 0.19687548  0.05897349 -0.031784    0.80635226]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1004 is [False, True, False, False, False, True]
Scene graph at timestep 1004 is [False, True, False, False, False, True]
State prediction error at timestep 1004 is tensor(6.7596e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1004 of -1
Current timestep = 1005. State = [[0.04771243 0.17711292]]. Action = [[ 0.16484082 -0.2305792   0.162736   -0.45476115]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1005 is [False, True, False, False, False, True]
Scene graph at timestep 1005 is [False, True, False, False, False, True]
State prediction error at timestep 1005 is tensor(9.4425e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1005 of -1
Current timestep = 1006. State = [[0.05158141 0.17755963]]. Action = [[ 0.24260783 -0.21851777  0.03565395  0.6823809 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1006 is [False, True, False, False, False, True]
Scene graph at timestep 1006 is [False, False, True, False, False, True]
State prediction error at timestep 1006 is tensor(4.3084e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1006 of -1
Current timestep = 1007. State = [[0.05158141 0.17755963]]. Action = [[ 0.21974769 -0.21240902 -0.165674   -0.08958209]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1007 is [False, False, True, False, False, True]
Scene graph at timestep 1007 is [False, False, True, False, False, True]
State prediction error at timestep 1007 is tensor(2.9671e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1007 of -1
Current timestep = 1008. State = [[0.05158141 0.17755963]]. Action = [[0.1209133  0.03100288 0.20518929 0.57825744]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1008 is [False, False, True, False, False, True]
Current timestep = 1009. State = [[0.05158141 0.17755963]]. Action = [[ 0.1103124  -0.23801054 -0.09137806  0.5271621 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1009 is [False, False, True, False, False, True]
Scene graph at timestep 1009 is [False, False, True, False, False, True]
State prediction error at timestep 1009 is tensor(4.6223e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1009 of 0
Current timestep = 1010. State = [[0.05158141 0.17755963]]. Action = [[ 0.17783982 -0.03807785  0.2110343  -0.09793675]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1010 is [False, False, True, False, False, True]
Current timestep = 1011. State = [[0.05158141 0.17755963]]. Action = [[-0.01759709 -0.00601862 -0.23888427  0.44726217]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1011 is [False, False, True, False, False, True]
Current timestep = 1012. State = [[0.05161336 0.17740767]]. Action = [[ 0.1962778   0.00466123  0.10176724 -0.46780694]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1012 is [False, False, True, False, False, True]
Current timestep = 1013. State = [[0.05161336 0.17740767]]. Action = [[ 0.22125483 -0.02826004  0.1452176   0.336192  ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1013 is [False, False, True, False, False, True]
Scene graph at timestep 1013 is [False, False, True, False, False, True]
State prediction error at timestep 1013 is tensor(5.2143e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1013 of 0
Current timestep = 1014. State = [[0.05251747 0.17477329]]. Action = [[ 0.03703317 -0.04509962  0.04817554  0.8177717 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1014 is [False, False, True, False, False, True]
Current timestep = 1015. State = [[0.05597938 0.16361184]]. Action = [[-0.07431784 -0.16340421  0.20137697 -0.7100518 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1015 is [False, False, True, False, False, True]
Scene graph at timestep 1015 is [False, False, True, False, False, True]
State prediction error at timestep 1015 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1015 of 1
Current timestep = 1016. State = [[0.05801166 0.15308952]]. Action = [[ 0.09639797 -0.08995081  0.05833182 -0.23013687]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1016 is [False, False, True, False, False, True]
Scene graph at timestep 1016 is [False, False, True, False, False, True]
State prediction error at timestep 1016 is tensor(4.3718e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1016 of 1
Current timestep = 1017. State = [[0.05801166 0.15308952]]. Action = [[ 0.15668091  0.23772603 -0.0496822  -0.6055386 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1017 is [False, False, True, False, False, True]
Current timestep = 1018. State = [[0.05801166 0.15308952]]. Action = [[0.131782   0.17414823 0.22074586 0.95646834]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1018 is [False, False, True, False, False, True]
Scene graph at timestep 1018 is [False, False, True, False, False, True]
State prediction error at timestep 1018 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1018 of 0
Current timestep = 1019. State = [[0.05801166 0.15308952]]. Action = [[ 0.10960546 -0.05764219 -0.23773594 -0.3365897 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1019 is [False, False, True, False, False, True]
Current timestep = 1020. State = [[0.05419846 0.16331661]]. Action = [[-0.03158744  0.21215433 -0.21972463 -0.82915944]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1020 is [False, False, True, False, False, True]
Scene graph at timestep 1020 is [False, False, True, False, False, True]
State prediction error at timestep 1020 is tensor(5.0146e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1020 of -1
Current timestep = 1021. State = [[0.05052346 0.17331152]]. Action = [[ 0.02170101 -0.0336659  -0.03335439  0.1839323 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1021 is [False, False, True, False, False, True]
Scene graph at timestep 1021 is [False, False, True, False, False, True]
State prediction error at timestep 1021 is tensor(4.4632e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1021 of -1
Current timestep = 1022. State = [[0.04966636 0.17725039]]. Action = [[ 0.0287616   0.09985283 -0.18765923 -0.25712597]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1022 is [False, False, True, False, False, True]
Current timestep = 1023. State = [[0.04515339 0.18946381]]. Action = [[-1.6742130e-01  1.1776495e-01  6.6548586e-04  8.5168076e-01]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1023 is [False, True, False, False, False, True]
Scene graph at timestep 1023 is [False, True, False, False, False, True]
State prediction error at timestep 1023 is tensor(8.2370e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1023 of -1
Current timestep = 1024. State = [[0.03285317 0.21399099]]. Action = [[-0.19697046  0.12270606 -0.2175649   0.48847985]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1024 is [False, True, False, False, False, True]
Scene graph at timestep 1024 is [False, True, False, False, False, True]
State prediction error at timestep 1024 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1024 of -1
Current timestep = 1025. State = [[0.02402325 0.22742859]]. Action = [[ 0.12721995 -0.00249146  0.11597922 -0.69147205]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1025 is [False, True, False, False, False, True]
Current timestep = 1026. State = [[0.0209464  0.23309928]]. Action = [[-0.14804155  0.08615395  0.06812775 -0.25346625]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1026 is [False, True, False, False, False, True]
Scene graph at timestep 1026 is [False, True, False, False, False, True]
State prediction error at timestep 1026 is tensor(7.9439e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1026 of -1
Current timestep = 1027. State = [[0.01755014 0.23743111]]. Action = [[-0.00820413 -0.05245641  0.1918875  -0.1346752 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1027 is [False, True, False, False, False, True]
Current timestep = 1028. State = [[0.02015515 0.22514379]]. Action = [[ 0.03928074 -0.17584355 -0.18489     0.93889105]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1028 is [False, True, False, False, False, True]
Scene graph at timestep 1028 is [False, True, False, False, False, True]
State prediction error at timestep 1028 is tensor(1.8331e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1028 of 1
Current timestep = 1029. State = [[0.02001554 0.21312328]]. Action = [[ 0.02200201  0.04634571 -0.14674997  0.8151307 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1029 is [False, True, False, False, False, True]
Scene graph at timestep 1029 is [False, True, False, False, False, True]
State prediction error at timestep 1029 is tensor(6.1611e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1029 of 1
Current timestep = 1030. State = [[0.02376501 0.2079114 ]]. Action = [[ 0.18398944 -0.06520092 -0.03254008 -0.40300435]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1030 is [False, True, False, False, False, True]
Scene graph at timestep 1030 is [False, True, False, False, False, True]
State prediction error at timestep 1030 is tensor(1.5796e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1030 of 0
Current timestep = 1031. State = [[0.03323039 0.18980086]]. Action = [[ 0.10984045 -0.21193568 -0.15262923 -0.9260954 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1031 is [False, True, False, False, False, True]
Scene graph at timestep 1031 is [False, True, False, False, False, True]
State prediction error at timestep 1031 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1031 of 1
Current timestep = 1032. State = [[0.04253212 0.1707928 ]]. Action = [[ 0.22366038 -0.19551334 -0.0790289  -0.7123861 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1032 is [False, True, False, False, False, True]
Current timestep = 1033. State = [[0.04253212 0.1707928 ]]. Action = [[0.21178937 0.06099501 0.02731308 0.5790236 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1033 is [False, True, False, False, False, True]
Scene graph at timestep 1033 is [False, True, False, False, False, True]
State prediction error at timestep 1033 is tensor(5.4108e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1033 of 1
Current timestep = 1034. State = [[0.04465421 0.16530071]]. Action = [[-0.01546693 -0.09865843  0.07835081 -0.07268411]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1034 is [False, True, False, False, False, True]
Scene graph at timestep 1034 is [False, True, False, False, False, True]
State prediction error at timestep 1034 is tensor(8.2319e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1034 of 1
Current timestep = 1035. State = [[0.04695537 0.15977904]]. Action = [[-0.0330883   0.03827655  0.11044201  0.8714094 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1035 is [False, True, False, False, False, True]
Current timestep = 1036. State = [[0.05040475 0.14939186]]. Action = [[ 0.08713654 -0.19677232 -0.14476784  0.5597391 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1036 is [False, True, False, False, False, True]
Scene graph at timestep 1036 is [False, False, True, False, False, True]
State prediction error at timestep 1036 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1036 of 1
Current timestep = 1037. State = [[0.0557745  0.13528085]]. Action = [[ 0.22530556  0.02315882 -0.09423338  0.7625172 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1037 is [False, False, True, False, False, True]
Scene graph at timestep 1037 is [False, False, True, False, False, True]
State prediction error at timestep 1037 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1037 of 0
Current timestep = 1038. State = [[0.05686471 0.1222431 ]]. Action = [[-0.05448544 -0.23262635 -0.2025927  -0.311579  ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1038 is [False, False, True, False, False, True]
Scene graph at timestep 1038 is [False, False, True, False, True, False]
State prediction error at timestep 1038 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1038 of 1
Current timestep = 1039. State = [[0.06160749 0.10446583]]. Action = [[ 0.1781004  -0.05019967  0.19395739 -0.7292939 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1039 is [False, False, True, False, True, False]
Scene graph at timestep 1039 is [False, False, True, False, True, False]
State prediction error at timestep 1039 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1039 of 1
Current timestep = 1040. State = [[0.0615445  0.10382213]]. Action = [[ 0.03501683 -0.09156731  0.14481306  0.9920387 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1040 is [False, False, True, False, True, False]
Current timestep = 1041. State = [[0.0615445  0.10382213]]. Action = [[ 0.16309255 -0.2092307  -0.19415946  0.2618667 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1041 is [False, False, True, False, True, False]
Current timestep = 1042. State = [[0.06141661 0.10368646]]. Action = [[-0.04017341  0.00834855 -0.15018658 -0.5258847 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1042 is [False, False, True, False, True, False]
Current timestep = 1043. State = [[0.05287871 0.10559977]]. Action = [[-0.2415109   0.03792945  0.01767194  0.8627398 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1043 is [False, False, True, False, True, False]
Scene graph at timestep 1043 is [False, False, True, False, True, False]
State prediction error at timestep 1043 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1043 of 1
Current timestep = 1044. State = [[-0.2616954  -0.00660541]]. Action = [[-0.06990373 -0.16859667 -0.2085225   0.00501382]]. Reward = [100.]
Curr episode timestep = 71
Scene graph at timestep 1044 is [False, False, True, False, True, False]
Current timestep = 1045. State = [[-0.2625123  -0.02056496]]. Action = [[-0.0199195  -0.22150095  0.16490313  0.5471115 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1045 is [True, False, False, False, True, False]
Current timestep = 1046. State = [[-0.25720096 -0.02863256]]. Action = [[ 0.22443637  0.13082802 -0.13957404  0.53142536]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1046 is [True, False, False, False, True, False]
Scene graph at timestep 1046 is [True, False, False, False, True, False]
State prediction error at timestep 1046 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1046 of 1
Current timestep = 1047. State = [[-0.24471293 -0.01582784]]. Action = [[ 0.0297313   0.14133012  0.09920266 -0.3526674 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1047 is [True, False, False, False, True, False]
Current timestep = 1048. State = [[-0.24566895 -0.01613906]]. Action = [[-0.1585178  -0.18460093 -0.18501054 -0.943407  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1048 is [True, False, False, False, True, False]
Current timestep = 1049. State = [[-0.25119573 -0.03660232]]. Action = [[-0.03289554 -0.211178    0.09773847 -0.16277087]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1049 is [True, False, False, False, True, False]
Scene graph at timestep 1049 is [True, False, False, False, True, False]
State prediction error at timestep 1049 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1049 of 0
Current timestep = 1050. State = [[-0.2603739  -0.05924276]]. Action = [[-0.09467381 -0.0418466  -0.1803895  -0.07033384]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1050 is [True, False, False, False, True, False]
Current timestep = 1051. State = [[-0.26320547 -0.05039091]]. Action = [[0.10304469 0.20329982 0.05641833 0.69505525]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1051 is [True, False, False, False, True, False]
Current timestep = 1052. State = [[-0.25320777 -0.03024609]]. Action = [[0.23163274 0.17370099 0.06447774 0.7728014 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1052 is [True, False, False, False, True, False]
Current timestep = 1053. State = [[-0.22689906 -0.02517546]]. Action = [[ 0.19976234 -0.17897822  0.1997127   0.7295891 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1053 is [True, False, False, False, True, False]
Scene graph at timestep 1053 is [True, False, False, False, True, False]
State prediction error at timestep 1053 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1053 of 1
Current timestep = 1054. State = [[-0.20151769 -0.03074165]]. Action = [[0.15882143 0.03288913 0.11096221 0.59070134]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1054 is [True, False, False, False, True, False]
Current timestep = 1055. State = [[-0.17659329 -0.04009732]]. Action = [[ 0.2009502  -0.18185633  0.2246224   0.6409023 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1055 is [True, False, False, False, True, False]
Scene graph at timestep 1055 is [True, False, False, False, True, False]
State prediction error at timestep 1055 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1055 of 1
Current timestep = 1056. State = [[-0.14352332 -0.05180976]]. Action = [[ 0.20891973 -0.00101314 -0.16059561  0.32507598]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1056 is [True, False, False, False, True, False]
Scene graph at timestep 1056 is [True, False, False, False, True, False]
State prediction error at timestep 1056 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1056 of 1
Current timestep = 1057. State = [[-0.11541123 -0.05872873]]. Action = [[ 0.15150246 -0.09838173  0.02710646 -0.4418534 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1057 is [True, False, False, False, True, False]
Scene graph at timestep 1057 is [True, False, False, False, True, False]
State prediction error at timestep 1057 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1057 of 1
Current timestep = 1058. State = [[-0.10045771 -0.07490664]]. Action = [[-0.03670022 -0.10326901 -0.02759364  0.05178666]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1058 is [True, False, False, False, True, False]
Current timestep = 1059. State = [[-0.09594315 -0.08472198]]. Action = [[ 0.1481446  -0.05861713 -0.0437068  -0.7310529 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1059 is [True, False, False, False, True, False]
Scene graph at timestep 1059 is [True, False, False, False, True, False]
State prediction error at timestep 1059 is tensor(4.1999e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1059 of 1
Current timestep = 1060. State = [[-0.07742272 -0.08766706]]. Action = [[ 0.12291121  0.06559917 -0.23536469  0.27936327]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1060 is [True, False, False, False, True, False]
Current timestep = 1061. State = [[-0.06651385 -0.07939763]]. Action = [[ 0.04715025  0.12019932 -0.248533    0.8332472 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1061 is [True, False, False, False, True, False]
Scene graph at timestep 1061 is [True, False, False, False, True, False]
State prediction error at timestep 1061 is tensor(1.3526e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1061 of 1
Current timestep = 1062. State = [[-0.0648354  -0.06766687]]. Action = [[-0.23434271  0.07235     0.00823277  0.06172657]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1062 is [True, False, False, False, True, False]
Scene graph at timestep 1062 is [True, False, False, False, True, False]
State prediction error at timestep 1062 is tensor(2.4417e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1062 of 0
Current timestep = 1063. State = [[-0.07059181 -0.04659054]]. Action = [[ 0.08227241  0.23368606 -0.2425738  -0.03193802]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1063 is [True, False, False, False, True, False]
Current timestep = 1064. State = [[-0.06834511 -0.03243906]]. Action = [[ 0.13757837 -0.04501018  0.11619562 -0.00806648]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1064 is [True, False, False, False, True, False]
Scene graph at timestep 1064 is [True, False, False, False, True, False]
State prediction error at timestep 1064 is tensor(2.9692e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1064 of 1
Current timestep = 1065. State = [[-0.0576485 -0.0344063]]. Action = [[ 0.08598632 -0.05508269  0.10884419 -0.09972799]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1065 is [True, False, False, False, True, False]
Scene graph at timestep 1065 is [True, False, False, False, True, False]
State prediction error at timestep 1065 is tensor(5.0008e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1065 of 1
Current timestep = 1066. State = [[-0.05277975 -0.02632946]]. Action = [[-0.11140865  0.1929943   0.06350324  0.42921603]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1066 is [True, False, False, False, True, False]
Current timestep = 1067. State = [[-0.05441779 -0.01331864]]. Action = [[ 0.10836977  0.03628391 -0.19623928  0.18767083]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1067 is [True, False, False, False, True, False]
Current timestep = 1068. State = [[-0.05565536 -0.00657761]]. Action = [[-0.16843112  0.04122525 -0.14838664 -0.59930766]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1068 is [True, False, False, False, True, False]
Scene graph at timestep 1068 is [True, False, False, False, True, False]
State prediction error at timestep 1068 is tensor(2.5092e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1068 of -1
Current timestep = 1069. State = [[-0.05307999 -0.01411497]]. Action = [[ 0.22770208 -0.23795356  0.18501464 -0.663809  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1069 is [True, False, False, False, True, False]
Current timestep = 1070. State = [[-0.03690722 -0.02519959]]. Action = [[ 0.14815906  0.04430705 -0.16539867 -0.507734  ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1070 is [True, False, False, False, True, False]
Current timestep = 1071. State = [[-0.19269983  0.14912991]]. Action = [[-0.11765161  0.20141381 -0.0063833  -0.6619964 ]]. Reward = [100.]
Curr episode timestep = 26
Scene graph at timestep 1071 is [False, True, False, False, True, False]
Scene graph at timestep 1071 is [True, False, False, False, False, True]
State prediction error at timestep 1071 is tensor(0.0274, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1071 of -1
Current timestep = 1072. State = [[-0.17550935  0.16037647]]. Action = [[ 0.11283109 -0.11836021 -0.21766569  0.7509358 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1072 is [True, False, False, False, False, True]
Current timestep = 1073. State = [[-0.16752344  0.14400601]]. Action = [[-0.1196095  -0.19970016 -0.10859914  0.17776513]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1073 is [True, False, False, False, False, True]
Scene graph at timestep 1073 is [True, False, False, False, False, True]
State prediction error at timestep 1073 is tensor(6.9664e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1073 of 1
Current timestep = 1074. State = [[-0.16740274  0.11919318]]. Action = [[-0.0472123  -0.13601449  0.15890989  0.4954877 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1074 is [True, False, False, False, False, True]
Current timestep = 1075. State = [[-0.16644444  0.11350895]]. Action = [[ 0.15361652  0.08446205 -0.11508419 -0.8431327 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1075 is [True, False, False, False, True, False]
Current timestep = 1076. State = [[-0.15380903  0.11070495]]. Action = [[ 0.22600216 -0.08029589 -0.14424127 -0.29501605]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1076 is [True, False, False, False, True, False]
Current timestep = 1077. State = [[-0.14022426  0.10078595]]. Action = [[-0.18738344 -0.13980585  0.22862333 -0.64124954]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1077 is [True, False, False, False, True, False]
Current timestep = 1078. State = [[-0.13578162  0.07764263]]. Action = [[ 0.18092513 -0.2416393  -0.15374893 -0.33483154]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1078 is [True, False, False, False, True, False]
Current timestep = 1079. State = [[-0.12125759  0.04614771]]. Action = [[ 0.21872234 -0.19631058 -0.15329467  0.8882452 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1079 is [True, False, False, False, True, False]
Scene graph at timestep 1079 is [True, False, False, False, True, False]
State prediction error at timestep 1079 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1079 of 1
Current timestep = 1080. State = [[-0.09500357  0.03071956]]. Action = [[ 0.17803103  0.11804879 -0.05997935  0.25997448]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1080 is [True, False, False, False, True, False]
Scene graph at timestep 1080 is [True, False, False, False, True, False]
State prediction error at timestep 1080 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1080 of 1
Current timestep = 1081. State = [[-0.06514975  0.03425102]]. Action = [[ 0.23044556 -0.05322666  0.23083857 -0.6605531 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1081 is [True, False, False, False, True, False]
Scene graph at timestep 1081 is [True, False, False, False, True, False]
State prediction error at timestep 1081 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1081 of 1
Current timestep = 1082. State = [[-0.04556908  0.04407167]]. Action = [[-0.09798482  0.1915212   0.076738    0.25788796]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1082 is [True, False, False, False, True, False]
Scene graph at timestep 1082 is [False, True, False, False, True, False]
State prediction error at timestep 1082 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1082 of -1
Current timestep = 1083. State = [[-0.0410763   0.04336533]]. Action = [[ 0.22138149 -0.2201421  -0.18472123  0.9004686 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1083 is [False, True, False, False, True, False]
Scene graph at timestep 1083 is [False, True, False, False, True, False]
State prediction error at timestep 1083 is tensor(1.9970e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1083 of 1
Current timestep = 1084. State = [[-0.26069486 -0.02439908]]. Action = [[ 0.09079117  0.02231476 -0.15006442  0.581897  ]]. Reward = [100.]
Curr episode timestep = 12
Scene graph at timestep 1084 is [False, True, False, False, True, False]
Scene graph at timestep 1084 is [True, False, False, False, True, False]
State prediction error at timestep 1084 is tensor(0.0291, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1084 of 0
Current timestep = 1085. State = [[-0.2554005  -0.02206996]]. Action = [[ 0.12089458  0.11794031 -0.24115026 -0.50967556]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1085 is [True, False, False, False, True, False]
Scene graph at timestep 1085 is [True, False, False, False, True, False]
State prediction error at timestep 1085 is tensor(1.6518e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1085 of 1
Current timestep = 1086. State = [[-0.24164082 -0.01227228]]. Action = [[0.14499038 0.09299365 0.13293618 0.72703063]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1086 is [True, False, False, False, True, False]
Scene graph at timestep 1086 is [True, False, False, False, True, False]
State prediction error at timestep 1086 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1086 of 1
Current timestep = 1087. State = [[-0.22019708 -0.01531112]]. Action = [[ 0.18110913 -0.21689741  0.14568073  0.90133595]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1087 is [True, False, False, False, True, False]
Current timestep = 1088. State = [[-0.19577214 -0.02952323]]. Action = [[ 0.18168527 -0.07804845 -0.16679282  0.833215  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1088 is [True, False, False, False, True, False]
Scene graph at timestep 1088 is [True, False, False, False, True, False]
State prediction error at timestep 1088 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1088 of 1
Current timestep = 1089. State = [[-0.18161453 -0.03015151]]. Action = [[-0.16744982  0.1523315  -0.21859604 -0.907586  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1089 is [True, False, False, False, True, False]
Scene graph at timestep 1089 is [True, False, False, False, True, False]
State prediction error at timestep 1089 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1089 of -1
Current timestep = 1090. State = [[-0.18048246 -0.02234372]]. Action = [[0.20682561 0.01180351 0.19885576 0.03559911]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1090 is [True, False, False, False, True, False]
Current timestep = 1091. State = [[-0.17077325 -0.02991173]]. Action = [[ 0.01588169 -0.16140991  0.2294088  -0.01141346]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1091 is [True, False, False, False, True, False]
Scene graph at timestep 1091 is [True, False, False, False, True, False]
State prediction error at timestep 1091 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1091 of 0
Current timestep = 1092. State = [[-0.15618342 -0.04155199]]. Action = [[ 0.22620168 -0.05925718  0.01325083  0.501518  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1092 is [True, False, False, False, True, False]
Current timestep = 1093. State = [[-0.13529035 -0.05176472]]. Action = [[ 0.04186893 -0.09386845  0.06575733 -0.8457025 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1093 is [True, False, False, False, True, False]
Scene graph at timestep 1093 is [True, False, False, False, True, False]
State prediction error at timestep 1093 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1093 of 1
Current timestep = 1094. State = [[-0.12472923 -0.05502997]]. Action = [[ 0.09366706  0.12637699  0.14756069 -0.9766764 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1094 is [True, False, False, False, True, False]
Scene graph at timestep 1094 is [True, False, False, False, True, False]
State prediction error at timestep 1094 is tensor(4.8553e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1094 of 1
Current timestep = 1095. State = [[-0.1205284  -0.04749615]]. Action = [[-0.21148977  0.02544066 -0.06767857 -0.4413622 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1095 is [True, False, False, False, True, False]
Current timestep = 1096. State = [[-0.12140192 -0.0418504 ]]. Action = [[0.16984856 0.07191104 0.04943752 0.18850434]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1096 is [True, False, False, False, True, False]
Current timestep = 1097. State = [[-0.12240101 -0.02373974]]. Action = [[-0.10973097  0.20960227  0.24338233 -0.7349317 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1097 is [True, False, False, False, True, False]
Current timestep = 1098. State = [[-0.11952808 -0.00084887]]. Action = [[0.20389438 0.10462305 0.22001177 0.66284835]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1098 is [True, False, False, False, True, False]
Current timestep = 1099. State = [[-0.10702349  0.01807897]]. Action = [[ 0.02786326  0.15383255 -0.19453163  0.5389024 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1099 is [True, False, False, False, True, False]
Current timestep = 1100. State = [[-0.09831855  0.01989454]]. Action = [[ 0.09683341 -0.17555925  0.0888139   0.1872232 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1100 is [True, False, False, False, True, False]
Scene graph at timestep 1100 is [True, False, False, False, True, False]
State prediction error at timestep 1100 is tensor(2.7137e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1100 of 1
Current timestep = 1101. State = [[-0.0934448   0.01153027]]. Action = [[-0.21696508 -0.01653612  0.05643994  0.49210608]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1101 is [True, False, False, False, True, False]
Scene graph at timestep 1101 is [True, False, False, False, True, False]
State prediction error at timestep 1101 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1101 of -1
Current timestep = 1102. State = [[-0.09709612  0.00761916]]. Action = [[ 0.09431678 -0.01475102 -0.14465979 -0.6098872 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1102 is [True, False, False, False, True, False]
Scene graph at timestep 1102 is [True, False, False, False, True, False]
State prediction error at timestep 1102 is tensor(1.6258e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1102 of -1
Current timestep = 1103. State = [[-0.09704066  0.00720987]]. Action = [[ 3.1073481e-02 -7.0340931e-04 -2.0304948e-01 -8.1222296e-01]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1103 is [True, False, False, False, True, False]
Current timestep = 1104. State = [[-8.6353153e-02 -3.3535853e-05]]. Action = [[ 0.24457592 -0.12095147 -0.24553667  0.69834614]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1104 is [True, False, False, False, True, False]
Current timestep = 1105. State = [[-0.07217714 -0.02020746]]. Action = [[-0.09885225 -0.1989366   0.23797521 -0.99365073]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1105 is [True, False, False, False, True, False]
Current timestep = 1106. State = [[-0.0672475  -0.02994216]]. Action = [[ 0.17332304  0.0642488  -0.17038657 -0.8876042 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1106 is [True, False, False, False, True, False]
Current timestep = 1107. State = [[-0.05935511 -0.02836683]]. Action = [[-0.05585158  0.03912476  0.11261764  0.88271785]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1107 is [True, False, False, False, True, False]
Current timestep = 1108. State = [[-0.06218363 -0.01533817]]. Action = [[-0.13405672  0.19558513 -0.01376079 -0.8379662 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1108 is [True, False, False, False, True, False]
Current timestep = 1109. State = [[-0.06547677 -0.00420517]]. Action = [[ 0.00874436 -0.04884742 -0.05505754  0.66683984]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1109 is [True, False, False, False, True, False]
Scene graph at timestep 1109 is [True, False, False, False, True, False]
State prediction error at timestep 1109 is tensor(7.2002e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1109 of 1
Current timestep = 1110. State = [[-0.06720137  0.00652615]]. Action = [[-0.01286413  0.18809855 -0.21297237  0.7284317 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1110 is [True, False, False, False, True, False]
Current timestep = 1111. State = [[-0.06567553  0.02035539]]. Action = [[ 0.18136486  0.04998025 -0.16410293 -0.41024143]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1111 is [True, False, False, False, True, False]
Scene graph at timestep 1111 is [True, False, False, False, True, False]
State prediction error at timestep 1111 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1111 of 1
Current timestep = 1112. State = [[-0.04880448  0.01769395]]. Action = [[ 0.2376059  -0.16268955  0.07545513 -0.36411572]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1112 is [True, False, False, False, True, False]
Current timestep = 1113. State = [[-0.14925422 -0.19238666]]. Action = [[ 0.05713913  0.23396769 -0.01279299 -0.41364223]]. Reward = [100.]
Curr episode timestep = 28
Scene graph at timestep 1113 is [False, True, False, False, True, False]
Scene graph at timestep 1113 is [True, False, False, True, False, False]
State prediction error at timestep 1113 is tensor(0.0291, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1113 of 1
Current timestep = 1114. State = [[-0.1242165  -0.21499977]]. Action = [[ 0.17147899  0.00676075  0.04798004 -0.21471113]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1114 is [True, False, False, True, False, False]
Scene graph at timestep 1114 is [True, False, False, True, False, False]
State prediction error at timestep 1114 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1114 of 1
Current timestep = 1115. State = [[-0.10272831 -0.20569815]]. Action = [[0.04771093 0.23525941 0.13455951 0.05204213]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1115 is [True, False, False, True, False, False]
Current timestep = 1116. State = [[-0.098502   -0.18118683]]. Action = [[-0.00714213  0.16469353 -0.12664928 -0.5810025 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1116 is [True, False, False, True, False, False]
Scene graph at timestep 1116 is [True, False, False, True, False, False]
State prediction error at timestep 1116 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1116 of 1
Current timestep = 1117. State = [[-0.09241031 -0.16011943]]. Action = [[0.13272774 0.05875954 0.2385242  0.77685714]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1117 is [True, False, False, True, False, False]
Current timestep = 1118. State = [[-0.08926495 -0.15484402]]. Action = [[-0.07324022  0.00783679  0.18104962  0.10102522]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1118 is [True, False, False, True, False, False]
Current timestep = 1119. State = [[-0.08368936 -0.16051069]]. Action = [[ 0.15873039 -0.14311844  0.23687142 -0.08851898]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1119 is [True, False, False, True, False, False]
Current timestep = 1120. State = [[-0.07485549 -0.15783142]]. Action = [[-0.17453346  0.18405765  0.11628866 -0.5723812 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1120 is [True, False, False, True, False, False]
Current timestep = 1121. State = [[-0.07642328 -0.14274427]]. Action = [[ 0.02529669  0.13409054 -0.18545929 -0.61944944]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1121 is [True, False, False, True, False, False]
Current timestep = 1122. State = [[-0.07641105 -0.12492742]]. Action = [[ 0.06488946  0.10374406  0.2305901  -0.80575556]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1122 is [True, False, False, True, False, False]
Current timestep = 1123. State = [[-0.07753883 -0.12271534]]. Action = [[-0.08261222 -0.11783743 -0.04703328  0.21668231]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1123 is [True, False, False, False, True, False]
Current timestep = 1124. State = [[-0.08220105 -0.12839614]]. Action = [[-0.13235064  0.01639324 -0.13751172 -0.4547547 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1124 is [True, False, False, False, True, False]
Current timestep = 1125. State = [[-0.08411371 -0.12984496]]. Action = [[ 0.1614601  -0.01219371 -0.12080742 -0.23907423]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1125 is [True, False, False, True, False, False]
Scene graph at timestep 1125 is [True, False, False, True, False, False]
State prediction error at timestep 1125 is tensor(6.5903e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1125 of 0
Current timestep = 1126. State = [[-0.07794848 -0.13114543]]. Action = [[ 0.13830024 -0.05283648 -0.16786689  0.04828668]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1126 is [True, False, False, True, False, False]
Current timestep = 1127. State = [[-0.06084734 -0.13001294]]. Action = [[ 0.24882394  0.03804663 -0.03556658 -0.9465708 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1127 is [True, False, False, True, False, False]
Current timestep = 1128. State = [[-0.02982549 -0.13653997]]. Action = [[ 0.19957513 -0.12858938  0.13933897  0.14971733]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1128 is [True, False, False, True, False, False]
Scene graph at timestep 1128 is [False, True, False, True, False, False]
State prediction error at timestep 1128 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1128 of 1
Current timestep = 1129. State = [[ 0.0014781  -0.13873337]]. Action = [[ 0.14666092  0.10539186 -0.18805183  0.38149905]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1129 is [False, True, False, True, False, False]
Scene graph at timestep 1129 is [False, True, False, True, False, False]
State prediction error at timestep 1129 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1129 of 1
Current timestep = 1130. State = [[ 0.02102446 -0.14069384]]. Action = [[ 0.05558798 -0.11370036  0.12879261 -0.3382244 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1130 is [False, True, False, True, False, False]
Current timestep = 1131. State = [[ 0.02796258 -0.13497458]]. Action = [[ 0.06297275  0.18139583 -0.23284946 -0.47886157]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1131 is [False, True, False, True, False, False]
Scene graph at timestep 1131 is [False, True, False, True, False, False]
State prediction error at timestep 1131 is tensor(3.6757e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1131 of 1
Current timestep = 1132. State = [[ 0.04234247 -0.11824653]]. Action = [[0.07598233 0.13317704 0.1106874  0.45791578]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1132 is [False, True, False, True, False, False]
Scene graph at timestep 1132 is [False, True, False, False, True, False]
State prediction error at timestep 1132 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1132 of 1
Current timestep = 1133. State = [[ 0.05003453 -0.10294253]]. Action = [[0.04470083 0.06671825 0.06570029 0.700243  ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1133 is [False, True, False, False, True, False]
Scene graph at timestep 1133 is [False, False, True, False, True, False]
State prediction error at timestep 1133 is tensor(9.2722e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1133 of 0
Current timestep = 1134. State = [[ 0.05130902 -0.09804751]]. Action = [[ 0.2298587   0.05431381  0.16763341 -0.9109078 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1134 is [False, False, True, False, True, False]
Current timestep = 1135. State = [[ 0.05117512 -0.09801652]]. Action = [[ 0.15651727 -0.1148728   0.08911791  0.09962571]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1135 is [False, False, True, False, True, False]
Current timestep = 1136. State = [[ 0.05117512 -0.09801652]]. Action = [[0.23141116 0.00738633 0.03567269 0.4381945 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1136 is [False, False, True, False, True, False]
Scene graph at timestep 1136 is [False, False, True, False, True, False]
State prediction error at timestep 1136 is tensor(6.0790e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1136 of 0
Current timestep = 1137. State = [[ 0.05496449 -0.08565698]]. Action = [[0.05542228 0.19162881 0.06501493 0.18738234]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1137 is [False, False, True, False, True, False]
Scene graph at timestep 1137 is [False, False, True, False, True, False]
State prediction error at timestep 1137 is tensor(6.9764e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1137 of -1
Current timestep = 1138. State = [[ 0.06731228 -0.06922667]]. Action = [[-0.00695071  0.00660032  0.01806179 -0.5946698 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1138 is [False, False, True, False, True, False]
Current timestep = 1139. State = [[ 0.06723205 -0.06995626]]. Action = [[-0.00449799 -0.03807116 -0.14686656  0.6359023 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1139 is [False, False, True, False, True, False]
Scene graph at timestep 1139 is [False, False, True, False, True, False]
State prediction error at timestep 1139 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1139 of -1
Current timestep = 1140. State = [[ 0.06543519 -0.07785873]]. Action = [[-0.17015232 -0.09373665 -0.13811368  0.00422406]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1140 is [False, False, True, False, True, False]
Current timestep = 1141. State = [[ 0.0637086  -0.08484982]]. Action = [[ 0.08916304  0.02962077  0.16833442 -0.20308322]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1141 is [False, False, True, False, True, False]
Scene graph at timestep 1141 is [False, False, True, False, True, False]
State prediction error at timestep 1141 is tensor(8.4246e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1141 of -1
Current timestep = 1142. State = [[ 0.06285004 -0.08804268]]. Action = [[ 0.1434725  -0.02922726  0.08559594  0.4903704 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1142 is [False, False, True, False, True, False]
Scene graph at timestep 1142 is [False, False, True, False, True, False]
State prediction error at timestep 1142 is tensor(6.5345e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1142 of -1
Current timestep = 1143. State = [[ 0.06257819 -0.08809228]]. Action = [[ 0.20289439 -0.16794111  0.04972029  0.21467352]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1143 is [False, False, True, False, True, False]
Scene graph at timestep 1143 is [False, False, True, False, True, False]
State prediction error at timestep 1143 is tensor(2.1296e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1143 of 0
Current timestep = 1144. State = [[ 0.06255139 -0.08808564]]. Action = [[ 0.12222815  0.2078278  -0.14302218  0.9811201 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1144 is [False, False, True, False, True, False]
Scene graph at timestep 1144 is [False, False, True, False, True, False]
State prediction error at timestep 1144 is tensor(9.8705e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1144 of 0
Current timestep = 1145. State = [[ 0.0623706 -0.0867626]]. Action = [[-2.8961897e-04  3.2582700e-02 -1.8055376e-01  8.3937550e-01]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1145 is [False, False, True, False, True, False]
Current timestep = 1146. State = [[ 0.06060328 -0.0753338 ]]. Action = [[-0.08534987  0.15520519 -0.0738447  -0.17367864]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1146 is [False, False, True, False, True, False]
Current timestep = 1147. State = [[ 0.05593681 -0.06110212]]. Action = [[ 0.20254645 -0.1711544  -0.23209195 -0.20314229]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1147 is [False, False, True, False, True, False]
Current timestep = 1148. State = [[ 0.05568923 -0.05952298]]. Action = [[ 0.22453278 -0.17073412 -0.23077229  0.4321463 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1148 is [False, False, True, False, True, False]
Scene graph at timestep 1148 is [False, False, True, False, True, False]
State prediction error at timestep 1148 is tensor(3.7603e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1148 of 1
Current timestep = 1149. State = [[ 0.05556635 -0.05913772]]. Action = [[ 0.15471137 -0.24446708  0.02128035  0.7542676 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1149 is [False, False, True, False, True, False]
Scene graph at timestep 1149 is [False, False, True, False, True, False]
State prediction error at timestep 1149 is tensor(3.5912e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1149 of 1
Current timestep = 1150. State = [[ 0.05550297 -0.05913025]]. Action = [[0.1219328  0.17567497 0.0986616  0.9058833 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1150 is [False, False, True, False, True, False]
Scene graph at timestep 1150 is [False, False, True, False, True, False]
State prediction error at timestep 1150 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1150 of 0
Current timestep = 1151. State = [[ 0.05550297 -0.05913025]]. Action = [[ 0.21918815 -0.19094802 -0.12960461 -0.4180584 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1151 is [False, False, True, False, True, False]
Current timestep = 1152. State = [[ 0.05550297 -0.05913025]]. Action = [[0.21661213 0.21614563 0.24132967 0.8438401 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1152 is [False, False, True, False, True, False]
Current timestep = 1153. State = [[ 0.05444733 -0.0507767 ]]. Action = [[-0.026952    0.13060737  0.08492616  0.90915287]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1153 is [False, False, True, False, True, False]
Scene graph at timestep 1153 is [False, False, True, False, True, False]
State prediction error at timestep 1153 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1153 of 0
Current timestep = 1154. State = [[ 0.05242151 -0.03942943]]. Action = [[ 0.24785215  0.15716335 -0.11327437 -0.697849  ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1154 is [False, False, True, False, True, False]
Current timestep = 1155. State = [[ 0.05242151 -0.03942943]]. Action = [[ 0.21631664  0.08794954 -0.21898666 -0.37523162]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1155 is [False, False, True, False, True, False]
Scene graph at timestep 1155 is [False, False, True, False, True, False]
State prediction error at timestep 1155 is tensor(5.9343e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1155 of 0
Current timestep = 1156. State = [[ 0.05242151 -0.03942943]]. Action = [[ 0.2384597  -0.13300475 -0.00590543 -0.42482114]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1156 is [False, False, True, False, True, False]
Scene graph at timestep 1156 is [False, False, True, False, True, False]
State prediction error at timestep 1156 is tensor(9.6886e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1156 of 0
Current timestep = 1157. State = [[ 0.05242151 -0.03942943]]. Action = [[ 0.22113425  0.14780039  0.02954042 -0.26012373]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1157 is [False, False, True, False, True, False]
Scene graph at timestep 1157 is [False, False, True, False, True, False]
State prediction error at timestep 1157 is tensor(2.8207e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1157 of 0
Current timestep = 1158. State = [[ 0.05242151 -0.03942943]]. Action = [[ 0.14657977 -0.1872417   0.17793551  0.32650828]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1158 is [False, False, True, False, True, False]
Current timestep = 1159. State = [[ 0.05242151 -0.03942943]]. Action = [[ 0.10082603  0.21960008 -0.05068535 -0.75406504]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1159 is [False, False, True, False, True, False]
Scene graph at timestep 1159 is [False, False, True, False, True, False]
State prediction error at timestep 1159 is tensor(8.8980e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1159 of 0
Current timestep = 1160. State = [[ 0.05079914 -0.03408465]]. Action = [[-0.06333873  0.07364672  0.18719023  0.73309946]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1160 is [False, False, True, False, True, False]
Scene graph at timestep 1160 is [False, False, True, False, True, False]
State prediction error at timestep 1160 is tensor(4.7238e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1160 of 1
Current timestep = 1161. State = [[ 0.0473683  -0.02770433]]. Action = [[0.18183559 0.08069068 0.04403186 0.07837367]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1161 is [False, False, True, False, True, False]
Scene graph at timestep 1161 is [False, True, False, False, True, False]
State prediction error at timestep 1161 is tensor(4.6978e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1161 of 0
Current timestep = 1162. State = [[ 0.04686083 -0.02780266]]. Action = [[ 0.20732671  0.1341998  -0.23513113  0.09108317]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1162 is [False, True, False, False, True, False]
Current timestep = 1163. State = [[ 0.04686083 -0.02780266]]. Action = [[ 0.20067608  0.07753277 -0.20529874 -0.49823415]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1163 is [False, True, False, False, True, False]
Scene graph at timestep 1163 is [False, True, False, False, True, False]
State prediction error at timestep 1163 is tensor(3.1432e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1163 of 0
Current timestep = 1164. State = [[ 0.04751142 -0.03801278]]. Action = [[ 0.05265677 -0.18979593  0.00294808 -0.22207546]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1164 is [False, True, False, False, True, False]
Current timestep = 1165. State = [[ 0.04647205 -0.04428432]]. Action = [[ 0.00673112  0.07662192 -0.2111548  -0.37819576]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1165 is [False, True, False, False, True, False]
Scene graph at timestep 1165 is [False, True, False, False, True, False]
State prediction error at timestep 1165 is tensor(1.9319e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1165 of 1
Current timestep = 1166. State = [[ 0.0463146  -0.04344745]]. Action = [[ 0.19903618 -0.12882619  0.01825464 -0.6264675 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1166 is [False, True, False, False, True, False]
Scene graph at timestep 1166 is [False, True, False, False, True, False]
State prediction error at timestep 1166 is tensor(1.1606e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1166 of 0
Current timestep = 1167. State = [[ 0.0463146  -0.04344745]]. Action = [[ 0.23069331  0.183882   -0.04685098 -0.41334867]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1167 is [False, True, False, False, True, False]
Current timestep = 1168. State = [[ 0.0463146  -0.04344745]]. Action = [[ 0.1519016   0.23068386  0.19122773 -0.06277293]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1168 is [False, True, False, False, True, False]
Scene graph at timestep 1168 is [False, True, False, False, True, False]
State prediction error at timestep 1168 is tensor(6.8601e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1168 of 0
Current timestep = 1169. State = [[ 0.04628738 -0.04344207]]. Action = [[ 0.24490708 -0.01584192  0.24810058  0.72539616]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1169 is [False, True, False, False, True, False]
Scene graph at timestep 1169 is [False, True, False, False, True, False]
State prediction error at timestep 1169 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1169 of 0
Current timestep = 1170. State = [[ 0.04509116 -0.04503099]]. Action = [[-0.04611632 -0.05048323 -0.19762458  0.2978698 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1170 is [False, True, False, False, True, False]
Scene graph at timestep 1170 is [False, True, False, False, True, False]
State prediction error at timestep 1170 is tensor(7.8460e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1170 of 1
Current timestep = 1171. State = [[ 0.04182716 -0.04704466]]. Action = [[ 0.1438101  -0.13064066  0.2038205  -0.9700659 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1171 is [False, True, False, False, True, False]
Scene graph at timestep 1171 is [False, True, False, False, True, False]
State prediction error at timestep 1171 is tensor(7.0035e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1171 of 1
Current timestep = 1172. State = [[ 0.03853462 -0.03670795]]. Action = [[-0.12997173  0.1892485   0.19366458 -0.65975183]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1172 is [False, True, False, False, True, False]
Scene graph at timestep 1172 is [False, True, False, False, True, False]
State prediction error at timestep 1172 is tensor(5.1995e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1172 of 1
Current timestep = 1173. State = [[-0.22883369 -0.10957206]]. Action = [[-0.22636093  0.05524045  0.12497285 -0.6411243 ]]. Reward = [100.]
Curr episode timestep = 59
Scene graph at timestep 1173 is [False, True, False, False, True, False]
Scene graph at timestep 1173 is [True, False, False, False, True, False]
State prediction error at timestep 1173 is tensor(0.0389, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1173 of 1
Current timestep = 1174. State = [[-0.21355231 -0.120326  ]]. Action = [[ 0.22916842  0.03096968 -0.04675561  0.0439651 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1174 is [True, False, False, False, True, False]
Scene graph at timestep 1174 is [True, False, False, False, True, False]
State prediction error at timestep 1174 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1174 of 1
Current timestep = 1175. State = [[-0.1851825  -0.13345163]]. Action = [[ 0.13215494 -0.20606674 -0.15261908 -0.04652697]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1175 is [True, False, False, False, True, False]
Current timestep = 1176. State = [[-0.1648594 -0.1506615]]. Action = [[ 0.1637564  -0.08457647  0.14157951 -0.9048848 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1176 is [True, False, False, True, False, False]
Scene graph at timestep 1176 is [True, False, False, True, False, False]
State prediction error at timestep 1176 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1176 of 1
Current timestep = 1177. State = [[-0.1485509 -0.1579672]]. Action = [[ 0.02571902  0.00252104  0.09642202 -0.34968245]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1177 is [True, False, False, True, False, False]
Scene graph at timestep 1177 is [True, False, False, True, False, False]
State prediction error at timestep 1177 is tensor(7.6682e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1177 of 1
Current timestep = 1178. State = [[-0.1396138  -0.16368721]]. Action = [[ 0.12258932 -0.08175875 -0.1578534  -0.21034133]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1178 is [True, False, False, True, False, False]
Current timestep = 1179. State = [[-0.1355102  -0.18376411]]. Action = [[-0.18624552 -0.19204211  0.06871474 -0.36262023]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1179 is [True, False, False, True, False, False]
Current timestep = 1180. State = [[-0.1369469  -0.19678111]]. Action = [[ 0.17756706  0.0295696   0.13887015 -0.81556094]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1180 is [True, False, False, True, False, False]
Current timestep = 1181. State = [[-0.12837021 -0.18825336]]. Action = [[ 0.0117797   0.182051   -0.10027623 -0.7349088 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1181 is [True, False, False, True, False, False]
Current timestep = 1182. State = [[-0.117059   -0.18944907]]. Action = [[ 0.20340386 -0.21300213  0.00556558 -0.11612439]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1182 is [True, False, False, True, False, False]
Scene graph at timestep 1182 is [True, False, False, True, False, False]
State prediction error at timestep 1182 is tensor(2.8938e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1182 of -1
Current timestep = 1183. State = [[-0.09727481 -0.19855827]]. Action = [[-0.06043036  0.07091066 -0.14270613 -0.9836933 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1183 is [True, False, False, True, False, False]
Current timestep = 1184. State = [[-0.09626235 -0.19997516]]. Action = [[ 0.08895576 -0.08883189  0.10149986  0.23244274]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1184 is [True, False, False, True, False, False]
Scene graph at timestep 1184 is [True, False, False, True, False, False]
State prediction error at timestep 1184 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1184 of -1
Current timestep = 1185. State = [[-0.09169395 -0.19449896]]. Action = [[-0.10568281  0.1915667  -0.23617908 -0.44497246]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1185 is [True, False, False, True, False, False]
Scene graph at timestep 1185 is [True, False, False, True, False, False]
State prediction error at timestep 1185 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1185 of 1
Current timestep = 1186. State = [[-0.09155093 -0.18142939]]. Action = [[-0.02049643  0.07726544 -0.12841053  0.49829972]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1186 is [True, False, False, True, False, False]
Current timestep = 1187. State = [[-0.08496673 -0.1627232 ]]. Action = [[0.23987553 0.18311608 0.19173735 0.96348345]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1187 is [True, False, False, True, False, False]
Current timestep = 1188. State = [[-0.07298199 -0.13916579]]. Action = [[ 0.08922651  0.13045537 -0.19252138 -0.04107785]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1188 is [True, False, False, True, False, False]
Current timestep = 1189. State = [[-0.05583886 -0.11431856]]. Action = [[ 0.23378515  0.2119981  -0.14212957  0.59276056]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1189 is [True, False, False, True, False, False]
Current timestep = 1190. State = [[-0.20226544 -0.11048874]]. Action = [[ 0.24434033 -0.01596025 -0.04774648 -0.7026805 ]]. Reward = [100.]
Curr episode timestep = 16
Scene graph at timestep 1190 is [True, False, False, False, True, False]
Current timestep = 1191. State = [[-0.18593895 -0.118256  ]]. Action = [[ 0.20090991  0.08616561 -0.04036021 -0.3303525 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1191 is [True, False, False, False, True, False]
Scene graph at timestep 1191 is [True, False, False, False, True, False]
State prediction error at timestep 1191 is tensor(2.8996e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1191 of 1
Current timestep = 1192. State = [[-0.15585664 -0.11822829]]. Action = [[ 0.21854651 -0.03977528  0.22793317  0.59837174]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1192 is [True, False, False, False, True, False]
Scene graph at timestep 1192 is [True, False, False, False, True, False]
State prediction error at timestep 1192 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1192 of 1
Current timestep = 1193. State = [[-0.13433287 -0.12197311]]. Action = [[-0.11159524  0.00653443 -0.08953771 -0.38689446]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1193 is [True, False, False, False, True, False]
Scene graph at timestep 1193 is [True, False, False, False, True, False]
State prediction error at timestep 1193 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1193 of 1
Current timestep = 1194. State = [[-0.13004348 -0.12349956]]. Action = [[ 0.21126628 -0.05800936 -0.04663837 -0.16189003]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1194 is [True, False, False, False, True, False]
Scene graph at timestep 1194 is [True, False, False, False, True, False]
State prediction error at timestep 1194 is tensor(3.5364e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1194 of 1
Current timestep = 1195. State = [[-0.1185372  -0.11297864]]. Action = [[0.03182852 0.23425972 0.15093386 0.93971586]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1195 is [True, False, False, False, True, False]
Current timestep = 1196. State = [[-0.11138698 -0.09192089]]. Action = [[0.09833816 0.13074642 0.01901269 0.06052721]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1196 is [True, False, False, False, True, False]
Current timestep = 1197. State = [[-0.09224903 -0.08418989]]. Action = [[ 0.24677086 -0.06869619 -0.12836444 -0.89216465]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1197 is [True, False, False, False, True, False]
Current timestep = 1198. State = [[-0.06031939 -0.10019377]]. Action = [[ 0.16325524 -0.24452674  0.1753346   0.82456017]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1198 is [True, False, False, False, True, False]
Current timestep = 1199. State = [[-0.03695681 -0.11810038]]. Action = [[ 0.13583854 -0.06106304  0.03653374 -0.25927198]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1199 is [True, False, False, False, True, False]
Scene graph at timestep 1199 is [False, True, False, False, True, False]
State prediction error at timestep 1199 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1199 of 1
Current timestep = 1200. State = [[-0.01030287 -0.11790328]]. Action = [[ 0.216802    0.15043765  0.02347368 -0.6253117 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1200 is [False, True, False, False, True, False]
Current timestep = 1201. State = [[-0.15522766  0.1749712 ]]. Action = [[-0.18254647  0.13607076  0.23733613  0.06409407]]. Reward = [100.]
Curr episode timestep = 10
Scene graph at timestep 1201 is [False, True, False, False, True, False]
Scene graph at timestep 1201 is [True, False, False, False, False, True]
State prediction error at timestep 1201 is tensor(0.0529, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1201 of -1
Current timestep = 1202. State = [[-0.13361204  0.19279282]]. Action = [[ 0.00480169 -0.08527464  0.16691518  0.0876838 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1202 is [True, False, False, False, False, True]
Current timestep = 1203. State = [[-0.12775905  0.17386395]]. Action = [[ 0.09756312 -0.21147706 -0.06690314  0.10055673]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1203 is [True, False, False, False, False, True]
Current timestep = 1204. State = [[-0.12396073  0.15826933]]. Action = [[-0.00797467 -0.02424009  0.23047477 -0.9308048 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1204 is [True, False, False, False, False, True]
Scene graph at timestep 1204 is [True, False, False, False, False, True]
State prediction error at timestep 1204 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1204 of 1
Current timestep = 1205. State = [[-0.11909877  0.14231446]]. Action = [[ 0.06059748 -0.18693413  0.1763561  -0.95998603]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1205 is [True, False, False, False, False, True]
Scene graph at timestep 1205 is [True, False, False, False, False, True]
State prediction error at timestep 1205 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1205 of 1
Current timestep = 1206. State = [[-0.11764035  0.12360856]]. Action = [[-0.18173245 -0.0920096  -0.22761782 -0.65336293]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1206 is [True, False, False, False, False, True]
Scene graph at timestep 1206 is [True, False, False, False, True, False]
State prediction error at timestep 1206 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1206 of 0
Current timestep = 1207. State = [[-0.11563021  0.10307512]]. Action = [[ 0.11057597 -0.22075166 -0.06271043  0.4669224 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1207 is [True, False, False, False, True, False]
Scene graph at timestep 1207 is [True, False, False, False, True, False]
State prediction error at timestep 1207 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1207 of 1
Current timestep = 1208. State = [[-0.10930376  0.07885635]]. Action = [[ 0.08746418 -0.1127246   0.01597658  0.6241915 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1208 is [True, False, False, False, True, False]
Scene graph at timestep 1208 is [True, False, False, False, True, False]
State prediction error at timestep 1208 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1208 of 0
Current timestep = 1209. State = [[-0.09814092  0.0583399 ]]. Action = [[ 0.17401129 -0.1655919   0.2019057   0.95541453]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1209 is [True, False, False, False, True, False]
Current timestep = 1210. State = [[-0.07882579  0.03517717]]. Action = [[ 0.16683859 -0.1677531  -0.21708176 -0.01607877]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1210 is [True, False, False, False, True, False]
Scene graph at timestep 1210 is [True, False, False, False, True, False]
State prediction error at timestep 1210 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1210 of 1
Current timestep = 1211. State = [[-0.06305299  0.02131397]]. Action = [[ 0.002435    0.06232056 -0.14109372 -0.09415126]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1211 is [True, False, False, False, True, False]
Current timestep = 1212. State = [[-0.06306846  0.02860841]]. Action = [[ 0.00775474  0.09776461 -0.22817546 -0.86065537]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1212 is [True, False, False, False, True, False]
Current timestep = 1213. State = [[-0.0623037   0.03386605]]. Action = [[-0.00245279  0.0087944   0.19999915  0.9553653 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1213 is [True, False, False, False, True, False]
Scene graph at timestep 1213 is [True, False, False, False, True, False]
State prediction error at timestep 1213 is tensor(9.4100e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1213 of 1
Current timestep = 1214. State = [[-0.05988529  0.03252335]]. Action = [[ 0.01956683 -0.0889675   0.21799469 -0.62340224]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1214 is [True, False, False, False, True, False]
Current timestep = 1215. State = [[-0.05421763  0.03608746]]. Action = [[ 0.12913841  0.13266927 -0.0666603  -0.6051398 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1215 is [True, False, False, False, True, False]
Current timestep = 1216. State = [[-0.04198885  0.02860327]]. Action = [[-0.10412872 -0.2407409   0.12647891 -0.68950754]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1216 is [True, False, False, False, True, False]
Current timestep = 1217. State = [[-0.03706506  0.02233737]]. Action = [[ 0.18234456  0.10356915 -0.02666248 -0.16921479]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1217 is [False, True, False, False, True, False]
Scene graph at timestep 1217 is [False, True, False, False, True, False]
State prediction error at timestep 1217 is tensor(9.5202e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1217 of 1
Current timestep = 1218. State = [[-0.1689776  -0.05683476]]. Action = [[ 0.1344375  -0.14643562 -0.06817885  0.4655106 ]]. Reward = [100.]
Curr episode timestep = 16
Scene graph at timestep 1218 is [False, True, False, False, True, False]
Scene graph at timestep 1218 is [True, False, False, False, True, False]
State prediction error at timestep 1218 is tensor(0.0133, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1218 of 1
Current timestep = 1219. State = [[-0.14364298 -0.06378783]]. Action = [[ 0.22432259 -0.01444508 -0.11568615  0.18281353]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1219 is [True, False, False, False, True, False]
Scene graph at timestep 1219 is [True, False, False, False, True, False]
State prediction error at timestep 1219 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1219 of 1
Current timestep = 1220. State = [[-0.11089373 -0.07745653]]. Action = [[ 0.20776904 -0.18815622 -0.01377727 -0.5892479 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1220 is [True, False, False, False, True, False]
Current timestep = 1221. State = [[-0.08175576 -0.095596  ]]. Action = [[ 0.24345013 -0.08081353 -0.01573245  0.3470416 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1221 is [True, False, False, False, True, False]
Current timestep = 1222. State = [[-0.0655206  -0.10153761]]. Action = [[-0.19557182  0.0525122  -0.04742025 -0.9528096 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1222 is [True, False, False, False, True, False]
Current timestep = 1223. State = [[-0.06863034 -0.09361812]]. Action = [[-0.02981904  0.14336923 -0.21208082  0.3941927 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1223 is [True, False, False, False, True, False]
Scene graph at timestep 1223 is [True, False, False, False, True, False]
State prediction error at timestep 1223 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1223 of 1
Current timestep = 1224. State = [[-0.06515947 -0.08753043]]. Action = [[ 0.22211304 -0.0779321   0.14236504  0.22032917]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1224 is [True, False, False, False, True, False]
Current timestep = 1225. State = [[-0.05469948 -0.07676729]]. Action = [[ 0.08210057  0.2261937  -0.05360115  0.6574514 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1225 is [True, False, False, False, True, False]
Scene graph at timestep 1225 is [True, False, False, False, True, False]
State prediction error at timestep 1225 is tensor(4.3121e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1225 of 1
Current timestep = 1226. State = [[-0.04432575 -0.07446939]]. Action = [[-0.11370143 -0.19267209  0.07405913 -0.25137913]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1226 is [True, False, False, False, True, False]
Current timestep = 1227. State = [[-0.04562597 -0.07650945]]. Action = [[ 0.0659073   0.12483338 -0.14675723 -0.8593993 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1227 is [False, True, False, False, True, False]
Current timestep = 1228. State = [[-0.04734326 -0.07096897]]. Action = [[-0.1665173   0.03197417  0.18862885 -0.9748497 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1228 is [False, True, False, False, True, False]
Scene graph at timestep 1228 is [False, True, False, False, True, False]
State prediction error at timestep 1228 is tensor(3.4231e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1228 of 0
Current timestep = 1229. State = [[-0.05385224 -0.0602755 ]]. Action = [[-0.13567232  0.11195779 -0.17967892  0.04534483]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1229 is [False, True, False, False, True, False]
Current timestep = 1230. State = [[-0.06695794 -0.03819486]]. Action = [[-0.09634468  0.2284987   0.02604464  0.13130522]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1230 is [True, False, False, False, True, False]
Current timestep = 1231. State = [[-0.07435984 -0.01811559]]. Action = [[0.15350604 0.02749187 0.05843371 0.93511856]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1231 is [True, False, False, False, True, False]
Current timestep = 1232. State = [[-0.07443945 -0.0124992 ]]. Action = [[-0.02361812  0.03489733 -0.17028545 -0.9396425 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1232 is [True, False, False, False, True, False]
Current timestep = 1233. State = [[-0.07219106 -0.00748849]]. Action = [[ 0.1062575   0.03101867 -0.20821346 -0.96844167]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1233 is [True, False, False, False, True, False]
Current timestep = 1234. State = [[-0.05956824 -0.01078585]]. Action = [[ 0.22220957 -0.11429615  0.17648804  0.40845037]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1234 is [True, False, False, False, True, False]
Current timestep = 1235. State = [[-0.05022181 -0.00524539]]. Action = [[-0.19033931  0.16028619  0.07220173 -0.3282891 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1235 is [True, False, False, False, True, False]
Scene graph at timestep 1235 is [True, False, False, False, True, False]
State prediction error at timestep 1235 is tensor(1.3762e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1235 of 0
Current timestep = 1236. State = [[-0.05237198  0.00351472]]. Action = [[ 0.07212645  0.01538268 -0.2260105  -0.9758367 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1236 is [True, False, False, False, True, False]
Scene graph at timestep 1236 is [True, False, False, False, True, False]
State prediction error at timestep 1236 is tensor(4.0146e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1236 of 0
Current timestep = 1237. State = [[-0.0458303   0.00431429]]. Action = [[ 0.19026455 -0.02523375 -0.1799079  -0.47146273]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1237 is [True, False, False, False, True, False]
Scene graph at timestep 1237 is [False, True, False, False, True, False]
State prediction error at timestep 1237 is tensor(1.0269e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1237 of 1
Current timestep = 1238. State = [[-0.15680549 -0.18571228]]. Action = [[ 0.13193494 -0.12391409 -0.20639017  0.0969578 ]]. Reward = [100.]
Curr episode timestep = 19
Scene graph at timestep 1238 is [False, True, False, False, True, False]
Scene graph at timestep 1238 is [True, False, False, True, False, False]
State prediction error at timestep 1238 is tensor(0.0257, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1238 of 1
Current timestep = 1239. State = [[-0.13124368 -0.20347251]]. Action = [[ 0.23793703  0.05378178  0.06794032 -0.98644847]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1239 is [True, False, False, True, False, False]
Scene graph at timestep 1239 is [True, False, False, True, False, False]
State prediction error at timestep 1239 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1239 of 1
Current timestep = 1240. State = [[-0.10654931 -0.2106872 ]]. Action = [[ 0.01854575 -0.11589059  0.18419778 -0.18400544]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1240 is [True, False, False, True, False, False]
Scene graph at timestep 1240 is [True, False, False, True, False, False]
State prediction error at timestep 1240 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1240 of -1
Current timestep = 1241. State = [[-0.09737503 -0.2062545 ]]. Action = [[ 0.18177006  0.16489077 -0.20597838  0.8411218 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1241 is [True, False, False, True, False, False]
Current timestep = 1242. State = [[-0.08682463 -0.19402003]]. Action = [[-0.19459082  0.13318443 -0.04607649  0.5061923 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1242 is [True, False, False, True, False, False]
Scene graph at timestep 1242 is [True, False, False, True, False, False]
State prediction error at timestep 1242 is tensor(2.9150e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1242 of 1
Current timestep = 1243. State = [[-0.09375998 -0.17433229]]. Action = [[-0.16066018  0.16477686  0.2484917   0.8328899 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1243 is [True, False, False, True, False, False]
Scene graph at timestep 1243 is [True, False, False, True, False, False]
State prediction error at timestep 1243 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1243 of 1
Current timestep = 1244. State = [[-0.11268438 -0.14928061]]. Action = [[-0.21435848  0.2081798   0.06434208 -0.822683  ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1244 is [True, False, False, True, False, False]
Scene graph at timestep 1244 is [True, False, False, True, False, False]
State prediction error at timestep 1244 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1244 of -1
Current timestep = 1245. State = [[-0.13078608 -0.1225749 ]]. Action = [[ 0.17573938  0.10353738 -0.2164184   0.9002631 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1245 is [True, False, False, True, False, False]
Current timestep = 1246. State = [[-0.12913182 -0.10877593]]. Action = [[-0.04816213  0.0946894   0.0027197  -0.8366657 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1246 is [True, False, False, False, True, False]
Scene graph at timestep 1246 is [True, False, False, False, True, False]
State prediction error at timestep 1246 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1246 of 1
Current timestep = 1247. State = [[-0.12862   -0.1001336]]. Action = [[ 0.07246512 -0.00094202  0.17820716  0.317729  ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1247 is [True, False, False, False, True, False]
Current timestep = 1248. State = [[-0.12707625 -0.08954591]]. Action = [[-0.00725263  0.15698427  0.011273    0.85760903]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1248 is [True, False, False, False, True, False]
Current timestep = 1249. State = [[-0.12631296 -0.07452303]]. Action = [[ 0.0198288   0.064527    0.13397098 -0.34853864]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1249 is [True, False, False, False, True, False]
