Current timestep = 0. State = [[-0.2302401  -0.01642315]]. Action = [[-0.13563247  0.00684047  0.18864083 -0.05631965]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Current timestep = 1. State = [[-0.22520539 -0.01531108]]. Action = [[0.2306673  0.04798689 0.24686849 0.59096146]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
Current timestep = 2. State = [[-0.21402507 -0.0010169 ]]. Action = [[ 0.04327393  0.21870226  0.18179056 -0.186162  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
Current timestep = 3. State = [[-0.19854793  0.02794108]]. Action = [[0.23366672 0.23120478 0.01779771 0.14667618]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0176, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3 of 1
Current timestep = 4. State = [[-0.17553861  0.04879184]]. Action = [[-0.10859685 -0.0322614  -0.18620794  0.05613935]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
Current timestep = 5. State = [[-0.17880754  0.05036706]]. Action = [[-0.08203238  0.02030113  0.14620349  0.4113084 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
Current timestep = 6. State = [[-0.189712   0.0544208]]. Action = [[-0.16614698  0.0241929   0.17820364  0.92791295]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
Current timestep = 7. State = [[-0.20548777  0.06260568]]. Action = [[-0.11636505  0.07286415  0.1374996   0.97221196]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0072, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.22139767  0.06470348]]. Action = [[ 0.08170962 -0.06017564  0.18052667  0.28995907]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
Current timestep = 9. State = [[-0.21603633  0.04992135]]. Action = [[ 0.114804   -0.16841723  0.22267386 -0.7321922 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
Current timestep = 10. State = [[-0.21476236  0.04727303]]. Action = [[-0.05630493  0.15779722 -0.20095034 -0.46306658]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
Current timestep = 11. State = [[-0.2095598  0.0390049]]. Action = [[ 0.13166729 -0.24295165  0.02042067  0.2686509 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
Current timestep = 12. State = [[-0.20263247  0.01233704]]. Action = [[-0.05856092 -0.22115591  0.07110456 -0.44633442]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
Current timestep = 13. State = [[-0.21156403 -0.018544  ]]. Action = [[-0.23628674 -0.23522408 -0.00662868  0.05990136]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of 0
Current timestep = 14. State = [[-0.22570316 -0.03354025]]. Action = [[-0.00425683  0.20335332 -0.10710251 -0.545518  ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
Current timestep = 15. State = [[-0.22295232 -0.01748312]]. Action = [[ 0.1746464   0.07616612 -0.01935883 -0.9444641 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
Current timestep = 16. State = [[-0.2069962  -0.01826386]]. Action = [[ 0.21947718 -0.17541528 -0.02608572  0.2749108 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.1916016  -0.02812334]]. Action = [[-0.22877751 -0.00890011  0.12613195 -0.36517817]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
Scene graph at timestep 17 is [True, False, False, False, True, False]
State prediction error at timestep 17 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 17 of -1
Current timestep = 18. State = [[-0.2127724  -0.02077867]]. Action = [[-0.2400354   0.21953619  0.07725868  0.27630484]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
Scene graph at timestep 18 is [True, False, False, False, True, False]
State prediction error at timestep 18 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 18 of -1
Current timestep = 19. State = [[-0.24170358 -0.0020338 ]]. Action = [[-0.21413837 -0.03505425  0.22786492  0.3156774 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 19 of -1
Current timestep = 20. State = [[-0.26619807 -0.00387094]]. Action = [[-0.12609808 -0.14327037 -0.00250612  0.711195  ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
Current timestep = 21. State = [[-0.26647604 -0.00386856]]. Action = [[-0.11066613  0.1905114  -0.13680679  0.39143598]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
Current timestep = 22. State = [[-0.26667443 -0.00387271]]. Action = [[-0.14286728  0.2466386   0.15630558  0.8306596 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
Scene graph at timestep 22 is [True, False, False, False, True, False]
State prediction error at timestep 22 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 22 of -1
Current timestep = 23. State = [[-0.2666328  -0.00389042]]. Action = [[-0.20213917 -0.16438861 -0.20809767  0.5612836 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
Current timestep = 24. State = [[-0.2666328  -0.00389042]]. Action = [[-0.17437506 -0.22995372  0.05623287  0.33143735]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
Current timestep = 25. State = [[-0.26045018 -0.00491581]]. Action = [[ 0.22159278 -0.01907992  0.18700862  0.41229784]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
Current timestep = 26. State = [[-0.24360171 -0.01978014]]. Action = [[ 0.16862974 -0.22414377 -0.11410527  0.24765992]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
Current timestep = 27. State = [[-0.22884771 -0.02304257]]. Action = [[0.06384003 0.22693112 0.19409236 0.45548463]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 27 of 1
Current timestep = 28. State = [[-0.22388847 -0.01282401]]. Action = [[-0.18916512 -0.04525571 -0.19764918 -0.45668542]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
Scene graph at timestep 28 is [True, False, False, False, True, False]
State prediction error at timestep 28 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 28 of -1
Current timestep = 29. State = [[-0.22319418 -0.02447924]]. Action = [[ 0.23775297 -0.17058823 -0.19018938 -0.59945405]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
Current timestep = 30. State = [[-0.20373525 -0.02413389]]. Action = [[ 0.23178643  0.19840449 -0.13520558  0.9587283 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
Scene graph at timestep 30 is [True, False, False, False, True, False]
State prediction error at timestep 30 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 30 of 1
Current timestep = 31. State = [[-0.18187296 -0.00867419]]. Action = [[-0.10816067  0.06412536  0.13337222  0.4544413 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
Scene graph at timestep 31 is [True, False, False, False, True, False]
State prediction error at timestep 31 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 31 of 0
Current timestep = 32. State = [[-0.19362986  0.00434767]]. Action = [[-0.23934689  0.1106801  -0.11704341 -0.38918817]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
Current timestep = 33. State = [[-0.20379232  0.02516681]]. Action = [[0.17799342 0.20181966 0.12296379 0.82806516]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
Current timestep = 34. State = [[-0.193581    0.04574551]]. Action = [[ 0.19494534  0.12948573 -0.24536437  0.63322425]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
Scene graph at timestep 34 is [True, False, False, False, True, False]
State prediction error at timestep 34 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of 0
Current timestep = 35. State = [[-0.1814652  0.0677395]]. Action = [[-0.15274888  0.11131844 -0.02086623 -0.975273  ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
Scene graph at timestep 35 is [True, False, False, False, True, False]
State prediction error at timestep 35 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 35 of -1
Current timestep = 36. State = [[-0.18474963  0.07110505]]. Action = [[ 0.03236598 -0.14174439 -0.03629796 -0.54438925]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
Scene graph at timestep 36 is [True, False, False, False, True, False]
State prediction error at timestep 36 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 36 of 1
Current timestep = 37. State = [[-0.18204871  0.06385078]]. Action = [[ 0.12469766  0.02223226  0.17504656 -0.02105933]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
Scene graph at timestep 37 is [True, False, False, False, True, False]
State prediction error at timestep 37 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 37 of 0
Current timestep = 38. State = [[-0.17322457  0.05606364]]. Action = [[ 0.1278637  -0.1412601  -0.03068873  0.12469673]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
Scene graph at timestep 38 is [True, False, False, False, True, False]
State prediction error at timestep 38 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.15775998  0.04044468]]. Action = [[ 0.02817392 -0.07546443  0.06629851 -0.9159353 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
Current timestep = 40. State = [[-0.15912777  0.02216681]]. Action = [[-0.18316457 -0.21122554 -0.05309981 -0.757369  ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
Scene graph at timestep 40 is [True, False, False, False, True, False]
State prediction error at timestep 40 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 40 of 1
Current timestep = 41. State = [[-0.16260438 -0.00434787]]. Action = [[ 0.0800139  -0.14154103  0.03419191 -0.73462987]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of 0
Current timestep = 42. State = [[-0.15561989 -0.02068974]]. Action = [[ 0.14627182 -0.05868119  0.05354896  0.6952684 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.13916521 -0.03019951]]. Action = [[ 0.15498787 -0.06335908  0.1855312  -0.21315467]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
Scene graph at timestep 43 is [True, False, False, False, True, False]
State prediction error at timestep 43 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.11913471 -0.04173032]]. Action = [[ 0.14277649 -0.10745275  0.1291011  -0.41374302]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 44 of 1
Current timestep = 45. State = [[-0.09405717 -0.05942527]]. Action = [[ 0.2307002  -0.14097148  0.15561026  0.5224192 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
Current timestep = 46. State = [[-0.08057062 -0.06268946]]. Action = [[-0.20027721  0.13908005  0.05995154 -0.94390893]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
Current timestep = 47. State = [[-0.07820043 -0.04977837]]. Action = [[ 0.16991282  0.1357134  -0.05248536 -0.34649485]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
Current timestep = 48. State = [[-0.07906647 -0.04659625]]. Action = [[-0.22513705 -0.10583302  0.02248242  0.02492571]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
Scene graph at timestep 48 is [True, False, False, False, True, False]
State prediction error at timestep 48 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 48 of 1
Current timestep = 49. State = [[-0.08503397 -0.05626684]]. Action = [[-0.06054664 -0.10351276  0.19315988 -0.91088694]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
Scene graph at timestep 49 is [True, False, False, False, True, False]
State prediction error at timestep 49 is tensor(0.0084, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 49 of -1
Current timestep = 50. State = [[-0.08730777 -0.05188491]]. Action = [[ 0.22886813  0.24009144 -0.09602299  0.88120985]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
Scene graph at timestep 50 is [True, False, False, False, True, False]
State prediction error at timestep 50 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 50 of 1
Current timestep = 51. State = [[-0.07280883 -0.0425096 ]]. Action = [[ 0.1846444  -0.16657403 -0.18771    -0.06457692]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 51 of 1
Current timestep = 52. State = [[-0.04582104 -0.05702269]]. Action = [[ 0.24042696 -0.07421839  0.20814347 -0.45733833]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
Scene graph at timestep 52 is [False, True, False, False, True, False]
State prediction error at timestep 52 is tensor(0.0088, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.24112749 -0.22436449]]. Action = [[-0.18398532 -0.16305599 -0.10094085 -0.5045942 ]]. Reward = [100.]
Curr episode timestep = 53
Scene graph at timestep 53 is [False, True, False, False, True, False]
Scene graph at timestep 53 is [True, False, False, True, False, False]
State prediction error at timestep 53 is tensor(0.0320, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 53 of 0
Current timestep = 54. State = [[-0.238189   -0.24883363]]. Action = [[-0.23666845  0.1957218  -0.19617735  0.6126611 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 54 is [True, False, False, True, False, False]
Scene graph at timestep 54 is [True, False, False, True, False, False]
State prediction error at timestep 54 is tensor(0.0312, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 54 of -1
Current timestep = 55. State = [[-0.22926302 -0.23693277]]. Action = [[ 0.17323524  0.23566145 -0.04797053 -0.1884619 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 55 is [True, False, False, True, False, False]
Scene graph at timestep 55 is [True, False, False, True, False, False]
State prediction error at timestep 55 is tensor(0.0321, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.2180603 -0.223299 ]]. Action = [[-0.13033158 -0.01616064  0.11623961 -0.39872015]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 56 is [True, False, False, True, False, False]
Current timestep = 57. State = [[-0.2218405  -0.21706991]]. Action = [[-0.03992903  0.14475694  0.05915663 -0.05902064]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 57 is [True, False, False, True, False, False]
Scene graph at timestep 57 is [True, False, False, True, False, False]
State prediction error at timestep 57 is tensor(0.0280, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 57 of 0
Current timestep = 58. State = [[-0.21651462 -0.19467188]]. Action = [[ 0.24020344  0.21100003  0.00517878 -0.5777483 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 58 is [True, False, False, True, False, False]
Scene graph at timestep 58 is [True, False, False, True, False, False]
State prediction error at timestep 58 is tensor(0.0207, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 58 of 1
Current timestep = 59. State = [[-0.20567769 -0.17609434]]. Action = [[-0.10679717 -0.02940309 -0.09631062 -0.4723909 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 59 is [True, False, False, True, False, False]
Scene graph at timestep 59 is [True, False, False, True, False, False]
State prediction error at timestep 59 is tensor(0.0170, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 59 of 0
Current timestep = 60. State = [[-0.20328543 -0.16997199]]. Action = [[ 0.1546151   0.13369852  0.07995191 -0.03758872]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 60 is [True, False, False, True, False, False]
Current timestep = 61. State = [[-0.18968998 -0.15970589]]. Action = [[0.20279396 0.01488885 0.09626716 0.6314137 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 61 is [True, False, False, True, False, False]
Current timestep = 62. State = [[-0.17935935 -0.16647984]]. Action = [[-0.16869333 -0.14756669 -0.23375368  0.3257773 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 62 is [True, False, False, True, False, False]
Scene graph at timestep 62 is [True, False, False, True, False, False]
State prediction error at timestep 62 is tensor(0.0120, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 62 of 1
Current timestep = 63. State = [[-0.17850766 -0.1774319 ]]. Action = [[ 0.13981575 -0.06265756  0.06659502 -0.5142542 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 63 is [True, False, False, True, False, False]
Scene graph at timestep 63 is [True, False, False, True, False, False]
State prediction error at timestep 63 is tensor(0.0146, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 63 of 1
Current timestep = 64. State = [[-0.17584972 -0.18125398]]. Action = [[-0.15662205  0.0232954   0.06064597 -0.4351743 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 64 is [True, False, False, True, False, False]
Scene graph at timestep 64 is [True, False, False, True, False, False]
State prediction error at timestep 64 is tensor(0.0153, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 64 of 0
Current timestep = 65. State = [[-0.18288945 -0.19684266]]. Action = [[ 0.04985574 -0.23102683  0.14297825  0.22902727]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 65 is [True, False, False, True, False, False]
Scene graph at timestep 65 is [True, False, False, True, False, False]
State prediction error at timestep 65 is tensor(0.0174, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 65 of -1
Current timestep = 66. State = [[-0.18543443 -0.20573112]]. Action = [[-0.01799017  0.12041515 -0.21261589  0.95217323]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 66 is [True, False, False, True, False, False]
Current timestep = 67. State = [[-0.18143511 -0.21172394]]. Action = [[ 0.11098897 -0.20356919 -0.03712869  0.3570392 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 67 is [True, False, False, True, False, False]
Scene graph at timestep 67 is [True, False, False, True, False, False]
State prediction error at timestep 67 is tensor(0.0178, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of 0
Current timestep = 68. State = [[-0.17399836 -0.22232181]]. Action = [[ 0.04844588 -0.03964341  0.24264014  0.40920603]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 68 is [True, False, False, True, False, False]
Current timestep = 69. State = [[-0.17648429 -0.23356333]]. Action = [[-0.21579462 -0.08039352  0.22700226  0.10720301]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 69 is [True, False, False, True, False, False]
Scene graph at timestep 69 is [True, False, False, True, False, False]
State prediction error at timestep 69 is tensor(0.0214, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of -1
Current timestep = 70. State = [[-0.19744688 -0.25725693]]. Action = [[-0.20961249 -0.19682994 -0.0654873  -0.6579367 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 70 is [True, False, False, True, False, False]
Current timestep = 71. State = [[-0.20931794 -0.27062395]]. Action = [[ 0.04946566 -0.01943059  0.1054047  -0.59811205]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 71 is [True, False, False, True, False, False]
Scene graph at timestep 71 is [True, False, False, True, False, False]
State prediction error at timestep 71 is tensor(0.0268, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 71 of -1
Current timestep = 72. State = [[-0.20223254 -0.26113957]]. Action = [[ 0.20030957  0.22732317  0.19990784 -0.8723603 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 72 is [True, False, False, True, False, False]
Scene graph at timestep 72 is [True, False, False, True, False, False]
State prediction error at timestep 72 is tensor(0.0204, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 72 of 1
Current timestep = 73. State = [[-0.19527929 -0.2507341 ]]. Action = [[-0.13242833 -0.08112285 -0.20491435 -0.35008907]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 73 is [True, False, False, True, False, False]
Current timestep = 74. State = [[-0.19072989 -0.26056087]]. Action = [[ 0.22598934 -0.12885232  0.24298829 -0.712595  ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 74 is [True, False, False, True, False, False]
Current timestep = 75. State = [[-0.18165918 -0.2651243 ]]. Action = [[-0.05274557  0.07779372  0.14033282  0.96593285]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 75 is [True, False, False, True, False, False]
Scene graph at timestep 75 is [True, False, False, True, False, False]
State prediction error at timestep 75 is tensor(0.0154, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of 0
Current timestep = 76. State = [[-0.18552507 -0.27272794]]. Action = [[-0.11914642 -0.13879399 -0.19983236  0.8585479 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 76 is [True, False, False, True, False, False]
Scene graph at timestep 76 is [True, False, False, True, False, False]
State prediction error at timestep 76 is tensor(0.0177, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 76 of -1
Current timestep = 77. State = [[-0.19205017 -0.28315738]]. Action = [[-0.09992017 -0.187494   -0.13474736  0.10103762]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 77 is [True, False, False, True, False, False]
Current timestep = 78. State = [[-0.19208737 -0.28321022]]. Action = [[ 0.10121846 -0.22552676 -0.18946151 -0.827278  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 78 is [True, False, False, True, False, False]
Scene graph at timestep 78 is [True, False, False, True, False, False]
State prediction error at timestep 78 is tensor(0.0210, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 78 of -1
Current timestep = 79. State = [[-0.19303073 -0.28833935]]. Action = [[ 0.05356506 -0.11263633  0.23578697 -0.08035326]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 79 is [True, False, False, True, False, False]
Scene graph at timestep 79 is [True, False, False, True, False, False]
State prediction error at timestep 79 is tensor(0.0247, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.19430883 -0.2936532 ]]. Action = [[ 0.22747576 -0.11172134 -0.18103825  0.4378047 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 80 is [True, False, False, True, False, False]
Current timestep = 81. State = [[-0.19430883 -0.2936532 ]]. Action = [[ 0.19536537 -0.17733751 -0.03748259  0.03791666]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 81 is [True, False, False, True, False, False]
Current timestep = 82. State = [[-0.19430883 -0.2936532 ]]. Action = [[ 0.17978102 -0.10388781  0.18328482 -0.6679319 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 82 is [True, False, False, True, False, False]
Scene graph at timestep 82 is [True, False, False, True, False, False]
State prediction error at timestep 82 is tensor(0.0202, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 82 of -1
Current timestep = 83. State = [[-0.19207677 -0.28747845]]. Action = [[ 0.02295265  0.15878093 -0.01608121 -0.8310018 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 83 is [True, False, False, True, False, False]
Scene graph at timestep 83 is [True, False, False, True, False, False]
State prediction error at timestep 83 is tensor(0.0188, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 83 of 1
Current timestep = 84. State = [[-0.1883305 -0.2820385]]. Action = [[-0.07488    -0.06214806 -0.00863898 -0.21845531]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 84 is [True, False, False, True, False, False]
Current timestep = 85. State = [[-0.18548389 -0.28887013]]. Action = [[ 1.8007714e-01 -1.1335322e-01  2.2566319e-04 -6.1062461e-01]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 85 is [True, False, False, True, False, False]
Scene graph at timestep 85 is [True, False, False, True, False, False]
State prediction error at timestep 85 is tensor(0.0180, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 85 of 0
Current timestep = 86. State = [[-0.17044933 -0.28497753]]. Action = [[ 0.13197911  0.20285064  0.03897136 -0.6065089 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 86 is [True, False, False, True, False, False]
Current timestep = 87. State = [[-0.16077363 -0.27501613]]. Action = [[-0.02325569 -0.15431939  0.12833008  0.3088671 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 87 is [True, False, False, True, False, False]
Current timestep = 88. State = [[-0.16371682 -0.27692372]]. Action = [[-0.21442144 -0.01638246  0.07242662 -0.49112016]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 88 is [True, False, False, True, False, False]
Scene graph at timestep 88 is [True, False, False, True, False, False]
State prediction error at timestep 88 is tensor(0.0164, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.18187198 -0.2703671 ]]. Action = [[-0.1841054   0.18497339  0.17571613  0.5692725 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 89 is [True, False, False, True, False, False]
Scene graph at timestep 89 is [True, False, False, True, False, False]
State prediction error at timestep 89 is tensor(0.0103, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 89 of 1
Current timestep = 90. State = [[-0.19932672 -0.2537965 ]]. Action = [[-0.09966743  0.07332376  0.19318232 -0.8056023 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 90 is [True, False, False, True, False, False]
Scene graph at timestep 90 is [True, False, False, True, False, False]
State prediction error at timestep 90 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of -1
Current timestep = 91. State = [[-0.21516335 -0.25371477]]. Action = [[-0.09572239 -0.11570778 -0.06146485  0.24406672]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 91 is [True, False, False, True, False, False]
Current timestep = 92. State = [[-0.21722595 -0.2501036 ]]. Action = [[ 0.13074505  0.16024044 -0.17495377  0.37395024]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 92 is [True, False, False, True, False, False]
Scene graph at timestep 92 is [True, False, False, True, False, False]
State prediction error at timestep 92 is tensor(0.0084, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 92 of 1
Current timestep = 93. State = [[-0.20475522 -0.23094308]]. Action = [[ 0.21418154  0.08309922 -0.05325645 -0.7831784 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 93 is [True, False, False, True, False, False]
Scene graph at timestep 93 is [True, False, False, True, False, False]
State prediction error at timestep 93 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 93 of 1
Current timestep = 94. State = [[-0.19446255 -0.2216638 ]]. Action = [[-0.07699129 -0.00623578 -0.14334354  0.72390914]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 94 is [True, False, False, True, False, False]
Scene graph at timestep 94 is [True, False, False, True, False, False]
State prediction error at timestep 94 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.18660136 -0.22484556]]. Action = [[ 0.21786177 -0.08110359  0.24771091  0.27636147]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 95 is [True, False, False, True, False, False]
Current timestep = 96. State = [[-0.16213001 -0.23406686]]. Action = [[ 0.24552706 -0.10043333  0.14361668  0.9925604 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 96 is [True, False, False, True, False, False]
Current timestep = 97. State = [[-0.1450703  -0.24044378]]. Action = [[-0.03655764 -0.01347499 -0.07527587 -0.703162  ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 97 is [True, False, False, True, False, False]
Scene graph at timestep 97 is [True, False, False, True, False, False]
State prediction error at timestep 97 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 97 of 1
Current timestep = 98. State = [[-0.14541583 -0.25424555]]. Action = [[-0.00433245 -0.21438384 -0.07548659 -0.24880266]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 98 is [True, False, False, True, False, False]
Current timestep = 99. State = [[-0.15332963 -0.28225282]]. Action = [[-0.23550244 -0.15518567  0.02285358 -0.44342119]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 99 is [True, False, False, True, False, False]
Current timestep = 100. State = [[-0.16254632 -0.29832444]]. Action = [[-0.17143176 -0.15067233 -0.21390985 -0.71160316]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 100 is [True, False, False, True, False, False]
Current timestep = 101. State = [[-0.16363029 -0.30008367]]. Action = [[ 0.05615398 -0.17884365 -0.1753709   0.78590035]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 101 is [True, False, False, True, False, False]
Current timestep = 102. State = [[-0.16828287 -0.2940264 ]]. Action = [[-0.10985592  0.16558963 -0.0533689  -0.64112604]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 102 is [True, False, False, True, False, False]
Scene graph at timestep 102 is [True, False, False, True, False, False]
State prediction error at timestep 102 is tensor(0.0090, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 102 of -1
Current timestep = 103. State = [[-0.1735368  -0.28380308]]. Action = [[0.03518826 0.05227318 0.19124153 0.8230524 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 103 is [True, False, False, True, False, False]
Current timestep = 104. State = [[-0.17265992 -0.28044403]]. Action = [[ 0.15285748 -0.19225469 -0.07947972  0.0707202 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 104 is [True, False, False, True, False, False]
Current timestep = 105. State = [[-0.17369923 -0.27836272]]. Action = [[-0.06547123  0.03982463  0.18631166 -0.85900277]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 105 is [True, False, False, True, False, False]
Current timestep = 106. State = [[-0.1688541  -0.26382378]]. Action = [[ 0.19403401  0.17602992 -0.05330861  0.13369763]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 106 is [True, False, False, True, False, False]
Current timestep = 107. State = [[-0.17180155 -0.24403471]]. Action = [[-0.24088868  0.08383662  0.03941339 -0.29621887]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 107 is [True, False, False, True, False, False]
Current timestep = 108. State = [[-0.18208788 -0.24283011]]. Action = [[ 0.05001551 -0.1500324  -0.22114334  0.5114268 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 108 is [True, False, False, True, False, False]
Current timestep = 109. State = [[-0.18825608 -0.2562292 ]]. Action = [[-0.08184502 -0.1089337   0.2084474   0.5159682 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 109 is [True, False, False, True, False, False]
Current timestep = 110. State = [[-0.18360554 -0.26657823]]. Action = [[ 0.24710143 -0.08820409 -0.14539751 -0.90154266]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 110 is [True, False, False, True, False, False]
Current timestep = 111. State = [[-0.17003886 -0.28399035]]. Action = [[ 0.07620579 -0.20900036  0.14573202  0.9604924 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 111 is [True, False, False, True, False, False]
Current timestep = 112. State = [[-0.1552402  -0.29169613]]. Action = [[ 0.15603864  0.14138496 -0.23184569  0.7475505 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 112 is [True, False, False, True, False, False]
Scene graph at timestep 112 is [True, False, False, True, False, False]
State prediction error at timestep 112 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 112 of -1
Current timestep = 113. State = [[-0.14205708 -0.28688732]]. Action = [[ 0.09862372 -0.18402676 -0.14143328  0.949574  ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 113 is [True, False, False, True, False, False]
Scene graph at timestep 113 is [True, False, False, True, False, False]
State prediction error at timestep 113 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of 0
Current timestep = 114. State = [[-0.14658228 -0.2819966 ]]. Action = [[-0.22545756  0.14075765 -0.16469076  0.5191264 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 114 is [True, False, False, True, False, False]
Scene graph at timestep 114 is [True, False, False, True, False, False]
State prediction error at timestep 114 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 114 of 0
Current timestep = 115. State = [[-0.15269549 -0.28127262]]. Action = [[ 0.02225265 -0.13004974 -0.11998484 -0.80039656]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 115 is [True, False, False, True, False, False]
Current timestep = 116. State = [[-0.15876879 -0.28629005]]. Action = [[-0.11622766  0.03166938 -0.1872519  -0.9763034 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 116 is [True, False, False, True, False, False]
Current timestep = 117. State = [[-0.16515534 -0.2876235 ]]. Action = [[ 0.18140876 -0.2417404   0.1544649   0.68121815]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 117 is [True, False, False, True, False, False]
Scene graph at timestep 117 is [True, False, False, True, False, False]
State prediction error at timestep 117 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 117 of -1
Current timestep = 118. State = [[-0.16563883 -0.2848317 ]]. Action = [[ 0.02393401  0.05524901  0.22960627 -0.35964596]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 118 is [True, False, False, True, False, False]
Scene graph at timestep 118 is [True, False, False, True, False, False]
State prediction error at timestep 118 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 118 of 1
Current timestep = 119. State = [[-0.1709563 -0.2687641]]. Action = [[-0.14090148  0.23971069  0.12706417  0.9315369 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 119 is [True, False, False, True, False, False]
Current timestep = 120. State = [[-0.1771662  -0.24824023]]. Action = [[ 0.07751411  0.030283    0.23193187 -0.9490202 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 120 is [True, False, False, True, False, False]
Scene graph at timestep 120 is [True, False, False, True, False, False]
State prediction error at timestep 120 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 120 of 1
Current timestep = 121. State = [[-0.16667119 -0.22781135]]. Action = [[ 0.21402165  0.1972171  -0.13999821  0.96971035]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 121 is [True, False, False, True, False, False]
Current timestep = 122. State = [[-0.1656691  -0.20912088]]. Action = [[-0.21533003  0.06880525 -0.12297037  0.12992048]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 122 is [True, False, False, True, False, False]
Current timestep = 123. State = [[-0.17695513 -0.20163749]]. Action = [[-0.14321454  0.03436023 -0.20018083 -0.9648118 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 123 is [True, False, False, True, False, False]
Current timestep = 124. State = [[-0.18788393 -0.19893025]]. Action = [[ 0.0641976  -0.02544691 -0.1320567   0.42233646]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 124 is [True, False, False, True, False, False]
Current timestep = 125. State = [[-0.19633073 -0.21249163]]. Action = [[-0.17500179 -0.2158018  -0.05982921  0.00056052]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 125 is [True, False, False, True, False, False]
Current timestep = 126. State = [[-0.21689494 -0.21432409]]. Action = [[-0.21230157  0.21260634 -0.1376004   0.48684418]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 126 is [True, False, False, True, False, False]
Scene graph at timestep 126 is [True, False, False, True, False, False]
State prediction error at timestep 126 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.2422693  -0.19033547]]. Action = [[-0.05945304  0.21802947  0.08446729  0.23413587]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 127 is [True, False, False, True, False, False]
Current timestep = 128. State = [[-0.2451319  -0.16801481]]. Action = [[-0.01028588  0.08609354 -0.17076139 -0.32035196]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 128 is [True, False, False, True, False, False]
Scene graph at timestep 128 is [True, False, False, True, False, False]
State prediction error at timestep 128 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 128 of 0
Current timestep = 129. State = [[-0.23815131 -0.14940593]]. Action = [[ 0.24505156  0.1273073   0.05374429 -0.41068947]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 129 is [True, False, False, True, False, False]
Current timestep = 130. State = [[-0.23540014 -0.13965017]]. Action = [[-0.15650538  0.00513434 -0.1781784  -0.43885255]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 130 is [True, False, False, True, False, False]
Current timestep = 131. State = [[-0.22902948 -0.13093711]]. Action = [[ 0.23761085  0.08891395 -0.09751961  0.86869574]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 131 is [True, False, False, True, False, False]
Scene graph at timestep 131 is [True, False, False, True, False, False]
State prediction error at timestep 131 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.21254043 -0.1065219 ]]. Action = [[ 0.1780284   0.24365547  0.16422373 -0.761635  ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 132 is [True, False, False, True, False, False]
Scene graph at timestep 132 is [True, False, False, False, True, False]
State prediction error at timestep 132 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 132 of 1
Current timestep = 133. State = [[-0.18786508 -0.08004403]]. Action = [[ 0.18607908  0.08270055 -0.11231387 -0.8844493 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 133 is [True, False, False, False, True, False]
Scene graph at timestep 133 is [True, False, False, False, True, False]
State prediction error at timestep 133 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 133 of 1
Current timestep = 134. State = [[-0.16454051 -0.06973874]]. Action = [[ 0.13048866  0.04102439  0.107317   -0.07202297]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 134 is [True, False, False, False, True, False]
Current timestep = 135. State = [[-0.1566403  -0.07334302]]. Action = [[-0.09210788 -0.12643632  0.0246278  -0.54901296]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 135 is [True, False, False, False, True, False]
Current timestep = 136. State = [[-0.15833941 -0.06447764]]. Action = [[-0.00138454  0.24193949  0.16259709  0.4896766 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 136 is [True, False, False, False, True, False]
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.16831824 -0.05570535]]. Action = [[-0.22115149 -0.12741941  0.10339981 -0.38993686]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 137 is [True, False, False, False, True, False]
Scene graph at timestep 137 is [True, False, False, False, True, False]
State prediction error at timestep 137 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 137 of -1
Current timestep = 138. State = [[-0.17437875 -0.07462525]]. Action = [[ 0.20610666 -0.18888997 -0.18446706  0.29830027]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 138 is [True, False, False, False, True, False]
Scene graph at timestep 138 is [True, False, False, False, True, False]
State prediction error at timestep 138 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 138 of 1
Current timestep = 139. State = [[-0.1757308  -0.07461931]]. Action = [[-0.24002486  0.2343724   0.06186655  0.5924903 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 139 is [True, False, False, False, True, False]
Current timestep = 140. State = [[-0.18611002 -0.0497738 ]]. Action = [[-0.10685438  0.19033134 -0.12330779 -0.6577444 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 140 is [True, False, False, False, True, False]
Current timestep = 141. State = [[-0.20273875 -0.02510421]]. Action = [[-0.17213367  0.11600518 -0.17739996  0.53851223]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 141 is [True, False, False, False, True, False]
Current timestep = 142. State = [[-0.21240096 -0.02519774]]. Action = [[ 0.21602309 -0.17892355  0.14363885  0.9520496 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 142 is [True, False, False, False, True, False]
Current timestep = 143. State = [[-0.21603657 -0.04416056]]. Action = [[-0.21326698 -0.16853373  0.11355215  0.2111063 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 143 is [True, False, False, False, True, False]
Current timestep = 144. State = [[-0.2226922  -0.06144334]]. Action = [[-0.00106074 -0.06082118  0.21509436 -0.4668256 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 144 is [True, False, False, False, True, False]
Current timestep = 145. State = [[-0.23218566 -0.07625949]]. Action = [[-0.16407049 -0.13405941 -0.01005757  0.4660009 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 145 is [True, False, False, False, True, False]
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.24491327 -0.09731913]]. Action = [[ 0.08325455 -0.15069598 -0.056641    0.7520726 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 146 is [True, False, False, False, True, False]
Scene graph at timestep 146 is [True, False, False, False, True, False]
State prediction error at timestep 146 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 146 of 1
Current timestep = 147. State = [[-0.2410509  -0.09914774]]. Action = [[ 0.0166541   0.21518248  0.01053572 -0.15830642]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 147 is [True, False, False, False, True, False]
Current timestep = 148. State = [[-0.24135873 -0.09376089]]. Action = [[ 0.01098004 -0.10928269 -0.10109381 -0.0441044 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 148 is [True, False, False, False, True, False]
Current timestep = 149. State = [[-0.23639716 -0.08904792]]. Action = [[ 0.11735839  0.13472557  0.18050128 -0.77523696]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 149 is [True, False, False, False, True, False]
Scene graph at timestep 149 is [True, False, False, False, True, False]
State prediction error at timestep 149 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 149 of 1
Current timestep = 150. State = [[-0.22252    -0.06899152]]. Action = [[ 0.14361858  0.18845513 -0.16704611 -0.86735874]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 150 is [True, False, False, False, True, False]
Scene graph at timestep 150 is [True, False, False, False, True, False]
State prediction error at timestep 150 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 150 of 1
Current timestep = 151. State = [[-0.21465835 -0.06296276]]. Action = [[-0.1476041  -0.16314988 -0.05332096 -0.50461733]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 151 is [True, False, False, False, True, False]
Scene graph at timestep 151 is [True, False, False, False, True, False]
State prediction error at timestep 151 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 151 of 1
Current timestep = 152. State = [[-0.22903824 -0.07506198]]. Action = [[-0.22885884 -0.00983028  0.03548622 -0.2858317 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 152 is [True, False, False, False, True, False]
Scene graph at timestep 152 is [True, False, False, False, True, False]
State prediction error at timestep 152 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 152 of -1
Current timestep = 153. State = [[-0.24778536 -0.07176106]]. Action = [[-0.08798575  0.11548686  0.12049446  0.22657716]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 153 is [True, False, False, False, True, False]
Scene graph at timestep 153 is [True, False, False, False, True, False]
State prediction error at timestep 153 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 153 of -1
Current timestep = 154. State = [[-0.25157475 -0.07259856]]. Action = [[ 0.1407662  -0.21050625  0.11666152 -0.41787755]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 154 is [True, False, False, False, True, False]
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of 0
Current timestep = 155. State = [[-0.24847953 -0.07982385]]. Action = [[-0.0600512   0.14213753 -0.12392566 -0.22154862]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 155 is [True, False, False, False, True, False]
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of 0
Current timestep = 156. State = [[-0.24939144 -0.07443009]]. Action = [[-0.24493586  0.07013658 -0.21066554  0.4141668 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 156 is [True, False, False, False, True, False]
Scene graph at timestep 156 is [True, False, False, False, True, False]
State prediction error at timestep 156 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of 0
Current timestep = 157. State = [[-0.25126347 -0.08357258]]. Action = [[-0.05138503 -0.180369    0.02413464 -0.62239283]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 157 is [True, False, False, False, True, False]
Scene graph at timestep 157 is [True, False, False, False, True, False]
State prediction error at timestep 157 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 157 of -1
Current timestep = 158. State = [[-0.26191324 -0.09345566]]. Action = [[-0.23123439  0.12010142 -0.23456417  0.26539135]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 158 is [True, False, False, False, True, False]
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of 0
Current timestep = 159. State = [[-0.25799263 -0.08288492]]. Action = [[ 0.13730124  0.2041353  -0.12419996 -0.00939101]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 159 is [True, False, False, False, True, False]
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 159 of 1
Current timestep = 160. State = [[-0.25098333 -0.07000691]]. Action = [[-0.16732058  0.14291382 -0.14631869  0.32864213]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 160 is [True, False, False, False, True, False]
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of 1
Current timestep = 161. State = [[-0.24987708 -0.07021867]]. Action = [[-0.01068774 -0.06711653 -0.05186427  0.09050548]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 161 is [True, False, False, False, True, False]
Scene graph at timestep 161 is [True, False, False, False, True, False]
State prediction error at timestep 161 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of 0
Current timestep = 162. State = [[-0.24939582 -0.07153257]]. Action = [[-0.16289097  0.20133787 -0.01890737 -0.6385088 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 162 is [True, False, False, False, True, False]
Current timestep = 163. State = [[-0.24957143 -0.07095444]]. Action = [[ 0.00254738  0.05181104 -0.24734642  0.87294984]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 163 is [True, False, False, False, True, False]
Current timestep = 164. State = [[-0.24840574 -0.07883599]]. Action = [[ 0.01241791 -0.1658822  -0.21720363 -0.27486765]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 164 is [True, False, False, False, True, False]
Scene graph at timestep 164 is [True, False, False, False, True, False]
State prediction error at timestep 164 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 164 of 0
Current timestep = 165. State = [[-0.24560711 -0.09602702]]. Action = [[ 0.03236586 -0.1196963  -0.12881666 -0.72153634]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 165 is [True, False, False, False, True, False]
Current timestep = 166. State = [[-0.2444894  -0.10394415]]. Action = [[-0.23652524  0.14523733  0.09505877  0.28933048]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 166 is [True, False, False, False, True, False]
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of -1
Current timestep = 167. State = [[-0.2513614  -0.11867162]]. Action = [[-0.16012585 -0.1972115   0.20923239  0.30175006]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 167 is [True, False, False, False, True, False]
Scene graph at timestep 167 is [True, False, False, False, True, False]
State prediction error at timestep 167 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 167 of -1
Current timestep = 168. State = [[-0.2536816  -0.12522423]]. Action = [[ 0.18932447  0.21689636 -0.09100533  0.8994169 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 168 is [True, False, False, False, True, False]
Scene graph at timestep 168 is [True, False, False, True, False, False]
State prediction error at timestep 168 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of 1
Current timestep = 169. State = [[-0.2438516  -0.11106439]]. Action = [[-0.18214247 -0.0047162   0.05483782  0.72266364]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 169 is [True, False, False, True, False, False]
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of 1
Current timestep = 170. State = [[-0.24007836 -0.10899371]]. Action = [[ 0.0590812   0.02818328  0.13727778 -0.6350142 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 170 is [True, False, False, False, True, False]
Scene graph at timestep 170 is [True, False, False, False, True, False]
State prediction error at timestep 170 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 170 of 1
Current timestep = 171. State = [[-0.23659131 -0.10100488]]. Action = [[-0.1288705   0.08816022 -0.00982802 -0.832354  ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 171 is [True, False, False, False, True, False]
Scene graph at timestep 171 is [True, False, False, False, True, False]
State prediction error at timestep 171 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 171 of 0
Current timestep = 172. State = [[-0.24417648 -0.10439929]]. Action = [[-0.09768006 -0.1734026  -0.00905131  0.12994087]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 172 is [True, False, False, False, True, False]
Scene graph at timestep 172 is [True, False, False, False, True, False]
State prediction error at timestep 172 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 172 of -1
Current timestep = 173. State = [[-0.25460073 -0.1185194 ]]. Action = [[ 0.04516703 -0.07784031 -0.22146486 -0.08484882]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 173 is [True, False, False, False, True, False]
Current timestep = 174. State = [[-0.25420156 -0.1224183 ]]. Action = [[-0.22213215  0.19866034 -0.21253419 -0.24859685]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 174 is [True, False, False, False, True, False]
Scene graph at timestep 174 is [True, False, False, False, True, False]
State prediction error at timestep 174 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 174 of 0
Current timestep = 175. State = [[-0.25493285 -0.11986008]]. Action = [[ 0.00537348  0.11918473 -0.14679818  0.6729908 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 175 is [True, False, False, False, True, False]
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.255386   -0.11390482]]. Action = [[-0.04433894  0.02953207  0.16278964 -0.89110756]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 176 is [True, False, False, False, True, False]
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 176 of 1
Current timestep = 177. State = [[-0.24825537 -0.12131886]]. Action = [[ 0.23202798 -0.19398525 -0.1277539   0.83796954]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 177 is [True, False, False, False, True, False]
Scene graph at timestep 177 is [True, False, False, False, True, False]
State prediction error at timestep 177 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 177 of 1
Current timestep = 178. State = [[-0.23617604 -0.14082275]]. Action = [[-0.03684673 -0.13843633 -0.11544916 -0.711815  ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 178 is [True, False, False, False, True, False]
Current timestep = 179. State = [[-0.23010182 -0.15085807]]. Action = [[ 0.16681308 -0.015275    0.14624387 -0.8229259 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 179 is [True, False, False, True, False, False]
Scene graph at timestep 179 is [True, False, False, True, False, False]
State prediction error at timestep 179 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 179 of -1
Current timestep = 180. State = [[-0.22357836  0.07541133]]. Action = [[ 0.12097004  0.03313255  0.10909441 -0.5843684 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 180 is [True, False, False, True, False, False]
Scene graph at timestep 180 is [True, False, False, False, True, False]
State prediction error at timestep 180 is tensor(0.0221, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of 0
Current timestep = 181. State = [[-0.20656468  0.09258198]]. Action = [[ 0.20691818  0.15201199 -0.22897655  0.11699402]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 181 is [True, False, False, False, True, False]
Scene graph at timestep 181 is [True, False, False, False, True, False]
State prediction error at timestep 181 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.19235325  0.12174668]]. Action = [[-0.13913189  0.21897     0.14724123 -0.73270905]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 182 is [True, False, False, False, True, False]
Current timestep = 183. State = [[-0.1997541  0.1349885]]. Action = [[ 0.01565269 -0.05589548  0.18970555 -0.6457581 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 183 is [True, False, False, False, True, False]
Scene graph at timestep 183 is [True, False, False, False, False, True]
State prediction error at timestep 183 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.20748103  0.14186361]]. Action = [[-0.225476    0.07683954 -0.1316769   0.07249629]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 184 is [True, False, False, False, False, True]
Scene graph at timestep 184 is [True, False, False, False, False, True]
State prediction error at timestep 184 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 184 of -1
Current timestep = 185. State = [[-0.21225414  0.13659413]]. Action = [[ 0.2447516  -0.18890972  0.20387691 -0.33774722]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 185 is [True, False, False, False, False, True]
Current timestep = 186. State = [[-0.1990332   0.12863784]]. Action = [[ 0.12094378  0.08296096  0.17817676 -0.18822896]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 186 is [True, False, False, False, False, True]
Current timestep = 187. State = [[-0.188783    0.12157136]]. Action = [[-0.11353976 -0.1864448   0.20310986  0.9241383 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 187 is [True, False, False, False, False, True]
Current timestep = 188. State = [[-0.19021834  0.11101215]]. Action = [[-8.0484152e-02 -7.6292455e-04  2.2427917e-02 -8.8271540e-01]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 188 is [True, False, False, False, True, False]
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 188 of 1
Current timestep = 189. State = [[-0.19113646  0.11447584]]. Action = [[ 0.14630216  0.16083133 -0.21718915 -0.68219346]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 189 is [True, False, False, False, True, False]
Current timestep = 190. State = [[-0.19680795  0.13119264]]. Action = [[-0.23959081  0.1041635   0.23524648  0.5566125 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 190 is [True, False, False, False, True, False]
Scene graph at timestep 190 is [True, False, False, False, False, True]
State prediction error at timestep 190 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of -1
Current timestep = 191. State = [[-0.21650557  0.13958989]]. Action = [[-0.22469656 -0.12946402 -0.08333184 -0.545964  ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 191 is [True, False, False, False, False, True]
Scene graph at timestep 191 is [True, False, False, False, False, True]
State prediction error at timestep 191 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.24338488  0.14116256]]. Action = [[-0.14265786  0.19480976  0.19559178 -0.90254223]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 192 is [True, False, False, False, False, True]
Current timestep = 193. State = [[-0.24963962  0.14135496]]. Action = [[ 0.11691213 -0.2184925  -0.0013981  -0.41786468]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 193 is [True, False, False, False, False, True]
Current timestep = 194. State = [[-0.23993121  0.11849606]]. Action = [[ 0.18272716 -0.15243389  0.18809667 -0.5114222 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 194 is [True, False, False, False, False, True]
Current timestep = 195. State = [[-0.22460702  0.0983014 ]]. Action = [[ 0.15430018 -0.10705966 -0.13077055 -0.92469513]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 195 is [True, False, False, False, True, False]
Scene graph at timestep 195 is [True, False, False, False, True, False]
State prediction error at timestep 195 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 195 of 1
Current timestep = 196. State = [[-0.20331272  0.07102034]]. Action = [[ 0.15925407 -0.24800035  0.15247482 -0.64876246]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 196 is [True, False, False, False, True, False]
Current timestep = 197. State = [[-0.19441055  0.05544675]]. Action = [[-0.20238863  0.002819    0.22977501  0.58052886]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 197 is [True, False, False, False, True, False]
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.19194688  0.04406127]]. Action = [[ 0.19113731 -0.10742742  0.17520899 -0.97486144]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 198 is [True, False, False, False, True, False]
Current timestep = 199. State = [[-0.18230857  0.03810828]]. Action = [[ 0.15099484  0.0359624  -0.11008579  0.41310096]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 199 is [True, False, False, False, True, False]
Scene graph at timestep 199 is [True, False, False, False, True, False]
State prediction error at timestep 199 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of 1
Current timestep = 200. State = [[-0.16661654  0.03137482]]. Action = [[-0.05201383 -0.1339499   0.19096372 -0.9765331 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 200 is [True, False, False, False, True, False]
Scene graph at timestep 200 is [True, False, False, False, True, False]
State prediction error at timestep 200 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 200 of 1
Current timestep = 201. State = [[-0.16741683  0.03117941]]. Action = [[-0.07647392  0.17095542 -0.08830357  0.65153646]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 201 is [True, False, False, False, True, False]
Current timestep = 202. State = [[-0.16599797  0.0300132 ]]. Action = [[ 0.15680003 -0.17362827 -0.06550951 -0.26949525]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 202 is [True, False, False, False, True, False]
Scene graph at timestep 202 is [True, False, False, False, True, False]
State prediction error at timestep 202 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 202 of 1
Current timestep = 203. State = [[-0.1638258   0.02842205]]. Action = [[-0.06835034  0.16908526 -0.10237771 -0.94652694]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 203 is [True, False, False, False, True, False]
Current timestep = 204. State = [[-0.16963564  0.05028838]]. Action = [[-0.04280855  0.21612382 -0.06745949 -0.36728507]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 204 is [True, False, False, False, True, False]
Scene graph at timestep 204 is [True, False, False, False, True, False]
State prediction error at timestep 204 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 204 of -1
Current timestep = 205. State = [[-0.18170694  0.08196829]]. Action = [[-0.15771265  0.20317012 -0.21709566 -0.85734826]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 205 is [True, False, False, False, True, False]
Current timestep = 206. State = [[-0.1866895   0.11033685]]. Action = [[ 0.12851146  0.20701337 -0.18221852 -0.21725899]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 206 is [True, False, False, False, True, False]
Current timestep = 207. State = [[-0.18135226  0.11684247]]. Action = [[ 0.10488093 -0.14368029  0.12905258  0.8697486 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 207 is [True, False, False, False, True, False]
Current timestep = 208. State = [[-0.17453843  0.10786349]]. Action = [[-0.04991831 -0.10093208 -0.18501665  0.8674841 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 208 is [True, False, False, False, True, False]
Scene graph at timestep 208 is [True, False, False, False, True, False]
State prediction error at timestep 208 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 208 of -1
Current timestep = 209. State = [[-0.17994626  0.0991597 ]]. Action = [[-0.23926502 -0.03123218 -0.22565368  0.8581798 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 209 is [True, False, False, False, True, False]
Current timestep = 210. State = [[-0.19195604  0.09293904]]. Action = [[-0.09107038 -0.067729   -0.00593446 -0.6616526 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 210 is [True, False, False, False, True, False]
Current timestep = 211. State = [[-0.21165262  0.07824378]]. Action = [[-0.22409527 -0.14217547  0.0553841  -0.7573773 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 211 is [True, False, False, False, True, False]
Current timestep = 212. State = [[-0.22902021  0.07937785]]. Action = [[0.08322632 0.22168559 0.21301013 0.7006664 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 212 is [True, False, False, False, True, False]
Scene graph at timestep 212 is [True, False, False, False, True, False]
State prediction error at timestep 212 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[-0.23865066  0.09978279]]. Action = [[-0.136901    0.08871225  0.03398883  0.516839  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 213 is [True, False, False, False, True, False]
Scene graph at timestep 213 is [True, False, False, False, True, False]
State prediction error at timestep 213 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 213 of -1
Current timestep = 214. State = [[-0.24039322  0.10469972]]. Action = [[ 0.17448148 -0.07747093  0.16174129 -0.88897103]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 214 is [True, False, False, False, True, False]
Scene graph at timestep 214 is [True, False, False, False, True, False]
State prediction error at timestep 214 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 214 of 0
Current timestep = 215. State = [[-0.23262466  0.0901117 ]]. Action = [[-0.03218372 -0.17810728  0.16148454  0.3959428 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 215 is [True, False, False, False, True, False]
Current timestep = 216. State = [[-0.24060647  0.08118692]]. Action = [[-0.20297483  0.04025778 -0.22537966 -0.53902256]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 216 is [True, False, False, False, True, False]
Scene graph at timestep 216 is [True, False, False, False, True, False]
State prediction error at timestep 216 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 216 of 1
Current timestep = 217. State = [[-0.25398934  0.06632058]]. Action = [[-0.08132988 -0.21519373  0.19763532 -0.71558976]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 217 is [True, False, False, False, True, False]
Scene graph at timestep 217 is [True, False, False, False, True, False]
State prediction error at timestep 217 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 217 of 1
Current timestep = 218. State = [[-0.26097336  0.05940866]]. Action = [[ 0.15816283  0.22887775  0.03592044 -0.20253068]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 218 is [True, False, False, False, True, False]
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of -1
Current timestep = 219. State = [[-0.25851607  0.07451111]]. Action = [[-0.22291633 -0.11904898 -0.20340458 -0.57710534]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 219 is [True, False, False, False, True, False]
Scene graph at timestep 219 is [True, False, False, False, True, False]
State prediction error at timestep 219 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of 0
Current timestep = 220. State = [[-0.2594511   0.08113127]]. Action = [[-0.0124979   0.10294247 -0.05849394 -0.84771645]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 220 is [True, False, False, False, True, False]
Scene graph at timestep 220 is [True, False, False, False, True, False]
State prediction error at timestep 220 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 220 of -1
Current timestep = 221. State = [[-0.2525063   0.08634927]]. Action = [[ 0.15553182 -0.07156938 -0.08849737 -0.62573713]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 221 is [True, False, False, False, True, False]
Scene graph at timestep 221 is [True, False, False, False, True, False]
State prediction error at timestep 221 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 221 of 1
Current timestep = 222. State = [[-0.23302087  0.08304103]]. Action = [[ 0.19958717 -0.01692051  0.21589565 -0.9058541 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 222 is [True, False, False, False, True, False]
Scene graph at timestep 222 is [True, False, False, False, True, False]
State prediction error at timestep 222 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 222 of 1
Current timestep = 223. State = [[-0.20682627  0.07503773]]. Action = [[ 0.18934858 -0.11496325  0.09500307 -0.41888833]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 223 is [True, False, False, False, True, False]
Scene graph at timestep 223 is [True, False, False, False, True, False]
State prediction error at timestep 223 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 223 of 1
Current timestep = 224. State = [[-0.19479083  0.07613365]]. Action = [[-0.19918379  0.17072624 -0.15772314  0.96978545]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 224 is [True, False, False, False, True, False]
Current timestep = 225. State = [[-0.2002322   0.08020759]]. Action = [[-0.00444011 -0.12347731 -0.21995719  0.36661124]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 225 is [True, False, False, False, True, False]
Scene graph at timestep 225 is [True, False, False, False, True, False]
State prediction error at timestep 225 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 225 of 0
Current timestep = 226. State = [[-0.20651235  0.07358694]]. Action = [[-0.12935494  0.00153381 -0.19622773 -0.23741233]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 226 is [True, False, False, False, True, False]
Scene graph at timestep 226 is [True, False, False, False, True, False]
State prediction error at timestep 226 is tensor(8.8681e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 226 of 0
Current timestep = 227. State = [[-0.22125675  0.08039311]]. Action = [[-0.1847462   0.10704452  0.1343511   0.85853004]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 227 is [True, False, False, False, True, False]
Current timestep = 228. State = [[-0.23177159  0.08291825]]. Action = [[ 0.18417913 -0.07131571  0.01596227  0.83148146]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 228 is [True, False, False, False, True, False]
Current timestep = 229. State = [[-0.22515906  0.09014474]]. Action = [[ 0.12263381  0.208152    0.08312666 -0.56407845]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 229 is [True, False, False, False, True, False]
Current timestep = 230. State = [[-0.22032127  0.09434   ]]. Action = [[-0.19492856 -0.14381291 -0.19913897  0.04285932]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 230 is [True, False, False, False, True, False]
Current timestep = 231. State = [[-0.22341065  0.10047462]]. Action = [[ 0.11985844  0.21102995 -0.03754909 -0.4154191 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 231 is [True, False, False, False, True, False]
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(3.4329e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of -1
Current timestep = 232. State = [[-0.23139443  0.12777266]]. Action = [[-0.175066    0.2478793   0.10615218 -0.58926   ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 232 is [True, False, False, False, True, False]
Current timestep = 233. State = [[-0.23294318  0.13583647]]. Action = [[ 0.20867431 -0.18582936 -0.23741663  0.6299424 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 233 is [True, False, False, False, False, True]
Current timestep = 234. State = [[-0.2226054   0.13634105]]. Action = [[0.0691995  0.1562199  0.01560131 0.27363873]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 234 is [True, False, False, False, False, True]
Current timestep = 235. State = [[-0.21802649  0.13910314]]. Action = [[-0.1331931  -0.10197653  0.08334091  0.93212175]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 235 is [True, False, False, False, False, True]
Current timestep = 236. State = [[-0.22725403  0.14328699]]. Action = [[-0.18978754  0.07978883 -0.08231594 -0.86974794]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 236 is [True, False, False, False, False, True]
Scene graph at timestep 236 is [True, False, False, False, False, True]
State prediction error at timestep 236 is tensor(2.8669e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 236 of -1
Current timestep = 237. State = [[-0.24153498  0.14544913]]. Action = [[-0.01359499 -0.04325035 -0.16436425  0.02695835]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 237 is [True, False, False, False, False, True]
Current timestep = 238. State = [[-0.245213    0.12999941]]. Action = [[-7.7548265e-02 -2.2130503e-01  8.1205368e-04 -9.7854930e-01]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 238 is [True, False, False, False, False, True]
Current timestep = 239. State = [[-0.24656057  0.12296264]]. Action = [[ 0.18252888  0.17948341 -0.05473171  0.65454745]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 239 is [True, False, False, False, False, True]
Current timestep = 240. State = [[-0.23244384  0.11581183]]. Action = [[ 0.2364726  -0.2207294  -0.01551858  0.08811796]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 240 is [True, False, False, False, True, False]
Scene graph at timestep 240 is [True, False, False, False, True, False]
State prediction error at timestep 240 is tensor(8.5072e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 240 of 1
Current timestep = 241. State = [[-0.21073064  0.10726831]]. Action = [[ 0.09304723  0.11546406 -0.06068605  0.39963067]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 241 is [True, False, False, False, True, False]
Current timestep = 242. State = [[-0.2072429   0.12530337]]. Action = [[-0.10969006  0.2009517   0.06229889  0.27814293]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 242 is [True, False, False, False, True, False]
Scene graph at timestep 242 is [True, False, False, False, False, True]
State prediction error at timestep 242 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 242 of -1
Current timestep = 243. State = [[-0.21131141  0.13424143]]. Action = [[-0.09464292 -0.18406238 -0.14463983 -0.5599629 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 243 is [True, False, False, False, False, True]
Current timestep = 244. State = [[-0.20744447  0.1350552 ]]. Action = [[ 0.19356093  0.18730149 -0.0392147  -0.716952  ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 244 is [True, False, False, False, False, True]
Current timestep = 245. State = [[-0.19443882  0.15014383]]. Action = [[ 0.19118917  0.15490127 -0.23828565 -0.49123347]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 245 is [True, False, False, False, False, True]
Scene graph at timestep 245 is [True, False, False, False, False, True]
State prediction error at timestep 245 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 245 of -1
Current timestep = 246. State = [[-0.16913986  0.15715952]]. Action = [[ 0.07068607 -0.18021286  0.05992627 -0.96594405]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 246 is [True, False, False, False, False, True]
Scene graph at timestep 246 is [True, False, False, False, False, True]
State prediction error at timestep 246 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 246 of 1
Current timestep = 247. State = [[-0.15766181  0.13807285]]. Action = [[ 0.13638514 -0.13122153 -0.05766198  0.92205656]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 247 is [True, False, False, False, False, True]
Current timestep = 248. State = [[-0.15070666  0.1296056 ]]. Action = [[-0.11701921  0.0167093  -0.13530186 -0.6842987 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 248 is [True, False, False, False, False, True]
Scene graph at timestep 248 is [True, False, False, False, False, True]
State prediction error at timestep 248 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 248 of 1
Current timestep = 249. State = [[-0.14376143  0.13883673]]. Action = [[ 0.23489541  0.18929553 -0.17561291  0.92951703]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 249 is [True, False, False, False, False, True]
Scene graph at timestep 249 is [True, False, False, False, False, True]
State prediction error at timestep 249 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 249 of 1
Current timestep = 250. State = [[-0.12714873  0.14801383]]. Action = [[-0.19282603 -0.14196885 -0.06817152 -0.70155734]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 250 is [True, False, False, False, False, True]
Scene graph at timestep 250 is [True, False, False, False, False, True]
State prediction error at timestep 250 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 250 of 1
Current timestep = 251. State = [[-0.13260782  0.1484584 ]]. Action = [[ 0.12570226  0.20053184 -0.02545691 -0.40476263]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 251 is [True, False, False, False, False, True]
Current timestep = 252. State = [[-0.13196062  0.16761354]]. Action = [[ 0.03108478  0.17965543  0.10826749 -0.87320226]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 252 is [True, False, False, False, False, True]
Scene graph at timestep 252 is [True, False, False, False, False, True]
State prediction error at timestep 252 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.12617168  0.17326932]]. Action = [[-0.02135915 -0.23016866 -0.15738843 -0.6338252 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 253 is [True, False, False, False, False, True]
Current timestep = 254. State = [[-0.12444584  0.15042002]]. Action = [[-0.0369463  -0.21292499 -0.20636739 -0.39512134]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 254 is [True, False, False, False, False, True]
Current timestep = 255. State = [[-0.12693094  0.13668701]]. Action = [[-0.06820756  0.00610176  0.17339012  0.9254029 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 255 is [True, False, False, False, False, True]
Current timestep = 256. State = [[-0.13564554  0.13735588]]. Action = [[-0.18962663  0.0446023  -0.02917437  0.47433472]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 256 is [True, False, False, False, False, True]
Current timestep = 257. State = [[-0.14900953  0.14589499]]. Action = [[ 0.11189833  0.18179342 -0.17159429  0.19562101]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 257 is [True, False, False, False, False, True]
Scene graph at timestep 257 is [True, False, False, False, False, True]
State prediction error at timestep 257 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of 1
Current timestep = 258. State = [[-0.1531539   0.16051778]]. Action = [[ 0.00115636  0.0901393  -0.07897902  0.38864362]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 258 is [True, False, False, False, False, True]
Scene graph at timestep 258 is [True, False, False, False, False, True]
State prediction error at timestep 258 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 258 of -1
Current timestep = 259. State = [[-0.15765697  0.17417106]]. Action = [[-0.06344223  0.09162307  0.07082111  0.64300454]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 259 is [True, False, False, False, False, True]
Current timestep = 260. State = [[-0.1686838   0.18072426]]. Action = [[-0.24638192 -0.06193566  0.16591173 -0.73945594]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 260 is [True, False, False, False, False, True]
Scene graph at timestep 260 is [True, False, False, False, False, True]
State prediction error at timestep 260 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 260 of -1
Current timestep = 261. State = [[-0.18331258  0.1848244 ]]. Action = [[-0.08434206  0.0307833  -0.22127126  0.64203775]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 261 is [True, False, False, False, False, True]
Current timestep = 262. State = [[-0.19684152  0.19478892]]. Action = [[-0.14973277  0.08703268  0.23015088  0.48120213]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 262 is [True, False, False, False, False, True]
Current timestep = 263. State = [[-0.2217539   0.19070566]]. Action = [[-0.20900036 -0.15094757 -0.04944555 -0.27697933]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 263 is [True, False, False, False, False, True]
Current timestep = 264. State = [[-0.23430721  0.16723466]]. Action = [[ 0.12840188 -0.22827342 -0.06493002 -0.3745342 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 264 is [True, False, False, False, False, True]
Scene graph at timestep 264 is [True, False, False, False, False, True]
State prediction error at timestep 264 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 264 of 0
Current timestep = 265. State = [[-0.24182133  0.13806593]]. Action = [[-0.15024737 -0.12262484  0.17863315  0.6910337 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 265 is [True, False, False, False, False, True]
Scene graph at timestep 265 is [True, False, False, False, False, True]
State prediction error at timestep 265 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 265 of 1
Current timestep = 266. State = [[-0.2524773   0.11539315]]. Action = [[-0.02618214 -0.18405591  0.02582353  0.41035342]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 266 is [True, False, False, False, False, True]
Scene graph at timestep 266 is [True, False, False, False, True, False]
State prediction error at timestep 266 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 266 of 1
Current timestep = 267. State = [[-0.25968322  0.11116394]]. Action = [[-0.10777086  0.20739198  0.10590133 -0.81198853]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 267 is [True, False, False, False, True, False]
Scene graph at timestep 267 is [True, False, False, False, True, False]
State prediction error at timestep 267 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 267 of -1
Current timestep = 268. State = [[-0.2697548  0.1279373]]. Action = [[-0.00415877  0.05287448  0.04647335  0.8527112 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 268 is [True, False, False, False, True, False]
Current timestep = 269. State = [[-0.27158388  0.1308884 ]]. Action = [[-0.1739907   0.04620337  0.18212673 -0.91815996]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 269 is [True, False, False, False, False, True]
Scene graph at timestep 269 is [True, False, False, False, False, True]
State prediction error at timestep 269 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[-0.26728794  0.14163145]]. Action = [[ 0.14433175  0.18584543  0.02886051 -0.8795479 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 270 is [True, False, False, False, False, True]
Current timestep = 271. State = [[-0.2557886   0.15026519]]. Action = [[ 0.23432621 -0.01930916 -0.19782265  0.3256234 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 271 is [True, False, False, False, False, True]
Current timestep = 272. State = [[-0.23389913  0.14241233]]. Action = [[ 0.15071362 -0.20895477 -0.06086498  0.4076984 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 272 is [True, False, False, False, False, True]
Current timestep = 273. State = [[-0.21583317  0.11630654]]. Action = [[ 0.05411032 -0.19133528 -0.13185973  0.9277421 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 273 is [True, False, False, False, False, True]
Current timestep = 274. State = [[-0.21190165  0.09997394]]. Action = [[-0.14664185 -0.033768    0.1251052  -0.49401617]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 274 is [True, False, False, False, True, False]
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 274 of 1
Current timestep = 275. State = [[-0.21428296  0.09702465]]. Action = [[0.11394596 0.05076697 0.07641622 0.18260491]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 275 is [True, False, False, False, True, False]
Current timestep = 276. State = [[-0.2069689   0.08825275]]. Action = [[ 0.12550014 -0.16905676 -0.02905761  0.74054   ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 276 is [True, False, False, False, True, False]
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(5.0981e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of 1
Current timestep = 277. State = [[-0.19509697  0.07938053]]. Action = [[0.03278613 0.09611419 0.17085809 0.21906519]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 277 is [True, False, False, False, True, False]
Scene graph at timestep 277 is [True, False, False, False, True, False]
State prediction error at timestep 277 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 277 of 1
Current timestep = 278. State = [[-0.19183035  0.08126351]]. Action = [[-0.13515265 -0.10296513 -0.16013917 -0.96414655]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 278 is [True, False, False, False, True, False]
Scene graph at timestep 278 is [True, False, False, False, True, False]
State prediction error at timestep 278 is tensor(3.2434e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 278 of 1
Current timestep = 279. State = [[-0.18753433  0.07187836]]. Action = [[ 0.22015199 -0.07776448 -0.18815738 -0.85173315]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 279 is [True, False, False, False, True, False]
Current timestep = 280. State = [[-0.1786383   0.05254935]]. Action = [[-0.01240925 -0.19912715 -0.1694874   0.5748584 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 280 is [True, False, False, False, True, False]
Current timestep = 281. State = [[-0.17473891  0.03714487]]. Action = [[-0.07938412 -0.0249677   0.19593164 -0.10298336]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 281 is [True, False, False, False, True, False]
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 281 of 1
Current timestep = 282. State = [[-0.17627485  0.03436271]]. Action = [[-0.00174505  0.0597536  -0.17260003 -0.6891229 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 282 is [True, False, False, False, True, False]
Current timestep = 283. State = [[-0.18499725  0.04813068]]. Action = [[-0.20701723  0.2036407   0.05965275 -0.848059  ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 283 is [True, False, False, False, True, False]
Scene graph at timestep 283 is [True, False, False, False, True, False]
State prediction error at timestep 283 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 283 of -1
Current timestep = 284. State = [[-0.20341903  0.07877927]]. Action = [[ 0.04333299  0.21443343 -0.09181899 -0.37843925]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 284 is [True, False, False, False, True, False]
Scene graph at timestep 284 is [True, False, False, False, True, False]
State prediction error at timestep 284 is tensor(9.9538e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 284 of -1
Current timestep = 285. State = [[-0.20874809  0.08765711]]. Action = [[-0.04310599 -0.14728414  0.22459847  0.720808  ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 285 is [True, False, False, False, True, False]
Scene graph at timestep 285 is [True, False, False, False, True, False]
State prediction error at timestep 285 is tensor(2.8559e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 285 of 1
Current timestep = 286. State = [[-0.21106388  0.07389568]]. Action = [[-0.0353674  -0.11563319 -0.0457042  -0.27378136]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 286 is [True, False, False, False, True, False]
Current timestep = 287. State = [[-0.21589802  0.06565304]]. Action = [[-0.12157097 -0.00769867  0.08713901  0.8230094 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 287 is [True, False, False, False, True, False]
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 287 of -1
Current timestep = 288. State = [[-0.22901176  0.04982179]]. Action = [[-0.1906903  -0.19496791  0.10134003  0.6039808 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 288 is [True, False, False, False, True, False]
Current timestep = 289. State = [[-0.25665417  0.03443572]]. Action = [[-0.22107035 -0.01597501  0.00261402 -0.7831944 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 289 is [True, False, False, False, True, False]
Scene graph at timestep 289 is [True, False, False, False, True, False]
State prediction error at timestep 289 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 289 of -1
Current timestep = 290. State = [[-0.28160298  0.02683084]]. Action = [[-0.05494887  0.05673394  0.21206194 -0.91182256]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 290 is [True, False, False, False, True, False]
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 290 of 0
Current timestep = 291. State = [[-0.2729511  0.0142043]]. Action = [[ 0.24202564 -0.22030714  0.12592125 -0.7382125 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 291 is [True, False, False, False, True, False]
Scene graph at timestep 291 is [True, False, False, False, True, False]
State prediction error at timestep 291 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 291 of 1
Current timestep = 292. State = [[-0.25496322  0.00570758]]. Action = [[0.15953586 0.19955799 0.10041457 0.6353233 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 292 is [True, False, False, False, True, False]
Scene graph at timestep 292 is [True, False, False, False, True, False]
State prediction error at timestep 292 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 292 of 1
Current timestep = 293. State = [[-0.23445725  0.01089852]]. Action = [[ 0.21687263 -0.1518061  -0.22193813  0.7920028 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 293 is [True, False, False, False, True, False]
Scene graph at timestep 293 is [True, False, False, False, True, False]
State prediction error at timestep 293 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 293 of 1
Current timestep = 294. State = [[-0.21827543  0.00643789]]. Action = [[-0.05794856  0.10629207  0.1155788   0.30235696]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 294 is [True, False, False, False, True, False]
Current timestep = 295. State = [[-0.2233518  -0.00128689]]. Action = [[-0.11095452 -0.23730512 -0.05668971 -0.16597927]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 295 is [True, False, False, False, True, False]
Current timestep = 296. State = [[-0.21934398 -0.01051748]]. Action = [[ 0.21853027  0.04877493  0.0997557  -0.41879857]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 296 is [True, False, False, False, True, False]
Current timestep = 297. State = [[-0.20907728 -0.0083102 ]]. Action = [[ 0.04864696  0.051521   -0.11502618 -0.91451687]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 297 is [True, False, False, False, True, False]
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 297 of 1
Current timestep = 298. State = [[-0.19062307 -0.00785013]]. Action = [[ 0.22918385 -0.04649101 -0.17376025 -0.40591133]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 298 is [True, False, False, False, True, False]
Current timestep = 299. State = [[-0.17731985 -0.00958904]]. Action = [[-0.11304188 -0.0129844  -0.04127035 -0.9479742 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 299 is [True, False, False, False, True, False]
Scene graph at timestep 299 is [True, False, False, False, True, False]
State prediction error at timestep 299 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 299 of 1
Current timestep = 300. State = [[-0.17292804 -0.01737224]]. Action = [[ 0.1489343  -0.10233407  0.12011349 -0.6057334 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 300 is [True, False, False, False, True, False]
Scene graph at timestep 300 is [True, False, False, False, True, False]
State prediction error at timestep 300 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 300 of 1
Current timestep = 301. State = [[-0.1676171  -0.02125062]]. Action = [[-0.04561292  0.10079011  0.17678064 -0.13381839]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 301 is [True, False, False, False, True, False]
Scene graph at timestep 301 is [True, False, False, False, True, False]
State prediction error at timestep 301 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 301 of 1
Current timestep = 302. State = [[-0.17157334 -0.02642668]]. Action = [[-0.14483269 -0.19056873  0.01719403  0.17753565]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 302 is [True, False, False, False, True, False]
Scene graph at timestep 302 is [True, False, False, False, True, False]
State prediction error at timestep 302 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 302 of 0
Current timestep = 303. State = [[-0.18723539 -0.0494431 ]]. Action = [[-0.2298917  -0.17144342 -0.08165872  0.48826265]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 303 is [True, False, False, False, True, False]
Current timestep = 304. State = [[-0.19921736 -0.05749839]]. Action = [[ 0.09467798  0.09387699  0.20546857 -0.32467294]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 304 is [True, False, False, False, True, False]
Current timestep = 305. State = [[-0.19031282 -0.06428912]]. Action = [[ 0.24197799 -0.16247949 -0.08097722 -0.4789151 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 305 is [True, False, False, False, True, False]
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of -1
Current timestep = 306. State = [[-0.1761242  -0.08622772]]. Action = [[-0.03322797 -0.2139101  -0.21624702 -0.20502102]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 306 is [True, False, False, False, True, False]
Scene graph at timestep 306 is [True, False, False, False, True, False]
State prediction error at timestep 306 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 306 of -1
Current timestep = 307. State = [[-0.16399164  0.01146724]]. Action = [[ 0.20781994 -0.10611269  0.22014338 -0.64679474]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 307 is [True, False, False, False, True, False]
Current timestep = 308. State = [[-0.14466038  0.02549455]]. Action = [[ 0.12962526  0.17965034 -0.05631259 -0.8954635 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 308 is [True, False, False, False, True, False]
Scene graph at timestep 308 is [True, False, False, False, True, False]
State prediction error at timestep 308 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 308 of 0
Current timestep = 309. State = [[-0.13151076  0.04224658]]. Action = [[-0.08791251  0.02283847  0.22531146 -0.9260035 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 309 is [True, False, False, False, True, False]
Scene graph at timestep 309 is [True, False, False, False, True, False]
State prediction error at timestep 309 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 309 of 0
Current timestep = 310. State = [[-0.13057862  0.05210796]]. Action = [[0.15846562 0.1355637  0.03924027 0.9483007 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 310 is [True, False, False, False, True, False]
Scene graph at timestep 310 is [True, False, False, False, True, False]
State prediction error at timestep 310 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 310 of -1
Current timestep = 311. State = [[-0.12243722  0.05209487]]. Action = [[-0.20049001 -0.19427878 -0.04748273 -0.7073746 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 311 is [True, False, False, False, True, False]
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 311 of 1
Current timestep = 312. State = [[-0.12360943  0.03527102]]. Action = [[ 0.16402626 -0.07024299  0.06208655  0.68790674]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 312 is [True, False, False, False, True, False]
Current timestep = 313. State = [[-0.11348435  0.01893545]]. Action = [[ 0.1959328  -0.19611315  0.15820968 -0.27722466]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 313 is [True, False, False, False, True, False]
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 313 of 1
Current timestep = 314. State = [[-0.10089332  0.00876083]]. Action = [[-0.10394028  0.12626648 -0.12704659 -0.04379076]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 314 is [True, False, False, False, True, False]
Current timestep = 315. State = [[-0.10466379  0.00191162]]. Action = [[-0.11486343 -0.22065395 -0.0632199   0.46814084]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 315 is [True, False, False, False, True, False]
Scene graph at timestep 315 is [True, False, False, False, True, False]
State prediction error at timestep 315 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 315 of 0
Current timestep = 316. State = [[-0.10373393 -0.01758725]]. Action = [[ 0.21332616 -0.08696286  0.06129721  0.84760284]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 316 is [True, False, False, False, True, False]
Current timestep = 317. State = [[-0.08836142 -0.03029644]]. Action = [[ 0.20935678 -0.10416077  0.11525792  0.7250153 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 317 is [True, False, False, False, True, False]
Current timestep = 318. State = [[-0.06878435 -0.03269334]]. Action = [[ 0.03599358  0.10404593 -0.03701386 -0.25611675]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 318 is [True, False, False, False, True, False]
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 318 of 1
Current timestep = 319. State = [[-0.05224084 -0.03858805]]. Action = [[ 0.23686299 -0.17469925 -0.06991132  0.27492094]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 319 is [True, False, False, False, True, False]
Current timestep = 320. State = [[-0.21440968 -0.1874432 ]]. Action = [[ 0.07464641 -0.2153575   0.04391509 -0.69818664]]. Reward = [100.]
Curr episode timestep = 12
Scene graph at timestep 320 is [True, False, False, False, True, False]
Current timestep = 321. State = [[-0.1956104  -0.20096382]]. Action = [[ 0.22078615  0.13219517 -0.15919943 -0.6456789 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 321 is [True, False, False, True, False, False]
Current timestep = 322. State = [[-0.18371946 -0.18531372]]. Action = [[-0.14945695  0.22896928 -0.21273582 -0.30801368]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 322 is [True, False, False, True, False, False]
Scene graph at timestep 322 is [True, False, False, True, False, False]
State prediction error at timestep 322 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 322 of 1
Current timestep = 323. State = [[-0.17797822 -0.17190993]]. Action = [[ 0.18930984 -0.14129534  0.1542854   0.3075831 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 323 is [True, False, False, True, False, False]
Scene graph at timestep 323 is [True, False, False, True, False, False]
State prediction error at timestep 323 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 323 of 1
Current timestep = 324. State = [[-0.1721449  -0.18568668]]. Action = [[ 0.00542271 -0.14874005  0.05341047  0.08203971]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 324 is [True, False, False, True, False, False]
Current timestep = 325. State = [[-0.17056675 -0.19568826]]. Action = [[-0.08325186  0.02597705 -0.02841265  0.80549383]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 325 is [True, False, False, True, False, False]
Scene graph at timestep 325 is [True, False, False, True, False, False]
State prediction error at timestep 325 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 325 of -1
Current timestep = 326. State = [[-0.17972213 -0.1937452 ]]. Action = [[-0.20889345  0.10631064 -0.01098922  0.9659581 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 326 is [True, False, False, True, False, False]
Scene graph at timestep 326 is [True, False, False, True, False, False]
State prediction error at timestep 326 is tensor(2.6345e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 326 of -1
Current timestep = 327. State = [[-0.19132286 -0.19276877]]. Action = [[-0.00888687 -0.06300867  0.08274716  0.01108229]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 327 is [True, False, False, True, False, False]
Current timestep = 328. State = [[-0.19093455 -0.20424403]]. Action = [[ 0.14804494 -0.16351534 -0.18938568 -0.6996026 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 328 is [True, False, False, True, False, False]
Scene graph at timestep 328 is [True, False, False, True, False, False]
State prediction error at timestep 328 is tensor(1.1103e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 328 of -1
Current timestep = 329. State = [[-0.18384641 -0.20309016]]. Action = [[0.10305744 0.17503393 0.06431773 0.61793375]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 329 is [True, False, False, True, False, False]
Current timestep = 330. State = [[-0.17697522 -0.20252025]]. Action = [[ 0.03892982 -0.13656317 -0.00181167  0.10717499]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 330 is [True, False, False, True, False, False]
Current timestep = 331. State = [[-0.1599644  -0.22000545]]. Action = [[ 0.2300173  -0.23354374  0.17561477  0.8269613 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 331 is [True, False, False, True, False, False]
Scene graph at timestep 331 is [True, False, False, True, False, False]
State prediction error at timestep 331 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 331 of -1
Current timestep = 332. State = [[-0.12798043 -0.25144938]]. Action = [[ 0.17291117 -0.17553912  0.16543269 -0.8836249 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 332 is [True, False, False, True, False, False]
Current timestep = 333. State = [[-0.12193405 -0.2668157 ]]. Action = [[-0.24053875 -0.00082235 -0.01898566  0.77449477]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 333 is [True, False, False, True, False, False]
Scene graph at timestep 333 is [True, False, False, True, False, False]
State prediction error at timestep 333 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 333 of -1
Current timestep = 334. State = [[-0.12947401 -0.27456543]]. Action = [[ 0.24638444 -0.24279764  0.04714531  0.5240984 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 334 is [True, False, False, True, False, False]
Scene graph at timestep 334 is [True, False, False, True, False, False]
State prediction error at timestep 334 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 334 of -1
Current timestep = 335. State = [[-0.12293335 -0.26314062]]. Action = [[ 0.2149036   0.17090622  0.1830737  -0.9086633 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 335 is [True, False, False, True, False, False]
Current timestep = 336. State = [[-0.10623082 -0.2602551 ]]. Action = [[ 0.2218408  -0.17189977  0.20395103 -0.92549497]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 336 is [True, False, False, True, False, False]
Current timestep = 337. State = [[-0.07634485 -0.27340317]]. Action = [[ 0.23266089 -0.11948174  0.10616136  0.48402286]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 337 is [True, False, False, True, False, False]
Scene graph at timestep 337 is [True, False, False, True, False, False]
State prediction error at timestep 337 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 337 of 1
Current timestep = 338. State = [[-0.04835255 -0.28075123]]. Action = [[-0.08140729  0.1217466   0.08642596 -0.8764686 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 338 is [True, False, False, True, False, False]
Current timestep = 339. State = [[-0.04757088 -0.2760166 ]]. Action = [[ 0.07176575  0.01830128 -0.05367362 -0.54957   ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 339 is [False, True, False, True, False, False]
Scene graph at timestep 339 is [False, True, False, True, False, False]
State prediction error at timestep 339 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 339 of 1
Current timestep = 340. State = [[-0.04695476 -0.27239972]]. Action = [[-0.12118638  0.0366073  -0.1041624   0.6334827 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 340 is [False, True, False, True, False, False]
Current timestep = 341. State = [[-0.05638358 -0.28024673]]. Action = [[-0.18740043 -0.12230381  0.23772973  0.73877704]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 341 is [False, True, False, True, False, False]
Scene graph at timestep 341 is [True, False, False, True, False, False]
State prediction error at timestep 341 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 341 of -1
Current timestep = 342. State = [[-0.06874765 -0.28796747]]. Action = [[ 0.19620779 -0.1587563  -0.19273111 -0.5248638 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 342 is [True, False, False, True, False, False]
Current timestep = 343. State = [[-0.06584689 -0.2865525 ]]. Action = [[ 0.20037788 -0.00523995  0.16196686  0.39446008]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 343 is [True, False, False, True, False, False]
Scene graph at timestep 343 is [True, False, False, True, False, False]
State prediction error at timestep 343 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 343 of 1
Current timestep = 344. State = [[-0.06382395 -0.28188497]]. Action = [[-0.11098097  0.07937396 -0.10984331  0.1523763 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 344 is [True, False, False, True, False, False]
Current timestep = 345. State = [[-0.06882933 -0.28593525]]. Action = [[-0.13755183 -0.0970408  -0.13163003  0.7217221 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 345 is [True, False, False, True, False, False]
Current timestep = 346. State = [[-0.07400921 -0.29237244]]. Action = [[ 0.1924116  -0.24452001  0.15706602  0.85990334]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 346 is [True, False, False, True, False, False]
Current timestep = 347. State = [[-0.07746032 -0.29663837]]. Action = [[-0.05486999 -0.05292153  0.08741415 -0.5217972 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 347 is [True, False, False, True, False, False]
Current timestep = 348. State = [[-0.07643627 -0.2896212 ]]. Action = [[ 0.14408559  0.1578654  -0.12689836 -0.9701526 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 348 is [True, False, False, True, False, False]
Current timestep = 349. State = [[-0.07405598 -0.2797976 ]]. Action = [[ 0.07161361 -0.14714296  0.2348184   0.28749514]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 349 is [True, False, False, True, False, False]
Scene graph at timestep 349 is [True, False, False, True, False, False]
State prediction error at timestep 349 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 349 of 0
Current timestep = 350. State = [[-0.0736463  -0.27749878]]. Action = [[-0.02928853 -0.19608165  0.18580776  0.8413415 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 350 is [True, False, False, True, False, False]
Current timestep = 351. State = [[-0.07490345 -0.267042  ]]. Action = [[-0.14422056  0.19454291 -0.11531715 -0.84812486]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 351 is [True, False, False, True, False, False]
Current timestep = 352. State = [[-0.0792259 -0.2560652]]. Action = [[ 0.14791068 -0.06951663  0.03355822 -0.07477582]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 352 is [True, False, False, True, False, False]
Current timestep = 353. State = [[-0.07452988 -0.26430976]]. Action = [[ 0.12875801 -0.18506722 -0.24528857 -0.663716  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 353 is [True, False, False, True, False, False]
Current timestep = 354. State = [[-0.06577843 -0.28192726]]. Action = [[ 0.04167086 -0.13146283  0.07454169  0.82550573]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 354 is [True, False, False, True, False, False]
Current timestep = 355. State = [[-0.0536165 -0.2890312]]. Action = [[ 0.23558605  0.02416211  0.04471883 -0.6735285 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 355 is [True, False, False, True, False, False]
Current timestep = 356. State = [[-0.03859177 -0.2902216 ]]. Action = [[-0.1867681   0.07982978 -0.19702424 -0.30985355]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 356 is [True, False, False, True, False, False]
Current timestep = 357. State = [[-0.04216674 -0.2879848 ]]. Action = [[-0.11626393  0.04966769  0.04618073 -0.71061015]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 357 is [False, True, False, True, False, False]
Current timestep = 358. State = [[-0.04176782 -0.27723348]]. Action = [[ 0.18733028  0.10058102 -0.1123378  -0.14041978]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 358 is [False, True, False, True, False, False]
Current timestep = 359. State = [[-0.03310345 -0.25964484]]. Action = [[ 0.14480418  0.1070863   0.24503615 -0.33728087]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 359 is [False, True, False, True, False, False]
Scene graph at timestep 359 is [False, True, False, True, False, False]
State prediction error at timestep 359 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 359 of 1
Current timestep = 360. State = [[-0.01591729 -0.23770587]]. Action = [[ 0.14961675  0.14490747 -0.15007597  0.6341226 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 360 is [False, True, False, True, False, False]
Current timestep = 361. State = [[ 0.00538271 -0.21462628]]. Action = [[ 0.21937516  0.2173695  -0.06059603  0.15879405]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 361 is [False, True, False, True, False, False]
Scene graph at timestep 361 is [False, True, False, True, False, False]
State prediction error at timestep 361 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 361 of 1
Current timestep = 362. State = [[ 0.03142993 -0.19532701]]. Action = [[ 0.21896794 -0.04467456 -0.20598415  0.09143555]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 362 is [False, True, False, True, False, False]
Current timestep = 363. State = [[ 0.02898066 -0.2047668 ]]. Action = [[-0.16559674 -0.14949214  0.05539945 -0.8048481 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 363 is [False, True, False, True, False, False]
Current timestep = 364. State = [[ 0.02527527 -0.21360758]]. Action = [[ 0.24741888 -0.08165711 -0.10758702 -0.74822325]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 364 is [False, True, False, True, False, False]
Current timestep = 365. State = [[ 0.02489293 -0.20175612]]. Action = [[-0.05187099  0.24195611 -0.20000911 -0.45852518]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 365 is [False, True, False, True, False, False]
Current timestep = 366. State = [[ 0.02565109 -0.18946616]]. Action = [[ 0.17737281 -0.05026336  0.1594159  -0.8690667 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 366 is [False, True, False, True, False, False]
Current timestep = 367. State = [[ 0.03276099 -0.19006412]]. Action = [[ 0.16235363 -0.04800667  0.172537    0.22036195]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 367 is [False, True, False, True, False, False]
Scene graph at timestep 367 is [False, True, False, True, False, False]
State prediction error at timestep 367 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 367 of 1
Current timestep = 368. State = [[ 0.05149021 -0.18110444]]. Action = [[-0.12990852  0.1897583   0.09656698 -0.8747913 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 368 is [False, True, False, True, False, False]
Scene graph at timestep 368 is [False, False, True, True, False, False]
State prediction error at timestep 368 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 368 of 1
Current timestep = 369. State = [[ 0.04696412 -0.15904544]]. Action = [[-0.18679796  0.18532118  0.07466444  0.9537939 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 369 is [False, False, True, True, False, False]
Current timestep = 370. State = [[ 0.03910518 -0.14564025]]. Action = [[ 0.19563884  0.01698792 -0.03722849 -0.14779353]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 370 is [False, True, False, True, False, False]
Current timestep = 371. State = [[ 0.02857761 -0.12998983]]. Action = [[-0.18667027  0.21276939 -0.15407301  0.00356805]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 371 is [False, True, False, True, False, False]
Scene graph at timestep 371 is [False, True, False, True, False, False]
State prediction error at timestep 371 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 371 of 1
Current timestep = 372. State = [[-0.24249004  0.01268478]]. Action = [[ 0.16069475  0.15324646  0.21583617 -0.56293327]]. Reward = [100.]
Curr episode timestep = 51
Scene graph at timestep 372 is [False, True, False, True, False, False]
Scene graph at timestep 372 is [True, False, False, False, True, False]
State prediction error at timestep 372 is tensor(0.0345, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 372 of 1
Current timestep = 373. State = [[-0.22926095  0.01217781]]. Action = [[ 0.16078323 -0.08440724 -0.16350378 -0.83498   ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 373 is [True, False, False, False, True, False]
Current timestep = 374. State = [[-0.2233887   0.00792884]]. Action = [[-0.18383034 -0.03987354  0.1109488  -0.5426503 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 374 is [True, False, False, False, True, False]
Current timestep = 375. State = [[-2.3676601e-01 -1.8136505e-04]]. Action = [[-0.19553801 -0.05611303 -0.17901212  0.9701662 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 375 is [True, False, False, False, True, False]
Scene graph at timestep 375 is [True, False, False, False, True, False]
State prediction error at timestep 375 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 375 of -1
Current timestep = 376. State = [[-0.24433985 -0.02015359]]. Action = [[ 0.14194903 -0.23372887 -0.11035132  0.31153953]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 376 is [True, False, False, False, True, False]
Scene graph at timestep 376 is [True, False, False, False, True, False]
State prediction error at timestep 376 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 376 of 0
Current timestep = 377. State = [[-0.24394962 -0.03753284]]. Action = [[ 0.00586081  0.02415901 -0.23722118  0.50204253]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 377 is [True, False, False, False, True, False]
Scene graph at timestep 377 is [True, False, False, False, True, False]
State prediction error at timestep 377 is tensor(7.6768e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 377 of 0
Current timestep = 378. State = [[-0.2395541  -0.03604435]]. Action = [[ 0.09530196  0.03930423  0.06148559 -0.43123102]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 378 is [True, False, False, False, True, False]
Current timestep = 379. State = [[-0.22461762 -0.04165819]]. Action = [[ 0.20990616 -0.14612304  0.13927588  0.4630481 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 379 is [True, False, False, False, True, False]
Current timestep = 380. State = [[-0.20359516 -0.04647337]]. Action = [[ 0.07709542  0.02924427  0.10223728 -0.40867174]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 380 is [True, False, False, False, True, False]
Scene graph at timestep 380 is [True, False, False, False, True, False]
State prediction error at timestep 380 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 380 of 1
Current timestep = 381. State = [[-0.18876137 -0.03412421]]. Action = [[ 0.08467194  0.23452115 -0.08000505  0.28005612]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 381 is [True, False, False, False, True, False]
Current timestep = 382. State = [[-0.18886475 -0.00885973]]. Action = [[-0.15265     0.19059151  0.19557679 -0.5844645 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 382 is [True, False, False, False, True, False]
Scene graph at timestep 382 is [True, False, False, False, True, False]
State prediction error at timestep 382 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 382 of 1
Current timestep = 383. State = [[-0.20096254  0.01595151]]. Action = [[-0.17428833  0.06272945  0.13781512 -0.01972562]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 383 is [True, False, False, False, True, False]
Scene graph at timestep 383 is [True, False, False, False, True, False]
State prediction error at timestep 383 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 383 of -1
Current timestep = 384. State = [[-0.20769407  0.02240736]]. Action = [[ 0.01016262 -0.0194272   0.20531768 -0.6821147 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 384 is [True, False, False, False, True, False]
Current timestep = 385. State = [[-0.20011152  0.01208005]]. Action = [[ 0.2386713  -0.14758605 -0.00670873  0.75813615]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 385 is [True, False, False, False, True, False]
Current timestep = 386. State = [[-0.19813043  0.01481224]]. Action = [[-0.09084979  0.22528076 -0.19712095  0.76156354]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 386 is [True, False, False, False, True, False]
Scene graph at timestep 386 is [True, False, False, False, True, False]
State prediction error at timestep 386 is tensor(1.2026e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.19940317  0.03821691]]. Action = [[ 0.09301889  0.17034265 -0.06781586 -0.61474943]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 387 is [True, False, False, False, True, False]
Current timestep = 388. State = [[-0.19883816  0.04917642]]. Action = [[-0.17452025 -0.01750605  0.02851719  0.97051585]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 388 is [True, False, False, False, True, False]
Scene graph at timestep 388 is [True, False, False, False, True, False]
State prediction error at timestep 388 is tensor(2.4153e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 388 of -1
Current timestep = 389. State = [[-0.20180555  0.05053934]]. Action = [[ 0.0334641  -0.05099699  0.11491537 -0.50020367]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 389 is [True, False, False, False, True, False]
Scene graph at timestep 389 is [True, False, False, False, True, False]
State prediction error at timestep 389 is tensor(6.0546e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 389 of 1
Current timestep = 390. State = [[-0.20928471  0.04108337]]. Action = [[-0.22339438 -0.10816431 -0.06133884  0.6415459 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 390 is [True, False, False, False, True, False]
Current timestep = 391. State = [[-0.22359042  0.02558905]]. Action = [[-0.09102352 -0.12210368  0.01743254 -0.29673123]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 391 is [True, False, False, False, True, False]
Scene graph at timestep 391 is [True, False, False, False, True, False]
State prediction error at timestep 391 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 391 of -1
Current timestep = 392. State = [[-0.2365763   0.01571688]]. Action = [[-0.03732474  0.03405803 -0.19362916 -0.32388282]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 392 is [True, False, False, False, True, False]
Scene graph at timestep 392 is [True, False, False, False, True, False]
State prediction error at timestep 392 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 392 of 0
Current timestep = 393. State = [[-0.23915164  0.0176749 ]]. Action = [[-0.22656542 -0.11324036  0.08501673 -0.5602827 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 393 is [True, False, False, False, True, False]
Scene graph at timestep 393 is [True, False, False, False, True, False]
State prediction error at timestep 393 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 393 of 0
Current timestep = 394. State = [[-0.23429675  0.02720833]]. Action = [[ 0.21804902  0.16957429 -0.04072896 -0.73983335]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 394 is [True, False, False, False, True, False]
Current timestep = 395. State = [[-0.2185967  0.0446515]]. Action = [[ 0.20925945  0.15005702 -0.10902956  0.38223886]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 395 is [True, False, False, False, True, False]
Scene graph at timestep 395 is [True, False, False, False, True, False]
State prediction error at timestep 395 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 395 of 1
Current timestep = 396. State = [[-0.19465426  0.05388488]]. Action = [[ 0.11180109 -0.14343959 -0.0803657   0.780308  ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 396 is [True, False, False, False, True, False]
Scene graph at timestep 396 is [True, False, False, False, True, False]
State prediction error at timestep 396 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 396 of 1
Current timestep = 397. State = [[-0.17963517  0.05244801]]. Action = [[ 0.17244008  0.15288892  0.12344533 -0.38214016]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 397 is [True, False, False, False, True, False]
Current timestep = 398. State = [[-0.1601538   0.05259934]]. Action = [[ 0.05665296 -0.1451649  -0.01935245  0.7069316 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 398 is [True, False, False, False, True, False]
Current timestep = 399. State = [[-0.148323    0.03342479]]. Action = [[ 0.10405213 -0.22496815 -0.10722524 -0.44737124]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 399 is [True, False, False, False, True, False]
Scene graph at timestep 399 is [True, False, False, False, True, False]
State prediction error at timestep 399 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 399 of 1
Current timestep = 400. State = [[-0.13702492  0.00190181]]. Action = [[-0.03953116 -0.20936304  0.21773309  0.65148044]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 400 is [True, False, False, False, True, False]
Current timestep = 401. State = [[-0.13545527  0.00022253]]. Action = [[ 0.08222222  0.21687376 -0.11315662 -0.90111953]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 401 is [True, False, False, False, True, False]
Current timestep = 402. State = [[-0.12884197  0.01303479]]. Action = [[ 0.08362859  0.09112695 -0.08377652 -0.06378096]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 402 is [True, False, False, False, True, False]
Current timestep = 403. State = [[-0.10751219  0.02985095]]. Action = [[0.23426867 0.12828451 0.0456523  0.04283035]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 403 is [True, False, False, False, True, False]
Current timestep = 404. State = [[-0.08865692  0.02625145]]. Action = [[-0.03191771 -0.23087017  0.00859046  0.6979754 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 404 is [True, False, False, False, True, False]
Current timestep = 405. State = [[-0.08011641  0.02834377]]. Action = [[ 0.13760605  0.21786869 -0.22360848  0.44972634]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 405 is [True, False, False, False, True, False]
Scene graph at timestep 405 is [True, False, False, False, True, False]
State prediction error at timestep 405 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 405 of 1
Current timestep = 406. State = [[-0.07232069  0.05073649]]. Action = [[-0.18363027  0.15865058 -0.20662984 -0.66463286]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 406 is [True, False, False, False, True, False]
Scene graph at timestep 406 is [True, False, False, False, True, False]
State prediction error at timestep 406 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 406 of -1
Current timestep = 407. State = [[-0.08164003  0.07635087]]. Action = [[-0.0485701   0.15930474 -0.24704039  0.39699352]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 407 is [True, False, False, False, True, False]
Scene graph at timestep 407 is [True, False, False, False, True, False]
State prediction error at timestep 407 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 407 of -1
Current timestep = 408. State = [[-0.08705556  0.08886278]]. Action = [[-0.07059053 -0.01560187  0.16331929  0.75351   ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 408 is [True, False, False, False, True, False]
Scene graph at timestep 408 is [True, False, False, False, True, False]
State prediction error at timestep 408 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 408 of 0
Current timestep = 409. State = [[-0.08860181  0.09110118]]. Action = [[ 0.17169559  0.07799596 -0.1130195   0.24280226]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 409 is [True, False, False, False, True, False]
Scene graph at timestep 409 is [True, False, False, False, True, False]
State prediction error at timestep 409 is tensor(9.4914e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 409 of 0
Current timestep = 410. State = [[-0.0876069   0.09404989]]. Action = [[ 0.01667893 -0.02398118  0.16344854  0.2657597 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 410 is [True, False, False, False, True, False]
Current timestep = 411. State = [[-0.08378004  0.08004135]]. Action = [[ 0.02899319 -0.23025605 -0.02675442  0.46632576]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 411 is [True, False, False, False, True, False]
Scene graph at timestep 411 is [True, False, False, False, True, False]
State prediction error at timestep 411 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 411 of 1
Current timestep = 412. State = [[-0.06799438  0.05760094]]. Action = [[ 0.24675429 -0.1086244   0.21964651 -0.88528275]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 412 is [True, False, False, False, True, False]
Scene graph at timestep 412 is [True, False, False, False, True, False]
State prediction error at timestep 412 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 412 of 1
Current timestep = 413. State = [[-0.05148198  0.05167354]]. Action = [[-0.19368088  0.0557003   0.01977918  0.48148632]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 413 is [True, False, False, False, True, False]
Scene graph at timestep 413 is [True, False, False, False, True, False]
State prediction error at timestep 413 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 413 of 0
Current timestep = 414. State = [[-0.05351079  0.04080075]]. Action = [[ 0.00220555 -0.23689523 -0.05493903  0.15953839]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 414 is [True, False, False, False, True, False]
Scene graph at timestep 414 is [True, False, False, False, True, False]
State prediction error at timestep 414 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 414 of 1
Current timestep = 415. State = [[-0.0557356  0.0247459]]. Action = [[ 0.05436483  0.04259962  0.10687423 -0.5479121 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 415 is [True, False, False, False, True, False]
Scene graph at timestep 415 is [True, False, False, False, True, False]
State prediction error at timestep 415 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 415 of 1
Current timestep = 416. State = [[-0.0510939   0.03423135]]. Action = [[ 0.17220372  0.15231156 -0.13955396 -0.9216255 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 416 is [True, False, False, False, True, False]
Current timestep = 417. State = [[-0.1503926  -0.06168189]]. Action = [[ 0.17287731 -0.03131169 -0.03356656 -0.6035014 ]]. Reward = [100.]
Curr episode timestep = 44
Scene graph at timestep 417 is [True, False, False, False, True, False]
Current timestep = 418. State = [[-0.1350257  -0.07763772]]. Action = [[-0.11217763 -0.12399927  0.07754827  0.5091498 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 418 is [True, False, False, False, True, False]
Scene graph at timestep 418 is [True, False, False, False, True, False]
State prediction error at timestep 418 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 418 of -1
Current timestep = 419. State = [[-0.1312451  -0.08489537]]. Action = [[ 0.24253756  0.10192233 -0.09296371 -0.8090106 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 419 is [True, False, False, False, True, False]
Current timestep = 420. State = [[-0.10669381 -0.08115185]]. Action = [[ 0.22227535 -0.03057776 -0.1569754  -0.6731054 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 420 is [True, False, False, False, True, False]
Scene graph at timestep 420 is [True, False, False, False, True, False]
State prediction error at timestep 420 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 420 of 1
Current timestep = 421. State = [[-0.08017722 -0.07191826]]. Action = [[ 0.04797304  0.16146311  0.05430505 -0.7844852 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 421 is [True, False, False, False, True, False]
Scene graph at timestep 421 is [True, False, False, False, True, False]
State prediction error at timestep 421 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 421 of 1
Current timestep = 422. State = [[-0.07851555 -0.05079351]]. Action = [[-0.12471071  0.14982653 -0.1430486  -0.87284356]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 422 is [True, False, False, False, True, False]
Current timestep = 423. State = [[-0.08567777 -0.03158559]]. Action = [[-0.15385637  0.12157747  0.06518117 -0.94454086]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 423 is [True, False, False, False, True, False]
Current timestep = 424. State = [[-0.0884708 -0.015049 ]]. Action = [[ 0.1687172   0.09661126  0.05442786 -0.23828518]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 424 is [True, False, False, False, True, False]
Scene graph at timestep 424 is [True, False, False, False, True, False]
State prediction error at timestep 424 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 424 of 1
Current timestep = 425. State = [[-0.07860514 -0.00165718]]. Action = [[ 0.24175766  0.08987936 -0.16216938 -0.29760724]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 425 is [True, False, False, False, True, False]
Current timestep = 426. State = [[-0.05112808  0.00905469]]. Action = [[0.19242409 0.08671859 0.09275389 0.76443076]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 426 is [True, False, False, False, True, False]
Scene graph at timestep 426 is [True, False, False, False, True, False]
State prediction error at timestep 426 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 426 of 1
Current timestep = 427. State = [[-0.24075209  0.12496423]]. Action = [[ 0.24625915 -0.14105053  0.19811195 -0.75612885]]. Reward = [100.]
Curr episode timestep = 9
Scene graph at timestep 427 is [True, False, False, False, True, False]
Scene graph at timestep 427 is [True, False, False, False, True, False]
State prediction error at timestep 427 is tensor(0.0254, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 427 of 0
Current timestep = 428. State = [[-0.23373659  0.13993448]]. Action = [[ 0.02480906 -0.0171476  -0.2335993   0.76064813]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 428 is [True, False, False, False, True, False]
Scene graph at timestep 428 is [True, False, False, False, False, True]
State prediction error at timestep 428 is tensor(9.0215e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 428 of 0
Current timestep = 429. State = [[-0.23350774  0.1291213 ]]. Action = [[-0.18687634 -0.21924374 -0.18703035  0.9467273 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 429 is [True, False, False, False, False, True]
Current timestep = 430. State = [[-0.24513097  0.12986526]]. Action = [[-0.11609781  0.21525174  0.19846019  0.692433  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 430 is [True, False, False, False, False, True]
Current timestep = 431. State = [[-0.24806847  0.14927849]]. Action = [[ 0.23328918  0.1518929  -0.2100602  -0.783344  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 431 is [True, False, False, False, False, True]
Current timestep = 432. State = [[-0.23504518  0.16203745]]. Action = [[0.14176568 0.101013   0.22238636 0.2522595 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 432 is [True, False, False, False, False, True]
Current timestep = 433. State = [[-0.21858452  0.16125284]]. Action = [[ 0.04853714 -0.1939703  -0.03469792  0.90315807]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 433 is [True, False, False, False, False, True]
Scene graph at timestep 433 is [True, False, False, False, False, True]
State prediction error at timestep 433 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 433 of 0
Current timestep = 434. State = [[-0.21170723  0.14385585]]. Action = [[-0.0522268  -0.1349922  -0.20846212 -0.30175948]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 434 is [True, False, False, False, False, True]
Current timestep = 435. State = [[-0.21099764  0.12984641]]. Action = [[-0.06211248 -0.12169811  0.0473325   0.4777472 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 435 is [True, False, False, False, False, True]
Current timestep = 436. State = [[-0.22056781  0.12429365]]. Action = [[-0.20831002  0.04313433 -0.09163702 -0.06455165]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 436 is [True, False, False, False, False, True]
Current timestep = 437. State = [[-0.24302     0.11779664]]. Action = [[-0.20594624 -0.12746505  0.22962844  0.04329658]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 437 is [True, False, False, False, True, False]
Current timestep = 438. State = [[-0.25528586  0.11019886]]. Action = [[ 0.1617403   0.03324923 -0.0425299   0.8408048 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 438 is [True, False, False, False, True, False]
Current timestep = 439. State = [[-0.25134894  0.11126629]]. Action = [[ 0.08918422  0.02121606  0.19028145 -0.17762232]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 439 is [True, False, False, False, True, False]
Scene graph at timestep 439 is [True, False, False, False, True, False]
State prediction error at timestep 439 is tensor(1.2436e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 439 of -1
Current timestep = 440. State = [[-0.24196662  0.11901187]]. Action = [[ 0.1246742   0.1485678  -0.07006246 -0.5720614 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 440 is [True, False, False, False, True, False]
Scene graph at timestep 440 is [True, False, False, False, True, False]
State prediction error at timestep 440 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 440 of -1
Current timestep = 441. State = [[-0.22395572  0.12002451]]. Action = [[ 0.18040347 -0.13646486 -0.1858551  -0.081343  ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 441 is [True, False, False, False, True, False]
Current timestep = 442. State = [[-0.20618302  0.1045838 ]]. Action = [[ 0.12915373 -0.14847523 -0.15107854 -0.6948977 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 442 is [True, False, False, False, True, False]
Scene graph at timestep 442 is [True, False, False, False, True, False]
State prediction error at timestep 442 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 442 of 1
Current timestep = 443. State = [[-0.19091967  0.08974035]]. Action = [[-0.11715743 -0.0694665  -0.16762348  0.18603528]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 443 is [True, False, False, False, True, False]
Scene graph at timestep 443 is [True, False, False, False, True, False]
State prediction error at timestep 443 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 443 of 1
Current timestep = 444. State = [[-0.19389494  0.09067966]]. Action = [[ 0.00975248  0.14221555  0.2120598  -0.7566173 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 444 is [True, False, False, False, True, False]
Scene graph at timestep 444 is [True, False, False, False, True, False]
State prediction error at timestep 444 is tensor(1.2840e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 444 of -1
Current timestep = 445. State = [[-0.19389516  0.08543946]]. Action = [[ 0.01399446 -0.23096308  0.12016979  0.8873415 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 445 is [True, False, False, False, True, False]
Scene graph at timestep 445 is [True, False, False, False, True, False]
State prediction error at timestep 445 is tensor(8.4595e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 445 of 1
Current timestep = 446. State = [[-0.19914366  0.06715927]]. Action = [[-0.22642697 -0.08541331  0.04540187  0.504704  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 446 is [True, False, False, False, True, False]
Scene graph at timestep 446 is [True, False, False, False, True, False]
State prediction error at timestep 446 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 446 of -1
Current timestep = 447. State = [[-0.21531937  0.06994959]]. Action = [[-0.02176578  0.20258296 -0.14000592  0.4383428 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 447 is [True, False, False, False, True, False]
Current timestep = 448. State = [[-0.22413716  0.07942755]]. Action = [[-0.12493649 -0.06933227  0.15116593 -0.6584185 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 448 is [True, False, False, False, True, False]
Scene graph at timestep 448 is [True, False, False, False, True, False]
State prediction error at timestep 448 is tensor(9.4579e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 448 of -1
Current timestep = 449. State = [[-0.23754483  0.07203992]]. Action = [[-0.13518453 -0.11377169  0.12605274  0.00059807]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 449 is [True, False, False, False, True, False]
Scene graph at timestep 449 is [True, False, False, False, True, False]
State prediction error at timestep 449 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 449 of -1
Current timestep = 450. State = [[-0.25277373  0.06240338]]. Action = [[-0.22896567  0.21916819  0.16033399  0.8985729 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 450 is [True, False, False, False, True, False]
Current timestep = 451. State = [[-0.24838541  0.07295544]]. Action = [[0.19554421 0.2217803  0.15718123 0.47880983]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 451 is [True, False, False, False, True, False]
Current timestep = 452. State = [[-0.24404433  0.07277796]]. Action = [[-0.09705886 -0.2118451   0.16420025 -0.60663176]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 452 is [True, False, False, False, True, False]
Scene graph at timestep 452 is [True, False, False, False, True, False]
State prediction error at timestep 452 is tensor(2.3059e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 452 of 0
Current timestep = 453. State = [[-0.2498207   0.07458913]]. Action = [[-0.11746526  0.18132865 -0.11292949 -0.18975067]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 453 is [True, False, False, False, True, False]
Current timestep = 454. State = [[-0.2499642   0.07525972]]. Action = [[ 0.20393646 -0.16470183 -0.15732412 -0.04615366]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 454 is [True, False, False, False, True, False]
Scene graph at timestep 454 is [True, False, False, False, True, False]
State prediction error at timestep 454 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 454 of 0
Current timestep = 455. State = [[-0.24478729  0.06993151]]. Action = [[-0.05006133  0.10813835 -0.14730091 -0.9418444 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 455 is [True, False, False, False, True, False]
Scene graph at timestep 455 is [True, False, False, False, True, False]
State prediction error at timestep 455 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 455 of -1
Current timestep = 456. State = [[-0.23788409  0.06251632]]. Action = [[ 0.1970005  -0.22986557 -0.17490382 -0.70112574]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 456 is [True, False, False, False, True, False]
Scene graph at timestep 456 is [True, False, False, False, True, False]
State prediction error at timestep 456 is tensor(4.9169e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 456 of 1
Current timestep = 457. State = [[-0.22443311  0.04439318]]. Action = [[ 0.04928303 -0.01786233 -0.16574085  0.38475835]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 457 is [True, False, False, False, True, False]
Current timestep = 458. State = [[-0.21064945  0.04408713]]. Action = [[ 0.20143238  0.02025685 -0.16882415 -0.6008219 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 458 is [True, False, False, False, True, False]
Current timestep = 459. State = [[-0.201147    0.05119492]]. Action = [[-0.18520427  0.10045862  0.23906487 -0.94964176]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 459 is [True, False, False, False, True, False]
Current timestep = 460. State = [[-0.20574157  0.05950205]]. Action = [[-0.06118491  0.04520503  0.1210736   0.2904265 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 460 is [True, False, False, False, True, False]
Current timestep = 461. State = [[-0.20257413  0.06920972]]. Action = [[ 0.22743192  0.10651404 -0.23955105  0.78854835]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 461 is [True, False, False, False, True, False]
Current timestep = 462. State = [[-0.19745006  0.08894826]]. Action = [[-0.03188919  0.23558581  0.11070356  0.01212192]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 462 is [True, False, False, False, True, False]
Current timestep = 463. State = [[-0.19592915  0.09504595]]. Action = [[ 0.05242595 -0.19071399 -0.07823905  0.50365543]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 463 is [True, False, False, False, True, False]
Current timestep = 464. State = [[-0.19612923  0.08666496]]. Action = [[-0.16724303 -0.0616124   0.0449     -0.87980413]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 464 is [True, False, False, False, True, False]
Scene graph at timestep 464 is [True, False, False, False, True, False]
State prediction error at timestep 464 is tensor(4.4092e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 464 of 1
Current timestep = 465. State = [[-0.19237234  0.06937871]]. Action = [[ 0.2116164  -0.19789113  0.12178135 -0.9278946 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 465 is [True, False, False, False, True, False]
Scene graph at timestep 465 is [True, False, False, False, True, False]
State prediction error at timestep 465 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 465 of 1
Current timestep = 466. State = [[-0.1757236   0.04455333]]. Action = [[ 0.17315656 -0.11153552 -0.19309549 -0.88153666]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 466 is [True, False, False, False, True, False]
Current timestep = 467. State = [[-0.1591571   0.03890773]]. Action = [[ 0.09525353  0.05241039 -0.01496154  0.44652164]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 467 is [True, False, False, False, True, False]
Current timestep = 468. State = [[-0.14300855  0.03666267]]. Action = [[ 0.1235463  -0.07390574 -0.23112181  0.8131989 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 468 is [True, False, False, False, True, False]
Current timestep = 469. State = [[-0.13000865  0.03779668]]. Action = [[ 0.01474816  0.11670923 -0.2369739  -0.35743648]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 469 is [True, False, False, False, True, False]
Current timestep = 470. State = [[-0.13381757  0.05106929]]. Action = [[-0.21715267  0.13603854  0.17089462 -0.54815394]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 470 is [True, False, False, False, True, False]
Current timestep = 471. State = [[-0.13517088  0.07246598]]. Action = [[ 0.21747488  0.1713014   0.11797893 -0.6730992 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 471 is [True, False, False, False, True, False]
Scene graph at timestep 471 is [True, False, False, False, True, False]
State prediction error at timestep 471 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 471 of 1
Current timestep = 472. State = [[-0.12354059  0.07817585]]. Action = [[-0.06426688 -0.192223    0.01746753 -0.568036  ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 472 is [True, False, False, False, True, False]
Scene graph at timestep 472 is [True, False, False, False, True, False]
State prediction error at timestep 472 is tensor(7.5100e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 472 of 1
Current timestep = 473. State = [[-0.12499392  0.0704316 ]]. Action = [[-0.06511915  0.06151667  0.23142868 -0.6333517 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 473 is [True, False, False, False, True, False]
Scene graph at timestep 473 is [True, False, False, False, True, False]
State prediction error at timestep 473 is tensor(8.1121e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 473 of 0
Current timestep = 474. State = [[-0.13214159  0.07064768]]. Action = [[-0.15259662 -0.04442424  0.19656092 -0.8346013 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 474 is [True, False, False, False, True, False]
Current timestep = 475. State = [[-0.13648447  0.07836134]]. Action = [[ 0.13564613  0.17608345  0.15179282 -0.2491396 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 475 is [True, False, False, False, True, False]
Current timestep = 476. State = [[-0.12832251  0.07494856]]. Action = [[ 0.23206621 -0.2182448   0.04129982  0.9725132 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 476 is [True, False, False, False, True, False]
Scene graph at timestep 476 is [True, False, False, False, True, False]
State prediction error at timestep 476 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 476 of 1
Current timestep = 477. State = [[-0.10668713  0.04811411]]. Action = [[ 0.18534434 -0.21462071 -0.06162389 -0.11432499]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 477 is [True, False, False, False, True, False]
Scene graph at timestep 477 is [True, False, False, False, True, False]
State prediction error at timestep 477 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 477 of 1
Current timestep = 478. State = [[-0.09343687  0.03730031]]. Action = [[-0.23821624  0.10983098 -0.01147775 -0.28144532]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 478 is [True, False, False, False, True, False]
Scene graph at timestep 478 is [True, False, False, False, True, False]
State prediction error at timestep 478 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 478 of -1
Current timestep = 479. State = [[-0.09511241  0.03054278]]. Action = [[ 0.1876018  -0.23462798  0.22757244  0.58987606]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 479 is [True, False, False, False, True, False]
Current timestep = 480. State = [[-0.08220262  0.0071376 ]]. Action = [[ 0.21500671 -0.12791961  0.03098449 -0.09505534]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 480 is [True, False, False, False, True, False]
Current timestep = 481. State = [[-0.06143957  0.00630325]]. Action = [[ 0.08404496  0.19627947 -0.1335724   0.76165366]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 481 is [True, False, False, False, True, False]
Current timestep = 482. State = [[-0.0446458   0.02636046]]. Action = [[ 0.14288676  0.19885603 -0.00384851  0.000476  ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 482 is [True, False, False, False, True, False]
Scene graph at timestep 482 is [False, True, False, False, True, False]
State prediction error at timestep 482 is tensor(7.4199e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 482 of 1
Current timestep = 483. State = [[-0.24037799  0.03039521]]. Action = [[-0.16059841 -0.09708536  0.21916443 -0.69098824]]. Reward = [100.]
Curr episode timestep = 55
Scene graph at timestep 483 is [False, True, False, False, True, False]
Scene graph at timestep 483 is [True, False, False, False, True, False]
State prediction error at timestep 483 is tensor(0.0190, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 483 of 0
Current timestep = 484. State = [[-0.23682314  0.0387066 ]]. Action = [[0.03815097 0.06483236 0.18247274 0.06154037]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 484 is [True, False, False, False, True, False]
Current timestep = 485. State = [[-0.23735906  0.05489334]]. Action = [[-0.02204205  0.20463791  0.18195558 -0.38097775]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 485 is [True, False, False, False, True, False]
Scene graph at timestep 485 is [True, False, False, False, True, False]
State prediction error at timestep 485 is tensor(2.6496e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 485 of -1
Current timestep = 486. State = [[-0.23561455  0.06525042]]. Action = [[-0.11510743 -0.19531539  0.16639936  0.6361592 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 486 is [True, False, False, False, True, False]
Scene graph at timestep 486 is [True, False, False, False, True, False]
State prediction error at timestep 486 is tensor(6.0929e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 486 of 1
Current timestep = 487. State = [[-0.23524603  0.05413003]]. Action = [[0.13350862 0.0732682  0.07813746 0.21902406]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 487 is [True, False, False, False, True, False]
Current timestep = 488. State = [[-0.22260833  0.04258293]]. Action = [[ 0.17186892 -0.24575679 -0.12048614 -0.4597354 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 488 is [True, False, False, False, True, False]
Scene graph at timestep 488 is [True, False, False, False, True, False]
State prediction error at timestep 488 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 488 of 1
Current timestep = 489. State = [[-0.200437    0.03804754]]. Action = [[ 0.14717758  0.18705082 -0.13557474  0.2325294 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 489 is [True, False, False, False, True, False]
Scene graph at timestep 489 is [True, False, False, False, True, False]
State prediction error at timestep 489 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 489 of 1
Current timestep = 490. State = [[-0.17514966  0.05442354]]. Action = [[ 0.23665446  0.10068452 -0.1498215  -0.47438085]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 490 is [True, False, False, False, True, False]
Current timestep = 491. State = [[-0.14506452  0.06872215]]. Action = [[0.23395786 0.12616217 0.17820871 0.5739417 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 491 is [True, False, False, False, True, False]
Current timestep = 492. State = [[-0.11591428  0.06675721]]. Action = [[ 0.09797201 -0.2338612  -0.12981845 -0.56371456]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 492 is [True, False, False, False, True, False]
Current timestep = 493. State = [[-0.09855855  0.06986374]]. Action = [[ 0.15611684  0.24472171  0.02602571 -0.95741475]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 493 is [True, False, False, False, True, False]
Scene graph at timestep 493 is [True, False, False, False, True, False]
State prediction error at timestep 493 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 493 of 1
Current timestep = 494. State = [[-0.08544182  0.09701826]]. Action = [[-0.21871997  0.18804574  0.2346169   0.1915468 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 494 is [True, False, False, False, True, False]
Current timestep = 495. State = [[-0.09890234  0.11888462]]. Action = [[-0.15811013  0.06229323  0.07853013 -0.03341806]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 495 is [True, False, False, False, True, False]
Current timestep = 496. State = [[-0.11378325  0.13837813]]. Action = [[-0.09566239  0.16248834 -0.05553578 -0.8985655 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 496 is [True, False, False, False, True, False]
Scene graph at timestep 496 is [True, False, False, False, False, True]
State prediction error at timestep 496 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 496 of -1
Current timestep = 497. State = [[-0.1281644   0.14473574]]. Action = [[-0.09990394 -0.14516474 -0.14961497 -0.17252302]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 497 is [True, False, False, False, False, True]
Scene graph at timestep 497 is [True, False, False, False, False, True]
State prediction error at timestep 497 is tensor(4.6244e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 497 of 1
Current timestep = 498. State = [[-0.13949573  0.11973175]]. Action = [[-0.15570623 -0.24646768 -0.24715266  0.49435246]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 498 is [True, False, False, False, False, True]
Current timestep = 499. State = [[-0.15913479  0.08936568]]. Action = [[-0.1441185  -0.20498188  0.22876403  0.8531718 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 499 is [True, False, False, False, True, False]
Scene graph at timestep 499 is [True, False, False, False, True, False]
State prediction error at timestep 499 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of 1
Current timestep = 500. State = [[-0.17722914  0.07409355]]. Action = [[-0.0382027   0.09195188  0.15304959 -0.22636008]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 500 is [True, False, False, False, True, False]
Current timestep = 501. State = [[-0.18781109  0.067917  ]]. Action = [[-0.18447979 -0.19843182 -0.09275655 -0.81808555]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 501 is [True, False, False, False, True, False]
Current timestep = 502. State = [[-0.20860478  0.05609131]]. Action = [[-0.06979936  0.01841015 -0.12693484  0.45761907]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 502 is [True, False, False, False, True, False]
Scene graph at timestep 502 is [True, False, False, False, True, False]
State prediction error at timestep 502 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 502 of -1
Current timestep = 503. State = [[-0.21009965  0.04548428]]. Action = [[ 0.16377836 -0.13792796 -0.04927981 -0.81473863]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 503 is [True, False, False, False, True, False]
Current timestep = 504. State = [[-0.2026006   0.02287555]]. Action = [[ 0.03495932 -0.22797734  0.11354071 -0.4818287 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 504 is [True, False, False, False, True, False]
Current timestep = 505. State = [[-0.19224648 -0.00815637]]. Action = [[ 0.19594479 -0.21089551 -0.13204898  0.8730717 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 505 is [True, False, False, False, True, False]
Scene graph at timestep 505 is [True, False, False, False, True, False]
State prediction error at timestep 505 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 505 of 1
Current timestep = 506. State = [[-0.18402363 -0.03550275]]. Action = [[-0.1558856  -0.04749586  0.2027682  -0.57215476]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 506 is [True, False, False, False, True, False]
Current timestep = 507. State = [[-0.19330677 -0.03924932]]. Action = [[-0.15377484  0.01220691  0.10341567  0.03236485]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 507 is [True, False, False, False, True, False]
Current timestep = 508. State = [[-0.20082293 -0.02781791]]. Action = [[ 0.19416624  0.24641365 -0.17800908 -0.5119531 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 508 is [True, False, False, False, True, False]
Scene graph at timestep 508 is [True, False, False, False, True, False]
State prediction error at timestep 508 is tensor(8.4867e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 508 of 1
Current timestep = 509. State = [[-0.19119985 -0.00780659]]. Action = [[ 0.1563394   0.01402959 -0.01781346 -0.7399539 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 509 is [True, False, False, False, True, False]
Scene graph at timestep 509 is [True, False, False, False, True, False]
State prediction error at timestep 509 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 509 of 1
Current timestep = 510. State = [[-0.17990233 -0.01748201]]. Action = [[ 0.00243962 -0.22697675  0.03646553 -0.16295272]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 510 is [True, False, False, False, True, False]
Current timestep = 511. State = [[-0.17586765 -0.01913035]]. Action = [[ 0.03851384  0.21663806  0.16205275 -0.9386245 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 511 is [True, False, False, False, True, False]
Current timestep = 512. State = [[-0.17806406 -0.02297862]]. Action = [[-0.16524462 -0.22486395 -0.10567865 -0.37767023]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 512 is [True, False, False, False, True, False]
Current timestep = 513. State = [[-0.184811   -0.02351561]]. Action = [[-0.11397475  0.16271293  0.15575764 -0.06713456]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 513 is [True, False, False, False, True, False]
Scene graph at timestep 513 is [True, False, False, False, True, False]
State prediction error at timestep 513 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 513 of 0
Current timestep = 514. State = [[-0.20494479 -0.02990425]]. Action = [[-0.2107949  -0.23132838  0.16699815  0.74113476]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 514 is [True, False, False, False, True, False]
Scene graph at timestep 514 is [True, False, False, False, True, False]
State prediction error at timestep 514 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 514 of -1
Current timestep = 515. State = [[-0.2193208  -0.04089434]]. Action = [[ 0.15319067  0.14025849 -0.1923777  -0.23357487]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 515 is [True, False, False, False, True, False]
Current timestep = 516. State = [[-0.21263003 -0.04210443]]. Action = [[ 0.08950058 -0.17119718  0.06217712  0.76676655]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 516 is [True, False, False, False, True, False]
Current timestep = 517. State = [[-0.20454559 -0.05711624]]. Action = [[ 0.06055322 -0.12579648  0.19365174 -0.781594  ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 517 is [True, False, False, False, True, False]
Scene graph at timestep 517 is [True, False, False, False, True, False]
State prediction error at timestep 517 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 517 of 1
Current timestep = 518. State = [[-0.18851839 -0.0569839 ]]. Action = [[ 0.23727524  0.22195822  0.23984534 -0.5476311 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 518 is [True, False, False, False, True, False]
Scene graph at timestep 518 is [True, False, False, False, True, False]
State prediction error at timestep 518 is tensor(6.1717e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 518 of 1
Current timestep = 519. State = [[-0.16571303 -0.02856612]]. Action = [[ 0.09493414  0.210123    0.2260974  -0.8847706 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 519 is [True, False, False, False, True, False]
Scene graph at timestep 519 is [True, False, False, False, True, False]
State prediction error at timestep 519 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 519 of 1
Current timestep = 520. State = [[-0.15593033 -0.00290374]]. Action = [[ 0.02671465  0.13943869 -0.04314753  0.33806157]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 520 is [True, False, False, False, True, False]
Scene graph at timestep 520 is [True, False, False, False, True, False]
State prediction error at timestep 520 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 520 of 1
Current timestep = 521. State = [[-0.1439563   0.00276426]]. Action = [[ 0.21543449 -0.11529335 -0.21169582 -0.6262739 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 521 is [True, False, False, False, True, False]
Current timestep = 522. State = [[-0.12879916  0.00093068]]. Action = [[-0.02167092  0.05652189  0.15122837 -0.07057863]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 522 is [True, False, False, False, True, False]
Current timestep = 523. State = [[-0.12847173 -0.0042613 ]]. Action = [[-0.11342347 -0.12068543  0.10632628 -0.02937907]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 523 is [True, False, False, False, True, False]
Scene graph at timestep 523 is [True, False, False, False, True, False]
State prediction error at timestep 523 is tensor(9.5395e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 523 of 1
Current timestep = 524. State = [[-0.12398458 -0.00568848]]. Action = [[ 0.20834526  0.103609   -0.15595903  0.50788593]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 524 is [True, False, False, False, True, False]
Current timestep = 525. State = [[-0.1042754  -0.01498902]]. Action = [[ 0.19919103 -0.236384   -0.19261098  0.05620635]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 525 is [True, False, False, False, True, False]
Scene graph at timestep 525 is [True, False, False, False, True, False]
State prediction error at timestep 525 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 525 of 1
Current timestep = 526. State = [[-0.08607252 -0.02280316]]. Action = [[-0.10390715  0.13551128  0.24479687  0.0197413 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 526 is [True, False, False, False, True, False]
Current timestep = 527. State = [[-0.08346973 -0.01460544]]. Action = [[ 0.15982217  0.05092055 -0.00194366  0.38452554]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 527 is [True, False, False, False, True, False]
Current timestep = 528. State = [[-0.07990982 -0.00075993]]. Action = [[-0.14478008  0.1503518   0.08954567 -0.44501555]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 528 is [True, False, False, False, True, False]
Current timestep = 529. State = [[-0.08520743  0.00570275]]. Action = [[-0.09236325 -0.08879848 -0.22724369 -0.9681007 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 529 is [True, False, False, False, True, False]
Current timestep = 530. State = [[-0.08643584 -0.01011166]]. Action = [[ 0.04145023 -0.2292122  -0.07369933 -0.60544664]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 530 is [True, False, False, False, True, False]
Current timestep = 531. State = [[-0.09302348 -0.02143957]]. Action = [[-0.18490277  0.0495697  -0.05361873 -0.25929433]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 531 is [True, False, False, False, True, False]
Scene graph at timestep 531 is [True, False, False, False, True, False]
State prediction error at timestep 531 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 531 of 1
Current timestep = 532. State = [[-0.10248671 -0.0303519 ]]. Action = [[ 0.07844201 -0.10802755 -0.01665781  0.8110001 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 532 is [True, False, False, False, True, False]
Scene graph at timestep 532 is [True, False, False, False, True, False]
State prediction error at timestep 532 is tensor(3.3872e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 532 of 0
Current timestep = 533. State = [[-0.10340973 -0.02476231]]. Action = [[-0.00914259  0.2127989   0.18667799 -0.94167346]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 533 is [True, False, False, False, True, False]
Current timestep = 534. State = [[-0.09907588 -0.02292943]]. Action = [[ 0.19445693 -0.194652    0.23539516  0.88064194]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 534 is [True, False, False, False, True, False]
Scene graph at timestep 534 is [True, False, False, False, True, False]
State prediction error at timestep 534 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 534 of 1
Current timestep = 535. State = [[-0.09374123 -0.04528053]]. Action = [[-0.12252025 -0.19255282  0.1380493  -0.65718645]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 535 is [True, False, False, False, True, False]
Current timestep = 536. State = [[-0.09102114 -0.06881449]]. Action = [[ 0.1211625  -0.1601619   0.08059239 -0.07798159]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 536 is [True, False, False, False, True, False]
Scene graph at timestep 536 is [True, False, False, False, True, False]
State prediction error at timestep 536 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 536 of -1
Current timestep = 537. State = [[-0.09112467 -0.07215568]]. Action = [[-0.12755802  0.24874657  0.15937155 -0.30308855]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 537 is [True, False, False, False, True, False]
Scene graph at timestep 537 is [True, False, False, False, True, False]
State prediction error at timestep 537 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 537 of 1
Current timestep = 538. State = [[-0.09388233 -0.05148111]]. Action = [[ 0.08502391  0.06870079 -0.05359505  0.3387041 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 538 is [True, False, False, False, True, False]
Current timestep = 539. State = [[-0.09442192 -0.05626314]]. Action = [[-0.10494074 -0.1702393  -0.06069143  0.2903533 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 539 is [True, False, False, False, True, False]
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(1.7009e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 539 of 0
Current timestep = 540. State = [[-0.09576099 -0.053032  ]]. Action = [[ 0.0084931   0.18541276 -0.21050563  0.24584651]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 540 is [True, False, False, False, True, False]
Scene graph at timestep 540 is [True, False, False, False, True, False]
State prediction error at timestep 540 is tensor(2.0374e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 540 of 1
Current timestep = 541. State = [[-0.10361595 -0.0453487 ]]. Action = [[-0.19678113 -0.03939967 -0.04738358  0.412251  ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 541 is [True, False, False, False, True, False]
Scene graph at timestep 541 is [True, False, False, False, True, False]
State prediction error at timestep 541 is tensor(5.1431e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 541 of -1
Current timestep = 542. State = [[-0.11871285 -0.05141381]]. Action = [[-0.14211011 -0.06685942  0.16344023 -0.56549484]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 542 is [True, False, False, False, True, False]
Current timestep = 543. State = [[-0.126694   -0.04613534]]. Action = [[ 0.21175742  0.16894186 -0.16977747 -0.347158  ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 543 is [True, False, False, False, True, False]
Scene graph at timestep 543 is [True, False, False, False, True, False]
State prediction error at timestep 543 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 543 of -1
Current timestep = 544. State = [[-0.12322755 -0.04790733]]. Action = [[-0.0292591  -0.2286202   0.0786224  -0.07501459]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 544 is [True, False, False, False, True, False]
Scene graph at timestep 544 is [True, False, False, False, True, False]
State prediction error at timestep 544 is tensor(3.0811e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 544 of 0
Current timestep = 545. State = [[-0.12948471 -0.07375272]]. Action = [[-0.17331675 -0.19772808 -0.17676756 -0.83486176]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 545 is [True, False, False, False, True, False]
Current timestep = 546. State = [[-0.13298528 -0.08882185]]. Action = [[ 0.13559854 -0.0353286  -0.0996657  -0.20013058]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 546 is [True, False, False, False, True, False]
Current timestep = 547. State = [[-0.13220546 -0.09611058]]. Action = [[-0.08994263 -0.01867886  0.23396653 -0.59733534]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 547 is [True, False, False, False, True, False]
Current timestep = 548. State = [[-0.13469134 -0.09504148]]. Action = [[-0.07172495  0.09271678 -0.14735495 -0.26089728]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 548 is [True, False, False, False, True, False]
Scene graph at timestep 548 is [True, False, False, False, True, False]
State prediction error at timestep 548 is tensor(7.2610e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 548 of -1
Current timestep = 549. State = [[-0.14227079 -0.09309334]]. Action = [[-0.08856164 -0.01350428 -0.22311127  0.9405718 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 549 is [True, False, False, False, True, False]
Current timestep = 550. State = [[-0.14681852 -0.08051995]]. Action = [[ 0.0765858   0.23759276  0.14536858 -0.6656877 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 550 is [True, False, False, False, True, False]
Current timestep = 551. State = [[-0.14342412 -0.06394922]]. Action = [[ 0.15059394 -0.01461887 -0.12476406  0.37146926]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 551 is [True, False, False, False, True, False]
Scene graph at timestep 551 is [True, False, False, False, True, False]
State prediction error at timestep 551 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 551 of 1
Current timestep = 552. State = [[-0.13713758 -0.07261848]]. Action = [[-0.08692905 -0.21695283  0.16169515  0.9217744 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 552 is [True, False, False, False, True, False]
Current timestep = 553. State = [[-0.14602526 -0.07432542]]. Action = [[-0.18573497  0.16834241  0.1824877   0.66661596]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 553 is [True, False, False, False, True, False]
Scene graph at timestep 553 is [True, False, False, False, True, False]
State prediction error at timestep 553 is tensor(5.1441e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 553 of -1
Current timestep = 554. State = [[-0.15622522 -0.06194896]]. Action = [[0.04307705 0.09160417 0.19984451 0.14388931]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 554 is [True, False, False, False, True, False]
Current timestep = 555. State = [[-0.15019993 -0.06533592]]. Action = [[ 0.21219492 -0.19639097 -0.1423928   0.448627  ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 555 is [True, False, False, False, True, False]
Scene graph at timestep 555 is [True, False, False, False, True, False]
State prediction error at timestep 555 is tensor(7.5923e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 555 of 0
Current timestep = 556. State = [[-0.14182119 -0.07607752]]. Action = [[ 0.06201801 -0.00707033  0.1964122  -0.76563543]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 556 is [True, False, False, False, True, False]
Current timestep = 557. State = [[-0.13358358 -0.08529506]]. Action = [[ 0.09913915 -0.14878395 -0.18988258 -0.41248083]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 557 is [True, False, False, False, True, False]
Scene graph at timestep 557 is [True, False, False, False, True, False]
State prediction error at timestep 557 is tensor(1.1685e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 557 of 1
Current timestep = 558. State = [[-0.12554981 -0.10481936]]. Action = [[-0.20059441 -0.07564414  0.04502481  0.13626432]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 558 is [True, False, False, False, True, False]
Scene graph at timestep 558 is [True, False, False, False, True, False]
State prediction error at timestep 558 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 558 of -1
Current timestep = 559. State = [[-0.1287585  -0.11224498]]. Action = [[ 0.1860956  -0.00717124 -0.00706038 -0.28312433]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 559 is [True, False, False, False, True, False]
Scene graph at timestep 559 is [True, False, False, False, True, False]
State prediction error at timestep 559 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 559 of 0
Current timestep = 560. State = [[-0.12075073 -0.12297339]]. Action = [[ 0.03547707 -0.17456798 -0.17379774 -0.04661912]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 560 is [True, False, False, False, True, False]
Scene graph at timestep 560 is [True, False, False, False, True, False]
State prediction error at timestep 560 is tensor(3.0348e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 560 of -1
Current timestep = 561. State = [[-0.12068729 -0.135578  ]]. Action = [[-0.14510438  0.07536259  0.23061422 -0.41802108]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 561 is [True, False, False, False, True, False]
Scene graph at timestep 561 is [True, False, False, True, False, False]
State prediction error at timestep 561 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 561 of -1
Current timestep = 562. State = [[-0.12105751 -0.13499628]]. Action = [[ 0.03511003 -0.04159947 -0.01555215 -0.8036131 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 562 is [True, False, False, True, False, False]
Current timestep = 563. State = [[-0.12480327 -0.12443127]]. Action = [[-0.1395735   0.21670878  0.05683106 -0.864947  ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 563 is [True, False, False, True, False, False]
Scene graph at timestep 563 is [True, False, False, False, True, False]
State prediction error at timestep 563 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 563 of 1
Current timestep = 564. State = [[-0.132892   -0.12154907]]. Action = [[ 0.04310605 -0.23977429 -0.21664904 -0.03272963]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 564 is [True, False, False, False, True, False]
Current timestep = 565. State = [[-0.13248853 -0.13834763]]. Action = [[ 0.03788355 -0.06550318  0.2107701  -0.7222879 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 565 is [True, False, False, False, True, False]
Scene graph at timestep 565 is [True, False, False, True, False, False]
State prediction error at timestep 565 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 565 of -1
Current timestep = 566. State = [[-0.12999457 -0.15278107]]. Action = [[ 0.09978217 -0.11342776  0.21972722 -0.894954  ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 566 is [True, False, False, True, False, False]
Scene graph at timestep 566 is [True, False, False, True, False, False]
State prediction error at timestep 566 is tensor(8.7048e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 566 of -1
Current timestep = 567. State = [[-0.12861323 -0.15878098]]. Action = [[-0.16245459  0.11775774 -0.16133672 -0.4319793 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 567 is [True, False, False, True, False, False]
Current timestep = 568. State = [[-0.13089356 -0.16573621]]. Action = [[ 0.07424113 -0.21427028  0.10886812 -0.05735064]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 568 is [True, False, False, True, False, False]
Current timestep = 569. State = [[-0.1294749  -0.18018313]]. Action = [[ 0.09408289 -0.06861041  0.2226879  -0.7715856 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 569 is [True, False, False, True, False, False]
Current timestep = 570. State = [[-0.12462443 -0.18961722]]. Action = [[ 0.0675239  -0.05543137  0.18916017  0.14290571]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 570 is [True, False, False, True, False, False]
Scene graph at timestep 570 is [True, False, False, True, False, False]
State prediction error at timestep 570 is tensor(2.8519e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 570 of -1
Current timestep = 571. State = [[-0.1138941  -0.18407996]]. Action = [[8.9847445e-02 1.9819278e-01 2.5972724e-04 6.2359786e-01]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 571 is [True, False, False, True, False, False]
Scene graph at timestep 571 is [True, False, False, True, False, False]
State prediction error at timestep 571 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 571 of 1
Current timestep = 572. State = [[-0.10519014 -0.18213457]]. Action = [[-0.02408941 -0.17226608 -0.20240673  0.6281867 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 572 is [True, False, False, True, False, False]
Current timestep = 573. State = [[-0.1045761  -0.19841635]]. Action = [[ 0.06594586 -0.14913058  0.19635862  0.2554462 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 573 is [True, False, False, True, False, False]
Scene graph at timestep 573 is [True, False, False, True, False, False]
State prediction error at timestep 573 is tensor(6.8246e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 573 of -1
Current timestep = 574. State = [[-0.10057964 -0.20364985]]. Action = [[-0.03334273  0.17895037 -0.24436134  0.6963277 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 574 is [True, False, False, True, False, False]
Scene graph at timestep 574 is [True, False, False, True, False, False]
State prediction error at timestep 574 is tensor(9.7527e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 574 of 1
Current timestep = 575. State = [[-0.09437033 -0.19455154]]. Action = [[0.11878169 0.00082165 0.18149638 0.37278175]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 575 is [True, False, False, True, False, False]
Scene graph at timestep 575 is [True, False, False, True, False, False]
State prediction error at timestep 575 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 575 of 1
Current timestep = 576. State = [[-0.08324493 -0.19380227]]. Action = [[ 0.10188639 -0.04675987  0.18629754  0.40667105]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 576 is [True, False, False, True, False, False]
Scene graph at timestep 576 is [True, False, False, True, False, False]
State prediction error at timestep 576 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 576 of 1
Current timestep = 577. State = [[-0.06680725 -0.20655067]]. Action = [[ 0.17408758 -0.22627054  0.21458471 -0.89286536]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 577 is [True, False, False, True, False, False]
Scene graph at timestep 577 is [True, False, False, True, False, False]
State prediction error at timestep 577 is tensor(2.7256e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 577 of -1
Current timestep = 578. State = [[-0.04758462 -0.22926596]]. Action = [[ 0.04055643 -0.06613605 -0.23733906 -0.08893657]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 578 is [True, False, False, True, False, False]
Current timestep = 579. State = [[-0.04010891 -0.2233058 ]]. Action = [[ 0.0240317   0.2233257  -0.0976117  -0.00269008]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 579 is [False, True, False, True, False, False]
Current timestep = 580. State = [[-0.02941299 -0.19995423]]. Action = [[ 0.09276381  0.22081852 -0.22271486  0.5790731 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 580 is [False, True, False, True, False, False]
Scene graph at timestep 580 is [False, True, False, True, False, False]
State prediction error at timestep 580 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 580 of 1
Current timestep = 581. State = [[-0.01765445 -0.1843489 ]]. Action = [[ 0.15341613 -0.12534082 -0.2418864  -0.28102738]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 581 is [False, True, False, True, False, False]
Current timestep = 582. State = [[-0.00250452 -0.2026431 ]]. Action = [[ 0.06158531 -0.23970851  0.19336522  0.6429893 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 582 is [False, True, False, True, False, False]
Current timestep = 583. State = [[ 0.0167886  -0.21540911]]. Action = [[ 0.20380124  0.06631464 -0.00380285  0.3704989 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 583 is [False, True, False, True, False, False]
Current timestep = 584. State = [[ 0.03078958 -0.21189225]]. Action = [[-0.16731413  0.09671986  0.22318482  0.1305741 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 584 is [False, True, False, True, False, False]
Current timestep = 585. State = [[ 0.03460173 -0.19828807]]. Action = [[ 0.10362026  0.19205564  0.2308765  -0.19452018]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 585 is [False, True, False, True, False, False]
Current timestep = 586. State = [[ 0.04335954 -0.17368929]]. Action = [[0.16432601 0.12916547 0.06657177 0.50551176]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 586 is [False, True, False, True, False, False]
Scene graph at timestep 586 is [False, True, False, True, False, False]
State prediction error at timestep 586 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 586 of 1
Current timestep = 587. State = [[ 0.0583947  -0.16059177]]. Action = [[ 0.09903491 -0.19209313 -0.10053226 -0.38870215]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 587 is [False, True, False, True, False, False]
Current timestep = 588. State = [[ 0.0583947  -0.16059177]]. Action = [[ 0.1724624   0.19308776 -0.04611243  0.75467205]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 588 is [False, False, True, True, False, False]
Current timestep = 589. State = [[ 0.05734902 -0.14938898]]. Action = [[-0.15381807  0.19177759  0.24747148  0.7518995 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 589 is [False, False, True, True, False, False]
Scene graph at timestep 589 is [False, False, True, True, False, False]
State prediction error at timestep 589 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 589 of 1
Current timestep = 590. State = [[ 0.05647537 -0.13419808]]. Action = [[ 0.2220163   0.13431957 -0.13721034 -0.8530932 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 590 is [False, False, True, True, False, False]
Scene graph at timestep 590 is [False, False, True, True, False, False]
State prediction error at timestep 590 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 590 of 0
Current timestep = 591. State = [[ 0.05647537 -0.13419808]]. Action = [[ 0.23084545 -0.01469667 -0.09066109 -0.24525875]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 591 is [False, False, True, True, False, False]
Current timestep = 592. State = [[ 0.05354515 -0.12125512]]. Action = [[-0.1467189   0.23559213 -0.08306131 -0.85341245]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 592 is [False, False, True, True, False, False]
Current timestep = 593. State = [[ 0.04249879 -0.08851393]]. Action = [[-0.04596838  0.22667235 -0.22934134  0.8561847 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 593 is [False, False, True, False, True, False]
Current timestep = 594. State = [[-0.20469907 -0.1756547 ]]. Action = [[-0.05574808 -0.11579895  0.17972869 -0.5342148 ]]. Reward = [100.]
Curr episode timestep = 110
Scene graph at timestep 594 is [False, True, False, False, True, False]
Current timestep = 595. State = [[-0.20469967 -0.2069163 ]]. Action = [[-0.19959599 -0.15501001 -0.2407679  -0.8693395 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 595 is [True, False, False, True, False, False]
Current timestep = 596. State = [[-0.21111225 -0.21732187]]. Action = [[ 0.06773704  0.03787521  0.2271409  -0.8021768 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 596 is [True, False, False, True, False, False]
Scene graph at timestep 596 is [True, False, False, True, False, False]
State prediction error at timestep 596 is tensor(4.3787e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 596 of -1
Current timestep = 597. State = [[-0.20601581 -0.21956034]]. Action = [[ 0.13892066 -0.0729796   0.12957299 -0.10829252]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 597 is [True, False, False, True, False, False]
Current timestep = 598. State = [[-0.20478015 -0.22149816]]. Action = [[-0.21170396  0.07678351  0.02333537 -0.54870045]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 598 is [True, False, False, True, False, False]
Current timestep = 599. State = [[-0.21472381 -0.21165486]]. Action = [[-0.10484836  0.16668114 -0.18698822  0.34346306]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 599 is [True, False, False, True, False, False]
Scene graph at timestep 599 is [True, False, False, True, False, False]
State prediction error at timestep 599 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 599 of 0
Current timestep = 600. State = [[-0.2181265  -0.20412184]]. Action = [[ 0.18129477 -0.11553678 -0.0173315   0.42137825]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 600 is [True, False, False, True, False, False]
Current timestep = 601. State = [[-0.20831427 -0.19705117]]. Action = [[0.13838035 0.16914853 0.10908362 0.18230987]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 601 is [True, False, False, True, False, False]
Current timestep = 602. State = [[-0.20011903 -0.1815285 ]]. Action = [[-0.04319862  0.12656677  0.16057408 -0.9437819 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 602 is [True, False, False, True, False, False]
Scene graph at timestep 602 is [True, False, False, True, False, False]
State prediction error at timestep 602 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 602 of 1
Current timestep = 603. State = [[-0.19580197 -0.15843883]]. Action = [[ 0.08578846  0.20077592 -0.01652695 -0.6281804 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 603 is [True, False, False, True, False, False]
Scene graph at timestep 603 is [True, False, False, True, False, False]
State prediction error at timestep 603 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 603 of 1
Current timestep = 604. State = [[-0.19351834 -0.13179415]]. Action = [[-0.1527856   0.15674168  0.1669606  -0.44970894]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 604 is [True, False, False, True, False, False]
Current timestep = 605. State = [[-0.19061038 -0.1064642 ]]. Action = [[ 0.20103279  0.21925044  0.13584346 -0.36962712]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 605 is [True, False, False, True, False, False]
Current timestep = 606. State = [[-0.18127726 -0.07599991]]. Action = [[0.10374147 0.21192428 0.12211633 0.9190819 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 606 is [True, False, False, False, True, False]
Scene graph at timestep 606 is [True, False, False, False, True, False]
State prediction error at timestep 606 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 606 of 1
Current timestep = 607. State = [[-0.157717   -0.04400752]]. Action = [[ 0.2393015   0.16863552  0.0229845  -0.32141727]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 607 is [True, False, False, False, True, False]
Current timestep = 608. State = [[-0.14511548 -0.04664825]]. Action = [[-0.20359685 -0.231259   -0.09139596 -0.24920464]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 608 is [True, False, False, False, True, False]
Current timestep = 609. State = [[-0.15235136 -0.07211652]]. Action = [[-0.09730819 -0.21335983 -0.1643221   0.89109516]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 609 is [True, False, False, False, True, False]
Scene graph at timestep 609 is [True, False, False, False, True, False]
State prediction error at timestep 609 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 609 of 0
Current timestep = 610. State = [[-0.1618626  -0.08980639]]. Action = [[-0.01314545  0.04871029 -0.00810339 -0.19692677]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 610 is [True, False, False, False, True, False]
Current timestep = 611. State = [[-0.16364531 -0.08857781]]. Action = [[-0.03916541 -0.00604911 -0.0040521   0.92335343]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 611 is [True, False, False, False, True, False]
Scene graph at timestep 611 is [True, False, False, False, True, False]
State prediction error at timestep 611 is tensor(3.4602e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 611 of 0
Current timestep = 612. State = [[-0.16652901 -0.09507469]]. Action = [[ 0.00221729 -0.12438542 -0.09922987  0.1315279 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 612 is [True, False, False, False, True, False]
Current timestep = 613. State = [[-0.17136277 -0.09706545]]. Action = [[-0.09130064  0.08319739 -0.08274989 -0.9424738 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 613 is [True, False, False, False, True, False]
Current timestep = 614. State = [[-0.17256317 -0.08933869]]. Action = [[ 0.1736955   0.09126049 -0.14493656 -0.32144547]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 614 is [True, False, False, False, True, False]
Scene graph at timestep 614 is [True, False, False, False, True, False]
State prediction error at timestep 614 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 614 of 0
Current timestep = 615. State = [[-0.16386743 -0.09332294]]. Action = [[ 0.11883301 -0.23375666 -0.11455311 -0.08376789]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 615 is [True, False, False, False, True, False]
Scene graph at timestep 615 is [True, False, False, False, True, False]
State prediction error at timestep 615 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 615 of 1
Current timestep = 616. State = [[-0.15537375 -0.09768374]]. Action = [[ 0.02534652  0.1929428  -0.17373525  0.505527  ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 616 is [True, False, False, False, True, False]
Scene graph at timestep 616 is [True, False, False, False, True, False]
State prediction error at timestep 616 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 616 of 1
Current timestep = 617. State = [[-0.14301804 -0.07322435]]. Action = [[ 0.16177058  0.2445584   0.08281112 -0.1008752 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 617 is [True, False, False, False, True, False]
Scene graph at timestep 617 is [True, False, False, False, True, False]
State prediction error at timestep 617 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 617 of 1
Current timestep = 618. State = [[-0.12673853 -0.06291199]]. Action = [[-0.00438809 -0.2136635  -0.16219395 -0.4629259 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 618 is [True, False, False, False, True, False]
Scene graph at timestep 618 is [True, False, False, False, True, False]
State prediction error at timestep 618 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 618 of 1
Current timestep = 619. State = [[-0.12736952 -0.0642573 ]]. Action = [[-0.02680007  0.20206314 -0.23330116 -0.20428663]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 619 is [True, False, False, False, True, False]
Scene graph at timestep 619 is [True, False, False, False, True, False]
State prediction error at timestep 619 is tensor(5.1204e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 619 of 1
Current timestep = 620. State = [[-0.12993439 -0.05394671]]. Action = [[-0.14402041 -0.01661041 -0.10226327 -0.9156298 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 620 is [True, False, False, False, True, False]
Scene graph at timestep 620 is [True, False, False, False, True, False]
State prediction error at timestep 620 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 620 of 0
Current timestep = 621. State = [[-0.13165262 -0.04088975]]. Action = [[ 0.09694922  0.21562275  0.19775575 -0.5572702 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 621 is [True, False, False, False, True, False]
Current timestep = 622. State = [[-0.13179903 -0.03606388]]. Action = [[ 0.05717048 -0.17157128 -0.00660172 -0.9618174 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 622 is [True, False, False, False, True, False]
Scene graph at timestep 622 is [True, False, False, False, True, False]
State prediction error at timestep 622 is tensor(7.2676e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 622 of 1
Current timestep = 623. State = [[-0.12999117 -0.04128539]]. Action = [[-0.09605971  0.0175021   0.20679522  0.42615068]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 623 is [True, False, False, False, True, False]
Scene graph at timestep 623 is [True, False, False, False, True, False]
State prediction error at timestep 623 is tensor(8.2437e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 623 of 0
Current timestep = 624. State = [[-0.12364472 -0.04818943]]. Action = [[ 0.22345006 -0.11831886  0.03176913 -0.56462777]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 624 is [True, False, False, False, True, False]
Scene graph at timestep 624 is [True, False, False, False, True, False]
State prediction error at timestep 624 is tensor(4.5961e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 624 of 1
Current timestep = 625. State = [[-0.10542927 -0.04809679]]. Action = [[ 0.13970265  0.15096927  0.04960182 -0.13918638]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 625 is [True, False, False, False, True, False]
Current timestep = 626. State = [[-0.09924641 -0.03676007]]. Action = [[-0.19074821  0.06946319  0.22970027  0.638196  ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 626 is [True, False, False, False, True, False]
Scene graph at timestep 626 is [True, False, False, False, True, False]
State prediction error at timestep 626 is tensor(8.9908e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 626 of 1
Current timestep = 627. State = [[-0.09991279 -0.03599044]]. Action = [[ 0.12694019 -0.12121925  0.06409338 -0.56808525]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 627 is [True, False, False, False, True, False]
Scene graph at timestep 627 is [True, False, False, False, True, False]
State prediction error at timestep 627 is tensor(6.9420e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 627 of 1
Current timestep = 628. State = [[-0.09694538 -0.04157823]]. Action = [[ 0.05886301  0.01854625 -0.03745663 -0.3049522 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 628 is [True, False, False, False, True, False]
Current timestep = 629. State = [[-0.0853068  -0.04346292]]. Action = [[ 0.1972931  -0.04879537  0.0924114  -0.9207367 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 629 is [True, False, False, False, True, False]
Current timestep = 630. State = [[-0.06068049 -0.05255729]]. Action = [[ 0.13853317 -0.11114359 -0.00836132 -0.7309013 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 630 is [True, False, False, False, True, False]
Scene graph at timestep 630 is [True, False, False, False, True, False]
State prediction error at timestep 630 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 630 of 1
Current timestep = 631. State = [[-0.04932365 -0.06485605]]. Action = [[-0.2193285  -0.02958213  0.2424618  -0.7017778 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 631 is [True, False, False, False, True, False]
Current timestep = 632. State = [[-0.04826135 -0.08054631]]. Action = [[ 0.23230785 -0.19795991  0.19784015  0.26777005]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 632 is [False, True, False, False, True, False]
Scene graph at timestep 632 is [False, True, False, False, True, False]
State prediction error at timestep 632 is tensor(6.4703e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 632 of -1
Current timestep = 633. State = [[-0.21698458 -0.15549031]]. Action = [[ 0.13438207 -0.03849986  0.11481279 -0.9336122 ]]. Reward = [100.]
Curr episode timestep = 38
Scene graph at timestep 633 is [False, True, False, False, True, False]
Current timestep = 634. State = [[-0.20324208 -0.18086793]]. Action = [[ 0.11047903 -0.13570176 -0.18078175 -0.16423923]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 634 is [True, False, False, True, False, False]
Scene graph at timestep 634 is [True, False, False, True, False, False]
State prediction error at timestep 634 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 634 of 0
Current timestep = 635. State = [[-0.20289257 -0.20506096]]. Action = [[-0.22227925 -0.18563856 -0.08006631 -0.46586335]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 635 is [True, False, False, True, False, False]
Current timestep = 636. State = [[-0.20495033 -0.2102294 ]]. Action = [[0.2289607  0.16976053 0.07181436 0.79982233]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 636 is [True, False, False, True, False, False]
Scene graph at timestep 636 is [True, False, False, True, False, False]
State prediction error at timestep 636 is tensor(5.6521e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 636 of 0
Current timestep = 637. State = [[-0.20245248 -0.18845984]]. Action = [[-0.20887278  0.24460292 -0.20358847 -0.88754874]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 637 is [True, False, False, True, False, False]
Current timestep = 638. State = [[-0.19983862 -0.17986321]]. Action = [[ 0.23770383 -0.14356688 -0.2104795   0.31183624]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 638 is [True, False, False, True, False, False]
Current timestep = 639. State = [[-0.18619213 -0.19494124]]. Action = [[ 0.10511011 -0.21932058  0.18566668  0.5317682 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 639 is [True, False, False, True, False, False]
Current timestep = 640. State = [[-0.17077358 -0.20713495]]. Action = [[ 0.12225199  0.04771751 -0.19364737  0.95595574]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 640 is [True, False, False, True, False, False]
Scene graph at timestep 640 is [True, False, False, True, False, False]
State prediction error at timestep 640 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 640 of 1
Current timestep = 641. State = [[-0.15543525 -0.20066868]]. Action = [[ 0.08725777  0.14407495 -0.200038    0.12323141]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 641 is [True, False, False, True, False, False]
Scene graph at timestep 641 is [True, False, False, True, False, False]
State prediction error at timestep 641 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 641 of 1
Current timestep = 642. State = [[-0.13689953 -0.20004596]]. Action = [[ 0.13905919 -0.18555951  0.19573963 -0.39765757]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 642 is [True, False, False, True, False, False]
Current timestep = 643. State = [[-0.12424747 -0.19755279]]. Action = [[ 0.01241314  0.22982764 -0.21469991 -0.55812216]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 643 is [True, False, False, True, False, False]
Current timestep = 644. State = [[-0.12452296 -0.19518064]]. Action = [[-0.07038891 -0.13231054  0.08305153 -0.9345565 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 644 is [True, False, False, True, False, False]
Current timestep = 645. State = [[-0.12902877 -0.20307595]]. Action = [[-0.11582251 -0.03641292 -0.16476132 -0.927451  ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 645 is [True, False, False, True, False, False]
Scene graph at timestep 645 is [True, False, False, True, False, False]
State prediction error at timestep 645 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 645 of 0
Current timestep = 646. State = [[-0.1351719 -0.217662 ]]. Action = [[ 0.02105698 -0.14470801  0.0483377  -0.47406512]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 646 is [True, False, False, True, False, False]
Scene graph at timestep 646 is [True, False, False, True, False, False]
State prediction error at timestep 646 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 646 of -1
Current timestep = 647. State = [[-0.14084977 -0.23964474]]. Action = [[-0.01692711 -0.22348043  0.07957754  0.12184858]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 647 is [True, False, False, True, False, False]
Scene graph at timestep 647 is [True, False, False, True, False, False]
State prediction error at timestep 647 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 647 of -1
Current timestep = 648. State = [[-0.14248067 -0.25512406]]. Action = [[ 0.05723318  0.04902139 -0.23495817 -0.08490175]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 648 is [True, False, False, True, False, False]
Current timestep = 649. State = [[-0.1431373  -0.25839368]]. Action = [[-0.11103141 -0.06888974  0.04570931  0.8633599 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 649 is [True, False, False, True, False, False]
Scene graph at timestep 649 is [True, False, False, True, False, False]
State prediction error at timestep 649 is tensor(3.5102e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 649 of -1
Current timestep = 650. State = [[-0.14823554 -0.2698365 ]]. Action = [[-0.01991981 -0.08631265  0.0447911   0.45496845]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 650 is [True, False, False, True, False, False]
Current timestep = 651. State = [[-0.14885786 -0.26624608]]. Action = [[-0.02298626  0.18097222 -0.0487906  -0.5983797 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 651 is [True, False, False, True, False, False]
Scene graph at timestep 651 is [True, False, False, True, False, False]
State prediction error at timestep 651 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 651 of -1
Current timestep = 652. State = [[-0.14836468 -0.2598727 ]]. Action = [[-0.00187975 -0.0126597  -0.21106148  0.2093519 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 652 is [True, False, False, True, False, False]
Current timestep = 653. State = [[-0.14790441 -0.26575622]]. Action = [[ 0.09619904 -0.15777035  0.040941    0.20632493]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 653 is [True, False, False, True, False, False]
Current timestep = 654. State = [[-0.14104268 -0.27118883]]. Action = [[ 0.12888986 -0.0087043   0.00803944 -0.6856251 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 654 is [True, False, False, True, False, False]
Current timestep = 655. State = [[-0.12794478 -0.2651154 ]]. Action = [[0.07326502 0.1705187  0.1420294  0.5977702 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 655 is [True, False, False, True, False, False]
Scene graph at timestep 655 is [True, False, False, True, False, False]
State prediction error at timestep 655 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 655 of 0
Current timestep = 656. State = [[-0.12055551 -0.24382867]]. Action = [[-0.15048328  0.19666511 -0.18065423 -0.7512691 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 656 is [True, False, False, True, False, False]
Current timestep = 657. State = [[-0.11504813 -0.2205901 ]]. Action = [[ 0.22227383  0.18453479  0.24311724 -0.02762818]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 657 is [True, False, False, True, False, False]
Scene graph at timestep 657 is [True, False, False, True, False, False]
State prediction error at timestep 657 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 657 of 1
Current timestep = 658. State = [[-0.11271883 -0.18993361]]. Action = [[-0.20378669  0.18774146  0.15045577  0.86571527]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 658 is [True, False, False, True, False, False]
Scene graph at timestep 658 is [True, False, False, True, False, False]
State prediction error at timestep 658 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 658 of 1
Current timestep = 659. State = [[-0.12480124 -0.17929693]]. Action = [[-0.14970364 -0.03817225 -0.21404468  0.785347  ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 659 is [True, False, False, True, False, False]
Current timestep = 660. State = [[-0.1284933 -0.1788347]]. Action = [[ 0.21040225  0.02726689 -0.04714264 -0.55188966]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 660 is [True, False, False, True, False, False]
Scene graph at timestep 660 is [True, False, False, True, False, False]
State prediction error at timestep 660 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 660 of 0
Current timestep = 661. State = [[-0.11626134 -0.17844352]]. Action = [[ 0.20497894 -0.09360957  0.07728708  0.20399547]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 661 is [True, False, False, True, False, False]
Current timestep = 662. State = [[-0.1099463  -0.19389407]]. Action = [[-0.22943598 -0.15962233 -0.10408068 -0.7646914 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 662 is [True, False, False, True, False, False]
Scene graph at timestep 662 is [True, False, False, True, False, False]
State prediction error at timestep 662 is tensor(1.8367e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 662 of -1
Current timestep = 663. State = [[-0.11603636 -0.19921528]]. Action = [[ 0.02767953  0.18768257  0.02407771 -0.838445  ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 663 is [True, False, False, True, False, False]
Scene graph at timestep 663 is [True, False, False, True, False, False]
State prediction error at timestep 663 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 663 of 1
Current timestep = 664. State = [[-0.11169469 -0.17439415]]. Action = [[ 0.12909174  0.1880613   0.12013522 -0.58401036]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 664 is [True, False, False, True, False, False]
Scene graph at timestep 664 is [True, False, False, True, False, False]
State prediction error at timestep 664 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 664 of 1
Current timestep = 665. State = [[-0.099025   -0.14671877]]. Action = [[ 0.19369295  0.16391394 -0.10592446 -0.51059526]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 665 is [True, False, False, True, False, False]
Scene graph at timestep 665 is [True, False, False, True, False, False]
State prediction error at timestep 665 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 665 of 1
Current timestep = 666. State = [[-0.08279543 -0.13293333]]. Action = [[-0.1335837  -0.00217737 -0.05891368 -0.53857845]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 666 is [True, False, False, True, False, False]
Current timestep = 667. State = [[-0.08383404 -0.12538263]]. Action = [[-0.03035311  0.12206441  0.09395     0.26303768]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 667 is [True, False, False, True, False, False]
Scene graph at timestep 667 is [True, False, False, True, False, False]
State prediction error at timestep 667 is tensor(3.1515e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 667 of 1
Current timestep = 668. State = [[-0.09237813 -0.11472625]]. Action = [[-0.17667724  0.03910419 -0.13737267 -0.67314863]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 668 is [True, False, False, True, False, False]
Current timestep = 669. State = [[-0.10318074 -0.12326419]]. Action = [[ 0.14759767 -0.22000808 -0.18625738 -0.45042646]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 669 is [True, False, False, False, True, False]
Scene graph at timestep 669 is [True, False, False, False, True, False]
State prediction error at timestep 669 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 669 of 0
Current timestep = 670. State = [[-0.11110558 -0.14372192]]. Action = [[-0.19206516 -0.09153751  0.02983138 -0.58247495]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 670 is [True, False, False, False, True, False]
Current timestep = 671. State = [[-0.11571618 -0.14479394]]. Action = [[ 0.0108721   0.12608626  0.06429642 -0.43697244]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 671 is [True, False, False, True, False, False]
Scene graph at timestep 671 is [True, False, False, True, False, False]
State prediction error at timestep 671 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 671 of -1
Current timestep = 672. State = [[-0.11306027 -0.13856106]]. Action = [[ 0.16708094 -0.00770283  0.23041162 -0.3434407 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 672 is [True, False, False, True, False, False]
Current timestep = 673. State = [[-0.10188513 -0.13674453]]. Action = [[ 0.19266653  0.00798774 -0.17011537 -0.6378682 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 673 is [True, False, False, True, False, False]
Current timestep = 674. State = [[-0.08204897 -0.14252721]]. Action = [[ 0.09754145 -0.13848141  0.23684913 -0.903704  ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 674 is [True, False, False, True, False, False]
Current timestep = 675. State = [[-0.07532543 -0.15399265]]. Action = [[-0.08945206 -0.08848588  0.08395371 -0.5947699 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 675 is [True, False, False, True, False, False]
Current timestep = 676. State = [[-0.06839265 -0.1520788 ]]. Action = [[ 0.19281328  0.16177428 -0.00809009 -0.77905583]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 676 is [True, False, False, True, False, False]
Scene graph at timestep 676 is [True, False, False, True, False, False]
State prediction error at timestep 676 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 676 of 1
Current timestep = 677. State = [[-0.05476773 -0.13683084]]. Action = [[-0.08373679  0.14255968 -0.08723694 -0.2045356 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 677 is [True, False, False, True, False, False]
Current timestep = 678. State = [[-0.05635527 -0.1182773 ]]. Action = [[-0.06772491  0.14502814  0.10759521 -0.92935795]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 678 is [True, False, False, True, False, False]
Scene graph at timestep 678 is [True, False, False, False, True, False]
State prediction error at timestep 678 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 678 of 1
Current timestep = 679. State = [[-0.05490537 -0.11320703]]. Action = [[ 0.19751596 -0.18459189 -0.09007818  0.8742497 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 679 is [True, False, False, False, True, False]
Current timestep = 680. State = [[-0.04196306 -0.1131238 ]]. Action = [[ 0.17500836  0.15982044  0.16620588 -0.34406602]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 680 is [True, False, False, False, True, False]
Current timestep = 681. State = [[-0.22135258 -0.0988898 ]]. Action = [[ 0.16071442  0.18487433 -0.14169347  0.9102521 ]]. Reward = [100.]
Curr episode timestep = 47
Scene graph at timestep 681 is [False, True, False, False, True, False]
Current timestep = 682. State = [[-0.21006879 -0.11083019]]. Action = [[ 0.19515085 -0.01514137  0.16197509  0.11411631]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 682 is [True, False, False, False, True, False]
Current timestep = 683. State = [[-0.18029457 -0.11637259]]. Action = [[ 0.23114085 -0.01979806 -0.06389138 -0.28527528]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 683 is [True, False, False, False, True, False]
Current timestep = 684. State = [[-0.16313002 -0.1080183 ]]. Action = [[-0.08908251  0.19347072  0.16327205 -0.05471522]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 684 is [True, False, False, False, True, False]
Current timestep = 685. State = [[-0.16962582 -0.09588155]]. Action = [[-0.1948585   0.02943954 -0.13493556  0.26645958]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 685 is [True, False, False, False, True, False]
Scene graph at timestep 685 is [True, False, False, False, True, False]
State prediction error at timestep 685 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 685 of 1
Current timestep = 686. State = [[-0.17194153 -0.08318648]]. Action = [[ 0.1791774   0.12366951 -0.19971146 -0.18559527]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 686 is [True, False, False, False, True, False]
Current timestep = 687. State = [[-0.15964341 -0.08582523]]. Action = [[ 0.17325574 -0.20551671  0.10170904  0.260826  ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 687 is [True, False, False, False, True, False]
Current timestep = 688. State = [[-0.13624538 -0.10054482]]. Action = [[ 0.2211115  -0.10097748 -0.16969654 -0.29485816]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 688 is [True, False, False, False, True, False]
Scene graph at timestep 688 is [True, False, False, False, True, False]
State prediction error at timestep 688 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 688 of 1
Current timestep = 689. State = [[-0.11253843 -0.09902612]]. Action = [[-0.07027522  0.22003657  0.08032972 -0.60543   ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 689 is [True, False, False, False, True, False]
Scene graph at timestep 689 is [True, False, False, False, True, False]
State prediction error at timestep 689 is tensor(3.3650e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 689 of 1
Current timestep = 690. State = [[-0.11259124 -0.07145222]]. Action = [[ 0.09277412  0.24089187 -0.00417557 -0.2722118 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 690 is [True, False, False, False, True, False]
Scene graph at timestep 690 is [True, False, False, False, True, False]
State prediction error at timestep 690 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 690 of 1
Current timestep = 691. State = [[-0.10908807 -0.06177834]]. Action = [[-0.01430877 -0.1968428   0.10081306  0.7393689 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 691 is [True, False, False, False, True, False]
Scene graph at timestep 691 is [True, False, False, False, True, False]
State prediction error at timestep 691 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 691 of 1
Current timestep = 692. State = [[-0.1103703  -0.06240156]]. Action = [[-0.00906752  0.21137232 -0.20857058  0.78233314]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 692 is [True, False, False, False, True, False]
Current timestep = 693. State = [[-0.1143479  -0.03835918]]. Action = [[-0.13634987  0.19473118  0.023581   -0.8180885 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 693 is [True, False, False, False, True, False]
Current timestep = 694. State = [[-0.11982678 -0.02952543]]. Action = [[-0.09031098 -0.11512706  0.24803376 -0.47055185]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 694 is [True, False, False, False, True, False]
Current timestep = 695. State = [[-0.12278178 -0.02005092]]. Action = [[ 0.04814199  0.24354947  0.06406575 -0.23036772]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 695 is [True, False, False, False, True, False]
Scene graph at timestep 695 is [True, False, False, False, True, False]
State prediction error at timestep 695 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 695 of 1
Current timestep = 696. State = [[-0.12826553  0.00153564]]. Action = [[-0.03975303  0.0413225  -0.0160591  -0.87771803]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 696 is [True, False, False, False, True, False]
Current timestep = 697. State = [[-0.13589399 -0.00704144]]. Action = [[-0.1860379  -0.20281728  0.04125419  0.8198874 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 697 is [True, False, False, False, True, False]
Current timestep = 698. State = [[-0.14446957 -0.03168941]]. Action = [[-1.6891956e-04 -1.9416688e-01  1.6577476e-01 -7.6196223e-01]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 698 is [True, False, False, False, True, False]
Current timestep = 699. State = [[-0.15149733 -0.05992626]]. Action = [[-0.08697522 -0.22076768  0.03464618  0.77506804]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 699 is [True, False, False, False, True, False]
Current timestep = 700. State = [[-0.16273057 -0.07891317]]. Action = [[-0.07288015 -0.03193617  0.08372897 -0.24113667]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 700 is [True, False, False, False, True, False]
Current timestep = 701. State = [[-0.16934834 -0.07598531]]. Action = [[-0.00340971  0.16219589  0.21342286 -0.43619102]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 701 is [True, False, False, False, True, False]
Current timestep = 702. State = [[-0.17181541 -0.08135212]]. Action = [[-0.00200401 -0.24912655 -0.14952672 -0.63405424]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 702 is [True, False, False, False, True, False]
Current timestep = 703. State = [[-0.16613944 -0.10237403]]. Action = [[ 0.2158908  -0.14077218 -0.12768583  0.3187889 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 703 is [True, False, False, False, True, False]
Scene graph at timestep 703 is [True, False, False, False, True, False]
State prediction error at timestep 703 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 703 of -1
Current timestep = 704. State = [[-0.15673132 -0.10641708]]. Action = [[ 0.04564452  0.19197851 -0.20529017 -0.60022557]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 704 is [True, False, False, False, True, False]
Current timestep = 705. State = [[-0.16137496 -0.09631631]]. Action = [[-0.2461611   0.0325059   0.1993007  -0.76503944]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 705 is [True, False, False, False, True, False]
Scene graph at timestep 705 is [True, False, False, False, True, False]
State prediction error at timestep 705 is tensor(4.2092e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 705 of -1
Current timestep = 706. State = [[-0.16873977 -0.09768786]]. Action = [[-0.04808992 -0.09814234 -0.08231142  0.55061984]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 706 is [True, False, False, False, True, False]
Scene graph at timestep 706 is [True, False, False, False, True, False]
State prediction error at timestep 706 is tensor(1.5316e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 706 of -1
Current timestep = 707. State = [[-0.17848055 -0.10494713]]. Action = [[-0.06106767  0.0082829  -0.11431471 -0.22180647]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 707 is [True, False, False, False, True, False]
Scene graph at timestep 707 is [True, False, False, False, True, False]
State prediction error at timestep 707 is tensor(6.1601e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 707 of 0
Current timestep = 708. State = [[-0.19303471 -0.09675335]]. Action = [[-0.14776964  0.1144695   0.1382789  -0.70886153]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 708 is [True, False, False, False, True, False]
Scene graph at timestep 708 is [True, False, False, False, True, False]
State prediction error at timestep 708 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 708 of -1
Current timestep = 709. State = [[-0.1994771  -0.07388116]]. Action = [[0.24519879 0.23408502 0.06528416 0.873044  ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 709 is [True, False, False, False, True, False]
Scene graph at timestep 709 is [True, False, False, False, True, False]
State prediction error at timestep 709 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 709 of 1
Current timestep = 710. State = [[-0.19393407 -0.04314671]]. Action = [[-0.17135942  0.17156163  0.18357062  0.05438936]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 710 is [True, False, False, False, True, False]
Scene graph at timestep 710 is [True, False, False, False, True, False]
State prediction error at timestep 710 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 710 of -1
Current timestep = 711. State = [[-0.19820954 -0.03853711]]. Action = [[ 0.09180605 -0.18990634 -0.04398318 -0.17820066]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 711 is [True, False, False, False, True, False]
Scene graph at timestep 711 is [True, False, False, False, True, False]
State prediction error at timestep 711 is tensor(2.4686e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 711 of -1
Current timestep = 712. State = [[-0.20463412 -0.0442298 ]]. Action = [[-0.22234629  0.10646158 -0.08523478 -0.5319933 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 712 is [True, False, False, False, True, False]
Scene graph at timestep 712 is [True, False, False, False, True, False]
State prediction error at timestep 712 is tensor(1.9036e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 712 of -1
Current timestep = 713. State = [[-0.21827346 -0.04416557]]. Action = [[-0.0727365  -0.08908501  0.05093777  0.5307014 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 713 is [True, False, False, False, True, False]
Scene graph at timestep 713 is [True, False, False, False, True, False]
State prediction error at timestep 713 is tensor(9.7513e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 713 of -1
Current timestep = 714. State = [[-0.22432433 -0.04258554]]. Action = [[-0.02992678  0.13962823 -0.22825961  0.25403976]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 714 is [True, False, False, False, True, False]
Current timestep = 715. State = [[-0.22416082 -0.02203011]]. Action = [[ 0.15105784  0.1939531  -0.01697078 -0.7470863 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 715 is [True, False, False, False, True, False]
Current timestep = 716. State = [[-0.22615007  0.00537453]]. Action = [[-0.05695389  0.20965344  0.09417459 -0.5675752 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 716 is [True, False, False, False, True, False]
Scene graph at timestep 716 is [True, False, False, False, True, False]
State prediction error at timestep 716 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 716 of -1
Current timestep = 717. State = [[-0.22156225  0.02089259]]. Action = [[ 0.18505573 -0.11417887  0.16301548  0.21378815]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 717 is [True, False, False, False, True, False]
Current timestep = 718. State = [[-0.20815426  0.02362175]]. Action = [[0.06394169 0.13306189 0.11866495 0.63035274]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 718 is [True, False, False, False, True, False]
Current timestep = 719. State = [[-0.20085151  0.02887575]]. Action = [[ 0.04164648  0.01779294 -0.04276204 -0.12600881]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 719 is [True, False, False, False, True, False]
Scene graph at timestep 719 is [True, False, False, False, True, False]
State prediction error at timestep 719 is tensor(7.2610e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 719 of 1
Current timestep = 720. State = [[-0.19319262  0.03724159]]. Action = [[ 0.11510906  0.11103475 -0.17151266  0.31955397]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 720 is [True, False, False, False, True, False]
Current timestep = 721. State = [[-0.1717439   0.03816193]]. Action = [[ 0.19137871 -0.14239655 -0.17486456 -0.6632916 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 721 is [True, False, False, False, True, False]
Current timestep = 722. State = [[-0.1454662   0.02403323]]. Action = [[ 0.21715903 -0.17085382  0.13374168 -0.3665849 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 722 is [True, False, False, False, True, False]
Scene graph at timestep 722 is [True, False, False, False, True, False]
State prediction error at timestep 722 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 722 of 1
Current timestep = 723. State = [[-0.11284077 -0.0022493 ]]. Action = [[ 0.22290823 -0.19790444  0.20274362 -0.9205807 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 723 is [True, False, False, False, True, False]
Current timestep = 724. State = [[-0.0884953  -0.01785031]]. Action = [[ 0.1098626  -0.02512389  0.07608089  0.4643041 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 724 is [True, False, False, False, True, False]
Current timestep = 725. State = [[-0.07477137 -0.03704611]]. Action = [[ 0.02732465 -0.23644543 -0.23383322 -0.54931957]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 725 is [True, False, False, False, True, False]
Current timestep = 726. State = [[-0.06559403 -0.06378682]]. Action = [[ 0.105259   -0.15736409  0.24314076  0.42693973]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 726 is [True, False, False, False, True, False]
Current timestep = 727. State = [[-0.04804635 -0.09112061]]. Action = [[ 0.2052691  -0.22160803 -0.09804066  0.08269334]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 727 is [True, False, False, False, True, False]
Current timestep = 728. State = [[-0.03294916 -0.11735645]]. Action = [[-0.14477417 -0.10515593 -0.1286249   0.3292656 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 728 is [False, True, False, False, True, False]
Scene graph at timestep 728 is [False, True, False, False, True, False]
State prediction error at timestep 728 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 728 of 1
Current timestep = 729. State = [[-0.03747797 -0.13883987]]. Action = [[-0.1426341  -0.14496768 -0.12579985  0.7021481 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 729 is [False, True, False, False, True, False]
Current timestep = 730. State = [[-0.04122818 -0.1617234 ]]. Action = [[ 0.10580388 -0.19154747 -0.02393271  0.92825747]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 730 is [False, True, False, True, False, False]
Current timestep = 731. State = [[-0.0424053  -0.16812766]]. Action = [[-0.12692237  0.18135858 -0.01723994  0.51181793]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 731 is [False, True, False, True, False, False]
Scene graph at timestep 731 is [False, True, False, True, False, False]
State prediction error at timestep 731 is tensor(7.7973e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 731 of -1
Current timestep = 732. State = [[-0.04834503 -0.1669217 ]]. Action = [[-0.08258115 -0.09981821 -0.18812802  0.354226  ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 732 is [False, True, False, True, False, False]
Scene graph at timestep 732 is [False, True, False, True, False, False]
State prediction error at timestep 732 is tensor(3.1582e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 732 of -1
Current timestep = 733. State = [[-0.05390821 -0.16648464]]. Action = [[ 0.13687709  0.10030553 -0.00236049 -0.13834453]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 733 is [False, True, False, True, False, False]
Current timestep = 734. State = [[-0.05389258 -0.1626262 ]]. Action = [[-0.09987772 -0.02862892  0.00951818  0.8485427 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 734 is [True, False, False, True, False, False]
Scene graph at timestep 734 is [True, False, False, True, False, False]
State prediction error at timestep 734 is tensor(6.7624e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 734 of 1
Current timestep = 735. State = [[-0.06279874 -0.17182684]]. Action = [[-0.18121038 -0.13442956  0.0579325  -0.8839322 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 735 is [True, False, False, True, False, False]
Current timestep = 736. State = [[-0.07104519 -0.18178046]]. Action = [[ 0.015349   -0.02458242 -0.20125939 -0.08680177]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 736 is [True, False, False, True, False, False]
Scene graph at timestep 736 is [True, False, False, True, False, False]
State prediction error at timestep 736 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 736 of -1
Current timestep = 737. State = [[-0.07074643 -0.17245415]]. Action = [[ 0.12436068  0.20528647  0.02050042 -0.54370403]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 737 is [True, False, False, True, False, False]
Current timestep = 738. State = [[-0.06083293 -0.17086726]]. Action = [[ 0.207353   -0.21464002  0.07423252 -0.77469385]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 738 is [True, False, False, True, False, False]
Current timestep = 739. State = [[-0.04134989 -0.17549685]]. Action = [[ 0.22601855  0.07275987 -0.20485033  0.53452945]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 739 is [True, False, False, True, False, False]
Current timestep = 740. State = [[-0.01892841 -0.18453777]]. Action = [[ 0.08578271 -0.22377022  0.03988069  0.807026  ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 740 is [False, True, False, True, False, False]
Scene graph at timestep 740 is [False, True, False, True, False, False]
State prediction error at timestep 740 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 740 of 1
Current timestep = 741. State = [[ 0.00680311 -0.20699836]]. Action = [[ 0.2376501  -0.07278655  0.07461229  0.64873314]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 741 is [False, True, False, True, False, False]
Scene graph at timestep 741 is [False, True, False, True, False, False]
State prediction error at timestep 741 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 741 of -1
Current timestep = 742. State = [[ 0.03444404 -0.20274097]]. Action = [[-0.01028545  0.23129696  0.07693404  0.04953694]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 742 is [False, True, False, True, False, False]
Current timestep = 743. State = [[ 0.03512857 -0.18995354]]. Action = [[ 0.24186477 -0.22820893 -0.18275079 -0.7843866 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 743 is [False, True, False, True, False, False]
Current timestep = 744. State = [[ 0.03593112 -0.17660436]]. Action = [[-0.03428708  0.2144109  -0.13006096 -0.9269721 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 744 is [False, True, False, True, False, False]
Current timestep = 745. State = [[ 0.03718016 -0.16997059]]. Action = [[ 0.08480552 -0.19097403 -0.10041569  0.55819964]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 745 is [False, True, False, True, False, False]
Scene graph at timestep 745 is [False, True, False, True, False, False]
State prediction error at timestep 745 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 745 of 1
Current timestep = 746. State = [[ 0.04511284 -0.1794179 ]]. Action = [[ 0.15178052 -0.0573419  -0.17985785 -0.78419465]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 746 is [False, True, False, True, False, False]
Current timestep = 747. State = [[ 0.05947468 -0.18485494]]. Action = [[ 0.19725695 -0.10794622  0.03031605 -0.9622365 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 747 is [False, True, False, True, False, False]
Scene graph at timestep 747 is [False, False, True, True, False, False]
State prediction error at timestep 747 is tensor(6.7879e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 747 of -1
Current timestep = 748. State = [[ 0.06324684 -0.18571097]]. Action = [[ 0.17811161 -0.09225228  0.21851939  0.9263077 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 748 is [False, False, True, True, False, False]
Current timestep = 749. State = [[ 0.06266639 -0.19607021]]. Action = [[ 0.00396869 -0.1876298  -0.24483596  0.4693637 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 749 is [False, False, True, True, False, False]
Scene graph at timestep 749 is [False, False, True, True, False, False]
State prediction error at timestep 749 is tensor(3.3134e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 749 of -1
Current timestep = 750. State = [[ 0.06126734 -0.21357796]]. Action = [[-0.17258568  0.02600446 -0.22965026 -0.8314195 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 750 is [False, False, True, True, False, False]
Current timestep = 751. State = [[ 0.05421111 -0.20790422]]. Action = [[-0.20811744  0.16234851  0.14717367 -0.9634199 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 751 is [False, False, True, True, False, False]
Current timestep = 752. State = [[ 0.03273318 -0.21019047]]. Action = [[-0.15453099 -0.17677599 -0.14740391  0.95794487]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 752 is [False, False, True, True, False, False]
Scene graph at timestep 752 is [False, True, False, True, False, False]
State prediction error at timestep 752 is tensor(6.9945e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 752 of 1
Current timestep = 753. State = [[ 0.00632593 -0.22625546]]. Action = [[-0.16196844 -0.06317267  0.21247166  0.03211582]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 753 is [False, True, False, True, False, False]
Current timestep = 754. State = [[-0.00992832 -0.21832825]]. Action = [[-0.15219347  0.23536894  0.2122786  -0.08933836]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 754 is [False, True, False, True, False, False]
Scene graph at timestep 754 is [False, True, False, True, False, False]
State prediction error at timestep 754 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 754 of 1
Current timestep = 755. State = [[-0.02398372 -0.18672983]]. Action = [[ 0.10146546  0.18201041 -0.19520004 -0.76633865]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 755 is [False, True, False, True, False, False]
Scene graph at timestep 755 is [False, True, False, True, False, False]
State prediction error at timestep 755 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 755 of 1
Current timestep = 756. State = [[-0.01229374 -0.15589818]]. Action = [[ 0.23475152  0.23335806  0.03984389 -0.637442  ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 756 is [False, True, False, True, False, False]
Scene graph at timestep 756 is [False, True, False, True, False, False]
State prediction error at timestep 756 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 756 of 1
Current timestep = 757. State = [[-0.0151598  -0.14561576]]. Action = [[-0.21695316 -0.21750382 -0.15104814  0.4528376 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 757 is [False, True, False, True, False, False]
Scene graph at timestep 757 is [False, True, False, True, False, False]
State prediction error at timestep 757 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 757 of -1
Current timestep = 758. State = [[-0.01892656 -0.15087749]]. Action = [[-0.06521742  0.1783123  -0.11944851  0.6838856 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 758 is [False, True, False, True, False, False]
Current timestep = 759. State = [[-0.01969169 -0.13424504]]. Action = [[0.15335506 0.1016635  0.16019297 0.21925187]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 759 is [False, True, False, True, False, False]
Scene graph at timestep 759 is [False, True, False, True, False, False]
State prediction error at timestep 759 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 759 of 1
Current timestep = 760. State = [[-0.0124832  -0.12522626]]. Action = [[ 0.1784659  -0.05892661  0.1323621  -0.733851  ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 760 is [False, True, False, True, False, False]
Current timestep = 761. State = [[-0.00374741 -0.11641626]]. Action = [[ 0.03645161  0.16046357 -0.18224172 -0.7328977 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 761 is [False, True, False, True, False, False]
Scene graph at timestep 761 is [False, True, False, False, True, False]
State prediction error at timestep 761 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 761 of 1
Current timestep = 762. State = [[ 0.00422311 -0.11008214]]. Action = [[ 0.10931081 -0.05490184  0.17642301 -0.27205145]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 762 is [False, True, False, False, True, False]
Scene graph at timestep 762 is [False, True, False, False, True, False]
State prediction error at timestep 762 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 762 of 1
Current timestep = 763. State = [[ 0.02077747 -0.11262821]]. Action = [[ 0.14222777 -0.05870938 -0.04142867  0.8774681 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 763 is [False, True, False, False, True, False]
Current timestep = 764. State = [[ 0.0390487  -0.12967385]]. Action = [[ 0.24522683 -0.24389073 -0.04014412  0.24827921]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 764 is [False, True, False, False, True, False]
Scene graph at timestep 764 is [False, True, False, True, False, False]
State prediction error at timestep 764 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 764 of -1
Current timestep = 765. State = [[ 0.07363059 -0.15330622]]. Action = [[ 0.14802814  0.11089891 -0.03341724 -0.4118123 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 765 is [False, True, False, True, False, False]
Current timestep = 766. State = [[ 0.07441292 -0.14214306]]. Action = [[-0.04283342  0.22416621  0.18502378  0.65458894]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 766 is [False, False, True, True, False, False]
Current timestep = 767. State = [[ 0.07159449 -0.11596009]]. Action = [[-0.2183208   0.23562127 -0.01996875  0.5694308 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 767 is [False, False, True, True, False, False]
Current timestep = 768. State = [[ 0.0632563  -0.09590206]]. Action = [[ 0.23924726 -0.17486523  0.14490241 -0.8595308 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 768 is [False, False, True, False, True, False]
Scene graph at timestep 768 is [False, False, True, False, True, False]
State prediction error at timestep 768 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 768 of 1
Current timestep = 769. State = [[ 0.05424432 -0.09363834]]. Action = [[-1.7439614e-01 -4.0793419e-04 -1.5629821e-01 -6.4239603e-01]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 769 is [False, False, True, False, True, False]
Current timestep = 770. State = [[ 0.04147973 -0.09220619]]. Action = [[ 0.20443627  0.19702029  0.02944055 -0.6152821 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 770 is [False, False, True, False, True, False]
Current timestep = 771. State = [[-0.16212317  0.10973574]]. Action = [[ 0.2173928   0.12757221 -0.00185505 -0.20998216]]. Reward = [100.]
Curr episode timestep = 89
Scene graph at timestep 771 is [False, True, False, False, True, False]
Scene graph at timestep 771 is [True, False, False, False, True, False]
State prediction error at timestep 771 is tensor(0.0436, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 771 of 0
Current timestep = 772. State = [[-0.13402455  0.11389558]]. Action = [[ 0.20473358 -0.16502339  0.17349312  0.56909466]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 772 is [True, False, False, False, True, False]
Current timestep = 773. State = [[-0.1175994   0.09811863]]. Action = [[-0.0142086  -0.12916939 -0.14725432 -0.7757079 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 773 is [True, False, False, False, True, False]
Scene graph at timestep 773 is [True, False, False, False, True, False]
State prediction error at timestep 773 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 773 of 1
Current timestep = 774. State = [[-0.11931007  0.0908807 ]]. Action = [[-0.17823571  0.06455106 -0.1434097  -0.80168796]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 774 is [True, False, False, False, True, False]
Scene graph at timestep 774 is [True, False, False, False, True, False]
State prediction error at timestep 774 is tensor(3.7035e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 774 of -1
Current timestep = 775. State = [[-0.12828423  0.10783599]]. Action = [[-0.07067958  0.1922853   0.09129339 -0.6933082 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 775 is [True, False, False, False, True, False]
Current timestep = 776. State = [[-0.12752855  0.10610647]]. Action = [[ 0.23402879 -0.23443072  0.22320366 -0.9205389 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 776 is [True, False, False, False, True, False]
Current timestep = 777. State = [[-0.11651707  0.07878845]]. Action = [[ 0.11390215 -0.22762091  0.15023887  0.7516947 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 777 is [True, False, False, False, True, False]
Scene graph at timestep 777 is [True, False, False, False, True, False]
State prediction error at timestep 777 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 777 of 1
Current timestep = 778. State = [[-0.10508092  0.06732582]]. Action = [[-0.04083766  0.18114328  0.09947032  0.7739551 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 778 is [True, False, False, False, True, False]
Scene graph at timestep 778 is [True, False, False, False, True, False]
State prediction error at timestep 778 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 778 of -1
Current timestep = 779. State = [[-0.10402341  0.07323835]]. Action = [[ 0.08509314 -0.09421015 -0.0807368   0.16092813]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 779 is [True, False, False, False, True, False]
Current timestep = 780. State = [[-0.08846593  0.07130681]]. Action = [[ 0.24789202  0.03214574  0.00858527 -0.8751368 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 780 is [True, False, False, False, True, False]
Current timestep = 781. State = [[-0.05799295  0.0599574 ]]. Action = [[ 0.15837762 -0.21041605  0.1699391  -0.76482534]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 781 is [True, False, False, False, True, False]
Scene graph at timestep 781 is [True, False, False, False, True, False]
State prediction error at timestep 781 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 781 of 1
Current timestep = 782. State = [[-0.16743436  0.09962682]]. Action = [[-0.14683577  0.00124359  0.03509319  0.69391084]]. Reward = [100.]
Curr episode timestep = 10
Scene graph at timestep 782 is [True, False, False, False, True, False]
Current timestep = 783. State = [[-0.15146184  0.10124198]]. Action = [[-0.1417622  -0.21558574  0.2321378  -0.12701482]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 783 is [True, False, False, False, True, False]
Current timestep = 784. State = [[-0.14997499  0.09940697]]. Action = [[ 0.17833143  0.18268776 -0.03210063  0.7147701 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 784 is [True, False, False, False, True, False]
Scene graph at timestep 784 is [True, False, False, False, True, False]
State prediction error at timestep 784 is tensor(4.0271e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 784 of 0
Current timestep = 785. State = [[-0.13575381  0.10539943]]. Action = [[ 0.1742441  -0.05888118  0.07978484 -0.14508885]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 785 is [True, False, False, False, True, False]
Scene graph at timestep 785 is [True, False, False, False, True, False]
State prediction error at timestep 785 is tensor(3.2028e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 785 of 1
Current timestep = 786. State = [[-0.12245983  0.09684658]]. Action = [[-0.2262904  -0.14165899 -0.18983878  0.477898  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 786 is [True, False, False, False, True, False]
Current timestep = 787. State = [[-0.12160455  0.08599859]]. Action = [[ 0.24231237 -0.01177919  0.21855122  0.14108002]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 787 is [True, False, False, False, True, False]
Current timestep = 788. State = [[-0.1159463   0.09213555]]. Action = [[0.08722216 0.17752343 0.19436654 0.0843333 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 788 is [True, False, False, False, True, False]
Current timestep = 789. State = [[-0.11262469  0.11004383]]. Action = [[-0.12189138  0.13510692 -0.15767132  0.26962554]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 789 is [True, False, False, False, True, False]
Scene graph at timestep 789 is [True, False, False, False, True, False]
State prediction error at timestep 789 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 789 of -1
Current timestep = 790. State = [[-0.11587393  0.11274424]]. Action = [[ 5.7220459e-04 -1.8751928e-01 -2.2194894e-01 -9.6589613e-01]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 790 is [True, False, False, False, True, False]
Scene graph at timestep 790 is [True, False, False, False, True, False]
State prediction error at timestep 790 is tensor(7.0276e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 790 of 1
Current timestep = 791. State = [[-0.1128579   0.08703269]]. Action = [[-0.04850301 -0.24424759 -0.04173303  0.9565909 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 791 is [True, False, False, False, True, False]
Current timestep = 792. State = [[-0.11678827  0.07999829]]. Action = [[-0.11583653  0.12311554 -0.00597335 -0.49001026]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 792 is [True, False, False, False, True, False]
Current timestep = 793. State = [[-0.12189081  0.08338494]]. Action = [[ 0.04230988  0.0145773  -0.01203837 -0.63666826]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 793 is [True, False, False, False, True, False]
Current timestep = 794. State = [[-0.11561795  0.07326163]]. Action = [[ 0.2250827  -0.17772527 -0.24412438  0.83040845]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 794 is [True, False, False, False, True, False]
Current timestep = 795. State = [[-0.10792485  0.05396707]]. Action = [[-0.01817343 -0.13787083  0.13064304  0.8811388 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 795 is [True, False, False, False, True, False]
Scene graph at timestep 795 is [True, False, False, False, True, False]
State prediction error at timestep 795 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 795 of 1
Current timestep = 796. State = [[-0.09945632  0.04645016]]. Action = [[ 0.1483602   0.10865015  0.11936671 -0.93665576]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 796 is [True, False, False, False, True, False]
Current timestep = 797. State = [[-0.0892771   0.04502558]]. Action = [[ 0.01877478 -0.10899359  0.05609477 -0.6468497 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 797 is [True, False, False, False, True, False]
Scene graph at timestep 797 is [True, False, False, False, True, False]
State prediction error at timestep 797 is tensor(8.0856e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 797 of 1
Current timestep = 798. State = [[-0.09105199  0.04875915]]. Action = [[-0.1846796   0.1334858  -0.22651507 -0.28079152]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 798 is [True, False, False, False, True, False]
Scene graph at timestep 798 is [True, False, False, False, True, False]
State prediction error at timestep 798 is tensor(4.6537e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 798 of -1
Current timestep = 799. State = [[-0.09820744  0.04681177]]. Action = [[-0.09763478 -0.19001296 -0.12274343  0.9442334 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 799 is [True, False, False, False, True, False]
Scene graph at timestep 799 is [True, False, False, False, True, False]
State prediction error at timestep 799 is tensor(3.0977e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 799 of 1
Current timestep = 800. State = [[-0.09972619  0.03629487]]. Action = [[ 0.17022434  0.09998873 -0.05505779  0.1308533 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 800 is [True, False, False, False, True, False]
Scene graph at timestep 800 is [True, False, False, False, True, False]
State prediction error at timestep 800 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 800 of 0
Current timestep = 801. State = [[-0.09919221  0.03969265]]. Action = [[-0.11982036 -0.06774913  0.20440662 -0.33029842]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 801 is [True, False, False, False, True, False]
Scene graph at timestep 801 is [True, False, False, False, True, False]
State prediction error at timestep 801 is tensor(1.4028e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 801 of 0
Current timestep = 802. State = [[-0.10056353  0.04032113]]. Action = [[-0.0222895   0.06572002  0.14515042  0.48000407]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 802 is [True, False, False, False, True, False]
Current timestep = 803. State = [[-0.10593737  0.05424991]]. Action = [[-0.08494753  0.16323191  0.20947152 -0.5993527 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 803 is [True, False, False, False, True, False]
Scene graph at timestep 803 is [True, False, False, False, True, False]
State prediction error at timestep 803 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 803 of -1
Current timestep = 804. State = [[-0.11233768  0.06698567]]. Action = [[-0.01086843 -0.05260102 -0.13734725  0.2986275 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 804 is [True, False, False, False, True, False]
Current timestep = 805. State = [[-0.12285402  0.07719961]]. Action = [[-0.21015799  0.20871568  0.05173099  0.11736155]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 805 is [True, False, False, False, True, False]
Current timestep = 806. State = [[-0.13168237  0.07715717]]. Action = [[ 0.18447351 -0.22245254  0.23146635  0.09289527]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 806 is [True, False, False, False, True, False]
Scene graph at timestep 806 is [True, False, False, False, True, False]
State prediction error at timestep 806 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 806 of 0
Current timestep = 807. State = [[-0.12966035  0.05400564]]. Action = [[-0.10221826 -0.14355989  0.16675773  0.9202857 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 807 is [True, False, False, False, True, False]
Current timestep = 808. State = [[-0.13315636  0.04937667]]. Action = [[0.05554324 0.10190922 0.13392693 0.00313985]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 808 is [True, False, False, False, True, False]
Current timestep = 809. State = [[-0.12634172  0.04424554]]. Action = [[ 0.22751379 -0.13595596  0.14760315  0.63022935]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 809 is [True, False, False, False, True, False]
Current timestep = 810. State = [[-0.11288742  0.0390629 ]]. Action = [[0.03969929 0.04853714 0.12133107 0.8066585 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 810 is [True, False, False, False, True, False]
Current timestep = 811. State = [[-0.10570122  0.05348025]]. Action = [[ 0.08214974  0.23741618 -0.13695201 -0.8448493 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 811 is [True, False, False, False, True, False]
Current timestep = 812. State = [[-0.1010408   0.07031472]]. Action = [[-0.0608695   0.05605641 -0.07352608  0.8717754 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 812 is [True, False, False, False, True, False]
Current timestep = 813. State = [[-0.10738107  0.06762356]]. Action = [[-0.21897596 -0.19595341 -0.21184939 -0.9092841 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 813 is [True, False, False, False, True, False]
Current timestep = 814. State = [[-0.10965579  0.05490313]]. Action = [[ 0.14759743 -0.06090757 -0.01072562  0.20325911]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 814 is [True, False, False, False, True, False]
Scene graph at timestep 814 is [True, False, False, False, True, False]
State prediction error at timestep 814 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 814 of 1
Current timestep = 815. State = [[-0.10600567  0.04585356]]. Action = [[ 0.0460923  -0.05037493  0.04640096 -0.8331499 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 815 is [True, False, False, False, True, False]
Current timestep = 816. State = [[-0.10102304  0.04103968]]. Action = [[ 0.09357139 -0.00949427  0.21508086 -0.9386062 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 816 is [True, False, False, False, True, False]
Scene graph at timestep 816 is [True, False, False, False, True, False]
State prediction error at timestep 816 is tensor(5.5221e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 816 of 1
Current timestep = 817. State = [[-0.08620176  0.03754249]]. Action = [[ 0.22794104 -0.00697908 -0.1188795  -0.24963903]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 817 is [True, False, False, False, True, False]
Scene graph at timestep 817 is [True, False, False, False, True, False]
State prediction error at timestep 817 is tensor(7.1378e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 817 of 1
Current timestep = 818. State = [[-0.0561002   0.02682836]]. Action = [[ 0.16526365 -0.17040966  0.01514909  0.9071026 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 818 is [True, False, False, False, True, False]
Current timestep = 819. State = [[-0.04557481  0.00232812]]. Action = [[-0.17693186 -0.18370388  0.08883256 -0.40195084]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 819 is [True, False, False, False, True, False]
Scene graph at timestep 819 is [False, True, False, False, True, False]
State prediction error at timestep 819 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 819 of 1
Current timestep = 820. State = [[-0.04983121 -0.01441799]]. Action = [[-0.07775357 -0.02554262  0.06793964 -0.25217754]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 820 is [False, True, False, False, True, False]
Scene graph at timestep 820 is [False, True, False, False, True, False]
State prediction error at timestep 820 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 820 of 0
Current timestep = 821. State = [[-0.04941254 -0.03259021]]. Action = [[ 0.13337052 -0.22580361  0.15614742 -0.75279963]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 821 is [False, True, False, False, True, False]
Current timestep = 822. State = [[-0.04325666 -0.06046712]]. Action = [[ 0.12500405 -0.19935699 -0.11248264  0.22908807]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 822 is [False, True, False, False, True, False]
Current timestep = 823. State = [[-0.18500142 -0.1803565 ]]. Action = [[ 0.2359857  -0.12841943 -0.22491308 -0.73393726]]. Reward = [100.]
Curr episode timestep = 40
Scene graph at timestep 823 is [False, True, False, False, True, False]
Scene graph at timestep 823 is [True, False, False, True, False, False]
State prediction error at timestep 823 is tensor(0.0197, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 823 of 0
Current timestep = 824. State = [[-0.1763842  -0.19601955]]. Action = [[-0.08024269  0.12477568 -0.12252583  0.3799026 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 824 is [True, False, False, True, False, False]
Scene graph at timestep 824 is [True, False, False, True, False, False]
State prediction error at timestep 824 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 824 of 1
Current timestep = 825. State = [[-0.18291335 -0.18535987]]. Action = [[-0.1635278   0.12890494 -0.14671597 -0.675469  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 825 is [True, False, False, True, False, False]
Scene graph at timestep 825 is [True, False, False, True, False, False]
State prediction error at timestep 825 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 825 of -1
Current timestep = 826. State = [[-0.18900664 -0.1789471 ]]. Action = [[-0.00298004 -0.06445777  0.04638815 -0.7344684 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 826 is [True, False, False, True, False, False]
Scene graph at timestep 826 is [True, False, False, True, False, False]
State prediction error at timestep 826 is tensor(6.7041e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 826 of -1
Current timestep = 827. State = [[-0.1854801  -0.18448503]]. Action = [[ 0.20642233 -0.06128365  0.02680305 -0.20033562]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 827 is [True, False, False, True, False, False]
Current timestep = 828. State = [[-0.18643813 -0.20117459]]. Action = [[-0.12601443 -0.2346078  -0.13100351 -0.3805971 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 828 is [True, False, False, True, False, False]
Scene graph at timestep 828 is [True, False, False, True, False, False]
State prediction error at timestep 828 is tensor(3.0292e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 828 of -1
Current timestep = 829. State = [[-0.19255392 -0.21916996]]. Action = [[-0.07402354  0.01061583 -0.1809285   0.28052068]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 829 is [True, False, False, True, False, False]
Scene graph at timestep 829 is [True, False, False, True, False, False]
State prediction error at timestep 829 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 829 of 0
Current timestep = 830. State = [[-0.20295307 -0.23455295]]. Action = [[-0.13793413 -0.21007898  0.18827343 -0.49692154]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 830 is [True, False, False, True, False, False]
Current timestep = 831. State = [[-0.21872875 -0.26201287]]. Action = [[-0.10179397 -0.19419697  0.04524601  0.21694458]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 831 is [True, False, False, True, False, False]
Current timestep = 832. State = [[-0.22217824 -0.27184126]]. Action = [[0.1562705  0.11911997 0.21503007 0.8718095 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 832 is [True, False, False, True, False, False]
Scene graph at timestep 832 is [True, False, False, True, False, False]
State prediction error at timestep 832 is tensor(9.5662e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 832 of -1
Current timestep = 833. State = [[-0.21728635 -0.26205313]]. Action = [[-0.11043024  0.1039325   0.22405562  0.75674176]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 833 is [True, False, False, True, False, False]
Current timestep = 834. State = [[-0.22240853 -0.260146  ]]. Action = [[-0.05903527 -0.04097326  0.20082498  0.57257247]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 834 is [True, False, False, True, False, False]
Current timestep = 835. State = [[-0.23365961 -0.25715667]]. Action = [[-0.15063365  0.06347075 -0.00989155 -0.43770123]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 835 is [True, False, False, True, False, False]
Current timestep = 836. State = [[-0.23967078 -0.24373402]]. Action = [[ 0.13202167  0.15902016 -0.19493453  0.318933  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 836 is [True, False, False, True, False, False]
Scene graph at timestep 836 is [True, False, False, True, False, False]
State prediction error at timestep 836 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 836 of 0
Current timestep = 837. State = [[-0.24109985 -0.21920273]]. Action = [[-0.15717532  0.1836738  -0.17769736  0.88881457]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 837 is [True, False, False, True, False, False]
Scene graph at timestep 837 is [True, False, False, True, False, False]
State prediction error at timestep 837 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 837 of -1
Current timestep = 838. State = [[-0.25125438 -0.20712797]]. Action = [[ 0.08555767 -0.11479168 -0.05160522 -0.7931201 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 838 is [True, False, False, True, False, False]
Scene graph at timestep 838 is [True, False, False, True, False, False]
State prediction error at timestep 838 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 838 of -1
Current timestep = 839. State = [[-0.25319985 -0.22162248]]. Action = [[-0.03128988 -0.15217422  0.05452666  0.5057404 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 839 is [True, False, False, True, False, False]
Scene graph at timestep 839 is [True, False, False, True, False, False]
State prediction error at timestep 839 is tensor(6.6539e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 839 of -1
Current timestep = 840. State = [[-0.2536871  -0.22276849]]. Action = [[-0.01614562  0.19961077  0.19442728  0.14046478]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 840 is [True, False, False, True, False, False]
Current timestep = 841. State = [[-0.25856087 -0.22566608]]. Action = [[-0.11735988 -0.18766071  0.12226257 -0.59842974]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 841 is [True, False, False, True, False, False]
Scene graph at timestep 841 is [True, False, False, True, False, False]
State prediction error at timestep 841 is tensor(5.9458e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 841 of -1
Current timestep = 842. State = [[-0.26322517 -0.224804  ]]. Action = [[ 0.10513878  0.1970258  -0.22406407 -0.5526708 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 842 is [True, False, False, True, False, False]
Current timestep = 843. State = [[-0.25668287 -0.22094485]]. Action = [[ 0.09944353 -0.16854367 -0.00871208  0.14593995]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 843 is [True, False, False, True, False, False]
Current timestep = 844. State = [[-0.249116   -0.22471191]]. Action = [[0.06879696 0.00146171 0.06340596 0.84151864]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 844 is [True, False, False, True, False, False]
Current timestep = 845. State = [[-0.24494958 -0.22618979]]. Action = [[-0.18230458  0.20063657 -0.07568732 -0.39675617]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 845 is [True, False, False, True, False, False]
Scene graph at timestep 845 is [True, False, False, True, False, False]
State prediction error at timestep 845 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 845 of 0
Current timestep = 846. State = [[-0.23092835 -0.21574864]]. Action = [[0.22885603 0.1822935  0.23162657 0.213341  ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 846 is [True, False, False, True, False, False]
Scene graph at timestep 846 is [True, False, False, True, False, False]
State prediction error at timestep 846 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 846 of 1
Current timestep = 847. State = [[-0.21521512 -0.21076979]]. Action = [[-0.1310714  -0.13405629 -0.2319267  -0.16443002]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 847 is [True, False, False, True, False, False]
Current timestep = 848. State = [[-0.21093506 -0.20474605]]. Action = [[ 0.21023023  0.22417295  0.04312116 -0.47425282]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 848 is [True, False, False, True, False, False]
Scene graph at timestep 848 is [True, False, False, True, False, False]
State prediction error at timestep 848 is tensor(6.9317e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 848 of 1
Current timestep = 849. State = [[-0.18912217 -0.18243423]]. Action = [[ 0.21958071  0.10878193 -0.10264266 -0.00629169]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 849 is [True, False, False, True, False, False]
Current timestep = 850. State = [[-0.17266712 -0.1707904 ]]. Action = [[ 0.00638461  0.07967269 -0.14282095 -0.62447876]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 850 is [True, False, False, True, False, False]
Current timestep = 851. State = [[-0.15846111 -0.16099475]]. Action = [[ 0.22396594  0.03764185 -0.23100173  0.54922986]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 851 is [True, False, False, True, False, False]
Current timestep = 852. State = [[-0.13446076 -0.1622416 ]]. Action = [[ 0.11607802 -0.08714569 -0.09378888 -0.8000665 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 852 is [True, False, False, True, False, False]
Current timestep = 853. State = [[-0.1146303  -0.16928674]]. Action = [[ 0.12209666 -0.07493624  0.04110733  0.38734245]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 853 is [True, False, False, True, False, False]
Current timestep = 854. State = [[-0.09160726 -0.16366611]]. Action = [[ 0.23522887  0.18466228 -0.11142573 -0.18299747]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 854 is [True, False, False, True, False, False]
Scene graph at timestep 854 is [True, False, False, True, False, False]
State prediction error at timestep 854 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 854 of 1
Current timestep = 855. State = [[-0.06857324 -0.14395514]]. Action = [[-0.11450371  0.17801625 -0.14260148  0.5324172 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 855 is [True, False, False, True, False, False]
Current timestep = 856. State = [[-0.06492596 -0.13819928]]. Action = [[ 0.18228918 -0.14714791 -0.16651922  0.57192874]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 856 is [True, False, False, True, False, False]
Current timestep = 857. State = [[-0.05092724 -0.13000402]]. Action = [[ 0.13534063  0.22777867 -0.02695207  0.05286014]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 857 is [True, False, False, True, False, False]
Scene graph at timestep 857 is [True, False, False, True, False, False]
State prediction error at timestep 857 is tensor(5.8809e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 857 of 1
Current timestep = 858. State = [[-0.02756846 -0.11128589]]. Action = [[ 0.21582285  0.07245097 -0.13645522  0.19204247]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 858 is [True, False, False, True, False, False]
Scene graph at timestep 858 is [False, True, False, False, True, False]
State prediction error at timestep 858 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 858 of 1
Current timestep = 859. State = [[-0.21120757  0.07253558]]. Action = [[0.23648292 0.12518013 0.23529112 0.7820574 ]]. Reward = [100.]
Curr episode timestep = 35
Scene graph at timestep 859 is [False, True, False, False, True, False]
Scene graph at timestep 859 is [True, False, False, False, True, False]
State prediction error at timestep 859 is tensor(0.0342, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 859 of 0
Current timestep = 860. State = [[-0.18984556  0.0830427 ]]. Action = [[0.24041808 0.01683488 0.12890297 0.81141806]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 860 is [True, False, False, False, True, False]
Current timestep = 861. State = [[-0.16018787  0.08828907]]. Action = [[0.24660876 0.07166764 0.19836694 0.0917654 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 861 is [True, False, False, False, True, False]
Scene graph at timestep 861 is [True, False, False, False, True, False]
State prediction error at timestep 861 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 861 of 1
Current timestep = 862. State = [[-0.1193085   0.08150949]]. Action = [[ 0.23309821 -0.2308892   0.10755837  0.44487917]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 862 is [True, False, False, False, True, False]
Scene graph at timestep 862 is [True, False, False, False, True, False]
State prediction error at timestep 862 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 862 of 1
Current timestep = 863. State = [[-0.08950557  0.05875609]]. Action = [[ 0.18314278 -0.11669937 -0.22107366  0.93967605]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 863 is [True, False, False, False, True, False]
Scene graph at timestep 863 is [True, False, False, False, True, False]
State prediction error at timestep 863 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 863 of 1
Current timestep = 864. State = [[-0.07421929  0.0584079 ]]. Action = [[-0.02668728  0.15148306  0.19847226 -0.05547559]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 864 is [True, False, False, False, True, False]
Current timestep = 865. State = [[-0.07211804  0.06109377]]. Action = [[ 0.06573921 -0.07623637 -0.16594115  0.50464916]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 865 is [True, False, False, False, True, False]
Scene graph at timestep 865 is [True, False, False, False, True, False]
State prediction error at timestep 865 is tensor(3.6747e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 865 of 0
Current timestep = 866. State = [[-0.06085465  0.068941  ]]. Action = [[0.14135367 0.16962695 0.22043881 0.87847686]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 866 is [True, False, False, False, True, False]
Current timestep = 867. State = [[-0.0496031   0.09048729]]. Action = [[-0.02784862  0.21041113  0.04232338  0.21611714]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 867 is [True, False, False, False, True, False]
Current timestep = 868. State = [[-0.04918556  0.10162863]]. Action = [[-0.18200275 -0.12075894  0.06915161 -0.78617305]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 868 is [False, True, False, False, True, False]
Current timestep = 869. State = [[-0.04947311  0.09993784]]. Action = [[ 0.16738993  0.02643064  0.17970142 -0.9757514 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 869 is [False, True, False, False, True, False]
Scene graph at timestep 869 is [False, True, False, False, True, False]
State prediction error at timestep 869 is tensor(1.7142e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 869 of -1
Current timestep = 870. State = [[-0.04437867  0.09225189]]. Action = [[ 0.11365366 -0.13478653  0.04915294  0.36362684]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 870 is [False, True, False, False, True, False]
Scene graph at timestep 870 is [False, True, False, False, True, False]
State prediction error at timestep 870 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 870 of 1
Current timestep = 871. State = [[-0.03862512  0.08726986]]. Action = [[ 0.00530958  0.10659331 -0.12304349  0.27891493]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 871 is [False, True, False, False, True, False]
Current timestep = 872. State = [[-0.22496507 -0.02874272]]. Action = [[ 0.18563813 -0.21998036 -0.05522707  0.18794024]]. Reward = [100.]
Curr episode timestep = 12
Scene graph at timestep 872 is [False, True, False, False, True, False]
Current timestep = 873. State = [[-0.20606413 -0.0472971 ]]. Action = [[ 0.21908614 -0.22759287 -0.08340496 -0.32286298]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 873 is [True, False, False, False, True, False]
Scene graph at timestep 873 is [True, False, False, False, True, False]
State prediction error at timestep 873 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 873 of 1
Current timestep = 874. State = [[-0.18556637 -0.05788767]]. Action = [[-0.02771692  0.17240474 -0.03757064  0.60858274]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 874 is [True, False, False, False, True, False]
Scene graph at timestep 874 is [True, False, False, False, True, False]
State prediction error at timestep 874 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 874 of 1
Current timestep = 875. State = [[-0.18081963 -0.04781667]]. Action = [[ 0.10951445 -0.01539631 -0.21587476  0.9570823 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 875 is [True, False, False, False, True, False]
Scene graph at timestep 875 is [True, False, False, False, True, False]
State prediction error at timestep 875 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 875 of 1
Current timestep = 876. State = [[-0.17148009 -0.05474679]]. Action = [[ 0.08744976 -0.12822104 -0.16254394 -0.23264253]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 876 is [True, False, False, False, True, False]
Current timestep = 877. State = [[-0.16050589 -0.05184636]]. Action = [[0.09641725 0.17819199 0.21855384 0.7039671 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 877 is [True, False, False, False, True, False]
Scene graph at timestep 877 is [True, False, False, False, True, False]
State prediction error at timestep 877 is tensor(4.6019e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 877 of 1
Current timestep = 878. State = [[-0.14229493 -0.04373899]]. Action = [[ 0.10309735 -0.01930462  0.20629945 -0.50681025]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 878 is [True, False, False, False, True, False]
Scene graph at timestep 878 is [True, False, False, False, True, False]
State prediction error at timestep 878 is tensor(2.9514e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 878 of 1
Current timestep = 879. State = [[-0.13698456 -0.05659879]]. Action = [[-0.13668756 -0.22459905  0.06770802 -0.64639455]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 879 is [True, False, False, False, True, False]
Scene graph at timestep 879 is [True, False, False, False, True, False]
State prediction error at timestep 879 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 879 of -1
Current timestep = 880. State = [[-0.1402908  -0.07418498]]. Action = [[ 0.0259372  -0.02658089 -0.06529002  0.17396307]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 880 is [True, False, False, False, True, False]
Current timestep = 881. State = [[-0.13261838 -0.08311472]]. Action = [[ 0.22033513 -0.10552594 -0.00486372  0.9049138 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 881 is [True, False, False, False, True, False]
Current timestep = 882. State = [[-0.10754885 -0.10079061]]. Action = [[ 0.20527679 -0.16713318  0.0845266  -0.20381683]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 882 is [True, False, False, False, True, False]
Scene graph at timestep 882 is [True, False, False, False, True, False]
State prediction error at timestep 882 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 882 of 1
Current timestep = 883. State = [[-0.08768727 -0.12201896]]. Action = [[-0.16788663 -0.06382528 -0.18112475 -0.9869025 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 883 is [True, False, False, False, True, False]
Scene graph at timestep 883 is [True, False, False, False, True, False]
State prediction error at timestep 883 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 883 of -1
Current timestep = 884. State = [[-0.08729956 -0.13604826]]. Action = [[ 0.18372399 -0.11756244  0.03623852 -0.214396  ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 884 is [True, False, False, False, True, False]
Current timestep = 885. State = [[-0.07776184 -0.13084026]]. Action = [[0.11720231 0.20211056 0.21648455 0.19798136]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 885 is [True, False, False, True, False, False]
Current timestep = 886. State = [[-0.06454118 -0.11029992]]. Action = [[ 0.07064652  0.17624414  0.06827578 -0.807874  ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 886 is [True, False, False, True, False, False]
Scene graph at timestep 886 is [True, False, False, False, True, False]
State prediction error at timestep 886 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 886 of 1
Current timestep = 887. State = [[-0.04999344 -0.07952968]]. Action = [[ 0.12954897  0.22409654 -0.1958468  -0.833003  ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 887 is [True, False, False, False, True, False]
Current timestep = 888. State = [[-0.04536216 -0.06675494]]. Action = [[-0.16374521 -0.01843122 -0.16557077 -0.92109257]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 888 is [False, True, False, False, True, False]
Scene graph at timestep 888 is [False, True, False, False, True, False]
State prediction error at timestep 888 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 888 of 1
Current timestep = 889. State = [[-0.04589007 -0.05746112]]. Action = [[ 0.11142927  0.10858849  0.14272475 -0.78488564]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 889 is [False, True, False, False, True, False]
Current timestep = 890. State = [[-0.043525   -0.03813203]]. Action = [[ 0.07128483  0.19452876 -0.10957973 -0.2638378 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 890 is [False, True, False, False, True, False]
Scene graph at timestep 890 is [False, True, False, False, True, False]
State prediction error at timestep 890 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 890 of 1
Current timestep = 891. State = [[-0.28075206  0.1358641 ]]. Action = [[ 0.21481806 -0.12190914 -0.19151573  0.9075861 ]]. Reward = [100.]
Curr episode timestep = 18
Scene graph at timestep 891 is [False, True, False, False, True, False]
Current timestep = 892. State = [[-0.2747424  0.1460402]]. Action = [[0.13633698 0.16002381 0.24016663 0.8636267 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 892 is [True, False, False, False, False, True]
Scene graph at timestep 892 is [True, False, False, False, False, True]
State prediction error at timestep 892 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 892 of -1
Current timestep = 893. State = [[-0.25162926  0.15588023]]. Action = [[ 0.16054475 -0.11440498 -0.24351725 -0.7563219 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 893 is [True, False, False, False, False, True]
Scene graph at timestep 893 is [True, False, False, False, False, True]
State prediction error at timestep 893 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 893 of 1
Current timestep = 894. State = [[-0.23095632  0.13624367]]. Action = [[ 0.10155496 -0.23077013 -0.1857855   0.77974033]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 894 is [True, False, False, False, False, True]
Scene graph at timestep 894 is [True, False, False, False, False, True]
State prediction error at timestep 894 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 894 of 1
Current timestep = 895. State = [[-0.21264231  0.10367469]]. Action = [[ 0.20639956 -0.23696002 -0.19096246  0.8315985 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 895 is [True, False, False, False, False, True]
Scene graph at timestep 895 is [True, False, False, False, True, False]
State prediction error at timestep 895 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 895 of 1
Current timestep = 896. State = [[-0.18546437  0.09726945]]. Action = [[0.18958426 0.22902212 0.18719012 0.295516  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 896 is [True, False, False, False, True, False]
Scene graph at timestep 896 is [True, False, False, False, True, False]
State prediction error at timestep 896 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 896 of 1
Current timestep = 897. State = [[-0.16291055  0.10024773]]. Action = [[-0.07766157 -0.23340105  0.09923321  0.02037895]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 897 is [True, False, False, False, True, False]
Scene graph at timestep 897 is [True, False, False, False, True, False]
State prediction error at timestep 897 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 897 of 1
Current timestep = 898. State = [[-0.1611059   0.07351618]]. Action = [[ 0.00279146 -0.20311733 -0.08663821  0.12114489]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 898 is [True, False, False, False, True, False]
Scene graph at timestep 898 is [True, False, False, False, True, False]
State prediction error at timestep 898 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 898 of 1
Current timestep = 899. State = [[-0.16704749  0.05386975]]. Action = [[-0.1801851  -0.04435572 -0.16597714 -0.87622505]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 899 is [True, False, False, False, True, False]
Scene graph at timestep 899 is [True, False, False, False, True, False]
State prediction error at timestep 899 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 899 of -1
Current timestep = 900. State = [[-0.18626654  0.04348394]]. Action = [[-0.23549742 -0.11363335  0.01413262 -0.15996534]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 900 is [True, False, False, False, True, False]
Current timestep = 901. State = [[-0.19990368  0.04779432]]. Action = [[ 0.17666283  0.23287764 -0.22256579 -0.7822575 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 901 is [True, False, False, False, True, False]
Current timestep = 902. State = [[-0.18998593  0.06703658]]. Action = [[ 0.2226029   0.1487701   0.09159073 -0.58875114]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 902 is [True, False, False, False, True, False]
Scene graph at timestep 902 is [True, False, False, False, True, False]
State prediction error at timestep 902 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 902 of 0
Current timestep = 903. State = [[-0.17089011  0.09248444]]. Action = [[0.10557672 0.16944039 0.00745189 0.17626238]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 903 is [True, False, False, False, True, False]
Scene graph at timestep 903 is [True, False, False, False, True, False]
State prediction error at timestep 903 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 903 of -1
Current timestep = 904. State = [[-0.15197204  0.09991068]]. Action = [[ 0.21627691 -0.13182876  0.20353717 -0.8843983 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 904 is [True, False, False, False, True, False]
Scene graph at timestep 904 is [True, False, False, False, True, False]
State prediction error at timestep 904 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 904 of 1
Current timestep = 905. State = [[-0.12260518  0.09260387]]. Action = [[ 0.1889492   0.00441152 -0.02850324 -0.7215015 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 905 is [True, False, False, False, True, False]
Scene graph at timestep 905 is [True, False, False, False, True, False]
State prediction error at timestep 905 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 905 of 1
Current timestep = 906. State = [[-0.10701805  0.10181206]]. Action = [[-0.10019365  0.13833988  0.13423705 -0.3851632 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 906 is [True, False, False, False, True, False]
Current timestep = 907. State = [[-0.11499076  0.10820943]]. Action = [[-0.2301298  -0.04000007 -0.0246352   0.8353834 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 907 is [True, False, False, False, True, False]
Current timestep = 908. State = [[-0.11694873  0.10920054]]. Action = [[0.22072244 0.00126496 0.22167814 0.8168876 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 908 is [True, False, False, False, True, False]
Current timestep = 909. State = [[-0.11872797  0.10111692]]. Action = [[-0.22891878 -0.12916361  0.09361482  0.37004638]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 909 is [True, False, False, False, True, False]
Current timestep = 910. State = [[-0.13379921  0.10210644]]. Action = [[-0.22708338  0.11281288 -0.05280124 -0.20544755]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 910 is [True, False, False, False, True, False]
Scene graph at timestep 910 is [True, False, False, False, True, False]
State prediction error at timestep 910 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 910 of -1
Current timestep = 911. State = [[-0.15580341  0.11412208]]. Action = [[ 0.0170759   0.09891042  0.23456532 -0.01014638]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 911 is [True, False, False, False, True, False]
Scene graph at timestep 911 is [True, False, False, False, True, False]
State prediction error at timestep 911 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 911 of -1
Current timestep = 912. State = [[-0.1587626   0.12023897]]. Action = [[ 0.01691896 -0.03028673  0.0774323  -0.51839554]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 912 is [True, False, False, False, True, False]
Scene graph at timestep 912 is [True, False, False, False, True, False]
State prediction error at timestep 912 is tensor(5.9813e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 912 of -1
Current timestep = 913. State = [[-0.15651341  0.11959794]]. Action = [[ 9.5094085e-02  3.0335784e-04  8.7833047e-02 -6.2598896e-01]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 913 is [True, False, False, False, True, False]
Scene graph at timestep 913 is [True, False, False, False, True, False]
State prediction error at timestep 913 is tensor(2.0482e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 913 of -1
Current timestep = 914. State = [[-0.1451447   0.10409795]]. Action = [[ 0.20126456 -0.21801822  0.05498493 -0.4562912 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 914 is [True, False, False, False, True, False]
Scene graph at timestep 914 is [True, False, False, False, True, False]
State prediction error at timestep 914 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 914 of 1
Current timestep = 915. State = [[-0.13214506  0.07394535]]. Action = [[ 0.01890358 -0.17453746  0.19302207 -0.9051961 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 915 is [True, False, False, False, True, False]
Current timestep = 916. State = [[-0.11936124  0.05617065]]. Action = [[ 0.21422714 -0.08621711 -0.17634709  0.47390223]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 916 is [True, False, False, False, True, False]
Current timestep = 917. State = [[-0.10008588  0.05863578]]. Action = [[ 0.06923938  0.20343077 -0.14602593  0.49024916]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 917 is [True, False, False, False, True, False]
Current timestep = 918. State = [[-0.08328195  0.08346775]]. Action = [[ 0.22253102  0.24787778 -0.16876744 -0.863169  ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 918 is [True, False, False, False, True, False]
Scene graph at timestep 918 is [True, False, False, False, True, False]
State prediction error at timestep 918 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 918 of 1
Current timestep = 919. State = [[-0.05062082  0.11595967]]. Action = [[0.23542354 0.18312103 0.20248345 0.27383018]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 919 is [True, False, False, False, True, False]
Scene graph at timestep 919 is [True, False, False, False, True, False]
State prediction error at timestep 919 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 919 of -1
Current timestep = 920. State = [[-0.01835157  0.12514175]]. Action = [[ 0.0379492  -0.15876867 -0.17539278 -0.41390252]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 920 is [True, False, False, False, True, False]
Scene graph at timestep 920 is [False, True, False, False, False, True]
State prediction error at timestep 920 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 920 of 1
Current timestep = 921. State = [[-0.18898882 -0.01256582]]. Action = [[ 0.0951893  -0.16613871 -0.0972085   0.2284987 ]]. Reward = [100.]
Curr episode timestep = 29
Scene graph at timestep 921 is [False, True, False, False, False, True]
Current timestep = 922. State = [[-0.17010714 -0.02734955]]. Action = [[ 0.13001129 -0.20608035  0.12715808 -0.7900117 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 922 is [True, False, False, False, True, False]
Scene graph at timestep 922 is [True, False, False, False, True, False]
State prediction error at timestep 922 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 922 of 1
Current timestep = 923. State = [[-0.15032564 -0.03344284]]. Action = [[ 0.18890467  0.2002238  -0.22398749  0.5491141 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 923 is [True, False, False, False, True, False]
Scene graph at timestep 923 is [True, False, False, False, True, False]
State prediction error at timestep 923 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 923 of 1
Current timestep = 924. State = [[-0.12416804 -0.02956693]]. Action = [[ 0.13186243 -0.14599103 -0.00605737  0.5423217 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 924 is [True, False, False, False, True, False]
Current timestep = 925. State = [[-0.106853   -0.02771139]]. Action = [[ 0.14069182  0.15107915 -0.22794425  0.68321085]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 925 is [True, False, False, False, True, False]
Current timestep = 926. State = [[-0.08789241 -0.00925947]]. Action = [[0.12793654 0.22350842 0.07214308 0.9202044 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 926 is [True, False, False, False, True, False]
Scene graph at timestep 926 is [True, False, False, False, True, False]
State prediction error at timestep 926 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 926 of 1
Current timestep = 927. State = [[-0.0679099   0.00607881]]. Action = [[ 0.03016454 -0.11665171 -0.2273208  -0.87487674]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 927 is [True, False, False, False, True, False]
Current timestep = 928. State = [[-0.06213216  0.0083073 ]]. Action = [[ 0.11917254  0.14362305 -0.13283175 -0.611399  ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 928 is [True, False, False, False, True, False]
Current timestep = 929. State = [[-0.04463796  0.00846844]]. Action = [[ 0.15933424 -0.10957478 -0.2366376   0.96225977]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 929 is [True, False, False, False, True, False]
Scene graph at timestep 929 is [False, True, False, False, True, False]
State prediction error at timestep 929 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 929 of 1
Current timestep = 930. State = [[-0.19340609 -0.19508573]]. Action = [[-0.21839282 -0.2413294  -0.0106149   0.615335  ]]. Reward = [100.]
Curr episode timestep = 8
Scene graph at timestep 930 is [False, True, False, False, True, False]
Current timestep = 931. State = [[-0.18176639 -0.2085037 ]]. Action = [[-0.03060809  0.18176422 -0.24012157 -0.932987  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 931 is [True, False, False, True, False, False]
Current timestep = 932. State = [[-0.17770171 -0.20221196]]. Action = [[ 0.12488124 -0.02294157  0.05034485  0.99012697]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 932 is [True, False, False, True, False, False]
Scene graph at timestep 932 is [True, False, False, True, False, False]
State prediction error at timestep 932 is tensor(9.5004e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 932 of 1
Current timestep = 933. State = [[-0.17056452 -0.18478748]]. Action = [[ 0.04373586  0.22862044 -0.19787097  0.7232337 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 933 is [True, False, False, True, False, False]
Scene graph at timestep 933 is [True, False, False, True, False, False]
State prediction error at timestep 933 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 933 of 1
Current timestep = 934. State = [[-0.15885913 -0.17896777]]. Action = [[ 0.18293864 -0.20329084 -0.2353778   0.9409404 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 934 is [True, False, False, True, False, False]
Scene graph at timestep 934 is [True, False, False, True, False, False]
State prediction error at timestep 934 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 934 of 1
Current timestep = 935. State = [[-0.13549025 -0.17895436]]. Action = [[0.03718543 0.23548764 0.1555214  0.10755146]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 935 is [True, False, False, True, False, False]
Current timestep = 936. State = [[-0.12795383 -0.17130387]]. Action = [[ 0.11508375 -0.0920272   0.23151505 -0.11399412]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 936 is [True, False, False, True, False, False]
Scene graph at timestep 936 is [True, False, False, True, False, False]
State prediction error at timestep 936 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 936 of 1
Current timestep = 937. State = [[-0.11896519 -0.17239992]]. Action = [[-0.14658777  0.05909944  0.10572881 -0.8015634 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 937 is [True, False, False, True, False, False]
Current timestep = 938. State = [[-0.12177785 -0.17176372]]. Action = [[-0.06733346 -0.00864154  0.01900113  0.2192527 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 938 is [True, False, False, True, False, False]
Current timestep = 939. State = [[-0.13645884 -0.1590415 ]]. Action = [[-0.24215682  0.24365741  0.14980942  0.9408114 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 939 is [True, False, False, True, False, False]
Current timestep = 940. State = [[-0.14833862 -0.12877469]]. Action = [[0.140226   0.19054878 0.23284489 0.11227274]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 940 is [True, False, False, True, False, False]
Current timestep = 941. State = [[-0.13851841 -0.10762453]]. Action = [[0.24511272 0.06228638 0.05088934 0.20612979]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 941 is [True, False, False, True, False, False]
Scene graph at timestep 941 is [True, False, False, False, True, False]
State prediction error at timestep 941 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 941 of 1
Current timestep = 942. State = [[-0.13183656 -0.11232756]]. Action = [[-0.23661481 -0.23345654  0.02490732  0.82834136]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 942 is [True, False, False, False, True, False]
Current timestep = 943. State = [[-0.13389236 -0.13956024]]. Action = [[ 0.20626372 -0.22988816  0.08947521 -0.6606978 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 943 is [True, False, False, False, True, False]
Scene graph at timestep 943 is [True, False, False, True, False, False]
State prediction error at timestep 943 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 943 of -1
Current timestep = 944. State = [[-0.13568906 -0.16876093]]. Action = [[-0.2128934  -0.11914283 -0.12524815 -0.24802148]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 944 is [True, False, False, True, False, False]
Current timestep = 945. State = [[-0.1372742  -0.16636568]]. Action = [[ 0.170463    0.18754578  0.11704174 -0.1912595 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 945 is [True, False, False, True, False, False]
Current timestep = 946. State = [[-0.1349007  -0.15887587]]. Action = [[-0.09123459 -0.01117808  0.06756875 -0.6338544 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 946 is [True, False, False, True, False, False]
Scene graph at timestep 946 is [True, False, False, True, False, False]
State prediction error at timestep 946 is tensor(4.3010e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 946 of 0
Current timestep = 947. State = [[-0.13398859 -0.15185618]]. Action = [[0.09329113 0.10547686 0.12534034 0.31398463]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 947 is [True, False, False, True, False, False]
Current timestep = 948. State = [[-0.12546907 -0.13170864]]. Action = [[ 0.17417777  0.18488121  0.21990943 -0.32991564]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 948 is [True, False, False, True, False, False]
Current timestep = 949. State = [[-0.1171953  -0.12679493]]. Action = [[-0.13651092 -0.14903583 -0.22837318 -0.9622493 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 949 is [True, False, False, True, False, False]
Scene graph at timestep 949 is [True, False, False, True, False, False]
State prediction error at timestep 949 is tensor(1.9754e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 949 of 1
Current timestep = 950. State = [[-0.11859761 -0.1320514 ]]. Action = [[ 0.03896558 -0.00809315  0.1252724  -0.5679938 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 950 is [True, False, False, True, False, False]
Scene graph at timestep 950 is [True, False, False, True, False, False]
State prediction error at timestep 950 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 950 of 0
Current timestep = 951. State = [[-0.11538355 -0.118603  ]]. Action = [[0.08799791 0.24808478 0.1307939  0.77954876]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 951 is [True, False, False, True, False, False]
Scene graph at timestep 951 is [True, False, False, False, True, False]
State prediction error at timestep 951 is tensor(7.3221e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 951 of 1
Current timestep = 952. State = [[-0.1017381  -0.10639772]]. Action = [[ 0.23028648 -0.11331099 -0.05250861  0.95138717]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 952 is [True, False, False, False, True, False]
Current timestep = 953. State = [[-0.08028403 -0.09787881]]. Action = [[ 0.05378202  0.24353659  0.21041274 -0.83797854]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 953 is [True, False, False, False, True, False]
Current timestep = 954. State = [[-0.07704291 -0.08662241]]. Action = [[-0.21515903 -0.01414828 -0.03330693 -0.7941575 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 954 is [True, False, False, False, True, False]
Scene graph at timestep 954 is [True, False, False, False, True, False]
State prediction error at timestep 954 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 954 of 1
Current timestep = 955. State = [[-0.07962354 -0.09118351]]. Action = [[ 0.12526661 -0.12871496 -0.15334408 -0.6025168 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 955 is [True, False, False, False, True, False]
Current timestep = 956. State = [[-0.07759205 -0.09883824]]. Action = [[ 0.06943482 -0.04719174  0.21028256 -0.6092003 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 956 is [True, False, False, False, True, False]
Scene graph at timestep 956 is [True, False, False, False, True, False]
State prediction error at timestep 956 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 956 of 0
Current timestep = 957. State = [[-0.066522   -0.09173567]]. Action = [[ 0.21023363  0.21430725 -0.15635413  0.8616674 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 957 is [True, False, False, False, True, False]
Scene graph at timestep 957 is [True, False, False, False, True, False]
State prediction error at timestep 957 is tensor(5.6831e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 957 of 1
Current timestep = 958. State = [[-0.04439448 -0.08195551]]. Action = [[-0.19492386 -0.04394908 -0.06675893 -0.6265384 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 958 is [True, False, False, False, True, False]
Current timestep = 959. State = [[-0.04678137 -0.09166615]]. Action = [[ 0.02828363 -0.12551208  0.11037707 -0.7022695 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 959 is [False, True, False, False, True, False]
Scene graph at timestep 959 is [False, True, False, False, True, False]
State prediction error at timestep 959 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 959 of -1
Current timestep = 960. State = [[-0.04676271 -0.11137369]]. Action = [[ 0.14470747 -0.19451663 -0.20097594  0.8493676 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 960 is [False, True, False, False, True, False]
Current timestep = 961. State = [[-0.03794608 -0.13660969]]. Action = [[ 0.18402839 -0.22310852 -0.21268421  0.7746817 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 961 is [False, True, False, False, True, False]
Scene graph at timestep 961 is [False, True, False, True, False, False]
State prediction error at timestep 961 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 961 of -1
Current timestep = 962. State = [[-0.02121675 -0.1530928 ]]. Action = [[-0.06671073  0.09594288  0.10130242 -0.88392025]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 962 is [False, True, False, True, False, False]
Scene graph at timestep 962 is [False, True, False, True, False, False]
State prediction error at timestep 962 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 962 of 1
Current timestep = 963. State = [[-0.02781212 -0.14396954]]. Action = [[-0.23954275  0.12102547 -0.15464771  0.9300816 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 963 is [False, True, False, True, False, False]
Current timestep = 964. State = [[-0.03190612 -0.12799789]]. Action = [[ 0.0714528   0.11556908 -0.12115443 -0.5441485 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 964 is [False, True, False, True, False, False]
Current timestep = 965. State = [[-0.03434838 -0.11816955]]. Action = [[-0.13235989  0.01777488 -0.15337169 -0.51788807]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 965 is [False, True, False, True, False, False]
Scene graph at timestep 965 is [False, True, False, False, True, False]
State prediction error at timestep 965 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 965 of 1
Current timestep = 966. State = [[-0.03961429 -0.10428477]]. Action = [[0.04522392 0.17982319 0.20559928 0.40102112]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 966 is [False, True, False, False, True, False]
Scene graph at timestep 966 is [False, True, False, False, True, False]
State prediction error at timestep 966 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 966 of 1
Current timestep = 967. State = [[-0.03870429 -0.08942473]]. Action = [[ 0.15426373 -0.05894917 -0.05918276 -0.78248745]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 967 is [False, True, False, False, True, False]
Current timestep = 968. State = [[-0.19269957  0.18264376]]. Action = [[ 0.02197838  0.2201936   0.11201105 -0.75435704]]. Reward = [100.]
Curr episode timestep = 37
Scene graph at timestep 968 is [False, True, False, False, True, False]
Scene graph at timestep 968 is [True, False, False, False, False, True]
State prediction error at timestep 968 is tensor(0.0526, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 968 of 0
Current timestep = 969. State = [[-0.17176192  0.20918852]]. Action = [[0.15999627 0.09437478 0.04153022 0.23892236]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 969 is [True, False, False, False, False, True]
Current timestep = 970. State = [[-0.14928098  0.21494825]]. Action = [[ 0.17156851 -0.03852519 -0.03803855  0.27446878]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 970 is [True, False, False, False, False, True]
Scene graph at timestep 970 is [True, False, False, False, False, True]
State prediction error at timestep 970 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 970 of 0
Current timestep = 971. State = [[-0.12377516  0.21992563]]. Action = [[0.16378272 0.12028459 0.1663931  0.56582963]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 971 is [True, False, False, False, False, True]
Current timestep = 972. State = [[-0.11667293  0.23287827]]. Action = [[-1.7640950e-01  2.4203241e-02 -7.7056885e-04  8.1782842e-01]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 972 is [True, False, False, False, False, True]
Current timestep = 973. State = [[-0.1249224   0.24295935]]. Action = [[ 0.03403074  0.10498345 -0.11978382  0.98176837]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 973 is [True, False, False, False, False, True]
Current timestep = 974. State = [[-0.11914115  0.24938706]]. Action = [[ 0.2147018   0.04469338 -0.08164078 -0.6868079 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 974 is [True, False, False, False, False, True]
Scene graph at timestep 974 is [True, False, False, False, False, True]
State prediction error at timestep 974 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 974 of -1
Current timestep = 975. State = [[-0.09779668  0.2642865 ]]. Action = [[-0.19796763  0.03193757  0.15489995 -0.5239799 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 975 is [True, False, False, False, False, True]
Scene graph at timestep 975 is [True, False, False, False, False, True]
State prediction error at timestep 975 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 975 of -1
Current timestep = 976. State = [[-0.10865206  0.2700957 ]]. Action = [[-0.10440074  0.22493255  0.18914264  0.87516904]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 976 is [True, False, False, False, False, True]
Scene graph at timestep 976 is [True, False, False, False, False, True]
State prediction error at timestep 976 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 976 of -1
Current timestep = 977. State = [[-0.10323464  0.2581381 ]]. Action = [[ 0.08298802 -0.18853228 -0.13993862 -0.9340804 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 977 is [True, False, False, False, False, True]
Current timestep = 978. State = [[-0.10204861  0.24796033]]. Action = [[-0.14784744 -0.02697186  0.17692709 -0.66190165]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 978 is [True, False, False, False, False, True]
Scene graph at timestep 978 is [True, False, False, False, False, True]
State prediction error at timestep 978 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 978 of 1
Current timestep = 979. State = [[-0.10642733  0.24950437]]. Action = [[ 0.01610783  0.08368695 -0.10650937 -0.11050326]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 979 is [True, False, False, False, False, True]
Scene graph at timestep 979 is [True, False, False, False, False, True]
State prediction error at timestep 979 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 979 of -1
Current timestep = 980. State = [[-0.10233956  0.2404917 ]]. Action = [[ 0.13222128 -0.24002886 -0.23364294 -0.71894115]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 980 is [True, False, False, False, False, True]
Current timestep = 981. State = [[-0.08921809  0.21584933]]. Action = [[ 0.21655393 -0.10690889 -0.16453256  0.6229923 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 981 is [True, False, False, False, False, True]
Current timestep = 982. State = [[-0.07001233  0.19929078]]. Action = [[ 0.23891619 -0.00626072  0.21868265  0.26748824]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 982 is [True, False, False, False, False, True]
Current timestep = 983. State = [[-0.04308905  0.18457428]]. Action = [[ 0.05285251 -0.2253554   0.00808224  0.24779224]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 983 is [True, False, False, False, False, True]
Current timestep = 984. State = [[-0.02495288  0.15714338]]. Action = [[ 0.22753605 -0.19779606 -0.09218617 -0.82423306]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 984 is [False, True, False, False, False, True]
Current timestep = 985. State = [[0.00465287 0.12653843]]. Action = [[ 0.23319489 -0.22190297  0.24134856  0.06276941]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 985 is [False, True, False, False, False, True]
Scene graph at timestep 985 is [False, True, False, False, False, True]
State prediction error at timestep 985 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 985 of 1
Current timestep = 986. State = [[0.03869898 0.090954  ]]. Action = [[-0.01426159 -0.24023515 -0.22569777  0.7838824 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 986 is [False, True, False, False, False, True]
Current timestep = 987. State = [[0.03999522 0.0876259 ]]. Action = [[-0.00560382  0.22631615  0.12088233 -0.7853998 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 987 is [False, True, False, False, True, False]
Scene graph at timestep 987 is [False, True, False, False, True, False]
State prediction error at timestep 987 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 987 of 0
Current timestep = 988. State = [[-0.20359804 -0.00037576]]. Action = [[ 0.23078397  0.23999125 -0.16387857  0.95899713]]. Reward = [100.]
Curr episode timestep = 19
Scene graph at timestep 988 is [False, True, False, False, True, False]
Current timestep = 989. State = [[-0.19154026 -0.00829402]]. Action = [[-0.06304805 -0.16707578  0.19285792 -0.09278095]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 989 is [True, False, False, False, True, False]
Scene graph at timestep 989 is [True, False, False, False, True, False]
State prediction error at timestep 989 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 989 of 0
Current timestep = 990. State = [[-0.18803091 -0.01599482]]. Action = [[ 0.164947    0.08506289 -0.09153806 -0.13311082]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 990 is [True, False, False, False, True, False]
Scene graph at timestep 990 is [True, False, False, False, True, False]
State prediction error at timestep 990 is tensor(2.0827e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 990 of 1
Current timestep = 991. State = [[-0.17047432 -0.02051697]]. Action = [[ 0.15629846 -0.14907992  0.03561378 -0.6315652 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 991 is [True, False, False, False, True, False]
Scene graph at timestep 991 is [True, False, False, False, True, False]
State prediction error at timestep 991 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 991 of 1
Current timestep = 992. State = [[-0.16304812 -0.01801126]]. Action = [[-0.17963636  0.18346146  0.24003789  0.9423163 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 992 is [True, False, False, False, True, False]
Scene graph at timestep 992 is [True, False, False, False, True, False]
State prediction error at timestep 992 is tensor(3.3428e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 992 of 0
Current timestep = 993. State = [[-0.1618799  -0.00180637]]. Action = [[ 0.22402811  0.1021488   0.01053882 -0.5592825 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 993 is [True, False, False, False, True, False]
Current timestep = 994. State = [[-0.13989682 -0.00218876]]. Action = [[ 0.23000059 -0.12215391 -0.20773855 -0.872176  ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 994 is [True, False, False, False, True, False]
Current timestep = 995. State = [[-0.1264287   0.00419415]]. Action = [[-0.13484354  0.16920897  0.07785255  0.9137453 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 995 is [True, False, False, False, True, False]
Current timestep = 996. State = [[-0.1329532   0.02299285]]. Action = [[-0.13401048  0.1646331   0.23594576 -0.07672316]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 996 is [True, False, False, False, True, False]
Scene graph at timestep 996 is [True, False, False, False, True, False]
State prediction error at timestep 996 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 996 of 1
Current timestep = 997. State = [[-0.14884154  0.05295748]]. Action = [[-0.21682237  0.20206493 -0.1405193  -0.8575314 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 997 is [True, False, False, False, True, False]
Scene graph at timestep 997 is [True, False, False, False, True, False]
State prediction error at timestep 997 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 997 of -1
Current timestep = 998. State = [[-0.16021784  0.0665307 ]]. Action = [[ 0.12981111 -0.10232821  0.16493753 -0.71867216]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 998 is [True, False, False, False, True, False]
Scene graph at timestep 998 is [True, False, False, False, True, False]
State prediction error at timestep 998 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 998 of 1
Current timestep = 999. State = [[-0.15485914  0.05995689]]. Action = [[ 0.09801349  0.06342158 -0.02228908  0.5005319 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 999 is [True, False, False, False, True, False]
Scene graph at timestep 999 is [True, False, False, False, True, False]
State prediction error at timestep 999 is tensor(2.6073e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 999 of 1
Current timestep = 1000. State = [[-0.14785686  0.07711302]]. Action = [[ 0.11359978  0.24037942 -0.05457479 -0.83364624]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1000 is [True, False, False, False, True, False]
Current timestep = 1001. State = [[-0.13297664  0.09873385]]. Action = [[0.11730689 0.10563639 0.04078427 0.39666653]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1001 is [True, False, False, False, True, False]
Current timestep = 1002. State = [[-0.11370631  0.11060463]]. Action = [[ 0.1690104   0.04188061 -0.21925372  0.47363687]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1002 is [True, False, False, False, True, False]
Scene graph at timestep 1002 is [True, False, False, False, True, False]
State prediction error at timestep 1002 is tensor(6.8564e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1002 of 0
Current timestep = 1003. State = [[-0.09235878  0.1070037 ]]. Action = [[ 0.03819636 -0.19450752  0.21578729 -0.6113452 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1003 is [True, False, False, False, True, False]
Current timestep = 1004. State = [[-0.07983405  0.09571628]]. Action = [[ 0.21895823 -0.0154731   0.00176993  0.46032465]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1004 is [True, False, False, False, True, False]
Scene graph at timestep 1004 is [True, False, False, False, True, False]
State prediction error at timestep 1004 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1004 of 1
Current timestep = 1005. State = [[-0.06039288  0.08867374]]. Action = [[-0.15020779 -0.07310988 -0.18362895 -0.9927022 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1005 is [True, False, False, False, True, False]
Current timestep = 1006. State = [[-0.05766969  0.07039566]]. Action = [[ 0.16641828 -0.20888382 -0.05813426 -0.30107343]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1006 is [True, False, False, False, True, False]
Scene graph at timestep 1006 is [True, False, False, False, True, False]
State prediction error at timestep 1006 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1006 of 1
Current timestep = 1007. State = [[-0.04778402  0.04253823]]. Action = [[ 0.13413778 -0.17449912  0.12616682  0.5714233 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1007 is [True, False, False, False, True, False]
Scene graph at timestep 1007 is [False, True, False, False, True, False]
State prediction error at timestep 1007 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1007 of 1
Current timestep = 1008. State = [[-0.16990936  0.20579393]]. Action = [[ 0.24696079  0.02330616 -0.20029877  0.67802143]]. Reward = [100.]
Curr episode timestep = 19
Scene graph at timestep 1008 is [False, True, False, False, True, False]
Current timestep = 1009. State = [[-0.14312103  0.21658906]]. Action = [[ 0.1201185  -0.24575862  0.00459167  0.343809  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1009 is [True, False, False, False, False, True]
Scene graph at timestep 1009 is [True, False, False, False, False, True]
State prediction error at timestep 1009 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1009 of 1
Current timestep = 1010. State = [[-0.13140553  0.1840119 ]]. Action = [[ 0.04575878 -0.24211195  0.00263849  0.29994416]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1010 is [True, False, False, False, False, True]
Scene graph at timestep 1010 is [True, False, False, False, False, True]
State prediction error at timestep 1010 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1010 of 1
Current timestep = 1011. State = [[-0.1241294   0.15427013]]. Action = [[-0.07336226 -0.2227122   0.10134077  0.9348917 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1011 is [True, False, False, False, False, True]
Current timestep = 1012. State = [[-0.12578645  0.13785397]]. Action = [[-0.1161634  -0.07678071 -0.01898825 -0.09102428]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1012 is [True, False, False, False, False, True]
Scene graph at timestep 1012 is [True, False, False, False, False, True]
State prediction error at timestep 1012 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1012 of 1
Current timestep = 1013. State = [[-0.12549286  0.1160876 ]]. Action = [[ 0.211245   -0.17607316 -0.00425133 -0.07423174]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1013 is [True, False, False, False, False, True]
Current timestep = 1014. State = [[-0.11300111  0.08610675]]. Action = [[ 0.13082981 -0.24444817 -0.13813308 -0.6929469 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1014 is [True, False, False, False, True, False]
Current timestep = 1015. State = [[-0.10792701  0.05308626]]. Action = [[-0.23070726 -0.22991979 -0.20270526  0.82865167]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1015 is [True, False, False, False, True, False]
Scene graph at timestep 1015 is [True, False, False, False, True, False]
State prediction error at timestep 1015 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1015 of 1
Current timestep = 1016. State = [[-0.1120876   0.02273145]]. Action = [[-0.05982682 -0.15940006  0.03861964  0.10688841]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1016 is [True, False, False, False, True, False]
Scene graph at timestep 1016 is [True, False, False, False, True, False]
State prediction error at timestep 1016 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1016 of -1
Current timestep = 1017. State = [[-0.12360843 -0.00926999]]. Action = [[-0.07052341 -0.2344635   0.21215242  0.1840719 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1017 is [True, False, False, False, True, False]
Scene graph at timestep 1017 is [True, False, False, False, True, False]
State prediction error at timestep 1017 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1017 of -1
Current timestep = 1018. State = [[-0.12960319 -0.02682969]]. Action = [[0.0322873  0.05360284 0.13754481 0.04149365]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1018 is [True, False, False, False, True, False]
Current timestep = 1019. State = [[-0.12234019 -0.03859913]]. Action = [[ 0.21878079 -0.24735999 -0.05209607  0.9502492 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1019 is [True, False, False, False, True, False]
Scene graph at timestep 1019 is [True, False, False, False, True, False]
State prediction error at timestep 1019 is tensor(6.4790e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1019 of 1
Current timestep = 1020. State = [[-0.11584045 -0.07289626]]. Action = [[-0.11985546 -0.22290277 -0.16156174  0.71169305]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1020 is [True, False, False, False, True, False]
Current timestep = 1021. State = [[-0.11453059 -0.08392949]]. Action = [[ 0.16404557  0.08207041 -0.10502344  0.11657906]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1021 is [True, False, False, False, True, False]
Current timestep = 1022. State = [[-0.11415812 -0.09021018]]. Action = [[-0.12097478 -0.13439508  0.07227135  0.19615614]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1022 is [True, False, False, False, True, False]
Current timestep = 1023. State = [[-0.1150343  -0.10780236]]. Action = [[-0.02421692 -0.13415277 -0.09469697  0.28377903]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1023 is [True, False, False, False, True, False]
Current timestep = 1024. State = [[-0.11832846 -0.12329151]]. Action = [[-0.08946684 -0.06701142 -0.18042684  0.9428375 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1024 is [True, False, False, False, True, False]
Current timestep = 1025. State = [[-0.1206883  -0.13795125]]. Action = [[ 0.12500215 -0.12383172  0.10493937  0.5989995 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1025 is [True, False, False, False, True, False]
Current timestep = 1026. State = [[-0.11830726 -0.15488556]]. Action = [[ 0.05170995 -0.10322914 -0.24719064  0.82121134]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1026 is [True, False, False, True, False, False]
Current timestep = 1027. State = [[-0.12185382 -0.1753603 ]]. Action = [[-0.16330326 -0.18238845  0.00975832 -0.87266284]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1027 is [True, False, False, True, False, False]
Scene graph at timestep 1027 is [True, False, False, True, False, False]
State prediction error at timestep 1027 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1027 of -1
Current timestep = 1028. State = [[-0.12510005 -0.18087067]]. Action = [[ 0.06267512  0.23744395 -0.20515701  0.7387626 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1028 is [True, False, False, True, False, False]
Scene graph at timestep 1028 is [True, False, False, True, False, False]
State prediction error at timestep 1028 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1028 of 1
Current timestep = 1029. State = [[-0.11211286 -0.14995906]]. Action = [[ 0.24728218  0.2326048  -0.18452743 -0.7685222 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1029 is [True, False, False, True, False, False]
Current timestep = 1030. State = [[-0.09079883 -0.13340451]]. Action = [[ 0.18581122 -0.03462929 -0.01130374 -0.3166287 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1030 is [True, False, False, True, False, False]
Current timestep = 1031. State = [[-0.07210185 -0.11949122]]. Action = [[0.02685767 0.21039492 0.1941706  0.73858666]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1031 is [True, False, False, True, False, False]
Current timestep = 1032. State = [[-0.06474519 -0.11566392]]. Action = [[ 0.11252642 -0.19448818  0.07864872 -0.6660708 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1032 is [True, False, False, False, True, False]
Scene graph at timestep 1032 is [True, False, False, False, True, False]
State prediction error at timestep 1032 is tensor(8.1297e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1032 of 1
Current timestep = 1033. State = [[-0.0451673  -0.11780473]]. Action = [[ 0.22473443  0.12142548 -0.02775572  0.7329391 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1033 is [True, False, False, False, True, False]
Current timestep = 1034. State = [[-0.01624921 -0.11190035]]. Action = [[ 0.21692276  0.00733596 -0.09830177  0.8865756 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1034 is [False, True, False, False, True, False]
Current timestep = 1035. State = [[-0.17103033 -0.09809399]]. Action = [[ 0.16819748  0.22040743 -0.0650952   0.83882844]]. Reward = [100.]
Curr episode timestep = 26
Scene graph at timestep 1035 is [False, True, False, False, True, False]
Scene graph at timestep 1035 is [True, False, False, False, True, False]
State prediction error at timestep 1035 is tensor(0.0126, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1035 of 0
Current timestep = 1036. State = [[-0.158033   -0.10926799]]. Action = [[-0.07394601  0.04455775 -0.05204654  0.2804495 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1036 is [True, False, False, False, True, False]
Scene graph at timestep 1036 is [True, False, False, False, True, False]
State prediction error at timestep 1036 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1036 of 0
Current timestep = 1037. State = [[-0.16018204 -0.12129412]]. Action = [[-0.01162776 -0.20843616  0.16042262  0.7406137 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1037 is [True, False, False, False, True, False]
Scene graph at timestep 1037 is [True, False, False, False, True, False]
State prediction error at timestep 1037 is tensor(5.4579e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1037 of -1
Current timestep = 1038. State = [[-0.16433196 -0.14542128]]. Action = [[-0.0149634  -0.16218372  0.19695902 -0.38805926]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1038 is [True, False, False, False, True, False]
Scene graph at timestep 1038 is [True, False, False, True, False, False]
State prediction error at timestep 1038 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1038 of -1
Current timestep = 1039. State = [[-0.15950473 -0.15332364]]. Action = [[ 0.15828487  0.10325286  0.17354465 -0.3087579 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1039 is [True, False, False, True, False, False]
Current timestep = 1040. State = [[-0.14556311 -0.15034615]]. Action = [[ 0.13609579 -0.03291182 -0.15604274  0.19552064]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1040 is [True, False, False, True, False, False]
Scene graph at timestep 1040 is [True, False, False, True, False, False]
State prediction error at timestep 1040 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1040 of 1
Current timestep = 1041. State = [[-0.13551927 -0.14139047]]. Action = [[-0.18777741  0.19648653 -0.17216335 -0.78527606]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1041 is [True, False, False, True, False, False]
Current timestep = 1042. State = [[-0.1472125  -0.12017354]]. Action = [[-0.20029978  0.16019523  0.12073836 -0.6464778 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1042 is [True, False, False, True, False, False]
Scene graph at timestep 1042 is [True, False, False, False, True, False]
State prediction error at timestep 1042 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1042 of 1
Current timestep = 1043. State = [[-0.1738906  -0.11669303]]. Action = [[-0.22971421 -0.18816696 -0.11920077 -0.44285297]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1043 is [True, False, False, False, True, False]
Current timestep = 1044. State = [[-0.18542548 -0.1302573 ]]. Action = [[ 0.18736705 -0.07331386  0.0243108  -0.6926736 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1044 is [True, False, False, False, True, False]
Current timestep = 1045. State = [[-0.18708308 -0.14960472]]. Action = [[-0.12413913 -0.22965766  0.11337197 -0.2994157 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1045 is [True, False, False, True, False, False]
Current timestep = 1046. State = [[-0.20226255 -0.17869166]]. Action = [[-0.19220553 -0.14924625  0.13514182  0.15667224]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1046 is [True, False, False, True, False, False]
Current timestep = 1047. State = [[-0.2068796  -0.17902856]]. Action = [[ 0.21394008  0.22150922 -0.00230685  0.652678  ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1047 is [True, False, False, True, False, False]
Scene graph at timestep 1047 is [True, False, False, True, False, False]
State prediction error at timestep 1047 is tensor(7.0090e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1047 of -1
Current timestep = 1048. State = [[-0.20203781 -0.17310455]]. Action = [[-0.10400277 -0.12005243  0.14666605  0.09780526]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1048 is [True, False, False, True, False, False]
Scene graph at timestep 1048 is [True, False, False, True, False, False]
State prediction error at timestep 1048 is tensor(1.0333e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1048 of -1
Current timestep = 1049. State = [[-0.21073104 -0.17316726]]. Action = [[-0.12727338  0.15839428 -0.13124691 -0.3203104 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1049 is [True, False, False, True, False, False]
Scene graph at timestep 1049 is [True, False, False, True, False, False]
State prediction error at timestep 1049 is tensor(9.8047e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1049 of -1
Current timestep = 1050. State = [[-0.21062803 -0.14900933]]. Action = [[0.15035376 0.24078554 0.07956648 0.76360774]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1050 is [True, False, False, True, False, False]
Scene graph at timestep 1050 is [True, False, False, True, False, False]
State prediction error at timestep 1050 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1050 of 1
Current timestep = 1051. State = [[-0.20724936 -0.1276249 ]]. Action = [[-0.04381227 -0.01581237 -0.13558085  0.21615994]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1051 is [True, False, False, True, False, False]
Current timestep = 1052. State = [[-0.21295516 -0.1325462 ]]. Action = [[-0.16045715 -0.06989083  0.22400469  0.09900212]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1052 is [True, False, False, True, False, False]
Scene graph at timestep 1052 is [True, False, False, True, False, False]
State prediction error at timestep 1052 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1052 of -1
Current timestep = 1053. State = [[-0.22095855 -0.12446144]]. Action = [[ 0.15817809  0.21846932  0.22496393 -0.65372664]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1053 is [True, False, False, True, False, False]
Current timestep = 1054. State = [[-0.21651496 -0.09683952]]. Action = [[0.02318174 0.20463336 0.04085538 0.72847795]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1054 is [True, False, False, False, True, False]
Current timestep = 1055. State = [[-0.21610421 -0.08014451]]. Action = [[-0.00852497  0.00421473  0.21022728  0.8312285 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1055 is [True, False, False, False, True, False]
Scene graph at timestep 1055 is [True, False, False, False, True, False]
State prediction error at timestep 1055 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1055 of 1
Current timestep = 1056. State = [[-0.20506571 -0.06350166]]. Action = [[0.23687214 0.22030813 0.09412348 0.2343626 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1056 is [True, False, False, False, True, False]
Scene graph at timestep 1056 is [True, False, False, False, True, False]
State prediction error at timestep 1056 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1056 of 1
Current timestep = 1057. State = [[-0.19050488 -0.0380183 ]]. Action = [[-0.03017128  0.09262708  0.09122837  0.9326811 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1057 is [True, False, False, False, True, False]
Scene graph at timestep 1057 is [True, False, False, False, True, False]
State prediction error at timestep 1057 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1057 of 0
Current timestep = 1058. State = [[-0.19038938 -0.02794079]]. Action = [[ 0.03125328  0.04027906  0.08026621 -0.6434983 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1058 is [True, False, False, False, True, False]
Scene graph at timestep 1058 is [True, False, False, False, True, False]
State prediction error at timestep 1058 is tensor(1.4732e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1058 of 0
Current timestep = 1059. State = [[-0.17812997 -0.0222203 ]]. Action = [[ 0.21613145  0.0186339  -0.12591363  0.52964807]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1059 is [True, False, False, False, True, False]
Current timestep = 1060. State = [[-0.14871511 -0.03294908]]. Action = [[ 0.24324542 -0.23071027  0.06187719 -0.5383801 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1060 is [True, False, False, False, True, False]
Current timestep = 1061. State = [[-0.12953475 -0.05868287]]. Action = [[-0.01613878 -0.21957608 -0.03662694 -0.3247801 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1061 is [True, False, False, False, True, False]
Scene graph at timestep 1061 is [True, False, False, False, True, False]
State prediction error at timestep 1061 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1061 of 1
Current timestep = 1062. State = [[-0.11441109 -0.0861772 ]]. Action = [[ 0.22719675 -0.13083792 -0.11905256  0.05218756]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1062 is [True, False, False, False, True, False]
Current timestep = 1063. State = [[-0.09460368 -0.08422833]]. Action = [[ 0.01541945  0.21590328 -0.12297463  0.16337967]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1063 is [True, False, False, False, True, False]
Scene graph at timestep 1063 is [True, False, False, False, True, False]
State prediction error at timestep 1063 is tensor(9.3470e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1063 of 1
Current timestep = 1064. State = [[-0.07945596 -0.08589978]]. Action = [[ 0.24680644 -0.23545346  0.09652176  0.9087124 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1064 is [True, False, False, False, True, False]
Scene graph at timestep 1064 is [True, False, False, False, True, False]
State prediction error at timestep 1064 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1064 of 1
Current timestep = 1065. State = [[-0.04567046 -0.10653041]]. Action = [[ 0.19104403 -0.09723029 -0.14908795  0.88283277]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1065 is [True, False, False, False, True, False]
Scene graph at timestep 1065 is [False, True, False, False, True, False]
State prediction error at timestep 1065 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1065 of 1
Current timestep = 1066. State = [[-0.0169711  -0.11358294]]. Action = [[ 0.2188319   0.01514804 -0.22520265  0.75968003]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1066 is [False, True, False, False, True, False]
Current timestep = 1067. State = [[ 0.00717806 -0.10982575]]. Action = [[ 0.12481794  0.07517016 -0.14108765 -0.38813996]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1067 is [False, True, False, False, True, False]
Current timestep = 1068. State = [[ 0.03078118 -0.11423698]]. Action = [[ 0.15975106 -0.14103387  0.05007988  0.40709984]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1068 is [False, True, False, False, True, False]
Scene graph at timestep 1068 is [False, True, False, False, True, False]
State prediction error at timestep 1068 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1068 of -1
Current timestep = 1069. State = [[ 0.05284761 -0.12421097]]. Action = [[ 0.2152363  -0.19156666  0.06287569 -0.16441989]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1069 is [False, True, False, False, True, False]
Current timestep = 1070. State = [[ 0.05284761 -0.12421097]]. Action = [[ 0.22450393  0.07311413 -0.02488479  0.35807717]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1070 is [False, False, True, False, True, False]
Current timestep = 1071. State = [[ 0.05284761 -0.12421097]]. Action = [[ 0.13364059 -0.11048725  0.2296975   0.76148057]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1071 is [False, False, True, False, True, False]
Current timestep = 1072. State = [[ 0.05167154 -0.11442118]]. Action = [[-0.11882913  0.18512896 -0.06826495 -0.09303868]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1072 is [False, False, True, False, True, False]
Current timestep = 1073. State = [[ 0.04721836 -0.11262095]]. Action = [[-0.16208982 -0.1176836  -0.07118613 -0.9623035 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1073 is [False, False, True, False, True, False]
Scene graph at timestep 1073 is [False, True, False, False, True, False]
State prediction error at timestep 1073 is tensor(7.0430e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1073 of 1
Current timestep = 1074. State = [[ 0.04182449 -0.11686344]]. Action = [[-0.0558752   0.05763707 -0.05279371  0.12964988]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1074 is [False, True, False, False, True, False]
Scene graph at timestep 1074 is [False, True, False, False, True, False]
State prediction error at timestep 1074 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1074 of 1
Current timestep = 1075. State = [[-0.2615326  -0.12705246]]. Action = [[0.15204978 0.16436997 0.20052058 0.5192567 ]]. Reward = [100.]
Curr episode timestep = 39
Scene graph at timestep 1075 is [False, True, False, False, True, False]
Scene graph at timestep 1075 is [True, False, False, True, False, False]
State prediction error at timestep 1075 is tensor(0.0455, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1075 of 0
Current timestep = 1076. State = [[-0.26141235 -0.14177428]]. Action = [[-0.02409206 -0.02636157 -0.00044915 -0.28225577]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1076 is [True, False, False, True, False, False]
Scene graph at timestep 1076 is [True, False, False, True, False, False]
State prediction error at timestep 1076 is tensor(8.5894e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1076 of 0
Current timestep = 1077. State = [[-0.26052433 -0.15223372]]. Action = [[ 0.06888103 -0.10322687 -0.14097281  0.68915296]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1077 is [True, False, False, True, False, False]
Current timestep = 1078. State = [[-0.25091526 -0.14607301]]. Action = [[0.16190559 0.24597907 0.16695654 0.5005436 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1078 is [True, False, False, True, False, False]
Scene graph at timestep 1078 is [True, False, False, True, False, False]
State prediction error at timestep 1078 is tensor(9.4657e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1078 of 1
Current timestep = 1079. State = [[-0.2346918  -0.12225063]]. Action = [[0.06051844 0.15266946 0.15212345 0.6526717 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1079 is [True, False, False, True, False, False]
Current timestep = 1080. State = [[-0.22924207 -0.11751927]]. Action = [[ 0.00870013 -0.1130271  -0.02048406  0.9151101 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1080 is [True, False, False, False, True, False]
Current timestep = 1081. State = [[-0.22785802 -0.12034216]]. Action = [[-0.04110011 -0.0105343  -0.00762902  0.01677644]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1081 is [True, False, False, False, True, False]
Current timestep = 1082. State = [[-0.22812635 -0.12075128]]. Action = [[ 0.00488067  0.03139085 -0.00264515  0.12306356]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1082 is [True, False, False, False, True, False]
Scene graph at timestep 1082 is [True, False, False, False, True, False]
State prediction error at timestep 1082 is tensor(6.1262e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1082 of 1
Current timestep = 1083. State = [[-0.22862911 -0.12212963]]. Action = [[-0.06808019 -0.03428416  0.16263175 -0.4484495 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1083 is [True, False, False, False, True, False]
Current timestep = 1084. State = [[-0.22827259 -0.11456584]]. Action = [[ 0.08678463  0.1605075  -0.04549646 -0.76544887]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1084 is [True, False, False, False, True, False]
Current timestep = 1085. State = [[-0.22367655 -0.09244376]]. Action = [[ 0.1002149   0.22609532  0.08021009 -0.09267408]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1085 is [True, False, False, False, True, False]
Current timestep = 1086. State = [[-0.22142816 -0.06651855]]. Action = [[-0.06758618  0.14268738  0.08181855  0.75881267]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1086 is [True, False, False, False, True, False]
Current timestep = 1087. State = [[-0.22552545 -0.04383223]]. Action = [[-0.07702717  0.14858684  0.1609497   0.11019754]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1087 is [True, False, False, False, True, False]
Scene graph at timestep 1087 is [True, False, False, False, True, False]
State prediction error at timestep 1087 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1087 of 1
Current timestep = 1088. State = [[-0.22910413 -0.028196  ]]. Action = [[0.00262818 0.02800065 0.22461677 0.4765787 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1088 is [True, False, False, False, True, False]
Current timestep = 1089. State = [[-0.22949548 -0.02674625]]. Action = [[-0.01766652 -0.0124543   0.06622061 -0.21568221]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1089 is [True, False, False, False, True, False]
Current timestep = 1090. State = [[-0.22795638 -0.01812292]]. Action = [[ 0.11633942  0.14004657 -0.17342858 -0.5334661 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1090 is [True, False, False, False, True, False]
Scene graph at timestep 1090 is [True, False, False, False, True, False]
State prediction error at timestep 1090 is tensor(2.3196e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1090 of 1
Current timestep = 1091. State = [[-0.22204737  0.00490371]]. Action = [[0.11112845 0.20113575 0.19670135 0.5049827 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1091 is [True, False, False, False, True, False]
Scene graph at timestep 1091 is [True, False, False, False, True, False]
State prediction error at timestep 1091 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1091 of -1
Current timestep = 1092. State = [[-0.20271525  0.02221844]]. Action = [[ 0.12593779 -0.03616507  0.03730878  0.91110563]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1092 is [True, False, False, False, True, False]
Current timestep = 1093. State = [[-0.19646536  0.033272  ]]. Action = [[-0.10683639  0.18469375 -0.22144467 -0.8998404 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1093 is [True, False, False, False, True, False]
Current timestep = 1094. State = [[-0.19632165  0.03667622]]. Action = [[ 0.08227351 -0.14328718 -0.02944969 -0.5819604 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1094 is [True, False, False, False, True, False]
Scene graph at timestep 1094 is [True, False, False, False, True, False]
State prediction error at timestep 1094 is tensor(2.0871e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1094 of 1
Current timestep = 1095. State = [[-0.18544784  0.04178077]]. Action = [[ 0.19131586  0.21707356 -0.2278873  -0.596111  ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1095 is [True, False, False, False, True, False]
Scene graph at timestep 1095 is [True, False, False, False, True, False]
State prediction error at timestep 1095 is tensor(7.8605e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1095 of 1
Current timestep = 1096. State = [[-0.15609288  0.05803221]]. Action = [[ 0.21680504  0.02422413 -0.11518943 -0.03534162]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1096 is [True, False, False, False, True, False]
Current timestep = 1097. State = [[-0.12558542  0.05987005]]. Action = [[ 0.24010491 -0.02283809 -0.05432671 -0.8357992 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1097 is [True, False, False, False, True, False]
Current timestep = 1098. State = [[-0.09763626  0.06595655]]. Action = [[ 0.11434492  0.1029076   0.02224794 -0.9350523 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1098 is [True, False, False, False, True, False]
Scene graph at timestep 1098 is [True, False, False, False, True, False]
State prediction error at timestep 1098 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1098 of 1
Current timestep = 1099. State = [[-0.07139254  0.05877631]]. Action = [[ 0.215898   -0.22519101  0.02603361  0.30124724]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1099 is [True, False, False, False, True, False]
Current timestep = 1100. State = [[-0.05911564  0.04854677]]. Action = [[-0.16624308  0.02502054 -0.09935318 -0.28064263]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1100 is [True, False, False, False, True, False]
Current timestep = 1101. State = [[-0.05727824  0.04697119]]. Action = [[ 0.16609347  0.00687146  0.12310678 -0.3539726 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1101 is [True, False, False, False, True, False]
Current timestep = 1102. State = [[-0.0514465   0.03592312]]. Action = [[-0.07880545 -0.18550128  0.11634946 -0.06613058]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1102 is [True, False, False, False, True, False]
Current timestep = 1103. State = [[-0.05390577  0.01789936]]. Action = [[-0.10516538 -0.11122715 -0.18130258 -0.49881053]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1103 is [True, False, False, False, True, False]
Scene graph at timestep 1103 is [True, False, False, False, True, False]
State prediction error at timestep 1103 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1103 of 1
Current timestep = 1104. State = [[-0.05321375 -0.00361889]]. Action = [[ 0.17201877 -0.17865181 -0.2160122  -0.63502944]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1104 is [True, False, False, False, True, False]
Scene graph at timestep 1104 is [True, False, False, False, True, False]
State prediction error at timestep 1104 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1104 of 1
Current timestep = 1105. State = [[-0.05096375 -0.02471793]]. Action = [[-0.14243117 -0.11498094 -0.21788144  0.9611678 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1105 is [True, False, False, False, True, False]
Scene graph at timestep 1105 is [True, False, False, False, True, False]
State prediction error at timestep 1105 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1105 of 1
Current timestep = 1106. State = [[-0.05417539 -0.02282331]]. Action = [[-0.04374988  0.23726517  0.01443577 -0.99331915]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1106 is [True, False, False, False, True, False]
Scene graph at timestep 1106 is [True, False, False, False, True, False]
State prediction error at timestep 1106 is tensor(9.4015e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1106 of 1
Current timestep = 1107. State = [[-0.05438951 -0.00217948]]. Action = [[ 0.19742     0.06980583 -0.13304773  0.6863966 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1107 is [True, False, False, False, True, False]
Scene graph at timestep 1107 is [True, False, False, False, True, False]
State prediction error at timestep 1107 is tensor(5.9980e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1107 of 1
Current timestep = 1108. State = [[-0.04219003  0.0076113 ]]. Action = [[ 0.12097609  0.04341528  0.0067645  -0.4836144 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1108 is [True, False, False, False, True, False]
Current timestep = 1109. State = [[-0.16014194 -0.01652764]]. Action = [[ 0.03654319 -0.01492639 -0.11092587 -0.6632983 ]]. Reward = [100.]
Curr episode timestep = 33
Scene graph at timestep 1109 is [False, True, False, False, True, False]
Current timestep = 1110. State = [[-0.1349853  -0.01935406]]. Action = [[ 0.23600858 -0.00610571 -0.06827173  0.27432775]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1110 is [True, False, False, False, True, False]
Scene graph at timestep 1110 is [True, False, False, False, True, False]
State prediction error at timestep 1110 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1110 of 1
Current timestep = 1111. State = [[-0.10110902 -0.0110299 ]]. Action = [[0.20870066 0.18318474 0.1632433  0.2340771 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1111 is [True, False, False, False, True, False]
Current timestep = 1112. State = [[-0.08029442  0.00233357]]. Action = [[ 0.07741472  0.07721829 -0.01847303  0.7724199 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1112 is [True, False, False, False, True, False]
Current timestep = 1113. State = [[-0.06339257  0.00049358]]. Action = [[ 0.12671524 -0.15820175  0.13067067 -0.3485936 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1113 is [True, False, False, False, True, False]
Scene graph at timestep 1113 is [True, False, False, False, True, False]
State prediction error at timestep 1113 is tensor(3.0870e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1113 of 1
Current timestep = 1114. State = [[-0.04872987  0.00530261]]. Action = [[ 0.05521581  0.21393389 -0.07856682 -0.81217796]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1114 is [True, False, False, False, True, False]
Scene graph at timestep 1114 is [False, True, False, False, True, False]
State prediction error at timestep 1114 is tensor(2.5922e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1114 of 1
Current timestep = 1115. State = [[-0.04505534  0.02054405]]. Action = [[-0.11939676  0.0257878   0.11814648 -0.37805665]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1115 is [False, True, False, False, True, False]
Scene graph at timestep 1115 is [False, True, False, False, True, False]
State prediction error at timestep 1115 is tensor(9.3212e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1115 of 0
Current timestep = 1116. State = [[-0.04397114  0.01619513]]. Action = [[ 0.15070504 -0.13386905  0.03611556  0.10187769]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1116 is [False, True, False, False, True, False]
Scene graph at timestep 1116 is [False, True, False, False, True, False]
State prediction error at timestep 1116 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1116 of 1
Current timestep = 1117. State = [[-0.0432463   0.01980662]]. Action = [[-0.01163255  0.19593489  0.15300953  0.19298303]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1117 is [False, True, False, False, True, False]
Current timestep = 1118. State = [[-0.04263087  0.01831244]]. Action = [[-0.07337493 -0.19271949  0.124908    0.28666317]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1118 is [False, True, False, False, True, False]
Scene graph at timestep 1118 is [False, True, False, False, True, False]
State prediction error at timestep 1118 is tensor(7.0419e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1118 of 1
Current timestep = 1119. State = [[-0.04441744 -0.00450728]]. Action = [[-0.13894805 -0.23040263 -0.04854357  0.42404985]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1119 is [False, True, False, False, True, False]
Scene graph at timestep 1119 is [False, True, False, False, True, False]
State prediction error at timestep 1119 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1119 of 1
Current timestep = 1120. State = [[-0.04342498 -0.03078897]]. Action = [[ 0.20153752 -0.12096152 -0.20755853  0.8384744 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1120 is [False, True, False, False, True, False]
Scene graph at timestep 1120 is [False, True, False, False, True, False]
State prediction error at timestep 1120 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1120 of -1
Current timestep = 1121. State = [[-0.0381291  -0.04486275]]. Action = [[-0.00375065 -0.07484712  0.10044828 -0.00723255]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1121 is [False, True, False, False, True, False]
Current timestep = 1122. State = [[-0.03698466 -0.04826272]]. Action = [[ 0.02169532  0.06014463  0.15342939 -0.92850626]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1122 is [False, True, False, False, True, False]
Scene graph at timestep 1122 is [False, True, False, False, True, False]
State prediction error at timestep 1122 is tensor(3.4386e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1122 of -1
Current timestep = 1123. State = [[-0.2299835   0.07611888]]. Action = [[ 0.14046222  0.24045     0.16780567 -0.9083608 ]]. Reward = [100.]
Curr episode timestep = 13
Scene graph at timestep 1123 is [False, True, False, False, True, False]
Scene graph at timestep 1123 is [True, False, False, False, True, False]
State prediction error at timestep 1123 is tensor(0.0296, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1123 of 0
Current timestep = 1124. State = [[-0.2179812   0.08656009]]. Action = [[ 0.17606348  0.06625095  0.01640728 -0.22588289]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1124 is [True, False, False, False, True, False]
Scene graph at timestep 1124 is [True, False, False, False, True, False]
State prediction error at timestep 1124 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1124 of 1
Current timestep = 1125. State = [[-0.20318522  0.1059444 ]]. Action = [[-0.08159691  0.17826027  0.22030377  0.8695376 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1125 is [True, False, False, False, True, False]
Scene graph at timestep 1125 is [True, False, False, False, True, False]
State prediction error at timestep 1125 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1125 of -1
Current timestep = 1126. State = [[-0.20196928  0.12081918]]. Action = [[ 0.17081264 -0.03058527 -0.1658165   0.20010114]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1126 is [True, False, False, False, True, False]
Current timestep = 1127. State = [[-0.18715528  0.11160982]]. Action = [[ 0.00441074 -0.16888942  0.16729861  0.63255167]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1127 is [True, False, False, False, True, False]
Current timestep = 1128. State = [[-0.18518443  0.09890694]]. Action = [[-0.0072659  -0.04264304 -0.22080539  0.45970738]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1128 is [True, False, False, False, True, False]
Current timestep = 1129. State = [[-0.18102261  0.08124183]]. Action = [[ 0.06162664 -0.21760574 -0.17190549 -0.7038009 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1129 is [True, False, False, False, True, False]
Current timestep = 1130. State = [[-0.17648919  0.0530872 ]]. Action = [[ 0.01349059 -0.20138466 -0.04582521 -0.16545188]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1130 is [True, False, False, False, True, False]
Scene graph at timestep 1130 is [True, False, False, False, True, False]
State prediction error at timestep 1130 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1130 of 1
Current timestep = 1131. State = [[-0.17384797  0.02782429]]. Action = [[-0.10457191 -0.1161392  -0.07131448 -0.9554783 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1131 is [True, False, False, False, True, False]
Current timestep = 1132. State = [[-0.1708258   0.01523307]]. Action = [[ 0.19062299 -0.06016499 -0.14770868  0.10091877]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1132 is [True, False, False, False, True, False]
Scene graph at timestep 1132 is [True, False, False, False, True, False]
State prediction error at timestep 1132 is tensor(4.9695e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1132 of 1
Current timestep = 1133. State = [[-0.17003572 -0.00290906]]. Action = [[-0.21084505 -0.17755523  0.05622759 -0.3981892 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1133 is [True, False, False, False, True, False]
Scene graph at timestep 1133 is [True, False, False, False, True, False]
State prediction error at timestep 1133 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1133 of -1
Current timestep = 1134. State = [[-0.17002262 -0.02820248]]. Action = [[ 0.22346997 -0.16679819 -0.08237407  0.2762158 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1134 is [True, False, False, False, True, False]
Scene graph at timestep 1134 is [True, False, False, False, True, False]
State prediction error at timestep 1134 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1134 of 1
Current timestep = 1135. State = [[-0.1567636  -0.04593557]]. Action = [[ 0.10875392 -0.0572859   0.195755   -0.82219553]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1135 is [True, False, False, False, True, False]
Scene graph at timestep 1135 is [True, False, False, False, True, False]
State prediction error at timestep 1135 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1135 of 1
Current timestep = 1136. State = [[-0.13971627 -0.06218763]]. Action = [[ 0.1371716  -0.20998219 -0.10024273  0.510504  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1136 is [True, False, False, False, True, False]
Current timestep = 1137. State = [[-0.12232679 -0.08154298]]. Action = [[ 0.0734697  -0.04629067  0.13252223 -0.53506905]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1137 is [True, False, False, False, True, False]
Scene graph at timestep 1137 is [True, False, False, False, True, False]
State prediction error at timestep 1137 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1137 of 1
Current timestep = 1138. State = [[-0.10802633 -0.09573631]]. Action = [[ 0.15044042 -0.12496607  0.06213751 -0.2484616 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1138 is [True, False, False, False, True, False]
Current timestep = 1139. State = [[-0.09672792 -0.1113337 ]]. Action = [[-0.05884278 -0.08949871  0.20743921 -0.00744331]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1139 is [True, False, False, False, True, False]
Current timestep = 1140. State = [[-0.09879953 -0.13313791]]. Action = [[-0.06477848 -0.23125917  0.06819353 -0.3678555 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1140 is [True, False, False, False, True, False]
Scene graph at timestep 1140 is [True, False, False, True, False, False]
State prediction error at timestep 1140 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1140 of -1
Current timestep = 1141. State = [[-0.10640548 -0.15052842]]. Action = [[-0.2200868   0.10875121 -0.09748921 -0.56534994]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1141 is [True, False, False, True, False, False]
Scene graph at timestep 1141 is [True, False, False, True, False, False]
State prediction error at timestep 1141 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1141 of -1
Current timestep = 1142. State = [[-0.11775739 -0.1501553 ]]. Action = [[-0.11825612 -0.0289098   0.06408325  0.70589507]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1142 is [True, False, False, True, False, False]
Current timestep = 1143. State = [[-0.12625156 -0.1476344 ]]. Action = [[ 0.1656872   0.05466571 -0.19587196  0.9420233 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1143 is [True, False, False, True, False, False]
Current timestep = 1144. State = [[-0.12575579 -0.13809204]]. Action = [[-0.00840673  0.09533295  0.1289919   0.8558217 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1144 is [True, False, False, True, False, False]
Current timestep = 1145. State = [[-0.12479226 -0.12263061]]. Action = [[ 0.03674278  0.12355864  0.08522326 -0.1537664 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1145 is [True, False, False, True, False, False]
Scene graph at timestep 1145 is [True, False, False, False, True, False]
State prediction error at timestep 1145 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1145 of 1
Current timestep = 1146. State = [[-0.11258484 -0.10085735]]. Action = [[ 0.21039265  0.16089574 -0.05016319  0.7333635 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1146 is [True, False, False, False, True, False]
Current timestep = 1147. State = [[-0.09224594 -0.0854528 ]]. Action = [[ 0.16194227  0.05612561 -0.11471815  0.5512879 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1147 is [True, False, False, False, True, False]
Scene graph at timestep 1147 is [True, False, False, False, True, False]
State prediction error at timestep 1147 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1147 of 1
Current timestep = 1148. State = [[-0.07672817 -0.06859442]]. Action = [[ 0.02364865  0.16867357 -0.2327059  -0.13394618]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1148 is [True, False, False, False, True, False]
Scene graph at timestep 1148 is [True, False, False, False, True, False]
State prediction error at timestep 1148 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1148 of 1
Current timestep = 1149. State = [[-0.07201852 -0.05051128]]. Action = [[0.02937949 0.06064743 0.03851146 0.03803933]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1149 is [True, False, False, False, True, False]
Current timestep = 1150. State = [[-0.07052269 -0.05833627]]. Action = [[-0.03011104 -0.22502838 -0.22873229  0.27044427]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1150 is [True, False, False, False, True, False]
Scene graph at timestep 1150 is [True, False, False, False, True, False]
State prediction error at timestep 1150 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1150 of 0
Current timestep = 1151. State = [[-0.07280064 -0.05820811]]. Action = [[-0.12169605  0.24872434  0.07326883  0.6513684 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1151 is [True, False, False, False, True, False]
Scene graph at timestep 1151 is [True, False, False, False, True, False]
State prediction error at timestep 1151 is tensor(1.2898e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1151 of 1
Current timestep = 1152. State = [[-0.08032227 -0.04317684]]. Action = [[-0.1615835  -0.0220204   0.18372959 -0.06953198]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1152 is [True, False, False, False, True, False]
Scene graph at timestep 1152 is [True, False, False, False, True, False]
State prediction error at timestep 1152 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1152 of 0
Current timestep = 1153. State = [[-0.08460145 -0.0413895 ]]. Action = [[ 0.24476975  0.06303215  0.08090043 -0.48376876]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1153 is [True, False, False, False, True, False]
Scene graph at timestep 1153 is [True, False, False, False, True, False]
State prediction error at timestep 1153 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1153 of 1
Current timestep = 1154. State = [[-0.07719638 -0.03136541]]. Action = [[ 0.04088488  0.09797335  0.21771961 -0.1531105 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1154 is [True, False, False, False, True, False]
Current timestep = 1155. State = [[-0.06919608 -0.01046518]]. Action = [[ 0.07252142  0.20326596 -0.09566912 -0.23809361]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1155 is [True, False, False, False, True, False]
Scene graph at timestep 1155 is [True, False, False, False, True, False]
State prediction error at timestep 1155 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1155 of 1
Current timestep = 1156. State = [[-0.05544384  0.00796802]]. Action = [[ 0.17153549 -0.00843923 -0.09541398  0.62595975]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1156 is [True, False, False, False, True, False]
Current timestep = 1157. State = [[-0.2555586  -0.00737615]]. Action = [[ 0.18627918  0.18232241  0.24615204 -0.62770855]]. Reward = [100.]
Curr episode timestep = 33
Scene graph at timestep 1157 is [True, False, False, False, True, False]
Scene graph at timestep 1157 is [True, False, False, False, True, False]
State prediction error at timestep 1157 is tensor(0.0227, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1157 of 0
Current timestep = 1158. State = [[-0.24465941 -0.00627597]]. Action = [[0.2003848  0.09073359 0.20251974 0.8706641 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1158 is [True, False, False, False, True, False]
Current timestep = 1159. State = [[-0.22230013  0.00994129]]. Action = [[0.21941668 0.20709401 0.16820616 0.64683676]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1159 is [True, False, False, False, True, False]
Scene graph at timestep 1159 is [True, False, False, False, True, False]
State prediction error at timestep 1159 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1159 of 1
Current timestep = 1160. State = [[-0.19916265  0.03552738]]. Action = [[ 0.03801522  0.1177364  -0.09117827  0.5861223 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1160 is [True, False, False, False, True, False]
Scene graph at timestep 1160 is [True, False, False, False, True, False]
State prediction error at timestep 1160 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1160 of 1
Current timestep = 1161. State = [[-0.18692937  0.05179001]]. Action = [[0.17661572 0.13918892 0.21315885 0.74212074]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1161 is [True, False, False, False, True, False]
Current timestep = 1162. State = [[-0.16744955  0.06045368]]. Action = [[ 0.06974521 -0.0511488  -0.23090376  0.37904477]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1162 is [True, False, False, False, True, False]
Scene graph at timestep 1162 is [True, False, False, False, True, False]
State prediction error at timestep 1162 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1162 of 1
Current timestep = 1163. State = [[-0.14822997  0.0703541 ]]. Action = [[0.22977358 0.17492408 0.08762872 0.8742405 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1163 is [True, False, False, False, True, False]
Scene graph at timestep 1163 is [True, False, False, False, True, False]
State prediction error at timestep 1163 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1163 of 1
Current timestep = 1164. State = [[-0.11640101  0.07749952]]. Action = [[ 0.14172053 -0.12245676  0.16232657  0.09537041]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1164 is [True, False, False, False, True, False]
Current timestep = 1165. State = [[-0.09480544  0.07405341]]. Action = [[ 0.23144141  0.00952825 -0.2257987  -0.419155  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1165 is [True, False, False, False, True, False]
Current timestep = 1166. State = [[-0.06877581  0.07755377]]. Action = [[0.14215493 0.09884065 0.00448412 0.19965732]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1166 is [True, False, False, False, True, False]
Scene graph at timestep 1166 is [True, False, False, False, True, False]
State prediction error at timestep 1166 is tensor(9.1280e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1166 of 1
Current timestep = 1167. State = [[-0.04201528  0.07257434]]. Action = [[ 0.18057358 -0.16480869  0.06690443  0.48168266]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1167 is [True, False, False, False, True, False]
Scene graph at timestep 1167 is [False, True, False, False, True, False]
State prediction error at timestep 1167 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1167 of 1
Current timestep = 1168. State = [[-0.2549207  -0.08833198]]. Action = [[ 0.20003691 -0.13328442 -0.17426826  0.18395233]]. Reward = [100.]
Curr episode timestep = 10
Scene graph at timestep 1168 is [False, True, False, False, True, False]
Scene graph at timestep 1168 is [True, False, False, False, True, False]
State prediction error at timestep 1168 is tensor(0.0393, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1168 of 0
Current timestep = 1169. State = [[-0.23998031 -0.1091681 ]]. Action = [[ 0.23999524 -0.19347022  0.01986608 -0.35077274]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1169 is [True, False, False, False, True, False]
Scene graph at timestep 1169 is [True, False, False, False, True, False]
State prediction error at timestep 1169 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1169 of 1
Current timestep = 1170. State = [[-0.22121483 -0.1253659 ]]. Action = [[-0.01095718  0.03369784 -0.2313435  -0.34652674]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1170 is [True, False, False, False, True, False]
Scene graph at timestep 1170 is [True, False, False, True, False, False]
State prediction error at timestep 1170 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1170 of 1
Current timestep = 1171. State = [[-0.21726407 -0.13335562]]. Action = [[ 0.08110777 -0.15127485 -0.22077085  0.89374256]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1171 is [True, False, False, True, False, False]
Current timestep = 1172. State = [[-0.20131606 -0.13075896]]. Action = [[0.21634236 0.21191633 0.09045833 0.13382769]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1172 is [True, False, False, True, False, False]
Scene graph at timestep 1172 is [True, False, False, True, False, False]
State prediction error at timestep 1172 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1172 of 1
Current timestep = 1173. State = [[-0.17984164 -0.10744065]]. Action = [[-0.00258783  0.2084615  -0.09693128  0.08613229]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1173 is [True, False, False, True, False, False]
Scene graph at timestep 1173 is [True, False, False, False, True, False]
State prediction error at timestep 1173 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1173 of 1
Current timestep = 1174. State = [[-0.16938779 -0.07645473]]. Action = [[0.1943528  0.22075194 0.24408573 0.32511115]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1174 is [True, False, False, False, True, False]
Scene graph at timestep 1174 is [True, False, False, False, True, False]
State prediction error at timestep 1174 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1174 of 1
Current timestep = 1175. State = [[-0.14442776 -0.04472931]]. Action = [[ 0.19926137  0.24041444  0.05556864 -0.07768822]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1175 is [True, False, False, False, True, False]
Scene graph at timestep 1175 is [True, False, False, False, True, False]
State prediction error at timestep 1175 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1175 of 1
Current timestep = 1176. State = [[-0.12490366 -0.02472135]]. Action = [[-0.03351422 -0.00709324 -0.02669469 -0.22752285]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1176 is [True, False, False, False, True, False]
Current timestep = 1177. State = [[-0.12589067 -0.02783566]]. Action = [[-0.09015757 -0.05768687  0.13922852  0.00732517]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1177 is [True, False, False, False, True, False]
Scene graph at timestep 1177 is [True, False, False, False, True, False]
State prediction error at timestep 1177 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1177 of 1
Current timestep = 1178. State = [[-0.13479762 -0.03521972]]. Action = [[-0.21828914 -0.06265269 -0.17490707 -0.01322204]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1178 is [True, False, False, False, True, False]
Scene graph at timestep 1178 is [True, False, False, False, True, False]
State prediction error at timestep 1178 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1178 of -1
Current timestep = 1179. State = [[-0.14496066 -0.02972355]]. Action = [[ 0.20647937  0.1890102  -0.16269556 -0.57524973]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1179 is [True, False, False, False, True, False]
Scene graph at timestep 1179 is [True, False, False, False, True, False]
State prediction error at timestep 1179 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1179 of 1
Current timestep = 1180. State = [[-0.13486782 -0.00834321]]. Action = [[ 0.14626458  0.11598366  0.22221652 -0.89315337]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1180 is [True, False, False, False, True, False]
Current timestep = 1181. State = [[-0.12220736  0.00329959]]. Action = [[0.02880764 0.06377724 0.22622573 0.30756998]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1181 is [True, False, False, False, True, False]
Scene graph at timestep 1181 is [True, False, False, False, True, False]
State prediction error at timestep 1181 is tensor(7.0618e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1181 of 1
Current timestep = 1182. State = [[-0.10855258  0.00491013]]. Action = [[ 0.20888942 -0.10410082 -0.20531623 -0.7968444 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1182 is [True, False, False, False, True, False]
Current timestep = 1183. State = [[-0.08184611 -0.00951787]]. Action = [[ 0.19187349 -0.18541637 -0.13962457 -0.6281038 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1183 is [True, False, False, False, True, False]
Current timestep = 1184. State = [[-0.05333945 -0.00999289]]. Action = [[ 0.23457175  0.20632964  0.18090841 -0.41933274]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1184 is [True, False, False, False, True, False]
Current timestep = 1185. State = [[-0.24409477  0.07444678]]. Action = [[ 0.24290955 -0.24012099  0.23725343 -0.69169503]]. Reward = [100.]
Curr episode timestep = 16
Scene graph at timestep 1185 is [True, False, False, False, True, False]
Scene graph at timestep 1185 is [True, False, False, False, True, False]
State prediction error at timestep 1185 is tensor(0.0225, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1185 of 0
Current timestep = 1186. State = [[-0.22883165  0.08007209]]. Action = [[ 0.15091157 -0.09556526 -0.06262112  0.9144052 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1186 is [True, False, False, False, True, False]
Current timestep = 1187. State = [[-0.21731307  0.07571654]]. Action = [[ 0.07306731 -0.04129347 -0.18341821 -0.80675465]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1187 is [True, False, False, False, True, False]
Current timestep = 1188. State = [[-0.20110951  0.08222991]]. Action = [[ 0.1425516   0.1951535  -0.22795293  0.80125046]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1188 is [True, False, False, False, True, False]
Scene graph at timestep 1188 is [True, False, False, False, True, False]
State prediction error at timestep 1188 is tensor(7.9795e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1188 of 1
Current timestep = 1189. State = [[-0.18673214  0.08719291]]. Action = [[-0.0333589  -0.13808997  0.18391368 -0.9340941 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1189 is [True, False, False, False, True, False]
Scene graph at timestep 1189 is [True, False, False, False, True, False]
State prediction error at timestep 1189 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1189 of 1
Current timestep = 1190. State = [[-0.1866911   0.07992856]]. Action = [[-0.01932083  0.01789397 -0.13554187 -0.6377571 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1190 is [True, False, False, False, True, False]
Current timestep = 1191. State = [[-0.18029621  0.07033555]]. Action = [[ 0.17160851 -0.15979156  0.20477468 -0.6353213 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1191 is [True, False, False, False, True, False]
Scene graph at timestep 1191 is [True, False, False, False, True, False]
State prediction error at timestep 1191 is tensor(5.1969e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1191 of 1
Current timestep = 1192. State = [[-0.15895313  0.07165667]]. Action = [[0.24104184 0.2147915  0.04363242 0.30632675]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1192 is [True, False, False, False, True, False]
Scene graph at timestep 1192 is [True, False, False, False, True, False]
State prediction error at timestep 1192 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1192 of 1
Current timestep = 1193. State = [[-0.12647486  0.09225003]]. Action = [[0.13694859 0.14049643 0.23151612 0.79262745]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1193 is [True, False, False, False, True, False]
Scene graph at timestep 1193 is [True, False, False, False, True, False]
State prediction error at timestep 1193 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1193 of 1
Current timestep = 1194. State = [[-0.11107359  0.1035077 ]]. Action = [[ 8.8763058e-02  1.5023351e-04 -8.4086344e-02 -7.4321216e-01]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1194 is [True, False, False, False, True, False]
Scene graph at timestep 1194 is [True, False, False, False, True, False]
State prediction error at timestep 1194 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1194 of 1
Current timestep = 1195. State = [[-0.09129471  0.10672705]]. Action = [[ 0.23343527  0.02679437  0.07385805 -0.32182848]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1195 is [True, False, False, False, True, False]
Current timestep = 1196. State = [[-0.06290507  0.10972387]]. Action = [[ 0.19118237 -0.01636787 -0.05449301  0.35018635]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1196 is [True, False, False, False, True, False]
Current timestep = 1197. State = [[-0.21264723  0.03105935]]. Action = [[ 0.12947625 -0.23407474 -0.05963972 -0.56676626]]. Reward = [100.]
Curr episode timestep = 11
Scene graph at timestep 1197 is [True, False, False, False, True, False]
Scene graph at timestep 1197 is [True, False, False, False, True, False]
State prediction error at timestep 1197 is tensor(0.0154, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1197 of 0
Current timestep = 1198. State = [[-0.20365512  0.05094222]]. Action = [[ 0.0227614   0.21683773 -0.17254345  0.9174465 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1198 is [True, False, False, False, True, False]
Current timestep = 1199. State = [[-0.1918136   0.06887914]]. Action = [[0.24864137 0.08613062 0.21052033 0.5601337 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1199 is [True, False, False, False, True, False]
Current timestep = 1200. State = [[-0.16638781  0.08884761]]. Action = [[0.06676346 0.15390596 0.1934244  0.29626274]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1200 is [True, False, False, False, True, False]
Scene graph at timestep 1200 is [True, False, False, False, True, False]
State prediction error at timestep 1200 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1200 of 1
Current timestep = 1201. State = [[-0.15638398  0.10240874]]. Action = [[ 0.00470847 -0.03727236  0.22989264 -0.4494654 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1201 is [True, False, False, False, True, False]
Current timestep = 1202. State = [[-0.14818175  0.10370888]]. Action = [[ 0.1857292   0.05152574  0.13040614 -0.69362366]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1202 is [True, False, False, False, True, False]
Scene graph at timestep 1202 is [True, False, False, False, True, False]
State prediction error at timestep 1202 is tensor(3.1261e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1202 of 1
Current timestep = 1203. State = [[-0.11816167  0.10830874]]. Action = [[0.19971868 0.05112562 0.1861304  0.20199561]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1203 is [True, False, False, False, True, False]
Scene graph at timestep 1203 is [True, False, False, False, True, False]
State prediction error at timestep 1203 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1203 of 1
Current timestep = 1204. State = [[-0.0953344   0.11149127]]. Action = [[ 0.07549286 -0.04219247  0.00562441 -0.684982  ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1204 is [True, False, False, False, True, False]
Scene graph at timestep 1204 is [True, False, False, False, True, False]
State prediction error at timestep 1204 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1204 of 1
Current timestep = 1205. State = [[-0.07684916  0.100096  ]]. Action = [[ 0.22334439 -0.1513722   0.11033672 -0.69484967]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1205 is [True, False, False, False, True, False]
Current timestep = 1206. State = [[-0.06099012  0.07961127]]. Action = [[-0.10419697 -0.21118262  0.2077567   0.34494865]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1206 is [True, False, False, False, True, False]
Scene graph at timestep 1206 is [True, False, False, False, True, False]
State prediction error at timestep 1206 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1206 of 1
Current timestep = 1207. State = [[-0.06396324  0.07329424]]. Action = [[-0.07097599  0.20631397 -0.19226338 -0.667066  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1207 is [True, False, False, False, True, False]
Current timestep = 1208. State = [[-0.06216028  0.07323604]]. Action = [[ 0.15581018 -0.192654   -0.02376965  0.44479132]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1208 is [True, False, False, False, True, False]
Scene graph at timestep 1208 is [True, False, False, False, True, False]
State prediction error at timestep 1208 is tensor(1.6095e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1208 of 0
Current timestep = 1209. State = [[-0.05364591  0.06106198]]. Action = [[ 0.11149234 -0.04677938  0.19285583  0.29296684]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1209 is [True, False, False, False, True, False]
Current timestep = 1210. State = [[-0.18607016  0.02508479]]. Action = [[ 0.23575264  0.13102868  0.21786815 -0.4962116 ]]. Reward = [100.]
Curr episode timestep = 12
Scene graph at timestep 1210 is [True, False, False, False, True, False]
Scene graph at timestep 1210 is [True, False, False, False, True, False]
State prediction error at timestep 1210 is tensor(0.0110, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1210 of 0
Current timestep = 1211. State = [[-0.17039403  0.01976135]]. Action = [[ 0.10588354 -0.22991255 -0.14390713  0.92759395]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1211 is [True, False, False, False, True, False]
Scene graph at timestep 1211 is [True, False, False, False, True, False]
State prediction error at timestep 1211 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1211 of 1
Current timestep = 1212. State = [[-0.15850617  0.01617945]]. Action = [[ 0.10010305  0.19662857 -0.17703708 -0.23904967]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1212 is [True, False, False, False, True, False]
Scene graph at timestep 1212 is [True, False, False, False, True, False]
State prediction error at timestep 1212 is tensor(2.7526e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1212 of 1
Current timestep = 1213. State = [[-0.13956147  0.03388039]]. Action = [[0.1677984  0.13051009 0.04157081 0.3771516 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1213 is [True, False, False, False, True, False]
Current timestep = 1214. State = [[-0.12568504  0.03850337]]. Action = [[-0.04818688 -0.11658143 -0.01589903  0.8386588 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1214 is [True, False, False, False, True, False]
Scene graph at timestep 1214 is [True, False, False, False, True, False]
State prediction error at timestep 1214 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1214 of 1
Current timestep = 1215. State = [[-0.11568197  0.04368684]]. Action = [[ 0.23487353  0.16228038 -0.23118384  0.9560461 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1215 is [True, False, False, False, True, False]
Current timestep = 1216. State = [[-0.08535706  0.03858617]]. Action = [[ 0.21747506 -0.22470619  0.19964558  0.6406443 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1216 is [True, False, False, False, True, False]
Scene graph at timestep 1216 is [True, False, False, False, True, False]
State prediction error at timestep 1216 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1216 of 1
Current timestep = 1217. State = [[-0.06240314  0.0125557 ]]. Action = [[-0.06199412 -0.23708285  0.16784003 -0.04038346]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1217 is [True, False, False, False, True, False]
Scene graph at timestep 1217 is [True, False, False, False, True, False]
State prediction error at timestep 1217 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1217 of 1
Current timestep = 1218. State = [[-0.0572965  -0.00169235]]. Action = [[0.20636189 0.09337944 0.18187904 0.73423886]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1218 is [True, False, False, False, True, False]
Scene graph at timestep 1218 is [True, False, False, False, True, False]
State prediction error at timestep 1218 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1218 of 1
Current timestep = 1219. State = [[-0.04070649 -0.00936654]]. Action = [[-0.02089472 -0.199628   -0.06397107  0.8722503 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1219 is [True, False, False, False, True, False]
Scene graph at timestep 1219 is [False, True, False, False, True, False]
State prediction error at timestep 1219 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1219 of 1
Current timestep = 1220. State = [[-0.20427904  0.00631593]]. Action = [[0.20346412 0.2147919  0.05215958 0.7776418 ]]. Reward = [100.]
Curr episode timestep = 9
Scene graph at timestep 1220 is [False, True, False, False, True, False]
Scene graph at timestep 1220 is [True, False, False, False, True, False]
State prediction error at timestep 1220 is tensor(0.0140, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1220 of 0
Current timestep = 1221. State = [[-0.19122942  0.00906895]]. Action = [[ 0.1307596   0.0163323   0.09033284 -0.7592758 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1221 is [True, False, False, False, True, False]
Scene graph at timestep 1221 is [True, False, False, False, True, False]
State prediction error at timestep 1221 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1221 of 1
Current timestep = 1222. State = [[-0.16972306  0.00433599]]. Action = [[ 0.14385763 -0.14302191 -0.01531252 -0.05882037]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1222 is [True, False, False, False, True, False]
Scene graph at timestep 1222 is [True, False, False, False, True, False]
State prediction error at timestep 1222 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1222 of 1
Current timestep = 1223. State = [[-0.14695658 -0.00148128]]. Action = [[ 0.23491237  0.0294005  -0.14516163 -0.31528634]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1223 is [True, False, False, False, True, False]
Scene graph at timestep 1223 is [True, False, False, False, True, False]
State prediction error at timestep 1223 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1223 of 1
Current timestep = 1224. State = [[-0.12146686  0.00024101]]. Action = [[-0.01833209  0.04291916  0.09740916  0.31660748]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1224 is [True, False, False, False, True, False]
Scene graph at timestep 1224 is [True, False, False, False, True, False]
State prediction error at timestep 1224 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1224 of 1
Current timestep = 1225. State = [[-0.11439488  0.00434583]]. Action = [[ 0.17599541  0.03032315 -0.00585331  0.65303767]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1225 is [True, False, False, False, True, False]
Current timestep = 1226. State = [[-0.09285829  0.01697353]]. Action = [[ 0.18440658  0.17605117 -0.14626968 -0.61688215]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1226 is [True, False, False, False, True, False]
Current timestep = 1227. State = [[-0.07573655  0.03498543]]. Action = [[-0.00047913  0.0952656   0.12018842  0.14047599]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1227 is [True, False, False, False, True, False]
Current timestep = 1228. State = [[-0.06779489  0.04451877]]. Action = [[0.09649342 0.0310303  0.07470965 0.5576408 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1228 is [True, False, False, False, True, False]
Current timestep = 1229. State = [[-0.0518125   0.03570275]]. Action = [[ 0.16542393 -0.22513144 -0.08766806 -0.9133457 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1229 is [True, False, False, False, True, False]
Scene graph at timestep 1229 is [True, False, False, False, True, False]
State prediction error at timestep 1229 is tensor(2.7616e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1229 of 1
Current timestep = 1230. State = [[-0.2641639  -0.17843798]]. Action = [[ 0.17545009 -0.14114513  0.16705117 -0.4164257 ]]. Reward = [100.]
Curr episode timestep = 9
Scene graph at timestep 1230 is [True, False, False, False, True, False]
Current timestep = 1231. State = [[-0.2570634  -0.19697388]]. Action = [[ 0.18641803  0.02877545  0.12082699 -0.3148762 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1231 is [True, False, False, True, False, False]
Scene graph at timestep 1231 is [True, False, False, True, False, False]
State prediction error at timestep 1231 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1231 of 1
Current timestep = 1232. State = [[-0.23628587 -0.18853518]]. Action = [[0.09051865 0.14074826 0.15073201 0.9926659 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1232 is [True, False, False, True, False, False]
Current timestep = 1233. State = [[-0.21955825 -0.17859814]]. Action = [[0.22821847 0.02104008 0.1815486  0.06841445]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1233 is [True, False, False, True, False, False]
Current timestep = 1234. State = [[-0.2060652  -0.17821844]]. Action = [[-0.12834977 -0.05711931  0.03415895 -0.4777695 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1234 is [True, False, False, True, False, False]
Scene graph at timestep 1234 is [True, False, False, True, False, False]
State prediction error at timestep 1234 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1234 of 1
Current timestep = 1235. State = [[-0.20377426 -0.1897587 ]]. Action = [[ 0.10942188 -0.16867632 -0.1078316  -0.59973955]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1235 is [True, False, False, True, False, False]
Scene graph at timestep 1235 is [True, False, False, True, False, False]
State prediction error at timestep 1235 is tensor(3.2461e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1235 of -1
Current timestep = 1236. State = [[-0.19446082 -0.2106204 ]]. Action = [[ 0.12233824 -0.18247539 -0.04163723 -0.5294159 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1236 is [True, False, False, True, False, False]
Current timestep = 1237. State = [[-0.17763102 -0.2148747 ]]. Action = [[0.10067528 0.17816108 0.22060311 0.60126793]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1237 is [True, False, False, True, False, False]
Current timestep = 1238. State = [[-0.16684322 -0.21081045]]. Action = [[ 0.09975621 -0.0450592   0.20023066  0.4850844 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1238 is [True, False, False, True, False, False]
Scene graph at timestep 1238 is [True, False, False, True, False, False]
State prediction error at timestep 1238 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1238 of 0
Current timestep = 1239. State = [[-0.1477559 -0.197586 ]]. Action = [[ 0.145235    0.24413693 -0.08263162  0.8287456 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1239 is [True, False, False, True, False, False]
Scene graph at timestep 1239 is [True, False, False, True, False, False]
State prediction error at timestep 1239 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1239 of 1
Current timestep = 1240. State = [[-0.12639894 -0.18781012]]. Action = [[ 0.24320784 -0.19325006  0.10809615  0.25340664]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1240 is [True, False, False, True, False, False]
Scene graph at timestep 1240 is [True, False, False, True, False, False]
State prediction error at timestep 1240 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1240 of 1
Current timestep = 1241. State = [[-0.08759531 -0.18925011]]. Action = [[ 0.18535465  0.24602056  0.07018572 -0.0391565 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1241 is [True, False, False, True, False, False]
Scene graph at timestep 1241 is [True, False, False, True, False, False]
State prediction error at timestep 1241 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1241 of 1
Current timestep = 1242. State = [[-0.06345087 -0.18136325]]. Action = [[ 0.19260964 -0.16897397  0.16468233  0.24533772]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1242 is [True, False, False, True, False, False]
Scene graph at timestep 1242 is [True, False, False, True, False, False]
State prediction error at timestep 1242 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1242 of 0
Current timestep = 1243. State = [[-0.03772724 -0.18192759]]. Action = [[ 0.05807495  0.19667584  0.08236268 -0.55735546]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1243 is [True, False, False, True, False, False]
Current timestep = 1244. State = [[-0.02925496 -0.1622516 ]]. Action = [[ 0.16181195  0.12572801  0.07195461 -0.58985895]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1244 is [False, True, False, True, False, False]
Current timestep = 1245. State = [[-0.00612685 -0.15039521]]. Action = [[ 0.21969122  0.01777473 -0.06914388 -0.27330744]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1245 is [False, True, False, True, False, False]
Current timestep = 1246. State = [[ 0.02206678 -0.14871524]]. Action = [[ 1.6154420e-01  5.6114793e-04  2.3243943e-01 -6.5097845e-01]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1246 is [False, True, False, True, False, False]
Current timestep = 1247. State = [[ 0.04034683 -0.15405998]]. Action = [[-0.04318729 -0.10342839  0.15654165  0.39397418]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1247 is [False, True, False, True, False, False]
Current timestep = 1248. State = [[ 0.04076787 -0.14913943]]. Action = [[-0.12873083  0.17715383  0.00525331  0.59360874]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1248 is [False, True, False, True, False, False]
Current timestep = 1249. State = [[ 0.03996577 -0.13012762]]. Action = [[ 0.09632069  0.13131347 -0.21221496  0.35661232]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1249 is [False, True, False, True, False, False]
